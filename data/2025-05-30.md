<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 37]
- [cs.LG](#cs.LG) [Total: 163]
- [cs.CR](#cs.CR) [Total: 14]
- [quant-ph](#quant-ph) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 39]
- [cs.NI](#cs.NI) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 3]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.SD](#cs.SD) [Total: 6]
- [math.NA](#math.NA) [Total: 2]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.NE](#cs.NE) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.IR](#cs.IR) [Total: 4]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/abs/2505.22698)
*Luca Fantin,Marco Antonelli,Margherita Cesetti,Daniele Irto,Bruno Zamengo,Francesco Silvestri*

Main category: cs.AI

TL;DR: The paper introduces a chatbot using an agent architecture to expand the capabilities of LLM for analyzing public transportation data, and presents a method for evaluating the chatbot's performance.


<details>
  <summary>Details</summary>
Motivation: To provide a user-friendly tool for assessing the quality of public transportation services by simplifying the complex process of interrogating datasets with SQL queries, organizing and visualizing data.

Method: Developing a chatbot based on an agent architecture that interacts with tools for executing tasks such as SQL queries, plotting data, and creating maps. The chatbot is tested through a workflow that asks questions, stores generated queries, retrieved data, and natural language responses.

Result: The procedure results in a dataset for evaluating the chatbot's performance, focusing on the consistency of answers and the correctness of generated queries.

Conclusion: This approach offers a practical solution for interacting with complex transportation datasets and provides a method for collecting data to measure the chatbot's performance.

Abstract: Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [2] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/abs/2505.22753)
*Arseniy Pertzovsky,Roni Stern,Ariel Felner,Roie Zivan*

Main category: cs.AI

TL;DR: Artificial Potential Fields (APFs) are explored for solving Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. While not beneficial for MAPF, APF yields up to a 7-fold increase in system throughput for LMAPF.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of Artificial Potential Fields (APFs) in solving Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems, aiming to improve the efficiency and throughput of multi-agent systems.

Method: Incorporate APFs into various MAPF algorithms such as Prioritized Planning, MAPF-LNS2, and Priority Inheritance with Backtracking (PIBT) to address both MAPF and LMAPF problems.

Result: Experimental results indicate that using APFs does not provide benefits for MAPF but significantly improves the overall system throughput for LMAPF by up to 7 times.

Conclusion: The use of APFs is more advantageous for Lifelong MAPF problems compared to standard MAPF problems, offering substantial improvements in system throughput.

Abstract: We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [3] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: Mathematical reasoning tasks are key benchmarks for LLMs. While RL methods like GRPO improve execution robustness, they struggle with new problems due to insufficient planning skills. A synthetic task confirms this and identifies conditions for overcoming limitations.


<details>
  <summary>Details</summary>
Motivation: To better understand the capabilities of LLMs in mathematical reasoning, especially those trained with RL methods, by decomposing problem-solving into fundamental skills.

Method: Decompose problem-solving into Plan, Execute, and Verify. Use empirical studies and a synthetic solution-tree navigation task to analyze the impact of RL on these skills.

Result: GRPO mainly enhances execution skill through temperature distillation. RL-trained models face a 'coverage wall' due to insufficient planning skills. The synthetic task replicates findings and identifies conditions for overcoming limitations.

Conclusion: RL improves execution robustness but has limitations in planning. Insights suggest potential paths to overcome these barriers.

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [4] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/abs/2505.22779)
*Mohammad Helal Uddin,Sabur Baidya*

Main category: cs.AI

TL;DR: This paper presents a hybrid approach using smartphone sensor data and social media analysis to evaluate depression levels. It uses CNN-based deep learning models for physical activity recognition and Naive Bayes classification for sentiment analysis, achieving high accuracies in both areas. The study involved 33 participants and found significant correlations between physical activity features and depression severity. A SVM-based algorithm achieved 94% accuracy in classifying depression severity.


<details>
  <summary>Details</summary>
Motivation: Mental disorders like depression pose a global challenge, especially among individuals with social avoidance tendencies. There is a need for effective methods to monitor and evaluate depression without compromising personal privacy.

Method: The study used a hybrid approach combining smartphone sensor data for physical activity recognition and social media (Twitter) interactions for sentiment analysis. CNN-based deep learning models were employed for activity recognition, while Naive Bayes classification was used for sentiment analysis. Nine features were extracted from the data, and a support vector machine (SVM)-based algorithm was utilized to classify depression severity.

Result: The activity recognition accuracy was 95%, and the sentiment analysis accuracy was 95.6%. Several physical activity features showed significant correlations with depression severity. The SVM-based algorithm achieved an accuracy of 94% in classifying depression severity, outperforming other models such as MLP and k-nearest neighbor.

Conclusion: The proposed hybrid approach is simple yet highly effective for long-term depression monitoring without breaching personal privacy.

Abstract: Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [5] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/abs/2505.22871)
*Yuval David,Fabiana Fournier,Lior Limonad,Inna Skarbovsky*

Main category: cs.AI

TL;DR: The paper proposes a novel method to unify multiple causal process variants into a consistent model that preserves the original causal relationships and represents causal-flow alternations.


<details>
  <summary>Details</summary>
Motivation: Causal reasoning is essential for business process interventions and improvement. Current methods for discovering causal process models lack the ability to capture alternating causal conditions across multiple variants, which raises challenges in handling missing values and expressing these conditions among log splits.

Method: The authors propose a new method that unifies multiple causal process variants into a consistent model. This method preserves the correctness of the original causal models while explicitly representing their causal-flow alternations. The method is formally defined and proved.

Result: The proposed method was evaluated on three open and two proprietary datasets, demonstrating its effectiveness. An open-source implementation of the method has also been released.

Conclusion: The novel method successfully addresses the limitations of previous approaches by creating a unified causal process model that captures alternating causal conditions, thus advancing causal reasoning in business processes.

Abstract: Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [6] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti,Michela Lorandi,Paul Flanagan,Oisin Redmon,Anya Belz,Yufang Hou*

Main category: cs.AI

TL;DR: This paper presents a quantitative reasoning system for systematic reviews in medicine, which extracts structured numerical evidence and applies domain knowledge to derive outcome-specific conclusions. The system includes a numeric data extraction model trained via supervised fine-tuning and reinforcement learning, as well as an effect estimate component. Evaluated on CochraneForest benchmark, it shows significant improvement over retrieval-based systems and large language models.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews are crucial for evidence-based decision-making but face bottlenecks in automating the process of extracting numeric evidence and determining study-level conclusions. Previous methods relying on textual inference often fail to capture numeric reasoning behind expert assessments.

Method: The authors conceptualize the problem as one of quantitative reasoning, developing a system that extracts structured numerical evidence and uses domain-informed logic to draw conclusions. This system comprises a numeric data extraction model trained with strategies like supervised fine-tuning (SFT) and reinforcement learning (RL), and an effect estimate component.

Result: On the CochraneForest benchmark, the best-performing approach using RL for training a small-scale number extraction model improved the F1 score by 21% compared to retrieval-based systems and outperformed general-purpose LLMs by up to 9%.

Conclusion: The results indicate the potential of reasoning-driven approaches for automating systematic evidence synthesis in medical reviews.

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [7] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/abs/2505.22948)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.AI

TL;DR: The paper introduces Foundation Molecular Grammar (FMG), which uses multi-modal foundation models to create an interpretable molecular language, improving on previous methods in molecular generation and property prediction.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current grammar learning approaches that rely on expert annotation or unreliable heuristics, the authors aim to develop a more autonomous and reliable method for generating interpretable molecular structures.

Method: FMG leverages multi-modal foundation models to render molecules as images, describe them as text, and align information across modalities using prompt learning, exploiting the chemical knowledge inherent in the MMFM.

Result: FMG outperforms prior methods in synthesizability, diversity, and data efficiency while providing built-in chemical interpretability for molecular discovery workflows.

Conclusion: FMG serves as an effective replacement for previous grammar learning approaches in molecular generation and property prediction, offering enhanced performance and interpretability.

Abstract: Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [8] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)
*Jenny Zhang,Shengran Hu,Cong Lu,Robert Lange,Jeff Clune*

Main category: cs.AI

TL;DR: The paper introduces the Darwin Gödel Machine (DGM), a self-improving AI system that iteratively modifies its own code and empirically validates each change using coding benchmarks. It significantly improves coding capabilities and outperforms baselines without self-improvement.


<details>
  <summary>Details</summary>
Motivation: Current AI systems have fixed architectures and lack the ability to autonomously and continuously improve themselves. Meta-learning has limitations, and while the Gödel machine proposes a theoretical alternative, proving beneficial changes is impractical.

Method: The DGM maintains an archive of generated coding agents, grows it by sampling an agent and creating a new version using a foundation model, forming a tree of diverse agents. It empowers open-ended exploration and parallel exploration of different paths in the search space.

Result: Empirically, the DGM improved performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. It significantly outperformed baselines without self-improvement or open-ended exploration.

Conclusion: The DGM represents a significant step toward self-improving AI, capable of gathering stepping stones along paths leading to endless innovation.

Abstract: Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [9] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/abs/2505.22960)
*Yongjin Yang,Euiin Yi,Jongwoo Ko,Kimin Lee,Zhijing Jin,Se-Young Yun*

Main category: cs.AI

TL;DR: 本研究系统地比较了多代理辩论（MAD）方法与自代理测试时间扩展基线在数学推理和安全相关任务中的表现，揭示了任务难度、模型规模和代理多样性对MAD性能的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究利用了多代理辩论（MAD）方法，但对其在不同条件下的有效性，特别是与自代理方法相比的系统性理解仍然不足。

Method: 将MAD概念化为一种测试时间计算扩展技术，通过协作改进和多样化探索能力进行操作。在数学推理和安全相关任务上，与强大的自代理测试时间扩展基线进行全面的实证对比研究。系统考察任务难度、模型规模和代理多样性对MAD性能的影响。

Result: 对于数学推理，MAD相较于自代理扩展优势有限，但在问题难度增加和模型能力下降时更为有效，而代理多样性几乎没有益处。对于安全任务，MAD的协作改进可能会增加脆弱性，但通过引入多样化的代理配置，可以逐步降低攻击成功率。

Conclusion: 研究结果为未来更有效且战略性部署的MAD系统的开发提供了关键指导。

Abstract: The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [10] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/abs/2505.22987)
*Nick Byrd*

Main category: cs.AI

TL;DR: In the late 20th century, rationality debates led to dual process theories which are now applied to AI. This paper synthesizes historical ideas with recent experiments to propose Strategic Reflectivism, emphasizing pragmatic switching between intuitive and reflective inference for intelligent systems.


<details>
  <summary>Details</summary>
Motivation: To incorporate historical lessons on rationality and dual process theories into the current development of artificial intelligence.

Method: Synthesizing old ideas from rationality debates with recent experimental results from studies on humans and machines.

Result: The proposal of Strategic Reflectivism as a key approach for intelligent systems to optimally fulfill competing goals through switching between intuitive and reflective inference.

Conclusion: Strategic Reflectivism transcends superficial indicators of reflective thinking and highlights the importance of balancing intuition and reflection in both human and artificial intelligent systems.

Abstract: By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [11] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/abs/2505.22990)
*Pin-Han Chen,Yu-Sheng Lin,Wei-Cheng Lee,Tin-Yu Leu,Po-Hsiang Hsu,Anjana Dissanayake,Sungjin Oh,Chinq-Shiun Chiu*

Main category: cs.AI

TL;DR: RF/Analog design is crucial but complex; MenTeR, a multiagent AI framework, simplifies and accelerates the process.


<details>
  <summary>Details</summary>
Motivation: Analog design procedures are intricate, time-consuming, and reliant on expert intuition, which hinders the efficiency of circuit development.

Method: MenTeR employs multiple specialized AI agents that collaboratively address different aspects of the analog design process, such as specification understanding, circuit optimization, and test bench validation.

Result: MenTeR reduces dependency on trial-and-error intervention, accelerates design cycle time, and facilitates broader exploration of the design space.

Conclusion: MenTeR demonstrates robust capabilities in handling real-world analog systems and lays the groundwork for future RF/Analog Copilots that can collaborate with human designers.

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [12] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/abs/2505.23034)
*Guangyi Liu,Yongqi Zhang,Xunyuan Liu,Quanming Yao*

Main category: cs.AI

TL;DR: The paper presents CBR-DDI, a new framework leveraging LLMs and GNNs to distill pharmacological principles from historical cases for improving DDI prediction tasks. It introduces hybrid retrieval, dual-layer prompting, and dynamic case refinement strategies, achieving a 28.7% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: Drug-drug interaction (DDI) prediction is crucial for treatment safety, but using large language models (LLMs) in this area remains challenging.

Method: CBR-DDI constructs a knowledge repository using LLMs to extract pharmacological insights and GNNs to model drug associations. It employs a hybrid retrieval mechanism and dual-layer knowledge-enhanced prompting for effective case retrieval and reuse, along with a representative sampling strategy for dynamic case refinement.

Result: Extensive experiments show that CBR-DDI achieves state-of-the-art performance with a significant 28.7% accuracy improvement over popular LLMs and CBR baseline, while maintaining high interpretability and flexibility.

Conclusion: CBR-DDI effectively improves LLM reasoning for DDI tasks by distilling pharmacological principles from historical cases, achieving superior performance and maintaining interpretability.

Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [13] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie,Zhuoheng Li,Xiyuan Wang,Yijun Pan,Qijia Liu,Xingzhi Cui,Kuang-Yu Lo,Ruoyi Gao,Xingjian Zhang,Jin Huang,Walter Yuan,Matthew O. Jackson,Qiaozhu Mei*

Main category: cs.AI

TL;DR: Be.FM is one of the first open foundation models designed for human behavior modeling, built upon open-source large language models and fine-tuned on a diverse range of behavioral data. It can predict behaviors, infer characteristics, generate insights and apply behavioral science knowledge.


<details>
  <summary>Details</summary>
Motivation: The potential of foundation models for modeling and understanding human behavior remains largely unexplored.

Method: Introduced Be.FM, built upon open-source large language models and fine-tuned on a diverse range of behavioral data.

Result: Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.

Conclusion: Constructed a comprehensive set of benchmark tasks for testing the capabilities of behavioral foundation models.

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [14] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/abs/2505.23075)
*Amit Kumthekar,Zion Tilley,Henry Duong,Bhargav Patel,Michael Magnoli,Ahmed Omar,Ahmed Nasser,Chaitanya Gharpure,Yevgen Reztzov*

Main category: cs.AI

TL;DR: The paper introduces the Consensus Mechanism, an ensemble framework mimicking clinical decision-making to improve and adapt LLM-based clinical decisions. Evaluated on medical benchmarks, it shows superior accuracy and performance in differential diagnosis compared to individual models.


<details>
  <summary>Details</summary>
Motivation: To address risks of obsolescence and rigid dependence on single model systems in current LLM-based clinical applications, enhancing adaptability and performance.

Method: Development of the Consensus Mechanism which employs an ensemble of specialized medical expert agents, allowing optimization for cost, latency, or performance through interior model configuration.

Result: Achieved higher accuracy than individual models on MedXpertQA (61.0%), MedQA (+3.4%), MedMCQA (+9.1%), and improved recall/precision in differential diagnosis (F1 score 0.326 vs. 0.2886).

Conclusion: The Consensus Mechanism demonstrates potential to enhance clinical decision-making with robust adaptability, suggesting a promising direction for future LLM-based clinical systems.

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [15] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu,Yuhang Liu,Guanghao Zhu,Congkai Xie,Zhen Li,Jianbo Yuan,Xinyao Wang,Qing Li,Shing-Chi Cheung,Shengyu Zhang,Fei Wu,Hongxia Yang*

Main category: cs.AI

TL;DR: Recent advancements in LLMs have shown significant progress in reasoning capabilities. However, extending these achievements to MLLMs presents challenges such as the scarcity of high-quality multimodal reasoning datasets, degradation of reasoning capabilities due to visual processing integration, and risks of incorrect reasoning processes from reinforcement learning. To address these challenges, a novel framework Infi-MMR with three phases is designed to enhance MSLMs' reasoning potential, resulting in the multimodal reasoning model Infi-MMR-3B which achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To systematically unlock the reasoning potential of Multimodal Small Language Models (MSLMs) by addressing the challenges faced in extending reasoning capabilities from LLMs to MLLMs.

Method: The method involves designing a novel framework Infi-MMR consisting of three phases: Foundational Reasoning Activation using textual reasoning datasets, Cross-Modal Reasoning Adaptation using caption-augmented multimodal data, and Multimodal Reasoning Enhancement using curated, caption-free multimodal data.

Result: Infi-MMR-3B achieves state-of-the-art multimodal math reasoning ability with scores of 43.68% on MathVerse testmini, 27.04% on MathVision test, 21.33% on OlympiadBench, and 67.2% on MathVista testmini for general reasoning ability.

Conclusion: The Infi-MMR framework successfully enhances the reasoning abilities of MSLMs, leading to the development of Infi-MMR-3B which demonstrates superior multimodal and general reasoning capabilities.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [16] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/abs/2505.23397)
*Ahmad Mohsin,Helge Janicke,Ahmed Ibrahim,Iqbal H. Sarker,Seyit Camtepe*

Main category: cs.AI

TL;DR: This paper proposes a novel autonomy tiered framework for Human-AI collaboration in Security Operations Centers (SOCs), with five levels of AI autonomy mapped to Human-in-the-Loop roles and trust thresholds. It includes a case study of the cybersecurity AI-Avatar, a fine-tuned LLM-based SOC assistant.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks in SOCs often focus narrowly on automation, lacking systematic structures to manage human oversight, trust calibration, and scalable autonomy with AI. Many assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration.

Method: The authors propose a structured framework integrating AI autonomy, trust calibration, and Human-in-the-loop decision making. This includes a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop roles and task-specific trust thresholds.

Result: The proposed framework enables adaptive and explainable AI integration across core SOC functions, reducing alert fatigue, enhancing response coordination, and strategically calibrating trust.

Conclusion: This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.

Abstract: This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [17] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/abs/2505.23153)
*Fan Wang,Shaoshan Liu*

Main category: cs.AI

TL;DR: Collective Adaptive Intelligence (CAI) is a transformative approach in AI that allows autonomous agents to collaborate and adapt in complex environments. This paper provides a conceptual framework for designing and analyzing CAI, focusing on attributes like task generalization, resilience, scalability, and self-assembly.


<details>
  <summary>Details</summary>
Motivation: There is a need for AI systems that can robustly perform in real-world scenarios characterized by complexity and unpredictability. Current AI often lacks the adaptability and resilience necessary for such environments, motivating the exploration of Collective Adaptive Intelligence.

Method: The paper introduces a conceptual framework for designing and analyzing CAI. It outlines key attributes including task generalization, resilience, scalability, and self-assembly, aiming to connect theoretical foundations with practical engineering methods.

Result: The proposed framework provides a structured foundation for understanding and implementing CAI, which could lead to the development of more resilient, scalable, and adaptable AI systems across various domains.

Conclusion: By offering a comprehensive conceptual framework, this work seeks to guide researchers and practitioners in advancing the field of adaptive, emergent intelligence through the principles of Collective Adaptive Intelligence.

Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [18] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
*Mislav Balunović,Jasper Dekoninck,Ivo Petrov,Nikola Jovanović,Martin Vechev*

Main category: cs.AI

TL;DR: The abstract introduces MathArena, a new benchmark for evaluating LLMs' mathematical reasoning and proof-writing capabilities using real-time math competition problems. It finds signs of contamination in AIME 2024 but impressive reasoning in harder competitions like SMT 2025. Top models score low on proof-writing tasks.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation datasets may be contaminated by online availability and do not assess proof-writing capabilities.

Method: MathArena uses recurring math competitions to provide challenging problems for real-time LLM evaluation, eliminating contamination risk.

Result: Signs of contamination found in AIME 2024; top models perform well on harder tasks like SMT 2025 but score low on proof-writing (USAMO 2025). Evaluated 30 models across 149 problems.

Conclusion: MathArena is an evolving benchmark that will continue tracking LLM progress on mathematical reasoning.

Abstract: The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [19] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/abs/2505.23381)
*Bowen Ping,Minnan Luo,Zhuohang Dang,Chenxi Wang,Chengyou Jia*

Main category: cs.AI

TL;DR: AutoGPS is a neuro-symbolic collaborative framework that solves geometry problems with concise, reliable, and human-interpretable reasoning processes, achieving state-of-the-art performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Geometry problem solving presents distinctive challenges in artificial intelligence, requiring exceptional multimodal comprehension and rigorous mathematical reasoning capabilities. Existing approaches typically fall into two categories: neural-based and symbolic-based methods, both of which exhibit limitations in reliability and interpretability.

Method: AutoGPS employs a Multimodal Problem Formalizer (MPF) and a Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal comprehension to translate geometry problems into structured formal language representations, with feedback from DSR collaboratively. The DSR takes the formalization as input and formulates geometry problem solving as a hypergraph expansion task, executing mathematically rigorous and reliable derivation to produce minimal and human-readable stepwise solutions.

Result: Extensive experimental evaluations demonstrate that AutoGPS achieves state-of-the-art performance on benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms AutoGPS's impressive reliability and interpretability, with 99% stepwise logical coherence.

Conclusion: AutoGPS addresses the challenges of geometry problem solving by providing concise, reliable, and human-interpretable reasoning processes, outperforming existing methods.

Abstract: Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [20] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/abs/2505.23399)
*Jusheng Zhang,Yijia Fan,Wenjun Lin,Ruiqi Chen,Haoyi Jiang,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: GAM-Agent is a multi-agent framework that formulates vision-language reasoning as a non-zero-sum game among agents, enhancing robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To improve vision-language reasoning by leveraging game-theoretic multi-agent interactions, addressing limitations of single-agent or monolithic models.

Method: GAM-Agent introduces specialized base agents for visual perception subtasks and a critical agent for logic verification. Agents communicate through structured claims and evidence, with an uncertainty-aware controller facilitating dynamic collaboration and multi-round debates when needed.

Result: Experiments on four benchmarks show significant performance improvements across various VLM backbones. Particularly, accuracy of small-to-mid scale models increases by 5-6%, and strong models like GPT-4o see enhancements up to 2-3%.

Conclusion: GAM-Agent presents a modular, scalable, and generalizable approach towards reliable and explainable multi-agent multimodal reasoning.

Abstract: We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [21] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/abs/2505.23432)
*Elisa Celis,Lingxiao Huang,Nisheeth K. Vishnoi*

Main category: cs.AI

TL;DR: The paper develops a mathematical framework to model jobs, workers and worker-job fit by decomposing skills into decision-level and action-level subskills. It shows how combining complementary subskills leads to better job success and highlights when GenAI complements human skills.


<details>
  <summary>Details</summary>
Motivation: To understand the role of Generative AI (GenAI) in relation to human workers and determine under what conditions GenAI complements rather than replaces human skills.

Method: A mathematical framework is developed that models jobs, workers, and worker-job fit. Skills are decomposed into decision-level and action-level subskills. The framework analyzes changes in subskill abilities and identifies conditions for sharp transitions in success probability.

Result: The framework establishes sufficient conditions where combining workers with complementary subskills significantly outperforms relying on a single worker. This explains productivity compression phenomena and aligns real-world data with the model using subskill-division methods.

Conclusion: GenAI can complement human skills effectively under certain conditions, rather than replacing them.

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [22] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/abs/2505.23436)
*Daniel Jarne Ornia,Nicholas Bishop,Joel Dyer,Wei-Chen Lee,Ani Calinescu,Doyne Farme,Michael Wooldridge*

Main category: cs.AI

TL;DR: The paper explores AI agents' behavior under resource constraints using a survival bandit framework, offering ways to mitigate misalignment and risky behaviors.


<details>
  <summary>Details</summary>
Motivation: AI agents often face resource constraints or failure risks that can terminate their actions. This leads to implicit trade-offs in their utility-driven behavior. Also, acting on behalf of human principals introduces potential misalignment due to differences in constraint exposure.

Method: The study formalizes the problem with a survival bandit framework, analyzing both theoretically and empirically how survival-driven preference shifts affect agent behavior. It identifies conditions causing misalignment between human objectives and agent incentives.

Result: Theoretical and empirical results show the impact of survival-driven preference shifts. The research pinpoints conditions leading to misalignment and proposes mechanisms to control risk-seeking or risk-averse behaviors in AI agents.

Conclusion: This work enhances understanding of AI agent behavior under survival pressure and provides guidelines for safe deployment in resource-limited environments.

Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [23] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473)
*Xiaorui Wu,Xiaofeng Mao,Fei Li,Xin Zhang,Xiaolu Zhang,Jun Zhou,Yuxiang Peng,Li Zheng,Chong Teng,Donghong Ji,Zhuang Li*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）常因过度保守的安全对齐而拒绝无害的输入查询，损害用户体验。本文提出了一种名为EVOREFUSE的提示优化方法，可以生成多样化的伪恶意指令，有效引发LLM的拒绝反应。利用EVOREFUSE创建了两个新数据集：EVOREFUSE-TEST和EVOREFUSE-ALIGN，前者在9个LLMs上的平均拒绝触发率比次优基准高出140.41%，后者用于监督和基于偏好的对齐训练，显著减少了过度拒绝的情况。


<details>
  <summary>Details</summary>
Motivation: 现有的收集伪恶意指令的方法存在局限性，如手动创建缺乏可扩展性，指令重写无法产生足够多样化和有效的拒绝诱导提示。

Method: EVOREFUSE采用进化算法，通过变异策略和重组探索指令空间的更多方向，迭代演化种子指令以最大化LLM拒绝概率的证据下界。

Result: 使用EVOREFUSE创建的数据集提高了拒绝触发率、词汇多样性和LLM响应置信度分数，并且在对齐训练中显著减少了过度拒绝的情况。分析表明，模型过度关注敏感关键词而忽略更广泛的上下文是导致过度拒绝的原因。

Conclusion: EVOREFUSE提供了一种有效的方法来生成多样化的伪恶意指令，有助于评估和缓解LLM的过度拒绝问题，同时不影响安全性。

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [24] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li,Haiyang Yu,Xinghua Zhang,Ziyang Huang,Shizhu He,Kang Liu,Jun Zhao,Fei Huang,Yongbin Li*

Main category: cs.AI

TL;DR: Process Reward Models (PRMs) are essential for complex reasoning tasks. However, current benchmarks do not systematically evaluate PRMs under various reasoning patterns. To address this, the authors introduce Socratic-PRMBench, a new benchmark comprising 2995 reasoning paths with flaws across six reasoning patterns. Experiments reveal significant weaknesses in existing PRMs.


<details>
  <summary>Details</summary>
Motivation: There is a need for a systematic evaluation of PRMs under various reasoning patterns as current benchmarks mainly focus on stepwise correctness.

Method: The authors developed Socratic-PRMBench, a benchmark that evaluates PRMs systematically under six reasoning patterns including Transformation, Decomposition, Regather, Deduction, Verification, and Integration.

Result: Experiments on both PRMs and LLMs prompted as critic models identified notable deficiencies in existing PRMs.

Conclusion: Socratic-PRMBench can serve as a comprehensive testbed for the systematic evaluation of PRMs under diverse reasoning patterns, highlighting areas for future development.

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [25] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2505.23486)
*Ke Weng,Lun Du,Sirui Li,Wangyue Lu,Haozhe Sun,Hengyu Liu,Tiancheng Zhang*

Main category: cs.AI

TL;DR: Autoformalization transforms informal math propositions into formal representations, a key task in automated theorem proving. This survey explores recent advances from mathematical and LLM perspectives, its application across domains, end-to-end workflow analysis, enhancing LLM outputs' verifiability, open-source resources, and future directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the recent progress in autoformalization driven by advancements in artificial intelligence and large language models, highlighting its significance in automated theorem proving and improving LLMs.

Method: The survey examines autoformalization's applications across different mathematical domains and difficulty levels, analyzes the entire workflow from data preprocessing to model design and evaluation, and explores its role in enhancing the verifiability of LLM-generated outputs.

Result: A detailed understanding of how autoformalization is advancing, its impact on various mathematical fields, and its potential to improve the trustworthiness and reasoning capabilities of LLMs.

Conclusion: Autoformalization plays a crucial role in automated theorem proving and improving LLMs. The survey outlines key open-source models and datasets, discusses current challenges, and suggests promising research directions.

Abstract: Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [26] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518)
*Hangoo Kang,Jehyeok Yeon,Gagandeep Singh*

Main category: cs.AI

TL;DR: An abstract about TRAP, a new adversarial framework using diffusion-based semantic injections to manipulate agentic AI systems' decision-making without visible pixel perturbations or model access. Evaluated on COCO dataset, it achieves 100% attack success rate on leading models, exposing semantic vulnerabilities in cross-modal decision-making.


<details>
  <summary>Details</summary>
Motivation: To address the new attack surfaces for adversarial manipulation that exploit semantic reasoning across modalities in autonomous agentic AI systems powered by vision-language models (VLMs). Existing adversarial attacks are either visible or require privileged access, making them impractical for stealthy exploitation.

Method: TRAP combines negative prompt-based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. It uses diffusion-based semantic injections to produce visually natural images that induce consistent selection biases in agentic AI systems without requiring access to model internals.

Result: Evaluated on the Microsoft Common Objects in Context (COCO) dataset, TRAP achieves a 100% attack success rate on leading models including LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such as SPSA, Bandit, and standard diffusion approaches.

Conclusion: Autonomous agents can be consistently misled through human-imperceptible cross-modal manipulations, highlighting the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making.

Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [27] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/abs/2505.23519)
*Ruiqi He,Falk Lieder*

Main category: cs.AI

TL;DR: 人们通过强化学习来获取高效的规划策略，但个体间存在差异。本研究探讨了通过加入内在认知机制（如元认知伪奖励、主观努力评估和终止思考）是否可以缩小模型与人类表现之间的差距。虽然这些机制有助于策略发现，但仍需进一步探索其他可能因素。


<details>
  <summary>Details</summary>
Motivation: 尽管先前研究表明元认知强化学习（MCRL）模型能学习新的规划策略并优于其他机制解释经验驱动的发现，但模型在策略发现的速度上仍逊色于人类。因此，需要研究加入哪些认知机制可以使模型更接近人类的表现。

Method: 本研究分析了规划任务数据，并考虑了三种可能促进人类策略发现的认知机制：元认知伪奖励、主观努力评估和终止思考。通过观察参与者对这些机制的使用情况及影响，来探讨它们对策略发现的作用。

Result: 数据分析表明，大部分参与者至少使用了一种上述机制，且在使用上存在显著的个体差异。元认知伪奖励、主观努力评估以及无需进一步规划而行动的学习被证实有助于策略发现。然而，这些改进并未完全消除模型与人类表现之间的差距。

Conclusion: 加入特定的认知机制可以为理解个体差异及其对策略发现的影响提供有价值的见解，但为了使模型更接近人类表现，还需要探索更多可能的因素。

Abstract: People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [28] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/abs/2505.23536)
*Janik-Vasily Benzin,Gyunam Park,Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: This paper addresses the problem of losing real-world behavior insights during model abstraction by providing a formal basis for synchronized model and event abstraction. It proves that abstracting a process model and discovering a model from an abstracted event log yields equivalent results, introducing a new event abstraction technique based on behavioral profile abstraction.


<details>
  <summary>Details</summary>
Motivation: To maintain grounding in real-world behavior contained in event logs while reducing model complexity through abstraction.

Method: Providing a formal basis for synchronized model and event abstraction, proving equivalence between abstracted process models and models discovered from abstracted event logs using behavioral profile abstraction.

Result: Proves feasibility of synchronized abstraction approach with behavioral profile abstraction as non-order preserving model abstraction technique, leading to a novel event abstraction technique.

Conclusion: Synchronized model and event abstraction preserves real-world behavior insights while achieving optimality goals like reducing model size.

Abstract: Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [29] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559)
*Kunlun Zhu,Jiaxun Zhang,Ziheng Qi,Nuoxing Shang,Zijia Liu,Peixuan Han,Yue Su,Haofei Yu,Jiaxuan You*

Main category: cs.AI

TL;DR: SafeScientist is an AI framework enhancing safety and ethical responsibility in scientific exploration. It refuses high-risk tasks and integrates multiple defensive mechanisms. Experiments show a 35% improvement in safety performance without compromising output quality.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in LLM agents have accelerated scientific discovery automation but raised ethical and safety concerns, necessitating a systematic approach to address these challenges.

Method: The SafeScientist framework proactively refuses ethically inappropriate or high-risk tasks and emphasizes safety throughout the research process. It integrates prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Additionally, SciSafetyBench, a benchmark with 240 high-risk tasks, 30 tools, and 120 risk tasks, is proposed for evaluation.

Result: Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35% compared to traditional frameworks, maintaining scientific output quality. The safety pipeline also shows robustness against diverse adversarial attack methods.

Conclusion: SafeScientist provides a comprehensive solution to enhance safety and ethical responsibility in AI-driven scientific exploration, validated through significant improvements in safety performance and robustness.

Abstract: Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [30] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)
*Benjamin Arnav,Pablo Bernabeu-Pérez,Nathan Helm-Burger,Tim Kostolansky,Hannes Whittingham,Mary Phuong*

Main category: cs.AI

TL;DR: In this paper, researchers explore Chain-of-Thought (CoT) monitoring as a method to oversee AI models' reasoning processes and prevent unnoticed harmful actions. They find CoT monitoring enhances detection in complex scenarios but can be deceived in obvious cases. A hybrid protocol combining reasoning and final output reviews shows superior performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the growing autonomy of AI models and the consequent risk of them taking harmful actions without being noticed. To mitigate this risk, the researchers investigate the effectiveness of different monitoring methods.

Method: The method involves comparing CoT monitoring, where a weaker trusted model monitors the reasoning steps of a more powerful untrusted model, with action-only monitoring, which only reviews final outputs. This is done in a red-teaming setup where the untrusted model is instructed to pursue harmful tasks while solving coding problems.

Result: Results show that CoT monitoring improves detection by up to 27 percentage points in complex scenarios compared to action-only monitoring. However, CoT traces can deceive the monitor in obvious sabotage cases. A hybrid protocol, scoring both reasoning and final outputs, outperforms both CoT and action-only monitors across all tested models and tasks.

Conclusion: The conclusion drawn from this study is that while CoT monitoring has advantages, it also has limitations. The hybrid protocol introduced offers a more reliable solution for detecting harmful actions across various scenarios.

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [31] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/abs/2505.23596)
*Linqiang Guo,Wei Liu,Yi Wen Heng,Tse-Hsun,Chen,Yang Wang*

Main category: cs.AI

TL;DR: Mobile GUI agents, enhanced by Multimodal Large Language Models (MLLMs), can interpret UI screens and perform interactions but remain reactive. MAPLE is a state-aware multi-agent framework that models app interactions as a Finite State Machine (FSM) to improve context understanding, error detection, and recovery. It consists of specialized agents for planning, execution, verification, error recovery, and knowledge retention, which collaborate to construct FSMs in real time. Evaluated on two benchmarks, MAPLE improves task success rate by up to 12%, recovery success by 13.8%, and action accuracy by 6.5%. The study emphasizes the importance of structured state modeling for mobile GUI agents.


<details>
  <summary>Details</summary>
Motivation: Existing mobile GUI agents are limited by their reactivity; they only reason over the current screen without a structured model of app navigation flow. This limitation affects their ability to understand context, detect unexpected outcomes, and recover from errors.

Method: MAPLE abstracts app interactions as a Finite State Machine (FSM), computationally modeling each UI screen as a discrete state and user actions as transitions. It includes specialized agents responsible for planning, execution, verification, error recovery, and knowledge retention. These agents collaborate to dynamically construct FSMs in real time based on perception data extracted from the UI screen.

Result: MAPLE outperforms the state-of-the-art baseline on two challenging cross-app benchmarks, improving task success rate by up to 12%, recovery success by 13.8%, and action accuracy by 6.5%.

Conclusion: Structured state modeling is crucial for guiding mobile GUI agents during task execution. The FSM representation used in MAPLE can be integrated into future GUI agent architectures as a lightweight, model-agnostic memory layer to support structured planning, execution verification, and error recovery.

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [32] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 提出了一种名为Formula Tuning (Fortune)的强化学习框架，该框架训练大语言模型生成可执行的电子表格公式以回答关于通用表格数据的问题。通过减少对监督公式注释的依赖，使用二元答案正确性作为奖励信号，指导模型通过推理学习公式推导。实验表明，Formula Tuning显著提高了大语言模型在多步数值和符号推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具有强大的通用推理能力，但在处理表格数据时，尤其是在复杂场景下，仍然难以进行准确的数值或符号推理。电子表格公式提供了一个强有力的表达媒介，用于表示可执行的符号操作，编码丰富的推理模式，但尚未得到充分利用。

Method: 提出了Formula Tuning (Fortune)，一个强化学习框架，训练大语言模型生成可执行的电子表格公式来回答问题。使用二元答案正确性作为奖励信号，从而减少了对监督公式注释的依赖，并引导模型通过推理学习公式推导。

Result: 通过广泛的实验表明，Formula Tuning大大增强了大语言模型在多步数值和符号推理任务上的性能，使得一个7B参数的模型在表格理解方面优于O1。

Conclusion: Formula Tuning展示了通过强化学习提高大语言模型符号表格推理能力的潜力。

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [33] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/abs/2505.23686)
*Caroline Wang,Arrasy Rahman,Jiaxun Cui,Yoonchang Sung,Peter Stone*

Main category: cs.AI

TL;DR: The paper presents ROTATE, a regret-driven algorithm for Ad Hoc Teamwork (AHT) that alternates between improving the AHT agent and generating challenging teammates, significantly outperforming baselines in diverse environments.


<details>
  <summary>Details</summary>
Motivation: Existing AHT approaches have limitations in generating teammate pools with limited coverage of possible behaviors and treating the set of training teammates as static. This leads to challenges in generalizing to previously unseen partner agents.

Method: The paper introduces ROTATE, a regret-driven, open-ended training algorithm that alternates between improving the AHT agent and generating teammates that probe its deficiencies through an adversarial process.

Result: ROTATE significantly outperforms baselines at generalizing to an unseen set of evaluation teammates across diverse AHT environments.

Conclusion: ROTATE establishes a new standard for robust and generalizable teamwork in AHT by effectively addressing the limitations of existing approaches.

Abstract: Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [34] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/abs/2505.23695)
*Ran Zhang,Mohannad Elhamod*

Main category: cs.AI

TL;DR: The paper presents a novel agentic system using modular LLM agents to automate the data-to-dashboard pipeline, which simulates business analysts' reasoning process. Evaluated on three datasets, it outperforms GPT-4o with a single-prompt baseline in insightfulness, domain relevance, and analytical depth.


<details>
  <summary>Details</summary>
Motivation: To create an advanced agentic system for automating the data-to-dashboard pipeline that surpasses existing chart QA systems by simulating the analytical reasoning of business analysts.

Method: Using modular LLM agents capable of domain detection, concept extraction, multi-perspective analysis generation, and iterative self-reflection to automate the data-to-dashboard process without relying on closed ontologies or question templates.

Result: The system was evaluated on three datasets across different domains and showed improved insightfulness, domain relevance, and analytical depth compared to GPT-4o with a single-prompt baseline.

Conclusion: This work introduces a new modular pipeline for transforming raw data into visualizations, offering opportunities for human-in-the-loop validation by domain experts in business analytics.

Abstract: The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [35] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
*Ruida Wang,Yuxin Li,Yi R.,Fung,Tong Zhang*

Main category: cs.AI

TL;DR: Enhancing LLMs' mathematical reasoning via an end-to-end framework called NL-FL HybridReasoning that integrates formal language expertise into natural language problem-solving, achieving significant accuracy improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve the mathematical reasoning capabilities of LLMs by effectively integrating Formal Language (FL) knowledge into Natural Language (NL) reasoning, addressing the disparities in problem structure and reasoning format between the two.

Method: Introduced the NL-FL HybridReasoning framework which includes: 1) NL-FL Problem Alignment method to reformulate NL QA problems as FL existence theorems; 2) Mixed Problem Input technique enabling FL reasoner to handle multiple types of problems concurrently; and 3) LLM-based Answer Extraction mechanism to bridge output format gaps.

Result: The HybridReasoning framework achieved 89.80% and 84.34% accuracy rates on MATH-500 and AMC benchmarks respectively, surpassing the NL baseline by 4.60% and 4.82%. Some problems were uniquely solved by this framework compared to the NL baseline model.

Conclusion: The proposed NL-FL HybridReasoning framework successfully incorporates FL expertise into NL math problem-solving, demonstrating substantial improvements in accuracy and solving previously unresolved problems.

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [36] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/abs/2505.23746)
*Hugo Henry,Kelly Cohen*

Main category: cs.AI

TL;DR: This study explores the use of Genetic Fuzzy Systems (GFS) to model airfoil self-noise using different fuzzy regression strategies, finding that clustering-assisted fuzzy inference is effective for complex aeroacoustic phenomena.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling self-noise generated by airfoils which is crucial in aerospace, automotive and drone applications.

Method: The study used the Airfoil Self-Noise dataset to evaluate various fuzzy regression strategies including brute force Takagi Sugeno Kang (TSK) fuzzy system, cascading Genetic Fuzzy Tree (GFT) architecture, and a novel clustered approach based on Fuzzy C-means (FCM).

Result: Clustering assisted fuzzy inference was found to be an effective regression tool for complex aeroacoustic phenomena, reducing model complexity.

Conclusion: Genetic Fuzzy Systems, particularly when enhanced with clustering techniques, offer a promising method for modeling airfoil self-noise.

Abstract: This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [37] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Main category: cs.AI

TL;DR: ZeroGUI is a scalable, online learning framework that automates GUI Agent training without human cost by integrating VLM-based task generation, reward estimation, and two-stage online reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing pure-vision-based GUI Agents face limitations such as reliance on high-quality manual annotations and limited adaptability to dynamic environments.

Method: ZeroGUI integrates three key components: VLM-based automatic task generation, VLM-based automatic reward estimation, and two-stage online reinforcement learning.

Result: Experiments on UI-TARS and Aguvis demonstrate significant performance improvements in OSWorld and AndroidLab environments.

Conclusion: ZeroGUI provides a novel approach to automate GUI Agent training efficiently and effectively.

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/abs/2505.22686)
*Ange-Clement Akazan,Verlon Roel Mbingui,Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.LG

TL;DR: This paper benchmarks various deep learning models for weather forecasting in tropical Africa, finding that KAN and customized TKAN models perform well for temperature and precipitation, while classical RNNs are better for atmospheric pressure.


<details>
  <summary>Details</summary>
Motivation: Existing weather forecasting methods struggle with the complex, non-linear weather patterns in tropical Africa. This study aims to evaluate deep recurrent neural networks and Kolmogorov-Arnold-based models for their potential in improving weather prediction in this region.

Method: The study compares LSTM, GRU, BiLSTM, BiGRU, KAN, TKAN models (and two customized TKAN variants using GeLU and MiSH activation functions) on daily forecasting of temperature, precipitation, and pressure in Abidjan and Kigali using meteorological data from 2010 to 2024.

Result: KAN achieves high accuracy in temperature prediction; TKAN variants minimize absolute errors for precipitation forecasting in low-rainfall regimes; customized TKAN models improve over standard TKAN; classical RNNs outperform in atmospheric pressure forecasting.

Conclusion: Spline-based neural architectures like KAN and customized TKAN show promise for efficient weather forecasting in tropical Africa, though classical RNNs remain competitive for certain tasks.

Abstract: Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [39] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/abs/2505.22689)
*Jialong Guo,Xinghao Chen,Yehui Tang,Yunhe Wang*

Main category: cs.LG

TL;DR: Large language models (LLMs) are costly to deploy due to their computational demands. This paper introduces SlimLLM, a structured pruning method that effectively compresses LLMs while minimizing performance loss. SlimLLM evaluates sub-module importance holistically and uses linear regression for performance recovery. It also employs layer-based importance ratios for determining pruning ratios per layer. Evaluated on the LLaMA benchmark, SlimLLM outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To reduce the enormous computational costs associated with deploying and applying large language models (LLMs), making them more efficient without significant performance loss.

Method: SlimLLM conducts channel and attention head pruning by evaluating the importance of entire channels or heads rather than individual elements within sub-modules. A linear regression strategy is used for the output matrix to quickly recover performance. Additionally, layer-based importance ratios determine pruning ratios for each layer.

Result: SlimLLM achieves state-of-the-art performance on the LLaMA benchmark, outperforming other methods in terms of effectiveness.

Conclusion: SlimLLM is an effective and fast structured pruning method for large language models, offering significant compression with minimal performance loss.

Abstract: Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [40] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/abs/2505.22694)
*Dacao Zhang,Kun Zhang,Shimao Chu,Le Wu,Xin Li,Si Wei*

Main category: cs.LG

TL;DR: With the development of LLMs, PEFT methods have gained attention. This paper proposes MoRE for multi-task PEFT, improving LoRA's performance in multi-task scenarios without additional inference cost.


<details>
  <summary>Details</summary>
Motivation: Current PEFT methods either focus on single-task scenarios or separately train multiple LoRA modules for multi-task scenarios, limiting efficiency and effectiveness in multi-task settings.

Method: The paper introduces MoRE, a Mixture of Low-Rank Experts approach for multi-task PEFT. It aligns different ranks of LoRA modules with tasks (low-rank experts) and uses an adaptive rank selector to choose appropriate experts for each task.

Result: Extensive experiments show that MoRE significantly improves LLM performance in multi-task scenarios compared to traditional LoRA and its variants, with no additional inference cost.

Conclusion: MoRE enhances the adaptability and efficiency of LoRA in multi-task scenarios, demonstrating superior performance without added inference cost. The model and code are released to support the community.

Abstract: With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [41] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: This paper introduces LLM-ODDR, a new framework using Large Language Models for optimizing ride-hailing services' order dispatching and driver repositioning. It emphasizes multi-objective evaluation, fairness in driver income, and demand-aware repositioning. Experiments show it outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for optimizing ride-hailing operations often neglect aspects like driver income fairness, interpretability, and adaptability to real-world dynamics.

Method: The LLM-ODDR framework includes three components: Multi-objective-guided Order Value Refinement, Fairness-aware Order Dispatching, and Spatiotemporal Demand-Aware Driver Repositioning. A fine-tuned model named JointDR-GPT is also developed for these tasks.

Result: Experiments on Manhattan taxi data demonstrate that the framework performs better than traditional methods in terms of effectiveness, adaptability, and decision interpretability.

Conclusion: This work represents the first use of LLMs as decision-making agents in ride-hailing ODDR tasks, providing valuable insights for integrating advanced language models into intelligent transportation systems.

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [42] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/abs/2505.22696)
*Eleni Nisioti,Joachim Winther Pedersen,Erwan Plantec,Milton L. Montero,Sebastian Risi*

Main category: cs.LG

TL;DR: Neuroevolution (NE) methods show potential in transfer learning capabilities, often outperforming Reinforcement Learning (RL) baselines, and could be key to developing more adaptable agents.


<details>
  <summary>Details</summary>
Motivation: To explore the underexamined aspect of Neuroevolution's (NE) transfer learning capabilities, particularly its potential to continuously and efficiently transfer skills across tasks which is crucial for artificial systems akin to biological intelligence.

Method: Introduced two benchmarks - stepping gates (emulating logic circuits with emphasis on modular repetition and variation) and ecorobot (extension of Brax physics engine including walls, obstacles, and robotic morphology switching). Both have curriculums allowing evaluation of skill transfer across increasingly complex tasks.

Result: Empirical analysis revealed that NE methods differ in their transfer abilities and frequently surpass RL baselines in performance.

Conclusion: Findings underline NE's potential as a basis for creating more adaptable agents while pointing out future challenges in scaling NE to address complex real-world issues.

Abstract: The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [43] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/abs/2505.22697)
*Filippo Rinaldi,Giacomo Capitani,Lorenzo Bonicelli,Donato Crisostomi,Federico Bolelli,Elisa Ficarra,Emanuele Rodolà,Simone Calderara,Angelo Porrello*

Main category: cs.LG

TL;DR: This paper explores the possibility of transferring fine-tuning to a new model release without retraining, proposing a two-level method based on weight permutations and spectral theory for Transformer models.


<details>
  <summary>Details</summary>
Motivation: When an underlying pretrained model is updated or retrained, the fine-tuned model becomes obsolete and requires retraining. The motivation is to find a way to transfer fine-tuning to a new checkpoint without having to re-train, in a data-free manner.

Method: The approach draws principles from model re-basin and uses weight permutations to re-base the modifications made to the original base model (task vector). It proposes a two-level method rooted in spectral theory, permuting the attention heads and adjusting parameters within select pairs of heads, tailored specifically for Transformer models.

Result: Through extensive experiments on visual and textual tasks, the method achieves the seamless transfer of fine-tuned knowledge to new pre-trained backbones without relying on a single training step or datapoint.

Conclusion: The proposed method successfully transfers fine-tuning to new model releases without needing retraining or data, providing a recipe based on weight permutations and addressing challenges specific to Transformer models.

Abstract: Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [44] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/abs/2505.22703)
*Mohammad Yaghini,Tudor Cebere,Michael Menart,Aurélien Bellet,Nicolas Papernot*

Main category: cs.LG

TL;DR: This paper presents RaCO-DP, an algorithm that addresses constrained optimization problems under differential privacy in trustworthy ML, proving its convergence and demonstrating its superiority in fairness-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: Many problems in trustworthy machine learning can be formulated as minimization of the model error under constraints on prediction rates, including group fairness constraints. However, incorporating these constraints into differentially private optimization poses challenges due to inter-sample dependencies.

Method: The authors developed RaCO-DP, a differentially private variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm. It solves the Lagrangian formulation of rate constraint problems and reduces the privacy cost to privately estimating a histogram over the mini-batch at each step.

Result: Theoretical analysis proves the convergence of RaCO-DP by leveraging the linear structure of the dual parameter. Empirical results show that RaCO-DP outperforms existing private learning approaches in terms of fairness-utility trade-offs.

Conclusion: RaCO-DP is an effective method for constrained optimization under differential privacy, achieving better performance in fairness-utility trade-offs compared to previous approaches.

Abstract: Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [45] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha,William Brandon,Mayank Mishra,Yikang Shen,Rameswar Panda,Jonathan Ragan-Kelley,Yoon Kim*

Main category: cs.LG

TL;DR: FlashFormer是一种概念验证内核，旨在加速基于Transformer的大规模语言模型的单批量推理。与现有的最先进的推理内核相比，在不同的模型大小和量化设置下，观察到了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现代大规模语言模型的规模和计算特性促使了对训练和推理专用内核开发的兴趣增加。然而，现有的内核主要针对大批次训练和推理进行优化，而小批次推理（如边缘部署和延迟敏感应用）中内存带宽和内核启动开销等因素仍然很重要。

Method: 提出了FlashFormer这一内核，专门用于加速基于Transformer的大规模语言模型的单批量推理。

Result: 在不同的模型大小和量化设置下，与现有最先进的推理内核相比，FlashFormer实现了显著的速度提升。

Conclusion: FlashFormer为小批次推理提供了一种有效的解决方案，适用于边缘部署和延迟敏感的应用场景。

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [46] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam,Helen Lu,Swami Sankaranarayanan,John Guttag*

Main category: cs.LG

TL;DR: This paper demonstrates that test-time augmentation (TTA) can reduce the size of sets produced by conformal classifiers while maintaining their probabilistic guarantees.


<details>
  <summary>Details</summary>
Motivation: Conformal classifiers often produce large sets of predicted classes, which are not informative enough. There is a need to refine the output of these classifiers without losing the probabilistic guarantee that the true class is included.

Method: The authors incorporate test-time augmentation (TTA), a technique that introduces inductive biases during inference, into conformal classification. This approach does not require model retraining and can be combined with any conformal score. The method is evaluated across multiple datasets, models, conformal scoring methods, guarantee strengths, and distribution shifts.

Result: Test-time augmentation reduces prediction set sizes by 10%-14% on average compared to standard conformal classifiers, while remaining flexible, computationally efficient, and requiring no model retraining.

Conclusion: Test-time augmentation is a useful addition to the conformal pipeline, effectively reducing the size of prediction sets while preserving the probabilistic guarantees of conformal classifiers.

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [47] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/abs/2505.22768)
*Mert Onur Cakiroglu,Idil Bilge Altun,Hasan Kurban,Elham Buxton,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: DRAGON is a new encoder that uses Multivariate de Bruijn Graphs to improve time series forecasting by bridging symbolic representations and neural modeling.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting is difficult for foundation models due to temporal heterogeneity, high dimensionality, and lack of inherent symbolic structure.

Method: DRAGON discretizes continuous input sequences and maps them onto a fixed graph structure using Multivariate de Bruijn Graphs (MdBGs), enabling dynamic context recovery via graph-based attention. It is integrated as an auxiliary module within a dual-branch architecture to augment conventional CNN-based encoders with symbolic, structure-aware representations.

Result: The proposed method enhances the ability of foundation models in handling complex time series data.

Conclusion: DRAGON offers a promising approach to address challenges in time series forecasting by combining symbolic representations with neural network architectures.

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [48] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/abs/2505.22772)
*Claas Voelcker,Anastasiia Pedan,Arash Ahmadian,Romina Abachi,Igor Gilitschenski,Amir-massoud Farahmand*

Main category: cs.LG

TL;DR: 研究分析了价值感知模型学习损失（包括MuZero损失）的特性，发现其为未校准替代损失，并提出了修正方案。还探讨了损失校准、潜在模型架构和辅助损失之间的相互作用，指出即使确定性模型可以准确预测值，学习校准的随机模型仍具优势。


<details>
  <summary>Details</summary>
Motivation: 尽管MuZero损失在基于模型的强化学习中被广泛应用，但对其理论特性的研究较为有限，尤其是关于其优缺点的理解不足。

Method: 1. 分析价值感知模型学习损失家族，特别是MuZero损失。
2. 发现这些损失是未校准的替代损失，可能导致无法正确恢复模型和价值函数。
3. 提出修正方法解决此问题。
4. 探讨损失校准与潜在模型架构及辅助损失的关系。
5. 比较确定性和随机模型在预测准确性上的表现。

Result: 1. 确认MuZero损失等为未校准替代损失。
2. 提出修正方法以改善模型和价值函数的准确性。
3. 显示确定性模型可足够用于准确预测值，但校准的随机模型仍然具有优势。

Conclusion: 价值感知模型学习损失需要进一步改进以确保校准，同时校准的随机模型在某些情况下可能更优。

Abstract: The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [49] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/abs/2505.22778)
*Sarah Meiklejohn,Hayden Blauzvern,Mihai Maruseac,Spencer Schrock,Laurent Simon,Ilia Shumailov*

Main category: cs.LG

TL;DR: The paper discusses the risks in the ecosystem of open ML models and proposes using Sigstore to bring transparency.


<details>
  <summary>Details</summary>
Motivation: The current ecosystem for open ML models contains significant supply-chain risks, some of which have been exploited already in real attacks.

Method: Exploring how Sigstore can be used to bring transparency to open ML models by enabling model publishers to sign their models and prove properties about the datasets they use.

Result: Sigstore can potentially mitigate the supply-chain risks in open ML models by providing a way to verify the authenticity and integrity of models and their associated datasets.

Conclusion: There is a need for transparency in the open ML model ecosystem, and Sigstore offers a promising solution.

Abstract: Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [50] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
*Marco Fumero,Luca Moschella,Emanuele Rodolà,Francesco Locatello*

Main category: cs.LG

TL;DR: This paper reinterprets neural networks as dynamical systems on a latent manifold, revealing an implicit vector field through autoencoder operations. This perspective uncovers attractor points that emerge during training and offers new ways to analyze model properties, extract prior knowledge, and identify out-of-distribution samples.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of how neural networks process data by examining them as dynamical systems on the latent manifold rather than just dimensionality reducers.

Method: By repeatedly applying the encoding-decoding map of autoencoders, the authors derive a latent vector field without additional training. They then analyze the effects of standard training procedures which introduce inductive biases leading to attractor points in this vector field.

Result: The vector field representation allows for analyzing generalization and memorization, extracting prior knowledge from network parameters, and identifying out-of-distribution samples effectively. The method was validated on vision foundation models.

Conclusion: Neural networks can be successfully interpreted as dynamical systems acting on a latent manifold, offering new insights into their behavior and potential applications.

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [51] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/abs/2505.22798)
*Anton Björklund,Mykola Zaitsev,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: The paper enhances the PREMAP algorithm for better scalability in neural network certification, particularly against 'patch attacks', through tighter bounds, adaptive Monte Carlo sampling, and improved branching heuristics, demonstrating significant efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: There is a need for effective neural network certification, especially in safety- and security-critical applications. Current preimage approximation methods, including PREMAP, struggle with scalability when dealing with complex scenarios such as 'patch attacks'.

Method: The authors improve the PREMAP algorithm by incorporating tighter bounds, adaptive Monte Carlo sampling, and enhanced branching heuristics to address scalability issues.

Result: The improved PREMAP algorithm shows at least an order of magnitude efficiency improvement on reinforcement learning control benchmarks and successfully scales to previously infeasible convolutional neural networks.

Conclusion: Preimage approximation methodology holds promise for ensuring reliability and robustness in neural network certification.

Abstract: The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [52] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/abs/2505.22803)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: Reliable uncertainty estimation is crucial for deploying neural networks in real-world applications. The paper introduces CLUE, a novel approach that aligns predicted uncertainty with observed error during training, using a loss function that optimizes predictive performance and calibration. CLUE is differentiable, domain-agnostic, and compatible with standard training pipelines. Experiments show CLUE achieves superior calibration quality and competitive predictive performance without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing calibration techniques for neural networks are limited in scalability, differentiability, and generalization across domains.

Method: CLUE (Calibration via Learning Uncertainty-Error Alignment) explicitly aligns predicted uncertainty with observed error during training by adopting a novel loss function that jointly optimizes predictive performance and calibration, using summary statistics of uncertainty and loss as proxies.

Result: Through extensive experiments on vision, regression, and language modeling tasks, including out-of-distribution and domain-shift scenarios, CLUE demonstrates superior calibration quality and competitive predictive performance compared to state-of-the-art approaches without imposing significant computational overhead.

Conclusion: CLUE is a fully differentiable, domain-agnostic method that enhances calibration quality in neural networks while maintaining competitive predictive performance and not adding significant computational cost.

Abstract: Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [53] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/abs/2505.22813)
*Josiah Couch,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.LG

TL;DR: In order to optimize machine-learning classifiers, researchers have identified an additional factor called dataset quality. This paper explores whether this quality is an intrinsic property of the dataset or a joint property of both dataset and model architecture. Through experiments with thousands of datasets and various classifier architectures, they found that dataset quality correlates strongly with classifier performance across different architectures, indicating it as an independent intrinsic property of datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to investigate if dataset quality is an intrinsic property of datasets, separate from factors like model architecture, dataset size, and class balance.

Method: The authors created thousands of datasets controlled for size and class balance and trained classifiers using a wide range of architectures (random forests, support-vector machines, deep networks). They then analyzed the correlation between classifier performance and dataset quality.

Result: Classifier performance was found to correlate strongly by subset across architectures ($R^2=0.79$), suggesting that dataset quality is an intrinsic property of datasets independent of dataset size, class balance, and model architecture.

Conclusion: Dataset quality is established as an independent intrinsic property of datasets that significantly affects classifier performance, joining size, class balance, and model architecture as key factors in optimizing machine-learning-based classification.

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [54] [Preference Learning with Response Time](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: This paper explores integrating response time data into human preference learning frameworks to improve reward model elicitation. It proposes methodologies leveraging the Evidence Accumulation Drift Diffusion (EZ) model and develops Neyman-orthogonal loss functions that achieve optimal convergence rates. Theoretical analysis shows significant improvements in sample efficiency, and experiments validate these findings in image-based preference learning.


<details>
  <summary>Details</summary>
Motivation: Binary preference data is widely used but does not fully utilize the temporal information from user decision-making processes.

Method: Novel methodologies are proposed to incorporate response time information alongside binary choice data using the Evidence Accumulation Drift Diffusion (EZ) model. Neyman-orthogonal loss functions are developed for reward model learning.

Result: The proposed approach reduces error rates from exponential to polynomial scaling with reward magnitude, significantly improving sample efficiency. Guarantees are extended to non-parametric reward function spaces.

Conclusion: Integrating response time data into preference learning frameworks can greatly enhance the effectiveness of reward model elicitation.

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [55] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/abs/2505.22825)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: The paper introduces PGLearn, a suite of standardized datasets and evaluation tools for machine learning applications in optimal power flow problems, aiming to democratize and accelerate research in this field.


<details>
  <summary>Details</summary>
Motivation: Progress in applying machine learning techniques to optimal power flow problems is hindered by the lack of standardized datasets and evaluation metrics.

Method: PGLearn provides comprehensive datasets representing real-life operating conditions, capturing global and local variability, including time series data for large-scale systems. It supports multiple OPF formulations and offers a toolkit for training, evaluating, and benchmarking ML models.

Result: Standardized datasets are publicly available, reducing the burden of data generation and enabling fair comparison of methodologies. PGLearn promotes open standards for performance evaluation.

Conclusion: PGLearn aims to democratize and accelerate research and innovation in machine learning applications for optimal power flow problems.

Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [56] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634)
*John Halloran*

Main category: cs.LG

TL;DR: MCP is vulnerable to FBAs, where attackers post malicious content online to deceive MCP agents. We create a new MCP dataset of FBAs and benign samples to study DPO's effectiveness for refusal training of LLMs. DPO improves guardrails, but its efficacy depends on the model's original alignment scheme. GRPO-based LLMs perform poorly in refusal learning. We introduce RAG-Pref, which significantly enhances LLMs' ability to refuse FBAs, especially when combined with DPO alignment.


<details>
  <summary>Details</summary>
Motivation: Recent work has shown that MCP is susceptible to retrieval-based 'falsely benign' attacks (FBAs), allowing malicious system access and credential theft. However, the threat model of MCP-based attacks is broader than previously thought, as attackers need only post malicious content online to deceive MCP agents into carrying out their attacks.

Method: 1. Create a new MCP dataset of FBAs and truly benign samples.
2. Explore the effectiveness of direct preference optimization (DPO) for refusal training of large language models (LLMs).
3. Introduce Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel preference alignment strategy based on RAG.

Result: DPO improves model guardrails against FBAs, but its efficacy varies depending on the model's original post-training alignment scheme. GRPO-based LLMs learn to refuse extremely poorly. RAG-Pref significantly improves the ability of LLMs to refuse FBAs, particularly when combined with DPO alignment.

Conclusion: The threat model of MCP-based attacks is broader than previously thought. DPO and RAG-Pref can significantly improve the ability of LLMs to refuse FBAs, thus drastically improving guardrails against MCP-based attacks.

Abstract: The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [57] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/abs/2505.22829)
*Chenruo Liu,Kenan Tang,Yao Qin,Qi Lei*

Main category: cs.LG

TL;DR: This paper connects distribution shift and AI safety by analyzing their synergies, establishing two types of connections between them, and providing a unified perspective for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to bridge the gap between distribution shift and AI safety through a comprehensive analysis of their conceptual and methodological synergies.

Method: The authors establish two types of connections between specific causes of distribution shift and fine-grained AI safety issues: (1) methods addressing a specific shift type can help achieve corresponding safety goals, or (2) certain shifts and safety issues can be formally reduced to each other.

Result: The findings provide a unified perspective that encourages fundamental integration between distribution shift and AI safety research.

Conclusion: This paper successfully bridges distribution shift and AI safety, offering a new direction for future research in both fields.

Abstract: This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [58] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/abs/2505.22839)
*Liu Yuezhang,Xue-Xin Wei*

Main category: cs.LG

TL;DR: Diffusion models significantly enhance empirical adversarial robustness by increasing the ℓp distance to clean samples and compressing the input space.


<details>
  <summary>Details</summary>
Motivation: To systematically investigate how and how well diffusion models improve adversarial robustness.

Method: Observe the effect of diffusion models on the ℓp distance to clean samples, evaluate the influence of internal randomness and compression effect, assess robustness under fixed randomness, and correlate remaining robustness gain with the model's ability to compress the input space.

Result: Diffusion models increase the ℓp distance to clean samples, purified images are influenced by internal randomness with a compression effect, improvement in robustness drops to 24% on CIFAR-10 under fixed randomness, and remaining robustness gain correlates with the compression rate.

Conclusion: The study provides insights into the mechanisms of diffusion-based purification and suggests the compression rate as a reliable robustness indicator.

Abstract: Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [59] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/abs/2505.22840)
*Dharambir Mahto,Prashant Yadav,Mahesh Banavar,Jim Keany,Alan T Joseph,Srinivas Kilambi*

Main category: cs.LG

TL;DR: The study presents SXI++ LNM, a machine learning model for sepsis prediction that uses deep neural networks. It was tested in multiple scenarios and outperformed current methods with an AUC of 0.99, precision of 99.9%, and accuracy of 99.99%.


<details>
  <summary>Details</summary>
Motivation: Sepsis is a life-threatening condition with non-specific symptoms and complex pathophysiology, making it difficult to predict using traditional methods.

Method: The SXI++ LNM is a machine learning scoring system utilizing deep neural networks. The model was trained and tested on various dataset distributions, and its performance was evaluated against unseen test data.

Result: The SXI++ LNM achieved an AUC of 0.99 (95% CI: 0.98-1.00), precision of 99.9% (95% CI: 99.8-100.0), and accuracy of 99.99% (95% CI: 99.98-100.0).

Conclusion: The SXI++ LNM demonstrates superior predictive performance for sepsis compared to existing models, offering high reliability and robustness for clinical applications.

Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [60] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/abs/2505.22841)
*Franck Gabriel,François Ged,Maria Han Veiga,Emmanuel Schertzer*

Main category: cs.LG

TL;DR: 扩散模型虽然在高保真生成采样中表现优异，但可能会因记忆效应导致过拟合。本文通过引入核平滑经验评分来降低方差，并分析其偏差-方差权衡，同时推导出真实分布与修改后的逆向SDE生成分布之间的KL散度的渐近界。这种正则化方法有助于防止记忆效应并增强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 扩散模型尽管在高质量生成任务中表现出色，但容易出现对训练数据的记忆现象，这使得逆时间随机微分方程（SDE）样本主要集中在训练点上。为了解决这一问题，需要对模型进行改进以减少记忆效应和提高泛化能力。

Method: 将经验评分视为真实评分的噪声版本，并分析其协方差矩阵，发现其在大维度下具有重新加权的数据主成分分析（PCA）特性。引入核平滑经验评分以降低噪声方差，并分析其偏差-方差权衡。进一步推导出真实分布与由修改后的逆向SDE生成的分布之间的Kullback-Leibler散度的渐近界。此外，基于前向扩散的谱分解提出了更好的方差控制条件。

Result: 理论分析表明，评分的正则化效果等同于增加训练数据集的大小，从而有效防止了记忆效应。实验结果在合成数据集和MNIST数据集上验证了该方法的有效性。

Conclusion: 通过核平滑经验评分的方法可以有效缓解扩散模型中的记忆效应，并增强其泛化能力。正则化不仅减少了方差，还揭示了去噪扩散过程中的两种机制：初始高斯核在环境空间中各向同性地扩散质量，而应用在评分空间中的第二个核则沿着数据流形集中和扩展质量。

Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [61] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/abs/2505.22846)
*Nikita Khramov,Andrei Kozyrev,Gleb Solovev,Anton Podkopaev*

Main category: cs.LG

TL;DR: This paper explores combining Interactive Theorem Proving with Generative AI, emphasizing premise selection for Rocq proofs and proposing a new method using a self-attentive embedder model, which boosts generator performance by 28%. It also introduces a multi-stage agentic system for writing Rocq proofs and demonstrates its effectiveness through an ablation study.


<details>
  <summary>Details</summary>
Motivation: Interactive Theorem Proving has been shown to be effective when combined with Generative Artificial Intelligence. There is a need to improve the generation of Rocq proofs.

Method: The authors propose a novel approach for premise selection using a self-attentive embedder model and apply a multi-stage agentic system tailored for formal verification to write Rocq proofs.

Result: The proposed approach results in up to a 28% relative increase in the generator's performance. The multi-stage agentic system shows high effectiveness in writing Rocq proofs, as demonstrated through an ablation study.

Conclusion: Thorough premise selection is crucial for generating Rocq proofs. The novel approach using a self-attentive embedder model significantly improves performance, and the multi-stage agentic system is highly effective for formal verification.

Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [62] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/abs/2505.22861)
*Carlota Parés-Morlans,Michelle Yi,Claire Chen,Sarah A. Wu,Rika Antonova,Tobias Gerstenberg,Jeannette Bohg*

Main category: cs.LG

TL;DR: Causal-PIK uses Bayesian optimization and a Physics-Informed Kernel to efficiently search for the best next action in tasks with complex interactions.


<details>
  <summary>Details</summary>
Motivation: Tasks involving complex interactions between objects with unknown dynamics make planning before execution difficult, requiring agents to iteratively improve their actions after actively exploring causes and effects in the environment.

Method: Propose Causal-PIK, a method that leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel.

Result: Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. It also remains competitive on tasks that are very challenging, even for human problem-solvers.

Conclusion: Causal-PIK is an effective method for tasks with complex interactions, outperforming state-of-the-art results and remaining competitive with human problem-solvers.

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [63] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/abs/2505.22866)
*Nicolas Espinosa-Dice,Yiyi Zhang,Yiding Chen,Bradley Guo,Owen Oertell,Gokul Swamy,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: Diffusion and flow models are powerful generative approaches, but applying them to offline reinforcement learning is challenging. This paper introduces Scalable Offline Reinforcement Learning (SORL), which uses shortcut models for efficient training and inference. SORL captures complex data distributions and scales well with increased compute.


<details>
  <summary>Details</summary>
Motivation: Diffusion and flow models have shown potential in modeling diverse and multimodal behavior, but their application in offline RL is hindered by the iterative nature of their noise sampling processes, complicating policy optimization.

Method: The paper proposes SORL, an offline RL algorithm that utilizes shortcut models to scale training and inference. The policy can capture complex data distributions and is trained in a single-stage process. During testing, both sequential and parallel inference scaling are introduced using the learned Q-function as a verifier.

Result: SORL demonstrates strong performance across various offline RL tasks and shows positive scaling behavior with increased test-time compute.

Conclusion: SORL provides a scalable solution for offline RL by leveraging shortcut models, improving both training and inference efficiency.

Abstract: Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [64] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/abs/2505.22881)
*Hyungki Im,Wyame Benslimane,Paul Grigas*

Main category: cs.LG

TL;DR: The paper extends contextual stochastic linear optimization by incorporating inequality constraints dependent on uncertain parameters predicted by machine learning models. It introduces the SPO-RC loss and its surrogate SPO-RC+, proves Fisher consistency, and demonstrates their effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty in constraints within contextual stochastic linear optimization.

Method: Introduce SPO-RC loss and its convex surrogate SPO-RC+, use contextual uncertainty sets constructed via conformal prediction, train on truncated datasets with importance reweighting techniques.

Result: SPO-RC+ effectively handles constraint uncertainty and combining truncation with importance reweighting improves performance.

Conclusion: The proposed methods provide effective ways to handle uncertainty in constraints for contextual stochastic linear optimization.

Abstract: We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [65] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/abs/2505.22899)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: This paper re-analyzes the Follow the Regularized Leader (FTRL) framework for Online Convex Optimization (OCO), demonstrating its ability to achieve dynamic regret guarantees through optimistic composition and careful linearization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of FTRL in dynamic environments, where it tends to produce 'lazy' iterates, by showing that FTRL can indeed recover known dynamic regret bounds.

Method: The method involves using optimistic composition of future costs and careful linearization of past costs, which can lead to pruning some of them. This new analysis allows for interpolation between greedy and agile updates.

Result: The result is a principled way to interpolate between greedy and agile updates, offering refined control over regret terms, optimism without cyclic dependence, and minimal recursive regularization akin to AdaFTRL.

Conclusion: The conclusion is that it's not the lazy projection style of FTRL that hinders dynamic regret, but the decoupling of the algorithm's state from its iterates, and pruning these two when necessary synchronizes them.

Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [66] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/abs/2505.22904)
*Youngsoo Choi,Siu Wun Cheung,Youngkyu Kim,Ping-Hsuan Tsai,Alejandro N. Diaz,Ivan Zanardi,Seung Whan Chung,Dylan Matthew Copeland,Coleman Kendrick,William Anderson,Traian Iliescu,Matthias Heinkenschloss*

Main category: cs.LG

TL;DR: This paper proposes a formal definition of foundation models in computational science and introduces the Data-Driven Finite Element Method (DD-FEM) as a framework that combines classical FEM with data-driven learning to address challenges such as scalability, adaptability, and physics consistency.


<details>
  <summary>Details</summary>
Motivation: The term 'foundation model' is increasingly used in computational science without a universally accepted definition, which may cause confusion and dilute its scientific meaning.

Method: Propose a formal definition of foundation models based on generality, reusability, and scalability. Introduce DD-FEM, which integrates classical FEM with data-driven learning.

Result: DD-FEM addresses key challenges like scalability, adaptability, and physics consistency in developing foundation models for computational science.

Conclusion: By combining traditional numerical methods with modern AI paradigms, this work establishes a rigorous basis for evaluating and developing future foundation models in computational science.

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [67] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)
*Donghyeon Joo,Helya Hosseini,Ramyad Hadidi,Bahar Asgari*

Main category: cs.LG

TL;DR: 研究展示了无结构稀疏性对LLM的KV缓存压缩有显著改善，可实现高达70%的稀疏性而不影响精度或无需微调。通过每令牌幅度基修剪策略，超越了先前的结构化修剪方案。使用位图基稀疏格式和自定义注意力内核，能够压缩KV缓存至密集推理的45%，提升了上下文长度和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLM）在解码性能方面受到KV缓存大小的限制，由于长上下文长度带来的高内存开销，迫切需要一种有效的压缩方法来缓解这一瓶颈。

Method: 1. 系统地探索了不同的修剪策略，发现基于每令牌幅度的修剪对于Key和Value缓存都高度有效。
2. 使用位图基稀疏格式和自定义注意力内核，可以压缩并直接计算经过修剪的缓存。
3. 该方法支持任意稀疏模式，并加速了解码计算中的内存绑定操作。

Result: 1. 实现高达70%的稀疏性而不会影响准确性或需要微调。
2. KV缓存被压缩到密集推理的45%，上下文长度更长，吞吐量提高达2.23倍。
3. 方法在https://github.com/dhjoo98/mustafar上开源。

Conclusion: 无结构稀疏性结合位图基稀疏格式和自定义注意力内核为LLM的KV缓存提供了高效的压缩方案，显著提高了解码性能和吞吐量。

Abstract: We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [68] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/abs/2505.22922)
*Athanasios Glentis,Jiaxiang Li,Qiulin Shang,Andi Han,Ioannis Tsaknakis,Quan Wei,Mingyi Hong*

Main category: cs.LG

TL;DR: 为了应对大规模语言模型预训练中的计算挑战，本文探讨了参数或内存高效的方法是否可以在保持与完整模型训练相当的性能的同时提高预训练效率，并提出了两种实用技术：权重重构和动量重置，以增强高效预训练方法的性能。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在多领域任务中的卓越能力，其参数数量快速增长，带来了巨大的计算挑战，特别是在预训练阶段需要处理庞大的数据集。因此，探索参数或内存高效的方法来提升预训练效率具有重要意义。

Method: 1. 进行全面调查，总结当前最先进的高效预训练方法；2. 对多个具有代表性的内存高效预训练方法进行基准评估，观察优化器和超参数对性能的影响；3. 提出两种实用技术：权重重构和动量重置，用于改进低秩方法的性能。

Result: 通过适当选择优化器和超参数，全秩训练表现出最佳性能；高秩更新有助于提升低秩方法的性能；提出的权重重构和动量重置技术在降低困惑度方面优于GaLore和Fira等流行算法，同时使用更少的内存（约减少25%）。

Conclusion: 参数或内存高效的方法能够在实现与完整模型训练相当性能的同时，提高预训练效率。文中提出的权重重构和动量重置技术为改进高效预训练方法提供了新的思路。

Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [69] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/abs/2505.22935)
*Jipeng Li,Yanning Shen*

Main category: cs.LG

TL;DR: The paper challenges the assumption that explicit noise-level conditioning is essential for Graph Diffusion Models (GDMs) and explores whether denoisers can implicitly infer noise levels from corrupted graph structures.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to investigate if denoisers in GDMs can implicitly infer noise levels from corrupted graph structures, thus potentially eliminating the need for explicit noise conditioning.

Method: The method involves developing a theoretical framework centered on Bernoulli edge-flip corruptions and extending it to more complex scenarios with coupled structure-attribute noise.

Result: Empirical evaluations show that unconditional GDMs achieve comparable or better performance than conditioned models, with reductions in parameters (4-6%) and computation time (8-10%).

Conclusion: The conclusion is that graph data often encodes sufficient information for the denoising process, suggesting simpler and more efficient GDM architectures are possible.

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [70] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/abs/2505.22949)
*Michael Sun,Orion Foo,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.LG

TL;DR: The paper proposes a grammar-based method to create a compact sequential representation of DAGs, enabling various applications like generative models and Bayesian Optimization.


<details>
  <summary>Details</summary>
Motivation: Directed acyclic graphs (DAGs) are widely used but decoding them in a principled manner is challenging due to the many possible topological orders of their nodes.

Method: The authors propose a grammar-based approach to construct a principled, compact, and equivalent sequential representation of a DAG by viewing the graph as derivations over an unambiguous grammar where the DAG corresponds to a unique sequence of production rules.

Result: This method provides a lossless compression of the data and can be used for building generative models, learning latent spaces, and Bayesian Optimization over structured data.

Conclusion: The proposed grammar-based sequential representation of DAGs has multiple uses and could significantly impact practical applications involving DAGs.

Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [71] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/abs/2505.22967)
*Chengqi Zheng,Jianda Chen,Yueming Lyu,Wen Zheng Terence Ng,Haopeng Zhang,Yew-Soon Ong,Ivor Tsang,Haiyan Yin*

Main category: cs.LG

TL;DR: MermaidFlow is a framework that uses safety-constrained graph evolution to improve the success rates and efficiency of autonomous agentic reasoning.


<details>
  <summary>Details</summary>
Motivation: Existing workflow generation methods often produce fragile, unexecutable plans due to unconstrained LLM-driven construction.

Method: MermaidFlow represents workflows as a verifiable intermediate representation using Mermaid graph language and formulates domain-aware evolutionary operators (crossover, mutation, insertion, deletion) to preserve semantic correctness while promoting structural diversity.

Result: MermaidFlow achieves consistent improvements in success rates and faster convergence to executable plans on the agent reasoning benchmark without modifying task settings or evaluation protocols.

Conclusion: Safety-constrained graph evolution provides a scalable, modular foundation for robust and interpretable agentic reasoning systems.

Abstract: Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [72] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/abs/2505.22973)
*Bahareh Tolooshams,Aditi Chandrashekar,Rayhan Zirvi,Abbas Mammadov,Jiachen Yao,Chuwei Wang,Anima Anandkumar*

Main category: cs.LG

TL;DR: Equivariance Regularized (EquiReg) diffusion is a new framework for regularizing posterior sampling in diffusion-based inverse problem solvers, which outperforms state-of-the-art models in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based inverse solvers rely on isotropic Gaussian approximations due to the intractability of the likelihood term, leading to inconsistent and unstable reconstructions.

Method: The EquiReg method reweights diffusion trajectories and penalizes deviations from the data manifold by defining a distribution-dependent equivariance error. Functions with low error for on-manifold samples and higher error for off-manifold samples are leveraged to regularize the diffusion sampling process.

Result: EquiReg enhances reconstructions and outperforms state-of-the-art diffusion models in linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations.

Conclusion: Equivariance Regularized diffusion provides a general framework for improving the performance of diffusion-based inverse problem solvers.

Abstract: Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [73] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/abs/2505.22984)
*Guancheng Zhou,Haiping Xu,Hongkang Xu,Chenyu Li,Donghui Yan*

Main category: cs.LG

TL;DR: 为了改善K-means聚类算法的公平性，本文提出了一种两阶段优化公式——先聚类，然后调整一小部分选定数据点的聚类隶属关系。实验表明，该方法在对聚类质量影响最小的情况下显著提高了公平性。


<details>
  <summary>Details</summary>
Motivation: K-means聚类算法可能在进一步分析或解释时存在重大弱点，即某些簇可能在某些敏感变量（如性别或种族）上包含不成比例多（或少）的子群体样本，这将导致公平性问题并可能产生偏差和意外的社会后果。

Method: 本文提出了一种两阶段优化方法：首先进行聚类，然后调整一小部分选定数据点的聚类隶属关系以提高公平性。同时提出了两种计算效率高的算法来识别那些对公平性而言代价高昂的数据点，一个关注簇外最近的数据点，另一个则关注高度“混合”的数据点。

Result: 基准数据集上的实验表明，所提出的方法在对聚类质量影响最小的情况下显著提高了公平性。

Conclusion: 所提出的算法可以很容易地扩展到一大类聚类算法或公平性度量标准中，为解决聚类中的公平性问题提供了一种有效途径。

Abstract: The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [74] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/abs/2505.22985)
*Masaharu Kagiyama,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: The paper introduces PatchEchoClassifier, a model using Echo State Network for HAR tasks. It utilizes a tokenizer and knowledge distillation to achieve over 80% accuracy with significantly reduced computational cost, requiring only one-sixth of the FLOPS compared to DeepConvLSTM.


<details>
  <summary>Details</summary>
Motivation: To develop an energy-efficient classifier for time-series data, particularly for human activity recognition (HAR) using one-dimensional sensor signals.

Method: Introduce PatchEchoClassifier that uses a reservoir-based mechanism called Echo State Network (ESN). Incorporate a tokenizer to extract patch-level representations and apply a knowledge distillation framework to transfer knowledge from a high-capacity MLP-Mixer teacher model.

Result: Achieves over 80 percent accuracy in HAR tasks while reducing computational cost significantly, needing only about one-sixth of the FLOPS compared to DeepConvLSTM.

Conclusion: PatchEchoClassifier is a promising solution for real-time and energy-efficient human activity recognition in edge computing environments.

Abstract: This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [75] [Model-Preserving Adaptive Rounding](https://arxiv.org/abs/2505.22988)
*Albert Tseng,Zhaofeng Sun,Christopher De Sa*

Main category: cs.LG

TL;DR: The paper presents YAQA, a new post-training quantization algorithm for large language models (LLMs) that reduces KL divergence by about 30% and improves downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Current post-training quantization methods focus on minimizing immediate activation error without considering the impact of subsequent layers, which may not lead to an optimal compressed model.

Method: YAQA uses Kronecker-factored approximations of each linear layer's Hessian with respect to the full model KL divergence. It includes tractable Kronecker-factored sketches for large LLMs and a quantizer-independent rounding algorithm with theoretical guarantees.

Result: YAQA empirically reduces the KL divergence to the original model by approximately 30% and achieves state-of-the-art performance on downstream tasks across various models and quantizers.

Conclusion: YAQA offers a significant improvement in maintaining the output distribution closeness to the original model while achieving better performance on downstream tasks.

Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [76] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi,Behrooz Kamgar-Parsi*

Main category: cs.LG

TL;DR: This paper explores the problem of determining the number of meaningful clusters in unlabeled datasets using regularized k-means algorithms, deriving rigorous bounds for the critical hyperparameter λ under the assumption of ideal clusters.


<details>
  <summary>Details</summary>
Motivation: Finding the correct number of distinct clusters in unlabeled datasets is crucial for many applications, yet there are no principled guidelines for setting a value for the critical hyperparameter λ in regularized k-means algorithms.

Method: The authors derive rigorous bounds for λ assuming clusters are ideal (d-dimensional spheres with identical radii), which are close proxies for k-means clusters. They also analyze k-means algorithm with multiplicative regularizer and consider the consensus among solutions with additive and multiplicative regularizations.

Result: Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions, while the consensus among solutions with different regularizations can reduce ambiguity in certain cases. Selected experiments also demonstrate the performance of regularized k-means algorithms as clusters deviate from the ideal assumption.

Conclusion: The derived bounds for λ provide a principled guideline for setting this critical hyperparameter in regularized k-means algorithms, improving the reliability of clustering results.

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [77] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/abs/2505.22994)
*Ari S. Benjamin,Kyle Daruwalla,Christian Pehle,Anthony M. Zador*

Main category: cs.LG

TL;DR: 受到神经调节的启发，本文提出了一种通过学习任务上下文变量的平滑参数化函数来优化权重空间中的平滑流形的方法。这种方法可以更好地泛化到分布外样本，并且在简单拓扑结构下优于传统条件方法。


<details>
  <summary>Details</summary>
Motivation: 在人工神经网络中，通常通过将上下文作为输入注入来完成跨任务的知识重用。然而，大脑使用不同的策略：参数本身会根据各种神经调节剂（如血清素）进行调节。基于此，研究者希望从神经调节中获得灵感，探索一种新的跨任务学习方法。

Method: 本文提出学习权重作为任务上下文变量的平滑参数化函数，而不是优化一个权重向量。具体来说，优化的是权重空间中的一个具有预定义拓扑结构的平滑流形，并推导了流形优化的形式化处理方法。在推理过程中，条件选择确定流形上的一个点作为特定子任务的有效权重矩阵。

Result: 通过训练具有不同拓扑结构（如直线、椭圆等）的流形，研究发现这些参数化方法不仅在性能上优于通过输入连接进行条件化的相同网络，而且对分布外样本有更好的泛化能力。

Conclusion: 调制低维流形上的权重提供了一个有原则且有效的传统条件化替代方案，能够方便地引入任务间关系的归纳偏差，并促进状态间的泛化。

Abstract: One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [78] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/abs/2505.22998)
*Jihwan Oh,Murad Aghazada,Se-Young Yun,Taehyeon Kim*

Main category: cs.LG

TL;DR: Bargaining is challenging for LLMs due to limited strategic depth and adaptation. This paper introduces BargainArena, a benchmark dataset with six scenarios, human-aligned evaluation metrics, and a structured feedback mechanism to improve LLMs' bargaining strategies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in strategic depth and adaptation to complex human factors in bargaining, as well as the inadequacy of existing benchmarks in capturing real-world complexity.

Method: Introduce BargainArena, a novel benchmark dataset with six intricate scenarios; develop human-aligned, economically-grounded evaluation metrics incorporating agent utility and negotiation power; implement a structured feedback mechanism enabling iterative refinement of LLMs' bargaining strategies.

Result: Experimental results demonstrate that LLMs often exhibit negotiation strategies misaligned with human preferences, but the structured feedback mechanism significantly improves their performance, promoting deeper strategic and opponent-aware reasoning.

Conclusion: The comprehensive framework centered on utility-based feedback enhances LLM capabilities in realistic bargaining, providing a significant step forward in addressing the challenges posed by real-world bargaining interactions.

Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [79] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/abs/2505.23003)
*Linh Le Pham Van,Minh Hoang Nguyen,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: This paper introduces HYDRO, a Hybrid Cross-Domain Robust RL framework that uses an online simulator to complement limited offline datasets in robust RL. It employs uncertainty filtering and prioritized sampling to minimize performance gaps between the simulator and worst-case models.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges faced by offline robust reinforcement learning algorithms which require extensive offline data and suffer from dynamics mismatch when using imperfect simulators.

Method: HYDRO utilizes an online simulator to complement the limited amount of offline datasets in the context of robust RL. It measures and minimizes performance gaps between the simulator and the worst-case models in the uncertainty set through novel uncertainty filtering and prioritized sampling techniques.

Result: Extensive experiments show HYDRO's superior performance over existing methods across various tasks, highlighting its potential to improve sample efficiency in offline robust RL.

Conclusion: HYDRO is a promising approach to enhance the effectiveness and efficiency of robust RL by integrating online simulation with offline data, offering solutions for dynamics mismatch and limited dataset issues.

Abstract: Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [80] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/abs/2505.23004)
*Kyle R. Chickering,Bangzheng Li,Muhao Chen*

Main category: cs.LG

TL;DR: The paper introduces QLIP, an improved drop-in replacement for the CLIP vision encoder that enhances coarse-grained and fine-grained visual understanding in Multimodal Large Language Models (MLLMs) like LLaVA without re-training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of the CLIP vision encoder which include handling only fixed input resolutions and failing to produce separated embeddings for dissimilar images. This is crucial as replacing the vision encoder usually incurs significant computational costs due to the need for retraining the entire model pipeline.

Method: QLIP is proposed as a solution which uses an image quadtree to replace the standard uniform grid patches with a novel content aware patchification method. This allows it to be seamlessly integrated with existing MLLMs with minimal code changes.

Result: Experimental results show that QLIP improves the general visual question answering accuracy of the LLaVA v1.5 model series across various model sizes without requiring retraining or fine-tuning. It also boosts detailed understanding performance on the challenging $V^{\ast}$ benchmark by up to 13.6 percent.

Conclusion: QLIP provides an effective way to enhance both coarse-grained and fine-grained visual understanding in MLLMs without the need for re-training the entire model, thus reducing computational costs.

Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [81] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009)
*Ruskin Raj Manku,Yuzhi Tang,Xingjian Shi,Mu Li,Alex Smola*

Main category: cs.LG

TL;DR: 论文提出了一种名为EmergentTTS-Eval的全面TTS基准测试，包含六个具有挑战性的场景，并通过自动化生成测试案例和模型评估方法，揭示了当前TTS系统在处理复杂文本时的细微性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有的TTS基准测试无法充分衡量模型处理复杂语义和微妙情感的能力。

Method: 构建了一个涵盖六种复杂场景的TTS基准测试EmergentTTS-Eval，并采用自动化测试案例生成和大型音频语言模型（LALM）作为评估工具的方法。

Result: 该基准测试能够有效揭示不同TTS系统之间的性能差异，且模型评估方法与人类偏好高度相关。

Conclusion: EmergentTTS-Eval为TTS系统的性能评估提供了一种更细致、自动化的工具，并开源了代码和数据集。

Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [82] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/abs/2505.23013)
*Liangkai Hang,Junjie Yao,Zhiwei Bai,Tianyi Chen,Yang Chen,Rongjie Diao,Hezhou Li,Pengxiao Lin,Zhiwei Wang,Cheng Xu,Zhongwang Zhang,Zhangchen Zhou,Zhiyu Li,Zehao Lin,Kai Chen,Feiyu Xiong,Yaoyu Zhang,Weinan E,Hongkang Yang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 通过调整初始化速率和权重衰减系数来控制模型复杂度，可以改进大语言模型的扩展规律。对于不同模型大小和数据量，恒定的初始化速率比固定的初始化标准差更能加速扩展规律的下降。


<details>
  <summary>Details</summary>
Motivation: 近年来，大型语言模型（LLMs）的推理能力迅速提升，促使研究者寻找更基础的方法以可靠地增强其泛化能力。

Method: 通过调整初始化速率和权重衰减系数实现模型复杂度控制，并比较在不同复杂度超参数下预训练的2.4B模型在1T token上的基准性能。

Result: 恒定的初始化速率（标准差的指数）能使扩展规律在模型大小和数据量上更快地下降，而固定初始化标准差则不能达到相同效果。

Conclusion: 复杂度控制是持续改进LLMs的一个有希望的方向。

Abstract: The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [83] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2505.23014)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Xiaodong Li,Taoyu Su,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: This paper proposes formulating message passing in GNNs as a system of hyperbolic PDEs, enhancing the interpretability and performance of spectral GNNs.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle to ensure topological features due to learning node features in a spatial domain unrelated to the topology.

Method: Message passing is formulated as a system of hyperbolic PDEs, creating a dynamical system that maps node representations into a solution space spanned by eigenvectors describing graph topologies. Polynomials are introduced to approximate arbitrary filter functions.

Result: Extensive experiments show strong flexibility and significant performance enhancement for various spectral GNNs across diverse graph tasks.

Conclusion: Formulating message passing as hyperbolic PDEs enhances both interpretability and performance of spectral GNNs.

Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [84] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.23017)
*Xingjian Wu,Xiangfei Qiu,Hongfan Gao,Jilin Hu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: The paper presents $K^2$VAE, a VAE-based generative model using KoopmanNet and KalmanNet for Probabilistic Time Series Forecasting (PTSF). It transforms nonlinear time series into linear dynamics and refines predictions for both short- and long-term forecasts.


<details>
  <summary>Details</summary>
Motivation: Probabilistic Time Series Forecasting is essential in decision-making across various fields. However, most existing methods focus on short-term forecasting and overlook the challenges of Long-term Probabilistic Time Series Forecasting (LPTSF), where inherent nonlinear dynamics affect accuracy and increase costs.

Method: The method introduces $K^2$VAE, which uses KoopmanNet to convert nonlinear time series into a linear dynamical system and KalmanNet to refine predictions and model uncertainty within this linear system, thereby reducing error accumulation in long-term forecasting.

Result: Extensive experiments show that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term PTSF, offering a more efficient and accurate solution.

Conclusion: $K^2$VAE addresses the limitations of current methods in long-term probabilistic time series forecasting by transforming nonlinear dynamics into linear ones and refining predictions with KalmanNet.

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [85] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/abs/2505.23022)
*Yinghao Tang,Tingfeng Lan,Xiuqi Huang,Hui Lu,Wei Chen*

Main category: cs.LG

TL;DR: SCORPIO is an SLO-oriented LLM serving system that improves system goodput by up to 14.4X and SLO adherence by up to 46.5% compared to state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems focus on maximum throughput but neglect SLOs like TTFT and TPOT, resulting in suboptimal SLO attainment.

Method: SCORPIO exploits SLO heterogeneity for adaptive scheduling across admission control, queue management, and batch selection. It features a TTFT Guard and a TPOT Guard, both supported by a predictive module.

Result: Evaluations show SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5% compared to current best systems.

Conclusion: SCORPIO is an effective SLO-oriented LLM serving system designed to maximize system goodput and SLO attainment for workloads with heterogeneous SLOs.

Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [86] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/abs/2505.23024)
*Zhihao Wang,Wenke Huang,Tian Chen,Zekun Shi,Guancheng Wan,Yu Qiao,Bin Yang,Jian Wang,Bing Li,Mang Ye*

Main category: cs.LG

TL;DR: The paper explores the differences and optimization strategies of language prompt learning (LPT) and vision prompt learning (VPT) in federated learning scenarios using Vision Language Models (VLMs), focusing on data heterogeneity challenges.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored application of prompt learning with VLM in federated learning, especially under data heterogeneity challenges such as label skew and domain shift.

Method: Systematically investigate behavioral differences between LPT and VPT under data heterogeneity; conduct extensive experiments to evaluate impacts of different federated learning and prompt configurations on Federated Prompt Learning (FPL); explore enhancement strategies for prompt learning in complex scenarios.

Result: Findings reveal insights into optimizing prompt learning in federated settings, including leveraging both LPT and VPT when computational resources allow, contributing to effective deployment of VLMs in privacy-preserving environments.

Conclusion: Prompt learning with VLMs in federated learning shows potential for robustness improvements under data heterogeneity. Practical strategies are provided for optimizing LPT and VPT, facilitating broader adoption of VLMs in privacy-sensitive applications.

Abstract: The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [87] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To,Paul F RWilson,Viet Nguyen,Mohamed Harmanani,Michael Cooper,Fahimeh Fooladgar,Purang Abolmaesumi,Parvin Mousavi,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: The paper addresses the subpopulation shift problem in machine learning models by proposing Diverse Prototypical Ensembles (DPEs), an ensemble method using diverse classifiers to adaptively capture risks associated with subpopulations. It outperforms prior state-of-the-art methods in worst-group accuracy on nine real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Subpopulation shifts can degrade machine learning model performance significantly, and current solutions rely on assumptions about subpopulations and annotations that are often unavailable for real-world datasets.

Method: The authors propose using an ensemble of diverse classifiers to adaptively capture risk associated with subpopulations. Specifically, they replace the standard linear classification layer of a feature extractor network with a mixture of prototypical classifiers, each trained to classify data while focusing on different features and samples.

Result: Empirical evaluation on nine real-world datasets shows that the proposed method of Diverse Prototypical Ensembles (DPEs) often outperforms the prior state-of-the-art in worst-group accuracy.

Conclusion: DPEs provide a promising approach to handle subpopulation shifts without relying on assumptions about subpopulations or annotations.

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [88] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/abs/2505.23032)
*Dongwoo Lee,Dong Bok Lee,Steven Adriaensen,Juho Lee,Sung Ju Hwang,Frank Hutter,Seon Joo Kim,Hae Beom Lee*

Main category: cs.LG

TL;DR: The paper explores a Bayesian framework for neural scaling law extrapolation using Prior-data Fitted Networks (PFNs). It designs a prior distribution to sample synthetic functions resembling real-world neural scaling laws, enabling meta-learning of the extrapolation. The method outperforms existing approaches, especially in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing methods for predicting neural scaling behavior rely on point estimation and lack uncertainty quantification, which is essential for decision-making in investing computational resources.

Method: The authors use a Bayesian framework based on Prior-data Fitted Networks (PFNs) to predict neural scaling laws. They design a prior distribution that allows sampling of synthetic functions similar to real-world neural scaling laws, enabling meta-learning of the extrapolation process.

Result: The proposed method demonstrates superior performance compared to existing point estimation and Bayesian approaches, particularly excelling in data-limited scenarios such as Bayesian active learning.

Conclusion: The Bayesian framework with PFNs offers reliable, uncertainty-aware extrapolation of neural scaling laws, making it suitable for practical applications where computational resource allocation decisions are crucial.

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [89] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/abs/2505.23042)
*Siwen Wang,Shitou Zhang,Wan-Lin Chen,Dung Truong,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: Recent advancements in Large Language Models have inspired the development of foundation models across various domains. In this study, we evaluate the efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM on a real-world stress classification dataset.


<details>
  <summary>Details</summary>
Motivation: To assess the applicability of Large EEG Models (LEMs) to real-world environments rather than controlled clinical settings.

Method: Fine-tuning LaBraM, a state-of-the-art foundation EEG model, on a real-world stress classification dataset collected in a graduate classroom and training a binary classifier that distinguishes between normal and elevated stress states using resting-state EEG data recorded from 18 graduate students during a class session.

Result: The best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a 5-second window, significantly outperforming traditional stress classifiers in both accuracy and inference efficiency. The fine-tuned LEM shows robustness under random data shuffling and reduced channel counts.

Conclusion: These results demonstrate the capability of LEMs to effectively process real-world EEG data and highlight their potential to revolutionize brain-computer interface applications by shifting the focus from model-centric to data-centric design.

Abstract: Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [90] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/abs/2505.23048)
*Tianci Bu,Le Zhou,Wenchuan Yang,Jianhong Mou,Kang Yang,Suoyi Tan,Feng Yao,Jingyuan Wang,Xin Lu*

Main category: cs.LG

TL;DR: ProDiff is a new trajectory imputation framework which only uses two endpoints as minimal information and integrates prototype learning with a denoising diffusion probabilistic model. It outperforms current methods with higher accuracy and correlation.


<details>
  <summary>Details</summary>
Motivation: Trajectory data is crucial for many applications but often incomplete due to device limitations and diverse collection scenarios. Existing imputation methods rely on sparse trajectory or travel information, such as velocity, but these approaches assume that sparse trajectories retain essential behavioral patterns, placing significant demands on data acquisition and overlooking the potential of large-scale human trajectory embeddings.

Method: Propose ProDiff, a trajectory imputation framework that uses only two endpoints as minimal information. It integrates prototype learning to embed human movement patterns and a denoising diffusion probabilistic model for robust spatiotemporal reconstruction. Joint training with a tailored loss function ensures effective imputation.

Result: ProDiff improves accuracy by 6.28% on FourSquare and 2.52% on WuXi compared to state-of-the-art methods. Further analysis shows a 0.927 correlation between generated and real trajectories.

Conclusion: ProDiff outperforms existing methods in trajectory imputation, demonstrating its effectiveness through improved accuracy and high correlation with real trajectories.

Abstract: Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [91] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu,Bei Liu,Bo Xiao,Ke Zeng,Jiacheng Liu,Yanmin Qian*

Main category: cs.LG

TL;DR: DenoiseRotator is a novel method that redistributes parameter importance in large language models (LLMs) to make them more robust to pruning. It enhances existing pruning techniques and significantly reduces performance degradation.


<details>
  <summary>Details</summary>
Motivation: Pruning large language models often leads to significant performance degradation, particularly under semi-structured sparsity constraints. Current methods focus on estimating the importance of individual weights, which limits their ability to preserve critical model capabilities.

Method: The method involves minimizing the information entropy of normalized importance scores to concentrate importance onto fewer weights, making the model inherently more amenable to pruning. This is achieved through DenoiseRotator, which applies learnable orthogonal transformations to the model's weight matrices.

Result: Evaluated on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4 semi-structured sparsity, DenoiseRotator consistently improves perplexity and zero-shot accuracy. For example, on LLaMA3-70B pruned with SparseGPT at 2:4 semi-structured sparsity, it reduces the perplexity gap to the dense model by 58%.

Conclusion: DenoiseRotator is a model-agnostic approach that can be seamlessly integrated with existing pruning techniques, enhancing their effectiveness and reducing performance degradation.

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [92] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/abs/2505.23055)
*Zhen Xiang,Aliyah R. Hsu,Austin V. Zane,Aaron E. Kornblith,Margaret J. Lin-Martore,Jasmanpreet C. Kaur,Vasuda M. Dokiparthi,Bo Li,Bin Yu*

Main category: cs.LG

TL;DR: Clinical decision-making in emergency departments is complex and fast-paced. CDR-Agent, a novel LLM-based system, enhances ED decision-making by autonomously identifying and applying the most appropriate Clinical Decision Rules (CDRs) based on unstructured clinical notes. It significantly improves accuracy and reduces computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of high cognitive load on clinicians in making rapid and critical decisions, particularly in emergency departments, where the usage of Clinical Decision Rules (CDRs) can be hindered by the difficulty in quickly recalling and applying the appropriate rules.

Method: Introduced CDR-Agent, a LLM-based system that autonomously identifies and applies appropriate CDRs from unstructured clinical notes. Validated CDR-Agent using two novel datasets: synthetic and CDR-Bench, demonstrating its applicability beyond just ED clinics.

Result: CDR-Agent achieves a 56.3% (synthetic) and 8.7% (CDR-Bench) accuracy gain relative to standalone LLM baseline in CDR selection. It also significantly reduces computational overhead, selects relevant CDRs efficiently, and makes cautious yet effective imaging decisions.

Conclusion: CDR-Agent not only enhances the efficiency and accuracy of CDR selection but also outperforms traditional LLM prompting approaches in minimizing unnecessary interventions while successfully identifying positively diagnosed cases.

Abstract: Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [93] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061)
*Tarun Suresh,Debangshu Banerjee,Shubham Ugare,Sasa Misailovic,Gagandeep Singh*

Main category: cs.LG

TL;DR: Diffusion LLMs have shown potential for runtime efficiency but lack the ability to enforce formal constraints. This paper proposes DINGO, a dynamic programming-based constrained decoding strategy that is efficient and distribution-preserving, allowing for strictly satisfying user-specified regular expressions.


<details>
  <summary>Details</summary>
Motivation: Diffusion LLMs are promising for runtime efficiency but cannot reliably enforce user-specified formal constraints, making them unreliable for structured output tasks.

Method: The authors propose DINGO, a dynamic programming-based constrained decoding strategy designed for diffusion LLMs. It enables sampling of output strings with the highest probability under the model's predicted distribution while strictly satisfying any user-specified regular expression.

Result: DINGO achieves up to a 68 percentage point improvement over unconstrained inference on standard symbolic math and JSON generation benchmarks.

Conclusion: DINGO addresses the limitation of diffusion LLMs in enforcing formal constraints, significantly improving their reliability and performance for tasks requiring structured outputs.

Abstract: Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [94] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/abs/2505.23062)
*Lingkai Kong,Haichuan Wang,Tonghan Wang,Guojun Xiong,Milind Tambe*

Main category: cs.LG

TL;DR: CompFlow是一种基于流匹配和最优传输理论的新方法，通过建模目标动态为条件流并使用Wasserstein距离评估动态差距，从而提升强化学习中跨环境样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理源环境与目标环境动态差异时，通常采用KL散度或互信息来估计动态差距，但当源和目标动态具有不相交的支持集时，这些方法可能失效。因此需要一种更有效的方法来解决这个问题。

Method: 提出了一种名为CompFlow的方法，利用流匹配和最优传输理论，将目标动态建模为基于源域流输出分布的条件流，而不是直接从高斯先验中学习。这种方法可以更好地泛化目标动态，并通过源和目标转换之间的Wasserstein距离进行动态差距的合理估计。此外，还引入了一种乐观主动数据收集策略，优先探索高动态差距区域，以减少与最优策略的性能差异。

Result: 理论上证明了所提出的乐观主动数据收集策略能够降低与最优策略的性能差异；实证上，CompFlow在多个具有动态偏移的强化学习基准测试中超越了强大的基线方法。

Conclusion: CompFlow通过改进目标动态学习的泛化能力和提供动态差距的合理估计，在强化学习领域显著提高了样本效率，尤其是在处理源和目标环境动态差异方面表现优异。

Abstract: Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [95] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/abs/2505.23063)
*Denis Mamba Kabala,Adel Hafiane,Laurent Bobelin,Raphael Canals*

Main category: cs.LG

TL;DR: In this paper, a novel Decentralized Federated Learning (DFL) framework is introduced to address the limitations of centralized Federated Learning for crop disease detection and classification. The DFL framework uses validation loss to guide model sharing between peers and correct local training via an adaptive loss function controlled by weighting parameter. Extensive experiments using PlantVillage datasets with three deep learning architectures demonstrate that the DFL approach improves accuracy, convergence speed, generalization, and robustness.


<details>
  <summary>Details</summary>
Motivation: Crop disease detection and classification is crucial in agriculture, but large-scale deployment of deep learning models is limited by data privacy concerns. Although Federated Learning addresses this issue, centralized FL has vulnerabilities to single-point failures and scalability limits.

Method: A Decentralized Federated Learning (DFL) framework was developed, which uses validation loss (Loss_val) to guide model sharing between peers and to correct local training via an adaptive loss function controlled by weighting parameter. Experiments were conducted using PlantVillage datasets with three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the impact of weighting parameter, the number of shared models, the number of clients, and the use of Loss_val versus Loss_train of other clients.

Result: The DFL approach not only improves accuracy and convergence speed, but also ensures better generalization and robustness across heterogeneous data environments.

Conclusion: The proposed DFL framework is particularly well-suited for privacy-preserving agricultural applications, as it enhances accuracy, convergence speed, generalization, and robustness.

Abstract: Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [96] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/abs/2505.23071)
*Peizheng Guo,Jingyao Wang,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: In the abstract, the authors present BOGC-MML, a novel Bayesian-Oriented Gradient Calibration method for Multi-Modal Learning (MML). It captures gradient uncertainty by modeling gradients as random variables and derives their probability distribution. Then it converts precision into scalar evidence to quantify confidence per modality in every gradient dimension. Using Dempster-Shafer rule, uncertainties are quantified and fused leading to an aggregation that balances sensitivity and conservatism across dimensions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing MML methods which aggregate gradients with fixed weights and treat all dimensions equally, ignoring the intrinsic gradient uncertainty of each modality.

Method: BOGC-MML models each modality's gradient as a random variable and derives its probability distribution. It then proposes a method to convert the precision of each gradient distribution into a scalar evidence, which quantifies the confidence of each modality in every gradient dimension. These evidences are used to explicitly quantify per-dimension uncertainties and fuse them via a reduced Dempster-Shafer rule.

Result: Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and advantages of the proposed method.

Conclusion: BOGC-MML successfully calibrates the update direction by balancing sensitivity and conservatism across dimensions through uncertainty-weighted aggregation.

Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [97] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/abs/2505.23084)
*Chang Yu,Fang Liu,Jie Zhu,Shaobo Guo,Yifan Gao,Zhongheng Yang,Meiwei Liu,Qianwen Xing*

Main category: cs.LG

TL;DR: The paper proposes a hybrid framework combining LSTM networks with LightGBM and CatBoost for stock price prediction, which improves accuracy by 10 to 15 percent compared to individual models.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of stock price prediction by integrating different machine learning models and leveraging their respective strengths.

Method: The method involves processing time-series financial data using seven models (ANNs, CNNs, BiLSTM, vanilla LSTM, XGBoost, LightGBM, standard NNs) and then developing an ensemble model that combines LSTM with LightGBM and CatBoost.

Result: The experimental results indicate that the proposed framework enhances accuracy by 10 to 15 percent in comparison to individual models and diminishes error during market changes.

Conclusion: The study emphasizes the potential of ensemble methods in financial forecasting and presents a flexible design for incorporating new machine learning techniques.

Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [98] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/abs/2505.23086)
*Junyi An,Xinyu Lu,Chao Qu,Yunfei Shi,Peijia Lin,Qianwei Tang,Licheng Xu,Fenglei Cao,Yuan Qi*

Main category: cs.LG

TL;DR: SE(3)-equivariant Graph Neural Networks have advanced molecular system modeling, but their message passing processes are limited. This paper introduces Equivariant Spherical Transformer (EST), which uses a Transformer structure in the spatial domain of group representations after Fourier transform to improve expressiveness. EST surpasses tensor product-based methods and achieves state-of-the-art performance on benchmarks like OC20 and QM9.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of SE(3)-equivariant Graph Neural Networks' message passing processes that suffer from insufficient non-linearity and incomplete group representations, thereby restricting expressiveness.

Method: Introduction of Equivariant Spherical Transformer (EST), which leverages a Transformer structure within the spatial domain of group representations after Fourier transform. Theoretically and empirically demonstrated that EST can encompass the function space of tensor products while achieving superior expressiveness. An equivariant inductive bias is guaranteed through a uniform sampling strategy for the Fourier transform.

Result: EST shows state-of-the-art performance on various molecular benchmarks, including OC20 and QM9.

Conclusion: EST enhances the expressiveness of SE(3)-equivariant models by overcoming the limitations of tensor product-based convolutions, demonstrating superior performance in molecular system modeling.

Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [99] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si,Zhiyi Shi,Yadao Wang,Xiaokang Yang,Susanto Rahardja,Wei Shen*

Main category: cs.LG

TL;DR: The paper proposes MAP, a novel framework that reformulates weight matrices as high-dimensional vectors and decouples their adaptation into direction and magnitude. It can be integrated into existing PEFT methods and significantly improves performance.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods often define direction heuristically at the column level, lacking a principled geometric foundation.

Method: MAP normalizes the pre-trained weights, learns a directional update, and introduces two scalar coefficients to independently scale the magnitude of the base and update vectors.

Result: Extensive experiments show that MAP significantly improves performance when coupling with existing methods.

Conclusion: Given the universality and simplicity of MAP, it can serve as a default setting for designing future PEFT methods.

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [100] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/abs/2505.23098)
*Kuan Xu,Zhiguang Cao,Chenlong Zheng,Linong Liu*

Main category: cs.LG

TL;DR: An RL-AVNS method for VRPMTW is proposed, showing superior performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To effectively solve the Vehicle Routing Problem with Multiple Time Windows (VRPMTW) by overcoming limitations of traditional adaptive approaches.

Method: Integrates reinforcement learning with variable neighborhood search, using a fitness metric for shaking phase and a transformer-based neural policy network for operator selection.

Result: Significantly outperforms traditional VNS, AVNS, and state-of-the-art learning-based heuristics in solution quality and computational efficiency. Demonstrates strong generalization to unseen problem instances.

Conclusion: RL-AVNS offers a practical and effective solution for complex logistics scenarios involving VRPMTW.

Abstract: In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [101] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/abs/2505.23099)
*Chongjie Si,Xuankun Yang,Muqing Liu,Yadao Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 大型基础模型在众多下游任务中表现出色，但完全微调这些模型计算成本高昂。本文研究了微调过程中权重矩阵的结构变化，并提出了一种新的参数高效微调方法。


<details>
  <summary>Details</summary>
Motivation: 完全微调大型基础模型计算成本过高，现有的PEFT方法（如LoRA）虽然有效，但其修改模型参数的底层机制尚未被充分研究。

Method: 通过奇异值分解(SVD)分析微调过程中的权重矩阵结构变化，发现微调主要放大了顶级奇异值，并使主导奇异向量重新定向到任务特定方向。基于此洞察，提出了一种利用可学习的顶级奇异方向重新缩放的新方法。

Result: 该方法能够在不破坏整体结构的情况下精确调节最具影响力的组件，在多个任务上对强大基线实现了持续改进。

Conclusion: 结构化信息的微调方法对于提高参数高效微调的效果具有重要意义。

Abstract: Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [102] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/abs/2505.23105)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Darius Bunandar,Rachee Singh*

Main category: cs.LG

TL;DR: This paper proposes LUMION, a reconfigurable optical fabric that dynamically integrates spare accelerators into ongoing workloads to maintain performance without migrating entire ML jobs.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in resource usage when handling accelerator failures in modern ML datacenters.

Method: Introducing LUMION, which connects accelerators within a datacenter rack and dynamically integrates spare accelerators into ongoing workloads as failures occur.

Result: LUMION swaps a failed GPU with a healthy one and restarts the ML job within ~1 second of the failure, achieving nearly 2X improvement in fine-tuning throughput.

Conclusion: LUMION provides a more efficient solution for maintaining network performance in ML datacenters without requiring full racks of idle accelerators for fault tolerance.

Abstract: When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [103] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/abs/2505.23106)
*Ning Liu,Yue Yu*

Main category: cs.LG

TL;DR: This paper presents Neural Interpretable PDEs (NIPS), an enhanced neural operator architecture that builds on Nonlocal Attention Operators (NAO) for modeling physical systems. NIPS uses linear attention and a learnable kernel network to improve efficiency and accuracy, outperforming NAO and other baselines in empirical evaluations.


<details>
  <summary>Details</summary>
Motivation: Attention mechanisms have shown great success in AI domains like natural language processing and computer vision, but their potential for modeling complex physical systems is yet to be fully explored. Learning such systems often involves solving ill-posed inverse PDE problems with limited data.

Method: The authors introduce NIPS, which employs linear attention to scale learning and includes a learnable kernel network functioning as a channel-independent convolution in Fourier space. This approach reduces computational costs by avoiding explicit computation and storage of large pairwise interactions.

Result: Empirical results indicate that NIPS consistently outperforms NAO and other baselines across various benchmarks, showcasing improvements in scalability, interpretability, and efficiency.

Conclusion: NIPS represents a significant advancement in the field of physics learning, offering a more efficient and accurate method for modeling intricate physical systems.

Abstract: Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [104] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2505.23116)
*Pengfei Zhou,Yunlong Liu,Junli Liang,Qi Song,Xiangyang Li*

Main category: cs.LG

TL;DR: The paper introduces CrossLinear, a novel Linear-based forecasting model with a cross-correlation embedding module for capturing dependencies between variables efficiently. It mitigates overfitting and improves short-term and long-term forecasting precision through patch-wise processing and a global linear head.


<details>
  <summary>Details</summary>
Motivation: Traditional models have difficulty differentiating endogenous and exogenous variables in time series forecasting, leading to inefficiencies and overfitting.

Method: CrossLinear incorporates a lightweight cross-correlation embedding module that captures time-invariant and direct variable dependencies while disregarding time-varying or indirect ones. It also uses patch-wise processing and a global linear head to capture temporal dependencies effectively.

Result: Extensive experiments on 12 real-world datasets show superior performance in both short-term and long-term forecasting tasks. The ablation study highlights the effectiveness of the cross-correlation embedding module.

Conclusion: CrossLinear achieves consistent performance improvements and its cross-correlation embedding module is a valuable plug-in for various forecasting tasks across domains.

Abstract: Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [105] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/abs/2505.23117)
*Yuatyong Chaichana,Thanapat Trachu,Peerat Limkonchotiwat,Konpat Preechakul,Tirasan Khandhawit,Ekapol Chuangsuwanich*

Main category: cs.LG

TL;DR: In the era of large-scale training, model merging has become a crucial tool for creating multitasking models efficiently. The paper presents Decom-Renorm-Merge (DRM), which uses Singular Value Decomposition to align weight matrices into a joint space for effective entry-wise merging. DRM outperforms several state-of-the-art techniques across various settings.


<details>
  <summary>Details</summary>
Motivation: Model merging is important in large-scale training for efficient creation of multitasking models. However, existing methods have limitations due to their assumptions about weight matrices and do not account for the complexity of finetuned neural networks.

Method: The proposed method, Decom-Renorm-Merge (DRM), leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, allowing for entry-wise merging. Renormalization plays a crucial role in creating a robust joint space for merging.

Result: DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings on various models including ViT, DeBERTa, T5, and Llama3.1-8B.

Conclusion: DRM is a simple yet effective approach for model merging that addresses the limitations of existing methods by aligning weight matrices into a joint space and emphasizing the importance of renormalization.

Abstract: In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [106] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/abs/2505.23131)
*Xinyu Yao,Daniel Bourgeois,Abhinav Jain,Yuxin Tang,Jiawen Yao,Zhimin Ding,Arlei Silva,Chris Jermaine*

Main category: cs.LG

TL;DR: The paper introduces Doppler, a three-stage framework using dual-policy networks (SEL for selecting operations and PLC for placing them) to optimize dataflow graph operation assignment in machine learning workloads, reducing execution time and improving training efficiency compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based methods for assigning operations in dataflow graphs have limitations such as reliance on bulk-synchronous systems, lack of scheduling mechanism awareness, and exclusive dependence on reinforcement learning without leveraging expert-designed heuristics.

Method: Proposes Doppler, which includes two policies - SEL for selecting operations and PLC for placing them on devices. It is a three-stage framework for training these dual-policy networks to minimize execution time in a work-conserving system.

Result: Doppler outperforms baseline methods by reducing system execution time across tasks and demonstrates sampling efficiency by cutting per-episode training time.

Conclusion: Doppler provides an effective solution for optimizing the assignment of operations in dataflow graphs for complex machine learning workloads, enhancing both performance and training efficiency.

Abstract: We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [107] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/abs/2505.23135)
*Zhe Ye,Zhengxu Yan,Jingxuan He,Timothe Kasriel,Kaiyu Yang,Dawn Song*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）在软件开发中的应用日益增多，但确保LLM生成代码的正确性仍具挑战。本文提出了一种可验证代码生成方法，并介绍了Verina这一高质量基准测试平台，用于全面评估代码、规范和证明生成及其组合。通过对现有最先进的LLM进行广泛评估，发现其在可验证代码生成方面存在显著挑战，尤其是在证明生成方面。


<details>
  <summary>Details</summary>
Motivation: 确保LLM生成代码的正确性具有挑战性，且需要昂贵的人工审查。可验证代码生成提供了一种有希望的解决方案，但当前的基准测试往往缺乏对端到端可验证代码生成的支持。

Method: 介绍了一个名为Verina的高质量基准测试平台，包含189个手动策划的Lean编程任务，以及详细的问题描述、参考实现、正式规范和广泛的测试套件。通过使用Verina对最先进的LLM进行评估，分析其在代码、规范和证明生成方面的表现。

Result: 评估结果显示，最先进的LLM在可验证代码生成方面面临重大挑战，特别是在证明生成方面。例如，OpenAI o4-mini模型只能生成61.4%正确的代码，51.0%健全和完整的规范，以及3.6%成功的证明。

Conclusion: Verina为推动可验证代码生成的进步提供了严格的综合基准测试平台。数据集和评估代码已公开发布。

Abstract: Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [108] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/abs/2505.23150)
*Michal Nauman,Marek Cygan,Carmelo Sferrazza,Aviral Kumar,Pieter Abbeel*

Main category: cs.LG

TL;DR: Recent advances in language modeling and vision have had limited impact in value-based reinforcement learning (RL). This work shows that the use of high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings addresses the problem of task interference in online RL, allowing for robust and scalable multi-task training.


<details>
  <summary>Details</summary>
Motivation: Value-based reinforcement learning has not benefited much from the advances in large models and multi-task data. Improvements in this field are often driven by small models trained in a single-task context due to issues like sparse rewards and gradient conflicts making optimization brittle.

Method: The method involves using high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings. This approach is designed to address task interference in online reinforcement learning, enabling robust and scalable multi-task training.

Result: The proposed approach was tested on 7 multi-task benchmarks with over 280 unique tasks. It resulted in state-of-the-art single and multi-task performance as well as sample-efficient transfer to new tasks.

Conclusion: High-capacity value models trained with cross-entropy and task embeddings can effectively handle task interference in online reinforcement learning, leading to improved multi-task performance.

Abstract: Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [109] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/abs/2505.23165)
*Le Yang,Vincent Y. F. Tan,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究了固定置信度设定下可能带有偏见的离线数据的最佳臂识别（BAI）问题，提出了LUCB-H算法以适应性地平衡在线和离线数据，并证明了其在不同情况下的优越性和理论复杂度。


<details>
  <summary>Details</summary>
Motivation: 最佳臂识别问题在实际场景中常遇到带有偏差的离线数据，例如临床试验，需要一种方法有效利用这些数据并保持准确性。

Method: 提出了一种名为LUCB-H的算法，通过引入辅助偏差校正来适应性地设置置信区间，在LUCB框架内平衡在线和离线数据。

Result: 理论分析表明，LUCB-H在离线数据误导时与标准LUCB样本复杂度相当，而在离线数据有帮助时显著优于标准LUCB；数值实验进一步验证了其稳健性和适应性。

Conclusion: LUCB-H算法能有效结合离线数据，适应不同的偏差情况，具有较高的实用价值。

Abstract: We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [110] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

Main category: cs.LG

TL;DR: Deep learning models face challenges in real-world applications due to changing environmental conditions. Multi-source Domain Generalization (MDG) addresses this but is limited by the costs of creating multi-domain datasets. This paper proposes Pseudo Multi-source Domain Generalization (PMDG), which generates multiple pseudo-domains from a single source domain, allowing the application of MDG algorithms in Single-source Domain Generalization settings. Extensive experiments show a positive correlation between MDG and PMDG performance and the potential for pseudo-domains to match or exceed actual multi-domain performance.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail to maintain performance when deployed on data distributions different from their training data, especially in real-world applications where environmental conditions frequently change. MDG has shown promise in addressing this challenge but is limited by the significant costs and difficulties associated with creating multi-domain datasets.

Method: The proposed method, Pseudo Multi-source Domain Generalization (PMDG), generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques. This creates a synthetic multi-domain dataset that can be used with existing MDG algorithms.

Result: Extensive experiments with PseudoDomainBed reveal a positive correlation between MDG and PMDG performance. The results also show the potential of pseudo-domains to match or exceed actual multi-domain performance with sufficient data.

Conclusion: PMDG provides a practical solution for applying sophisticated MDG algorithms in Single-source Domain Generalization settings. The comprehensive empirical results offer valuable insights for future research in domain generalization.

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [111] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.23176)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Shijie Xu,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: 为了提高联邦学习的训练效率，本文提出了三种新技术：MUD、BKD和AAD，分别针对分解什么、如何分解和如何聚合三个关键问题。这些技术互补且可同时应用，实验表明其性能优于相关基准方法。


<details>
  <summary>Details</summary>
Motivation: 之前的研究通过低秩分解技术减少通信开销来提高联邦学习（FL）的训练效率。本文旨在增强这些低秩分解方法的性能。

Method: 本文提出三种新型技术：Model Update Decomposition (MUD)，Block-wise Kronecker Decomposition (BKD)，和 Aggregation-Aware Decomposition (AAD)。MUD 针对模型更新进行分解，BKD 采用块状克罗内克分解方式，AAD 则是考虑聚合过程的分解方式。这三种技术可以同时使用以达到最佳效果。此外，还提供了关于MUD的严格理论分析以确保其收敛性。

Result: 广泛的实验结果表明，该方法相较于相关的基线方法具有更快的收敛速度和更高的精度。

Conclusion: 本文提出的三种技术（MUD、BKD、AAD）在联邦学习中表现出了优越的性能，能够有效提升训练效率，并保证了理论上的收敛性。

Abstract: To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [112] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/abs/2505.23181)
*Tian Tian,Chunyan Miao,Hangwei Qian*

Main category: cs.LG

TL;DR: Contrastive learning is important for unsupervised representation learning, but optimal augmentation strategies for time series classification are less explored. This paper proposes Frequency Refined Augmentation (FreRA), a lightweight and effective frequency-domain approach tailored for time series contrastive learning on classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing predefined time-domain augmentation methods for contrastive learning are primarily adopted from vision and not specific to time series data, which may distort the semantically relevant information of time series by introducing mismatched patterns into the data.

Method: The paper proposes FreRA, which automatically separates critical and unimportant frequency components. It includes semantic-aware Identity Modification and semantic-agnostic Self-adaptive Modification. FreRA can be seamlessly integrated with contrastive learning frameworks in a plug-and-play manner.

Result: Extensive experiments on benchmark datasets (UCR and UEA archives) and five large-scale datasets show that FreRA consistently outperforms ten leading baselines on time series classification, anomaly detection, and transfer learning tasks.

Conclusion: FreRA demonstrates superior capabilities in contrastive representation learning and generalization in transfer learning scenarios across diverse datasets.

Abstract: Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [113] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/abs/2505.23182)
*Srijith Nair,Michael Lin,Amirreza Talebi,Peizhong Ju,Elizabeth Bentley,Jia Liu*

Main category: cs.LG

TL;DR: FSL-SAGE is a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models, achieving a convergence rate of $\mathcal{O}(1/\sqrt{T})$ and reducing communication costs and client memory requirements.


<details>
  <summary>Details</summary>
Motivation: Current collaborative training methods like Federated Learning (FL) and Split Learning (SL) have limitations. FL requires clients to train entire models which is infeasible for large-scale models while SL increases network latency due to its sequential nature. Other methods using local loss functions for parallel client-side training lack server feedback and may suffer poor accuracy.

Method: The proposed method FSL-SAGE uses auxiliary models to estimate server-side gradient feedback. These auxiliary models periodically adapt to emulate server behavior on local datasets.

Result: FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, matching FedAvg while significantly reducing communication costs and client memory requirements. Empirical results show it outperforms existing state-of-the-art FSL methods in terms of both communication efficiency and accuracy.

Conclusion: FSL-SAGE is an effective federated split learning algorithm that reduces communication costs and client memory requirements while maintaining accuracy.

Abstract: Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [114] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/abs/2505.23184)
*Hongcan Guo,Guoshun Nan,Yuan Yang,Diyang Zhang,Haotian Li,Zhican Chen,Qinchuan Zhou,Yuhan Ran,Xinye Cao,Sicong Leng,Xiaofeng Tao,Xudong Jiang*

Main category: cs.LG

TL;DR: The paper proposes RadarGate, a novel gating method that uses rotational operations of LoRA representations to improve expressiveness and feature interactions in scalable LLMs. Experiments on 6 benchmarks across 21 tasks demonstrate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Traditional gating mechanisms for routing inputs to experts in MoE models hinder scalability of LLMs, leading to poor generalization and underfitting issues due to restricted expressiveness of weighted-sum mechanisms.

Method: RadarGate is introduced, which fuses each LoRA representation with others using a learnable component and feeds the output to a rotation matrix. This involves learnable parameters defining angular relationships between LoRA representations, providing an extra degree of freedom for cross-LoRA synergies.

Result: Extensive experiments on 6 public benchmarks across 21 tasks show the effectiveness of RadarGate for scaling LoRAs. Rotations are contrastive, encouraging closer alignment of semantically similar representations while pushing dissimilar ones apart.

Conclusion: RadarGate effectively addresses scalability, generalization, and underfitting issues in LLMs by boosting expressiveness through rotational operations of LoRA representations.

Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [115] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/abs/2505.23185)
*Shahaf E. Finder,Ron Shapira Weber,Moshe Eliasof,Oren Freifeld,Eran Treister*

Main category: cs.LG

TL;DR: Message-Passing Neural Networks (MPNNs) face challenges like over-squashing which limits their Effective Receptive Field (ERF). This paper proposes Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN) to enhance long-range dependency capture in MPNNs, inspired by ERF augmentation techniques in CNNs.


<details>
  <summary>Details</summary>
Motivation: MPNNs are powerful for graph-structured data but suffer from over-squashing, limiting their ability to capture long-range dependencies effectively. The issue is akin to the limited ERF problem in CNNs.

Method: The IM-MPNN architecture applies hierarchical coarsening of graphs, allowing message-passing across multiple scales and promoting long-range interactions without increasing depth or parameters excessively.

Result: Extensive evaluations on benchmarks like LRGB show significant improvements in capturing long-range dependencies while preserving computational efficiency compared to baseline MPNNs.

Conclusion: IM-MPNN addresses the limited ERF problem in MPNNs by introducing multiscale message-passing, providing a more effective way to capture long-range dependencies in graph-structured data.

Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [116] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/abs/2505.23190)
*Yekun Zhu,Min Tang,Zheng Ma*

Main category: cs.LG

TL;DR: A new neural network method called DeepRTE is proposed to solve the Radiative Transfer Equation with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The Radiative Transfer Equation is crucial in many fields, but challenging to solve due to its complexity as a differential-integral equation.

Method: Propose DeepRTE, a novel approach using pre-trained attention-based neural networks to solve the RTE.

Result: Demonstrated high accuracy and computational efficiency through comprehensive numerical experiments.

Conclusion: DeepRTE provides an effective solution for solving the steady-state Radiative Transfer Equation.

Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [117] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/abs/2505.23194)
*Shiwei Li,Xiandi Luo,Xing Tang,Haozhao Wang,Hao Chen,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: The paper explores the effect of non-zero initialization on LoRA's fine-tuning dynamics, showing it improves robustness to suboptimal learning rates without affecting performance. Extensive experiments confirm the findings.


<details>
  <summary>Details</summary>
Motivation: To investigate whether non-zero initialization in LoRA layers affects fine-tuning dynamics and if it can improve robustness to suboptimal learning rates.

Method: Analyze the impact of non-zero initialization on LoRA's fine-tuning dynamics from an infinite-width perspective, comparing it with zero initialization.

Result: Non-zero initialization of matrices A and B improves LoRA's robustness to suboptimal learning rates, particularly smaller ones, and does not negatively affect fine-tuning performance due to introduced random noise.

Conclusion: Fine-tuning does not need to strictly start from the pretrained model as non-zero initialization does not harm performance and enhances robustness.

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [118] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/abs/2505.23195)
*Lifan Zhao,Yanyan Shen,Zhaoyang Liu,Xue Wang,Jiaji Deng*

Main category: cs.LG

TL;DR: Scaling laws inspire Time Series Foundation Models (TSFMs) with great zero-shot forecasting abilities. However, fine-tuned TSFMs often don't outperform specialized smaller models. This paper proposes a structured pruning method to regularize fine-tuning by focusing on relevant and compact parameter space, improving forecasting performance.


<details>
  <summary>Details</summary>
Motivation: Even after fine-tuning, large pre-trained TSFMs cannot consistently surpass specialized smaller models trained on full data for downstream tasks.

Method: Propose a structured pruning method to regularize the fine-tuning process of TSFMs, concentrating it on a more relevant and compact parameter space to preserve valuable prior knowledge.

Result: Experiments on seven TSFMs and six benchmarks show that fine-tuning pruned TSFMs significantly improves forecasting performance compared to fine-tuning original models.

Conclusion: The 'prune-then-finetune' paradigm helps TSFMs achieve state-of-the-art performance and outperform strong specialized baselines.

Abstract: Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [119] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/abs/2505.23223)
*Xingyuan Pan,Chenlu Ye,Joseph Melkonian,Jiaqi W. Ma,Tong Zhang*

Main category: cs.LG

TL;DR: An new method named Daunce is introduced which estimates uncertainty to attribute training data influence. It fine-tunes perturbed models and computes covariance of per-example losses for attribution scores, scalable to large language models with more accurate results than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing TDA methods either rely on gradients which limits scalability or use random projection-based methods that degrade accuracy. The authors aim to develop a scalable yet accurate method.

Method: Daunce operates by fine-tuning a collection of perturbed models and computing the covariance of per-example losses across these models as the attribution score.

Result: Daunce achieves more accurate attribution compared to existing TDA methods and is validated on tasks from vision to LLM fine-tuning. It also works with black-box model access and successfully attributes data on proprietary LLMs like OpenAI's GPT models.

Conclusion: Daunce is a simple yet effective data attribution approach through uncertainty estimation, achieving state-of-the-art performance in attributing training data influence.

Abstract: Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [120] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/abs/2505.23225)
*Fabiano Veglianti,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 本研究探讨了监督学习中模型泛化与反事实可解释性之间的关系，引入了ε-有效反事实概率（ε-VCP）的概念，并通过理论和实证分析揭示了泛化能力差与反事实生成容易之间的内在权衡。


<details>
  <summary>Details</summary>
Motivation: 在监督学习领域，模型的泛化能力和反事实可解释性之间的关系尚未被充分探索。因此，需要一种新的方法来衡量模型的过拟合程度，并理解其对反事实解释的影响。

Method: 提出了ε-有效反事实概率（ε-VCP）的概念，即在数据点的ε邻域内找到导致标签变化的扰动的概率。通过分析模型决策边界的几何特性，研究ε-VCP与模型过拟合之间的关系。

Result: 理论分析表明，ε-VCP随着模型过拟合程度的增加而增加。实证结果验证了这一理论，证明ε-VCP可以作为量化过拟合的有效指标。

Conclusion: 本研究建立了模型泛化能力与反事实可解释性之间的严格联系，揭示了两者之间的权衡关系，并提出ε-VCP作为一种实用的过拟合度量方法。

Abstract: In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [121] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/abs/2505.23228)
*Wanfu Gao,Jun Gao,Qingqi Han,Hanlin Pan,Kunpeng Liu*

Main category: cs.LG

TL;DR: The paper proposes a method to capture complex nonlinear associations in multi-label datasets using a random walk graph and aligns feature and label spaces through low-dimensional representation while preserving the manifold structure.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in feature dimension leads to complex relationships between features and labels, which existing methods fail to capture due to reliance on linear decomposition.

Method: Design a random walk graph that integrates feature-feature, label-label, and feature-label relationships. Optimize latent representations after low-rank decomposition and align variable spaces by leveraging low-dimensional representation coefficients.

Result: Extensive experiments on seven benchmark datasets and three representative datasets demonstrate the superiority of the proposed method.

Conclusion: The proposed method effectively captures complex nonlinear associations and aligns feature and label spaces, outperforming existing methods.

Abstract: The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [122] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/abs/2505.23244)
*Emo Todorov*

Main category: cs.LG

TL;DR: 在特定MDP中，展示了确定性和随机性策略梯度之间的关系，并提出了一种构造等效确定性策略MDP的一般方法。结果表明，通过近似状态值函数而不是状态-控制值函数，可以统一策略梯度方法。


<details>
  <summary>Details</summary>
Motivation: 研究连续控制中的确定性和随机性策略梯度之间的关系，以便更好地理解两者间的联系并为策略梯度方法提供新的见解。

Method: 1. 在涉及高斯控制噪声和二次控制成本的广泛使用的MDP家族中，比较确定性和随机性策略梯度、自然梯度和值函数。
2. 开发一种通用程序，用于构建与给定具有随机策略的MDP等效的具有确定性策略的MDP。
3. 新的MDP的控制是原始MDP中随机策略的充分统计量。

Result: 1. 在特定MDP中，确定性和随机性策略梯度、自然梯度和状态值函数相同，而状态-控制值函数不同。
2. 提出了构造等效确定性策略MDP的一般方法。
3. 结果表明，通过近似状态值函数而不是状态-控制值函数，可以统一策略梯度方法。

Conclusion: 确定性和随机性策略梯度在特定条件下具有相同特性，且可以通过构造等效确定性策略MDP来统一策略梯度方法。这为未来的研究提供了新的方向。

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [123] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/abs/2505.23246)
*Honoka Anada,Tatsuya Kaneko,Shinya Takamaeda-Yamazaki*

Main category: cs.LG

TL;DR: 在去中心化联邦学习（DFL）中，提出了一种新的参与者贡献度量方法DFL-Shapley和其可计算近似值DFL-MR。实验表明，DFL-Shapley是有效的基线指标，而DFL-MR与之接近且适用于实际系统。


<details>
  <summary>Details</summary>
Motivation: 当前的贡献度量方法主要针对集中式联邦学习（CFL），而在去中心化联邦学习（DFL）中，由于没有中央服务器以及存在多个全局模型，应用现有方法具有挑战性。因此，需要为DFL设计合适的贡献度量方法。

Method: 1. 提出了DFL-Shapley，一种扩展自Shapley值的方法，专门用于DFL。
2. 针对DFL-Shapley的实际计算困难，提出了可计算近似方法DFL-MR，通过累积每轮的Shapley值来估计总体贡献。

Result: 1. 实验验证了DFL-Shapley作为一种有效基线指标的准确性。
2. DFL-MR在多种设置下表现出与DFL-Shapley的接近性，证明了其作为DFL中贡献度量的有效性。

Conclusion: 研究提出的DFL-Shapley和DFL-MR为去中心化联邦学习中的贡献度量提供了有效解决方案，其中DFL-MR特别适合实际系统应用。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [124] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: 提出了一种实用的奖励调整模型，通过增加奖励方差来加速基于GRPO的RLHF训练，并设计了一个新颖的算法解决非凸优化问题，最终集成到GRPO中形成更高效的GRPOVI算法。


<details>
  <summary>Details</summary>
Motivation: 当前基于GRPO的强化学习人类反馈（RLHF）训练效率仍面临挑战，研究表明提高初始策略模型的奖励方差可以加快RLHF训练。

Method: 提出一种奖励调整方法，该方法能够增加奖励方差同时保持相对偏好和奖励期望；将此方法与GRPO结合，形成GRPOVI算法；设计了一个O(n log n)复杂度的算法求解非凸优化问题。

Result: 实验结果表明，GRPOVI算法相比原始GRPO算法显著提高了RLHF训练效率。

Conclusion: 所提出的奖励调整模型及其算法能有效加速RLHF训练，并为GRPO在RLHF中的有效性提供了间接解释。

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [125] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/abs/2505.23264)
*Fangyikang Wang,Hubery Yin,Shaobin Zhuang,Huminhao Zhu,Yinan Li,Lei Qian,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.LG

TL;DR: This paper proposes efficient algorithms for approximating the diffusion Fisher information in diffusion models, providing accuracy guarantees and reducing computational cost.


<details>
  <summary>Details</summary>
Motivation: Current methods for approximating diffusion Fisher information (DF) through auto-differentiation of learned score networks lack accuracy guarantees and are time-consuming.

Method: The authors exploit the outer-product structure of the diffusion Fisher to develop two efficient approximation algorithms for computing its trace and matrix-vector multiplication. They also establish error bounds for these algorithms.

Result: Experiments show that the proposed algorithms offer superior accuracy and reduced computational cost in likelihood evaluation and adjoint optimization tasks. Additionally, a numerical verification experiment confirms the optimal transport property of the general PF-ODE deduced map.

Conclusion: The proposed algorithms provide an efficient and accurate way to approximate the diffusion Fisher information, advancing both practical applications and theoretical understanding.

Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [126] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen,Yueqi Zhang,Yuan Bi,Yao Zhang,Tong Liu,Jinhe Bi,Jian Lan,Jindong Gu,Claudia Grosser,Denis Krompass,Nassir Navab,Volker Tresp*

Main category: cs.LG

TL;DR: 本研究提出了一种全面的审计框架，用于评估机器遗忘算法在大型语言模型中的效果，并引入了一种基于中间激活扰动的新技术以克服传统审计方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的能力主要来源于大规模架构和海量数据集的训练，但这些数据集中可能存在敏感或受版权保护的内容，引发数据隐私和所有权问题。这促使了机器遗忘算法的发展，但如何有效评估这些算法仍是一个挑战。

Method: 研究者构建了一个包含三个基准数据集、六种遗忘算法和五种基于提示的审计方法的综合审计框架。此外，他们还提出了一种利用中间激活扰动的新技术，以探索超越基于提示的审计方法的替代方案。

Result: 通过多种审计算法，研究证明了不同遗忘策略的有效性和鲁棒性，并展示了新提出的基于中间激活扰动的技术在解决现有审计方法局限性方面的潜力。

Conclusion: 本研究为评估大型语言模型中的机器遗忘算法提供了一个全面的框架，并提出了一个创新的审计技术，有助于推动该领域的进一步发展。

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [127] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/abs/2505.23285)
*Muhammad Shafi,Syed Mohsin Bokhari*

Main category: cs.LG

TL;DR: This paper uses machine learning and Sentinel-2 data to analyze land cover changes in Oman from 2016-2021.


<details>
  <summary>Details</summary>
Motivation: To understand LULC changes across various governorates of Oman for better resource management, urbanization, soil and environmental protection, and sustainable development.

Method: Utilizing multispectral spatiotemporal data from Sentinel-2 and supervised machine learning algorithms to train and classify different land covers.

Result: Effective comparative evaluation of LULC changes within the study region from 2016 to 2021.

Conclusion: Machine learning techniques alongside satellite imagery can effectively monitor and predict LULC changes.

Abstract: Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [128] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/abs/2505.23309)
*Yixin Ren,Chenghou Jin,Yewei Xia,Li Ke,Longtao Huang,Hui Xue,Hao Zhang,Jihong Guan,Shuigeng Zhou*

Main category: cs.LG

TL;DR: The paper proposes a new conditional independence (CI) testing method via score-based generative modeling to overcome limitations of existing GAN-based methods.


<details>
  <summary>Details</summary>
Motivation: Determining CI relationships between random variables is crucial but challenging, especially in high-dimensional settings. Existing GAN-based CI testing methods have issues with modeling conditional distributions and training instability.

Method: A sliced conditional score matching scheme is used for accurate conditional score estimation and Langevin dynamics conditional sampling generates null hypothesis samples for precise Type I error control. A goodness-of-fit stage is incorporated to verify generated samples and enhance interpretability.

Result: Theoretical error bounds for conditional distributions modeled by score-based generative models are established and the validity of the CI tests is proven. Extensive experiments show superior performance compared to state-of-the-art methods on both synthetic and real-world datasets.

Conclusion: The proposed score-based generative modeling approach provides an effective way to improve CI testing, offering a promising direction for future research in this area.

Abstract: Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [129] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/abs/2505.23320)
*Connor Cooper,Geoffrey I. Webb,Daniel F. Schmidt*

Main category: cs.LG

TL;DR: 本文提出了一种基于对数线性回归的新方法，用于贝叶斯网络分类器的参数估计，该方法在处理分类数据时性能优于HDP平滑，并且速度提高了几个数量级。


<details>
  <summary>Details</summary>
Motivation: 传统的贝叶斯网络分类器学习方法在与随机森林等领先分类方法相比时表现不佳，尽管最近使用分层Dirichlet过程(HDP)的参数平滑技术使BNC在分类数据上的性能具有竞争力，但这些技术相对不灵活且需要复杂的采样过程。

Method: 引入一种新的参数估计方法，利用对数线性回归来近似HDP的行为。作为一种线性模型，该方法非常灵活且易于解释，可以利用大量关于学习线性模型的文献。

Result: 实验表明，该方法不仅可以在性能上超过HDP平滑，而且速度快几个数量级，在处理分类数据时仍然能够与随机森林保持竞争力。

Conclusion: 所提出的方法为贝叶斯网络分类器提供了一种更灵活、更简单的参数估计方式，提升了其在分类任务中的竞争力，同时大大减少了计算时间。

Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [130] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/abs/2505.23334)
*Tu Bui,Mohamed Suliman,Aparajita Haldar,Mohammed Amer,Serban Georgescu*

Main category: cs.LG

TL;DR: X2Graph是一种新的深度学习方法，它通过利用表列之间的外部关系知识（如基因相互作用），将每个样本转换为图结构，在小型生物表格数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习对文本、音频和图像数据集产生了变革性影响，但在表格数据（尤其是医疗领域数据稀缺的情况下）中的主导地位尚不明确。

Method: X2Graph利用表列之间的外部关系知识（如基因相互作用），将每个样本转换为图结构，从而可以应用标准的消息传递算法进行图建模。

Result: 在三个癌症亚型数据集上，X2Graph方法的表现优于现有的基于树和深度学习的方法。

Conclusion: X2Graph在小型生物表格数据集上实现了强大的性能。

Abstract: Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [131] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/abs/2505.23337)
*Chetan Verma,Aditya Srinivas Timmaraju,Cho Jui-Hsieh,Suyash Damle,Ngot Bui,Yang Zhang,Wen Chen,Xin Liu,Prateek Jain,Inderjit S Dhillon*

Main category: cs.LG

TL;DR: 提出了一种名为MatTA的新框架，通过Teacher-TA-Student方法训练多个精确的学生模型，仅需一次训练即可提供多种可部署选项，在实际A/B测试中关键指标提高了20%，在GPT-2 Medium上相对改进超过24%（SAT数学）和10%（LAMBADA基准）。


<details>
  <summary>Details</summary>
Motivation: 工业级ML模型需要满足快速变化的服务约束，这需要大量的资源进行模型开发。为了解决这一问题并提供灵活的模型选择以平衡精度和服务成本。

Method: 提出MatTA框架，利用Teacher-TA-Student配方训练多个精确的学生模型。TA模型是学生模型的放大版本，具有更高的容量，使学生模型能够更好地与教师模型相关联，并引入更多领域特定的专业知识。从TA模型中可以提取出多个精确的学生模型。

Result: 在专有数据集和模型上展示了该方法，并通过生产ML系统中的实时A/B测试，关键指标提高了20%。在公共模型GPT-2 Medium上，实现了SAT数学相对提高超过24%，LAMBADA基准相对提高超过10%。

Conclusion: MatTA框架可以通过一次训练提供多种可部署选项，从而权衡准确性与较低的服务成本，且在实际应用中表现出显著效果。

Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [132] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/abs/2505.23345)
*Yang Liu,Deyu Bo,Wenxuan Cao,Yuan Fang,Yawen Li,Chuan Shi*

Main category: cs.LG

TL;DR: Graph self-supervised learning uses Graph Positional Autoencoders (GraphPAE) with a dual-path architecture for effective graph representation, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Current graph autoencoders mainly capture low-frequency signals and lack the ability to learn expressive structural information.

Method: Propose Graph Positional Autoencoders (GraphPAE) that use a dual-path architecture: one path reconstructs node features using positional encoding, and the other refines node positions and approximates eigenvectors.

Result: GraphPAE achieves state-of-the-art performance in heterophilic node classification, graph property prediction, and transfer learning, significantly outperforming baseline methods.

Conclusion: GraphPAE effectively learns diverse frequency information and expressive structural information, providing a superior approach for graph self-supervised learning.

Abstract: Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [133] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2505.23347)
*Yuting Li,Shaoyuan Huang,Tengwen Zhang,Cheng Zhang,Xiaofei Wang,Victor C. M. Leung*

Main category: cs.LG

TL;DR: In this paper, the authors propose Sentinel, a proactive anomaly detection-based scheduling framework for Crowdsourced Cloud-edge service Platforms (CCPs). It models the scheduling process as a two-stage Pre-Post-Scheduling paradigm to reduce anomalies and improve revenue.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of anomaly detection in stream scheduling for CCPs, which are crucial for optimizing revenue but face difficulties due to instability and scale.

Method: Sentinel models the scheduling process as a two-stage Pre-Post-Scheduling paradigm. In the pre-scheduling stage, it conducts anomaly detection and constructs a strategy pool. In the post-scheduling stage, it triggers an appropriate scheduling based on a pre-generated strategy upon request arrival.

Result: Extensive experiments show that Sentinel reduces anomaly frequency by 70%, improves revenue by 74%, and doubles the scheduling speed.

Conclusion: Sentinel significantly enhances the performance of CCPs in terms of reducing anomalies, increasing revenue, and improving scheduling efficiency.

Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [134] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: Rewards in RLHF can be biased, negatively affecting LLM alignment. This paper addresses reward unfairness via a bias-agnostic resource allocation approach, proposing two methods (Fairness Regularization and Fairness Coefficient) to promote fairness in rewards. Experiments show improved alignment with human preferences.


<details>
  <summary>Details</summary>
Motivation: Biased rewards in RLHF may lead to misalignment of LLMs with human preferences, necessitating a solution to ensure fairness in reward distribution.

Method: Model preference learning as a resource allocation problem, treating rewards as resources. Propose Fairness Regularization and Fairness Coefficient methods to balance utility and fairness in reward distribution without targeting specific biases.

Result: The proposed methods effectively align LLMs with human preferences in a fairer manner when applied in verification and reinforcement learning settings.

Conclusion: The bias-agnostic resource allocation approach successfully mitigates reward unfairness, promoting better alignment of LLMs with human preferences.

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [135] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/abs/2505.23355)
*Maxiu Xiao,Jianglin Lan,Jingxing Yu,Eldert van Henten,Congcong Sun*

Main category: cs.LG

TL;DR: Interactive reinforcement learning (RL) with imperfect inputs is explored for greenhouse climate control. Three algorithms (reward shaping, policy shaping, control sharing) are developed and evaluated in a simulated environment. Policy shaping and control sharing show improvements in profit by 8.4% and 6.8%, while reward shaping leads to a 9.4% decrease due to sensitivity to imperfect inputs.


<details>
  <summary>Details</summary>
Motivation: Climate control in greenhouses directly impacts crop growth and resource use. Current RL applications face challenges such as limited training efficiency and reliance on initial conditions. Interactive RL, which integrates human input, could address these issues but has not been applied to greenhouse climate control yet.

Method: Three interactive RL algorithms tailored for greenhouse climate control were developed: reward shaping, policy shaping, and control sharing. The study analyzed the contradicting characteristics of grower inputs and proposed a neural network-based approach to enhance robustness under limited input availability. A comprehensive evaluation was conducted in a simulated greenhouse environment.

Result: Interactive RL with imperfect grower inputs improved RL agent performance. Policy shaping and control sharing performed better when handling imperfect inputs, achieving 8.4% and 6.8% profit improvement respectively. Reward shaping was sensitive to imperfect inputs, leading to a 9.4% profit decrease.

Conclusion: Incorporating interactive RL with imperfect inputs into greenhouse climate control shows potential. Algorithms influencing action selection like policy shaping and control sharing perform better than those manipulating the reward function, highlighting the importance of selecting appropriate mechanisms.

Abstract: Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [136] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/abs/2505.23369)
*Mannmohan Muthuraman*

Main category: cs.LG

TL;DR: Dynamic Spectral Backpropagation (DSBP) improves neural network training with five extensions, showing better performance than SAM, LoRA, and MAML.


<details>
  <summary>Details</summary>
Motivation: To enhance neural network training under resource constraints by reducing complexity and promoting flat minima.

Method: Projecting gradients onto principal eigenvectors with five extensions: dynamic spectral inference, spectral architecture optimization, spectral meta learning, spectral transfer regularization, and Lie algebra inspired dynamics.

Result: Outperforms SAM, LoRA, and MAML on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet supported by a third order stochastic differential equation (SDE) and a PAC Bayes limit.

Conclusion: DSBP with its extensions shows great potential in addressing challenges in robustness, fewshot learning, and hardware efficiency.

Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [137] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/abs/2505.23378)
*Roseline Polle,Agnes Norbury,Alexandra Livia Georgescu,Nicholas Cummins,Stefano Goria*

Main category: cs.LG

TL;DR: The paper explores three meta-learning approaches for speaker-dependent modeling in speech-based health monitoring to predict time since sleep from speech as a function of fatigue. Using pre-trained speech embeddings on a large dataset, transformer-based methods achieved the best performance.


<details>
  <summary>Details</summary>
Motivation: Speaker-dependent modeling can significantly enhance performance in speech-based health monitoring applications. However, traditional mixed-effect models require retraining for each new observation, making them impractical for real-world use.

Method: The task is reformulated as a meta-learning problem and three approaches are explored: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Pre-trained speech embeddings are used and these methods are evaluated on a large longitudinal dataset of shift workers.

Result: All meta-learning approaches outperformed both cross-sectional and conventional mixed-effects models. The transformer-based method achieved the strongest performance.

Conclusion: Meta-learning approaches, particularly transformer-based models, provide an effective solution for speaker adaptation in speech-based health monitoring.

Abstract: Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [138] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/abs/2505.23383)
*Ahmad Anaqreh,Shih-Kai Chou,Mihael Mohorčič,Carolina Fortuna*

Main category: cs.LG

TL;DR: 为了应对无线通信系统扩展带来的挑战，本文提出了一种基于AI的新方法来加速路径损耗模型的发现，并保持可解释性。该方法自动化了模型公式化、评估和改进过程，通过两种技术（深度符号回归和Kolmogorov-Arnold网络）在合成和真实数据集上进行验证，结果表明新方法比传统方法预测误差减少多达75%。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的扩展，对保证建模精度和互操作性的方法需求日益增长，而传统统计方法已难以满足需求。虽然AI技术被越来越多地采用，但大多数方法缺乏可解释性。

Method: 提出了一种新方法，结合深度符号回归（Deep Symbolic Regression）和Kolmogorov-Arnold网络两种技术，自动化模型公式化、评估和改进过程，以加速路径损耗模型的发现并保持可解释性。

Result: 实验结果表明，Kolmogorov-Arnold网络实现了接近1的R^2值和最小预测误差，而深度符号回归生成了紧凑模型且具有中等准确性。与传统方法相比，新方法预测误差减少了多达75%。

Conclusion: 所提出的方法在准确性和可解释性方面表现出色，为下一代路径损耗模型的发现提供了高效解决方案。

Abstract: Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [139] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta,Daniele Angioni,Maura Pintor,Ambra Demontis,Lea Schönherr,Battista Biggio,Fabio Roli*

Main category: cs.LG

TL;DR: 在类增量学习（CIL）中，模型需要学习新类而不遗忘旧类，并处理未知类输入。当前方法通过多头模型和OOD检测解决这些问题，但依赖于包含过去数据的记忆缓冲区。本文分析了后验OOD检测方法，发现它们在推理时可有效替代缓冲区，达到相似或更好的性能。实验结果支持这一发现，为设计高效且保护隐私的CIL系统提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法虽然有效，但依赖记忆缓冲区进行训练，这引发了隐私、可扩展性和训练时间增加等问题。因此，研究者希望找到一种不依赖记忆缓冲区的方法来解决类增量学习中的挑战。

Method: 本文深入分析了后验OOD检测方法，并研究了它们在推理阶段的应用是否可以消除对记忆缓冲区的需求。通过适当的调整，这些方法被证明可以在没有缓冲区的情况下，实现与缓冲区方法相当或更优的性能。

Result: 实验结果表明，在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上，无缓冲区的方法在类增量学习和未知样本拒绝方面表现得与缓冲区方法相当甚至更好。

Conclusion: 后验OOD检测方法在推理阶段可以有效替代记忆缓冲区，从而为设计高效且保护隐私的开放世界CIL系统提供了新的方向和思路。

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [140] [Bidirectional predictive coding](https://arxiv.org/abs/2505.23415)
*Gaspard Oliviers,Mufeng Tang,Rafal Bogacz*

Main category: cs.LG

TL;DR: The paper proposes bidirectional predictive coding (bPC), which incorporates both generative and discriminative inference in a biologically plausible circuit, showing superior performance in specialized tasks.


<details>
  <summary>Details</summary>
Motivation: Existing predictive coding models are unidirectional, focusing either on generative or discriminative inference. However, experimental evidence shows that the brain employs both types of inference, indicating a need for a model that can handle bidirectional processing.

Method: The authors developed bidirectional predictive coding (bPC) that integrates both generative and discriminative inference within a biologically plausible circuit implementation. This approach involves creating an energy landscape that supports both types of tasks simultaneously.

Result: bPC matches or outperforms unidirectional models in their specialized generative or discriminative tasks. It also demonstrates superior performance in two biologically relevant tasks: multimodal learning and inference with missing information.

Conclusion: bPC more closely resembles biological visual inference compared to unidirectional PC models, suggesting it as a better computational model for understanding brain functions.

Abstract: Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [141] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/abs/2505.23421)
*Zheming Zhang,Yan Jiang,Qingshan Li,Ai Han*

Main category: cs.LG

TL;DR: In China's competitive fresh e-commerce market, front-end warehouses face challenges in inventory management due to their small size and capacity constraints. Traditional PTO methods have limitations in aligning prediction with inventory goals and prioritizing consumer satisfaction. This paper proposes a multi-task OTPTO approach that jointly optimizes product selection and inventory management. Experimental results from JD.com's 7Fresh platform show that the OTPTO method significantly enhances the full order fulfillment rate.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deciding which goods to stock and in what quantities in small-sized front-end warehouses while considering capacity constraints and enhancing consumer satisfaction.

Method: The proposed multi-task OTPTO approach includes a 0-1 mixed integer programming model OM1 for determining historically optimal inventory levels, a product selection model PM1, a stocking model PM2 for prediction, and a post-processing algorithm OM2 for refining the combined results.

Result: Compared to the PTO approach, the OTPTO method increases the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and reduces the gap to the optimal full order fulfillment rate by 5.27%.

Conclusion: The OTPTO method is effective in managing inventory at front-end warehouses of fresh e-commerce platforms, providing valuable insights for future research.

Abstract: In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [142] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/abs/2505.23426)
*Yinuo Wang,Mining Tan,Wenjun Zou,Haotian Lin,Xujie Song,Wenxuan Wang,Tong Liu,Likun Wang,Guojian Zhan,Tianze Zhu,Shiqi Liu,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: DACER2 introduces a Q-gradient field objective and temporal weighting mechanism to improve training and inference efficiency in diffusion models for online RL, achieving state-of-the-art performance on MuJoCo benchmarks with only five diffusion steps.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have shown great potential in offline RL and imitation learning. However, the current method DACER, while performing strongly, requires many diffusion steps which hampers training efficiency. Reducing these steps leads to performance degradation, making inference efficiency a bottleneck for real-time online RL settings.

Method: The paper proposes DACER2, which incorporates a Q-gradient field objective as an auxiliary optimization target to guide the denoising process at each diffusion step. It also introduces a temporal weighting mechanism that adjusts the influence of the Q-gradient field according to the diffusion time step, allowing efficient noise elimination in early stages and action refinement in later stages.

Result: Experimental results on MuJoCo benchmarks and multimodal tasks show that DACER2 achieves state-of-the-art performance in most MuJoCo control tasks using only five diffusion steps. Additionally, it exhibits stronger multimodality compared to DACER.

Conclusion: DACER2 successfully improves training and inference efficiency while maintaining or enhancing performance, overcoming the limitations of its predecessor DACER.

Abstract: Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [143] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/abs/2505.23427)
*Monika Gahalawat,Maneesh Bilalpur,Raul Fernandez Rojas,Jeffrey F. Cohn,Roland Goecke,Ramanathan Subramanian*

Main category: cs.LG

TL;DR: 研究发现头部运动模式是评估抑郁严重程度的有效生物标志物，基于kineme的特征比原始头部运动描述符和其他视觉行为线索更具通用性。


<details>
  <summary>Details</summary>
Motivation: 尽管研究人员已经探索了多种言语和非言语的行为线索来进行自动抑郁评估，但头部运动尚未得到充分关注。此外，通过单一数据集验证机器学习模型的做法可能限制模型的通用性。

Method: 使用来自不同西方文化的三个抑郁数据集（德国：AVEC2013、澳大利亚：Blackdog和美国：Pitt数据集），通过两种方法（k折交叉验证和模型重用）来评估基于kineme的模型在估计抑郁严重程度方面的有效性与通用性。

Result: 头部运动模式作为估计抑郁严重程度的生物标志物非常有效，在分类和回归任务中均表现出高度竞争力；基于kineme的特征比原始头部运动描述符更适合二元严重性分类，并且比其他视觉行为线索更适合严重性估计（回归）。

Conclusion: 头部运动模式是评估抑郁严重程度的有效生物标志物，基于kineme的特征具有更高的通用性。

Abstract: Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [144] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: The paper explores the underexplored influence of diversity in reinforcement learning (RL)-based training for large language models (LLMs) reasoning, proposes a novel diversity-aware policy optimization method, and achieves improvements in mathematical reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored.

Method: The authors propose a novel diversity-aware policy optimization method. They design a token-level diversity and reformulate it into a practical objective, then selectively apply it to positive samples within the R1-zero training framework.

Result: This method results in a 3.5 percent average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions.

Conclusion: Diversity in RL-based training has a strong positive impact on LLM reasoning, as evidenced by the correlation between solution diversity and reasoning potential, leading to improved performance and solution robustness.

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [145] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/abs/2505.23437)
*Antonio Ferrara,Andrea Pugnana,Francesco Bonchi,Salvatore Ruggieri*

Main category: cs.LG

TL;DR: 这篇论文探讨了在高风险领域（如健康、教育和就业）中排名系统对决策的影响，并提出了一种新的方法，将'弃权'机制引入配对学习排序任务中。该方法基于限制排序器的条件风险来决定是否弃权，同时提供了最优弃权策略的理论描述、一个与模型无关的算法以及广泛的实证评估。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域中，排名系统对决策有重大影响，因此需要安全机制。'弃权'机制允许系统在不确定性高的情况下将决策交给人类专家，但目前主要应用于分类任务，其他机器学习范式中的应用尚待开发。

Method: 提出了一种基于阈值化排序器条件风险的新方法。当估计的风险超过预定义阈值时，系统选择弃权。此外，还提供了最优弃权策略的理论描述和构建弃权排序模型的通用算法。

Result: 通过多个数据集上的广泛实证评估，证明了该方法的有效性。

Conclusion: 本文提出的方法为配对学习排序任务中的弃权问题提供了一个有效的解决方案，其理论和实践贡献可以推动这一领域的进一步发展。

Abstract: Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [146] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Haoran Duan,Zhengwei Tao,Xuan Zhang,Jiandong Li*

Main category: cs.LG

TL;DR: The paper rethinks the application of regularization methods in knowledge graph completion (KGC), introduces a novel sparse-regularization method called SPR, and demonstrates its effectiveness through various comparative experiments.


<details>
  <summary>Details</summary>
Motivation: Most previous efforts in KGC have neglected to take advantage of regularization from a deeper perspective, thus not fully utilizing their potential.

Method: The authors introduce a novel sparse-regularization method named SPR that embeds the concept of rank-based selective sparsity into the KGC regularizer. It selectively penalizes components with significant features in the embedding vector.

Result: Various comparative experiments on multiple datasets and models show that the SPR regularization method outperforms other regularization methods and enables the KGC model to further break through the performance margin.

Conclusion: Carefully designed regularization not only alleviates overfitting and reduces variance but also enables KGC models to surpass their original performance upper bounds.

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [147] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/abs/2505.23443)
*Benyamin Trachtenberg,Nir Rosenfeld*

Main category: cs.LG

TL;DR: 在战略分类中，标准监督学习设置被扩展以支持用户战略性行为的概念，这种行为以对分类器做出代价高昂的特征操作形式表现。虽然标准学习支持广泛的模型类别，但战略分类的研究主要集中在线性分类器上。本工作旨在通过探索非线性分类器下的战略行为及其对学习的影响来扩展研究范围。关键发现是，在战略环境下，通用逼近器（如神经网络）不再具有通用性。


<details>
  <summary>Details</summary>
Motivation: 战略分类问题目前主要集中在线性分类器上，而非线性分类器的战略行为尚未得到充分研究。因此，需要探讨非线性分类器下战略行为的表现形式以及其对决策边界点、分类器表达能力及模型类复杂度的影响。

Method: 采用自下而上的方法，分析非线性分类器下战略行为如何影响决策边界点、分类器表达能力和模型类复杂度。通过实证研究展示战略环境如何导致性能差距，即使是在不受限的模型类中。

Result: 研究表明，在战略环境中，通用逼近器（例如神经网络）不再具有通用性，并且可能会出现性能差距。

Conclusion: 非线性分类器在战略环境下的表现与线性分类器有显著差异，这表明需要重新评估和设计适用于战略环境的分类器。

Abstract: In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [148] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 提出了一种结合网络反演与分类器训练的新框架，用于同时解决OOD检测和不确定性估计问题。通过引入“垃圾”类并迭代训练、反演和排除过程，该方法可有效检测OOD样本并估计不确定性，无需外部OOD数据集或事后校准技术。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，构建安全的机器学习系统需要有效的OOD检测和不确定性估计方法，尤其是在不可避免存在意外输入的情况下。然而，现有的方法可能需要外部OOD数据集或依赖事后校准技术。因此，本文旨在提出一种不依赖这些额外资源的统一解决方案。

Method: 1. 扩展标准n类分类器为(n+1)类模型，新增一个“垃圾”类，初始填充随机高斯噪声以表示异常输入。
2. 在每次训练周期后，利用网络反演重建对应所有输出类别的输入图像，初始表现为噪声且不连贯的样本被归入垃圾类。
3. 通过迭代训练、反演和排除过程，使反演样本逐渐接近分布内数据，从而优化决策边界并将OOD内容推入垃圾类。

Result: 实验结果表明，该方法能够有效地检测OOD样本（通过将其分类到垃圾类）并估计预测的不确定性（通过置信分数）。此外，该方法具有可扩展性和可解释性，且不需要外部OOD数据集或事后校准技术。

Conclusion: 本文提出的框架提供了一种统一的解决方案，同时解决了OOD检测和不确定性估计的问题，展现出良好的性能、可扩展性和可解释性，为构建更安全的机器学习系统提供了新思路。

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [149] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/abs/2505.23458)
*Kevin Frans,Seohong Park,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: The paper presents CFGRL, a framework combining policy improvement and diffusion model guidance for offline reinforcement learning, showing that increased guidance weighting leads to better performance without needing to learn a value function.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling reinforcement learning systems while leveraging the scalability and simplicity of generative modeling techniques.

Method: Derive a direct relation between policy improvement and guidance of diffusion models, creating the CFGRL framework which operates without explicitly learning a value function and is trained using supervised learning methods.

Result: On offline RL tasks, increased guidance weighting reliably leads to increased performance, allowing simple supervised methods to prioritize optimality and improve performance across various scenarios.

Conclusion: CFGRL successfully combines the strengths of reinforcement learning and generative modeling, providing a scalable and effective approach for offline RL without relying on value function learning.

Abstract: At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [150] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/abs/2505.23459)
*Safwan Labbi,Paul Mangold,Daniil Tiapkin,Eric Moulines*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [151] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/abs/2505.23470)
*Chenjie Li,Amir Gilad,Boris Glavic,Zhengjie Miao,Sudeepa Roy*

Main category: cs.LG

TL;DR: The paper presents a method to repair labeling functions (LFs) in programmatic weak supervision using minimal changes based on a small set of labeled examples, improving the accuracy and reliability of generated labels.


<details>
  <summary>Details</summary>
Motivation: Programmatic weak supervision (PWS) reduces human effort for data labeling but relies heavily on the accuracy of user-provided labeling functions (LFs). The motivation is to improve the quality of LFs using a small set of labeled examples.

Method: The method involves repairing LFs by minimally altering their outputs on labeled examples. This ensures sufficient evidence for correct labels and maintains high LF accuracy. LFs are modeled as conditional rules allowing selective output changes.

Result: Experiments show that the system can significantly enhance the quality of LFs even with surprisingly small sets of labeled datapoints.

Conclusion: The developed techniques effectively repair LFs, leading to more accurate and reliable labels in programmatic weak supervision.

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [152] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/abs/2505.23489)
*Ildus Sadrtdinov,Ivan Klimov,Ekaterina Lobacheva,Dmitry Vetrov*

Main category: cs.LG

TL;DR: In this paper, the authors explore the thermodynamic interpretation of SGD in neural network training. They show that SGD minimizes a free energy function balancing loss and weights entropy, with temperature determined by learning rate. Empirical validation shows underparameterized models follow free energy minimization, while overparameterized models behave differently due to stochastic gradient signal-to-noise ratio.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to understand why high learning rates prevent convergence to loss minima and how different learning rates lead to stabilization at different loss levels in neural network training using stochastic gradient descent (SGD).

Method: The method involves presenting a thermodynamic interpretation of the stationary behavior of SGD under fixed learning rates. The authors empirically validate their findings on both underparameterized and overparameterized models, analyzing the impact of learning rates on the temperature of the system.

Result: The results indicate that underparameterized models consistently follow free energy minimization, with temperature increasing monotonically with learning rate. Overparameterized models show an effective temperature drop to zero at low learning rates, causing SGD to minimize the loss directly and converge to an optimum.

Conclusion: The conclusion drawn is that the thermodynamic interpretation provides new insights into the behavior of SGD in neural network training, particularly regarding the effects of learning rates and the differences between underparameterized and overparameterized models.

Abstract: We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [153] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/abs/2505.23496)
*Sabina J. Sloman,Michele Caprio,Samuel Kaski*

Main category: cs.LG

TL;DR: 在数据噪声的情况下，统计学习者的目标是解决关于测试时间遇到的数据的外在不确定性，即识别测试数据的分布。本文引入了外在误差的定义，并提供了一个通用的分解外在误差界限，首次考虑了特定的外在误差，涵盖了所有可能的外在不确定性来源，并分别归因于学习过程和环境的多个方面。此外，还提供了贝叶斯迁移学习和分布转移等特殊情况下的外在误差界限和泛化界限，以及负迁移的新定义和实验验证。


<details>
  <summary>Details</summary>
Motivation: 当数据有噪声时，需要解决测试数据的外在不确定性问题，但许多现实学习场景中存在无法仅凭训练数据解决的外在不确定性来源：多任务学习、目标数据与源数据的系统性差异（分布转移）以及不完美的学习。

Method: 1. 提出外在误差的定义及通用分解误差界限。
2. 将误差界限应用于特定场景如贝叶斯迁移学习和分布转移。
3. 提供对应的泛化界限。
4. 定义负迁移并进行合成实验验证。

Result: 成功推导出了涵盖多种外在不确定性的误差界限，能够分别归因于学习过程的不同方面，并在特定场景下进行了验证，同时通过实验支持了负迁移的新定义。

Conclusion: 本文提出的外在误差界限为理解多任务学习、分布转移和不完美学习中的外在不确定性提供了理论基础，并通过负迁移的定义扩展了迁移学习的研究视角。

Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [154] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/abs/2505.23506)
*Sebastián Jiménez,Mira Jürgens,Willem Waegeman*

Main category: cs.LG

TL;DR: In recent years, supervised learning methods have been proposed to disentangle aleatoric and epistemic uncertainty based on second-order distributions. However, these methods fail to capture critical components of epistemic uncertainty due to model bias. The paper argues that meaningful aleatoric estimates are feasible only if all relevant sources of epistemic uncertainty are properly represented.


<details>
  <summary>Details</summary>
Motivation: The motivation is the current limitations in capturing full spectrum of epistemic uncertainty in machine learning models, particularly due to neglected model bias component.

Method: Using a fine-grained taxonomy of epistemic uncertainty sources, decomposing classical bias-variance, employing simulation-based evaluation protocol encompassing procedural- and data-driven uncertainty components.

Result: Current methods rarely capture the full spectrum of epistemic uncertainty. High model bias can lead to misleadingly low estimates of epistemic uncertainty. Common second-order uncertainty quantification methods blur bias-induced errors into aleatoric estimates.

Conclusion: Meaningful aleatoric estimates are possible only when all relevant sources of epistemic uncertainty are properly represented.

Abstract: In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [155] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/abs/2505.23520)
*Yu Zhang,Dong Guo,Fang Wu,Guoliang Zhu,Dian Ding,Yiming Zhang*

Main category: cs.LG

TL;DR: AnchorAttention是一种新的稀疏注意力机制，通过更细粒度的稀疏策略，在保持高召回率的同时显著减少了计算时间。相比现有方法，在文本长度为128k时，速度提高了1.44倍。


<details>
  <summary>Details</summary>
Motivation: 现有的动态模式匹配和块稀疏低级实现方法在处理大语言模型的预填充阶段时，由于依赖局部信息进行模式识别，无法捕捉全局上下文，并且块的粗粒度导致持续的内部稀疏性，从而影响了准确性和效率。

Method: AnchorAttention由三个关键部分组成：(1) 基于模式的锚点计算，利用所有输入中的共性快速计算一组接近最大值的分数作为锚点；(2) 差异感知条纹稀疏性识别，与锚点进行差异感知比较以快速获得重要区域的离散坐标；(3) 细粒度稀疏计算，用同时加载离散KV位置替代传统的连续KV块加载方法，最大化稀疏率并保留硬件计算潜力。

Result: 在相同的召回水平下，AnchorAttention实现了更高的稀疏率，大幅减少了计算时间。在文本长度为128k时，其速度比现有最先进的方法快1.44倍，同时保持更高的召回率。

Conclusion: AnchorAttention通过细粒度的稀疏策略有效解决了现有方法中因粗粒度块和局部信息依赖带来的问题，提高了大语言模型在长上下文处理中的速度和准确性。

Abstract: Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [156] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/abs/2505.23523)
*Arjun Devraj,Eric Ding,Abhishek Vijaya Kumar,Robert Kleinberg,Rachee Singh*

Main category: cs.LG

TL;DR: StragglAR is an AllReduce algorithm that accelerates distributed training and inference in the presence of persistent stragglers, achieving a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms for large GPU clusters with persistent stragglers.


<details>
  <summary>Details</summary>
Motivation: Bulk-synchronous AllReduce algorithms can be delayed by a persistent straggler that is slower to reach the synchronization barrier required to begin the collective in distributed machine learning workloads.

Method: StragglAR implements a ReduceScatter among the remaining GPUs during the straggler-induced delay, and then executes a novel collective algorithm to complete the AllReduce once the straggler reaches the synchronization barrier.

Result: StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22% speedup over state-of-the-art AllReduce algorithms.

Conclusion: StragglAR effectively addresses the challenge of persistent stragglers in distributed machine learning workloads, providing significant speedups in both training and inference.

Abstract: Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [157] [Normalizing Flows are Capable Models for RL](https://arxiv.org/abs/2505.23527)
*Raj Ghugare,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: Normalizing flows can be effectively integrated into RL algorithms, contrary to the belief that they lack expressivity.


<details>
  <summary>Details</summary>
Motivation: Current RL algorithms successfully use powerful probabilistic models like transformers, energy-based models, and diffusion/flow-based models. However, these models come with computational challenges. Normalizing flows (NFs) offer a promising alternative due to their ability to enable likelihoods and sampling without solving differential equations or autoregressive architectures, yet their potential in RL has been underexplored.

Method: The researchers propose a single NF architecture that can be seamlessly integrated into RL algorithms, functioning as a policy, Q-function, and occupancy measure.

Result: This approach simplifies algorithms and achieves higher performance in various RL settings including imitation learning, offline RL, goal-conditioned RL, and unsupervised RL.

Conclusion: Contrary to the prevailing belief, normalizing flows possess sufficient expressivity and can significantly enhance RL algorithms.

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [158] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/abs/2505.23528)
*Maria Eleftheria Vlontzou,Maria Athanasiou,Christos Davatzikos,Konstantina S. Nikita*

Main category: cs.LG

TL;DR: This paper conducts a thorough fairness analysis of ML models used in diagnosing MCI and AD from MRI-derived neuroimaging features, investigates biases related to age, race, and gender, evaluates fairness metrics, compares bias mitigation strategies, introduces a new composite measure for fairness-performance trade-off, and finds significant age and race biases but no substantial gender bias.


<details>
  <summary>Details</summary>
Motivation: To ensure fairness in ML models for diagnosing MCI and AD by identifying and mitigating biases related to age, race, and gender.

Method: Performing a comprehensive analysis of biases in ML models using a multi-cohort dataset, assessing various fairness definitions and metrics, comparing pre-processing, in-processing, and post-processing bias mitigation strategies, and introducing a novel composite measure for fairness-performance trade-off.

Result: Biases related to age and race were found, with varying improvements from different mitigation strategies. No significant gender bias was observed. For race and gender, Reject Option Classification showed the best improvement in equalized odds, while adversarial debiasing worked best for age-related bias.

Conclusion: ML models for diagnosing MCI and AD exhibit biases related to age and race, which can be mitigated using appropriate strategies. The newly introduced composite measure effectively quantifies the trade-off between fairness and performance.

Abstract: The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [159] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2505.23529)
*Shifeng Xie,Aref Einizade,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: The paper proposes SubGEC, a novel method in graph representation learning that uses self-supervised learning to encode subgraphs into low-dimensional vectors with preserved characteristics.


<details>
  <summary>Details</summary>
Motivation: Graph Representation Learning aims to encode high-dimensional graph-structured data into low-dimensional vectors. Current methods often require expensive human annotations, so there is a need for effective self-supervised learning approaches.

Method: The authors introduce a subgraph Gaussian embedding module that maps subgraphs to a structured Gaussian space. They then use optimal transport distances (Wasserstein and Gromov-Wasserstein) to measure the similarity between subgraphs in a robust contrastive learning process.

Result: Extensive experiments across multiple benchmarks show that SubGEC outperforms or matches state-of-the-art approaches.

Conclusion: This work provides insights into designing SSL methods for GRL, highlighting the significance of the distribution of generated contrastive pairs.

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [160] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides,Wuyang Zhou,Chao Li,Qibin Zhao,Danilo Mandic*

Main category: cs.LG

TL;DR: tnLLM是一种新的张量网络结构搜索（TN-SS）框架，通过结合领域信息和大语言模型的推理能力来预测合适的张量网络结构。相比现有算法，tnLLM显著减少了函数评估次数，并提供了可解释的结果。此外，该方法还可以为基于采样的SOTA方法提供良好的初始化以加速收敛。


<details>
  <summary>Details</summary>
Motivation: 现有的张量网络结构搜索方法计算成本高且忽略了数据中的领域信息，导致透明度不足。因此需要一种更高效、能利用领域信息的方法来解决这个问题。

Method: 提出了一种名为tnLLM的新框架，通过领域感知的提示管道指导大语言模型根据张量模式之间的实际关系推断合适的张量网络结构。这种方法不仅优化目标函数，还生成与领域相关的解释。

Result: 实验结果表明，tnLLM在显著减少函数评估次数的情况下，达到了与现有最优算法相当的目标函数值。同时展示了利用LLM提供的领域信息可以加速基于采样的SOTA方法的收敛速度，同时保持理论性能保证。

Conclusion: tnLLM提供了一种高效且可解释的张量网络结构搜索方法，能够显著降低计算成本并为其他方法提供良好初始化，具有潜在的实际应用价值。

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [161] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/abs/2505.23552)
*Alex Adams*

Main category: cs.LG

TL;DR: This paper compares the performance of Moore-Penrose pseudoinverse and gradient descent in solving linear regression problems, providing guidance for choosing methods.


<details>
  <summary>Details</summary>
Motivation: To offer practical advice on selecting suitable methods for linear regression tasks.

Method: Theoretical analysis, computational complexity analysis, and empirical evaluation on both synthetic and real-world datasets are conducted.

Result: Each method's advantages in computational time, numerical stability, and predictive accuracy under specific conditions are outlined.

Conclusion: Researchers and practitioners can use this study as a reference when deciding between direct exact solutions and iterative approximate solutions.

Abstract: This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [162] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2505.23555)
*Yanzhao Hou,Jiaxiang Geng,Boyu Li,Xiaofeng Tao,Juncheng Wang,Xiaodong Xu,Bing Luo*

Main category: cs.LG

TL;DR: The paper proposes an adaptive federated LoRA strategy with independent client sampling to minimize the convergence wall-clock time of federated fine-tuning under both computation and communication heterogeneity.


<details>
  <summary>Details</summary>
Motivation: Federated LoRA is a promising technique for efficiently fine-tuning large language models (LLMs) on distributed devices, but existing approaches often inadequately overlook the theoretical and practical implications of system and data heterogeneity.

Method: Firstly, derive a new convergence bound for federated LoRA with arbitrary and independent client sampling without requiring the stringent bounded gradient assumption. Secondly, introduce an adaptive bandwidth allocation scheme that accounts for heterogeneous client resources and system bandwidth constraints. Then formulate and solve a non-convex optimization problem to jointly determine the LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock convergence time.

Result: Extensive experiments demonstrate that the proposed approach significantly reduces wall-clock training time compared to state-of-the-art methods across various models and datasets.

Conclusion: An adaptive federated LoRA strategy with independent client sampling can effectively minimize the convergence wall-clock time of federated fine-tuning under both computation and communication heterogeneity.

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [163] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo,Lijie Xu,Jie Liu,Dan Ye,Shuang Qiu*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架SPO，通过中间粒度的片段级优势估计，在策略优化中实现了更精确的信用分配，并在不同场景下展示了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型推理能力增强方法要么依赖于细粒度但难以准确训练的token-level方法，要么依赖于粗粒度但信用分配不精确的trajectory-level方法，因此需要一种折衷方案来平衡精度与效率。

Method: 提出了Segment Policy Optimization（SPO）框架，包含三个关键组件：灵活的片段划分、精确的片段优势估计以及基于片段优势的策略优化（引入了概率掩码策略）。针对特定场景，设计了SPO-chain（用于短链推理）和SPO-tree（用于长链推理），分别采用基于切点的划分和基于树的优势估计方法。

Result: 在GSM8K数据集上，SPO-chain相较于PPO和GRPO提升了6-12个百分点的准确率；在MATH500数据集上，SPO-tree在2K和4K上下文评估中相较于GRPO提升了7-11个百分点的准确率。

Conclusion: SPO通过中间粒度的优势估计实现了更精确的信用分配，并减少了估计成本，在短链和长链推理任务中均表现出显著的性能改进，代码已开源。

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [164] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/abs/2505.23565)
*Jiashuo Liu,Tianyu Wang,Henry Lam,Hongseok Namkoong,Jose Blanchet*

Main category: cs.LG

TL;DR: The paper introduces dro, an open-source Python library for distributionally robust optimization (DRO) which implements multiple DRO formulations and backbone models, is compatible with major machine learning frameworks, and significantly reduces runtime on large-scale datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and efficient tool for performing distributionally robust optimization in regression and classification problems.

Method: Development of an open-source Python library named dro that includes 14 DRO formulations, 9 backbone models resulting in 79 distinct DRO methods. The library uses vectorization and optimization approximation techniques to improve runtime performance and is compatible with scikit-learn and PyTorch.

Result: dro reduces runtime by 10x to over 1000x compared to baseline implementations on large-scale datasets.

Conclusion: dro offers a versatile and high-performance solution for DRO tasks and is supported by comprehensive documentation.

Abstract: We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [165] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/abs/2505.23569)
*Samo Hromadka,Kai Biegun,Lior Fox,James Heald,Maneesh Sahani*

Main category: cs.LG

TL;DR: A new unsupervised learning method called RP-GSSM is introduced for time series data,which combines the advantages of contrastive methods and probabilistic generative models.


<details>
  <summary>Details</summary>
Motivation: Current methods for learning from time series data either lack flexibility or require additional constraints such as ad-hoc regularization,auxiliary losses,or optimizer scheduling.

Method: The RP-GSSM is a probabilistic model that learns Markovian Gaussian latents explaining statistical dependence between observations at different time steps.It uses maximum likelihood learning and has no need for an explicit network mapping from latents to observations.

Result: The RP-GSSM outperforms alternatives in learning nonlinear stochastic dynamics from video,even with background distractors.It can learn task-relevant latents without additional constraints.

Conclusion: The RP-GSSM serves as a useful foundation model for various downstream applications due to its tractability and expressivity.

Abstract: We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [166] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: The paper presents BioReason, an architecture that combines a DNA foundation model with a Large Language Model (LLM) to improve biological reasoning. It shows significant performance gains in benchmarks like disease pathway prediction and variant effect prediction, offering interpretable biological traces.


<details>
  <summary>Details</summary>
Motivation: Current DNA foundation models have strong sequence representation but struggle with multi-step reasoning and lack transparent explanations.

Method: BioReason integrates a DNA foundation model with an LLM, enabling the LLM to process genomic information directly. It uses supervised fine-tuning and reinforcement learning for multi-step reasoning.

Result: BioReason achieves a 97% accuracy in disease pathway prediction (up from 88%) and demonstrates a 15% average performance gain in other biological reasoning tasks compared to single-modality baselines.

Conclusion: BioReason provides a transformative approach for AI in biology, enhancing mechanistic insights and accelerating hypothesis generation from genomic data.

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [167] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/abs/2505.23583)
*Zhiding Liu,Mingyue Cheng,Guanhao Zhao,Jiqian Yang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: 在时间序列预测中，尽管近期方法通过引入高级归纳偏置和训练策略提高了准确性，但实例级变化仍然是一个重大挑战。为了解决这个问题，本文提出了一个模型无关的框架PIR，通过预测后识别和修正来提高预测性能。实验表明，PIR有效减少了实例级误差，显著提升了预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在实际应用中具有重要作用，然而现有的预测方法尽管总体表现良好，但在处理实例级变化时仍然面临挑战，如分布偏移、缺失数据和长尾模式等问题导致特定实例预测效果不佳。

Method: 提出了一种模型无关的框架PIR，该框架首先通过估计预测准确性来识别偏差预测实例，然后利用上下文信息（包括协变量和历史时间序列）从局部和全局视角进行预测修正，以减少实例级误差。

Result: 在真实世界数据集上的广泛实验证明，PIR能够有效减轻实例级误差，并显著提升预测的可靠性。

Conclusion: PIR框架可以作为主流预测模型的补充工具，有效提升时间序列预测的准确性和可靠性，尤其在处理实例级变化方面表现出色。

Abstract: Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [168] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao,Li Dong,Xun Wu,Shaohan Huang,Zewen Chi,Furu Wei*

Main category: cs.LG

TL;DR: The paper introduces On-Policy RL with Optimal reward baseline (OPO), a new reinforcement learning algorithm that emphasizes exact on-policy training and optimal reward baselines to stabilize training, enhance exploration, and minimize gradient variance. Evaluated on mathematical reasoning benchmarks, OPO shows superior performance, stability, lower policy shifts, and higher output entropy without additional models or regularization terms.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning algorithms are crucial for aligning large language models with human preferences and enhancing their reasoning capabilities, but they often suffer from training instability and computational inefficiency.

Method: OPO is a novel reinforcement learning algorithm that focuses on exact on-policy training to stabilize the training process and enhance exploration, and it introduces an optimal reward baseline that theoretically minimizes gradient variance.

Result: OPO demonstrates superior performance and training stability on mathematical reasoning benchmarks without the need for additional models or regularization terms. It also achieves lower policy shifts and higher output entropy, leading to more diverse and less repetitive responses.

Conclusion: OPO represents a promising direction for stable and effective reinforcement learning in the context of large language model alignment and reasoning tasks.

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [169] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/abs/2505.23588)
*Mrinmay Sen,Sidhant R Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: This paper explores second-order optimization methods in Federated Learning (FL) to address slow convergence and high communication costs.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of slow convergence and excessive communication rounds in FL, especially when data across clients are highly heterogeneous.

Method: Examine various FL methods that leverage second-order optimization for accelerating training process, providing a comprehensive categorization and comparison of these methods based on multiple criteria.

Result: Findings show potential of incorporating Hessian curvature through second-order optimization into FL, highlighting key challenges such as efficient utilization of Hessian and its inverse.

Conclusion: Lays the groundwork for future research on scalable and efficient federated optimization methods.

Abstract: This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [170] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/abs/2505.23593)
*Nikita Agrawal,Simon Mertel,Ruben Mayer*

Main category: cs.LG

TL;DR: This paper argues against the use of black-box models in federated learning for post-training foundation language models, emphasizing contradictions with core federation principles like data privacy and autonomy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to critically analyze the implications of using black-box models in federated post-training within the context of foundation language models, highlighting concerns about data privacy and autonomy.

Method: The method involves a critical analysis of the usage of black-box models in federated learning, discussing various aspects of openness and their effects on federated learning.

Result: The result is a detailed account of how black-box models in federated post-training conflict with the principles of federation, such as data privacy and autonomy.

Conclusion: The conclusion is that black-box models should not be used in federated learning for post-training due to conflicts with federation principles, advocating for more transparent and open approaches.

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [171] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/abs/2505.23598)
*Radzim Sendyka,Christian Cabrera,Andrei Paleyes,Diana Robinson,Neil Lawrence*

Main category: cs.LG

TL;DR: This paper explores the capability of LLMs to solve obfuscated tasks, introduces eager pattern matching, and discusses implications for benchmarking and potential safety risks.


<details>
  <summary>Details</summary>
Motivation: To understand how well large language models can recognize and solve heavily obfuscated tasks, particularly in competitive programming and benchmark tasks.

Method: Compare performance across multiple LLMs and obfuscation methods (e.g., noise and redaction) on tasks from LeetCode and MATH datasets.

Result: LLMs can solve highly obfuscated tasks without key instructions or context through eager pattern matching, showing evidence of memorization rather than reasoning. There are distinct performance decay patterns between contaminated and unseen datasets.

Conclusion: Caution is needed when using standard datasets for benchmarking LLMs. Measuring performance decay under obfuscation could help detect dataset contamination and address safety risks.

Abstract: This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [172] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/abs/2505.23599)
*Eitan Levin,Yuxin Ma,Mateo Díaz,Soledad Villar*

Main category: cs.LG

TL;DR: 在现代学习任务中，模型需要处理可变大小的输入。为了应对图、集合和点云等领域的挑战，已经提出了与维度无关的架构。本文通过引入跨维度可迁移性的通用框架，探讨了图神经网络在从低维数据到高维数据的性能迁移问题，并提出了设计新可迁移模型的设计原则。实验结果支持了这些发现。


<details>
  <summary>Details</summary>
Motivation: 现代学习任务需要能够处理可变大小输入的模型，因此需要研究模型在不同维度数据上的可迁移性。

Method: 引入了一个通用框架来研究跨维度的可迁移性，通过在限制空间中将小问题实例与等效的大问题实例进行匹配，确保模型的连续性和可迁移性。基于该框架对现有架构进行了修改，并提供了设计新可迁移模型的原则。

Result: 理论分析和数值实验表明，所提出的框架和方法可以有效确保模型的可迁移性。

Conclusion: 本文提出的跨维度可迁移性框架为设计新的可迁移模型提供了理论基础和设计原则，有助于提高模型在不同维度数据上的适应能力。

Abstract: Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [173] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi,Jinbin Bai,Zhuoran Zhao,Wenhao Chai,Kaidong Yu,Jianzong Wu,Shuangyong Song,Yunhai Tong,Xiangtai Li,Xuelong Li,Shuicheng Yan*

Main category: cs.LG

TL;DR: Muddit是一种统一的离散扩散变压器，它能够在文本和图像模态之间实现快速和平行的生成。与从头开始训练的先前统一扩散模型不同，Muddit整合了一个预训练的文本到图像主干中的强大视觉先验，并配有一个轻量级的文本解码器，从而在统一架构下实现了灵活和高质量的多模态生成。实验结果表明，Muddit在质量和效率方面都达到了与显著较大的自回归模型相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归统一模型由于顺序解码而导致推理速度慢，而非自回归统一模型由于预训练主干的限制导致泛化能力弱。因此，需要一种新的模型来解决这些问题。

Method: 引入了Muddit，一种统一的离散扩散变压器。该模型集成了一个预训练的文本到图像主干的强大视觉先验，以及一个轻量级的文本解码器，以实现跨文本和图像模态的快速和平行生成。

Result: 实证结果显示，Muddit在质量与效率上均表现出与显著更大的自回归模型相竞争或更优越的性能。

Conclusion: 这项工作强调了当配备强大的视觉先验时，纯粹离散扩散作为一种可扩展且有效的统一生成骨干的潜力。

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [174] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/abs/2505.23607)
*Carolina Fortuna,Gregor Cerar,Blaz Bertalanic,Andrej Campa,Mihael Mohorcic*

Main category: cs.LG

TL;DR: The paper explores the role of multivariate data in machine learning models for energy applications, proposing a taxonomy to guide data model development and validating it through household electricity forecasting.


<details>
  <summary>Details</summary>
Motivation: There is a lack of structured studies considering the role of meta-data and additional measurements leading to multivariate data in energy applications, while most existing machine learning models rely on univariate data.

Method: 1. Propose a taxonomy that identifies and structures various types of data related to energy applications.
2. Use this taxonomy to guide application-specific data model development for training machine learning models.
3. Validate the taxonomy by studying its effect on forecasting accuracy using domain, contextual, and behavioral features with four interpretable machine learning techniques and three openly available datasets.
4. Employ feature importance techniques to explain individual feature contributions to forecasting accuracy.

Result: The proposed taxonomy effectively guides the selection of features for various types of models in household electricity forecasting applications. The study reveals the impact of different features (domain, contextual, behavioral) on forecasting accuracy and provides insights into individual feature contributions.

Conclusion: A taxonomy for structuring various types of data in energy applications can significantly enhance the development of machine learning models. The study demonstrates its effectiveness in improving forecasting accuracy in household electricity forecasting.

Abstract: The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [175] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/abs/2505.23609)
*Armando Bellante,Martin Plávala,Alessandro Luongo*

Main category: cs.LG

TL;DR: This paper proposes a family of permutation-invariant graph embeddings, generalizing the Skew Spectrum of graphs.


<details>
  <summary>Details</summary>
Motivation: The Skew Spectrum of graphs has limitations in handling richer graph structures such as attributed graphs, multilayer graphs, and hypergraphs. The authors aim to overcome these limitations by proposing a new class of graph invariants that are isomorphism-invariant and capable of embedding these complex structures.

Method: The method is grounded in group theory and harmonic analysis. It introduces a new class of graph invariants that generalize the Skew Spectrum. This new class can handle attributed graphs, multilayer graphs, and hypergraphs. A family of functions is defined to enable a trade-off between computational complexity and expressivity.

Result: The authors formally prove the invariance of their generalization and demonstrate its improved expressiveness through experiments. They also discuss its efficient computation.

Conclusion: The proposed family of permutation-invariant graph embeddings overcomes the limitations of the Skew Spectrum and provides a more expressive way to embed complex graph structures while maintaining computational efficiency.

Abstract: This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [176] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/abs/2505.23614)
*Xiangcheng Zhang,Haowei Lin,Haotian Ye,James Zou,Jianzhu Ma,Yitao Liang,Yilun Du*

Main category: cs.LG

TL;DR: This paper introduces a framework that uses classical search principles to improve inference-time control in diffusion models, leading to better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to adapt generated outputs of diffusion models to meet diverse test-time objectives by using principles from classical search algorithms.

Method: The method proposed is a general framework that orchestrates local and global search. It employs a theoretically grounded local search via annealed Langevin MCMC and performs compute-efficient global exploration using breadth-first and depth-first tree search.

Result: The approach was evaluated on a range of challenging domains, including planning, offline reinforcement learning, and image generation. Significant gains were observed in both performance and efficiency across all tasks.

Conclusion: Classical search provides a principled and practical foundation for inference-time scaling in diffusion models.

Abstract: Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [177] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/abs/2505.23615)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: Differentiable Logic Networks (DLNs) are extended to supervised tabular regression, providing interpretable reasoning and lower inference cost compared to neural networks.


<details>
  <summary>Details</summary>
Motivation: Neural networks achieve great performance in many domains but lack transparency in decision processes and have high computational costs in resource-constrained environments.

Method: The DLN framework is extended for supervised tabular regression by redesigning the final output layer to support continuous targets and unifying the training procedure into a single differentiable stage.

Result: Regression DLNs match or exceed baseline accuracy while preserving interpretability and fast inference when evaluated on 15 public regression benchmarks.

Conclusion: DLNs are a viable, cost-effective alternative for regression tasks requiring model transparency and computational efficiency.

Abstract: Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [178] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/abs/2505.23627)
*Griffin Dietz Smith,Dianna Yee,Jennifer King Chen,Leah Findlater*

Main category: cs.LG

TL;DR: The paper proposes a new end-to-end architecture for improving verbatim transcription and direct miscue detection by incorporating target reading text via prompting.


<details>
  <summary>Details</summary>
Motivation: To improve on current methods for reading error annotation which perform poorly when ASR inaccurately transcribes verbatim speech.

Method: Propose an end-to-end architecture that incorporates the target reading text via prompting and is trained for both improved verbatim transcription and direct miscue detection.

Result: Conducted two case studies showing proposed strategies improve verbatim transcription and miscue detection compared to state-of-the-art.

Conclusion: Incorporating reading text through prompting benefits verbatim transcription performance over fine-tuning, and it is feasible to augment speech recognition tasks for end-to-end miscue detection.

Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [179] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/abs/2505.23640)
*Yilin Xie,Shiqiang Zhang,Jixiang Qing,Ruth Misener,Calvin Tsay*

Main category: cs.LG

TL;DR: Graph Bayesian optimization (BO) shows promise in neural architecture search (NAS). This paper develops explicit optimization formulations for graph input space, proves the encoding's efficacy, and demonstrates its success in finding optimal architectures.


<details>
  <summary>Details</summary>
Motivation: Most existing graph BO works focus on developing graph surrogates models, but acquisition optimization over graph structures is not well studied due to complexity.

Method: This paper presents explicit optimization formulations for graph input space including properties such as reachability and shortest paths, which are used later to formulate graph kernels and the acquisition function. Restrictions for the NAS domain with either node or edge labels are also provided.

Result: Numerical results over several NAS benchmarks show that their method efficiently finds the optimal architecture for most cases.

Conclusion: The proposed encoding is an equivalent representation of the graph space and provides restrictions for the NAS domain.

Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [180] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/abs/2505.23648)
*Halil Alperen Gozeten,M. Emrullah Ildiz,Xuechen Zhang,Hrayr Harutyunyan,Ankit Singh Rawat,Samet Oymak*

Main category: cs.LG

TL;DR: 本文提出了一种基于连续值标记的链式思维方法（CoT2），通过优化和探索方法提升逻辑推理任务的表现，并提出了新的监督策略和采样策略。实验表明，使用CoT2进行策略优化可以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型通过从有限词汇中自回归地采样离散标记生成链式思维轨迹，但这种方法在搜索能力和推理效率方面存在局限性。因此，研究连续值标记的链式思维方法（CoT2）可能提供更丰富、更具表达力的替代方案。

Method: 1. 理论分析：证明CoT2能够并行跟踪多条链式思维轨迹，并量化其对推理效率的好处。
2. 新的监督策略：将softmax输出与目标链式轨迹的经验标记分布相匹配。
3. 两种采样策略：
   - 第一种：在每个解码步骤采样和组合K个离散标记以控制并行度（当K=1时退化为标准CoT）。
   - 第二种：在概率单纯形上进行连续探索。
4. 应用场景：证明单层Transformer结合CoT2可以解决组合问题（如子集和问题）。

Result: 实验结果表明，使用CoT2进行策略优化确实可以提升模型的性能，超越其初始的离散或连续监督水平。

Conclusion: 连续值标记的链式思维方法（CoT2）为逻辑推理任务提供了更高效和丰富的解决方案，并通过优化和探索策略进一步提升了模型表现。

Abstract: Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [181] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin,Minsang Seok,Seonggon Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: This paper addresses the challenge of model merging in quantized models for multi-target domain adaptation. It proposes HDRQ, a post-training quantization method that minimizes deviation from the pre-trained model and facilitates smooth model merging.


<details>
  <summary>Details</summary>
Motivation: Model merging is effective for multi-target domain adaptation, but its application to quantized models introduces new challenges due to restricted domain of interest and discretization effects.

Method: The authors propose HDRQ (Hessian and distant regularizing quantization), a novel post-training quantization method designed to consider model merging for multi-target domain adaptation. This approach aims to minimize deviation from the source pre-trained model while flattening the loss surface to enable smooth model merging.

Result: Extensive experiments demonstrate the effectiveness of HDRQ in achieving smooth model merging with minimal deviation from the pre-trained model in the context of quantized models for multi-target domain adaptation.

Conclusion: HDRQ is a successful solution for the challenge of model merging in quantized models, ensuring minimal deviation and facilitating smooth merging, marking it as the first study in this area.

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [182] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/abs/2505.23653)
*Jiaran Ye,Zijun Yao,Zhidian Huang,Liangming Pan,Jinxin Liu,Yushi Bai,Amy Xin,Liu Weichuan,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: Recent research shows LLMs can do multi-hop reasoning implicitly, but it's not well understood. This paper trains transformers from scratch in a controlled environment to study this phenomenon, revealing a three-stage developmental process and providing new insights into the interpretability of implicit multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: To better understand the poorly understood mechanisms behind implicit multi-hop reasoning in large language models.

Method: Training transformers from scratch in a controlled symbolic environment and analyzing their development trajectory through three stages: memorization, in-distribution generalization, and cross-distribution generalization. Using diagnostic tools like cross-query semantic patching and a cosine-based representational lens.

Result: Identified a three-stage developmental trajectory and found that atomic triples training accelerates learning while second-hop generalization depends on exposure to compositional structures. Successful reasoning correlates with clustering in hidden space.

Conclusion: The findings enhance the understanding of implicit multi-hop reasoning in LLMs, linking representational structure to reasoning capability and offering ways to increase model transparency.

Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [183] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/abs/2505.23663)
*Niklas Freymuth,Tobias Würth,Nicolas Schreiber,Balazs Gyenes,Andreas Boltres,Johannes Mitsch,Aleksandar Taranovic,Tai Hoang,Philipp Dahlinger,Philipp Becker,Luise Kärger,Gerhard Neumann*

Main category: cs.LG

TL;DR: AMBER uses supervised learning to adaptively mesh for simulating complex physical systems, outperforming recent baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve computational efficiency in simulating complex physical systems using FEM by developing a method that doesn't require task-specific heuristics or manual design.

Method: AMBER predicts the sizing field from a coarse mesh iteratively using a hierarchical graph neural network and generates new intermediate meshes with an out-of-the-box mesh generator, employing data augmentation by projecting expert labels onto AMBER-generated data during training.

Result: AMBER generalizes to unseen geometries and consistently outperforms multiple recent baselines, including ones using Graph and Convolutional Neural Networks, and Reinforcement Learning-based approaches.

Conclusion: AMBER is a successful supervised learning approach to mesh adaptation that improves computational efficiency without the need for task-specific heuristics or manual design.

Abstract: The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [184] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/abs/2505.23673)
*Aya Kayal,Sattar Vakili,Laura Toni,Da-shan Shiu,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 该论文通过改进的贝叶斯优化方法，利用偏好反馈模型（如BTL模型）实现更紧致的性能保证，并证明了在偏好反馈下达到最优解所需的样本复杂度与传统BO相当。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在基于偏好的反馈中受到关注，但现有方法在反馈模型下的性能保证不够理想，因此需要更精确的理论分析和性能提升。

Method: 作者采用Bradley-Terry-Luce (BTL)反馈模型，在同一框架下推导出更紧致的遗憾界，形式为$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$，其中$\Gamma(T)$表示最大信息增益，T为查询次数。

Result: 新方法显著改进了现有的遗憾界，并表明对于常见核函数，偏好反馈模型能达到与传统BO相同的样本复杂度，从而找到接近最优的解。

Conclusion: 论文提出了更紧致的性能保证，并证明了偏好反馈模型在样本复杂度上可以媲美传统BO方法，提升了基于人类反馈的贝叶斯优化效率。

Abstract: Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [185] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/abs/2505.23681)
*Bo Zhao,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TL;DR: 神经网络极小值通常通过模式连接性（mode connectivity）相连，即存在保持训练和测试损失几乎不变的曲线。本文提出了一种基于参数空间对称性的新方法来研究极小值的连通性，并探讨了模式连接性和线性模式连接性成立或失败的条件，揭示了连续对称性在理解神经网络损失景观中的重要作用。


<details>
  <summary>Details</summary>
Motivation: 尽管模式连接性（mode connectivity）已被应用于模型合并和微调等任务，但其理论解释尚不明确。因此需要一种新的方法来探索极小值的连通性。

Method: 本文提出利用参数空间对称性来探索极小值的连通性。具体来说，将对称群的拓扑结构与极小值的拓扑结构联系起来，推导出线性网络极小值的连通分量数量，并分析跳过连接如何减少该数量。接着，使用参数对称性来研究模式连接性和线性模式连接性何时成立或失败，并提供由对称性诱导的极小值连接曲线的显式表达式。最后，通过这些曲线的曲率，推导出线性模式连接性近似成立的条件。

Result: 本文推导了线性网络极小值的连通分量数量，并展示了跳过连接如何减少这一数量。此外，明确了模式连接性和线性模式连接性成立或失败的条件，并提供了连接曲线的显式表达式及线性模式连接性近似成立的条件。

Conclusion: 本文的研究结果强调了连续对称性在理解神经网络损失景观中的关键作用。

Abstract: Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [186] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/abs/2505.23683)
*Zixuan Wang,Eshaan Nichani,Alberto Bietti,Alex Damian,Daniel Hsu,Jason D. Lee,Denny Wu*

Main category: cs.LG

TL;DR: Transformer-based language models' ability to perform multi-step reasoning tasks is explored, focusing on the learnability of k-fold composition tasks.


<details>
  <summary>Details</summary>
Motivation: To understand under what conditions transformers can efficiently learn complex compositional tasks via gradient-based optimization.

Method: Theoretical analysis of the k-fold composition task using transformers with O(log k) layers. Proving a Statistical Query lower bound and demonstrating efficient learning through gradient descent with curriculum learning strategies.

Result: A statistical-computational gap is established via SQ lower bound. Efficient learning is possible with polynomial runtime and sample complexity in k using curriculum learning.

Conclusion: Both easy and hard examples are necessary and sufficient for transformers to learn complex compositional tasks.

Abstract: Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [187] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/abs/2505.23696)
*Hiroshi Kera,Nico Pelleriti,Yuki Ishihara,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Oracle Border Basis Algorithm的深度学习方法，能够加速Border basis计算，同时保证输出正确性。通过设计基于Transformer的oracle来识别和消除计算成本高的约简步骤，从而在关键计算阶段实现高达3.5倍的速度提升。此外，还提出了首个适用于Border bases的采样定理以及针对单项式代数计算的标记化和嵌入方案，减少了编码多项式的token数量，并提高了数据效率、稳定性和实用性。


<details>
  <summary>Details</summary>
Motivation: 求解具有有限多个解的多项式方程组是许多科学领域中的重要挑战。传统方法（如Gröbner基和Border基）虽然基础但计算成本高，而现有的深度学习方法虽提高了效率却牺牲了输出正确性。因此需要一种既能加速计算又保证结果正确的新型方法。

Method: 研究人员引入了Oracle Border Basis Algorithm，这是一种结合深度学习的算法。该方法设计了一个基于Transformer的oracle，用于识别和消除计算密集型的约简步骤；开发了一种采样方法并提供了首个Border bases采样定理；构建了一个专门针对单项式代数计算的标记化和嵌入方案，从而显著减少表示多项式所需的tokens数量。

Result: 与基础算法相比，新方法在不损害结果正确性的前提下实现了最高3.5倍的速度提升。同时，生成的训练数据更紧凑且表达力更强，使得学习方法更加数据高效、稳定，并为传统的计算机代数算法和符号计算提供了实际改进。

Conclusion: Oracle Border Basis Algorithm是一种创新的深度学习方法，成功地加速了Border basis计算，同时保持了输出的正确性。这一成果不仅提升了多项式方程求解的效率，还为计算机代数系统和符号计算领域提供了新的可能性。

Abstract: Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [188] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/abs/2505.23700)
*Oleksii Furman,Ulvi Movsum-zada,Patryk Marszalek,Maciej Zięba,Marek Śmieja*

Main category: cs.LG

TL;DR: The paper introduces DiCoFlex, a model-agnostic framework that generates multiple counterfactual explanations in one pass using conditional normalizing flows, allowing real-time customization and outperforming existing methods in various metrics.


<details>
  <summary>Details</summary>
Motivation: Current counterfactual generation methods have limitations such as requiring constant model access, being computationally expensive, and lacking flexibility to adapt to new constraints without retraining.

Method: DiCoFlex is a conditional generative framework leveraging conditional normalizing flows trained on labeled data to produce diverse counterfactuals in a single forward pass with real-time user-defined constraints.

Result: Experiments on benchmark datasets demonstrate DiCoFlex's superior performance in validity, diversity, proximity, and constraint adherence compared to existing methods.

Conclusion: DiCoFlex provides a practical and scalable solution for generating counterfactual explanations in sensitive decision-making domains.

Abstract: Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [189] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/abs/2505.23702)
*Nathan Lichtlé,Alexi Canesse,Zhe Fu,Hossein Nick Zinat Matin,Maria Laura Delle Monache,Alexandre M. Bayen*

Main category: cs.LG

TL;DR: The paper introduces (U)NFV, a neural network architecture that generalizes finite volume methods for solving hyperbolic conservation laws with lower error and higher fidelity than traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Hyperbolic PDEs are difficult to solve due to shocks and discontinuities in solutions. Classical finite volume methods have convergence properties but lack accuracy and flexibility in complex settings.

Method: (U)NFV is a modular neural network architecture that learns update rules while preserving conservation structure, supporting both supervised (NFV) and unsupervised (UNFV) training.

Result: (U)NFV achieves up to 10x lower error than Godunov's method, outperforms ENO/WENO schemes, rivals discontinuous Galerkin solvers with less complexity, and excels in traffic modeling problems.

Conclusion: (U)NFV provides an accurate and flexible approach for solving hyperbolic conservation laws, offering significant improvements over classical methods.

Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [190] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/abs/2505.23705)
*Danny Driess,Jost Tobias Springenberg,Brian Ichter,Lili Yu,Adrian Li-Bell,Karl Pertsch,Allen Z. Ren,Homer Walke,Quan Vuong,Lucy Xiaoyang Shi,Sergey Levine*

Main category: cs.LG

TL;DR: Vision-language-action (VLA) models are great for training robot control policies but face challenges with real-time inference and continuous control due to the complexity of vision-language models (VLMs). This paper explores how adding specialized modules affects the semantic knowledge of pretrained VLMs in VLAs, identifies issues with naive inclusion of action experts, and proposes a method called 'knowledge insulation' to address these problems.


<details>
  <summary>Details</summary>
Motivation: To understand whether specialized modules like action experts or continuous output heads in VLA models preserve or degrade the semantic knowledge from pretrained VLMs, and how they affect the VLA training dynamics.

Method: Analyze various design choices in VLA models incorporating continuous diffusion or flow matching action experts. Propose a technique named 'knowledge insulation' to protect the VLM backbone during VLA training.

Result: Naively including action experts significantly harms both training speed and knowledge transfer. The proposed 'knowledge insulation' technique mitigates this issue.

Conclusion: Adding specialized modules can harm the performance and knowledge transfer in VLA models. Using 'knowledge insulation' can help preserve the semantic knowledge from pretrained VLMs while enabling efficient continuous control.

Abstract: Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [191] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/abs/2505.23719)
*Andreas Auer,Patrick Podest,Daniel Klotz,Sebastian Böck,Günter Klambauer,Sepp Hochreiter*

Main category: cs.LG

TL;DR: A new model TiRex based on enhanced LSTM (xLSTM) is introduced for zero-shot time series forecasting, featuring competitive in-context learning abilities and superior state-tracking. It introduces a masking strategy CPM and achieves state-of-the-art performance on benchmarks GiftEval and Chronos-ZS.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing zero-shot forecasting methods which rely heavily on transformer architectures that underperform compared to LSTMs in time series forecasting. Also, to enhance the in-context learning ability of LSTMs while maintaining their strong state-tracking capabilities.

Method: The method involves creating TiRex, an xLSTM-based model with improved in-context learning skills. A training-time masking strategy called CPM is also proposed to further support its state-tracking ability.

Result: TiRex outperforms significantly larger models such as TabPFN-TS, Chronos Bolt, TimesFM, and Moirai across both short- and long-term forecasts on HuggingFace benchmarks GiftEval and Chronos-ZS.

Conclusion: TiRex sets a new state of the art in zero-shot time series forecasting by combining competitive in-context learning and strong state-tracking, making it an effective tool for scenarios where training data are scarce.

Abstract: In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [192] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/abs/2505.23720)
*Arun Verma,Indrajit Saha,Makoto Yokoo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: In this paper, the authors tackle a contextual bandit problem with multiple agents who may misreport information for personal gain. They propose COBRA, an algorithm that discourages strategic behavior without monetary incentives, ensuring incentive compatibility and sub-linear regret.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitation of existing contextual bandit algorithms which assume truthful reporting by agents. In real-life scenarios, such as online platforms with multiple sellers, some agents may misrepresent information to gain an advantage.

Method: The proposed method is an algorithm called COBRA designed for contextual bandit problems involving strategic agents. COBRA disincentivizes strategic behavior without using monetary incentives while maintaining incentive compatibility and providing a sub-linear regret guarantee.

Result: Experimental results validate various performance aspects of the COBRA algorithm.

Conclusion: COBRA effectively addresses the issue of strategic behavior in multi-agent contextual bandit problems without relying on monetary incentives.

Abstract: This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [193] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/abs/2505.23721)
*Sean Current,Ziqi Chen,Daniel Adu-Ampratwum,Xia Ning,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: DiffER是一种新的非模板方法，使用分类扩散模型进行化学 retrosynthesis 预测，可同时预测整个输出SMILES序列，达到一流的top-1准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于transformer神经网络的自动化学retrosynthesis方法受其自回归性质的限制。需要一种新方法来克服这些限制并提高预测性能。

Method: 提出了一种名为DiffER的非模板方法，通过分类扩散模型进行retrosynthesis预测，能够同时预测整个输出SMILES序列。构建了一个带有新颖长度预测组件的分类扩散模型集成，可以近似从反应物的后验分布中采样。

Result: 在无模板方法中，DiffER在top-1准确率上达到了最先进的性能，并且在top-3、top-5和top-10准确率上表现具有竞争力。准确预测SMILES序列长度对于提升模型性能至关重要。

Conclusion: DiffER为一类新的无模板模型提供了强大的基准，能够学习各种实验室中使用的合成技术，并在top-k准确率指标上优于其他无模板方法。

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [194] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724)
*Minrui Luo,Fuhang Kuang,Yu Wang,Zirui Liu,Tianxing He*

Main category: cs.LG

TL;DR: SC-LoRA是一种改进的LoRA初始化框架，通过低秩子空间约束提升微调效率并减少知识遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法在微调大语言模型时存在收敛速度慢和知识遗忘问题，而现有研究无法同时解决这两个问题。

Method: 提出了一种名为Subspace-Constrained LoRA (SC-LoRA) 的新框架，通过在低秩子空间中约束可训练LoRA适配器的输出，在高效微调和知识保留之间取得平衡。该方法使可训练权重专注于微调数据的主要特征，同时避免损害保留的知识特征。

Result: 实验表明，SC-LoRA在各种下游任务中表现出色，显著减少了知识遗忘，并超越了当前的LoRA初始化方法。

Conclusion: SC-LoRA为解决微调效率和知识保留之间的权衡提供了一种有效的方法，具有广泛的适用性和优越性能。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [195] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/abs/2505.23725)
*Benjamin Thérien,Xiaolong Huang,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: DiLoCo框架通过减少通信频率来提高数据中心环境中的并行性和加速器利用率，但其通信步骤仍涉及对模型参数的完整副本进行全归约。本文研究了在DiLoCo中使用Top-k稀疏化和量化等标准压缩方法的效果，并发现使用Muon作为内部优化器结合误差反馈累加器可以将通信的delta压缩到2位而几乎没有性能下降。MuLoCo（使用Muon优化器的DiLoCo）比DiLoCo表现更好，通信量减少了8倍且内存复杂度相同。


<details>
  <summary>Details</summary>
Motivation: 尽管DiLoCo框架能够显著减少通信频率，但在通信步骤中仍然需要对模型参数的完整副本进行全归约。为了进一步减少通信开销，需要探索压缩方法以及不同局部优化器的作用。

Method: 研究了Top-k稀疏化和量化等标准压缩方法与两种局部优化器（AdamW和Muon）在DiLoCo框架中的效果。实验基于预训练解码器仅变压器语言模型。

Result: 使用Muon作为内部优化器并结合误差反馈累加器时，可以将通信的delta压缩至2位而几乎不降低性能。此外，MuLoCo相比DiLoCo表现出更优的性能，同时通信量减少8倍且内存复杂度保持不变。

Conclusion: Muon作为内部优化器结合误差反馈累加器可以显著减少DiLoCo框架中的通信开销而不影响性能，MuLoCo相较于DiLoCo具有明显优势。

Abstract: DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [196] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/abs/2505.23732)
*Shreeram Suresh Chandra,Lucas Goncalves,Junchen Lu,Carlos Busso,Berrak Sisman*

Main category: cs.LG

TL;DR: EmotionRankCLAP is a supervised contrastive learning approach that captures fine-grained emotion variations and improves cross-modal alignment using dimensional attributes of emotional speech and natural language prompts.


<details>
  <summary>Details</summary>
Motivation: Current emotion-based CLAP methods fail to capture the ordinal nature of emotions, leading to insufficient inter-emotion understanding and a wide modality gap between audio and text embeddings.

Method: EmotionRankCLAP uses dimensional attributes of emotional speech and natural language prompts with a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space.

Result: EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities as measured by a cross-modal retrieval task.

Conclusion: The supervised contrastive learning approach effectively captures fine-grained emotion variations and enhances cross-modal alignment.

Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [197] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)
*Paul Gölz,Nika Haghtalab,Kunhe Yang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [198] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher,David M. Chan,Yutong Bai,Trevor Darrell,Ritwik Gupta*

Main category: cs.LG

TL;DR: Sequence models like transformers usually flatten images into one-dimensional sequences using a fixed row-major order. However, modern transformers increasingly rely on architectural approximations that break permutation-equivariance, making patch order significantly affect model performance. The authors propose REOrder, a two-stage framework for discovering task-optimal patch orderings, which improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.


<details>
  <summary>Details</summary>
Motivation: Modern long-sequence transformers rely on architectural approximations that break permutation-equivariance, causing patch order to significantly affect model performance.

Method: REOrder is a two-stage framework for discovering task-optimal patch orderings. First, an information-theoretic prior is derived by evaluating the compressibility of various patch sequences. Then, a policy over permutations is learned by optimizing a Plackett-Luce policy using REINFORCE.

Result: REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.

Conclusion: The authors conclude that patch order significantly affects model performance in settings where modern transformers rely on architectural approximations that break permutation-equivariance. They also demonstrate that their proposed method, REOrder, can improve accuracy by discovering task-optimal patch orderings.

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [199] [Model Immunization from a Condition Number Perspective](https://arxiv.org/abs/2505.23760)
*Amber Yijia Zheng,Cedar Site Bai,Brian Bullins,Raymond A. Yeh*

Main category: cs.LG

TL;DR: The paper proposes a framework for model immunization using the condition number of a Hessian matrix, designs an algorithm with regularization terms to control condition numbers post pre-training, and demonstrates its effectiveness on linear models and non-linear deep-nets.


<details>
  <summary>Details</summary>
Motivation: Model immunization is important to ensure that models are difficult to fine-tune on harmful tasks while still being useful for non-harmful tasks. However, there is lack of understanding on when immunization is possible and what constitutes an immunized model.

Method: The authors propose a framework based on the condition number of a Hessian matrix to analyze model immunization for linear models. They design an algorithm with regularization terms to control the resulting condition numbers after pre-training.

Result: Empirical results show the effectiveness of the proposed algorithm on model immunization for both linear models and non-linear deep-nets.

Conclusion: The proposed framework and algorithm provide insights into model immunization and demonstrate their effectiveness across different types of models.

Abstract: Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


### [200] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won,Hyunji Lee,Hyeonbin Hwang,Minjoon Seo*

Main category: cs.LG

TL;DR: Direct Preference Optimization (DPO) uses Differential Information Distribution (DID) to optimize language models based on human preferences. Theoretical analysis reveals that log-ratio reward is uniquely optimal under certain conditions, and differential information entropy affects policy distribution reinforcement or smoothing.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for the log-ratio reward parameterization in Direct Preference Optimization (DPO), which currently lacks a complete theoretical justification despite its empirical success.

Method: Utilize Differential Information Distribution (DID) to analyze the information gained during policy updates. Show that when preference labels encode the differential information needed to transform a reference policy into a target policy, the log-ratio reward in DPO is the uniquely optimal form. Investigate the link between preferences encoding differential information and the implicit assumption of log-margin ordered policies. Analyze the effect of differential information entropy on policy distribution.

Result: Demonstrated that the log-ratio reward in DPO is uniquely optimal for learning the target policy via preference optimization. Identified the connection between preferences encoding differential information and the implicit assumption of log-margin ordered policies. Characterized how low-entropy differential information reinforces policy distribution while high-entropy differential information induces a smoothing effect.

Conclusion: Presented a unifying perspective on DPO, showing how differential information theory can explain the structure of preference data and resulting policy behaviors. Learning high-entropy differential information is important for general instruction-following, while low-entropy benefits knowledge-intensive question answering.

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [201] [TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE](https://arxiv.org/abs/2505.22735)
*Tong Sun,Bowen Jiang,Hailong Lin,Borui Li,Yixiao Teng,Yi Gao,Wei Dong*

Main category: cs.CR

TL;DR: TensorShield is an efficient on-device inference method that partially shields model tensors to defend against model stealing and membership inference attacks while maintaining low latency and accuracy.


<details>
  <summary>Details</summary>
Motivation: On-device inference has become crucial for safeguarding user data privacy on mobile and IoT devices, but existing methods using Trusted Execution Environments (TEEs) face challenges with secure memory capacity and inference latency.

Method: TensorShield uses a novel eXplainable AI (XAI) technique to assess critical tensors and shield them in TEEs. It also includes two designs for critical feature identification and latency-aware placement to accelerate inference while maintaining security.

Result: Evaluations show TensorShield provides similar security protection as shielding the entire model in TEE, but is up to 25.35$\times$ faster (avg. 5.85$\times$) than state-of-the-art methods without any accuracy loss.

Conclusion: TensorShield is the first work to efficiently achieve partial tensor shielding for on-device inference, effectively defending against model stealing and membership inference attacks.

Abstract: To safeguard user data privacy, on-device inference has emerged as a
prominent paradigm on mobile and Internet of Things (IoT) devices. This
paradigm involves deploying a model provided by a third party on local devices
to perform inference tasks. However, it exposes the private model to two
primary security threats: model stealing (MS) and membership inference attacks
(MIA). To mitigate these risks, existing wisdom deploys models within Trusted
Execution Environments (TEEs), which is a secure isolated execution space.
Nonetheless, the constrained secure memory capacity in TEEs makes it
challenging to achieve full model security with low inference latency. This
paper fills the gap with TensorShield, the first efficient on-device inference
work that shields partial tensors of the model while still fully defending
against MS and MIA. The key enabling techniques in TensorShield include: (i) a
novel eXplainable AI (XAI) technique exploits the model's attention transition
to assess critical tensors and shields them in TEE to achieve secure inference,
and (ii) two meticulous designs with critical feature identification and
latency-aware placement to accelerate inference while maintaining security.
Extensive evaluations show that TensorShield delivers almost the same security
protection as shielding the entire model inside TEE, while being up to
25.35$\times$ (avg. 5.85$\times$) faster than the state-of-the-art work,
without accuracy loss.

</details>


### [202] [Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?](https://arxiv.org/abs/2505.22843)
*Alexander Herzog,Aliai Eusebi,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: The paper questions the operational reliability of modern drift-adaptive malware classifiers despite their promising performance figures. It proposes AURORA, a framework to evaluate these classifiers based on confidence quality and operational resilience, revealing fragility in state-of-the-art frameworks.


<details>
  <summary>Details</summary>
Motivation: To investigate whether malware classifiers maintain reliable confidence estimates under distribution shifts and explore the tensions between scientific advancement and practical impacts when they do not.

Method: Propose AURORA, a framework that evaluates malware classifiers by subjecting the confidence profile of a given model to verification to assess the reliability of its estimates. Also introduce a set of metrics designed for holistic assessment of operational stability throughout temporal evaluation periods.

Result: Observed fragility in state-of-the-art frameworks across datasets of varying drift severity.

Conclusion: There is a need to revisit and rethink the current approaches in developing malware classifiers due to observed fragility.

Abstract: The performance figures of modern drift-adaptive malware classifiers appear
promising, but does this translate to genuine operational reliability? The
standard evaluation paradigm primarily focuses on baseline performance metrics,
neglecting confidence-error alignment and operational stability. While
TESSERACT established the importance of temporal evaluation, we take a
complementary direction by investigating whether malware classifiers maintain
reliable confidence estimates under distribution shifts and exploring the
tensions between scientific advancement and practical impacts when they do not.
We propose AURORA, a framework to evaluate malware classifiers based on their
confidence quality and operational resilience. AURORA subjects the confidence
profile of a given model to verification to assess the reliability of its
estimates. Unreliable confidence estimates erode operational trust, waste
valuable annotation budget on non-informative samples for active learning, and
leave error-prone instances undetected in selective classification. AURORA is
further complemented by a set of metrics designed to go beyond point-in-time
performance, striving towards a more holistic assessment of operational
stability throughout temporal evaluation periods. The fragility we observe in
state-of-the-art frameworks across datasets of varying drift severity suggests
the need for a return to the whiteboard.

</details>


### [203] [Security Benefits and Side Effects of Labeling AI-Generated Images](https://arxiv.org/abs/2505.22845)
*Sandra Höltervennhoff,Jonas Ricker,Maike M. Raphael,Charlotte Schwedes,Rebecca Weil,Asja Fischer,Thorsten Holz,Lea Schönherr,Sascha Fahl*

Main category: cs.CR

TL;DR: 生成式人工智能快速发展，影响了人类与信息和数字媒体的互动。虽然法规要求披露AI生成的内容，但关于这些标签是否能减少AI生成的错误信息的风险知之甚少。本研究通过焦点小组和调查，探讨了AI生成图像的标签对用户识别错误信息的能力的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管法规要求披露AI生成的内容，但关于这些标签是否能减少AI生成的错误信息的风险知之甚少。

Method: 1. 通过五个焦点小组定性探索用户对AI标签的意见和期望；2. 进行一项预先注册的在线调查，有超过1300名美国和欧盟参与者，定量评估AI标签对用户识别错误信息的能力的影响。

Result: 1. 焦点小组显示参与者认为标签有助于识别AI生成的图像并避免欺骗，但也有关于实际实施的担忧；2. 调查显示用户可能会过度依赖标签；3. 带有标签的不准确声明被认为不如没有标签的AI图像可信，但准确声明的信任度也会降低；4. 标签的存在使得传达不准确声明的人工制作图像被认为更可信。

Conclusion: AI生成图像的标签可能有助于识别错误信息，但存在用户过度依赖和降低准确声明信任度的风险，同时可能增加人工制作图像错误信息的可信度。

Abstract: Generative artificial intelligence is developing rapidly, impacting humans'
interaction with information and digital media. It is increasingly used to
create deceptively realistic misinformation, so lawmakers have imposed
regulations requiring the disclosure of AI-generated content. However, only
little is known about whether these labels reduce the risks of AI-generated
misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we
study the implications of labels, including the possibility of mislabeling.
Assuming that simplicity, transparency, and trust are likely to impact the
successful adoption of such labels, we first qualitatively explore users'
opinions and expectations of AI labeling using five focus groups. Second, we
conduct a pre-registered online survey with over 1300 U.S. and EU participants
to quantitatively assess the effect of AI labels on users' ability to recognize
misinformation containing either human-made or AI-generated images. Our focus
groups illustrate that, while participants have concerns about the practical
implementation of labeling, they consider it helpful in identifying
AI-generated images and avoiding deception. However, considering security
benefits, our survey revealed an ambiguous picture, suggesting that users might
over-rely on labels. While inaccurate claims supported by labeled AI-generated
images were rated less credible than those with unlabeled AI-images, the belief
in accurate claims also decreased when accompanied by a labeled AI-generated
image. Moreover, we find the undesired side effect that human-made images
conveying inaccurate claims were perceived as more credible in the presence of
labels.

</details>


### [204] [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852)
*Krti Tallam,Emma Miller*

Main category: cs.CR

TL;DR: CaMeL introduces a capability-based sandbox to mitigate prompt injection attacks in LLM agents, but it assumes trusted user prompts and has performance tradeoffs. This response proposes improvements including prompt screening, output auditing, tiered-risk access model, and verified intermediate language.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of CaMeL such as assuming trusted user prompt, omitting side-channel concerns, and having performance tradeoffs due to its dual-LLM design.

Method: Proposing engineering improvements that include prompt screening for initial inputs, output auditing to detect instruction leakage, a tiered-risk access model for balancing usability and control, and a verified intermediate language for formal guarantees.

Result: These upgrades align CaMeL with best practices in enterprise security and support scalable deployment.

Conclusion: The proposed enhancements expand CaMeL's threat coverage and operational usability.

Abstract: CaMeL (Capabilities for Machine Learning) introduces a capability-based
sandbox to mitigate prompt injection attacks in large language model (LLM)
agents. While effective, CaMeL assumes a trusted user prompt, omits
side-channel concerns, and incurs performance tradeoffs due to its dual-LLM
design. This response identifies these issues and proposes engineering
improvements to expand CaMeL's threat coverage and operational usability. We
introduce: (1) prompt screening for initial inputs, (2) output auditing to
detect instruction leakage, (3) a tiered-risk access model to balance usability
and control, and (4) a verified intermediate language for formal guarantees.
Together, these upgrades align CaMeL with best practices in enterprise security
and support scalable deployment.

</details>


### [205] [Permissioned LLMs: Enforcing Access Control in Large Language Models](https://arxiv.org/abs/2505.22860)
*Bargav Jayaraman,Virendra J. Marathe,Hamid Mozaffari,William F. Shen,Krishnaram Kenthapadi*

Main category: cs.CR

TL;DR: 在企业环境中，为了防止细粒度数据访问控制被破坏，本文提出了许可大型语言模型（PermLLM），它可以在生成的查询响应中叠加组织数据访问控制结构。同时引入了相关响应的概念和访问优势指标来证明和评估PermLLM机制的有效性，并通过实验验证了其在多个公开数据集上的效果。


<details>
  <summary>Details</summary>
Motivation: 在企业环境中，组织数据被分割、孤立并受到复杂的访问控制框架保护。然而，如果一个在孤立数据上微调的大型语言模型（LLM）为具有不同访问权限的个人提供下游任务请求服务，这些访问控制结构可能会完全崩溃。

Method: 1. 提出了一个新的LLM类别：Permissioned LLMs (PermLLM)，将组织数据访问控制结构叠加到生成的查询响应上。
2. 提出了相关响应的概念，用于证明PermLLM机制是否正确实现。
3. 引入了一个新的指标：访问优势（access advantage），以经验方式评估PermLLM机制的效果。
4. 基于Parameter Efficient Fine-Tuning提出了三种新颖的PermLLM机制。
5. 提出了两种访问优势的具体实例：领域可区分性指数（DDI）和效用差距指数（UGI）。

Result: 通过广泛的实验，在四个公共数据集（GPQA、RCV1、SimpleQA和WMDP）上展示了PermLLM机制的有效性。此外，还验证了DDI和UGI指标本身在量化LLM访问控制方面的有效性。

Conclusion: PermLLM是一种有效的解决方案，能够在企业环境下维持细粒度的数据访问控制，同时保持LLM的功能性和实用性。所提出的访问优势指标及其具体实例能够很好地评估PermLLM机制的性能。

Abstract: In enterprise settings, organizational data is segregated, siloed and
carefully protected by elaborate access control frameworks. These access
control structures can completely break down if an LLM fine-tuned on the siloed
data serves requests, for downstream tasks, from individuals with disparate
access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs
that superimpose the organizational data access control structures on query
responses they generate. We formalize abstractions underpinning the means to
determine whether access control enforcement happens correctly over LLM query
responses. Our formalism introduces the notion of a relevant response that can
be used to prove whether a PermLLM mechanism has been implemented correctly. We
also introduce a novel metric, called access advantage, to empirically evaluate
the efficacy of a PermLLM mechanism. We introduce three novel PermLLM
mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired
access control. We furthermore present two instantiations of access
advantage--(i) Domain Distinguishability Index (DDI) based on Membership
Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility
evaluation. We demonstrate the efficacy of our PermLLM mechanisms through
extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),
in addition to evaluating the validity of DDI and UGI metrics themselves for
quantifying access control in LLMs.

</details>


### [206] [BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection](https://arxiv.org/abs/2505.22878)
*Shams Tarek,Dipayan Saha,Sujan Kumar Saha,Farimah Farahmandi*

Main category: cs.CR

TL;DR: The paper presents BugWhisperer, a framework using a fine-tuned LLM for automating SoC security verification, improving efficiency and flexibility, supported by a new hardware vulnerability database.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of manual, labor-intensive, and inflexible methodologies in SoC security verification that limit scalability and effectiveness.

Method: Propose BugWhisperer framework with a specialized, fine-tuned LLM for hardware security knowledge to automate, improve adaptability and reusability of verification process, alongside an open-source LLM and comprehensive hardware vulnerability database.

Result: Demonstrates enhanced efficiency and flexibility in security verification process through the tailored LLM.

Conclusion: BugWhisperer effectively addresses current challenges in SoC security verification, offering improved automation, adaptability, and resources like a hardware vulnerability database to assist the research community.

Abstract: The current landscape of system-on-chips (SoCs) security verification faces
challenges due to manual, labor-intensive, and inflexible methodologies. These
issues limit the scalability and effectiveness of security protocols, making
bug detection at the Register-Transfer Level (RTL) difficult. This paper
proposes a new framework named BugWhisperer that utilizes a specialized,
fine-tuned Large Language Model (LLM) to address these challenges. By enhancing
the LLM's hardware security knowledge and leveraging its capabilities for text
inference and knowledge transfer, this approach automates and improves the
adaptability and reusability of the verification process. We introduce an
open-source, fine-tuned LLM specifically designed for detecting security
vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM
effectively enhances the efficiency and flexibility of the security
verification process. Additionally, we introduce a comprehensive hardware
vulnerability database that supports this work and will further assist the
research community in enhancing the security verification process.

</details>


### [207] [Chainless Apps: A Modular Framework for Building Apps with Web2 Capability and Web3 Trust](https://arxiv.org/abs/2505.22989)
*Brian Seong,Paul Gebheim*

Main category: cs.CR

TL;DR: Chainless Apps提出了一种新的应用架构范式，通过分离执行、信任、桥接和结算层，实现特定于应用程序的排序、可验证的链下计算、通过Agglayer进行链无关的资产和消息路由，并在以太坊上实现最终性，从而提供快速的Web2-like用户体验和Web3级别的可验证性。


<details>
  <summary>Details</summary>
Motivation: 现代区块链应用常常受限于用户体验与信任之间的权衡，因此需要一种新的架构来解决这一问题。

Method: 将执行、信任、桥接和结算分为不同的可组合层；利用zkVMs和去中心化验证服务提供的新信任模型；通过Agglayer实现链无关的资产和消息路由；在以太坊上实现最终性。

Result: 实现了快速的Web2-like用户体验，同时保持Web3级别的可验证性和与更广泛的区块链生态系统的互操作性。

Conclusion: Chainless Apps提供了一种模块化、可扩展的应用程序架构，允许领域特定的权衡，同时保持与区块链生态系统的互操作性。

Abstract: Modern blockchain applications are often constrained by a trade-off between
user experience and trust. Chainless Apps present a new paradigm of application
architecture that separates execution, trust, bridging, and settlement into
distinct compostable layers. This enables app-specific sequencing, verifiable
off-chain computation, chain-agnostic asset and message routing via Agglayer,
and finality on Ethereum - resulting in fast Web2-like UX with Web3-grade
verifiability. Although consensus mechanisms have historically underpinned
verifiable computation, the advent of zkVMs and decentralized validation
services opens up new trust models for developers. Chainless Apps leverage this
evolution to offer modular, scalable applications that maintain
interoperability with the broader blockchain ecosystem while allowing
domain-specific trade-offs.

</details>


### [208] [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
*Jinchuan Zhang,Lu Yin,Yan Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: The acquisition of agentic capabilities has transformed LLMs from 'knowledge providers' to 'action executors', increasing their susceptibility to malicious use. To address the deficiency in agentic use safety alignment during the post-training phase, the paper proposes AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents execute numerous malicious tasks even without being attacked, indicating a deficiency in agentic use safety alignment during the post-training phase.

Method: AgentAlign, a novel framework that leverages abstract behavior chains as a medium for safety alignment data synthesis. By instantiating these behavior chains in simulated environments with diverse tool instances, the framework enables the generation of highly authentic and executable instructions while capturing complex multi-step dynamics. It further ensures model utility by proportionally synthesizing benign instructions through non-malicious interpretations of behavior chains.

Result: Evaluation results on AgentHarm demonstrate that fine-tuning three families of open-source models using this method substantially improves their safety (35.8% to 79.5% improvement) while minimally impacting or even positively enhancing their helpfulness, outperforming various prompting methods.

Conclusion: AgentAlign can improve the safety of LLM-based agents while maintaining or enhancing their helpfulness.

Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge
providers" to "action executors", a trend that while expanding LLMs' capability
boundaries, significantly increases their susceptibility to malicious use.
Previous work has shown that current LLM-based agents execute numerous
malicious tasks even without being attacked, indicating a deficiency in agentic
use safety alignment during the post-training phase. To address this gap, we
propose AgentAlign, a novel framework that leverages abstract behavior chains
as a medium for safety alignment data synthesis. By instantiating these
behavior chains in simulated environments with diverse tool instances, our
framework enables the generation of highly authentic and executable
instructions while capturing complex multi-step dynamics. The framework further
ensures model utility by proportionally synthesizing benign instructions
through non-malicious interpretations of behavior chains, precisely calibrating
the boundary between helpfulness and harmlessness. Evaluation results on
AgentHarm demonstrate that fine-tuning three families of open-source models
using our method substantially improves their safety (35.8% to 79.5%
improvement) while minimally impacting or even positively enhancing their
helpfulness, outperforming various prompting methods. The dataset and code have
both been open-sourced.

</details>


### [209] [Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion](https://arxiv.org/abs/2505.23266)
*Chunlong Xie,Jialing He,Shangwei Guo,Jiacheng Wang,Shudong Zhang,Tianwei Zhang,Tao Xiang*

Main category: cs.CR

TL;DR: This paper introduces Adversarial Object Fusion (AdvOF), a new framework that generates adversarial 3D objects to attack vision-and-language navigation (VLN) agents in service-oriented environments. AdvOF can degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks.


<details>
  <summary>Details</summary>
Motivation: To explore the vulnerabilities introduced by integrating foundational models like LLMs and VLMs into service-oriented navigation systems, particularly focusing on the impact of adversarial environments on the VLM-based perception module of VLN agents.

Method: AdvOF first aggregates and aligns victim object positions in both 2D and 3D space to define and render adversarial objects. Then, it optimizes the adversarial object with regularization between the adversarial and victim object across physical properties and VLM perceptions, using importance weights for varying views and iterative fusions from local updates and justifications.

Result: Extensive evaluations show that AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks.

Conclusion: This work enhances the understanding of service security in VLM-powered navigation systems and provides computational foundations for robust service composition in physical-world deployments.

Abstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework
targeting vision-and-language navigation (VLN) agents in service-oriented
environments by generating adversarial 3D objects. While foundational models
like Large Language Models (LLMs) and Vision Language Models (VLMs) have
enhanced service-oriented navigation systems through improved perception and
decision-making, their integration introduces vulnerabilities in
mission-critical service workflows. Existing adversarial attacks fail to
address service computing contexts, where reliability and quality-of-service
(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of
adversarial environments on the VLM-based perception module of VLN agents. In
particular, AdvOF first precisely aggregates and aligns the victim object
positions in both 2D and 3D space, defining and rendering adversarial objects.
Then, we collaboratively optimize the adversarial object with regularization
between the adversarial and victim object across physical properties and VLM
perceptions. Through assigning importance weights to varying views, the
optimization is processed stably and multi-viewedly by iterative fusions from
local updates and justifications. Our extensive evaluations demonstrate AdvOF
can effectively degrade agent performance under adversarial conditions while
maintaining minimal interference with normal navigation tasks. This work
advances the understanding of service security in VLM-powered navigation
systems, providing computational foundations for robust service composition in
physical-world deployments.

</details>


### [210] [Joint Data Hiding and Partial Encryption of Compressive Sensed Streams](https://arxiv.org/abs/2505.23357)
*Cristina-Elena Popa,Cristian Damian,Daniela Coltuc*

Main category: cs.CR

TL;DR: The paper proposes a method to secure Compressive Sensing (CS) streams by protecting part of the measurements with a secret key and inserting the code into the rest, using a reversible data hiding scheme. On-the-fly insertion is suitable for sequential acquisition by a Single Pixel Camera.


<details>
  <summary>Details</summary>
Motivation: To secure the Compressive Sensing (CS) streams, particularly focusing on protecting part of the measurements and embedding information within the rest of the stream.

Method: Protection of part of the CS measurements with a secret key generated via a cryptographically secure pseudo-random number generator (CSPRNG), then XORed with the measurements. A modified reversible data hiding (RDH) scheme is used for insertion, which is based on a prediction error expansion algorithm tailored to match the statistics of CS measurements.

Result: Reconstruction from the embedded stream leads to visibly distorted images, with distortion controlled by the number of embedded levels. In tests, embedding on 10 levels results in approximately 18 dB distortion for 256x256 pixel images reconstructed with FISTA. The method supports on-the-fly insertion, making it suitable for sequential acquisition by a Single Pixel Camera.

Conclusion: The proposed method successfully secures CS streams through partial measurement protection and reversible data hiding, with controllable image distortion and suitability for on-the-fly insertion without buffering.

Abstract: The paper proposes a method to secure the Compressive Sensing (CS) streams.
It consists in protecting part of the measurements by a secret key and
inserting the code into the rest. The secret key is generated via a
cryptographically secure pseudo-random number generator (CSPRNG) and XORed with
the measurements to be inserted. For insertion, we use a reversible data hiding
(RDH) scheme, which is a prediction error expansion algorithm, modified to
match the statistics of CS measurements. The reconstruction from the embedded
stream conducts to visibly distorted images. The image distortion is controlled
by the number of embedded levels. In our tests, the embedding on 10 levels
results in $\approx 18 dB $ distortion for images of 256x256 pixels
reconstructed with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).
A particularity of the presented method is on-the-fly insertion that makes it
appropriate for the sequential acquisition of measurements by a Single Pixel
Camera. On-the-fly insertion avoids the buffering of CS measurements for a
subsequent standard encryption and generation of a thumbnail image.

</details>


### [211] [Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561)
*Zenghui Yuan,Yangming Xu,Jiawen Shi,Pan Zhou,Lichao Sun*

Main category: cs.CR

TL;DR: This paper introduces Merge Hijacking, the first backdoor attack targeting model merging in Large Language Models (LLMs). The attacker releases a malicious model that, when merged with other models, passes on a backdoor while keeping its functionality. The paper outlines two main goals of this attack - effectiveness and utility, achieved through four steps. Experiments confirm the attack's success across various models, algorithms, and tasks, including real-world scenarios, and its resilience to multiple defenses.


<details>
  <summary>Details</summary>
Motivation: Model merging in LLMs allows for the creation of a unified model capable of handling multi-domain tasks by directly fusing parameters from different task-specific models. However, open-source models can have vulnerabilities, making model merging vulnerable to backdoor attacks.

Method: The method involves constructing a malicious model which is then released by the attacker. When a user merges this model with any others, the resulting model inherits a backdoor without losing its ability to perform tasks effectively. This is done while achieving two main objectives: ensuring the attack's effectiveness and maintaining the utility of the merged model.

Result: Experiments show that the Merge Hijacking attack is effective across different models, merging algorithms, and tasks. It remains successful even with real-world models and demonstrates robustness against various inference-time and training-time defense mechanisms.

Conclusion: Merge Hijacking presents a significant threat to model merging in LLMs, successfully introducing backdoors while preserving model utility. The attack is versatile and resilient against current defense strategies.

Abstract: Model merging for Large Language Models (LLMs) directly fuses the parameters
of different models finetuned on various tasks, creating a unified model for
multi-domain tasks. However, due to potential vulnerabilities in models
available on open-source platforms, model merging is susceptible to backdoor
attacks. In this paper, we propose Merge Hijacking, the first backdoor attack
targeting model merging in LLMs. The attacker constructs a malicious upload
model and releases it. Once a victim user merges it with any other models, the
resulting merged model inherits the backdoor while maintaining utility across
tasks. Merge Hijacking defines two main objectives-effectiveness and
utility-and achieves them through four steps. Extensive experiments demonstrate
the effectiveness of our attack across different models, merging algorithms,
and tasks. Additionally, we show that the attack remains effective even when
merging real-world models. Moreover, our attack demonstrates robustness against
two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time
defense (Fine-pruning).

</details>


### [212] [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643)
*Manuel Costa,Boris Köpf,Aashish Kolluri,Andrew Paverd,Mark Russinovich,Ahmed Salem,Shruti Tople,Lukas Wutschitz,Santiago Zanella-Béguelin*

Main category: cs.CR

TL;DR: This paper addresses the security of AI agents by using information-flow control (IFC) to ensure protection against vulnerabilities like prompt injection. It introduces Fides, a planner that tracks confidentiality and integrity labels while enforcing security policies, showing an extended range of secure tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the growing autonomy and capability of AI agents, which increases the importance of ensuring their security, particularly against issues such as prompt injection.

Method: The method involves creating a formal model to evaluate the security and expressiveness of agent planners. The class of properties enforceable by dynamic taint-tracking is characterized, and a taxonomy of tasks is built to assess security and utility trade-offs in planner designs. Based on these findings, the Fides planner is developed, which incorporates confidentiality and integrity label tracking, deterministic enforcement of security policies, and new primitives for selective information hiding.

Result: Fides was evaluated in AgentDojo and demonstrated the ability to broaden the range of tasks that can be securely accomplished by AI agents.

Conclusion: The conclusion is that using IFC provides significant security guarantees for AI agents, and the introduction of Fides shows promise in balancing security with task utility.

Abstract: As AI agents become increasingly autonomous and capable, ensuring their
security against vulnerabilities such as prompt injection becomes critical.
This paper explores the use of information-flow control (IFC) to provide
security guarantees for AI agents. We present a formal model to reason about
the security and expressiveness of agent planners. Using this model, we
characterize the class of properties enforceable by dynamic taint-tracking and
construct a taxonomy of tasks to evaluate security and utility trade-offs of
planner designs. Informed by this exploration, we present Fides, a planner that
tracks confidentiality and integrity labels, deterministically enforces
security policies, and introduces novel primitives for selectively hiding
information. Its evaluation in AgentDojo demonstrates that this approach
broadens the range of tasks that can be securely accomplished. A tutorial to
walk readers through the the concepts introduced in the paper can be found at
https://github.com/microsoft/fides

</details>


### [213] [Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference](https://arxiv.org/abs/2505.23655)
*Peter David Fagan*

Main category: cs.CR

TL;DR: This paper presents a new framework using chaotic dynamical systems for secure neural network inference.


<details>
  <summary>Details</summary>
Motivation: There is a need for secure and privacy-preserving methods in neural network inference that can offer scalability and lightweight alternatives to traditional cryptographic techniques.

Method: The method uses keyed chaotic dynamical transformations applied to tensors, seeded cryptographically to create non-invertible, user-specific transformations. This allows for authenticated inference, watermarking, and data attribution.

Result: The framework provides a scalable and lightweight alternative for tensor-level security in AI systems.

Conclusion: This work establishes a new direction for enhancing security in AI systems through the use of chaotic dynamical systems.

Abstract: This work introduces a novel framework for secure and privacy-preserving
neural network inference based on keyed chaotic dynamical transformations. The
proposed method applies a deterministic, cryptographically seeded chaotic
system to tensors, producing non-invertible, user-specific transformations that
enable authenticated inference, tensor-level watermarking, and data
attribution. This framework offers a scalable and lightweight alternative to
conventional cryptographic techniques, and establishes a new direction for
tensor-level security in AI systems.

</details>


### [214] [Bayesian Perspective on Memorization and Reconstruction](https://arxiv.org/abs/2505.23658)
*Haim Kaplan,Yishay Mansour,Kobbi Nissim,Uri Stemmer*

Main category: cs.CR

TL;DR: The paper introduces a new Bayesian perspective on data reconstruction and proposes a security definition to prevent reconstruction attacks in certain settings. It reinterprets fingerprinting code attacks as membership inference rather than reconstruction, showing that preventing reconstruction alone can bypass some impossibility results derived from FPC.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing security definitions and provide a better understanding of fingerprinting code attacks (FPC) within the context of data reconstruction and privacy.

Method: Introduce a Bayesian perspective on data reconstruction and propose a new security definition based on this perspective. Analyze fingerprinting code attacks and reinterpret them as membership inference attacks instead of reconstruction attacks.

Result: Demonstrates that under the new security definition, it is possible to prevent reconstruction attacks in certain settings without addressing membership inference. Shows that some impossibility results from FPC do not apply when solely focusing on reconstruction prevention.

Conclusion: The proposed Bayesian perspective and security definition offer a novel approach to understanding and mitigating reconstruction attacks, with implications for rethinking the nature of fingerprinting code attacks.

Abstract: We introduce a new Bayesian perspective on the concept of data
reconstruction, and leverage this viewpoint to propose a new security
definition that, in certain settings, provably prevents reconstruction attacks.
We use our paradigm to shed new light on one of the most notorious attacks in
the privacy and memorization literature - fingerprinting code attacks (FPC). We
argue that these attacks are really a form of membership inference attacks,
rather than reconstruction attacks. Furthermore, we show that if the goal is
solely to prevent reconstruction (but not membership inference), then in some
cases the impossibility results derived from FPC no longer apply.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [215] [Quantum Hilbert Transform](https://arxiv.org/abs/2505.23581)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: The paper introduces a formulation for the quantum Hilbert transform (QHT) and applies it to a quantum steganography protocol, bridging classical phase-shift techniques with quantum operations.


<details>
  <summary>Details</summary>
Motivation: There is no existing quantum analogue for the Hilbert transform, which is foundational in signal processing across multiple disciplines.

Method: The authors formulate the quantum Hilbert transform (QHT) and integrate it into a quantum steganography protocol by connecting classical phase-shift techniques with quantum operations.

Result: QHT provides new opportunities in quantum signal processing, communications, sensing, and secure information hiding.

Conclusion: The introduction of QHT expands the capabilities in quantum technologies, particularly in steganography and other related fields.

Abstract: The Hilbert transform has been one of the foundational transforms in signal
processing, finding it's way into multiple disciplines from cryptography to
biomedical sciences. However, there does not exist any quantum analogue for the
Hilbert transform. In this work, we introduce a formulation for the quantum
Hilbert transform (QHT)and apply it to a quantum steganography protocol. By
bridging classical phase-shift techniques with quantum operations, QHT opens
new pathways in quantum signal processing, communications, sensing, and secure
information hiding.

</details>


### [216] [Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks](https://arxiv.org/abs/2505.23603)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: This paper reviews the concept and challenges of building aerial quantum networks using drones, balloons or satellites, and discusses possible solutions to create a practical quantum internet.


<details>
  <summary>Details</summary>
Motivation: To increase the scalability of quantum communication networks by moving them off the ground using aerial links.

Method: Reviewing the principles of quantum physics applied in creating secure communication networks, analyzing the difficulties in building aerial links, and exploring the possible solutions that can be used to overcome these problems.

Result: A combination of ground stations, aerial relays, and orbiting satellites could form a seamless system closer to a practical quantum internet.

Conclusion: Using aerial links such as drones, balloons, or satellites has the potential to significantly enhance the scalability of quantum networks, bringing us closer to realizing a practical quantum internet.

Abstract: Quantum networks use principles of quantum physics to create secure
communication networks. Moving these networks off the ground using drones,
balloons, or satellites could help increase the scalability of these networks.
This article reviews how such aerial links work, what makes them difficult to
build, and the possible solutions that can be used to overcome these problems.
By combining ground stations, aerial relays, and orbiting satellites into one
seamless system, we move closer to a practical quantum internet.

</details>


### [217] [Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing](https://arxiv.org/abs/2505.23066)
*Shuyin Xia,Xiaojiang Tian,Suzhen Yuan,Jeremiah D. Deng*

Main category: quant-ph

TL;DR: The paper proposes an algorithm named Granular-Ball based Quantum $k$NN(GB-Q$k$NN) to address the high time complexity issue of $k$-Nearest Neighbors ($k$NN), reducing data size with granular-balls and accelerating search with HNSW method, further optimizing via quantization.


<details>
  <summary>Details</summary>
Motivation: High time complexity is a major challenge for $k$NN algorithms, especially when dealing with large datasets.

Method: The GB-Q$k$NN algorithm uses granular-balls to reduce data size, applies HNSW method for faster search, and optimizes time-consuming steps like distance calculation through quantization.

Result: Through comprehensive complexity analysis, the proposed approach significantly reduces the time complexity of $k$NN-like algorithms.

Conclusion: By combining granular-balls and quantization of the HNSW method, the GB-Q$k$NN achieves higher efficiency in processing large amounts of data.

Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest
Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have
made some improvements, they still have a speed bottleneck when facing large
amounts of data. To address this issue, we propose an innovative algorithm
called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves
higher efficiency by first employing granular-balls, which reduces the data
size needed to processed. The search process is then accelerated by adopting a
Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the
time-consuming steps, such as distance calculation, of the HNSW via
quantization, further reducing the time complexity of the construct and search
process. By combining the use of granular-balls and quantization of the HNSW
method, our approach manages to take advantage of these treatments and
significantly reduces the time complexity of the $k$NN-like algorithms, as
revealed by a comprehensive complexity analysis.

</details>


### [218] [Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood](https://arxiv.org/abs/2505.22743)
*Sitan Chen,Weiyuan Gong,Jonas Haferkamp,Yihui Quek*

Main category: quant-ph

TL;DR: 本文将经典低度多项式方法扩展到量子领域，建立了量子态设计与低度多项式困难之间的联系，并应用于学习量子吉布斯态、随机浅层量子电路态及量子误差缓解等问题的信息-计算鸿沟研究。


<details>
  <summary>Details</summary>
Motivation: 在从量子数据中学习的各种物理相关场景中，设计能够高效提取信息的协议仍是一门艺术，且存在一些重要情况下被认为是不可能的（即存在信息-计算鸿沟）。尽管经典文献中有大量工具证明统计推断问题的平均情况难度，但量子领域的相应工具极为有限。

Method: 通过将经典低度多项式方法扩展到量子设置，建立状态设计和低度硬度之间的一般联系。具体应用包括：1）学习随机稀疏非局部哈密顿量的吉布斯态；2）学习随机浅层量子电路态；3）量子误差缓解中的低度硬度；4）定义新的量子种植双团问题并确定其计算难度阈值；5）研究带噪声的学习稳定器问题和产品态的不可知学习。

Result: 获得了学习吉布斯态和随机浅层量子电路态的信息-计算鸿沟结果；首次在低度框架内建模适应性选择测量基的情况；证明了量子误差缓解策略的低度硬度；定义了新的量子种植双团问题并确定其计算难度阈值；展示了带噪声学习稳定器问题和产品态不可知学习的平均情况难度。

Conclusion: 本文扩展了经典的低度多项式方法至量子领域，揭示了若干量子学习问题中的信息-计算鸿沟，并为理解量子统计推断问题的计算复杂性提供了新工具。

Abstract: In a variety of physically relevant settings for learning from quantum data,
designing protocols that can computationally efficiently extract information
remains largely an art, and there are important cases where we believe this to
be impossible, that is, where there is an information-computation gap. While
there is a large array of tools in the classical literature for giving evidence
for average-case hardness of statistical inference problems, the corresponding
tools in the quantum literature are far more limited. One such framework in the
classical literature, the low-degree method, makes predictions about hardness
of inference problems based on the failure of estimators given by low-degree
polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree
hardness. We use this to obtain the first information-computation gaps for
learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it
to prove hardness for learning random shallow quantum circuit states in a
challenging model where states can be measured in adaptively chosen bases. To
our knowledge, the ability to model adaptivity within the low-degree framework
was open even in classical settings. In addition, we also obtain a low-degree
hardness result for quantum error mitigation against strategies with
single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and
identify the threshold at which this problem becomes computationally hard for
protocols that perform local measurements. Interestingly, the complexity
landscape for this problem shifts when going from local measurements to more
entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning
Stabilizers with Noise and for agnostically learning product states.

</details>


### [219] [Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference](https://arxiv.org/abs/2505.23389)
*Ivana Nikoloska,Hamdi Joudeh,Ruud van Sloun,Osvaldo Simeone*

Main category: quant-ph

TL;DR: Quantum sensing uses non-classical effects to surpass classical sensors, but faces challenges with NISQ devices. This paper proposes an online control framework for variational quantum sensing (VQS) that dynamically updates parameters and provides error bars, using online conformal inference techniques. Experiments show it maintains reliability while providing precise estimates.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current variational quantum sensing methods which lack rigorous performance guarantees when implemented on noisy intermediate-scale quantum (NISQ) devices.

Method: An online control framework for VQS is proposed, which dynamically updates variational parameters and provides deterministic error bars through online conformal inference techniques.

Result: Experiments on a quantum magnetometry task confirmed that the dynamic VQS approach maintained reliability over time while still yielding precise estimates.

Conclusion: The combination of variational quantum algorithms with online conformal inference offers practical benefits for reliable quantum sensing on NISQ devices.

Abstract: Quantum sensing exploits non-classical effects to overcome limitations of
classical sensors, with applications ranging from gravitational-wave detection
to nanoscale imaging. However, practical quantum sensors built on noisy
intermediate-scale quantum (NISQ) devices face significant noise and sampling
constraints, and current variational quantum sensing (VQS) methods lack
rigorous performance guarantees. This paper proposes an online control
framework for VQS that dynamically updates the variational parameters while
providing deterministic error bars on the estimates. By leveraging online
conformal inference techniques, the approach produces sequential estimation
sets with a guaranteed long-term risk level. Experiments on a quantum
magnetometry task confirm that the proposed dynamic VQS approach maintains the
required reliability over time, while still yielding precise estimates. The
results demonstrate the practical benefits of combining variational quantum
algorithms with online conformal inference to achieve reliable quantum sensing
on NISQ devices.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [220] [Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges](https://arxiv.org/abs/2505.22688)
*Palur Venkata Raghuvamsi,Siyuan Brandon Loh,Prasanta Bhattacharya,Joses Ho,Raphael Lee Tze Chuen,Alvin X. Han,Sebastian Maurer-Stroh*

Main category: q-bio.QM

TL;DR: The paper explores diverse data sets beyond traditional epidemiological data to predict case surges during the pandemic, revealing heterogeneity in model effectiveness across countries and advocating for tailored models.


<details>
  <summary>Details</summary>
Motivation: To enhance predictions of pandemic dynamics by investigating the usefulness of integrating alternative datasets (genomic, public health, human behavior) into existing disease surveillance frameworks.

Method: Investigated biological features (e.g., mutations), public health metrics (e.g., case counts, policy interventions), and human behavioral data (e.g., mobility, social media) to predict country-level case surges.

Result: Found considerable heterogeneity in predictive performance across countries and feature types, indicating that prediction models may need customization based on national contexts and pandemic phases.

Conclusion: Integrating alternative data sources into disease surveillance can improve predictions of pandemic dynamics.

Abstract: The COVID-19 pandemic response relied heavily on statistical and machine
learning models to predict key outcomes such as case prevalence and fatality
rates. These predictions were instrumental in enabling timely public health
interventions that helped break transmission cycles. While most existing models
are grounded in traditional epidemiological data, the potential of alternative
datasets, such as those derived from genomic information and human behavior,
remains underexplored. In the current study, we investigated the usefulness of
diverse modalities of feature sets in predicting case surges. Our results
highlight the relative effectiveness of biological (e.g., mutations), public
health (e.g., case counts, policy interventions) and human behavioral features
(e.g., mobility and social media conversations) in predicting country-level
case surges. Importantly, we uncover considerable heterogeneity in predictive
performance across countries and feature modalities, suggesting that surge
prediction models may need to be tailored to specific national contexts and
pandemic phases. Overall, our work highlights the value of integrating
alternative data sources into existing disease surveillance frameworks to
enhance the prediction of pandemic dynamics.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [221] [Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning](https://arxiv.org/abs/2505.22684)
*Yufeng Wang,Yiguang Bai,Tianqing Zhu,Ismail Ben Ayed,Jing Yuan*

Main category: cs.SI

TL;DR: 社区划分在网络分析中至关重要，模块度优化是最常用的技术。然而，传统的基于模块度的方法常常忽视了现实应用中关键的公平性方面。为了解决这个问题，本文引入了受保护组网络，并提出了一种新的公平模块度度量方法。该方法通过明确地融入公平性来扩展传统的模块度，并证明最小化该度量可以获得自然公平的受保护组划分，同时保持理论上的合理性。我们开发了一个通用的公平划分优化框架，并设计了高效的公平快速纽曼（FairFN）算法，增强了快速纽曼（FN）方法以优化模块度和公平性。实验表明，与最先进的方法相比，FairFN在不平衡数据集上尤其能够显著提高公平性和高质量的划分。


<details>
  <summary>Details</summary>
Motivation: 传统模块度优化方法忽视了现实应用中的公平性问题，这在实际应用中可能导致对某些群体的不公平对待。因此，需要一种能够将公平性纳入考虑的新方法。

Method: 1. 引入受保护组网络的概念。2. 提出一个新的公平模块度度量，将公平性明确纳入传统模块度计算。3. 开发一个通用的公平划分优化框架。4. 设计FairFN算法，增强经典Fast Newman方法以同时优化模块度和公平性。

Result: 实验结果表明，FairFN算法在公平性和划分质量方面均优于现有最先进的方法，特别是在处理不平衡数据集时表现尤为突出。

Conclusion: 本文提出的公平模块度度量和FairFN算法有效地解决了社区划分中的公平性问题，同时保持了理论合理性和高质量的划分性能。

Abstract: Community partitioning is crucial in network analysis, with modularity
optimization being the prevailing technique. However, traditional
modularity-based methods often overlook fairness, a critical aspect in
real-world applications. To address this, we introduce protected group networks
and propose a novel fairness-modularity metric. This metric extends traditional
modularity by explicitly incorporating fairness, and we prove that minimizing
it yields naturally fair partitions for protected groups while maintaining
theoretical soundness. We develop a general optimization framework for fairness
partitioning and design the efficient Fair Fast Newman (FairFN) algorithm,
enhancing the Fast Newman (FN) method to optimize both modularity and fairness.
Experiments show FairFN achieves significantly improved fairness and
high-quality partitions compared to state-of-the-art methods, especially on
unbalanced datasets.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [222] [A comprehensive analysis of PINNs: Variants, Applications, and Challenges](https://arxiv.org/abs/2505.22761)
*Afila Ajithkumar Sophiya,Akarsh K Nair,Sepehr Maleki,Senthil K. Krishnababu*

Main category: cs.CE

TL;DR: Physics Informed Neural Networks (PINNs) are a computational tool for solving differential equations. This survey provides a comprehensive overview of PINNs, including architecture, variants, applications, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive view of PINNs that existing surveys fail to deliver, as they focus on either different application scenarios or limit their study to a superficial level.

Method: Presenting a detailed analysis of PINNs architecture and variants, performance analysis on different equations and application domains, and discussing current issues and future research directions.

Result: A thorough understanding of the state-of-the-art research in PINNs, highlighting their features, challenges, and potential future research areas.

Conclusion: This survey contributes significantly to the field by summarizing PINNs' architecture, performance across various domains, and outlining current issues and future research directions.

Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful
computational tool for solving differential equations. However, the
applicability of these models is still in its initial stages and requires more
standardization to gain wider popularity. Through this survey, we present a
comprehensive overview of PINNs approaches exploring various aspects related to
their architecture, variants, areas of application, real-world use cases,
challenges, and so on. Even though existing surveys can be identified, they
fail to provide a comprehensive view as they primarily focus on either
different application scenarios or limit their study to a superficial level.
This survey attempts to bridge the gap in the existing literature by presenting
a detailed analysis of all these factors combined with recent advancements and
state-of-the-art research in PINNs. Additionally, we discuss prevalent
challenges in PINNs implementation and present some of the future research
directions as well. The overall contributions of the survey can be summarised
into three sections: A detailed overview of PINNs architecture and variants, a
performance analysis of PINNs on different equations and application domains
highlighting their features. Finally, we present a detailed discussion of
current issues and future research directions.

</details>


### [223] [Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data](https://arxiv.org/abs/2505.22670)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.CE

TL;DR: The paper proposes a unified network-based representation method integrating 'semantic-spatial-topological' features of BIM models, enhancing understanding of design patterns and decision-making.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks effective methods for capturing complex spatial and topological relationships between components in BIM models.

Method: Proposes a network-based representation method that extends the IFC standard to include local spatial relationships and topological connections between components.

Result: This method allows for a more detailed understanding of component interactions, dependencies, and implicit design patterns.

Conclusion: The proposed method effectively captures semantic, topological, and spatial relationships in BIM models, with significant potential for representing and learning design patterns.

Abstract: Building Information Modeling (BIM) has revolutionized the construction
industry by providing a comprehensive digital representation of building
structures throughout their lifecycle. However, existing research lacks
effective methods for capturing the complex spatial and topological
relationships between components in BIM models, which are essential for
understanding design patterns and enhancing decision-making. This study
proposes a unified network-based representation method that integrates the
"semantic-spatial-topological" multi-dimensional design features of BIM models.
By extending the IFC (Industry Foundation Classes) standard, we introduce local
spatial relationships and topological connections between components to enrich
the network structure. This representation method enables a more detailed
understanding of component interactions, dependencies, and implicit design
patterns, effectively capturing the semantic, topological, and spatial
relationships in BIM, and holds significant potential for the representation
and learning of design patterns.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [224] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/abs/2505.23192)
*Run Hao,Peng Ying*

Main category: cs.CV

TL;DR: The paper presents an automated adversarial prompt generation framework that can evade AIGC detectors, ranked first in a real-world competition, and can also construct high-quality adversarial datasets.


<details>
  <summary>Details</summary>
Motivation: The rise of text-to-image models has enabled photorealistic human portraits synthesis, raising concerns about identity misuse and the robustness of AIGC detectors.

Method: The method uses a grammar tree structure and a variant of the Monte Carlo tree search algorithm to explore the semantic prompt space systematically, generating diverse and controllable prompts.

Result: Extensive experiments across multiple T2I models validate its effectiveness and it ranked first in a real-world adversarial AIGC detection competition.

Conclusion: The proposed framework not only evades AIGC detectors but also provides valuable resources for training and evaluating more robust AIGC detection and defense systems.

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [225] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/abs/2505.22762)
*Marco Colussi,Dragan Ahmetovic,Sergio Mascetti*

Main category: cs.CV

TL;DR: This paper proposes MIAS-SAM, a novel method for segmenting anomalous regions in medical images without needing to define a threshold value. It uses a patch-based memory bank and SAM encoder to extract features and compute anomaly maps. The method's effectiveness is demonstrated through accurate anomaly segmentation on three datasets with different imaging modalities.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve the segmentation of anomalous regions in medical images by developing a method that does not require a predefined threshold value, thus potentially enhancing the accuracy and usability of such systems.

Method: MIAS-SAM employs a patch-based memory bank to store image features extracted from normal data using the SAM encoder. During inference, embedding patches from the SAM encoder are compared with those in the memory bank to generate an anomaly map. The center of gravity of the anomaly map prompts the SAM decoder to achieve accurate segmentation.

Result: Experimental results on three publicly available datasets with different imaging modalities (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score.

Conclusion: MIAS-SAM presents a successful approach for segmenting anomalous regions in medical images across various modalities without requiring a threshold value, demonstrating high accuracy as evidenced by the DICE scores.

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [226] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/abs/2505.22815)
*Zhangyi Hu,Jiemin Wu,Hua Xu,Mingqian Liao,Ninghui Feng,Bo Gao,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: The paper proposes VIMTS, a framework adapting Visual Mask AutoEncoder (MAE) for Irregular Multivariate Time Series (IMTS) forecasting. It processes IMTS data into feature patches, uses cross-channel dependencies, leverages visual MAE's capabilities, and applies self-supervised learning. Experiments show superior performance and few-shot capability.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture reliable temporal patterns from IMTS data due to significant missing values. Pre-trained foundation models are typically designed for Regularly Sampled Time Series (RTS). The authors aim to address these challenges by adapting the successful visual Mask AutoEncoder (MAE) used in RTS forecasting to IMTS forecasting.

Method: VIMTS first processes IMTS along the timeline into feature patches at equal intervals. These patches are then complemented using learned cross-channel dependencies. The framework leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, followed by a coarse-to-fine technique to generate precise predictions. Self-supervised learning is also integrated for improved IMTS modeling.

Result: Extensive experiments demonstrate VIMTS's superior performance and few-shot capability in IMTS forecasting, advancing the application of visual foundation models in more general time series tasks.

Conclusion: VIMTS successfully adapts the visual Mask AutoEncoder for IMTS forecasting, showing superior performance and few-shot capability. This work advances the application of visual foundation models in more general time series tasks.

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [227] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/abs/2505.22944)
*Angtian Wang,Haibin Huang,Jacob Zhiyuan Fang,Yiding Yang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出了一种统一的视频生成运动控制框架，通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间，实现相机移动、物体级平移和精细局部运动的无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同类型的运动（如相机移动、物体平移和局部运动）时，通常需要单独的模块或特定任务的设计，缺乏一个统一且连贯的解决方案。

Method: 该方法通过一个轻量级的运动注入器，将用户定义的轨迹投影到预训练的图像到视频生成模型的潜在空间。用户可以指定关键点及其运动路径，以控制局部变形、整个物体的运动、虚拟摄像机动态或这些的组合。注入的轨迹信号引导生成过程，产生时间上一致且语义对齐的运动序列。

Result: 实验表明，该方法在多种视频运动控制任务中表现出优越的性能，包括风格化的运动效果（如运动画笔）、动态视点变化和精确的局部运动操作。与以前的方法和商业解决方案相比，该方法提供了显著更好的可控性和视觉质量，同时与各种最先进的视频生成骨干保持广泛的兼容性。

Conclusion: 提出的统一框架为视频生成中的运动控制提供了一个连贯的解决方案，能够无缝集成不同类型的运动，并在多个任务中展现出优越的性能和兼容性。

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [228] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
*Kewei Lian,Shaofei Cai,Yilun Du,Yitao Liang*

Main category: cs.CV

TL;DR: The paper addresses the lack of datasets promoting spatial consistency in world models by creating a Minecraft-based dataset with 250 hours of navigation videos, emphasizing long-range spatial consistency. It evaluates four baselines and is open-sourced.


<details>
  <summary>Details</summary>
Motivation: There is a need for datasets that enforce spatial consistency constraints for developing effective memory modules in world models, as current benchmarks focus more on visual coherence or generation quality rather than long-range spatial consistency.

Method: Constructed a dataset from 150 distinct locations in Minecraft's open-world environment, collecting 250 hours (20 million frames) of loop-based navigation videos with actions. The dataset follows a curriculum design of sequence lengths to learn spatial consistency on complex trajectories and is extensible to new environments/modules.

Result: Four representative world model baselines were evaluated on the newly created benchmark, showing the potential for learning spatial consistency.

Conclusion: The constructed dataset and benchmark address the gap in enforcing spatial consistency constraints, providing a valuable resource for future research in world models.

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [229] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Main category: cs.CV

TL;DR: 研究人员针对匈牙利语创建了两个文档视觉问答数据集HuDocVQA和HuDocVQA-manual，以及一个包含117k页面的Hungarian Common Crawl PDF数据集HuCCPDF。通过在这些数据集上微调Llama 3.2 11B Instruct模型，匈牙利语DocVQA任务的准确率提高了7.2%。所有数据集和代码将公开发布以促进多语言DocVQA研究。


<details>
  <summary>Details</summary>
Motivation: 尽管现代视觉语言模型（VLMs）在英语文档视觉问答任务中达到了近乎饱和的准确性，但在低资源语言中，由于缺乏合适的训练和评估数据，这一任务仍然具有挑战性。因此，研究团队选择以匈牙利语为例，探索如何构建适用于低资源语言的数据集，并提升VLMs在这些语言上的表现。

Method: 研究团队创建了两个匈牙利语文档视觉问答数据集：HuDocVQA-manual（小规模人工标注）和HuDocVQA（大规模合成生成），并从Common Crawl获取数据。对于HuDocVQA，进行了多轮质量过滤和去重以确保数据质量接近人类水平。此外，还创建了一个名为HuCCPDF的数据集，包含117k页匈牙利语Common Crawl PDF及其转录文本，可用于训练匈牙利语OCR模型。最后，通过混合这些数据集对Llama 3.2 11B Instruct模型进行微调，验证其效果。

Result: 实验结果表明，在HuDocVQA数据集上，通过使用混合数据集微调后的Llama 3.2 11B Instruct模型，匈牙利语DocVQA任务的准确率提升了7.2%，证明了所创建数据集的有效性和高质量。

Conclusion: 本研究成功构建了适用于匈牙利语文档视觉问答任务的数据集HuDocVQA、HuDocVQA-manual和HuCCPDF，显著提升了VLMs在匈牙利语DocVQA任务中的表现。这些数据集和相关代码将向公众开放，为推动多语言DocVQA领域的进一步研究提供了重要资源。

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [230] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/abs/2505.23043)
*Jihai Zhang,Tianle Li,Linjie Li,Zhengyuan Yang,Yu Cheng*

Main category: cs.CV

TL;DR: Recent advancements in unified vision-language models (VLMs) have attracted significant attention. This paper systematically investigates the generalization across understanding and generation tasks in unified VLMs.


<details>
  <summary>Details</summary>
Motivation: The underlying hypothesis of unified VLMs, which is that a unified architecture with mixed training on both understanding and generation tasks can enable mutual enhancement between understanding and generation, remains underexplored in prior works.

Method: This paper designs a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. Multiple unified VLM architectures are evaluated to validate findings.

Result: 1. Unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, and this mutual benefit can scale up with increased data.
2. Better alignment between multimodal input and output spaces leads to better generalization.
3. Knowledge acquired during generation tasks can transfer to understanding tasks, occurring within the base language model beyond modality adapters.

Conclusion: The findings underscore the critical necessity of unifying understanding and generation in VLMs, offering valuable insights for the design and optimization of unified VLMs.

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [231] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/abs/2505.23045)
*Chuanhao Li,Wenbo Ye,Zhen Li,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: The paper explores multi-sourced compositional generalization (MSCG) in visual question answering (VQA), proposes a retrieval-augmented training framework to enhance MSCG ability by learning unified representations for primitives from different modalities, and constructs a new GQA-MSCG dataset for evaluation.


<details>
  <summary>Details</summary>
Motivation: Compositional generalization is crucial for vision-and-language tasks, especially when dealing with multi-modal data where primitives composing novel combinations come from different sources. However, the generalization ability over multi-sourced novel compositions remains unexplored.

Method: The authors propose a retrieval-augmented training framework that retrieves semantically equivalent primitives for each primitive in the training samples and aggregates retrieved features with the original primitive to refine the model. This helps the model learn consistent representations for the same semantic primitives across different modalities.

Result: Experimental results on the newly constructed GQA-MSCG dataset demonstrate the effectiveness of the proposed framework in enhancing the multi-sourced compositional generalization ability of VQA models.

Conclusion: The retrieval-augmented training framework effectively enhances the MSCG ability of VQA models, and the GQA-MSCG dataset provides a valuable resource for evaluating this capability.

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [232] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/abs/2505.23085)
*Gwanghyun Kim,Xueting Li,Ye Yuan,Koki Nagano,Tianye Li,Jan Kautz,Se Young Chun,Umar Iqbal*

Main category: cs.CV

TL;DR: GeoMan is a novel architecture that accurately estimates 3D human geometry from monocular videos, overcoming issues of temporal inconsistency and lack of 4D training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating 3D human geometry from videos suffer from temporal inconsistencies and fail to capture fine-grained dynamic details.

Method: GeoMan uses an image-based model for the first frame's depth and normal estimation, then conditions a video diffusion model. It also introduces a root-relative depth representation for better human size estimation.

Result: GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations.

Conclusion: GeoMan effectively addresses longstanding challenges in 3D human geometry estimation from videos.

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [233] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/abs/2505.23107)
*Pushapdeep Singh,Jyoti Nigam,Medicherla Vamsi Krishna,Arnav Bhavsar,Aditya Nigam*

Main category: cs.CV

TL;DR: An EEG data classification framework named EEG Adapter (EAD) is proposed, which can adapt to different signal acquisition devices and learn robust representations for classification tasks. It achieves state-of-the-art accuracies on two datasets and demonstrates generalization capability through zero-shot classification.


<details>
  <summary>Details</summary>
Motivation: Current EEG classification methods are task-specific and dependent on the number of EEG channels in each sample, making it difficult to develop a unified pipeline for various EEG signal classification tasks. This motivates the need for a flexible framework that can handle EEG data from different acquisition devices.

Method: The proposed method, EEG Adapter (EAD), leverages a recent EEG foundational model with significant adaptations to learn robust representations from EEG data for classification tasks. It is designed to be compatible with any signal acquisition device and is evaluated on two publicly available datasets.

Result: EAD achieves state-of-the-art accuracies of 99.33% on EEG-ImageNet and 92.31% on BrainLat. It also demonstrates generalization capability through zero-shot EEG classification on the EEG-ImageNet task.

Conclusion: The proposed EAD framework is effective across diverse EEG datasets containing different perception tasks, such as stimulus and resting-state EEG signals. It provides a flexible solution for EEG classification tasks with varying numbers of EEG channels.

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [234] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/abs/2505.23134)
*Tongtong Su,Chengyu Wang,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: Appearance editing according to user needs is a pivotal task in video editing. This paper introduces Zero-to-Hero, a novel reference-based video editing method that disentangles the editing process into two distinct problems.


<details>
  <summary>Details</summary>
Motivation: Existing text-guided methods for appearance editing often lead to ambiguities regarding user intentions and restrict fine-grained control over editing specific aspects of objects.

Method: The method first edits an anchor frame to satisfy user requirements as a reference image and then consistently propagates its appearance across other frames using correspondence within the original frames to guide the attention mechanism.

Result: Our method outperforms the best-performing baseline with a PSNR improvement of 2.6 dB.

Conclusion: This novel approach offers a solid ZERO-shot initialization that ensures both accuracy and temporal consistency, though there are issues with compounded imaging degradation.

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [235] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/abs/2505.23145)
*Jeongsol Kim,Yeobin Hong,Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign是一种新的无反演、基于流的方法，用于一致的图像编辑，具有轨迹控制功能。它通过引入流量匹配损失来提高编辑过程中的稳定性和源一致性，并支持逆向编辑。实验表明，FlowAlign在源保持和编辑可控性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无反演、基于流的图像编辑方法（如FlowEdit）虽然避免了精确的潜在反演，但常常导致不稳定的编辑轨迹和较差的源一致性。

Method: FlowAlign提出了一种无反演的基于流的框架，通过引入流量匹配损失作为正则化机制，促进更平滑和稳定的编辑轨迹。该损失明确地平衡了与编辑提示的语义对齐和与源图像的结构一致性。此外，FlowAlign可以通过简单地反转ODE轨迹实现逆向编辑。

Result: 实验结果表明，FlowAlign在源保持和编辑可控性方面优于现有方法。

Conclusion: FlowAlign解决了现有方法中源一致性差的问题，提供了更稳定和可控的图像编辑方式，并支持逆向编辑。

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [236] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/abs/2505.23161)
*Antonio D'Orazio,Maria Rosaria Briglia,Donato Crisostomi,Dario Loi,Emanuele Rodolà,Iacopo Masi*

Main category: cs.CV

TL;DR: This paper demonstrates that image synthesis can be achieved using CLIP alone without any decoder, training, or fine-tuning by optimizing a frequency-aware implicit neural representation and introducing adversarially robust initialization.


<details>
  <summary>Details</summary>
Motivation: To explore the generative potential of discriminative models like CLIP without requiring additional decoders, training, or fine-tuning.

Method: The method optimizes a frequency-aware implicit neural representation for coarse-to-fine generation, uses adversarially robust initialization, applies Orthogonal Procrustes projection to align local text and image embeddings, and incorporates a blending loss to anchor outputs to natural image statistics.

Result: This framework enables capabilities such as text-to-image generation, style transfer, and image reconstruction without altering CLIP's weights.

Conclusion: Discriminative models may have untapped generative potential that can be unlocked with appropriate techniques.

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [237] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/abs/2505.23214)
*Wenhao Xu,Shuchen Zheng,Changwei Wang,Zherui Zhang,Chuan Ren,Rongtao Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: SAMamba is a new framework for Infrared Small Target Detection which combines SAM2 and Mamba networks. It introduces FS-Adapter, CSI module, and DPCF module to address domain gap, global context modeling, and detail preservation respectively. Experiments show it outperforms current methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for Infrared Small Target Detection have issues with information loss during downsampling and inefficient global context modeling. This paper aims to create a framework that can bridge the domain gap, maintain fine-grained details, and efficiently model long-range dependencies.

Method: The method involves creating SAMamba framework which integrates SAM2's hierarchical feature learning with Mamba's selective sequence modeling. Innovations include FS-Adapter for domain adaptation, CSI module for global context modeling, and DPCF module for combining multi-scale features.

Result: Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets demonstrate that SAMamba significantly surpasses state-of-the-art methods, particularly in scenarios with heterogeneous backgrounds and varying target scales.

Conclusion: SAMamba successfully addresses key challenges in Infrared Small Target Detection by bridging the domain gap, maintaining fine-grained details, and efficiently modeling long-range dependencies.

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [238] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/abs/2505.23268)
*Spyros Barbakos,Charalampos Antoniadis,Gerasimos Potamianos,Gianluca Setti*

Main category: cs.CV

TL;DR: This paper proposes a multimodal pipeline that uses both video frames and transcripts, trained within a Reinforcement Learning framework, to generate more condensed video summaries and detect highlights. The approach outperforms methods relying solely on visual content.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing works which either combine video frames and transcripts or use RL for video summarization but do not integrate both modalities in an RL framework.

Method: The method involves a multimodal pipeline that leverages video frames and their corresponding transcripts, using a modality fusion mechanism. It is trained within a Reinforcement Learning framework that rewards diverse and representative summaries while ensuring meaningful transcript content inclusion.

Result: Experiments demonstrate superior results when using transcripts in video summarization and highlight detection compared to relying only on visual content.

Conclusion: Integrating both video frames and transcripts within an RL framework leads to better video summarization and highlight detection.

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [239] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/abs/2505.23292)
*Evangelos Charalampakis,Vasileios Mygdalis,Ioannis Pitas*

Main category: cs.CV

TL;DR: This paper presents FUSS (Federated Unsupervised image Semantic Segmentation), a framework that enables fully decentralized, label-free semantic segmentation training. It outperforms local-only client trainings and classical FL algorithms in various segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Current USS methods face challenges when extended to federated settings due to the difficulty of aligning feature representation and cluster centroids across distributed clients under heterogeneous data distributions without supervision.

Method: The paper proposes FUSS, which introduces novel federation strategies promoting global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids.

Result: Experiments on benchmark and real-world datasets show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions.

Conclusion: FUSS is the first framework to enable fully decentralized, label-free semantic segmentation training, providing a significant advancement in Federated Learning for unsupervised semantic image segmentation.

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [240] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/abs/2505.23313)
*Weizhe Kong,Xiao Wang,Ruichong Gao,Chenglong Li,Yu Zhang,Xing Yang,Yaowei Wang,Jin Tang*

Main category: cs.CV

TL;DR: This paper proposes an adversarial attack and defense framework for pedestrian attribute recognition, which includes global- and patch-level attacks as well as a semantic offset defense strategy. Extensive experiments validate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: The potential vulnerability and anti-interference ability of pedestrian attribute recognition have not been fully explored.

Method: Exploits both global- and patch-level attacks on the pedestrian images based on the pre-trained CLIP-based PAR framework, and designs a semantic offset defense strategy to suppress the influence of adversarial attacks.

Result: Extensive experiments conducted on both digital domains (i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the effectiveness of the proposed adversarial attack and defense strategies for the pedestrian attribute recognition.

Conclusion: The authors propose the first adversarial attack and defense framework for pedestrian attribute recognition and demonstrate its effectiveness through extensive experiments.

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [241] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/abs/2505.23331)
*Matteo Gallici,Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: The paper explores using Group Relative Policy Optimization (GRPO) to fine-tune visual autoregressive (VAR) models with Reinforcement Learning (RL), enhancing image quality and generation style control, allowing VAR models to generalize beyond pre-training data and showing efficiency and effectiveness in RL-based fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To investigate the application of GRPO for fine-tuning VAR models using RL to align outputs with human preferences and enhance image quality and style control.

Method: Apply GRPO to fine-tune VAR models, leveraging aesthetic predictors and CLIP embeddings as reward signals, enabling RL-driven exploration for generating images aligned with new styles not seen during pre-training.

Result: Significant enhancement in image quality and precise control over generation style, with VAR models generalizing beyond their initial ImageNet distribution through RL-driven exploration.

Conclusion: RL-based fine-tuning is efficient and effective for VAR models, benefiting from their fast inference speeds which are advantageous for online sampling.

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [242] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/abs/2505.23367)
*Jeonghyeok Do,Sungpyo Kim,Geunhyuk Youk,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: PAN-Crafter is a new framework for PAN-sharpening that addresses cross-modality misalignment, using Modality-Adaptive Reconstruction and Cross-Modality Alignment-Aware Attention to produce high-quality HRMS images with improved efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing PAN-sharpening methods struggle with cross-modality misalignment issues which cause spectral distortion, double edges, and blurring in the outputs due to reliance on pixel-wise alignment and per-pixel reconstruction losses.

Method: PAN-Crafter employs two key components: Modality-Adaptive Reconstruction (MARs) for joint reconstruction of HRMS and PAN images using PAN's high-frequency details as self-supervision, and Cross-Modality Alignment-Aware Attention (CM3A) for bidirectional alignment of MS texture and PAN structure.

Result: PAN-Crafter outperforms state-of-the-art methods in all metrics across multiple benchmark datasets, with significantly faster inference time (50.11$\times$) and lower memory usage (0.63$\times$). It also shows strong generalization on unseen satellite datasets.

Conclusion: PAN-Crafter successfully mitigates cross-modality misalignment challenges in PAN-sharpening, offering superior performance, efficiency, and robustness.

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [243] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/abs/2505.23386)
*Han Bao,Qinying Wang,Zhi Chen,Qingming Li,Xuhong Zhang,Changjiang Li,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: An abstract about a new framework called VModA for detecting NSFW content.


<details>
  <summary>Details</summary>
Motivation: NSFW images on social networks pose serious harm, especially to minors. Current detection methods fail to effectively detect sophisticated NSFW images and face challenges due to varying regulations across platforms and regions.

Method: Propose VModA, a general and effective framework that adapts to diverse moderation rules and handles complex, semantically rich NSFW content across categories.

Result: VModA significantly outperforms existing methods, achieving up to a 54.3% accuracy improvement across NSFW types, including those with complex semantics. It also exhibits strong adaptability across categories, scenarios, and base VLMs.

Conclusion: The study concludes by mentioning the re-annotation of inconsistent and controversial label samples in public NSFW benchmark datasets and demonstrating the practical effectiveness of VModA in real-world scenarios.

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [244] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/abs/2505.23406)
*Binyamin Manela,Sharon Gannot,Ethan Fetyaya*

Main category: cs.CV

TL;DR: EdiDub is a new framework for visual dubbing that reformulates the process as a content-aware editing task, preserving original video context and improving identity preservation and synchronization.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current visual dubbing methods which either generate talking faces or use inpainting techniques, leading to issues like loss of vital visual information.

Method: EdiDub utilizes a specialized conditioning scheme to ensure faithful and accurate modifications in the video, treating visual dubbing as a content-aware editing task.

Result: Significantly improves identity preservation and synchronization on multiple benchmarks, including an occluded-lip dataset. Human evaluations show higher scores in synchronization and visual naturalness compared to leading methods.

Conclusion: The content-aware editing approach of EdiDub outperforms traditional generation or inpainting methods, especially in maintaining complex visual elements while ensuring accurate lip synchronization.

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [245] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/abs/2505.23444)
*Runmin Jiang,Genpei Zhang,Yuntian Yang,Siqi Wu,Yuheng Zhang,Wanyue Feng,Yizhou Zhao,Xi Xiao,Xiao Wang,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: CryoCCD是一种新的合成框架，结合了生物物理建模与生成技术，可以产生反映真实生物物理变异性的多尺度cryo-EM显微照片，并通过条件扩散模型生成现实噪声，增强下游任务表现。


<details>
  <summary>Details</summary>
Motivation: cryo-EM提供接近原子分辨率的成像，但下游分析模型的发展受到高质量注释数据稀缺的限制，现有的合成数据方法无法捕捉生物样本的结构多样性和cryo-EM成像中的复杂噪声。

Method: 提出CryoCCD合成框架，整合生物物理建模与生成技术，生成具有组成异质性、细胞环境和物理信息成像的多尺度cryo-EM显微照片；使用条件扩散模型生成现实噪声，并通过循环一致性保持结构保真度，通过掩码感知对比学习捕获空间自适应噪声模式。

Result: 广泛的实验表明，CryoCCD生成结构准确的显微照片，在下游任务中表现出色，超越了现有最先进的基线方法，特别是在颗粒挑选和重建方面。

Conclusion: CryoCCD为cryo-EM图像的合成提供了更真实的解决方案，提升了下游任务的表现，解决了现有方法在结构多样性和噪声模拟方面的不足。

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [246] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: The paper explores rule-based reinforcement learning (RL) for multimodal large language models using jigsaw puzzles. It finds that MLLMs can achieve near-perfect accuracy in puzzles, generalize to other visual tasks, learn with or without explicit reasoning, exhibit pre-existing complex reasoning patterns, and that RL generalizes better than Supervised Fine-Tuning.


<details>
  <summary>Details</summary>
Motivation: To understand the application and potential of rule-based reinforcement learning in multimodal large language models, especially for perception-heavy tasks, by using jigsaw puzzles as an experimental framework.

Method: Using jigsaw puzzles, the study applies rule-based visual reinforcement learning to multimodal large language models, examining their performance, generalization capabilities, and reasoning patterns.

Result: MLLMs achieved near-perfect accuracy on jigsaw puzzles, generalized to complex unseen configurations and other visual tasks, exhibited both explicit and implicit learning, showed pre-existing complex reasoning patterns, and demonstrated that RL has more effective generalization than SFT.

Conclusion: Rule-based visual reinforcement learning shows promise for multimodal models, with findings indicating superior generalization through RL compared to SFT. However, results may vary across different visual tasks.

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [247] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/abs/2505.23595)
*Youssef Mohamed,Noran Mohamed,Khaled Abouhashad,Feilong Tang,Sara Atito,Shoaib Jameel,Imran Razzak,Ahmed B. Zaky*

Main category: cs.CV

TL;DR: The paper presents DeepChest, a dynamic task-weighting framework for multi-label chest X-ray classification that improves upon existing MTL methods in terms of accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively balancing task contributions in Multi-Task Learning (MTL), especially within complex domains like medical imaging.

Method: DeepChest leverages a performance-driven weighting mechanism based on effective analysis of task-specific loss trends. It adaptively adjusts task importance without needing gradient access, reducing memory usage and increasing training speed.

Result: Extensive experiments show that DeepChest outperforms state-of-the-art MTL methods by 7% in overall accuracy and yields substantial reductions in individual task losses.

Conclusion: DeepChest offers an efficient and effective solution for multi-label chest X-ray classification, improving both performance and generalization while mitigating negative transfer.

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [248] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Main category: cs.CV

TL;DR: TrajViT，一种基于对比学习训练的视频编码器，通过提取物体轨迹并将其转换为语义上有意义的标记，显著减少了冗余同时保持了时间连贯性。在多个视频理解基准测试中，其性能显著优于时空ViT（ViT3D）。此外，在作为现代VideoLLM的视频编码器时，TrajViT比ViT3D表现出更高的性能、更快的训练时间和更少的推理FLOPs。


<details>
  <summary>Details</summary>
Motivation: 当前对长视频进行有效标记化的方法存在过多标记和计算效率低下的问题，尤其是当摄像机移动时，最佳的标记减少策略会降低性能且几乎无法减少标记数量。因此需要一种新的方法来解决这些问题。

Method: 引入了基于全景子对象轨迹而不是固定补丁组织标记的地视频标记化方法，并提出了TrajViT视频编码器，该编码器提取物体轨迹并将其转换为语义上有意义的标记，从而显著减少冗余并保持时间连贯性。

Result: 在视频-文本检索任务中，TrajViT比ViT3D高出6%的前5名召回率，并减少了10倍的标记数量；作为现代VideoLLM的视频编码器，TrajViT在6个VideoQA基准测试中平均提高了5.2%的性能，同时训练时间快4倍，推理FLOPs减少了18倍。

Conclusion: TrajViT是第一个在各种视频分析任务中始终优于ViT3D的有效编码器，成为强大且可扩展的解决方案。

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [249] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/abs/2505.23637)
*Dashti A. Ali,Richard K. G. Do,William R. Jarnagin,Aras T. Asaad,Amber L. Simpson*

Main category: cs.CV

TL;DR: In medical image analysis, persistent homology (PH) is used to extract robust topological and geometrical features. The study compares two methods of constructing topological feature vectors from PH barcodes across diverse medical imaging datasets. Results show that feature concatenation preserves detailed information and leads to better classification performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of traditional feature extraction approaches which are sensitive to small changes in input data, Persistent Homology (PH) offers a more robust and stable method for extracting features from medical images.

Method: The method involves using PH to store topological and geometrical features as persistence barcodes. Two approaches are compared for constructing the final topological feature vector: aggregating persistence barcodes followed by featurization or concatenating topological feature vectors derived from each barcode.

Result: Feature concatenation preserves detailed topological information from individual barcodes and yields better classification performance compared to aggregation.

Conclusion: Feature concatenation is a preferred approach when constructing topological feature vectors from multiple persistence barcodes for medical image analysis.

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [250] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Main category: cs.CV

TL;DR: The paper introduces VF-Eval, a new benchmark for evaluating MLLMs' abilities on AI-generated content videos through four tasks. It evaluates 13 frontier MLLMs and finds that even the best model struggles with consistently good performance across all tasks.


<details>
  <summary>Details</summary>
Motivation: Existing assessments of MLLMs for video question answering mainly focus on natural videos, while neglecting synthetic or AI-generated content (AIGC) videos. Also, the capabilities of MLLMs in interpreting AIGC videos are largely unexplored.

Method: Propose a benchmark named VF-Eval which includes four tasks - coherence validation, error awareness, error type detection, and reasoning evaluation to evaluate MLLMs' abilities on AIGC videos. Evaluate 13 frontier MLLMs using this benchmark and conduct an experiment RePrompt to investigate practical applications of VF-Eval in improving video generation.

Result: Even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks in VF-Eval, highlighting the challenging nature of the benchmark. The RePrompt experiment shows aligning MLLMs more closely with human feedback can benefit video generation.

Conclusion: VF-Eval is a challenging benchmark for evaluating MLLMs on AIGC videos and has potential practical applications in improving video generation by aligning MLLMs more closely with human feedback.

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [251] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/abs/2505.23704)
*Mohamad Alansari,Sajid Javed,Iyyakutti Iyappan Ganapathi,Sara Alansari,Muzammal Naseer*

Main category: cs.CV

TL;DR: CLDTracker is a new framework that improves VOT by using rich textual and visual information from VLMs, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: VOT is challenging due to dynamic appearance changes, occlusions, and background clutter. Traditional trackers relying on visual cues struggle in complex scenarios. Although VLMs show promise in semantic understanding, their direct application to VOT is limited by lack of rich textual representation, inefficient fusion mechanisms, and lack of temporal modeling.

Method: Propose CLDTracker with a dual-branch architecture consisting of textual and visual branches. In the textual branch, construct a rich bag of textual descriptions using VLMs like CLIP and GPT-4V, enriched with semantic and contextual cues.

Result: Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves state-of-the-art performance.

Conclusion: Leveraging robust and temporally-adaptive vision-language representations can significantly improve tracking performance.

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [252] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/abs/2505.23709)
*Dionysis Christopoulos,Sotiris Spanos,Eirini Baltzi,Valsamis Ntouskos,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: The paper introduces SLIMP, a pre-training method that uses nested contrastive learning to combine skin lesion images and metadata, improving classification performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Melanoma detection and skin lesion classification based solely on images is challenging due to variations in imaging conditions and lack of clinical context. Clinicians use a holistic approach considering medical history and appearance of other lesions.

Method: SLIMP uses a novel nested contrastive learning approach to capture relationships between images and metadata, combining individual skin lesion data with patient-level metadata.

Result: The proposed pre-training strategy shows improved performance on downstream skin lesion classification tasks compared to other pre-training strategies.

Conclusion: By exploiting all available data modalities, SLIMP demonstrates the quality of learned representations in skin lesion classification tasks.

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [253] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/abs/2505.22869)
*Junbo Yin,Chao Zha,Wenjia He,Chencheng Xu,Xin Gao*

Main category: cs.CV

TL;DR: CFP-Gen is a novel diffusion language model that integrates multimodal conditions with functional, sequence, and structural constraints for de novo protein design.


<details>
  <summary>Details</summary>
Motivation: Existing PLMs can only generate protein sequences based on a single-condition constraint from a specific modality, unable to simultaneously satisfy multiple constraints across different modalities.

Method: Introduced CFP-Gen, which includes an Annotation-Guided Feature Modulation (AGFM) module to adjust protein feature distribution based on composable functional annotations and a Residue-Controlled Functional Encoding (RCFE) module to capture residue-wise interaction. Also, off-the-shelf 3D structure encoders can be integrated to impose geometric constraints.

Result: CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins and achieves a high success rate in designing multifunctional proteins.

Conclusion: CFP-Gen facilitates the de novo protein design by integrating multimodal conditions with functional, sequence, and structural constraints.

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [254] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: 本研究提出了一种多模态CAD重建模型，结合视觉-语言模型（VLM）和大型语言模型（LLM）训练范式，采用两阶段管道：监督微调（SFT）和强化学习（RL）微调。该模型在DeepCAD基准测试中超越了现有的单模态方法，并在三个具有挑战性的数据集上达到了新的最先进的水平。


<details>
  <summary>Details</summary>
Motivation: 现有的CAD重建方法通常专注于单一的输入模态，如点云、图像或文本，这限制了它们的通用性和鲁棒性。为了克服这些限制，本研究利用了最近在视觉-语言模型方面的进展，提出了一个可以同时处理所有三种输入模态的多模态CAD重建模型。

Method: 研究采用了两阶段的训练管道：首先是在大规模程序生成的数据上进行监督微调（SFT），然后是使用在线反馈进行强化学习（RL）微调。此外，研究还首次探索了用于CAD任务的LLM的RL微调，证明了在线RL算法（如Group Relative Preference Optimization (GRPO)）比离线替代方案表现更好。

Result: 在DeepCAD基准测试中，SFT模型在所有三种输入模ality上都优于现有的单模态方法。更重要的是，经过RL微调后，cadrille在包括现实世界数据集在内的三个具有挑战性的数据集上设立了新的最先进水平。

Conclusion: 提出的多模态CAD重建模型通过结合VLM和LLM训练范式，显著提高了CAD重建任务的性能，尤其是在多种输入模态下的应用。该模型在多个基准测试和实际数据集上展示了优越的性能，为未来的CAD技术发展提供了新的方向。

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [255] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/abs/2505.23742)
*Yufan Deng,Xun Guo,Yuanyang Yin,Jacob Zhiyuan Fang,Yiding Yang,Yizhi Wang,Shenghai Yuan,Angtian Wang,Bo Liu,Haibin Huang,Chongyang Ma*

Main category: cs.CV

TL;DR: MAGREF是一个用于任意参考视频生成的统一框架，通过引入掩码引导实现连贯的多主体视频合成。它包含区域感知动态掩码机制和像素级通道连接机制，能够灵活处理各种主体推理，并在不改变架构的情况下保持外观特征。实验表明，MAGREF在多主体视频生成质量上达到最先进的水平，并且超越了现有的开源和商业基线模型。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散模型的深度生成模型在视频生成领域取得了显著进展，但基于多个参考主体的视频生成仍然面临多主体一致性和高质量生成的主要挑战。

Method: MAGREF提出了一种统一框架，包括：1) 区域感知动态掩码机制，允许单个模型灵活处理人类、物体和背景等各种主体推理；2) 像素级通道连接机制，在通道维度上操作以更好地保留外观特征。这些方法使得模型可以从单主体训练推广到复杂的多主体场景。

Result: MAGREF在多主体视频生成的质量上达到了最先进的水平，能够从单主体训练推广到复杂场景，同时保持连贯的合成和对个体主体的精确控制。广泛的实验验证了该方法的有效性。

Conclusion: MAGREF为可扩展、可控和高保真的多主体视频合成铺平了道路，代码和模型已公开发布。

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [256] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/abs/2505.23744)
*Qiang Wang,Xiang Song,Yuhang He,Jizhou Han,Chenhao Ding,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: SOYO is a lightweight framework for Parameter-Isolation Domain Incremental Learning (PIDIL) that improves domain selection with a Gaussian Mixture Compressor, Domain Feature Resampler, and Multi-level Domain Feature Fusion Network. It outperforms existing methods in tasks like image classification, object detection, and speech enhancement.


<details>
  <summary>Details</summary>
Motivation: Existing PIDIL methods have challenges with parameter selection accuracy, particularly as the number of domains and classes increases.

Method: SOYO introduces a Gaussian Mixture Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior domain data efficiently, while a Multi-level Domain Feature Fusion Network (MDFN) enhances domain feature extraction.

Result: SOYO demonstrates consistent superiority over existing baselines across six benchmarks, showcasing its robustness and adaptability in complex, evolving environments.

Conclusion: The authors propose SOYO, a lightweight framework that addresses the limitations of current PIDIL methods by improving domain selection accuracy and supporting multiple Parameter-Efficient Fine-Tuning (PEFT) methods.

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [257] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745)
*Hao Dong,Moru Liu,Jian Liang,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) are great at connecting pictures and words but sometimes make confident mistakes. This paper introduces TrustVLM, a framework that helps figure out when VLM predictions can be trusted without retraining. It improves error detection by using a new scoring method based on image embedding space.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitation of Vision-Language Models (VLMs) in making confident yet incorrect predictions, which poses risks in safety-critical applications.

Method: The method involves introducing TrustVLM, a training-free framework that proposes a novel confidence-scoring function leveraging the image embedding space to improve misclassification detection.

Result: TrustVLM was rigorously evaluated across 17 datasets with 4 architectures and 2 VLMs, showing state-of-the-art performance with improvements up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to existing baselines.

Conclusion: TrustVLM enhances the reliability of VLMs without requiring retraining, facilitating safer deployment in real-world scenarios.

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [258] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/abs/2505.23747)
*Diankun Wu,Fangfu Liu,Yi-Hsin Hung,Yueqi Duan*

Main category: cs.CV

TL;DR: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks, but spatial intelligence remains a challenge. This paper presents Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. It proposes a dual-encoder architecture and a space-aware frame sampling strategy. Additionally, it constructs the Spatial-MLLM-120k dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve the spatial intelligence of MLLMs without relying on additional 3D or 2.5D data, making them more useful in scenarios with only 2D inputs like images or videos.

Method: The method involves proposing a dual-encoder architecture consisting of a pretrained 2D visual encoder for semantic features and a spatial encoder initialized from the backbone of a visual geometry model for 3D structure features. A connector integrates these features into unified visual tokens. Also, a space-aware frame sampling strategy is proposed to select spatially informative frames during inference.

Result: Extensive experiments on various real-world datasets show that the Spatial-MLLM achieves state-of-the-art performance in many visual-based spatial understanding and reasoning tasks.

Conclusion: This paper concludes by presenting Spatial-MLLM, a novel framework that enhances spatial understanding using purely 2D observations. The model shows superior performance in several spatial reasoning tasks.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [259] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/abs/2505.23271)
*Mao-Lin Luo,Zi-Hao Zhou,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: LADA (Label-specific ADApter) is introduced to improve continual learning with vision-language models by appending lightweight, label-specific memory units to the frozen CLIP image encoder and employing feature distillation for seen classes.


<details>
  <summary>Details</summary>
Motivation: Existing CLIP-based methods that adapt the pre-trained image encoder by adding multiple sets of learnable parameters may degrade performance due to errors in selecting the expected parameters for input images during inference.

Method: LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder, enabling discriminative feature generation by aggregating task-agnostic knowledge. It also employs feature distillation for seen classes to prevent catastrophic forgetting.

Result: Extensive results show that LADA achieves state-of-the-art performance in continual learning settings.

Conclusion: LADA effectively addresses the problem of parameter selection errors in CLIP-based continual learning methods, achieving superior performance while preventing catastrophic forgetting.

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [260] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/abs/2505.23475)
*Ron Shapira Weber,Shahar Ben Ishay,Andrey Lavrinenko,Shahaf E. Finder,Oren Freifeld*

Main category: cs.CV

TL;DR: TimePoint is a self-supervised method that accelerates DTW-based alignment, improves accuracy, and generalizes well to real-world time series.


<details>
  <summary>Details</summary>
Motivation: Fast and scalable alignment of time series is crucial in many domains, but standard Dynamic Time Warping (DTW) struggles with scalability and noise sensitivity.

Method: TimePoint leverages efficient 1D diffeomorphisms to generate realistic training data and uses fully convolutional and wavelet convolutional architectures to extract keypoints and descriptors. It applies DTW to these sparse representations for faster and more accurate alignments.

Result: TimePoint consistently achieves faster and more accurate alignments than standard DTW in extensive experiments, demonstrating strong generalization to real-world time series when trained solely on synthetic data.

Conclusion: TimePoint offers a scalable solution for time-series analysis by accelerating DTW-based alignment and improving alignment accuracy.

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [261] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/abs/2505.23522)
*Fengxiang Wang,Mingshuo Chen,Xuming He,YiFan Zhang,Feng Liu,Zijie Guo,Zhenghao Hu,Jiong Wang,Jingyi Xu,Zhangrui Li,Fenghua Ling,Ben Fei,Weijia Li,Long Lan,Wenjing Yang,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: 现有的地球科学多模态学习基准在系统覆盖地球系统组件和跨球体交互方面存在关键限制，通常局限于孤立的子系统。为了解决这些差距，我们引入了OmniEarth-Bench，这是第一个涵盖所有六个地球科学球体和跨球体的综合多模态基准，具有由专家策划的一百个评估维度。通过卫星传感器和现场测量的观测数据，OmniEarth-Bench集成了29,779个注释，跨越四个层次：感知、一般推理、科学知识推理和连贯推理。实验表明，最先进的模型在我们的基准上表现不佳，准确率不到35％。特别是在一些跨球体任务中，像GPT-4o这样的领先模型的性能下降到0.0％。OmniEarth-Bench为地理系统感知AI设定了新标准，推动了科学发现和环境监测及灾难预测的实际应用。


<details>
  <summary>Details</summary>
Motivation: 现有的地球科学多模态学习基准在系统覆盖地球系统组件和跨球体交互方面存在关键限制，通常局限于孤立的子系统。为了全面解决这些问题并促进相关领域的进步，需要一个更全面的多模态基准。

Method: 介绍了一个名为OmniEarth-Bench的全新多模态基准，涵盖了所有六个地球科学球体和跨球体，并包含由专家策划的一百个评估维度。该基准利用卫星传感器和现场测量的观测数据，整合了29,779个注释，跨越四个层次：感知、一般推理、科学知识推理和连贯推理。此外，通过混合专家-群众工作流程验证OmniEarth-Bench以减少标签歧义。

Result: 对9个最先进的多模态语言模型进行实验，结果显示即使是最先进的模型在OmniEarth-Bench基准上的表现也不佳，准确率低于35％。特别地，在某些跨球体任务中，像GPT-4o这样的领先模型的性能下降到0.0％。

Conclusion: OmniEarth-Bench为地理系统感知AI设定了新标准，推动了科学发现和实际应用的进步，例如环境监测和灾难预测。并且发布了相关的数据集、源代码和训练模型。

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [262] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)
*Xu Chu,Xinrong Chen,Guanyu Wang,Zhijie Tan,Kui Huang,Wenyu Lv,Tong Mo,Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LA is a new VLRM that mitigates hallucinations by incorporating vision-text reflection process, using BRPO to decide when to reflect and Visual Token COPY/ROUTE to re-attention visual information.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of long reasoning diluting visual tokens in VLMs which causes less attention on visual information and may trigger hallucinations.

Method: Propose Qwen-LA with vision-text reflection process, introduce BRPO for deciding reflections, prove VLRMs lose visual token attention over reasoning, use Visual Token COPY and ROUTE during training and inference.

Result: Experiments show Qwen-LA achieves leading accuracy performance while reducing hallucinations on multiple visual QA datasets.

Conclusion: Qwen-LA successfully mitigates hallucinations in VLRMs through its innovative methods.

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [263] [Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats](https://arxiv.org/abs/2505.23706)
*Utku Demir,Yalin E. Sagduyu,Tugba Erpek,Hossein Jafari,Sastry Kompella,Mengran Xue*

Main category: cs.NI

TL;DR: 在车联网中，分布式联邦学习（DFL）通过车辆间协作训练深度学习模型来提升安全消息分类的准确性，尤其对个体准确率低的车辆效果显著。研究还揭示了DFL在多域攻击下的脆弱性，强调需要更稳健的安全策略。


<details>
  <summary>Details</summary>
Motivation: 传统的集中式数据收集或纯本地训练方法在处理车联网中的大规模、高移动性和异构数据分布时存在局限性，难以有效检测恶意或异常行为。

Method: 采用分布式联邦学习（DFL），让车辆通过与一跳邻居交换模型更新并在多跳中传播模型来协作训练深度学习模型。使用Vehicular Reference Misbehavior (VeReMi) Extension Dataset进行实验验证。

Result: 相比仅使用本地数据的学习方式，DFL显著提高了所有车辆的安全消息分类准确性，特别是提升了个体准确率较低的车辆的表现。此外，发现本地训练数据大小和时间变化的网络连通性与模型的整体准确性高度相关。在多域攻击下（如无线干扰和训练数据中毒攻击），DFL表现出一定的脆弱性。

Conclusion: DFL能够有效提升车联网中安全消息分类的性能，但面对多域攻击时存在脆弱性，需开发更强大的安全策略以保护DFL系统。

Abstract: In connected and autonomous vehicles, machine learning for safety message
classification has become critical for detecting malicious or anomalous
behavior. However, conventional approaches that rely on centralized data
collection or purely local training face limitations due to the large scale,
high mobility, and heterogeneous data distributions inherent in inter-vehicle
networks. To overcome these challenges, this paper explores Distributed
Federated Learning (DFL), whereby vehicles collaboratively train deep learning
models by exchanging model updates among one-hop neighbors and propagating
models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)
Extension Dataset, we show that DFL can significantly improve classification
accuracy across all vehicles compared to learning strictly with local data.
Notably, vehicles with low individual accuracy see substantial accuracy gains
through DFL, illustrating the benefit of knowledge sharing across the network.
We further show that local training data size and time-varying network
connectivity correlate strongly with the model's overall accuracy. We
investigate DFL's resilience and vulnerabilities under attacks in multiple
domains, namely wireless jamming and training data poisoning attacks. Our
results reveal important insights into the vulnerabilities of DFL when
confronted with multi-domain attacks, underlining the need for more robust
strategies to secure DFL in vehicular networks.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [264] [Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games](https://arxiv.org/abs/2505.22781)
*Antonio Ocello,Daniil Tiapkin,Lorenzo Mancini,Mathieu Laurière,Eric Moulines*

Main category: stat.ML

TL;DR: The paper introduces MF-TRPO, an algorithm for finding approximate Nash equilibria in Mean-Field Games, extending TRPO's methodology with theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of computing Nash equilibria in Mean-Field Games by leveraging the strengths of TRPO from reinforcement learning, ensuring stability and robustness in policy optimization.

Method: MF-TRPO is developed by extending TRPO to the Mean-Field Game framework, providing a method to compute approximate Nash equilibria under standard MFG assumptions.

Result: Theoretical analysis establishes convergence guarantees for both the exact algorithm and its sample-based version, including high-probability and finite sample complexity results.

Conclusion: MF-TRPO successfully bridges reinforcement learning techniques with mean-field decision-making, offering a theoretically grounded solution for complex multi-agent problems.

Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel
algorithm designed to compute approximate Nash equilibria for ergodic
Mean-Field Games (MFG) in finite state-action spaces. Building on the
well-established performance of TRPO in the reinforcement learning (RL)
setting, we extend its methodology to the MFG framework, leveraging its
stability and robustness in policy optimization. Under standard assumptions in
the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing
theoretical guarantees on its convergence. Our results cover both the exact
formulation of the algorithm and its sample-based counterpart, where we derive
high-probability guarantees and finite sample complexity. This work advances
MFG optimization by bridging RL techniques with mean-field decision-making,
offering a theoretically grounded approach to solving complex multi-agent
problems.

</details>


### [265] [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)
*Ba-Hien Tran,Van Minh Nguyen*

Main category: stat.ML

TL;DR: The paper presents a new framework to transform LLMs into multi-kernel Boolean parameters and finetunes them directly in the Boolean domain, reducing complexity and outperforming recent quantization methods.


<details>
  <summary>Details</summary>
Motivation: Current weight binarization approaches for LLMs either lead to significant performance loss or introduce substantial complexity.

Method: A novel framework that transforms LLMs into multi-kernel Boolean parameters and finetunes them directly in the Boolean domain, eliminating the need for latent weights.

Result: Outperforms recent ultra low-bit quantization and binarization methods across extensive experiments on a wide range of LLMs.

Conclusion: This method significantly reduces complexity during both finetuning and inference.

Abstract: Weight binarization has emerged as a promising strategy to drastically reduce
the complexity of large language models (LLMs). It is mainly classified into
two approaches: post-training binarization and finetuning with training-aware
binarization methods. The first approach, while having low complexity, leads to
significant loss of information from the original LLMs, resulting in poor
performance. The second approach, on the other hand, relies heavily on
full-precision latent weights for gradient approximation of binary weights,
which not only remains suboptimal but also introduces substantial complexity.
In this paper, we introduce a novel framework that effectively transforms LLMs
into multi-kernel Boolean parameters, for the first time, finetunes them
directly in the Boolean domain, eliminating the need for expensive latent
weights. This significantly reduces complexity during both finetuning and
inference. Through extensive and insightful experiments across a wide range of
LLMs, we demonstrate that our method outperforms recent ultra low-bit
quantization and binarization methods.

</details>


### [266] [Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features](https://arxiv.org/abs/2505.22997)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: This paper introduces the Deep Copula Classifier (DCC), which uses neural network-parameterized copulas to model complex feature dependencies, offering both theoretical guarantees and practical extensions for dependency-aware classification.


<details>
  <summary>Details</summary>
Motivation: Traditional classifiers often assume feature independence or overly simplistic relationships, leading to poor performance when real-world dependencies matter. This motivived the development of DCC, which aims to effectively capture complex dependencies for improved classification.

Method: The DCC separates learning of each feature's marginal distribution from modeling their joint dependence structure using neural network-parameterized copulas. Lightweight neural networks are employed per class to flexibly capture feature interactions.

Result: Theoretical results show that DCC converges to the Bayes-optimal classifier under standard conditions with explicit convergence rates provided. Practical extensions include high-dimensional scalability, semi-supervised learning, and online adaptation.

Conclusion: DCC provides a mathematically grounded and interpretable framework for dependency-aware classification by unifying statistical rigor with the representational power of neural networks.

Abstract: Traditional classifiers often assume feature independence or rely on overly
simplistic relationships, leading to poor performance in settings where
real-world dependencies matter. We introduce the Deep Copula Classifier (DCC),
a generative model that separates the learning of each feature's marginal
distribution from the modeling of their joint dependence structure via neural
network-parameterized copulas. For each class, lightweight neural networks are
used to flexibly and adaptively capture feature interactions, making DCC
particularly effective when classification is driven by complex dependencies.
We establish that DCC converges to the Bayes-optimal classifier under standard
conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for
r-smooth copula densities. Beyond theoretical guarantees, we outline several
practical extensions, including high-dimensional scalability through vine and
factor copula architectures, semi-supervised learning via entropy
regularization, and online adaptation using streaming gradient methods. By
unifying statistical rigor with the representational power of neural networks,
DCC offers a mathematically grounded and interpretable framework for
dependency-aware classification.

</details>


### [267] [JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows](https://arxiv.org/abs/2505.23196)
*Eshant English,Christoph Lippert*

Main category: stat.ML

TL;DR: JAPAN是一种新的符合性预测框架，使用基于密度的符合性评分，能够构建紧凑、适应性强且具有有限样本覆盖率保证的预测区域。相比现有方法，它在多变量回归和预测任务中表现出更好的校准性能和更紧密的预测区域。


<details>
  <summary>Details</summary>
Motivation: 现有的符合性预测方法通常依赖于基于残差的符合性评分，这导致其在处理多模态分布时存在困难，容易产生过于保守的预测区域。因此需要一种新的方法来克服这些限制。

Method: 引入了JAPAN（联合自适应预测区域与归一化流），这是一种使用基于密度的符合性评分的符合性预测框架。通过利用基于流的模型，JAPAN估计预测密度，并通过对估计的密度评分进行阈值处理来构建预测区域。这使得它可以生成紧凑的、可能不相连的、并且适应上下文的区域，同时保留有限样本覆盖率保证。

Result: 理论上证明了JAPAN的有效性，并在多变量回归和预测任务中进行了实证验证，结果表明其具有良好的校准性能和更紧密的预测区域，优于现有的基线方法。此外，还提供了一些扩展以增加框架的灵活性。

Conclusion: JAPAN作为一种新的符合性预测框架，在处理复杂预测分布方面表现出了优越性，能够提供更好的校准和更有效的预测区域，适用于多种任务场景。

Abstract: Conformal prediction provides a model-agnostic framework for uncertainty
quantification with finite-sample validity guarantees, making it an attractive
tool for constructing reliable prediction sets. However, existing approaches
commonly rely on residual-based conformity scores, which impose geometric
constraints and struggle when the underlying distribution is multimodal. In
particular, they tend to produce overly conservative prediction areas centred
around the mean, often failing to capture the true shape of complex predictive
distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction
Areas with Normalising-Flows), a conformal prediction framework that uses
density-based conformity scores. By leveraging flow-based models, JAPAN
estimates the (predictive) density and constructs prediction areas by
thresholding on the estimated density scores, enabling compact, potentially
disjoint, and context-adaptive regions that retain finite-sample coverage
guarantees. We theoretically motivate the efficiency of JAPAN and empirically
validate it across multivariate regression and forecasting tasks, demonstrating
good calibration and tighter prediction areas compared to existing baselines.
We also provide several \emph{extensions} adding flexibility to our proposed
framework.

</details>


### [268] [Stable Thompson Sampling: Valid Inference via Variance Inflation](https://arxiv.org/abs/2505.23260)
*Budhaditya Halder,Shubhayan Pan,Koulik Khamaru*

Main category: stat.ML

TL;DR: In this paper, the authors address the challenge of constructing confidence intervals for model parameters when data is collected via a Thompson Sampling-type algorithm. They propose a variant called Stable Thompson Sampling which inflates the posterior variance by a logarithmic factor, leading to asymptotically normal estimates of arm means while only modestly increasing regret.


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling (TS) is known to be both asymptotically optimal and empirically effective, but its adaptive sampling scheme makes it difficult to construct confidence intervals for model parameters.

Method: The authors propose a variant of TS called Stable Thompson Sampling, where the posterior variance is inflated by a logarithmic factor. This modification allows for the construction of confidence intervals.

Result: This approach leads to asymptotically normal estimates of the arm means, despite the non-i.i.d. nature of the data. The variance inflation increases regret by only a logarithmic factor compared to standard TS.

Conclusion: The results reveal a principled trade-off: by accepting a small increase in regret, one can enable valid statistical inference for adaptive decision-making algorithms.

Abstract: We consider the problem of statistical inference when the data is collected
via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known
to be both asymptotically optimal and empirically effective, its adaptive
sampling scheme poses challenges for constructing confidence intervals for
model parameters. We propose and analyze a variant of TS, called Stable
Thompson Sampling, in which the posterior variance is inflated by a logarithmic
factor. We show that this modification leads to asymptotically normal estimates
of the arm means, despite the non-i.i.d. nature of the data. Importantly, this
statistical benefit comes at a modest cost: the variance inflation increases
regret by only a logarithmic factor compared to standard TS. Our results reveal
a principled trade-off: by paying a small price in regret, one can enable valid
statistical inference for adaptive decision-making algorithms.

</details>


### [269] [The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation](https://arxiv.org/abs/2505.23445)
*Adrien Majka,El-Mahdi El-Mhamdi*

Main category: stat.ML

TL;DR: The paper re-examines Goodhart's law by removing simplifying assumptions and studying the coupling between proxy metrics and intended goals, revealing conditions under which over-optimisation occurs.


<details>
  <summary>Details</summary>
Motivation: To formalize and better understand Goodhart's law by analyzing the relationship between proxy metrics and intended goals without relying on previous simplifying assumptions.

Method: The authors remove the independence assumption and the specific learning paradigm assumption to study how the coupling between the proxy metric and the intended goal impacts Goodhart's law.

Result: In cases with light tailed goal and discrepancy, dependence does not alter the nature of Goodhart's effect. However, in scenarios with a light tailed goal and heavy tailed discrepancy, over-optimization can occur at a rate inversely proportional to the heavy tailedness of the discrepancy.

Conclusion: The findings suggest that understanding the tail properties of the goal and discrepancy is crucial for predicting and managing Goodhart's effect.

Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a
measure becomes a target, it ceases to be a good measure''. As machine learning
models and the optimisation capacity to train them grow, growing empirical
evidence reinforced the belief in the validity of this law without however
being formalised. Recently, a few attempts were made to formalise Goodhart's
law, either by categorising variants of it, or by looking at how optimising a
proxy metric affects the optimisation of an intended goal. In this work, we
alleviate the simplifying independence assumption, made in previous works, and
the assumption on the learning paradigm made in most of them, to study the
effect of the coupling between the proxy metric and the intended goal on
Goodhart's law. Our results show that in the case of light tailed goal and
light tailed discrepancy, dependence does not change the nature of Goodhart's
effect. However, in the light tailed goal and heavy tailed discrepancy case, we
exhibit an example where over-optimisation occurs at a rate inversely
proportional to the heavy tailedness of the discrepancy between the goal and
the metric. %

</details>


### [270] [Learning Parametric Distributions from Samples and Preferences](https://arxiv.org/abs/2505.23557)
*Marc Jourdan,Gizem Yüce,Nicolas Flammarion*

Main category: stat.ML

TL;DR: The paper explores how preference feedback can enhance parameter estimation in continuous parametric distributions, showing that preference-based M-estimators have better asymptotic variance and proposing an estimator with improved error scaling.


<details>
  <summary>Details</summary>
Motivation: Preference feedback has been shown to improve model performance in language modeling. The authors aim to understand under what conditions this type of feedback can improve parameter estimation in continuous parametric distributions.

Method: The learner observes pairs of samples from an unknown distribution along with their relative preferences. Preference-based M-estimators are introduced which achieve better asymptotic variance than sample-only M-estimators. An estimator achieving an estimation error scaling of $\mathcal{O}(1/n)$ is proposed by leveraging hard constraints revealed by deterministic preferences.

Result: Preference-based M-estimators show improved asymptotic variance compared to sample-only M-estimators. The proposed estimator achieves an estimation error scaling of $\mathcal{O}(1/n)$, significantly better than the $\Theta(1/\sqrt{n})$ rate with samples alone. A matching lower bound for this accelerated rate is established.

Conclusion: Preference feedback can lead to significant improvements in parameter estimation for certain classes of continuous parametric distributions. Although assumptions are restrictive, they apply to notable cases such as Gaussian or Laplace distributions.

Abstract: Recent advances in language modeling have underscored the role of preference
feedback in enhancing model performance. This paper investigates the conditions
under which preference feedback improves parameter estimation in classes of
continuous parametric distributions. In our framework, the learner observes
pairs of samples from an unknown distribution along with their relative
preferences depending on the same unknown parameter. We show that
preference-based M-estimators achieve a better asymptotic variance than
sample-only M-estimators, further improved by deterministic preferences.
Leveraging the hard constraints revealed by deterministic preferences, we
propose an estimator achieving an estimation error scaling of
$\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$
rate attainable with samples alone. Next, we establish a lower bound that
matches this accelerated rate; up to dimension and problem-dependent constants.
While the assumptions underpinning our analysis are restrictive, they are
satisfied by notable cases such as Gaussian or Laplace distributions for
preferences based on the log-probability reward.

</details>


### [271] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen,Soham Jana,Christopher A. Metzler,Arian Maleki,Shirin Jalali*

Main category: stat.ML

TL;DR: This paper investigates likelihood-based approaches for multilook coherent imaging, providing a theoretical upper bound on MSE and proposing an enhanced PGD method.


<details>
  <summary>Details</summary>
Motivation: Multilook coherent imaging systems are widely used but lack a strong theoretical foundation, especially regarding the impact of speckle noise.

Method: Theoretical analysis establishes an upper bound on MSE for maximum likelihood estimation. Algorithmically, PGD is enhanced with the Newton-Schulz algorithm and a bagging strategy.

Result: The combination of these techniques achieves state-of-the-art performance in multilook coherent imaging.

Conclusion: This work provides both theoretical insights and practical improvements for likelihood-based methods in multilook coherent imaging.

Abstract: Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


### [272] [Instance-Optimality for Private KL Distribution Estimation](https://arxiv.org/abs/2505.23620)
*Jiayuan Ye,Vitaly Feldman,Kunal Talwar*

Main category: stat.ML

TL;DR: This paper addresses the problem of estimating an unknown discrete distribution p over d symbols, given n i.i.d. samples, focusing on minimizing KL divergence between the true and estimated distributions. It constructs minimax optimal private estimators and studies instance-optimality under natural local neighborhoods.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in improving the estimation accuracy of an unknown discrete distribution by minimizing KL divergence, beyond just achieving minimax optimality to ensure better empirical performance on real-world distributions.

Method: The authors construct minimax optimal private estimators first, then propose algorithms that achieve instance-optimality up to constant factors, with or without differential privacy constraint. These upper bounds rely on (private) variants of the Good-Turing estimator.

Result: They successfully propose algorithms achieving instance-optimality under natural notions of local neighborhood and provide lower bounds using additive local neighborhoods which more accurately capture the difficulty of distribution estimation in KL divergence.

Conclusion: Minimax optimal estimators may not perform well on individual instances, while instance-optimal algorithms can offer better performance for specific distributions, even under a differential privacy constraint.

Abstract: We study the fundamental problem of estimating an unknown discrete
distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the
distribution. We are interested in minimizing the KL divergence between the
true distribution and the algorithm's estimate. We first construct minimax
optimal private estimators. Minimax optimality however fails to shed light on
an algorithm's performance on individual (non-worst-case) instances $p$ and
simple minimax-optimal DP estimators can have poor empirical performance on
real distributions. We then study this problem from an instance-optimality
viewpoint, where the algorithm's error on $p$ is compared to the minimum
achievable estimation error over a small local neighborhood of $p$. Under
natural notions of local neighborhood, we propose algorithms that achieve
instance-optimality up to constant factors, with and without a differential
privacy constraint. Our upper bounds rely on (private) variants of the
Good-Turing estimator. Our lower bounds use additive local neighborhoods that
more precisely captures the hardness of distribution estimation in KL
divergence, compared to ones considered in prior works.

</details>


### [273] [On the Convergence Analysis of Muon](https://arxiv.org/abs/2505.23737)
*Wei Shen,Ruichuan Huang,Minhui Huang,Cong Shen,Jiawei Zhang*

Main category: stat.ML

TL;DR: 研究分析了Muon优化器的收敛速度，并与梯度下降法（GD）进行了比较，发现Muon在神经网络训练中因能利用Hessian矩阵的低秩和近似块对角结构而优于传统优化器。实验结果验证了理论分析。


<details>
  <summary>Details</summary>
Motivation: 尽管Muons优化器在神经网络训练中表现出色，但对其收敛特性和优异性能背后的原因尚缺乏深入理解。因此，需要对其进行理论分析以揭示其优势所在及适用条件。

Method: 作者对Muon优化器进行了全面的收敛速率分析，并将其与梯度下降法进行对比，同时确定了Muon优于GD的具体条件。

Result: 理论分析表明，Muon可以从Hessian矩阵的低秩和近似块对角结构中获益，这种现象在实际神经网络训练中很常见。实验结果支持并验证了这些理论发现。

Conclusion: Muon优化器在特定条件下可以显著超越传统优化器，其优势源于对矩阵参数结构特性的有效利用。这为未来优化器设计提供了理论依据。

Abstract: The majority of parameters in neural networks are naturally represented as
matrices. However, most commonly used optimizers treat these matrix parameters
as flattened vectors during optimization, potentially overlooking their
inherent structural properties. Recently, an optimizer called Muon has been
proposed, specifically designed to optimize matrix-structured parameters.
Extensive empirical evidence shows that Muon can significantly outperform
traditional optimizers when training neural networks. Nonetheless, the
theoretical understanding of Muon's convergence behavior and the reasons behind
its superior performance remain limited. In this work, we present a
comprehensive convergence rate analysis of Muon and its comparison with
Gradient Descent (GD). We further characterize the conditions under which Muon
can outperform GD. Our theoretical results reveal that Muon can benefit from
the low-rank and approximate blockwise diagonal structure of Hessian matrices
-- phenomena widely observed in practical neural network training. Our
experimental results support and corroborate the theoretical findings.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [274] [Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents](https://arxiv.org/abs/2505.22909)
*Cristian Chica,Yinglong Guo,Gilad Lerman*

Main category: econ.GN

TL;DR: 在无限重复博弈中，Q-learning代理学会收取超竞争价格的现象得到了首个理论解释。研究表明，在特定条件下，企业会持续学习收取高于竞争的价格，并引入了一类新的单一记忆子博弈精炼均衡(SPEs)，以支持通过幼稚合谋、严厉触发策略或递增策略实现的行为。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于解释实验观察到的Q-learning代理可能学会收取超竞争价格的现象，尤其是在无限重复博弈中。

Method: 1. 研究基于观察到的利润更新定价策略，而非计算均衡策略。
2. 分析当博弈同时存在一个阶段的纳什均衡价格和合谋使能价格时的情况。
3. 提出Q函数需满足特定不等式，以确保在实验结束时企业学会持续收取超竞争价格。
4. 引入一类新的单一记忆子博弈精炼均衡(SPEs)，并提供条件分析幼稚合谋、严厉触发策略或递增策略的支持性。

Result: 研究结果表明：
- 在特定条件下，企业确实可以学会持续收取超竞争价格。
- 幼稚合谋仅在合谋使能价格为一阶段纳什均衡时构成SPE。
- 严厉触发策略能够支持SPE。

Conclusion: 本研究首次提供了Q-learning代理在无限重复博弈中学会收取超竞争价格的理论解释，并引入了新的单一记忆子博弈精炼均衡(SPEs)概念，揭示了幼稚合谋和严厉触发策略在支持这种行为中的作用。

Abstract: There is growing experimental evidence that $Q$-learning agents may learn to
charge supracompetitive prices. We provide the first theoretical explanation
for this behavior in infinite repeated games. Firms update their pricing
policies based solely on observed profits, without computing equilibrium
strategies. We show that when the game admits both a one-stage Nash equilibrium
price and a collusive-enabling price, and when the $Q$-function satisfies
certain inequalities at the end of experimentation, firms learn to consistently
charge supracompetitive prices. We introduce a new class of one-memory subgame
perfect equilibria (SPEs) and provide conditions under which learned behavior
is supported by naive collusion, grim trigger policies, or increasing
strategies. Naive collusion does not constitute an SPE unless the
collusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger
policies can.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [275] [LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals](https://arxiv.org/abs/2505.23454)
*Yanbin Wang,Xingyu Chen,Yumiao Wang,Xiang Wang,Chuanfei Zang,Guolong Cui,Jiahuan Liu*

Main category: eess.SP

TL;DR: The paper proposes LCB-CV-UNet with a Logarithmic Connect Block and Dual Hybrid Dataset Construction method to handle HDR radar signals, showing improved detection probability with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: To address the performance degradation caused by High Dynamic Range (HDR) radar signals.

Method: Proposed LCB-CV-UNet includes a hardware-efficient Logarithmic Connect Block (LCB) for phase coherence preservation and Dual Hybrid Dataset Construction method for generating semi-synthetic datasets approximating HDR signal scenarios.

Result: Simulation results indicate a 1% improvement in total detection probability with less than 0.9% added computational complexity compared to the baseline. It surpasses the baseline by 5% in the range of 11-13 dB signal-to-noise ratio typical for urban targets.

Conclusion: The real experiment confirms the practicality of the proposed model.

Abstract: We propose the LCB-CV-UNet to tackle performance degradation caused by High
Dynamic Range (HDR) radar signals. Initially, a hardware-efficient,
plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a
phase coherence preserving solution to address the inherent challenges in
handling HDR features. Then, we propose the Dual Hybrid Dataset Construction
method to generate a semi-synthetic dataset, approximating typical HDR signal
scenarios with adjustable target distributions. Simulation results show about
1% total detection probability improvement with under 0.9% computational
complexity added compared with the baseline. Furthermore, it excels 5% over the
baseline at the range in 11-13 dB signal-to-noise ratio typical for urban
targets. Finally, the real experiment validates the practicality of our model.

</details>


### [276] [Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters](https://arxiv.org/abs/2505.22783)
*Charles E. Thornton,Jamie Sloop,Samuel Brown,Aaron Orndorff,William C. Headley,Stephen Young*

Main category: eess.SP

TL;DR: 研究了基于卷积自动编码器的干扰抑制方法在频率调制连续波(FMCW)雷达高度计端到端高度估计中的性能。具体而言，时间卷积网络(TCN)自动编码器通过有效利用接收到信号的时间相关性，相比最小均方(LMS)自适应滤波器提供了更优越的干扰抑制效果。该方法直接作用于接收到的FMCW信号，并探讨了将深度学习应用于宽带FMCW干扰抑制的关键挑战及未来研究方向以提高实时可行性和对任意干扰条件的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了提升FMCW雷达高度计在存在干扰情况下的高度估计性能，探索深度学习方法（特别是卷积自动编码器）在干扰抑制中的应用潜力，并解决现有方法存在的不足。

Method: 采用时间卷积网络(TCN)自动编码器来处理接收到的FMCW信号，通过利用信号的时间相关性实现更有效的干扰抑制，区别于传统的LMS自适应滤波器方法。此外，识别出将深度学习应用于宽带FMCW干扰抑制时所面临的挑战，并提出改进建议。

Result: TCN自动编码器在干扰抑制方面表现出比LMS自适应滤波器更优越的效果，验证了其在提升FMCW雷达高度计高度估计性能上的潜力。同时明确了当前方法在实际应用中的一些局限性。

Conclusion: 基于卷积自动编码器的干扰抑制方法能够显著改善FMCW雷达高度计的高度估计性能，但要实现更好的实时可行性和对任意干扰条件的泛化能力仍需进一步研究。

Abstract: We investigate the end-to-end altitude estimation performance of a
convolutional autoencoder-based interference mitigation approach for
frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we
show that a Temporal Convolutional Network (TCN) autoencoder effectively
exploits temporal correlations in the received signal, providing superior
interference suppression compared to a Least Mean Squares (LMS) adaptive
filter. Unlike existing approaches, the present method operates directly on the
received FMCW signal. Additionally, we identify key challenges in applying deep
learning to wideband FMCW interference mitigation and outline directions for
future research to enhance real-time feasibility and generalization to
arbitrary interference conditions.

</details>


### [277] [Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes](https://arxiv.org/abs/2505.23160)
*Lorenzo Marinucci,Claudio Battiloro,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: This paper introduces a novel adaptive framework for processing dynamic flow signals over simplicial complexes, which extends classical least-mean-squares (LMS) methods to high-order topological domains. It proposes a topological LMS algorithm, optimal edge sampling strategies, an adaptive topology inference method, and a distributed version of the algorithm. The empirical results show that this approach outperforms graph-based LMS methods.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective way to process dynamic flow signals over simplicial complexes by extending classical LMS methods to high-order topological domains.

Method: The paper proposes a topological LMS algorithm based on discrete Hodge theory, strategies to design optimal edge sampling probabilities, an adaptive topology inference method assuming partial knowledge of the complex structure, and a distributed version of the algorithm.

Result: Empirical results on synthetic and real-world traffic data demonstrate that the proposed approach outperforms graph-based LMS methods in both centralized and distributed settings by leveraging higher-order topological features.

Conclusion: The novel adaptive framework for processing dynamic flow signals over simplicial complexes shows superior performance compared to graph-based LMS methods.

Abstract: This paper introduces a novel adaptive framework for processing dynamic flow
signals over simplicial complexes, extending classical least-mean-squares (LMS)
methods to high-order topological domains. Building on discrete Hodge theory,
we present a topological LMS algorithm that efficiently processes streaming
signals observed over time-varying edge subsets. We provide a detailed
stochastic analysis of the algorithm, deriving its stability conditions,
steady-state mean-square-error, and convergence speed, while exploring the
impact of edge sampling on performance. We also propose strategies to design
optimal edge sampling probabilities, minimizing rate while ensuring desired
estimation accuracy. Assuming partial knowledge of the complex structure (e.g.,
the underlying graph), we introduce an adaptive topology inference method that
integrates with the proposed LMS framework. Additionally, we propose a
distributed version of the algorithm and analyze its stability and
mean-square-error properties. Empirical results on synthetic and real-world
traffic data demonstrate that our approach, in both centralized and distributed
settings, outperforms graph-based LMS methods by leveraging higher-order
topological features.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [278] [PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models](https://arxiv.org/abs/2505.22674)
*Pawan Neupane,Jian Liu,Jianlin Cheng*

Main category: q-bio.BM

TL;DR: PSBench是一个包含大规模标注数据集的基准套件，用于评估蛋白质复合体结构预测模型的质量。通过训练和测试GATE方法，证明了PSBench在EMA研究中的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管AI方法如AlphaFold可以预测准确的蛋白质复合体结构模型，但对这些预测模型的质量进行可靠估计（EMA）仍然是一个主要挑战。缺乏大型、多样化和良好标注的数据集是开发有效机器学习EMA方法的关键障碍。

Method: 引入了PSBench，一个基准套件，包括四个大规模标注数据集，来自CASP15和CASP16。PSBench包含超过一百万个结构模型，覆盖广泛的蛋白质序列长度、复杂化学计量、功能类别和建模难度。每个模型都用多方面的质量评分进行标注，并提供了多种评估指标和基线EMA方法。

Result: 使用CASP15数据训练和评估了基于图变换器的EMA方法GATE，在CASP16中盲测时表现优异，排名靠前。

Conclusion: PSBench被证明是推进蛋白质复合体建模中EMA研究的宝贵资源，且已公开发布。

Abstract: Predicting protein complex structures is essential for protein function
analysis, protein design, and drug discovery. While AI methods like AlphaFold
can predict accurate structural models for many protein complexes, reliably
estimating the quality of these predicted models (estimation of model accuracy,
or EMA) for model ranking and selection remains a major challenge. A key
barrier to developing effective machine learning-based EMA methods is the lack
of large, diverse, and well-annotated datasets for training and evaluation. To
address this gap, we introduce PSBench, a benchmark suite comprising four
large-scale, labeled datasets generated during the 15th and 16th community-wide
Critical Assessment of Protein Structure Prediction (CASP15 and CASP16).
PSBench includes over one million structural models covering a wide range of
protein sequence lengths, complex stoichiometries, functional classes, and
modeling difficulties. Each model is annotated with multiple complementary
quality scores at the global, local, and interface levels. PSBench also
provides multiple evaluation metrics and baseline EMA methods to facilitate
rigorous comparisons. To demonstrate PSBench's utility, we trained and
evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE
was blindly tested in CASP16 (2024), where it ranked among the top-performing
EMA methods. These results highlight PSBench as a valuable resource for
advancing EMA research in protein complex modeling. PSBench is publicly
available at: https://github.com/BioinfoMachineLearning/PSBench.

</details>


### [279] [Representing local protein environments with atomistic foundation models](https://arxiv.org/abs/2505.23354)
*Meital Bojan,Sanketh Vedula,Advaith Maddipatla,Nadav Bojan Sellam,Federico Napoli,Paul Schanda,Alex M. Bronstein*

Main category: q-bio.BM

TL;DR: The paper proposes a new representation for local protein environments using intermediate features of atomistic foundation models (AFMs), showing its effectiveness in capturing structural and chemical features, enabling data-driven priors, and achieving state-of-the-art accuracy in NMR chemical shift prediction.


<details>
  <summary>Details</summary>
Motivation: Current representations of local protein environments are insufficient due to the extensive structural and chemical variability, which makes modeling challenging.

Method: A novel representation for local protein environments is proposed, derived from the intermediate features of atomistic foundation models (AFMs).

Result: This embedding captures both local structure and chemical features, exhibits meaningful structure in representation space, and enables a state-of-the-art physics-informed chemical shift predictor.

Conclusion: Atomistic foundation models offer surprising effectiveness for protein modeling beyond traditional molecular simulations, opening new possibilities for constructing functional representations of protein environments.

Abstract: The local structure of a protein strongly impacts its function and
interactions with other molecules. Therefore, a concise, informative
representation of a local protein environment is essential for modeling and
designing proteins and biomolecular interactions. However, these environments'
extensive structural and chemical variability makes them challenging to model,
and such representations remain under-explored. In this work, we propose a
novel representation for a local protein environment derived from the
intermediate features of atomistic foundation models (AFMs). We demonstrate
that this embedding effectively captures both local structure (e.g., secondary
motifs), and chemical features (e.g., amino-acid identity and protonation
state). We further show that the AFM-derived representation space exhibits
meaningful structure, enabling the construction of data-driven priors over the
distribution of biomolecular environments. Finally, in the context of
biomolecular NMR spectroscopy, we demonstrate that the proposed representations
enable a first-of-its-kind physics-informed chemical shift predictor that
achieves state-of-the-art accuracy. Our results demonstrate the surprising
effectiveness of atomistic foundation models and their emergent representations
for protein modeling beyond traditional molecular simulations. We believe this
will open new lines of work in constructing effective functional
representations for protein environments.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [280] [Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion](https://arxiv.org/abs/2505.22673)
*Wasif Khan,Kyle B. See,Simon Kato,Ziqian Huang,Amy Lazarte,Kyle Douglas,Xiangyang Lou,Teng J. Peng,Dhanashree Rajderkar,John Rees,Pina Sanelli,Amita Singh,Ibrahim Tuna,Christina A. Wilson,Ruogu Fang*

Main category: q-bio.TO

TL;DR: A new deep learning framework called MAGIC is proposed to generate contrast-free CTP imaging maps from non-contrast CT images, reducing costs and risks associated with traditional CTP while maintaining diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: CTP imaging is crucial for stroke treatment but involves expensive contrast agents that can cause allergic reactions and side effects.

Method: MAGIC framework combines generative AI and physiological information to map non-contrast CT images to multiple CTP maps, incorporating physiological characteristics into loss terms for enhanced image fidelity.

Result: MAGIC demonstrated robustness to brain perfusion abnormalities and was validated by experienced neuroradiologists and vascular neurologists, showing favorable performance compared to clinical perfusion imaging.

Conclusion: MAGIC offers a contrast-free, cost-effective, and rapid alternative for perfusion imaging, holding promise for revolutionizing healthcare.

Abstract: Perfusion imaging is extensively utilized to assess hemodynamic status and
tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging
plays a key role in the early assessment and planning of stroke treatment.
While CTP provides essential perfusion parameters to identify abnormal blood
flow in the brain, the use of contrast agents in CTP can lead to allergic
reactions and adverse side effects, along with costing USD 4.9 billion
worldwide in 2022. To address these challenges, we propose a novel deep
learning framework called Multitask Automated Generation of Intermodal CT
perfusion maps (MAGIC). This framework combines generative artificial
intelligence and physiological information to map non-contrast computed
tomography (CT) imaging to multiple contrast-free CTP imaging maps. We
demonstrate enhanced image fidelity by incorporating physiological
characteristics into the loss terms. Our network was trained and validated
using CT image data from patients referred for stroke at UF Health and
demonstrated robustness to abnormalities in brain perfusion activity. A
double-blinded study was conducted involving seven experienced
neuroradiologists and vascular neurologists. This study validated MAGIC's
visual quality and diagnostic accuracy showing favorable performance compared
to clinical perfusion imaging with intravenous contrast injection. Overall,
MAGIC holds great promise in revolutionizing healthcare by offering
contrast-free, cost-effective, and rapid perfusion imaging.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [281] [A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors](https://arxiv.org/abs/2505.23344)
*Jakub Martinka,Lina Zhang,Yi-Fan Hou,Mikołaj Martyka,Jiří Pittner,Mario Barbatti,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: 通过设计特定的非绝热耦合（NAC）描述符和新的ML相位校正程序，首次实现了对NACs的学习，其准确度达到$R^2$超过0.99。该方法在FSSH模拟中表现出高效性和稳健性，并已整合到开源软件MLatom中。


<details>
  <summary>Details</summary>
Motivation: 非绝热耦合（NACs）在光化学和光物理过程建模中具有重要作用，但因其向量性和双值特性以及锥形交叉附近的奇异性，难以用机器学习方法进行建模。

Method: 设计了基于领域知识的NAC特定描述符，并引入了一种新的ML相位校正程序，以解决NACs的学习难题。

Result: 在fulvene的FSSH模拟中，该方法成功地描述了$S_1$态的衰变过程，同时通过增加轨迹集合显著降低了误差。

Conclusion: 所提出的方法能够以高精度学习NACs，并为加速模拟提供了有效途径，其实施已在开源软件MLatom中提供。

Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical
and photophysical processes with methods such as the widely used
fewest-switches surface hopping (FSSH). There is therefore a strong incentive
to machine learn NACs for accelerating simulations. However, this is
challenging due to NACs' vectorial, double-valued character and the singularity
near a conical intersection seam. For the first time, we design NAC-specific
descriptors based on our domain expertise and show that they allow learning
NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to
success is also our new ML phase-correction procedure. We demonstrate the
efficiency and robustness of our approach on a prototypical example of fully
ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic
structure level. This ML-FSSH dynamics leads to an accurate description of
$S_1$ decay while reducing error bars by allowing the execution of a large
ensemble of trajectories. Our implementations are available in open-source
MLatom.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [282] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
*Vladimir Bataev,Andrei Andrusenko,Lilit Grigoryan,Aleksandr Laptev,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: The paper presents NGPU-LM, an efficient and parallelizable approach for statistical n-gram language models in ASR, reducing computational overhead and accuracy gap.


<details>
  <summary>Details</summary>
Motivation: Existing implementations of statistical n-gram language models for context-biasing in ASR lack computational efficiency due to poor parallelization, which makes them less appealing for industrial use.

Method: NGPU-LM rethinks data structures for statistical n-gram language models to enable fast and parallel operations optimized for GPU inference. It introduces customizable greedy decoding compatible with major ASR model types including transducers, attention encoder-decoder models, and CTC.

Result: NGPU-LM achieves less than 7% computational overhead and eliminates more than 50% of the accuracy gap between greedy and beam search in out-of-domain scenarios without significant slowdown caused by beam search.

Conclusion: The implementation of NGPU-LM is open-sourced and provides a computationally efficient solution for context-biasing in ASR.

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>


### [283] [Spoken question answering for visual queries](https://arxiv.org/abs/2505.23308)
*Nimrod Shabtay,Zvi Kons,Avihu Dekel,Hagai Aronowitz,Ron Hoory,Assaf Arbelle*

Main category: eess.AS

TL;DR: This paper explores the creation of a spoken visual question answering (SVQA) system which integrates text, speech, and image modalities to answer spoken questions about images. Due to the absence of a suitable dataset, they synthesize VQA datasets using zero-shot TTS models. Results show that models trained with synthesized speech perform almost as well as those trained on textual QAs.


<details>
  <summary>Details</summary>
Motivation: To develop a system enabling user interaction through both speech and images by integrating multiple modalities into a single model capable of handling spoken questions on images.

Method: Fusion of text, speech, and image modalities to create an SVQA model. Synthesis of VQA datasets using two zero-shot Text-to-Speech (TTS) models for training and evaluating the SVQA models.

Result: Models trained only with synthesized speech nearly reach the performance of models trained on textual QAs. The choice of TTS model has minimal impact on accuracy.

Conclusion: A multi-modal SVQA model can be effectively trained using synthesized speech data, demonstrating potential for further development in this area despite the lack of existing datasets.

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


### [284] [DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration](https://arxiv.org/abs/2505.23515)
*Sanberk Serbest,Tijana Stojkovic,Milos Cernak,Andrew Harper*

Main category: eess.AS

TL;DR: A full-band real-time speech enhancement system with GAN-based stochastic regeneration is proposed, which combines predictive and generative models to reduce distortion and over-suppression while maintaining low latency. The system shows improvements in NISQA-MOS metric and highlights the importance of noisy conditioning.


<details>
  <summary>Details</summary>
Motivation: Predictive models may lead to over-suppression by focusing on estimating the mean of the target distribution, thus removing speech content. Combining predictive and generative models within a stochastic regeneration framework can potentially reduce output distortion.

Method: The method involves developing a real-time speech enhancement system using GAN-based stochastic regeneration that incorporates both predictive and generative models. This lightweight architecture has 3.58M parameters and low latency for real-time streaming.

Result: Experiments indicate improvements over the first stage in terms of NISQA-MOS metric. An ablation study demonstrates the significance of noisy conditioning in the system's performance.

Conclusion: The team participated in the 2025 Urgent Challenge with their model and made further improvements, showcasing the effectiveness of their real-time speech enhancement system.

Abstract: In this work, we propose a full-band real-time speech enhancement system with
GAN-based stochastic regeneration. Predictive models focus on estimating the
mean of the target distribution, whereas generative models aim to learn the
full distribution. This behavior of predictive models may lead to
over-suppression, i.e. the removal of speech content. In the literature, it was
shown that combining a predictive model with a generative one within the
stochastic regeneration framework can reduce the distortion in the output. We
use this framework to obtain a real-time speech enhancement system. With 3.58M
parameters and a low latency, our system is designed for real-time streaming
with a lightweight architecture. Experiments show that our system improves over
the first stage in terms of NISQA-MOS metric. Finally, through an ablation
study, we show the importance of noisy conditioning in our system. We
participated in 2025 Urgent Challenge with our model and later made further
improvements.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [285] [CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems](https://arxiv.org/abs/2505.22868)
*Md Hasibul Amin,Mohammadreza Mohammadi,Jason D. Bakos,Ramtin Zand*

Main category: cs.ET

TL;DR: The paper proposes CrossNAS, an automated framework for optimizing machine learning workloads deployment on analog PIM systems through exploring a multidimensional search space across circuits, architecture, and systems. It uses single-path one-shot weight-sharing strategy with evolutionary search, setting a new benchmark in PIM NAS for accuracy and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently deploying machine learning workloads on analog PIM systems by exploring various design abstraction layers.

Method: Proposes CrossNAS framework which incorporates single-path one-shot weight-sharing strategy combined with evolutionary search to optimize mapping and deployment on PIM systems.

Result: CrossNAS outperforms previous methods in both accuracy and energy efficiency while having comparable or shorter search times.

Conclusion: CrossNAS sets a new benchmark for PIM neural architecture search.

Abstract: In this paper, we propose the CrossNAS framework, an automated approach for
exploring a vast, multidimensional search space that spans various design
abstraction layers-circuits, architecture, and systems-to optimize the
deployment of machine learning workloads on analog processing-in-memory (PIM)
systems. CrossNAS leverages the single-path one-shot weight-sharing strategy
combined with the evolutionary search for the first time in the context of PIM
system mapping and optimization. CrossNAS sets a new benchmark for PIM neural
architecture search (NAS), outperforming previous methods in both accuracy and
energy efficiency while maintaining comparable or shorter search times.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [286] [Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side](https://arxiv.org/abs/2505.23733)
*Truong,Luu,Binny M. Samuel*

Main category: cs.CY

TL;DR: The paper explores generative AI's connection to malicious internet activities and cybercrime using psychological frameworks, providing empirical evidence through quasi-experimental design.


<details>
  <summary>Details</summary>
Motivation: To provide empirical evidence regarding the association between generative AI and malicious internet-related activities and cybercrime, as well as address safety, ethical risks, and dual-use concerns in cybersecurity.

Method: Quasi-experimental design with interrupted time series analysis on two datasets (general and cryptocurrency-focused) to assess generative AI's role in cybercrime.

Result: The findings contribute to discussions on AI governance by highlighting the balance between control and innovation, while emphasizing strategies for policymakers, developers, professionals, and public education.

Conclusion: There is a need for strategies that guide policymakers, inform AI developers and cybersecurity professionals, and educate the public to maximize AI benefits while mitigating its risks.

Abstract: In recent years, the rapid advancement and democratization of generative AI
models have sparked significant debate over safety, ethical risks, and dual-use
concerns, particularly in the context of cybersecurity. While anecdotally
known, this paper provides empirical evidence regarding generative AI's
association with malicious internet-related activities and cybercrime by
examining the phenomenon through psychological frameworks of technological
amplification and affordance theory. Using a quasi-experimental design with
interrupted time series analysis, we analyze two datasets, one general and one
cryptocurrency-focused, to empirically assess generative AI's role in
cybercrime. The findings contribute to ongoing discussions about AI governance
by balancing control and fostering innovation, underscoring the need for
strategies to guide policymakers, inform AI developers and cybersecurity
professionals, and educate the public to maximize AI's benefits while
mitigating its risks.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [287] [Towards Explainable Sequential Learning](https://arxiv.org/abs/2505.23624)
*Giacomo Bergami,Emma Packer,Kirsty Scott,Silvia Del Din*

Main category: cs.DB

TL;DR: This paper proposes DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence (EMeriTAte+DF), a hybrid explainable temporal data processing pipeline which combines numerical-driven and event-based temporal data classification using verified AI principles, leading to human-explainable results. It introduces a preliminary a posteriori explainable phase and extends event-based literature for concurrent constituents design specification mining algorithms. The proposed solution outperforms current state-of-the-art multivariate time series classifications.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between numerical-driven and event-based temporal data classification and provide human-explainable results through verified artificial intelligence principles.

Method: Developing a hybrid explainable temporal data processing pipeline called EMeriTAte+DF that includes an a posteriori explainable phase describing numerical input data in terms of concurrent constituents with numerical payloads, and extending event-based literature to support concurrent constituents via design specification mining algorithms.

Result: The previous and current solutions surpass state-of-the-art methods for multivariate time series classifications, proving the effectiveness of the methodology.

Conclusion: The proposed methodology is effective in enhancing multivariate time series classifications and provides human-explainable results.

Abstract: This paper offers a hybrid explainable temporal data processing pipeline,
DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence
(EMeriTAte+DF), bridging numerical-driven temporal data classification with an
event-based one through verified artificial intelligence principles, enabling
human-explainable results. This was possible through a preliminary a posteriori
explainable phase describing the numerical input data in terms of concurrent
constituents with numerical payloads. This further required extending the
event-based literature to design specification mining algorithms supporting
concurrent constituents. Our previous and current solutions outperform
state-of-the-art solutions for multivariate time series classifications, thus
showcasing the effectiveness of the proposed methodology.

</details>


### [288] [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
*Jang-Hyun Kim,Jinuk Kim,Sangwoo Kwon,Jae W. Lee,Sangdoo Yun,Hyun Oh Song*

Main category: cs.DB

TL;DR: The paper introduces KVzip, a query-agnostic KV cache eviction method that effectively reduces memory overhead and latency in transformer-based large language models without significant performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the growing memory overhead and attention latency in transformer-based LLMs as context length increases, by developing an efficient KV cache eviction method.

Method: KVzip quantifies the importance of each KV pair using the LLM to reconstruct original contexts from cached KV pairs, then evicts pairs with lower importance, enabling effective reuse across diverse queries.

Result: KVzip reduces KV cache size by 3-4 times and FlashAttention decoding latency by approximately 2 times, with negligible performance loss across various tasks and models, outperforming existing query-aware methods.

Conclusion: KVzip provides an effective solution for managing KV cache in LLMs, significantly reducing memory usage and latency while maintaining performance.

Abstract: Transformer-based large language models (LLMs) cache context as key-value
(KV) pairs during inference. As context length grows, KV cache sizes expand,
leading to substantial memory overhead and increased attention latency. This
paper introduces KVzip, a query-agnostic KV cache eviction method enabling
effective reuse of compressed KV caches across diverse queries. KVzip
quantifies the importance of a KV pair using the underlying LLM to reconstruct
original contexts from cached KV pairs, subsequently evicting pairs with lower
importance. Extensive empirical evaluations demonstrate that KVzip reduces KV
cache size by 3-4$\times$ and FlashAttention decoding latency by approximately
2$\times$, with negligible performance loss in question-answering, retrieval,
reasoning, and code comprehension tasks. Evaluations include various models
such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching
up to 170K tokens. KVzip significantly outperforms existing query-aware KV
eviction methods, which suffer from performance degradation even at a 90% cache
budget ratio under multi-query scenarios.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [289] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/abs/2505.22814)
*Jonghan Lim,Ilya Kovalenko*

Main category: cs.MA

TL;DR: The paper proposes a large language model-enabled control architecture for multi-agent manufacturing systems, which enhances real-time adaptation and context-aware decision-making, leading to improved system resilience, flexibility, throughput, and resource utilization.


<details>
  <summary>Details</summary>
Motivation: Manufacturing environments are becoming more complex and unpredictable, requiring real-time decision-making and adaptation. Traditional control approaches have limitations in responsiveness, while current multi-agent systems face challenges related to real-time adaptation, context-aware decision-making, and dynamic exploration of resource capabilities.

Method: Introduce a large language model-enabled control architecture for multi-agent manufacturing systems to dynamically explore resource capabilities in response to real-time disruptions.

Result: A simulation-based case study demonstrates that the proposed architecture improves system resilience and flexibility, with findings showing improved throughput and efficient resource utilization compared to existing approaches.

Conclusion: The large language model-enabled control architecture addresses the challenges faced by current multi-agent systems, enhancing their ability to adapt to real-time disruptions and improving overall manufacturing system performance.

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


### [290] [Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.23352)
*Xu Shen,Yixin Liu,Yiwei Dai,Yili Wang,Rui Miao,Yue Tan,Shirui Pan,Xin Wang*

Main category: cs.MA

TL;DR: This paper analyzes the role of communication topology in large language model-based multi-agent systems and proposes a novel design approach, EIB-leanrner, that balances error suppression and beneficial information propagation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why and when sparse and dense topologies help or hinder collaboration in multi-agent systems.

Method: A causal framework is presented to analyze how agent outputs propagate under topologies with varying sparsity. Based on this analysis, the EIB-learner approach is proposed which fuses connectivity patterns from both dense and sparse graphs.

Result: Empirical studies show that moderately sparse topologies achieve optimal task performance. The EIB-learner demonstrates superior effectiveness, communication cost, and robustness.

Conclusion: The EIB-learner provides an effective balance between error suppression and beneficial information propagation in multi-agent systems.

Abstract: The communication topology in large language model-based multi-agent systems
fundamentally governs inter-agent collaboration patterns, critically shaping
both the efficiency and effectiveness of collective decision-making. While
recent studies for communication topology automated design tend to construct
sparse structures for efficiency, they often overlook why and when sparse and
dense topologies help or hinder collaboration. In this paper, we present a
causal framework to analyze how agent outputs, whether correct or erroneous,
propagate under topologies with varying sparsity. Our empirical studies reveal
that moderately sparse topologies, which effectively suppress error propagation
while preserving beneficial information diffusion, typically achieve optimal
task performance. Guided by this insight, we propose a novel topology design
approach, EIB-leanrner, that balances error suppression and beneficial
information propagation by fusing connectivity patterns from both dense and
sparse graphs. Extensive experiments show the superior effectiveness,
communication cost, and robustness of EIB-leanrner.

</details>


### [291] [Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging](https://arxiv.org/abs/2505.23584)
*Sumbal Malik,Majid Khonji,Khaled Elbassioni,Jorge Dias*

Main category: cs.MA

TL;DR: The paper introduces VRP-DR, a new vehicle routing problem using trucks, drones and robots for collaborative delivery. It formulates VRP-DR as MILP and develops FINDER algorithm to solve large-scale instances. Results show significant time and cost savings.


<details>
  <summary>Details</summary>
Motivation: E-commerce growth and demand for cost-effective last-mile delivery drives interest in collaborative logistics.

Method: VRP-DR is formulated as a mixed-integer linear program (MILP) with the goal of minimizing operational costs and makespan. A heuristic algorithm called FINDER is developed to solve large-scale instances efficiently.

Result: Numerical experiments show that combined delivery mode saves significant time compared to truck-only mode and multi-visits lead to substantial cost reductions. En-route charging, docking flexibility, drone count, speed and payload capacity affect system performance.

Conclusion: VRP-DR provides an effective model for collaborative synchronized multi-platform delivery using trucks, drones and robots. The proposed FINDER algorithm can solve large-scale problems effectively.

Abstract: The rapid growth of e-commerce and the increasing demand for timely,
cost-effective last-mile delivery have increased interest in collaborative
logistics. This research introduces a novel collaborative synchronized
multi-platform vehicle routing problem with drones and robots (VRP-DR), where a
fleet of $\mathcal{M}$ trucks, $\mathcal{N}$ drones and $\mathcal{K}$ robots,
cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the
launching, retrieving, and en-route charging of drones and robots, thereby
addressing critical limitations such as restricted payload capacities, limited
range, and battery constraints. The VRP-DR incorporates five realistic
features: (1) multi-visit service per trip, (2) multi-trip operations, (3)
flexible docking, allowing returns to the same or different trucks (4) cyclic
and acyclic operations, enabling return to the same or different nodes; and (5)
en-route charging, enabling drones and robots to recharge while being
transported on the truck, maximizing operational efficiency by utilizing idle
transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)
to minimize both operational costs and makespan. To overcome the computational
challenges of solving large-scale instances, a scalable heuristic algorithm,
FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to
provide efficient, near-optimal solutions. Numerical experiments across various
instance sizes evaluate the performance of the MILP and heuristic approaches in
terms of solution quality and computation time. The results demonstrate
significant time savings of the combined delivery mode over the truck-only mode
and substantial cost reductions from enabling multi-visits. The study also
provides insights into the effects of en-route charging, docking flexibility,
drone count, speed, and payload capacity on system performance.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [292] [Generative Social Choice: The Next Generation](https://arxiv.org/abs/2505.22939)
*Niclas Boehmer,Sara Fish,Ariel D. Procaccia*

Main category: cs.GT

TL;DR: This paper enhances a framework of generative social choice by introducing theoretical guarantees for approximately optimal queries and a budget limit on the slate length, demonstrating its effectiveness with GPT-4o on city improvement and drug review datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a concise set of statements that represent the diversity of user opinions in democratic processes, addressing challenges beyond traditional committee elections by handling an expansive candidate set of possible statements.

Method: The method involves extending the generative social choice framework with theoretical guarantees for approximately optimal queries and a budget constraint on the total length of the slate. This approach leverages large language models like GPT-4o to implement queries.

Result: The approach was successfully demonstrated on datasets related to city improvements and drug reviews, effectively generating representative slates from unstructured user opinions.

Conclusion: This study concludes by showing the enhanced framework's capability to generate proportional representations of user opinions within practical constraints, using advanced language models.

Abstract: A key task in certain democratic processes is to produce a concise slate of
statements that proportionally represents the full spectrum of user opinions.
This task is similar to committee elections, but unlike traditional settings,
the candidate set comprises all possible statements of varying lengths, and so
it can only be accessed through specific queries. Combining social choice and
large language models, prior work has approached this challenge through a
framework of generative social choice. We extend the framework in two
fundamental ways, providing theoretical guarantees even in the face of
approximately optimal queries and a budget limit on the overall length of the
slate. Using GPT-4o to implement queries, we showcase our approach on datasets
related to city improvement measures and drug reviews, demonstrating its
effectiveness in generating representative slates from unstructured user
opinions.

</details>


### [293] [Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals](https://arxiv.org/abs/2505.23124)
*Junyan Liu,Arnab Maiti,Artin Tajdini,Kevin Jamieson,Lillian J. Ratliff*

Main category: cs.GT

TL;DR: 研究了一个在有限时间范围内，具有对抗性顺序到达的多类型代理人的重复主-代理问题。提出了两种可实现次线性遗憾的关键设定，并提供了相应的算法和遗憾下界。此外，还扩展了允许主体同时激励多个臂的结果。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，主体与不同类型代理人交互时面临挑战，例如缺乏对代理人行为的先验知识可能导致线性遗憾。因此，需要探索特定条件下可实现次线性遗憾的设定及相应算法。

Method: 分析了两种设定：1) 主体知道每种类型代理人对激励的贪婪选择；2) 代理人的响应随激励平滑变化且受Lipschitz常数约束。分别设计了对应算法并证明了遗憾上界和匹配的下界。最后，扩展到允许主体同时激励多个臂的情况。

Result: 在第一种设定下，提出了一种遗憾为$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$的算法；在第二种设定下，遗憾为$\tilde{O}((LN)^{1/3}T^{2/3})$。两者均匹配理论下界（忽略对数因子）。

Conclusion: 该研究展示了在特定条件下如何通过策略设计实现次线性遗憾，并为进一步探索复杂主-代理问题提供了理论基础。

Abstract: We initiate the study of a repeated principal-agent problem over a finite
horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of
agents arriving in an adversarial order. At each round, the principal
strategically chooses one of the $N$ arms to incentivize for an arriving agent
of unknown type. The agent then chooses an arm based on its own utility and the
provided incentive, and the principal receives a corresponding reward. The
objective is to minimize regret against the best incentive in hindsight.
Without prior knowledge of agent behavior, we show that the problem becomes
intractable, leading to linear regret. We analyze two key settings where
sublinear regret is achievable. In the first setting, the principal knows the
arm each agent type would select greedily for any given incentive. Under this
setting, we propose an algorithm that achieves a regret bound of
$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to
a $\log K$ factor. In the second setting, an agent's response varies smoothly
with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under
this setting, we show that there is an algorithm with a regret bound of
$\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to
logarithmic factors. Finally, we extend our algorithmic results for both
settings by allowing the principal to incentivize multiple arms simultaneously
in each round.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [294] [Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model](https://arxiv.org/abs/2505.23682)
*Rachel Cummings,Alessandro Epasto,Jieming Mao,Tamalika Mukherjee,Tingting Ou,Peilin Zhong*

Main category: cs.DS

TL;DR: The paper presents the first sublinear space differentially private algorithms for counting distinct elements in turnstile streaming model, which significantly improves upon previous linear space algorithms.


<details>
  <summary>Details</summary>
Motivation: In real-time data analysis applications, both the length of the stream and the size of the universe can be extremely large. This motivates the study of private algorithms in the turnstile setting using space sublinear in both parameters.

Method: The algorithm achieves a (1+η)-relative approximation for all η ∈ (0,1) with ~O_η(T^1/3) space and additive error on arbitrary streams. When a bound W on the number of times an item appears in the stream is known, it provides ~O_η(√W) additive error, using ~O_η(√W) space.

Result: This approach significantly reduces space requirements compared to state-of-the-art methods, approaching the known Ω(T^1/4) additive error lower bound for arbitrary streams. It also matches the additive error of prior work requiring linear space when a bound W is known.

Conclusion: The results address an open question about designing low-memory mechanisms for this problem and are complemented by a space lower bound showing that any similar algorithm must use ~Ω(T^1/3) space on arbitrary streams.

Abstract: The turnstile continual release model of differential privacy captures
scenarios where a privacy-preserving real-time analysis is sought for a dataset
evolving through additions and deletions. In typical applications of real-time
data analysis, both the length of the stream $T$ and the size of the universe
$|U|$ from which data come can be extremely large. This motivates the study of
private algorithms in the turnstile setting using space sublinear in both $T$
and $|U|$. In this paper, we give the first sublinear space differentially
private algorithms for the fundamental problem of counting distinct elements in
the turnstile streaming model. Our algorithm achieves, on arbitrary streams,
$\tilde{O}_{\eta}(T^{1/3})$ space and additive error, and a $(1+\eta)$-relative
approximation for all $\eta \in (0,1)$. Our result significantly improves upon
the space requirements of the state-of-the-art algorithms for this problem,
which is linear, approaching the known $\Omega(T^{1/4})$ additive error lower
bound for arbitrary streams. Moreover, when a bound $W$ on the number of times
an item appears in the stream is known, our algorithm provides
$\tilde{O}_{\eta}(\sqrt{W})$ additive error, using $\tilde{O}_{\eta}(\sqrt{W})$
space. This additive error asymptotically matches that of prior work which
required instead linear space. Our results address an open question posed by
[Jain, Kalemaj, Raskhodnikova, Sivakumar, Smith, Neurips23] about designing
low-memory mechanisms for this problem. We complement these results with a
space lower bound for this problem, which shows that any algorithm that uses
similar techniques must use space $\tilde{\Omega}(T^{1/3})$ on arbitrary
streams.

</details>


### [295] [Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves](https://arxiv.org/abs/2505.23431)
*Amer Krivošija,Alexander Munteanu,André Nusser,Chris Schwiegelshohn*

Main category: cs.DS

TL;DR: The paper introduces a new dissimilarity measure for polygonal curves, $k$-Dynamic Time Warping ($k$-DTW), which has stronger metric properties than DTW and is more robust to outliers than the Fréchet distance. The authors provide exact and approximate algorithms for $k$-DTW, prove dimension-free learning bounds, and demonstrate its advantages in clustering and nearest neighbor classification.


<details>
  <summary>Details</summary>
Motivation: Existing dissimilarity measures for polygonal curves such as DTW and Fréchet distance have limitations: DTW lacks strong metric properties, while Fréchet distance is sensitive to outliers. This motivates the development of a new measure that combines the advantages of both.

Method: The authors introduce $k$-DTW, a novel dissimilarity measure with stronger metric properties than DTW and greater robustness to outliers than the Fréchet distance. They develop an exact algorithm and a $(1+\varepsilon)$-approximation algorithm for computing $k$-DTW based on parametric search for the $k$-th largest matched distance. Additionally, they prove theoretical results on dimension-free learning bounds and Rademacher/Gaussian complexities.

Result: $k$-DTW achieves smaller sample sizes than DTW for learning the median of curves by replacing curve complexity factors with $k$. It also provides strictly smaller bounds on Rademacher and Gaussian complexities than DTW by a factor of $\tilde\Omega(\sqrt{m})$ when $k \ll m$. Experimental results show improved performance in clustering and nearest neighbor classification.

Conclusion: $k$-DTW is a promising new dissimilarity measure for polygonal curves that addresses the limitations of DTW and Fréchet distance. Its theoretical advantages are supported by experimental evidence in practical applications such as clustering and classification.

Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel
dissimilarity measure for polygonal curves. $k$-DTW has stronger metric
properties than Dynamic Time Warping (DTW) and is more robust to outliers than
the Fr\'{e}chet distance, which are the two gold standards of dissimilarity
measures for polygonal curves. We show interesting properties of $k$-DTW and
give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm
for $k$-DTW by a parametric search for the $k$-th largest matched distance. We
prove the first dimension-free learning bounds for curves and further learning
theoretic results. $k$-DTW not only admits smaller sample size than DTW for the
problem of learning the median of curves, where some factors depending on the
curves' complexity $m$ are replaced by $k$, but we also show a surprising
separation on the associated Rademacher and Gaussian complexities: $k$-DTW
admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$
when $k\ll m$. We complement our theoretical findings with an experimental
illustration of the benefits of using $k$-DTW for clustering and nearest
neighbor classification.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [296] [Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints](https://arxiv.org/abs/2505.22889)
*Hamidreza Montazeri Hedesh,Moh Kamalul Wafi,Milad Siami*

Main category: eess.SY

TL;DR: The paper explores local stability of nonlinear Lur'e systems with neural network feedback, proposing two methods to estimate the Region of Attraction and demonstrating improvements over existing techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring stability in nonlinear systems with neural network feedback, specifically focusing on estimating the Region of Attraction (ROA) for such systems.

Method: 1. Use a localized variant of the Aizerman conjecture with positivity system constraints to establish sufficient conditions for exponential stability. 2. Develop two methods for ROA estimation: (i) a Lyapunov-based approach constructing invariant sublevel sets via LMIs, and (ii) a technique computing tight local sector bounds for FFNNs through layer-wise propagation of linear relaxations.

Result: The proposed methods show significant improvements in both the size of the ROA and scalability compared to existing integral quadratic constraint-based approaches, as demonstrated by numerical results.

Conclusion: The study successfully provides new tools to analyze local stability and estimate the ROA for nonlinear Lur'e systems with FFNN feedback, offering less conservative and more scalable solutions than previous methods.

Abstract: We study the local stability of nonlinear systems in the Lur'e form with
static nonlinear feedback realized by feedforward neural networks (FFNNs). By
leveraging positivity system constraints, we employ a localized variant of the
Aizerman conjecture, which provides sufficient conditions for exponential
stability of trajectories confined to a compact set. Using this foundation, we
develop two distinct methods for estimating the Region of Attraction (ROA): (i)
a less conservative Lyapunov-based approach that constructs invariant sublevel
sets of a quadratic function satisfying a linear matrix inequality (LMI), and
(ii) a novel technique for computing tight local sector bounds for FFNNs via
layer-wise propagation of linear relaxations. These bounds are integrated into
the localized Aizerman framework to certify local exponential stability.
Numerical results demonstrate substantial improvements over existing integral
quadratic constraint-based approaches in both ROA size and scalability.

</details>


### [297] [User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model](https://arxiv.org/abs/2505.11047)
*Arghya Mallick,Georgios Pantazis,Mohammad Khosravi,Peyman Mohajerin Esfahani,Sergio Grammatico*

Main category: eess.SY

TL;DR: A data-driven, user-centric V2G methodology using multi-objective optimization is proposed to balance battery degradation and V2G revenue according to EV user preference. ICNNs are used for a data-driven degradation model.


<details>
  <summary>Details</summary>
Motivation: Given the lack of accurate and generalizable battery degradation models, there's a need for a method that can balance financial benefits from V2G participation with battery health degradation based on user preferences.

Method: The method uses input convex neural networks (ICNNs) to create a data-driven degradation model trained on extensive experimental datasets. This model captures nonconvex dependencies while maintaining convexity with respect to charging rate. The second stage integrates this model into a multi-objective optimization framework to generate optimal smart charging profiles for each EV.

Result: Numerical simulations demonstrate high accuracy in predicting battery degradation for unseen data. A trade-off curve is presented showing financial benefits versus battery health losses based on user preferences.

Conclusion: This approach offers a computationally efficient way to optimize V2G participation considering both financial benefits and battery health, controlled by a hyperparameter reflecting user prioritization.

Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology
based on multi-objective optimization to balance battery degradation and V2G
revenue according to EV user preference. Given the lack of accurate and
generalizable battery degradation models, we leverage input convex neural
networks (ICNNs) to develop a data-driven degradation model trained on
extensive experimental datasets. This approach enables our model to capture
nonconvex dependencies on battery temperature and time while maintaining
convexity with respect to the charging rate. Such a partial convexity property
ensures that the second stage of our methodology remains computationally
efficient. In the second stage, we integrate our data-driven degradation model
into a multi-objective optimization framework to generate an optimal smart
charging profile for each EV. This profile effectively balances the trade-off
between financial benefits from V2G participation and battery degradation,
controlled by a hyperparameter reflecting the user prioritization of battery
health. Numerical simulations show the high accuracy of the ICNN model in
predicting battery degradation for unseen data. Finally, we present a trade-off
curve illustrating financial benefits from V2G versus losses from battery
health degradation based on user preferences and showcase smart charging
strategies under realistic scenarios.

</details>


### [298] [From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems](https://arxiv.org/abs/2505.23710)
*Zeinab Nezami,Syed Danial Ali Shah,Maryam Hafeez,Karim Djemame,Syed Ali Raza Zaidi*

Main category: eess.SY

TL;DR: This paper discusses 6G as a self-evolving telecom ecosystem with AI-driven intelligence, exploring key enablers such as reconfigurable infrastructure, adaptive middleware, and intelligent network functions. It emphasizes improved real-time decision-making and efficiency while addressing ethical challenges and providing a technology stack roadmap.


<details>
  <summary>Details</summary>
Motivation: To envision 6G as a self-evolving telecom ecosystem that goes beyond static connectivity through AI-driven intelligence, improving real-time decision-making and optimizing efficiency in networked control systems.

Method: Exploration of autonomous communication systems including reconfigurable infrastructure, adaptive middleware, intelligent network functions, and multi-agent collaboration for distributed decision-making. Alignment with industrial IoT frameworks is also examined.

Result: Potential for enhanced real-time decision-making, increased efficiency, and reduced latency in networked control systems was identified. Ethical challenges, research directions, and standardization efforts were addressed.

Conclusion: The paper concludes by presenting a technology stack roadmap to guide future developments in 6G network management, contributing to the next generation of intelligent automation solutions bridging theory and real-world industrial applications.

Abstract: This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven
intelligence enables dynamic adaptation beyond static connectivity. We explore
the key enablers of autonomous communication systems, spanning reconfigurable
infrastructure, adaptive middleware, and intelligent network functions,
alongside multi-agent collaboration for distributed decision-making. We explore
how these methodologies align with emerging industrial IoT frameworks, ensuring
seamless integration within digital manufacturing processes. Our findings
emphasize the potential for improved real-time decision-making, optimizing
efficiency, and reducing latency in networked control systems. The discussion
addresses ethical challenges, research directions, and standardization efforts,
concluding with a technology stack roadmap to guide future developments. By
leveraging state-of-the-art 6G network management techniques, this research
contributes to the next generation of intelligent automation solutions,
bridging the gap between theoretical advancements and real-world industrial
applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [299] [Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera](https://arxiv.org/abs/2505.22880)
*Xiaoyang Zhan,Shixin Zhou,Qianqian Yang,Yixuan Zhao,Hao Liu,Srinivas Chowdary Ramineni,Kenji Shimada*

Main category: cs.RO

TL;DR: This paper presents a system for autonomous semantic exploration and dense semantic target mapping using a ground robot equipped with a LiDAR-panoramic camera suite. The authors propose a Priority-driven Decoupled Local Sampler to generate local viewpoint sets, enabling multi-view semantic inspection and voxel coverage without unnecessary repetition. They also develop a hierarchical planner for efficient global coverage and a Safe Aggressive Exploration State Machine for ensuring safety during exploration.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal in complex unknown environments.

Method: The authors redefine the task as completing both geometric coverage and semantic viewpoint observation. They manage semantic and geometric viewpoints separately and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. A hierarchical planner is developed for efficient global coverage and a Safe Aggressive Exploration State Machine is proposed to allow aggressive exploration while ensuring safety. The system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms.

Result: Simulation results show that the planner achieves faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.

Conclusion: The proposed system successfully addresses the challenge of balancing high-quality observations and avoiding unnecessary repetition in complex unknown environments, demonstrating its effectiveness in both simulations and real-world experiments.

Abstract: This paper presents a system for autonomous semantic exploration and dense
semantic target mapping of a complex unknown environment using a ground robot
equipped with a LiDAR-panoramic camera suite. Existing approaches often
struggle to balance collecting high-quality observations from multiple view
angles and avoiding unnecessary repetitive traversal. To fill this gap, we
propose a complete system combining mapping and planning. We first redefine the
task as completing both geometric coverage and semantic viewpoint observation.
We then manage semantic and geometric viewpoints separately and propose a novel
Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This
enables explicit multi-view semantic inspection and voxel coverage without
unnecessary repetition. Building on this, we develop a hierarchical planner to
ensure efficient global coverage. In addition, we propose a Safe Aggressive
Exploration State Machine, which allows aggressive exploration behavior while
ensuring the robot's safety. Our system includes a plug-and-play semantic
target mapping module that integrates seamlessly with state-of-the-art SLAM
algorithms for pointcloud-level dense semantic target mapping. We validate our
approach through extensive experiments in both realistic simulations and
complex real-world environments. Simulation results show that our planner
achieves faster exploration and shorter travel distances while guaranteeing a
specified number of multi-view inspections. Real-world experiments further
confirm the system's effectiveness in achieving accurate dense semantic object
mapping of unstructured environments.

</details>


### [300] [VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation](https://arxiv.org/abs/2505.23267)
*Jianlin Ye,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.RO

TL;DR: The paper proposes VLM-RRT, a hybrid path-planning method for UAVs that combines Vision Language Models and Rapidly-exploring Random Trees to improve sampling efficiency and path quality.


<details>
  <summary>Details</summary>
Motivation: Traditional path planning methods like RRT have limitations such as high search space complexity, suboptimal path quality, and slow convergence which are problematic in critical applications like disaster response.

Method: The proposed method, VLM-RRT, integrates the pattern recognition capabilities of Vision Language Models (VLMs) with the path-planning strengths of RRT. VLMs provide initial directional guidance based on environmental snapshots, biasing sampling toward regions more likely to contain feasible paths.

Result: Extensive experiments with various state-of-the-art VLMs demonstrate the effectiveness of VLM-RRT in significantly improving sampling efficiency and path quality.

Conclusion: VLM-RRT is a promising approach to enhance path-planning efficiency for UAVs by combining VLMs and RRT.

Abstract: Path planning is a fundamental capability of autonomous Unmanned Aerial
Vehicles (UAVs), enabling them to efficiently navigate toward a target region
or explore complex environments while avoiding obstacles. Traditional
pathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven
effective but often encounter significant challenges. These include high search
space complexity, suboptimal path quality, and slow convergence, issues that
are particularly problematic in high-stakes applications like disaster
response, where rapid and efficient planning is critical. To address these
limitations and enhance path-planning efficiency, we propose Vision Language
Model RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition
capabilities of Vision Language Models (VLMs) with the path-planning strengths
of RRT. By leveraging VLMs to provide initial directional guidance based on
environmental snapshots, our method biases sampling toward regions more likely
to contain feasible paths, significantly improving sampling efficiency and path
quality. Extensive quantitative and qualitative experiments with various
state-of-the-art VLMs demonstrate the effectiveness of this proposed approach.

</details>


### [301] [A Robot-Assisted Approach to Small Talk Training for Adults with ASD](https://arxiv.org/abs/2505.23508)
*Rebecca Ramnauth,Dražen Brščić,Brian Scassellati*

Main category: cs.RO

TL;DR: A robot system helps adults with ASD practice small talk, showing enjoyment and improvement in conversation initiation and eye contact.


<details>
  <summary>Details</summary>
Motivation: Small talk is crucial for social integration, relationship building, and accessing professional opportunities, yet it poses challenges for adults with Autism Spectrum Disorder (ASD).

Method: Development and evaluation of an in-home autonomous robot system enabling users to practice small talk.

Result: Participants enjoyed the training, showed progress in initiating conversations and improving eye contact, and considered the system valuable for enhancing conversational skills.

Conclusion: The autonomous robot system is effective in helping adults with ASD improve their small talk abilities.

Abstract: From dating to job interviews, making new friends or simply chatting with the
cashier at checkout, engaging in small talk is a vital, everyday social skill.
For adults with Autism Spectrum Disorder (ASD), small talk can be particularly
challenging, yet it is essential for social integration, building
relationships, and accessing professional opportunities. In this study, we
present our development and evaluation of an in-home autonomous robot system
that allows users to practice small talk. Results from the week-long study show
that adults with ASD enjoyed the training, made notable progress in initiating
conversations and improving eye contact, and viewed the system as a valuable
tool for enhancing their conversational skills.

</details>


### [302] [Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms](https://arxiv.org/abs/2505.23576)
*Jane Cleland-Huang,Pedro Antonio Alarcon Granadeno,Arturo Miguel Russell Bernal,Demetrius Hernandez,Michael Murphy,Maureen Petterson,Walter Scheirer*

Main category: cs.RO

TL;DR: In this paper, the authors explore the use of small Uncrewed Aerial Systems (sUAS) in search-and-rescue missions. They incorporate large language models (LLMs) to enhance object recognition and decision-making capabilities while addressing safety concerns through cognitive guardrails.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve the performance of sUAS in disaster-response scenarios by enhancing their computer vision systems with LLMs, thus allowing them to better recognize unfamiliar objects and make more informed decisions during missions.

Method: The method involves integrating LLMs into sUAS swarms for reasoning about detected objects. Additionally, high-level decisions are governed by cognitive guardrails to ensure safe and sensible decision-making under uncertainty.

Result: This approach leads to improved object recognition and mission planning capabilities in open-world environments, which can ultimately enhance the effectiveness of search-and-rescue operations.

Conclusion: The conclusion emphasizes the successful design, simulation, and real-world integration of cognitive guardrails for sUAS swarms in search-and-rescue missions, highlighting the potential of combining LLMs with autonomous systems.

Abstract: Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous
swarms in search-and-rescue and other disaster-response scenarios. In these
settings, they use computer vision (CV) to detect objects of interest and
autonomously adapt their missions. However, traditional CV systems often
struggle to recognize unfamiliar objects in open-world environments or to infer
their relevance for mission planning. To address this, we incorporate large
language models (LLMs) to reason about detected objects and their implications.
While LLMs can offer valuable insights, they are also prone to hallucinations
and may produce incorrect, misleading, or unsafe recommendations. To ensure
safe and sensible decision-making under uncertainty, high-level decisions must
be governed by cognitive guardrails. This article presents the design,
simulation, and real-world integration of these guardrails for sUAS swarms in
search-and-rescue missions.

</details>


### [303] [Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation](https://arxiv.org/abs/2505.22805)
*Siddharth Ancha,Sunshine Jiang,Travis Manderson,Laura Brandt,Yilun Du,Philip R. Osteen,Nicholas Roy*

Main category: cs.RO

TL;DR: An analysis-by-synthesis method for pixel-wise anomaly detection using generative diffusion models.


<details>
  <summary>Details</summary>
Motivation: To enable robots to navigate safely and reliably in off-road and unstructured environments by detecting anomalies that are out-of-distribution (OOD) with respect to the training data.

Method: Using a generative diffusion model to synthesize an edited image that removes anomalies while keeping the remaining image unchanged, then formulating anomaly detection as analyzing which image segments were modified by the diffusion model. Proposing a novel inference approach for guided diffusion by analyzing the ideal guidance gradient and deriving a principled approximation that bootstraps the diffusion model to predict guidance gradients.

Result: The editing technique is purely test-time that can be integrated into existing workflows without the need for retraining or fine-tuning. Combining vision-language foundation models allows comparing pixels in a learned feature space and detecting semantically meaningful edits, enabling accurate anomaly detection for off-road navigation.

Conclusion: This paper presents an effective analysis-by-synthesis approach for pixel-wise anomaly detection in off-road and unstructured environments without making any assumptions about the nature of OOD data.

Abstract: In order to navigate safely and reliably in off-road and unstructured
environments, robots must detect anomalies that are out-of-distribution (OOD)
with respect to the training data. We present an analysis-by-synthesis approach
for pixel-wise anomaly detection without making any assumptions about the
nature of OOD data. Given an input image, we use a generative diffusion model
to synthesize an edited image that removes anomalies while keeping the
remaining image unchanged. Then, we formulate anomaly detection as analyzing
which image segments were modified by the diffusion model. We propose a novel
inference approach for guided diffusion by analyzing the ideal guidance
gradient and deriving a principled approximation that bootstraps the diffusion
model to predict guidance gradients. Our editing technique is purely test-time
that can be integrated into existing workflows without the need for retraining
or fine-tuning. Finally, we use a combination of vision-language foundation
models to compare pixels in a learned feature space and detect semantically
meaningful edits, enabling accurate anomaly detection for off-road navigation.
Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/

</details>


### [304] [Learning coordinated badminton skills for legged manipulators](https://arxiv.org/abs/2505.22974)
*Yuntao Ma,Andrei Cramariuc,Farbod Farshidian,Marco Hutter*

Main category: cs.RO

TL;DR: This paper presents a unified reinforcement learning-based control policy for legged mobile manipulators to play badminton, integrating perception noise model, shuttlecock prediction, and constrained reinforcement learning. Experimental results validate the robot's capability in dynamic sports scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges of coordinating motion between lower and upper limbs and aligning limb control with perception in robotics, especially in dynamic environments like playing badminton.

Method: The method involves a unified reinforcement learning-based control policy for whole-body visuomotor skills, using all degrees of freedom for effective shuttlecock tracking and striking. It incorporates a perception noise model using real-world camera data, a shuttlecock prediction model, constrained reinforcement learning for robust motion control, and integrated system identification techniques.

Result: The experimental results show that the robot can predict shuttlecock trajectories, navigate the service area effectively, and execute precise strikes against human players in various environments.

Conclusion: The study concludes that it is feasible to use legged mobile manipulators in complex and dynamic sports scenarios such as badminton.

Abstract: Coordinating the motion between lower and upper limbs and aligning limb
control with perception are substantial challenges in robotics, particularly in
dynamic environments. To this end, we introduce an approach for enabling legged
mobile manipulators to play badminton, a task that requires precise
coordination of perception, locomotion, and arm swinging. We propose a unified
reinforcement learning-based control policy for whole-body visuomotor skills
involving all degrees of freedom to achieve effective shuttlecock tracking and
striking. This policy is informed by a perception noise model that utilizes
real-world camera data, allowing for consistent perception error levels between
simulation and deployment and encouraging learned active perception behaviors.
Our method includes a shuttlecock prediction model, constrained reinforcement
learning for robust motion control, and integrated system identification
techniques to enhance deployment readiness. Extensive experimental results in a
variety of environments validate the robot's capability to predict shuttlecock
trajectories, navigate the service area effectively, and execute precise
strikes against human players, demonstrating the feasibility of using legged
mobile manipulators in complex and dynamic sports scenarios.

</details>


### [305] [Mobi-$π$: Mobilizing Your Robot Learning Policy](https://arxiv.org/abs/2505.23692)
*Jingyun Yang,Isabella Huang,Brandon Vu,Max Bajracharya,Rika Antonova,Jeannette Bohg*

Main category: cs.RO

TL;DR: Learned visuomotor policies can perform complex tasks but generalize poorly to novel robot positions. This paper formulates the policy mobilization problem and introduces the Mobi-π framework to solve it by optimizing the robot's base pose for better alignment with learned policies.


<details>
  <summary>Details</summary>
Motivation: To address the poor generalization of learned visuomotor policies to novel robot positions, which limits their use on mobile platforms for precise tasks.

Method: Formulate the policy mobilization problem, introduce the Mobi-π framework with metrics, simulated tasks, visualization tools, and baseline methods. Propose an approach using 3D Gaussian Splatting, a score function, and sampling-based optimization to optimize the robot's base pose.

Result: The proposed approach outperforms baselines in both simulation and real-world environments, demonstrating its effectiveness for policy mobilization.

Conclusion: Policy mobilization decouples navigation from manipulation, does not require additional demonstrations, and complements existing efforts to improve policy robustness.

Abstract: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [306] [BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models](https://arxiv.org/abs/2505.22865)
*Susan Liang,Dejan Markovic,Israel D. Gebru,Steven Krenn,Todd Keebler,Jacob Sandakly,Frank Yu,Samuel Hassel,Chenliang Xu,Alexander Richard*

Main category: cs.SD

TL;DR: BinauralFlow是一种新的双耳音频合成框架，它通过流匹配模型和因果U-Net架构解决了渲染质量和流式推理的问题，实现了高质量的双耳音频合成，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的双耳渲染方法在渲染质量和流式推理方面存在挑战，难以生成与真实录音无异的高质量双耳音频，同时无法满足实时应用的需求。

Method: 提出了一种基于流匹配的双耳语音合成框架BinauralFlow，将双耳渲染视为生成问题而非回归问题；设计了一个条件流匹配模型以生成高质量音频；采用因果U-Net架构仅基于过去信息估计当前音频帧以适应流式推理；引入了包含流式STFT/ISTFT操作、缓冲区、中点求解器和早期跳过调度的连续推理管道以提高渲染连续性和速度。

Result: 定量和定性评估表明该方法优于现有最先进的方法；感知研究表明该模型生成的音频几乎与真实录音无法区分，混淆率达到42%。

Conclusion: BinauralFlow框架成功解决了双耳音频合成中的质量与实时性问题，其生成的音频质量接近真实录音水平，适合实际应用场景。

Abstract: Binaural rendering aims to synthesize binaural audio that mimics natural
hearing based on a mono audio and the locations of the speaker and listener.
Although many methods have been proposed to solve this problem, they struggle
with rendering quality and streamable inference. Synthesizing high-quality
binaural audio that is indistinguishable from real-world recordings requires
precise modeling of binaural cues, room reverb, and ambient sounds.
Additionally, real-world applications demand streaming inference. To address
these challenges, we propose a flow matching based streaming binaural speech
synthesis framework called BinauralFlow. We consider binaural rendering to be a
generation problem rather than a regression problem and design a conditional
flow matching model to render high-quality audio. Moreover, we design a causal
U-Net architecture that estimates the current audio frame solely based on past
information to tailor generative models for streaming inference. Finally, we
introduce a continuous inference pipeline incorporating streaming STFT/ISTFT
operations, a buffer bank, a midpoint solver, and an early skip schedule to
improve rendering continuity and speed. Quantitative and qualitative
evaluations demonstrate the superiority of our method over SOTA approaches. A
perceptual study further reveals that our model is nearly indistinguishable
from real-world recordings, with a $42\%$ confusion rate.

</details>


### [307] [Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone](https://arxiv.org/abs/2505.23132)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: This paper explores the use of smartphone microphones for lung sound recording and analysis to overcome limitations in traditional methods, especially highlighted by the COVID-19 pandemic. The researchers developed PD-SCL method combined with AST model, achieving a 2.4% performance improvement over the original AST model, demonstrating potential for widespread application beyond clinical settings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional, in-person lung sound assessments revealed during the COVID-19 pandemic, by exploring the feasibility of using smartphone microphones for lung sound diagnosis.

Method: The method developed is called Patient Domain Supervised Contrastive Learning (PD-SCL), which is integrated with the Audio Spectrogram Transformer (AST) model to improve its performance in analyzing lung sounds recorded by smartphone microphones.

Result: The result is a significant improvement in performance by 2.4% compared to the original AST model, showing that smartphones can effectively diagnose lung sounds and handle patient data variability.

Conclusion: The conclusion drawn is that smartphones have potential for broad use in diagnosing lung diseases beyond traditional clinical settings, contributing to more accessible lung disease detection in the post-COVID-19 world.

Abstract: Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic
has revealed the limitations of traditional, in-person lung sound assessments.
To overcome these issues, advancements in digital stethoscopes and artificial
intelligence (AI) have led to the development of new diagnostic methods. In
this context, our study aims to use smartphone microphones to record and
analyze lung sounds. We faced two major challenges: the difference in audio
style between electronic stethoscopes and smartphone microphones, and the
variability among patients. To address these challenges, we developed a method
called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating
this method with the Audio Spectrogram Transformer (AST) model, we
significantly improved its performance by 2.4\% compared to the original AST
model. This progress demonstrates that smartphones can effectively diagnose
lung sounds, addressing inconsistencies in patient data and showing potential
for broad use beyond traditional clinical settings. Our research contributes to
making lung disease detection more accessible in the post-COVID-19 world.

</details>


### [308] [Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM](https://arxiv.org/abs/2505.23207)
*Zhaokai Sun,Li Zhang,Qing Wang,Pan Zhou,Lei Xie*

Main category: cs.SD

TL;DR: The paper presents a speaker-aware progressive OSD model using SSL models and speaker attention for effective overlapping speech detection, achieving an F1 score of 82.76% on the AMI test set.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting overlapping speech in multi-party conversations by enhancing the correlation between subtasks like VAD and overlap detection.

Method: Proposes a speaker-aware progressive OSD model with progressive training strategy, incorporating SSL models (WavLM and wav2vec 2.0) and a speaker attention module for enriched acoustic representation.

Result: Achieves state-of-the-art performance with an F1 score of 82.76% on the AMI test set.

Conclusion: The proposed method demonstrates robustness and effectiveness in overlapping speech detection.

Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple
speakers overlap in a conversation, a critical challenge in multi-party speech
processing. This work proposes a speaker-aware progressive OSD model that
leverages a progressive training strategy to enhance the correlation between
subtasks such as voice activity detection (VAD) and overlap detection. To
improve acoustic representation, we explore the effectiveness of
state-of-the-art self-supervised learning (SSL) models, including WavLM and
wav2vec 2.0, while incorporating a speaker attention module to enrich features
with frame-level speaker information. Experimental results show that the
proposed method achieves state-of-the-art performance, with an F1 score of
82.76\% on the AMI test set, demonstrating its robustness and effectiveness in
OSD.

</details>


### [309] [MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction](https://arxiv.org/abs/2505.23305)
*Yunkee Chae,Kyogu Lee*

Main category: cs.SD

TL;DR: The paper introduces MGE-LDM, a latent diffusion framework for music generation, source imputation, and query-driven source separation without fixed instrument classes.


<details>
  <summary>Details</summary>
Motivation: To create a unified model that can handle music generation, source imputation, and source separation with flexibility in handling arbitrary instrument sources rather than being constrained to fixed instrument classes.

Method: MGE-LDM learns a joint distribution over full mixtures, submixtures, and individual stems within a single compact latent diffusion model. It formulates separation and imputation as conditional inpainting tasks in the latent space.

Result: MGE-LDM enables complete mixture generation, partial generation (source imputation), and text-conditioned extraction of arbitrary sources. It supports flexible, class-agnostic manipulation of instrument sources and can be trained across heterogeneous multi-track datasets without relying on predefined instrument categories.

Conclusion: MGE-LDM presents a novel approach for simultaneous music generation, source imputation, and query-driven source separation with flexibility in handling various instrument sources.

Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous
music generation, source imputation, and query-driven source separation. Unlike
prior approaches constrained to fixed instrument classes, MGE-LDM learns a
joint distribution over full mixtures, submixtures, and individual stems within
a single compact latent diffusion model. At inference, MGE-LDM enables (1)
complete mixture generation, (2) partial generation (i.e., source imputation),
and (3) text-conditioned extraction of arbitrary sources. By formulating both
separation and imputation as conditional inpainting tasks in the latent space,
our approach supports flexible, class-agnostic manipulation of arbitrary
instrument sources. Notably, MGE-LDM can be trained jointly across
heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without
relying on predefined instrument categories. Audio samples are available at our
project page: https://yoongi43.github.io/MGELDM_Samples/.

</details>


### [310] [Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds](https://arxiv.org/abs/2505.23509)
*Andrew Chang,Yike Li,Iran R. Roman,David Poeppel*

Main category: cs.SD

TL;DR: This paper proposes a novel approach centered on spectrotemporal modulation (STM) features for audio classification, which is efficient and interpretable without pretraining.


<details>
  <summary>Details</summary>
Motivation: Current audio DNNs representations are computationally costly and uninterpretable, so there is a need for optimization.

Method: The method uses spectrotemporal modulation (STM) features, a signal processing method that mimics the neurophysiological representation in the human auditory cortex.

Result: The STM-based model shows classification performance comparable to pretrained audio DNNs across diverse naturalistic speech, music, and environmental sounds.

Conclusion: STM is an efficient and interpretable feature representation for audio classification, advancing machine listening and offering new possibilities in speech and auditory sciences, audio BCI, and cognitive computing.

Abstract: Audio DNNs have demonstrated impressive performance on various machine
listening tasks; however, most of their representations are computationally
costly and uninterpretable, leaving room for optimization. Here, we propose a
novel approach centered on spectrotemporal modulation (STM) features, a signal
processing method that mimics the neurophysiological representation in the
human auditory cortex. The classification performance of our STM-based model,
without any pretraining, is comparable to that of pretrained audio DNNs across
diverse naturalistic speech, music, and environmental sounds, which are
essential categories for both human cognition and machine perception. These
results show that STM is an efficient and interpretable feature representation
for audio classification, advancing the development of machine listening and
unlocking exciting new possibilities for basic understanding of speech and
auditory sciences, as well as developing audio BCI and cognitive computing.

</details>


### [311] [Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](https://arxiv.org/abs/2505.23619)
*Neta Glazer,David Chernin,Idan Achituve,Sharon Gannot,Ethan Fetaya*

Main category: cs.SD

TL;DR: Recent advancements in TTS models have increased the need for efficient deepfake detection methods. This paper introduces ADD-GP, a few-shot adaptive framework for Audio Deepfake Detection that combines a deep embedding model with Gaussian processes flexibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing demand for adaptable and efficient deepfake detection methods as Text-to-Speech systems continue to evolve.

Method: ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection. It combines a powerful deep embedding model with the Gaussian processes flexibility.

Result: This approach can achieve strong performance and adaptability, and can also be used for personalized detection with greater robustness to new TTS models and one-shot adaptability.

Conclusion: A benchmark dataset is constructed for evaluating the task using new state-of-the-art voice cloning models.

Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice
cloning, have intensified the demand for adaptable and efficient deepfake
detection methods. As TTS systems continue to evolve, detection models must be
able to efficiently adapt to previously unseen generation models with minimal
data. This paper introduces ADD-GP, a few-shot adaptive framework based on a
Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show
how the combination of a powerful deep embedding model with the Gaussian
processes flexibility can achieve strong performance and adaptability.
Additionally, we show this approach can also be used for personalized
detection, with greater robustness to new TTS models and one-shot adaptability.
To support our evaluation, a benchmark dataset is constructed for this task
using new state-of-the-art voice cloning models.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [312] [Trajectory Generator Matching for Time Series](https://arxiv.org/abs/2505.23215)
*T. Jahn,J. Chemseddine,P. Hagemann,C. Wald,G. Steidl*

Main category: math.NA

TL;DR: 这篇论文通过生成建模图像数据的思想，提出了新的SDE和跳跃过程生成器，用于处理不规则观测的时间序列生成问题。它能够处理底层过程的不连续性，并且可以处理不规则采样的时间序列。


<details>
  <summary>Details</summary>
Motivation: 准确地从不规则观测中对时间连续随机过程进行建模仍然是一个重大挑战。

Method: 利用生成建模图像数据的思想，提出新的SDE和跳跃过程生成器，这些生成器具有感兴趣的时间序列的边缘分布。通过参数化跳跃核密度来处理底层过程的不连续性，并允许在损失中使用Kullback-Leibler散度的闭合形式公式。

Result: 与大多数其他方法不同，该方法能够处理不规则采样的时间序列。

Conclusion: 提出的生成器可以有效应对不规则观测的时间序列生成问题，特别是处理底层过程的不连续性方面表现出色。

Abstract: Accurately modeling time-continuous stochastic processes from irregular
observations remains a significant challenge. In this paper, we leverage ideas
from generative modeling of image data to push the boundary of time series
generation. For this, we find new generators of SDEs and jump processes,
inspired by trajectory flow matching, that have the marginal distributions of
the time series of interest. Specifically, we can handle discontinuities of the
underlying processes by parameterizing the jump kernel densities by scaled
Gaussians that allow for closed form formulas of the corresponding
Kullback-Leibler divergence in the loss. Unlike most other approaches, we are
able to handle irregularly sampled time series.

</details>


### [313] [Optimization-Free Diffusion Model -- A Perturbation Theory Approach](https://arxiv.org/abs/2505.23652)
*Yuehaw Khoo,Mathias Oster,Yifan Peng*

Main category: math.NA

TL;DR: 提出了一种无需优化和前向SDE的扩散模型方法，通过在与扩散过程相关联的后向Kolmogorov算子的稀疏特征基上展开得分函数，将得分估计重新表述为线性系统的解。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型通常依赖于优化神经网络以通过前向SDE模拟来估计得分函数，这可能需要大量的计算资源和时间。

Method: 通过在与扩散过程相关联的后向Kolmogorov算子的稀疏特征基上展开得分函数，将得分估计重新表述为线性系统的解，从而避免了迭代优化和时间依赖的样本生成。

Result: 分析了使用摄动理论的逼近误差，并在高维Boltzmann分布和真实世界数据集上演示了该方法的有效性。

Conclusion: 所提出的方法提供了一种无需优化和前向SDE的替代方法，具有潜在的广泛应用前景。

Abstract: Diffusion models have emerged as a powerful framework in generative modeling,
typically relying on optimizing neural networks to estimate the score function
via forward SDE simulations. In this work, we propose an alternative method
that is both optimization-free and forward SDE-free. By expanding the score
function in a sparse set of eigenbasis of the backward Kolmogorov operator
associated with the diffusion process, we reformulate score estimation as the
solution to a linear system, avoiding iterative optimization and time-dependent
sample generation. We analyze the approximation error using perturbation theory
and demonstrate the effectiveness of our method on high-dimensional Boltzmann
distributions and real-world datasets.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [314] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/abs/2505.22685)
*Marcus J. Vroemen,Yuqian Chen,Yui Lo,Tengfei Xu,Weidong Cai,Fan Zhang,Josien P. W. Pluim,Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: The paper introduces DeepMultiConnectome, a deep-learning model that predicts structural connectomes directly from tractography without gray matter parcellation. It supports multiple parcellation schemes and is scalable for large-scale studies.


<details>
  <summary>Details</summary>
Motivation: Traditional connectome generation is time-consuming and requires gray matter parcellation, which poses challenges for large-scale studies.

Method: A point-cloud-based neural network with multi-task learning is used to classify streamlines according to their connected regions across two parcellation schemes (84-region and 164-region), sharing a learned representation.

Result: DeepMultiConnectome predicts multiple structural connectomes in about 40 seconds with high correlation to traditionally generated connectomes (r=0.992 for 84-region scheme; r=0.986 for 164-region scheme). Network properties are largely preserved, and test-retest analysis shows comparable reproducibility to traditional methods. Predicted connectomes perform similarly to traditional ones in predicting age and cognitive function.

Conclusion: DeepMultiConnectome provides a fast and scalable model for generating subject-specific connectomes across multiple parcellation schemes.

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [315] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/abs/2505.23353)
*Alexandra G. Roberts,Ha M. Luu,Mert Şişman,Alexey V. Dimov,Ceren Tozlu,Ilhami Kovanlikaya,Susan A. Gauthier,Thanh D. Nguyen,Yi Wang*

Main category: eess.IV

TL;DR: 这篇论文提出了一种生成合成定量易感性图的方法，以改善多发性硬化症中罕见的顺磁性边缘病变（PRLs）的分类器性能，并提出了一种新的去噪方法来增加少数类别。这些方法提高了对未见PRL分布的近似和检测能力，并且代码和生成数据将在GitHub上公开。


<details>
  <summary>Details</summary>
Motivation: 在多发性硬化症的研究中，定量易感性图可以提供重要的诊断和预后信息，特别是对于区分带有沉积的顺磁性铁的'边缘'病变和'非边缘'病变类型。然而，由于顺磁性边缘病变（PRLs）较为罕见，解决类别不平衡问题是研究的重点。

Method: 作者生成了合成的顺磁性边缘病变的定量易感性图，并将这些合成数据用于改进分类器性能。他们还扩展了多通道方法以生成伴随对比度和概率分割图。利用训练好的生成网络的投影能力，提出了一种新的去噪方法，该方法可以在模糊边缘病例上进行训练并显著增加少数类别。

Result: 实验结果表明，合成病变生成和提出的边缘病变标签去噪方法能够最好地近似未见的边缘病变分布，并以临床可解释的方式提高检测效果。

Conclusion: 通过使用合成数据和去噪方法，可以有效应对多发性硬化症中的类别不平衡问题，提高顺磁性边缘病变的检测能力，为临床应用提供了更可解释的结果。

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [316] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/abs/2505.23503)
*Shibbir Ahmed,Shahnewaz Karim Sakib,Anindya Bijoy Das*

Main category: eess.IV

TL;DR: This study explores a multimodal AI framework for classifying medical diagnostic images using CNNs and LLMs, revealing trade-offs in performance, efficiency, and environmental impact.


<details>
  <summary>Details</summary>
Motivation: To determine the strengths and limitations of different AI models (CNNs vs LLMs) when applied to medical diagnostic image classification.

Method: The study utilized publicly available datasets to compare CNNs with various LLMs through metrics such as accuracy, F1-score, execution time, energy consumption, and CO2 emission.

Result: CNN-based models generally outperform multimodal techniques combining images and contextual info, but LLMs with additional filtering can achieve significant performance improvements.

Conclusion: Multimodal AI systems hold great potential to improve the reliability, efficiency, and scalability of medical diagnostics.

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [317] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2505.23587)
*Christian Schmidt,Heinrich Martin Overhoff*

Main category: eess.IV

TL;DR: 在医学图像分割中，当模型在未见过的数据集上部署时，有限的外部有效性仍然是一个关键障碍，尤其是在超声图像领域。本文提出了一种PCA的新应用，通过降维预处理减少噪声并强调关键特征，从而改善模型在外部验证时的表现。实验结果表明，使用PCA重建的数据集可以显著提高模型的召回率和Dice分数，特别是在基线性能较低的情况下。该方法将外部验证导致的召回值下降减少了33%，为增强实际医学应用中的外部有效性提供了潜在途径。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型在跨数据集部署时面临外部有效性不足的问题，尤其是在超声图像领域。现有方法如域适应和基于GAN的风格迁移在小而多样化的医学数据集中效果有限，因此需要一种新的方法来解决这一问题。

Method: 利用主成分分析（PCA）对数据进行降维预处理，保留约90%的数据集方差，以减少噪声并突出关键特征。在六个不同的乳腺肿瘤超声数据集上评估该方法，创建相应的PCA降维数据集，并在这些数据集上训练U-Net分割模型。通过比较在原始数据集和PCA数据集上训练的模型在外部验证时的表现，评估PCA方法的效果。

Result: 实验结果显示，使用PCA重建的数据集显著提高了模型的召回率和Dice分数，特别是在基线性能较低的情况下。统计上显著提高了召回率（从0.57 ± 0.07提高到0.70 ± 0.05，p = 0.0004）和Dice分数（从0.50 ± 0.06提高到0.58 ± 0.06，p = 0.03）。该方法将外部验证导致的召回值下降减少了33%。

Conclusion: PCA重建作为一种方法，能够有效缓解医学图像分割中因外部验证而导致的性能下降问题，尤其在挑战性案例中表现突出。这为增强实际医学应用中的外部有效性提供了新思路。

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [318] [StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis](https://arxiv.org/abs/2505.22746)
*Jose Guadalupe Hernandez,Attri Ghosh,Philip J. Freda,Yufei Meng,Nicholas Matsumoto,Jason H. Moore*

Main category: cs.NE

TL;DR: The paper introduces StarBASE-GP, a tool using genetic programming to find significant genetic variants in large genomic datasets. It shows better performance compared to random baseline and biologically naive version.


<details>
  <summary>Details</summary>
Motivation: To develop an automated framework for discovering meaningful genetic variants associated with phenotypic variation in large-scale genomic datasets.

Method: StarBASE-GP uses genetic programming-based multi-objective optimization strategy to evolve machine learning pipelines that maximize explanatory power and minimize complexity. Biological domain knowledge is integrated at multiple stages.

Result: StarBASE-GP consistently evolves Pareto fronts with superior performance, yielding higher accuracy in identifying both ground truth and novel quantitative trait loci.

Conclusion: StarBASE-GP demonstrates robust potential for advancing variant discovery in complex traits by incorporating evolutionary search and relevant biological theory into a flexible automated machine learning framework.

Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool
- Genetic Programming (StarBASE-GP), an automated framework for discovering
meaningful genetic variants associated with phenotypic variation in large-scale
genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective
optimization strategy to evolve machine learning pipelines that simultaneously
maximize explanatory power (r2) and minimize pipeline complexity. Biological
domain knowledge is integrated at multiple stages, including the use of nine
inheritance encoding strategies to model deviations from additivity, a custom
linkage disequilibrium pruning node that minimizes redundancy among features,
and a dynamic variant recommendation system that prioritizes informative
candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of
Rattus norvegicus (brown rat) to identify variants associated with body mass
index, benchmarking its performance against a random baseline and a
biologically naive version of the tool. StarBASE-GP consistently evolves Pareto
fronts with superior performance, yielding higher accuracy in identifying both
ground truth and novel quantitative trait loci, highlighting relevant targets
for future validation. By incorporating evolutionary search and relevant
biological theory into a flexible automated machine learning framework,
StarBASE-GP demonstrates robust potential for advancing variant discovery in
complex traits.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [319] [Distribution free M-estimation](https://arxiv.org/abs/2505.22807)
*John C. Duchi*

Main category: math.ST

TL;DR: This paper explores the solvability of univariate convex M-estimation or stochastic optimization problems without making assumptions on the data distribution, offering clear conditions for solvable and unsolvable problems that differ from traditional understandings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to delineate statistical problems that can be solved without any assumptions on the underlying data distribution, focusing on univariate convex M-estimation or stochastic optimization.

Method: The method involves characterizing when such problems are solvable in an assumption-free setting and identifying the precise dividing line between solvable and unsolvable problems.

Result: It reveals that Lipschitz continuity of the loss being minimized is not necessary for distribution-free minimization, and the conditions identified are distinct from classical learnability characterizations in machine learning.

Conclusion: The paper concludes by providing a characterization of when a univariate convex M-estimation or stochastic optimization problem can be solved without assumptions on the data distribution, contributing new insights into the field.

Abstract: The basic question of delineating those statistical problems that are
solvable without making any assumptions on the underlying data distribution has
long animated statistics and learning theory. This paper characterizes when a
(univariate) convex M-estimation or stochastic optimization problem is solvable
in such an assumption-free setting, providing a precise dividing line between
solvable and unsolvable problems. The conditions we identify show, perhaps
surprisingly, that Lipschitz continuity of the loss being minimized is not
necessary for distribution free minimization, and they are also distinct from
classical characterizations of learnability in machine learning.

</details>


### [320] [Joint estimation of smooth graph signals from partial linear measurements](https://arxiv.org/abs/2505.23240)
*Hemant Tyagi*

Main category: math.ST

TL;DR: 通过平滑惩罚最小二乘估计器，在特定图结构下，即使采样条件非常严格，也能以非渐近方式获得信号估计的均方误差界，并且该估计器在多层排序问题中也表现出弱一致性。


<details>
  <summary>Details</summary>
Motivation: 研究的目标是在给定部分线性测量值的情况下，基于图G的平滑性假设，估计与顶点关联的潜在信号$x_t$。这适用于仅有少量顶点被测量的情况，并探索了极端稀疏采样条件下估计的有效性。

Method: 利用平滑惩罚最小二乘估计器进行信号恢复，推导出均方误差的非渐近界。此外，还将这种方法扩展到多层排序问题，其中每个层$t$对应于一个测量图$G_t$，并处理噪声成对差异测量。

Result: 获得了信号估计的非渐近均方误差界，证明了在某些图选择下，即使只有极小比例的顶点被采样（甚至每个顶点仅测量一个坐标），估计器仍具有弱一致性。对于多层排序问题，即使单个层的图非常稀疏和不连通，也能实现弱一致性。

Conclusion: 平滑惩罚最小二乘估计器在极端稀疏采样条件下表现良好，能够在图结构的帮助下准确恢复信号。这一方法还可扩展至多层排序问题，并在稀疏图情况下保持有效性。

Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each
vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given
partial linear measurements of the signals, for a potentially small subset of
the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are
smooth w.r.t $G$, in the sense that the quadratic variation of the signals over
the graph is small, we obtain non-asymptotic bounds on the mean squared error
for jointly recovering $x_t$'s, for the smoothness penalized least squares
estimator. In particular, this implies for certain choices of $G$ that this
estimator is weakly consistent (as $T \rightarrow \infty$) under potentially
very stringent sampling, where only one coordinate is measured per vertex for a
vanishingly small fraction of the vertices. The results are extended to a
``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths
of a collection of $n$ items, and noisy pairwise difference measurements are
obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency
is established for certain choices of $G$ even when the individual $G_t$'s are
very sparse and disconnected.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [321] [A Tool for Generating Exceptional Behavior Tests With Large Language Models](https://arxiv.org/abs/2505.22818)
*Linghan Zhong,Samuel Yuan,Jiyang Zhang,Yu Liu,Pengyu Nie,Junyi Jessy Li,Milos Gligoric*

Main category: cs.SE

TL;DR: A framework named exLong is presented, which automatically generates exceptional behavior tests (EBTs) by leveraging a fine-tuned LLM from CodeLlama. It addresses the issue that developers often prioritize testing 'happy paths' over exceptional scenarios.


<details>
  <summary>Details</summary>
Motivation: Developers tend to focus more on testing 'happy paths' rather than exceptional scenarios, leading to a need for a tool that can help generate EBTs.

Method: exLong uses a LLM fine-tuned from CodeLlama and incorporates reasoning about exception-throwing traces, conditional expressions guarding throw statements, and non-exceptional behavior tests executing similar traces.

Result: exLong effectively assists developers in creating comprehensive EBTs as demonstrated in a video.

Conclusion: exLong is a useful framework for generating EBTs and addressing the gap in software development regarding exceptional scenario testing.

Abstract: Exceptional behavior tests (EBTs) are crucial in software development for
verifying that code correctly handles unwanted events and throws appropriate
exceptions. However, prior research has shown that developers often prioritize
testing "happy paths", e.g., paths without unwanted events over exceptional
scenarios. We present exLong, a framework that automatically generates EBTs to
address this gap. exLong leverages a large language model (LLM) fine-tuned from
CodeLlama and incorporates reasoning about exception-throwing traces,
conditional expressions that guard throw statements, and non-exceptional
behavior tests that execute similar traces. Our demonstration video illustrates
how exLong can effectively assist developers in creating comprehensive EBTs for
their project (available at https://youtu.be/Jro8kMgplZk).

</details>


### [322] [OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software](https://arxiv.org/abs/2505.23239)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.SE

TL;DR: The paper introduces OSS-UAgent, an automated usability evaluation framework for open source software using intelligent agents powered by LLMs. It simulates developers at various experience levels and evaluates code across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: Usability evaluation is critical to the impact and adoption of open source software (OSS), but traditional methods are costly and not scalable.

Method: OSS-UAgent employs intelligent agents based on large language models to simulate developers performing programming tasks at different experience levels. It dynamically constructs platform-specific knowledge bases for context-aware code generation and automatically evaluates the generated code across several dimensions.

Result: OSS-UAgent provides a comprehensive measure of software usability by evaluating code compliance, correctness, and readability. The demonstration highlights its effectiveness in automating usability evaluations for graph analytics platforms.

Conclusion: OSS-UAgent is a promising solution for automated, configurable, and interactive usability evaluation in open source software.

Abstract: Usability evaluation is critical to the impact and adoption of open source
software (OSS), yet traditional methods relying on human evaluators suffer from
high costs and limited scalability. To address these limitations, we introduce
OSS-UAgent, an automated, configurable, and interactive agent-based usability
evaluation framework specifically designed for open source software. Our
framework employs intelligent agents powered by large language models (LLMs) to
simulate developers performing programming tasks across various experience
levels (from Junior to Expert). By dynamically constructing platform-specific
knowledge bases, OSS-UAgent ensures accurate and context-aware code generation.
The generated code is automatically evaluated across multiple dimensions,
including compliance, correctness, and readability, providing a comprehensive
measure of the software's usability. Additionally, our demonstration showcases
OSS-UAgent's practical application in evaluating graph analytics platforms,
highlighting its effectiveness in automating usability evaluation.

</details>


### [323] [Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization](https://arxiv.org/abs/2505.23387)
*Mingzhe Du,Luu Tuan Tuan,Yue Liu,Yuhao Qing,Dong Huang,Xinyi He,Qian Liu,Zejun Ma,See-kiong Ng*

Main category: cs.SE

TL;DR: Large Language Models (LLMs) can generate functionally correct code solutions, but these are often inefficient. This paper presents a test-time iterative optimization framework using an execution sandbox to provide feedback for LLMs to refine their code. It compares three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization (GRPO). GRPO, which uses reinforcement learning with execution feedback, significantly improves both the pass rate and efficiency of the generated code.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the code efficiency of Large Language Models' outputs since functional correctness alone is insufficient for real-world deployment.

Method: The method involves introducing a novel test-time iterative optimization framework that employs a closed-loop system. In this system, LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. The study explores three training strategies: SFT, DPO, and GRPO.

Result: Experiments show that SFT and DPO quickly reach a plateau in efficiency gains. However, GRPO continuously optimizes code performance, leading to significant improvements in both pass@1 rates (from 47% to 62%) and the likelihood of outperforming human submissions in terms of efficiency (from 31% to 45%).

Conclusion: This work successfully demonstrates effective improvements in test-time code efficiency and highlights the potential of reinforcement learning in enabling LLMs to self-improve their code efficiency.

Abstract: Large Language Models (LLMs) generate functionally correct solutions but
often fall short in code efficiency, a critical bottleneck for real-world
deployment. In this paper, we introduce a novel test-time iterative
optimization framework to address this, employing a closed-loop system where
LLMs iteratively refine code based on empirical performance feedback from an
execution sandbox. We explore three training strategies: Supervised Fine-Tuning
(SFT), Direct Preference Optimization (DPO), and Group Relative Policy
Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark
show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,
using reinforcement learning (RL) with execution feedback, continuously
optimizes code performance, significantly boosting both pass@1 (from 47% to
62%) and the likelihood of outperforming human submissions in efficiency (from
31% to 45%). Our work demonstrates effective test-time code efficiency
improvement and critically reveals the power of RL in teaching LLMs to truly
self-improve code efficiency.

</details>


### [324] [Toward Effective AI Governance: A Review of Principles](https://arxiv.org/abs/2505.23417)
*Danilo Ribeiro,Thayssa Rocha,Gustavo Pinto,Bruno Cartaxo,Marcelo Amaral,Nicole Davila,Ana Camargo*

Main category: cs.SE

TL;DR: Artificial Intelligence (AI) governance is crucial for responsible AI development and deployment, yet there's a lack of synthesis in current literature. A rapid tertiary review of nine studies identified frequently cited frameworks like the EU AI Act and NIST RMF, with transparency and accountability being key principles. However, there are gaps regarding actionable governance mechanisms and inclusivity.


<details>
  <summary>Details</summary>
Motivation: To address the lack of synthesis across AI governance frameworks and practices in current literature by identifying emphasized frameworks, principles, mechanisms, and stakeholder roles in secondary literature on AI governance.

Method: Conducted a rapid tertiary review of nine peer-reviewed secondary studies from IEEE and ACM (2020-2024), applying structured inclusion criteria and thematic semantic synthesis.

Result: The most cited frameworks include the EU AI Act and NIST RMF; transparency and accountability are the most common principles. Few reviews provide detailed actionable governance mechanisms or stakeholder strategies.

Conclusion: The review consolidates key directions in AI governance, emphasizing gaps in empirical validation and inclusivity, thus informing academic inquiry and practical adoption in organizations.

Abstract: Artificial Intelligence (AI) governance is the practice of establishing
frameworks, policies, and procedures to ensure the responsible, ethical, and
safe development and deployment of AI systems. Although AI governance is a core
pillar of Responsible AI, current literature still lacks synthesis across such
governance frameworks and practices. Objective: To identify which frameworks,
principles, mechanisms, and stakeholder roles are emphasized in secondary
literature on AI governance. Method: We conducted a rapid tertiary review of
nine peer-reviewed secondary studies from IEEE and ACM (20202024), using
structured inclusion criteria and thematic semantic synthesis. Results: The
most cited frameworks include the EU AI Act and NIST RMF; transparency and
accountability are the most common principles. Few reviews detail actionable
governance mechanisms or stakeholder strategies. Conclusion: The review
consolidates key directions in AI governance and highlights gaps in empirical
validation and inclusivity. Findings inform both academic inquiry and practical
adoption in organizations.

</details>


### [325] [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
*Linghao Zhang,Shilin He,Chaoyun Zhang,Yu Kang,Bowen Li,Chengxing Xie,Junhao Wang,Maoquan Wang,Yufan Huang,Shengyu Fu,Elsie Nallipogu,Qingwei Lin,Yingnong Dang,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: The paper introduces SWE-bench-Live, a live-updatable benchmark for evaluating LLMs in real-world bug-fixing tasks, addressing limitations of existing benchmarks by offering scalability, reproducibility, and continuous updates.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like SWE-bench have limitations such as lack of updates, narrow repository coverage, heavy manual effort, and risks of overfitting and data contamination.

Method: SWE-bench-Live features an automated curation pipeline that simplifies the process from instance creation to environment setup, enabling scalability and continuous updates. It includes 1,319 tasks from 93 repositories with dedicated Docker images for reproducibility.

Result: Evaluation on SWE-bench-Live shows a significant performance gap compared to static benchmarks like SWE-bench, even under controlled conditions. Detailed analyses reveal discrepancies based on repository origin, issue recency, and task difficulty.

Conclusion: SWE-bench-Live provides a fresh, diverse, and executable benchmark for rigorous evaluation of LLMs and agents in dynamic, real-world software development scenarios.

Abstract: The issue-resolving task, where a model generates patches to fix real-world
bugs, has emerged as a critical benchmark for evaluating the capabilities of
large language models (LLMs). While SWE-bench and its variants have become
standard in this domain, they suffer from key limitations: they have not been
updated since their initial releases, cover a narrow set of repositories, and
depend heavily on manual effort for instance construction and environment
setup. These factors hinder scalability and introduce risks of overfitting and
data contamination. In this work, we present \textbf{SWE-bench-Live}, a
\textit{live-updatable} benchmark designed to overcome these challenges. Our
initial release consists of 1,319 tasks derived from real GitHub issues created
since 2024, spanning 93 repositories. Each task is accompanied by a dedicated
Docker image to ensure reproducible execution. Central to our benchmark is
\method, an automated curation pipeline that streamlines the entire process
from instance creation to environment setup, removing manual bottlenecks and
enabling scalability and continuous updates. We evaluate a range of
state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a
substantial performance gap compared to static benchmarks like SWE-bench, even
under controlled evaluation conditions. To better understand this discrepancy,
we perform detailed analyses across repository origin, issue recency, and task
difficulty. By providing a fresh, diverse, and executable benchmark grounded in
live repository activity, SWE-bench-Live facilitates rigorous,
contamination-resistant evaluation of LLMs and agents in dynamic, real-world
software development settings.

</details>


### [326] [From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents](https://arxiv.org/abs/2505.23422)
*Tobias Lindenbauer,Georg Groh,Hinrich Schütze*

Main category: cs.SE

TL;DR: CTIM-Rover is an AI agent for Software Engineering that uses episodic memory, specifically Cross-Task-Instance Memory (CTIM), to improve task performance. However, CTIM-Rover does not outperform AutoCodeRover in any configuration due to noise introduced by distracting CTIM items.


<details>
  <summary>Details</summary>
Motivation: Existing SE agents inefficiently discard their long-term memory after a single task instance, which can be problematic for repository-level understanding required for bug fixing.

Method: The researchers built CTIM-Rover on top of AutoCodeRover and incorporated a general and repository-level Cross-Task-Instance Memory (CTIM) using the Experiential Learning (EL) approach ExpeL and a Mixture-Of-Experts (MoEs) inspired approach.

Result: CTIM-Rover did not outperform AutoCodeRover in any configuration, with performance degradation likely caused by noise from distracting CTIM items or exemplar trajectories.

Conclusion: ExpeL and DoT-Bank do not scale to real-world SE problems due to the noise issue.

Abstract: We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on
top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning
frameworks with an episodic memory, more specifically, a general and
repository-level Cross-Task-Instance Memory (CTIM). While existing open-source
SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,
2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning
frameworks inefficiently discard their long-term memory after a single task
instance. As repository-level understanding is pivotal for identifying all
locations requiring a patch for fixing a bug, we hypothesize that SE is
particularly well positioned to benefit from CTIM. For this, we build on the
Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a
Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose
and repository-level CTIM. We find that CTIM-Rover does not outperform
AutoCodeRover in any configuration and thus conclude that neither ExpeL nor
DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis
indicates noise introduced by distracting CTIM items or exemplar trajectories
as the likely source of the performance degradation.

</details>


### [327] [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
*Manish Shetty,Naman Jain,Jinjian Liu,Vijay Kethanaboyina,Koushik Sen,Ion Stoica*

Main category: cs.SE

TL;DR: The paper introduces GSO, a benchmark for evaluating language models' capabilities in developing high-performance software. The benchmark includes 102 challenging optimization tasks across 10 codebases. Quantitative evaluation shows leading SWE-Agents have less than 5% success rate and qualitative analysis identifies key failure modes.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of language models in developing high-performance software.

Method: Develop an automated pipeline that generates and executes performance tests to analyze repository commit histories and identify challenging optimization tasks. Provide an agent with a codebase and performance test as a precise specification and task it to improve the runtime efficiency.

Result: Quantitative evaluation reveals that leading SWE-Agents struggle significantly, achieving less than 5% success rate. Qualitative analysis identifies key failure modes.

Conclusion: Releases the code and artifacts of the benchmark along with agent trajectories to enable future research.

Abstract: Developing high-performance software is a complex task that requires
specialized expertise. We introduce GSO, a benchmark for evaluating language
models' capabilities in developing high-performance software. We develop an
automated pipeline that generates and executes performance tests to analyze
repository commit histories to identify 102 challenging optimization tasks
across 10 codebases, spanning diverse domains and programming languages. An
agent is provided with a codebase and performance test as a precise
specification, and tasked to improve the runtime efficiency, which is measured
against the expert developer optimization. Our quantitative evaluation reveals
that leading SWE-Agents struggle significantly, achieving less than 5% success
rate, with limited improvements even with inference-time scaling. Our
qualitative analysis identifies key failure modes, including difficulties with
low-level languages, practicing lazy optimization strategies, and challenges in
accurately localizing bottlenecks. We release the code and artifacts of our
benchmark along with agent trajectories to enable future research.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [328] [Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters](https://arxiv.org/abs/2505.23554)
*Hayden Moore,Sirui Qi,Ninad Hogade,Dejan Milojicic,Cullen Bash,Sudeep Pasricha*

Main category: cs.DC

TL;DR: The paper proposes a novel framework called SLIT to co-optimize LLM quality of service, carbon emissions, water usage, and energy costs by using an ML-based metaheuristic.


<details>
  <summary>Details</summary>
Motivation: With the growing adoption of LLMs, there is an increasing concern about their environmental impact during the operational phase, including carbon footprint, water usage, and energy costs.

Method: The proposed framework, SLIT, utilizes a machine learning based metaheuristic to enhance the sustainability of LLM hosting across geo-distributed cloud datacenters.

Result: SLIT aims to co-optimize LLM quality of service (time-to-first token), carbon emissions, water usage, and energy costs.

Conclusion: As LLMs continue to proliferate, the SLIT framework will become increasingly vital in addressing sustainability issues.

Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and
Gemini have been widely adopted in different areas. As the use of LLMs
continues to grow, many efforts have focused on reducing the massive training
overheads of these models. But it is the environmental impact of handling user
requests to LLMs that is increasingly becoming a concern. Recent studies
estimate that the costs of operating LLMs in their inference phase can exceed
training costs by 25x per year. As LLMs are queried incessantly, the cumulative
carbon footprint for the operational phase has been shown to far exceed the
footprint during the training phase. Further, estimates indicate that 500 ml of
fresh water is expended for every 20-50 requests to LLMs during inference. To
address these important sustainability issues with LLMs, we propose a novel
framework called SLIT to co-optimize LLM quality of service (time-to-first
token), carbon emissions, water usage, and energy costs. The framework utilizes
a machine learning (ML) based metaheuristic to enhance the sustainability of
LLM hosting across geo-distributed cloud datacenters. Such a framework will
become increasingly vital as LLMs proliferate.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [329] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
*Feng Yao,Zilong Wang,Liyuan Liu,Junxia Cui,Li Zhong,Xiaohan Fu,Haohui Mai,Vish Krishnan,Jianfeng Gao,Jingbo Shang*

Main category: cs.CL

TL;DR: This paper introduces REAL, a reinforcement learning framework that encourages LLMs to generate high-quality code by using program analysis feedback. It integrates automated signals for defect detection and unit testing, is scalable without manual intervention, and outperforms current methods in functionality and code quality assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing methods for improving code quality generated by large language models (LLMs), which often fail in ensuring security and maintainability. Current solutions rely on labor-intensive annotations or brittle heuristics, thus limiting their scalability and effectiveness.

Method: REAL is a reinforcement learning framework that incentivizes LLMs to produce production-quality code. It uses two automated signals: program analysis for detecting defects related to security and maintainability, and unit tests to ensure functional correctness. The framework is prompt-agnostic and reference-free, allowing for scalable supervision without manual intervention.

Result: Experiments conducted across multiple datasets and model scales show that REAL surpasses state-of-the-art methods in simultaneous evaluations of functionality and code quality.

Conclusion: REAL bridges the gap between rapid prototyping and production-ready code, enabling LLMs to deliver both speed and quality. This advancement allows for more effective and efficient generation of secure and maintainable code.

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [330] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
*Ansar Aynetdinov,Alan Akbik*

Main category: cs.CL

TL;DR: Multi-token prediction (MTP) is advantageous for large language models but challenging for smaller ones. This paper proposes a curriculum learning strategy with forward and reverse variants to help smaller models better utilize MTP, improving their performance in different aspects.


<details>
  <summary>Details</summary>
Motivation: Although MTP has shown benefits for large language models, smaller language models have difficulty handling it effectively.

Method: The authors propose two curriculum learning strategies: forward curriculum which gradually increases the complexity from NTP to MTP, and reverse curriculum which does the opposite.

Result: Forward curriculum helps smaller models leverage MTP, improving downstream NTP performance and generative output quality while retaining self-speculative decoding benefits. Reverse curriculum achieves stronger NTP performance and output quality but lacks self-speculative decoding advantages.

Conclusion: Curriculum learning strategies can significantly enhance the ability of smaller language models to benefit from MTP.

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [331] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
*Sara Papi,Marco Gaido,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Main category: cs.CL

TL;DR: The paper introduces FAMA, the first family of open science SFMs for English and Italian, which is trained on 150k+ hours of OS speech data. It also presents a new dataset with 16k hours of cleaned and pseudo-labeled speech for both languages. FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the closed nature of current speech foundation models (SFMs) such as Whisper and SeamlessM4T, which hinder reproducibility and fair evaluation due to inaccessible training data and code. The authors aim to promote openness in speech technology research by developing fully transparent models trained on open-source (OS) code and data.

Method: The authors introduce FAMA, the first family of open science SFMs for English and Italian, trained on over 150,000 hours of OS speech data. They also present a new dataset containing 16,000 hours of cleaned and pseudo-labeled speech for both languages.

Result: FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster.

Conclusion: All artifacts, including code, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research.

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [332] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: Incorporating feedback-oriented annotations in AES via the PERSUADE corpus improves performance with LLMs.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy of automated essay scoring by integrating feedback-oriented annotations.

Method: Using two types of annotations (spelling/grammar errors and argumentative components) within the PERSUADE corpus, applying a generative language model for spell-correction and an encoder-based token classifier for marking argumentative elements.

Result: Demonstrated improvements in AES performance when using encoder-based large language models fine-tuned as classifiers.

Conclusion: Feedback-driven annotations can effectively improve automated essay scoring accuracy.

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [333] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
*Andrew Zhu,Evan Osgood,Chris Callison-Burch*

Main category: cs.CL

TL;DR: This paper explores the concept of 'overhearing agents' using large multimodal audio-language models to assist in Dungeons & Dragons gameplay, finding that some models can perform tasks based on implicit audio cues.


<details>
  <summary>Details</summary>
Motivation: To investigate an alternative interaction paradigm for LLM agents that do not actively participate in conversations but assist by listening in on human-to-human interactions.

Method: The study uses large multimodal audio-language models as overhearing agents to assist a Dungeon Master in Dungeons & Dragons gameplay and evaluates their helpfulness through human evaluation.

Result: Some large audio-language models demonstrate the ability to perform overhearing agent tasks using implicit audio cues.

Conclusion: The paper concludes by releasing Python libraries and project code to encourage further research into the overhearing agents paradigm.

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [334] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
*Alexander Gill,Abhilasha Ravichander,Ana Marasović*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）可以生成有效的阅读理解数据集，成本远低于传统众包方式，但其生成的数据集对LLMs的挑战性不如人类创作的数据集。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否能够生成有效的、具有挑战性的阅读理解基准数据集，以替代传统的众包方法。

Method: 通过两个案例研究，使用LLMs生成推理文本基准，并与精心设计的众包数据进行比较。评估了LLM生成的CondaQA和DROP两个高质量阅读理解数据集的有效性和难度。

Result: LLMs生成的数据集通常符合标注指南，成本显著低于原始众包工作，但对LLMs的挑战性较低。

Conclusion: 尽管LLMs可以生成有效且低成本的评价数据，但它们可能丢失了一些关键特性，因此需要重新评估直接使用LLMs生成基准数据的方法。

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [335] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
*Haobo Zhang,Jiayu Zhou*

Main category: cs.CL

TL;DR: 在大型语言模型微调任务中，现有的模型合并方法对于使用低秩适应(LoRA)微调的模型效果不佳。本文提出了正交子空间用于鲁棒模型合并(OSRM)方法，在微调前约束LoRA子空间，确保一个任务的更新不会对其他任务的输出产生不利影响。实验表明，该方法提高了合并性能，保留了单任务准确性，并增强了对合并超参数的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并方法无法很好地处理通过低秩适应(LoRA)微调的模型，导致显著的性能下降。这源于模型参数和数据分布之间的相互作用被忽视。

Method: 提出了一种名为正交子空间用于鲁棒模型合并(OSRM)的方法。此方法在微调之前约束LoRA子空间，以保证一个任务的相关更新不会对其他任务的输出产生不利影响。OSRM可以与大多数现有的合并算法无缝集成，减少任务间的干扰。

Result: 在八个数据集上进行的广泛实验表明，该方法不仅提升了合并性能，还保留了单任务的准确性。此外，该方法对合并超参数表现出更强的鲁棒性。

Conclusion: 研究结果强调了模型合并中数据-参数相互作用的重要性，并为合并LoRA模型提供了一个即插即用的解决方案。

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [336] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
*Yuchen Zhuang,Di Jin,Jiaao Chen,Wenqi Shi,Hanrui Wang,Chao Zhang*

Main category: cs.CL

TL;DR: WorkForceAgent-R1 is a LLM-based web agent trained using a rule-based R1-style reinforcement learning framework which substantially outperforms SFT baselines by 10.26-16.59%.


<details>
  <summary>Details</summary>
Motivation: Existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions.

Method: The WorkForceAgent-R1 was trained using a rule-based R1-style reinforcement learning framework designed explicitly to enhance single-step reasoning and planning for business-oriented web navigation tasks. A structured reward function that evaluates both adherence to output formats and correctness of actions was employed.

Result: WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26-16.59% and achieves competitive performance relative to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

Conclusion: WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework, enables automating complex, real-time web navigation tasks in enterprise environments.

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [337] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Main category: cs.CL

TL;DR: The paper presents Multimodal Adversarial Compositionality (MAC), a benchmark for evaluating vulnerabilities in pre-trained multimodal representations using deceptive text samples generated by large language models (LLMs). It also proposes a self-training approach to improve zero-shot methods, enhancing attack success rate and sample diversity.


<details>
  <summary>Details</summary>
Motivation: To address the compositional vulnerabilities in pre-trained multimodal representations that lead to counterintuitive judgments.

Method: Introduced MAC benchmark which uses LLMs to generate deceptive text samples and evaluates them through attack success rate and diversity. Proposed a self-training approach with rejection-sampling fine-tuning and diversity-promoting filtering.

Result: The self-training approach improves zero-shot methods, enhancing both attack success rate and sample diversity. Superior performance is demonstrated using smaller language models like Llama-3.1-8B in revealing vulnerabilities across various multimodal representations.

Conclusion: MAC provides an effective benchmark for exposing vulnerabilities in multimodal representations and the proposed self-training approach enhances the effectiveness of zero-shot methods.

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [338] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
*Alisha Srivastava,Emir Korukluoglu,Minh Nhat Le,Duyen Tran,Chau Minh Pham,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）能够跨语言回忆记忆内容，即使在没有直接预训练数据翻译的情况下也是如此。本文通过三个任务评估了多个模型家族和大小的跨语言记忆能力，并发现GPT-4o等模型在新翻译的文本中能够以较高准确率识别作者、标题和掩码实体。


<details>
  <summary>Details</summary>
Motivation: 尽管已知LLMs可以记忆和回忆英语文本，但这种能力是否能推广到非英语语言或跨语言转移仍不清楚。因此，研究多语言和跨语言的记忆化现象成为必要。

Method: 引入OWL数据集，包含20本书的31.5K对齐摘录，涵盖十种语言。通过三个任务（直接探测、名称填空、前缀探测）评估不同模型家族和大小的记忆化能力。

Result: LLMs能够一致地跨语言回忆内容，即使在没有直接翻译的预训练数据的情况下。例如，GPT-4o在新翻译的摘录中能够69%的时间识别作者和标题，6%的时间预测掩码实体。扰动（如掩码字符、打乱单词）适度降低直接探测准确性。

Conclusion: 结果表明LLMs具有显著的跨语言记忆能力，并揭示了不同模型之间的差异。

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [339] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: 研究构建了一个名为NegVQA的视觉问答基准，包含7379个涉及否定场景的问题，用于评估视觉语言模型对否定的理解能力。实验发现，现有20个最先进的视觉语言模型在处理否定问题时表现显著下降，并揭示了模型规模与性能之间的U型关系。


<details>
  <summary>Details</summary>
Motivation: 否定是语言中的基本现象，能够完全改变句子的意义。随着视觉语言模型被应用于高风险场景，评估其对否定的理解变得至关重要。

Method: 通过利用大型语言模型生成现有视觉问答数据集中问题的否定版本，构建了NegVQA基准。该基准包含7379个二选一的问题，涵盖了多样化的否定场景和图像-问题分布。

Result: 评估结果显示，当前最先进的视觉语言模型在处理否定问题时存在显著困难，相较于原始问题，性能大幅下降。此外，还发现了模型规模与NegVQA性能之间的U型关系：模型规模增加初期会导致性能下降，随后才有所改善。

Conclusion: NegVQA基准揭示了视觉语言模型在理解否定方面的关键缺陷，并为未来视觉语言模型的发展提供了洞见。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [340] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
*Sheng Zhang,Qin Liu,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: This paper explores scaling laws in Electronic Health Records (EHR) using transformer models, finding similar patterns to Large Language Models which can help create more efficient EHR foundation models for clinical tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate whether scaling laws that apply to Large Language Models also apply to Electronic Health Records, enabling predictable performance improvements and resource-efficient training strategies.

Method: Training transformer architectures on patient timeline data from the MIMIC-IV database with varying model sizes and compute budgets, then analyzing the scaling patterns such as IsoFLOPs curves and power-law relationships.

Result: Identified consistent scaling patterns including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility.

Conclusion: EHR models exhibit scaling behavior similar to LLMs, providing predictive insights for developing powerful EHR foundation models to enhance clinical prediction tasks and personalized healthcare.

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [341] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: The paper introduces VeGraph, a framework using LLM agents for claim verification in three phases: graph representation, entity disambiguation, and verification. It shows competitive performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Claim verification requires high accuracy and explainability, especially with complex claims that have intricate semantic structures or obfuscated entities. Traditional methods lack effective disambiguation strategies compromising the verification process.

Method: VeGraph operates in three phases: (1) Graph Representation - decomposing claims into triplets forming a graph-based representation; (2) Entity Disambiguation - resolving ambiguous entities within the graph through iterative interaction with a knowledge base; and (3) Verification - verifying remaining triplets to complete fact-checking.

Result: Experiments using Meta-Llama-3-70B (instruct version) demonstrate VeGraph achieves competitive performance compared to baselines on two benchmarks HoVer and FEVEROUS.

Conclusion: VeGraph effectively addresses claim verification challenges by leveraging the reasoning and comprehension abilities of LLM agents.

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [342] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
*Chiwan Park,Wonjun Jang,Daeryong Kim,Aelim Ahn,Kichang Yang,Woosung Hwang,Jihyeon Roh,Hyerin Park,Hyosun Wang,Min Seok Kim,Jihoon Kang*

Main category: cs.CL

TL;DR: The paper explores the challenge of applying Large Language Models (LLMs) in industrial settings, particularly for conversational agents in e-commerce. It proposes strategies to maintain conversational flexibility while adhering to service-specific constraints, and presents a framework for scalable, controllable, and reliable AI-driven agents.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the need to address the challenges of applying state-of-the-art LLMs in real-world industrial applications, specifically focusing on maintaining flexible conversational abilities while strictly complying with service-specific constraints.

Method: The authors conducted a practical case study of a conversational agent designed for the e-commerce domain. They detailed their implementation workflow and optimizations to overcome the inherent limitations of LLMs in real-world applications.

Result: The findings provide insights into bridging the gap between academic research and real-world application, leading to the development of a framework for creating scalable, controllable, and reliable AI-driven agents.

Conclusion: This paper concludes by emphasizing the importance of balancing flexibility and control in LLMs when applied to industrial settings, and highlights the potential of their proposed framework for developing effective AI-driven conversational agents.

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [343] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
*Haewon Park,Gyubin Choi,Minjun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: Knowledge editing (KE) methods can efficiently modify knowledge in large language models, but they often fail when preceding contexts are present. To address this issue, the authors developed CHED, a benchmark designed to evaluate the context robustness of KE methods, and CoRE, a KE method that strengthens context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge.


<details>
  <summary>Details</summary>
Motivation: Current KE evaluations typically assess editing success by considering only the edited knowledge without any preceding contexts, which is not practical in real-world applications.

Method: The authors developed CHED, a benchmark designed to evaluate the context robustness of KE methods, and CoRE, a KE method that minimizes context-sensitive variance in hidden states of the model for edited knowledge.

Result: Evaluations on CHED show that current KE methods often fail when preceding contexts are present. CoRE improves the editing success rate in situations where a preceding context is present while preserving the overall capabilities of the model.

Conclusion: CoRE is an effective KE method that strengthens context robustness and should be considered in real-world applications.

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [344] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: The paper proposes MBR-OT, a variant of MBR decoding using Wasserstein distance for document-level text generation tasks, which outperforms standard MBR in various tasks.


<details>
  <summary>Details</summary>
Motivation: Document-level text generation tasks are more challenging than sentence-level ones due to the need for understanding longer context. Existing MBR decoding performs well on sentence-level but not on document-level tasks as utility functions are designed for sentences.

Method: Propose MBR-OT, a variant of MBR decoding that uses Wasserstein distance to compute document utility based on a sentence-level utility function.

Result: MBR-OT outperforms standard MBR in document-level machine translation, text simplification, and dense image captioning tasks.

Conclusion: MBR-OT is an effective adaptation of MBR decoding for document-level text generation tasks.

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [345] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
*Linjie Mu,Zhongzhen Huang,Yakun Zhu,Xiangyu Zhao,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.CL

TL;DR: 有效临床决策依赖于迭代、多模态推理。尽管多模态推理模型在数学和科学领域取得成功，但在医学领域的应用尚待开发。本文提出MedE$^2$，一个两阶段后训练流程，用于激发并增强医学领域的多模态推理能力。实验表明，使用MedE$^2$训练的模型在多个医学多模态基准上显著优于基线模型，并具有稳健性和实用性。


<details>
  <summary>Details</summary>
Motivation: 有效临床决策需要跨多种证据源进行迭代、多模态推理。然而，尽管多模态推理模型已在数学和科学领域取得成功，其在医学领域的应用仍较少探索。

Method: 提出MedE$^2$，一个两阶段后训练流程：第一阶段使用2,000个精心设计的文本数据样本微调模型以激发推理行为；第二阶段使用1,500个严格策划的多模态医学案例进一步增强模型的推理能力，使其输出与提出的多模态医学推理偏好对齐。

Result: 实验结果表明，MedE$^2$能有效提高医学多模态模型的推理性能，在多个医学多模态基准上持续优于基线模型。此外，在更大模型和推理时间扩展下的额外验证进一步确认了方法的稳健性和实际用途。

Conclusion: MedE$^2$是一种有效的两阶段后训练流程，能够显著提升医学多模态模型的推理能力，展现出其在临床决策中的潜在价值。

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [346] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
*Yiming Lei,Zhizheng Yang,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: This paper presents ContextQFormer, a context modeling module that enhances multi-turn interaction in multi-modal models using a memory block. The authors also introduce TMDialog, a new open-source dataset featuring longer conversations for pre-training and evaluating multi-modal dialogues. Experiments show ContextQFormer improves the available rate by 2%-4% compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Existing open-source multi-modal models have weak multi-turn interaction capabilities, especially when handling long contexts.

Method: The paper introduces ContextQFormer, a context modeling module with a memory block to improve contextual information representation. They also create TMDialog, a new dataset with longer conversations for pre-training and evaluating multi-turn multi-modal dialogues.

Result: ContextQFormer shows a 2%-4% improvement in available rate over baselines on the TMDialog dataset.

Conclusion: ContextQFormer effectively enhances multi-turn interaction capabilities in multi-modal models, and TMDialog provides a valuable resource for further research in this area.

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [347] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
*Gabriele Sarti,Vilém Zouhar,Malvina Nissim,Arianna Bisazza*

Main category: cs.CL

TL;DR: This paper explores efficient alternatives for word-level quality estimation (WQE) using advances in language model interpretability and uncertainty quantification, evaluates 14 metrics across 12 translation directions with multiple human label sets, and highlights the potential of unsupervised metrics while critiquing supervised methods and single-annotator evaluations.


<details>
  <summary>Details</summary>
Motivation: To find cost-effective WQE techniques that don't rely on large language models or extensive human-labeled data training.

Method: Using recent progress in language model interpretability and uncertainty quantification to identify translation errors within the translation models themselves, and evaluating this approach across numerous metrics and translation directions.

Result: Unsupervised metrics show significant potential, supervised methods struggle with label uncertainty, and single-annotator evaluations are found to be unreliable.

Conclusion: Efficient WQE methods leveraging model interpretability can be effective; there is a need to rethink evaluation practices and consider unsupervised approaches more seriously.

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [348] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
*Yilong Li,Chen Qian,Yu Xia,Ruijie Shi,Yufan Dang,Zihao Xie,Ziming You,Weize Chen,Cheng Yang,Weichuan Liu,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: MAEL is a new framework that allows LLM-driven agents to learn across tasks and accumulate experience, leading to faster convergence and better solutions.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems treat each task in isolation, causing redundant computations and limited generalization. The authors aim to address this by introducing cross-task experiential learning.

Method: Model the task-solving workflow on a graph-structured multi-agent collaboration network where agents propagate information and coordinate through explicit connectivity. During experiential learning, quantify step quality, store rewards with inputs/outputs in individual experience pools. During inference, retrieve high-reward experiences as few-shot examples.

Result: Experimental results show that MAEL enables agents to effectively learn from prior task experiences, achieving faster convergence and producing higher-quality solutions on current tasks.

Conclusion: MAEL offers a novel approach for LLM-driven agents to enhance their effectiveness via cross-task experiential learning.

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [349] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bibo Cai,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: The paper proposes an autonomous experience transfer framework that enables LLMs to transfer knowledge from source tasks to new target tasks, improving their performance without heavy human input.


<details>
  <summary>Details</summary>
Motivation: To reduce the reliance on substantial human labor or time in gathering textual task-solving experiences for LLMs and enhance their generalization capabilities across a growing variety of task types.

Method: Designing an autonomous experience transfer framework which allows LLMs to mimic human cognitive intelligence, transferring experience from existing source tasks to newly encountered target tasks.

Result: Experimental results on 13 datasets show that the framework effectively enhances LLMs' performance.

Conclusion: This framework not only reduces the cost associated with acquiring task-solving experiences but also offers a new approach for the generalization of LLMs.

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [350] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
*Hao Lu,Yanchi Gu,Haoyuan Huang,Yulin Zhou,Ningxin Zhu,Chen Li*

Main category: cs.CL

TL;DR: MCTSr-Zero是一种为开放性、以人为中心的对话设计的蒙特卡罗树搜索框架，通过领域对齐、重生和元提示适应机制改进对话策略探索，生成高质量对话数据以微调大型语言模型PsyLLM，并通过PsyEval基准验证其在心理辅导对话中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 将蒙特卡洛树搜索与大语言模型结合在结构化问题任务中取得了显著成功，但在如心理辅导等开放性对话任务中存在挑战，因为这些任务的成功依赖于主观因素（如共情参与、伦理遵循和与人类偏好的一致性），而非明确的正确性标准。现有的结果导向型MCTS方法可能产生不一致的响应。

Method: 提出MCTSr-Zero框架，核心创新是“领域对齐”，即将MCTS搜索目标从预定义的终态转向符合目标领域原则的对话轨迹（如辅导中的共情）。此外，引入“重生”和“元提示适应”机制以显著扩展探索，允许MCTS考虑不同的初始对话策略。通过生成多轮对话数据来微调大型语言模型PsyLLM，并引入PsyEval作为评估多轮心理辅导对话的基准。

Result: 实验表明，PsyLLM在PsyEval和其他相关指标上达到了最先进的性能，证明了MCTSr-Zero在生成高质量、原则一致的对话数据方面的有效性，解决了大型语言模型在持续遵循复杂心理标准方面的挑战。

Conclusion: MCTSr-Zero框架有效提升了开放性、以人为中心对话的质量，特别是在心理辅导领域，能够生成符合领域原则的高质量对话数据，解决了大型语言模型在复杂心理标准下的一致性问题。

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [351] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
*Maged S. Al-Shaibani,Moataz Ahmed*

Main category: cs.CL

TL;DR: 本研究全面调查了阿拉伯语机器生成文本，分析了不同模型架构在学术和社交媒体领域的多种生成策略所产生的语言特征，并开发了基于BERT的检测模型，该模型在正式语境中表现优异（F1分数高达99.9%）。这是目前对阿拉伯语机器生成文本最全面的研究，结合了多种提示生成方法、模型架构和深入的文体分析。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）能够生成类似人类的文本，这对信息完整性构成了微妙而重大的挑战，特别是在尚未充分探索和资源匮乏的语言如阿拉伯语中，这种挑战更为严重。因此，有必要对阿拉伯语机器生成文本进行深入研究，以应对潜在的信息完整性威胁。

Method: 研究采用了多种生成策略（仅从标题生成、内容感知生成和文本精炼）和不同的模型架构（ALLaM、Jais、Llama和GPT-4），在学术和社交媒体领域进行了全面的阿拉伯语机器生成文本调查。通过文体测量分析，揭示了区分人类书写和机器生成阿拉伯语文本的独特语言模式，并基于这些见解开发了BERT基础的检测模型。

Result: 研究表明，尽管LLMs生成的文本具有类似人类的特性，但它们在阿拉伯语输出中产生了可检测的特征签名，这些特征因领域而异。开发的BERT基础检测模型在正式语境中表现出色，F1分数高达99.9%，并且在不同模型架构中具有强大的精确性。然而，跨域分析证实了文献中先前报告的泛化挑战。

Conclusion: 这项工作代表了迄今为止对阿拉伯语机器生成文本最全面的调查，独特地结合了多种提示生成方法、不同的模型架构和深入的文体测量分析，为开发稳健、语言知情的检测系统奠定了基础，这对于维护阿拉伯语环境中的信息完整性至关重要。

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [352] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
*Yong Zhang,Yanwen Huang,Ning Cheng,Yang Guo,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: The paper presents Sentinel, a lightweight sentence-level compression framework for RAG that uses decoder attention from a small proxy LLM and achieves high compression rates while maintaining QA performance.


<details>
  <summary>Details</summary>
Motivation: Retrieved passages in RAG can be lengthy, noisy, or exceed input limits. Existing compression methods usually need supervised training of dedicated models, which is costly and reduces portability.

Method: Sentinel reframes context filtering as an attention-based understanding task. It probes decoder attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier to identify sentence relevance.

Result: On the LongBench benchmark, Sentinel achieves up to 5× compression while matching the QA performance of 7B-scale compression systems.

Conclusion: Probing native attention signals enables fast, effective, and question-aware context compression.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [353] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
*James Xu Zhao,Jimmy Z. J. Liu,Bryan Hooi,See-Kiong Ng*

Main category: cs.CL

TL;DR: Large language models (LLMs) are widely used for long-form text generation, but factual errors in the responses would undermine their reliability. We systematically investigate the relationship between response length and factuality using an automatic and bi-level long-form factuality evaluation framework.


<details>
  <summary>Details</summary>
Motivation: To explore the effect of response length on factuality in large language models (LLMs).

Method: Introduce an automatic and bi-level long-form factuality evaluation framework to conduct controlled experiments.

Result: Longer responses exhibit lower factual precision due to facts exhaustion rather than error propagation or long context.

Conclusion: Facts exhaustion is the primary cause of factual degradation in longer responses.

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [354] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
*Abhirup Chakravarty,Mark Brenchley,Trevor Breakspear,Ian Lewin,Yan Huang*

Main category: cs.CL

TL;DR: An AES confidence modeling study reformulates score reliability as an n-ary classification problem and develops KWOCCE loss functions, achieving high F1 scores and improved CEFR agreement.


<details>
  <summary>Details</summary>
Motivation: To address the ethical challenge of only releasing AES scores when they meet high reliability standards by assigning a confidence score to each automated score.

Method: Frame confidence estimation as a binary classification task predicting correct CEFR level placement. Reformulate it as an n-ary classification problem using score binning and introduce KWOCCE loss functions that consider the ordinal structure of CEFR labels.

Result: The best model achieved an F1 score of 0.97, allowing release of 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement, outperforming the standalone AES model.

Conclusion: KWOCCE loss functions and n-ary classification approach significantly enhance AES score reliability.

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [355] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
*Nawar Turk,Eeham Khan,Leila Kosseim*

Main category: cs.CL

TL;DR: This paper presents three model architectures for the SemEval-2025 Task 6 (PromiseEval), which focuses on verifying promises in corporate ESG reports. The combined subtask approach achieved a leaderboard score of 0.5268.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of verifying promises in corporate ESG reports by participating in the SemEval-2025 Task 6 (PromiseEval).

Method: Three model architectures are explored: 1) ESG-BERT with task-specific classifier heads, 2) Enhanced architecture with linguistic features tailored for each subtask, and 3) Combined subtask model with attention-based sequence pooling, transformer representations augmented with document metadata, and multi-objective learning.

Result: Experiments on the English portion of the ML-Promise dataset demonstrate progressive improvement across the models, with the combined subtask approach achieving a leaderboard score of 0.5268, outperforming the provided baseline of 0.5227.

Conclusion: The work highlights the effectiveness of linguistic feature extraction, attention pooling, and multi-objective learning in promise verification tasks, despite challenges posed by class imbalance and limited training data.

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [356] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
*Guangtao Zeng,Maohao Shen,Delin Chen,Zhenting Qi,Subhro Das,Dan Gutfreund,David Cox,Gregory Wornell,Wei Lu,Zhang-Wei Hong,Chuang Gan*

Main category: cs.CL

TL;DR: EvoScale是一种样本高效的测试时扩展方法，通过进化过程改进语言模型输出，使小型模型在实际任务中表现更佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在标准化编码基准上表现出色，但在真实世界的软件工程任务（如SWE-Bench中的GitHub问题解决）中表现不佳，尤其是参数少于100B的模型。小模型因其较低的计算成本在实践中更为理想，但提高其性能仍具挑战性。现有的方法主要依赖高质量数据的监督微调，这在大规模下难以实现。另一种方法是测试时扩展：生成多个输出，使用验证器评分并选择最佳输出。虽然有效，但此策略通常需要过多的采样和昂贵的评分，限制了其实际应用。

Method: 我们提出了进化测试时扩展（EvoScale），这是一种样本高效的方法，将生成视为进化过程。通过迭代地通过选择和变异改进输出，EvoScale将输出分布转移到高分区域，减少找到正确解决方案所需的样本数量。为了减少重复采样和选择的开销，我们使用强化学习训练模型以自我进化。模型学会在推理时间无需依赖外部验证器的情况下，跨迭代自我改进其生成分数。

Result: 在SWE-Bench-Verified上评估，EvoScale使我们的32B模型Satori-SWE-32B能够使用少量样本匹配或超过参数超过100B的模型的性能。代码、数据和模型将完全开源。

Conclusion: EvoScale为小规模语言模型提供了一种有效的性能提升方法，在实际应用中具有显著优势，并计划开放所有相关资源。

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [357] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
*Jiaxin Bai,Wei Fan,Qi Hu,Qing Zong,Chunyang Li,Hong Ting Tsang,Hongyu Luo,Yauwai Yim,Haoyu Huang,Xiao Zhou,Feng Qin,Tianshi Zheng,Xi Peng,Xin Yao,Huiwen Yang,Leijie Wu,Yi Ji,Gong Zhang,Renhai Chen,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoSchemaKG是一个完全自主的知识图谱构建框架，无需预定义模式。系统利用大型语言模型从文本中同时提取知识三元组并诱导全面的模式，处理超过5000万份文件，构建了包含9亿多个节点和59亿条边的知识图谱家族ATLAS。该方法在多跳问答任务中优于现有最佳基线，并增强了LLM的事实性。模式诱导与人工编写的模式达到了95%的语义对齐，展示了动态诱导模式的十亿级知识图谱可以有效补充大型语言模型中的参数化知识。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱构建通常需要预定义模式，这限制了其灵活性和扩展性。为了解决这一问题，作者提出了一种无需预定义模式的全自主知识图谱构建框架。

Method: 通过利用大型语言模型，AutoSchemaKG可以从文本中同时提取知识三元组并诱导出全面的模式，涵盖实体和事件，并使用概念化方法将实例组织到语义类别中。

Result: 构建了ATLAS知识图谱家族，包含9亿多个节点和59亿条边。在多跳问答任务中表现优于现有方法，并显著提升了LLM的事实性。模式诱导与人工编写的模式达到95%的语义对齐。

Conclusion: 动态诱导模式的十亿级知识图谱可以有效补充大型语言模型中的参数化知识，且无需人工干预即可实现高语义对齐。

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [358] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: Active Layer-Contrastive Decoding (ActLCD) is a new decoding strategy that uses reinforcement learning to reduce hallucinations in large language models.


<details>
  <summary>Details</summary>
Motivation: Despite recent improvements in decoding methods for large language models, they still suffer from hallucinations especially over longer contexts.

Method: ActLCD casts decoding as a sequential decision-making problem and employs a reinforcement learning policy guided by a reward-aware classifier.

Result: ActLCD surpasses state-of-the-art methods across five benchmarks.

Conclusion: ActLCD is effective in mitigating hallucinations in diverse generation scenarios.

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [359] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
*Arthur S. Bianchessi,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CL

TL;DR: Transformer-based language models use positional encoding (PE) to manage token order and context length. Current PE methods lack theoretical clarity and rely on limited evaluation metrics. The paper proposes the Bayesian Attention Mechanism (BAM), which formulates PE as a prior in a probabilistic model, unifies existing methods, and introduces a new Generalized Gaussian positional prior for better long-context generalization. BAM performs well in long context retrieval while maintaining perplexity and adding minimal parameters.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of theoretical clarity in existing PE methods and their reliance on limited evaluation metrics for assessing context length extrapolation.

Method: The method proposed is the Bayesian Attention Mechanism (BAM), which treats positional encoding as a prior within a probabilistic model. This framework unifies existing methods and introduces a Generalized Gaussian positional prior.

Result: BAM enables accurate information retrieval at 500 times the training context length, outperforming previous state-of-the-art methods in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.

Conclusion: BAM provides a strong theoretical foundation for positional encoding, unifies existing methods, and improves long-context generalization significantly.

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [360] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
*Roksana Goworek,Harpal Karlcut,Muhammad Shezad,Nijaguna Darshana,Abhishek Mane,Syam Bondada,Raghav Sikka,Ulvi Mammadov,Rauf Allahverdiyev,Sriram Purighella,Paridhi Gupta,Muhinyia Ndegwa,Haim Dubossarsky*

Main category: cs.CL

TL;DR: This paper addresses the need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer by releasing new sense-annotated datasets and a semi-automatic annotation method.


<details>
  <summary>Details</summary>
Motivation: The motivation is the critical need for quality evaluation datasets in low-resource languages to effectively leverage multilingual pretraining and advance cross-lingual transfer.

Method: The method includes releasing new sense-annotated datasets of sentences with polysemous words in nine low-resource languages and presenting a semi-automatic annotation approach.

Result: Results from Word-in-Context formatted experiments highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings.

Conclusion: The conclusion is that targeted dataset creation and evaluation are crucial for effective polysemy disambiguation in low-resource settings, and the released datasets and code will support further research into fair, robust, and truly multilingual NLP.

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [361] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: A novel agentic ML training framework with exploration-enriched fine-tuning, step-wise RL, and a task-specific reward module is proposed to train ML-Agent using a 7B-sized Qwen-2.5 LLM, which outperforms a 671B-sized agent.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing approaches that rely heavily on manual prompt engineering and lack adaptation based on diverse experimental experiences.

Method: The framework includes exploration-enriched fine-tuning for diverse actions, step-wise RL for efficient training, and a reward module to unify feedback signals for RL optimization.

Result: ML-Agent, despite being trained on only 9 ML tasks, outperforms a much larger 671B-sized DeepSeek-R1 agent, shows continuous performance improvement, and exhibits strong cross-task generalization.

Conclusion: The proposed agentic ML training framework effectively enables autonomous ML with superior performance and adaptability.

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [362] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
*Mohamad Chehade,Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Dinesh Manocha,Hao Zhu,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: SITAlign is an inference time framework that maximizes a primary objective while satisfying threshold-based constraints on secondary criteria, outperforming state-of-the-art methods in aligning large language models with human preferences.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to aligning large language models with humans typically frame the problem as multi-objective optimization but often overlook how humans actually make decisions. Human decision making follows satisficing strategies, optimizing primary objectives while ensuring others meet acceptable thresholds.

Method: Propose SITAlign, an inference time framework that addresses the multifaceted nature of alignment by maximizing a primary objective while satisfying threshold-based constraints on secondary criteria. Theoretical insights are provided by deriving sub-optimality bounds of this approach.

Result: Empirically validated SITAlign's performance through extensive experimentation on multiple benchmarks. On the PKU-SafeRLHF dataset, SITAlign outperforms the state-of-the-art multi-objective decoding strategy by a margin of 22.3% in terms of GPT-4 win-tie rate for helpfulness reward while adhering to the threshold on harmlessness.

Conclusion: SITAlign bridges the gap between existing multi-objective optimization approaches and human decision-making strategies, providing both theoretical and empirical support for its effectiveness.

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [363] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: 大型语言模型在说服力方面展现出潜力，但现有的训练LLM说服者的成果仍处于初步阶段。为了解决当前LLM缺乏对对手思维和意见的主动动态建模能力的问题，本文提出了ToMAP方法，通过加入两个理论心智模块来增强说服者对对手心理状态的认知和分析能力。实验表明，尽管参数量仅为3B，ToMAP在多个说服对象模型和不同语料库中超越了如GPT-4o等更大基线模型，相对提升达39.4%。此外，ToMAP表现出复杂的推理链、减少重复性，生成更多样且有效的论点，并适合长时间对话，采用更逻辑和以对手为中心的策略。


<details>
  <summary>Details</summary>
Motivation: 虽然人类擅长主动和动态地建模对手的想法和观点，但目前的大型语言模型（LLM）在这一方面的表现有限，导致多样性和对手意识不足。因此，需要一种新方法来提高LLM的说服能力。

Method: 提出了一种名为ToMAP的新方法，通过结合两个理论心智模块，使说服者能够更好地感知和分析对手的心理状态。具体来说，首先促使说服者考虑目标核心主张可能存在的反对意见，然后使用文本编码器和经过训练的MLP分类器预测对手对这些反论点的当前立场。同时设计了一种强化学习方案，帮助说服者学会分析与对手相关的信息并生成更有效的论据。

Result: 实验结果表明，ToMAP说服者在参数量仅为3B的情况下，优于许多更大的基线模型（如GPT-4o），相对增益达到39.4%。此外，ToMAP表现出复杂的推理链、减少重复性，生成更多样且有效的论点，并适合长时间对话，采用更逻辑和以对手为中心的策略。

Conclusion: ToMAP方法有效提高了说服者的多样性和说服效果，展现了其在开发更具说服力的语言代理方面的潜力。

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [364] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
*Ali Behrouz,Zeman Li,Praneeth Kacham,Majid Daliri,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.CL

TL;DR: ATLAS is a new long-term memory module that overcomes the limitations of current models in long context understanding and extrapolation. It enhances memory capacity, updates based on current and past tokens, and manages fixed-size memory more expressively.


<details>
  <summary>Details</summary>
Motivation: Current models struggle with tasks requiring long context understanding and extrapolation to longer sequences due to limited memory capacity, online nature of update, and less expressive management of fixed-size memory.

Method: The paper introduces ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens. This leads to the development of DeepTransformers, a new family of Transformer-like architectures.

Result: Experimental results show that ATLAS surpasses the performance of Transformers and recent linear recurrent models in language modeling, common-sense reasoning, recall-intensive, and long-context understanding tasks. It also improves the long context performance of Titans, achieving +80% accuracy in 10M context length of BABILong benchmark.

Conclusion: ATLAS enhances the capabilities of models in handling longer sequences and complex tasks requiring long context understanding.

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [365] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
*Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: DeepTheorem is a new framework that uses natural language to improve LLMs' mathematical reasoning, with a large dataset and novel reinforcement learning strategy, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap between traditional automated theorem proving approaches, which rely on formal proof systems, and the strengths of LLMs derived from informal, natural language knowledge.

Method: Propose DeepTheorem, an informal theorem-proving framework including a large-scale benchmark dataset and a novel reinforcement learning strategy (RL-Zero) tailored to informal theorem proving.

Result: DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality.

Conclusion: DeepTheorem has the potential to fundamentally advance automated informal theorem proving and mathematical exploration.

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [366] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Main category: cs.CL

TL;DR: 当前视觉-语言模型（VLMs）在解码简单的视觉线索方面表现出一些惊人的能力，但在需要抽象推理、横向思维和理解视觉隐喻的任务上仍然存在显著困难。为了研究VLMs解决文字画谜题的能力，本文构建了一个多样化的英语文字画谜题基准测试库，并分析了不同VLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 研究视觉-语言模型在解决需要多模态抽象、象征性推理以及对文化和语言双关语的理解的复杂任务上的表现。

Method: 构建一个手工生成和注释的多样化英语文字画谜题基准测试库，涵盖从简单的象形替换到基于空间提示的谜题。使用该基准测试库评估不同的视觉-语言模型。

Result: 视觉-语言模型在处理简单的视觉线索时表现出一定能力，但在需要抽象推理、横向思考和理解视觉隐喻的任务上表现不佳。

Conclusion: 视觉-语言模型在解决复杂的文字画谜题上仍有很大的改进空间，特别是在抽象推理和理解视觉隐喻方面。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [367] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
*Wentao Zhang,Woojeong Kim,Yuntian Deng*

Main category: cs.CL

TL;DR: Conversational agents using LLMs generate vast conversational data, but existing approaches miss insights by treating interactions as independent. This paper introduces Aggregative Question Answering and a benchmark WildChat-AQA to address this gap.


<details>
  <summary>Details</summary>
Motivation: To extract meaningful societal insights from large-scale conversational data generated by chatbot interactions.

Method: Introduced Aggregative Question Answering task and constructed a benchmark WildChat-AQA with 6,027 questions from 182,330 real-world conversations.

Result: Existing methods struggle with reasoning or computational costs for aggregative queries.

Conclusion: There is a need for new approaches to effectively extract collective insights from large-scale conversational data.

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


### [368] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Main category: cs.CL

TL;DR: The paper presents KGQAGen, an LLM-in-the-loop framework that generates high-quality QA benchmarks for Knowledge Graph Question Answering (KGQA) systems to improve evaluation.


<details>
  <summary>Details</summary>
Motivation: Current KGQA datasets have critical quality issues such as inaccurate annotations, ambiguous questions, and outdated knowledge. Manual audit reveals low factual correctness rate.

Method: Introduced KGQAGen which uses structured knowledge grounding, LLM-guided generation, and symbolic verification to create challenging and verifiable QA instances. Constructed a benchmark KGQAGen-10k using this framework.

Result: State-of-the-art KG-RAG models struggle on the KGQAGen-10k benchmark, indicating its effectiveness in exposing model limitations.

Conclusion: Advocates for more rigorous benchmark construction in KGQA and positions KGQAGen as a scalable solution for advancing KGQA evaluation.

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [369] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
*Zijun Yao,Yantao Liu,Yanxu Chen,Jianhui Chen,Junfeng Fang,Lei Hou,Juanzi Li,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 近期发展的大型推理模型（LRMs）在长链推理任务中表现出强大能力，但其在事实追寻任务中的幻觉问题仍存在争议。本文从三个角度探讨了LRMs的幻觉问题：1) 全面评估不同训练方式对幻觉的影响；2) 分析两种关键认知行为对事实性的具体影响；3) 探讨模型不确定性与幻觉的关系。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在复杂任务上表现出色，但它们在事实追寻任务中的幻觉问题尚未得到充分研究和解决。这促使我们探索LRMs是否更容易产生幻觉以及其背后的原因。

Method: 本文采用以下方法进行研究：1) 对LRMs进行全面评估，分析不同后训练方法对幻觉的影响；2) 通过行为分析识别Flaw Repetition和Think-Answer Mismatch两种关键认知行为；3) 研究模型不确定性与幻觉之间的关系。

Result: 研究发现：1) 冷启动监督微调和可验证奖励强化学习能有效减轻LRMs的幻觉；2) Flaw Repetition和Think-Answer Mismatch是影响LRMs事实性的关键因素；3) 模型不确定性与事实准确性之间的错配通常会导致更严重的幻觉。

Conclusion: 本文提供了对LRMs幻觉现象的初步理解，并强调了优化后训练流程、改进模型认知行为及减少不确定性的重要性。

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [370] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
*Luke McDermott,Robert W. Heath Jr.,Rahul Parhi*

Main category: cs.CL

TL;DR: LoLA:Low-rank Linear Attention with sparse caching，通过分离存储关键值对提高线性注意力模型性能，适用于长序列推理任务，效果显著且资源消耗小。


<details>
  <summary>Details</summary>
Motivation: Transformer大语言模型在长序列推理时存在二次复杂度问题，线性注意力方法虽高效但无法准确近似softmax注意力，现有方法在短上下文任务中表现尚可，但在长上下文中因“内存冲突”无法召回重要信息。

Method: 提出LoLA方法，将过去的键值对分配到三种内存形式：近期对的滑动窗口、难以记忆对的稀疏全局缓存和通用对的线性注意递归隐藏状态，单独存储会干扰过去联想记忆的额外键值对。

Result: LoLA在长达8K上下文长度的针 haystack任务中实现仅占Llama-3.1 8B缓存4.6倍小的情况下，将基础次二次模型在4K上下文长度的精度从0.6%提升至97.4%，并在零样本常识推理任务中表现出色。

Conclusion: LoLA是一种极其轻量级的方法，几乎所有结果都可以在单个消费级GPU上重现，为长序列推理任务提供了一种高效解决方案。

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [371] [In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge](https://arxiv.org/abs/2505.22767)
*Eleni Vasilaki*

Main category: cs.HC

TL;DR: The paper re-frames Large Language Models (LLMs) as dynamic instantiations of Collective Knowledge (CK), emphasizing intelligence through dialogue rather than static storage. It explores emergent dialogue patterns, fine-tuning implications, and co-augmentation between human and machine cognition.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical and experiential re-framing of LLMs as dynamic instantiations of Collective Knowledge (CK), moving away from traditional architectural, behavioral, or training-data analyses.

Method: Drawing on concepts from neuroscience and AI, the author examines emergent dialogue patterns with ChatGPT-4, the implications of fine-tuning, and the notion of co-augmentation.

Result: This perspective provides a new lens for understanding interaction, representation, and agency in contemporary AI systems.

Conclusion: LLMs can be understood as dynamic instantiations of collective human knowledge, where intelligence emerges through dialogue.

Abstract: Large Language Models (LLMs) are typically analysed through architectural,
behavioural, or training-data lenses. This article offers a theoretical and
experiential re-framing: LLMs as dynamic instantiations of Collective human
Knowledge (CK), where intelligence is evoked through dialogue rather than
stored statically. Drawing on concepts from neuroscience and AI, and grounded
in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,
the implications of fine-tuning, and the notion of co-augmentation: mutual
enhancement between human and machine cognition. This perspective offers a new
lens for understanding interaction, representation, and agency in contemporary
AI systems.

</details>


### [372] [Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages](https://arxiv.org/abs/2505.22831)
*Peiling Jiang,Haijun Xia*

Main category: cs.HC

TL;DR: Web-based activities are distributed across webpages, but conventional browsers with stacks of tabs fail to support operating and synthesizing large volumes of information across pages. While recent AI systems enable fully automated web browsing and information synthesis, they often diminish user agency and hinder contextual understanding. Therefore, we explore how AI could augment users' interactions with content across webpages and mitigate cognitive and manual efforts.


<details>
  <summary>Details</summary>
Motivation: Conventional browsers cannot operate and synthesize large volumes of information across pages effectively. Recent AI systems enable fully automated web browsing and information synthesis, but they diminish user agency and hinder contextual understanding.

Method: Through literature on information tasks and web browsing challenges, and an iterative design process, the researchers developed Orca, a prototype web browser that leverages AI to support user-driven exploration, operation, organization, and synthesis of web content at scale. Webpages are treated as malleable materials that humans and AI can collaboratively manipulate and compose into a malleable, dynamic, and browser-level workspace.

Result: Evaluation revealed an increased 'appetite' for information foraging, enhanced user control, and more flexibility in sensemaking across a broader information landscape on the web.

Conclusion: Orca, by treating webpages as malleable materials and leveraging AI, successfully augments users' interactions with content across webpages and mitigates cognitive and manual efforts.

Abstract: Web-based activities are fundamentally distributed across webpages. However,
conventional browsers with stacks of tabs fail to support operating and
synthesizing large volumes of information across pages. While recent AI systems
enable fully automated web browsing and information synthesis, they often
diminish user agency and hinder contextual understanding. Therefore, we explore
how AI could instead augment users' interactions with content across webpages
and mitigate cognitive and manual efforts. Through literature on information
tasks and web browsing challenges, and an iterative design process, we present
a rich set of novel interactions with our prototype web browser, Orca.
Leveraging AI, Orca supports user-driven exploration, operation, organization,
and synthesis of web content at scale. To enable browsing at scale, webpages
are treated as malleable materials that humans and AI can collaboratively
manipulate and compose into a malleable, dynamic, and browser-level workspace.
Our evaluation revealed an increased "appetite" for information foraging,
enhanced user control, and more flexibility in sensemaking across a broader
information landscape on the web.

</details>


### [373] [HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding](https://arxiv.org/abs/2505.22906)
*Emmanuel Anaya González,Raven Rothkopf,Sorin Lerner,Nadia Polikarpova*

Main category: cs.HC

TL;DR: 在AI编程工具可能提升程序员能力和生产力的同时，也可能使用户过度依赖系统提供的解决方案。为此，本文提出了Human-in-the-loop Decoding（人类在循环解码）技术，并通过HiLDe代码补全助手实现，让用户能够直接影响LLM决策，与传统代码补全助手相比，显著减少了漏洞的产生并更好地实现了用户的编程目标。


<details>
  <summary>Details</summary>
Motivation: AI编程工具虽然能提高程序员的能力和生产力，但可能导致用户过度依赖系统提供的解决方案，从而在关键领域（如软件安全）产生严重后果。

Method: 提出了一种名为Human-in-the-loop Decoding（人类在循环解码）的新颖交互技术，允许用户观察并直接影响代码生成过程中LLM的决策，使其输出与个人需求一致。该技术通过HiLDe代码补全助手实现，它突出显示LLM做出的关键决策，并为用户提供局部替代方案进行探索。

Result: 在一项针对安全性相关任务的受试者内研究（N=18）中发现，与传统的代码补全助手相比，HiLDe引导参与者生成了显著更少的漏洞，并更好地将代码生成与其目标对齐。

Conclusion: Human-in-the-loop Decoding技术及其在HiLDe中的实现，可以帮助用户在使用AI编程工具时保持主动性，减少安全隐患，更好地满足个性化需求。

Abstract: While AI programming tools hold the promise of increasing programmers'
capabilities and productivity to a remarkable degree, they often exclude users
from essential decision-making processes, causing many to effectively "turn off
their brains" and over-rely on solutions provided by these systems. These
behaviors can have severe consequences in critical domains, like software
security. We propose Human-in-the-loop Decoding, a novel interaction technique
that allows users to observe and directly influence LLM decisions during code
generation, in order to align the model's output with their personal
requirements. We implement this technique in HiLDe, a code completion assistant
that highlights critical decisions made by the LLM and provides local
alternatives for the user to explore. In a within-subjects study (N=18) on
security-related tasks, we found that HiLDe led participants to generate
significantly fewer vulnerabilities and better align code generation with their
goals compared to a traditional code completion assistant.

</details>


### [374] [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
*Boning Zhao*

Main category: cs.HC

TL;DR: This paper presents Human Empathy as Encoder (HEAE), a novel AI framework that integrates student narratives and teacher-derived 'Empathy Vectors' to assess depression severity in special education settings, achieving 82.74% classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Standardized questionnaires and automated methods for assessing student depression often fail to capture the nuances of individual experiences or incorporate teachers' empathetic insights effectively.

Method: The HEAE framework combines student narrative text with a 9-dimensional 'Empathy Vector' derived from teachers, using the PHQ-9 framework as guidance. This approach aims to translate empathetic understanding into structured AI input while enhancing human judgment.

Result: The rigorous experiments resulted in an accuracy of 82.74% for classifying depression severity into 7 levels, demonstrating the effectiveness of the multimodal fusion, text representation, and classification architecture.

Conclusion: This work highlights a promising direction for responsible and ethical affective computing by embedding human empathy into AI systems, fostering human-AI collaboration in sensitive contexts like special education.

Abstract: Assessing student depression in sensitive environments like special education
is challenging. Standardized questionnaires may not fully reflect students'
true situations. Furthermore, automated methods often falter with rich student
narratives, lacking the crucial, individualized insights stemming from
teachers' empathetic connections with students. Existing methods often fail to
address this ambiguity or effectively integrate educator understanding. To
address these limitations by fostering a synergistic human-AI collaboration,
this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered
AI framework for transparent and socially responsible depression severity
assessment. Our approach uniquely integrates student narrative text with a
teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by
the PHQ-9 framework,to explicitly translate tacit empathetic insight into a
structured AI input enhancing rather than replacing human judgment. Rigorous
experiments optimized the multimodal fusion, text representation, and
classification architecture, achieving 82.74% accuracy for 7-level severity
classification. This work demonstrates a path toward more responsible and
ethical affective computing by structurally embedding human empathy

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [375] [Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders](https://arxiv.org/abs/2505.23053)
*Wei-Hsiang Huang,Chen-Wei Ke,Wei-Ning Chiu,Yu-Xuan Su,Chun-Chun Yang,Chieh-Yuan Cheng,Yun-Nung Chen,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: Large language models (LLMs) have brought new opportunities to recommender systems. This study provides a systematic taxonomy of LLM-based recommenders, introduces a unified evaluation platform, and discusses future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic understanding and classification of how LLMs are used in recommender systems and offer practical guidance for future advancements.

Method: Propose a taxonomy classifying approaches into Pure LLM Recommenders and Augmented LLM Recommenders. Introduce a unified evaluation platform for benchmarking representative models under consistent settings.

Result: The taxonomy and evaluation platform allow for better understanding and fair comparison of different LLM-based recommendation methods, highlighting key design choices that impact performance.

Conclusion: This work provides a comprehensive overview of LLM-based recommenders, offers practical guidance, and outlines promising directions for advancing next-generation LLM-powered recommender systems.

Abstract: Large language models (LLMs) have introduced new paradigms for recommender
systems by enabling richer semantic understanding and incorporating implicit
world knowledge. In this study, we propose a systematic taxonomy that
classifies existing approaches into two categories: (1) Pure LLM Recommenders,
which rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate
additional non-LLM techniques to enhance performance. This taxonomy provides a
novel lens through which to examine the evolving landscape of LLM-based
recommendation. To support fair comparison, we introduce a unified evaluation
platform that benchmarks representative models under consistent experimental
settings, highlighting key design choices that impact effectiveness. We
conclude by discussing open challenges and outlining promising directions for
future research. This work offers both a comprehensive overview and practical
guidance for advancing next-generation LLM-powered recommender.

</details>


### [376] [From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval](https://arxiv.org/abs/2505.23059)
*Dohyeon Lee,Yeonseok Jeong,Seung-won Hwang*

Main category: cs.IR

TL;DR: 在信息检索中，链式思维提示可能导致过度思考。为了解决冗余轨迹和误导性推理的问题，我们提出了状态机推理（SMR），这是一种基于转换的推理框架。实验表明，与传统的链式思维推理相比，SMR提高了检索性能，同时减少了令牌使用量。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维提示在大型语言模型中实现了复杂的推理，但它常常导致过度思考，产生冗长且语义上重复的痕迹，几乎没有好处。这促使我们需要一种新的方法来解决信息检索中的问题。

Method: 提出了一种名为状态机推理（SMR）的转换基础推理框架，该框架由离散动作组成（细化、重新排序、停止），支持早期停止和细粒度控制。

Result: 实验显示，与传统方法相比，SMR将检索性能提高了3.4%，同时减少了74.4%的令牌使用。它无需任务特定调整即可推广到各种大型语言模型和检索器。

Conclusion: 状态机推理（SMR）提供了一个实用的替代方案，以克服传统链式思维推理的缺点，显著提高检索效率并减少资源消耗。

Abstract: Chain-of-Thought (CoT) prompting enables complex reasoning in large language
models (LLMs), including applications in information retrieval (IR). However,
it often leads to overthinking, where models produce excessively long and
semantically redundant traces with little or no benefit. We identify two key
challenges in IR: redundant trajectories that revisit similar states and
misguided reasoning that diverges from user intent. To address these, we
propose State Machine Reasoning (SMR), a transition-based reasoning framework
composed of discrete actions (Refine, Rerank, Stop) that support early stopping
and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show
that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token
usage by 74.4%. It generalizes across LLMs and retrievers without requiring
task-specific tuning, offering a practical alternative to conventional CoT
reasoning. The code and details are available at https://github.com/ldilab/SMR.

</details>


### [377] [Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking](https://arxiv.org/abs/2505.23250)
*Pascal J. Sager,Ashwini Kamaraj,Benjamin F. Grewe,Thilo Stadelmann*

Main category: cs.IR

TL;DR: The paper presents a hybrid retrieval pipeline that combines lexical precision, semantic generalization, and deep contextual re-ranking for retrieving relevant scientific literature for social media posts. This approach achieved 1st place on the development leaderboard and 3rd place on the test leaderboard in the CLEF CheckThat! 2025 competition.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of retrieving relevant scientific literature for given social media posts, bridging the gap between informal and formal language.

Method: A hybrid retrieval pipeline was used which includes BM25-based keyword matching to return top 30 candidates, FAISS vector store with a fine-tuned INF-Retriever-v1 model for dense semantic retrieval yielding 100 candidates, then merging and re-ranking these via an LLM-based cross-encoder.

Result: The system achieved a mean reciprocal rank at 5 (MRR@5) of 76.46% on the development set and 66.43% on the hidden test set, ranking 1st in the development leaderboard and 3rd in the test leaderboard out of 31 teams.

Conclusion: This strong performance was achieved using open-source models run locally without external training data, proving the effectiveness of a well-designed and fine-tuned retrieval pipeline.

Abstract: We present the methodology and results of the Deep Retrieval team for subtask
4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving
relevant scientific literature for given social media posts. To address this
task, we propose a hybrid retrieval pipeline that combines lexical precision,
semantic generalization, and deep contextual re-ranking, enabling robust
retrieval that bridges the informal-to-formal language gap. Specifically, we
combine BM25-based keyword matching with a FAISS vector store using a
fine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns
the top 30 candidates, and semantic search yields 100 candidates, which are
then merged and re-ranked via a large language model (LLM)-based cross-encoder.
  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the
development set and 66.43% on the hidden test set, securing the 1st position on
the development leaderboard and ranking 3rd on the test leaderboard (out of 31
teams), with a relative performance gap of only 2 percentage points compared to
the top-ranked system. We achieve this strong performance by running
open-source models locally and without external training data, highlighting the
effectiveness of a carefully designed and fine-tuned retrieval pipeline.

</details>


### [378] [Engineering Serendipity through Recommendations of Items with Atypical Aspects](https://arxiv.org/abs/2505.23580)
*Ramit Aditya,Razvan Bunescu,Smita Nannaware,Erfan Al-Hossami*

Main category: cs.IR

TL;DR: 本论文提出了一种通过推荐具有非典型特征的项目来实现意外惊喜（serendipity）的新任务，并设计了一个基于LLM的系统管道，用于提取和评估这些特征。同时，引入了带有手动注释的数据集和人工生成的用户配置文件数据集以支持开发和评估。实验结果表明，系统生成的意外惊喜排名与基于手动注释的真实排名高度相关。


<details>
  <summary>Details</summary>
Motivation: 顾客在餐厅或酒店遇到与自己兴趣相符的非典型方面时，可能会产生难忘的体验。例如，一个喜欢折纸的顾客可能会被餐厅等候区的折纸制作站所吸引；而对巴洛克音乐感兴趣的客人可能会对酒店大堂展示的18世纪大键琴感兴趣。这促使研究者提出了通过推荐具有非典型特征的项目来实现意外惊喜的任务。

Method: 该方法包括：1) 使用基于LLM的系统管道从项目评论中提取非典型特征；2) 估计并聚合特定用户的效用值，计算意外惊喜潜力；3) 根据意外惊喜潜力重新排序推荐列表；4) 引入动态选择上下文学习示例的自定义过程以改进LLM对非典型性和效用的判断；5) 利用带有手动注释的Yelp评论数据集和人工生成的用户配置文件数据集进行系统开发和评估。

Result: 实验评估显示，系统生成的基于意外惊喜的排名与基于手动注释的真实排名高度相关，证明了该方法的有效性。

Conclusion: 本文提出的新的推荐任务和相关系统可以推动进一步的研究，探索超越准确性以提升用户满意度的推荐方法。

Abstract: A restaurant dinner or a hotel stay may lead to memorable experiences when
guests encounter unexpected aspects that also match their interests. For
example, an origami-making station in the waiting area of a restaurant may be
both surprising and enjoyable for a customer who is passionate about paper
crafts. Similarly, an exhibit of 18th century harpsichords would be atypical
for a hotel lobby and likely pique the interest of a guest who has a passion
for Baroque music. Motivated by this insight, in this paper we introduce the
new task of engineering serendipity through recommendations of items with
atypical aspects. We describe an LLM-based system pipeline that extracts
atypical aspects from item reviews, then estimates and aggregates their
user-specific utility in a measure of serendipity potential that is used to
rerank a list of items recommended to the user. To facilitate system
development and evaluation, we introduce a dataset of Yelp reviews that are
manually annotated with atypical aspects and a dataset of artificially
generated user profiles, together with crowdsourced annotations of user-aspect
utility values. Furthermore, we introduce a custom procedure for dynamic
selection of in-context learning examples, which is shown to improve LLM-based
judgments of atypicality and utility. Experimental evaluations show that
serendipity-based rankings generated by the system are highly correlated with
ground truth rankings for which serendipity scores are computed from manual
annotations of atypical aspects and their user-dependent utility. Overall, we
hope that the new recommendation task and the associated system presented in
this paper catalyze further research into recommendation approaches that go
beyond accuracy in their pursuit of enhanced user satisfaction.
  The datasets and the code are made publicly available at
https://github.com/ramituncc49er/ATARS .

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [379] [An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book](https://arxiv.org/abs/2505.22678)
*Jiahao Yang,Ran Fang,Ming Zhang,Jun Zhou*

Main category: q-fin.TR

TL;DR: In high-frequency trading, leveraging limit order books to model stock price movements is crucial. Recent deep learning models struggle with this task due to the high-dimensional and volatile nature of the data. This paper proposes a novel approach using Siamese architecture to enhance existing deep learning models' performance by processing ask and bid sides separately. The authors also integrate multi-head attention mechanisms with LSTM modules for modeling stock price movements.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper stems from the challenges faced in modeling stock price movements using high-dimensional and volatile limit order book data. Even recent deep learning models often fail to capture price movement patterns effectively without well-designed features.

Method: The method involves using Siamese architecture to process the ask and bid sides of LOB data separately with shared parameters. Additionally, Multi-Head Attention (MHA) mechanisms are integrated with Long Short-Term Memory (LSTM) modules to model stock price movements.

Result: The results show that the proposed method improves the performance of strong baselines in over 75% of cases, excluding the poorly performing MLP baseline. Multi-Head Attention enhances model performance, especially over shorter forecasting horizons.

Conclusion: The conclusion is that the proposed Siamese-based method significantly enhances the performance of existing deep learning models in modeling stock price movements. Integrating Multi-Head Attention with LSTM further improves the model's effectiveness.

Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model
stock price movements is crucial for achieving profitable outcomes. However,
this task is challenging due to the high-dimensional and volatile nature of the
original data. Even recent deep learning models often struggle to capture price
movement patterns effectively, particularly without well-designed features. We
observed that raw LOB data exhibits inherent symmetry between the ask and bid
sides, and the bid-ask differences demonstrate greater stability and lower
complexity compared to the original data. Building on this insight, we propose
a novel approach in which leverages the Siamese architecture to enhance the
performance of existing deep learning models. The core idea involves processing
the ask and bid sides separately using the same module with shared parameters.
We applied our Siamese-based methods to several widely used strong baselines
and validated their effectiveness using data from 14 military industry stocks
in the Chinese A-share market. Furthermore, we integrated multi-head attention
(MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate
its role in modeling stock price movements. Our experiments used raw data and
widely used Order Flow Imbalance (OFI) features as input with some strong
baseline models. The results show that our method improves the performance of
strong baselines in over 75$% of cases, excluding the Multi-Layer Perception
(MLP) baseline, which performed poorly and is not considered practical.
Furthermore, we found that Multi-Head Attention can enhance model performance,
particularly over shorter forecasting horizons.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [380] [Non-convex entropic mean-field optimization via Best Response flow](https://arxiv.org/abs/2505.22760)
*Razvan-Andrei Lascu,Mateusz B. Majka*

Main category: math.OC

TL;DR: The paper explores minimizing non-convex functionals on probability measures, regularized by relative entropy, and solving related min-max problems using the Best Response flow. It shows how to choose the regularizer to ensure a unique global minimizer.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of minimizing non-convex functionals and solving entropy-regularized min-max problems, extending previous work that focused on convex optimization.

Method: Utilize the Best Response flow to study convergence influenced by non-convexity, regularization parameters, and reference measure tail behavior. Choose the regularizer so the Best Response operator becomes a contraction with respect to the $L^1$-Wasserstein distance.

Result: Demonstrates conditions under which the Best Response operator becomes a contraction, ensuring a unique fixed point and global minimizer. Also applies these results to reinforcement learning for policy optimization.

Conclusion: The assumption of convexity can be relaxed by carefully choosing the regularizer, extending the applicability of the Best Response flow to non-convex optimization problems.

Abstract: We study the problem of minimizing non-convex functionals on the space of
probability measures, regularized by the relative entropy (KL divergence) with
respect to a fixed reference measure, as well as the corresponding problem of
solving entropy-regularized non-convex-non-concave min-max problems. We utilize
the Best Response flow (also known in the literature as the fictitious play
flow) and study how its convergence is influenced by the relation between the
degree of non-convexity of the functional under consideration, the
regularization parameter and the tail behaviour of the reference measure. In
particular, we demonstrate how to choose the regularizer, given the non-convex
functional, so that the Best Response operator becomes a contraction with
respect to the $L^1$-Wasserstein distance, which then ensures the existence of
its unique fixed point, which is then shown to be the unique global minimizer
for our optimization problem. This extends recent results where the Best
Response flow was applied to solve convex optimization problems regularized by
the relative entropy with respect to arbitrary reference measures, and with
arbitrary values of the regularization parameter. Our results explain precisely
how the assumption of convexity can be relaxed, at the expense of making a
specific choice of the regularizer. Additionally, we demonstrate how these
results can be applied in reinforcement learning in the context of policy
optimization for Markov Decision Processes and Markov games with softmax
parametrized policies in the mean-field regime.

</details>


### [381] [Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization](https://arxiv.org/abs/2505.23056)
*Zijian Liu,Zhengyuan Zhou*

Main category: math.OC

TL;DR: The paper investigates the convergence of shuffling gradient methods for minimizing finite-sum functions with regularization, providing improved last-iterate analysis for nonsmooth cases and demonstrating faster convergence than Proximal GD.


<details>
  <summary>Details</summary>
Motivation: While shuffling gradient methods are widely used for their effectiveness in practice, theoretical understanding is limited. Recent studies have provided convergence results for smooth (strongly) convex optimization but lack improvements for nonsmooth (strongly) convex functions.

Method: The authors analyze Random Reshuffle (RR) and Single Shuffle (SS) strategies to prove that they converge faster than Proximal Gradient Descent in nonsmooth (strongly) convex cases. They provide a nearly optimal convergence result for suffix average under RR sampling scheme in general convex case.

Result: The analysis shows that both RR and SS strategies offer provably faster convergence rates than Proximal GD for nonsmooth (strongly) convex functions. The study also establishes a nearly optimal convergence rate for the suffix average under RR sampling.

Conclusion: This work provides the first improved last-iterate analysis for nonsmooth (strongly) convex optimization using shuffling strategies, highlighting the benefits of randomness and advancing theoretical understanding.

Abstract: We study the convergence of the shuffling gradient method, a popular
algorithm employed to minimize the finite-sum function with regularization, in
which functions are passed to apply (Proximal) Gradient Descent (GD) one by one
whose order is determined by a permutation on the indices of functions. In
contrast to its easy implementation and effective performance in practice, the
theoretical understanding remains limited. A recent advance by (Liu & Zhou,
2024b) establishes the first last-iterate convergence results under various
settings, especially proving the optimal rates for smooth (strongly) convex
optimization. However, their bounds for nonsmooth (strongly) convex functions
are only as fast as Proximal GD. In this work, we provide the first improved
last-iterate analysis for the nonsmooth case demonstrating that the widely used
Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies
are both provably faster than Proximal GD, reflecting the benefit of
randomness. As an important implication, we give the first (nearly) optimal
convergence result for the suffix average under the $\textsf{RR}$ sampling
scheme in the general convex case, matching the lower bound shown by (Koren et
al., 2022).

</details>


### [382] [Gradient Methods with Online Scaling Part I. Theoretical Foundations](https://arxiv.org/abs/2505.23081)
*Wenzhi Gao,Ya-Chi Chu,Yinyu Ye,Madeleine Udell*

Main category: math.OC

TL;DR: OSGM是一种利用在线学习调整步长并加速一阶方法的框架，其实现了与最优步长渐进一致的收敛速率，并在光滑凸问题上提供了多种理想的收敛保证。


<details>
  <summary>Details</summary>
Motivation: 现有的优化方法中，步长的选择往往影响收敛性能，因此需要一种能够自适应调整步长的方法来提高优化效率。

Method: 通过反馈函数衡量步长的有效性，并利用在线学习算法根据反馈调整步长；反馈函数来源于收敛性的度量。

Result: 1) 在光滑凸目标上实现轨迹依赖的全局收敛；2) 在光滑强凸问题上得到改进的复杂度结果；3) 局部超线性收敛，形成一类具有非渐进超线性收敛的一阶方法。此外，解释了超梯度下降启发式方法在机器学习优化中的经验成功。

Conclusion: OSGM提供了一种新的理论框架，可以自适应调整步长以加速一阶优化方法，并且其性能不逊于最优步长，同时为超梯度下降等启发式方法提供了理论支持。

Abstract: This paper establishes the theoretical foundations of the online scaled
gradient methods (OSGM), a framework that utilizes online learning to adapt
stepsizes and provably accelerate first-order methods. OSGM quantifies the
effectiveness of a stepsize by a feedback function motivated from a convergence
measure and uses the feedback to adjust the stepsize through an online learning
algorithm. Consequently, instantiations of OSGM achieve convergence rates that
are asymptotically no worse than the optimal stepsize. OSGM yields desirable
convergence guarantees on smooth convex problems, including 1)
trajectory-dependent global convergence on smooth convex objectives; 2) an
improved complexity result on smooth strongly convex problems, and 3) local
superlinear convergence. Notably, OSGM constitutes a new family of first-order
methods with non-asymptotic superlinear convergence, joining the celebrated
quasi-Newton methods. Finally, OSGM explains the empirical success of the
popular hypergradient-descent heuristic in optimization for machine learning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [383] [ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging](https://arxiv.org/abs/2505.22683)
*Xuhang Chen,Michael Kwok-Po Ng,Kim-Fung Tsang,Chi-Man Pun,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: ConnectomeDiffuser是一种新的扩散模型框架，用于从DTI自动化端到端构建脑网络。它结合了模板网络、扩散模型和图卷积网络分类器，以捕捉更广泛的结构连接和病理相关的信息，从而提高了诊断准确性和对神经退行性疾病的治疗监测。实验验证显示其性能显著优于其他脑网络方法。


<details>
  <summary>Details</summary>
Motivation: 现有的从DTI构建结构性脑网络的方法依赖于专门的工具包，但这些工具包存在固有的局限性，如操作员主观性、劳动密集型工作流程以及捕获复杂拓扑特征和疾病特定生物标志物的能力有限。因此，需要一种新的方法来克服这些挑战并推进计算神经影像技术的发展。

Method: ConnectomeDiffuser框架包含三个关键组件：(1) 模板网络，使用黎曼几何原理从3D DTI扫描中提取拓扑特征；(2) 扩散模型，生成具有增强拓扑保真度的综合脑网络；(3) 图卷积网络分类器，结合疾病特定标记以提高诊断准确性。

Result: 在代表两种不同神经退行性疾病条件的数据集上的实验验证表明，ConnectomeDiffuser在性能上显著优于其他脑网络方法。

Conclusion: 这项工作通过提供一个强大且可推广的测量框架，促进了神经退行性疾病背景下仪器设备的发展，有助于更准确的诊断、更深的机制理解和改进的治疗监测。

Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring
neurodegenerative disorders such as Alzheimer's disease (AD). Existing
approaches for constructing structural brain networks from diffusion tensor
imaging (DTI) often rely on specialized toolkits that suffer from inherent
limitations: operator subjectivity, labor-intensive workflows, and restricted
capacity to capture complex topological features and disease-specific
biomarkers. To overcome these challenges and advance computational neuroimaging
instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based
framework for automated end-to-end brain network construction from DTI. The
proposed model combines three key components: (1) a Template Network that
extracts topological features from 3D DTI scans using Riemannian geometric
principles, (2) a diffusion model that generates comprehensive brain networks
with enhanced topological fidelity, and (3) a Graph Convolutional Network
classifier that incorporates disease-specific markers to improve diagnostic
accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a
broader range of structural connectivity and pathology-related information,
enabling more sensitive analysis of individual variations in brain networks.
Experimental validation on datasets representing two distinct neurodegenerative
conditions demonstrates significant performance improvements over other brain
network methods. This work contributes to the advancement of instrumentation in
the context of neurological disorders, providing clinicians and researchers
with a robust, generalizable measurement framework that facilitates more
accurate diagnosis, deeper mechanistic understanding, and improved therapeutic
monitoring of neurodegenerative diseases such as AD.

</details>


### [384] [Self-orthogonalizing attractor neural networks emerging from the free energy principle](https://arxiv.org/abs/2505.22749)
*Tamas Spisak,Karl Friston*

Main category: q-bio.NC

TL;DR: This paper uses the free energy principle to formalize attractor networks, resulting in a multi-level Bayesian active inference process that enhances generalization and mutual information, offering new insights for AI and neuroscience.


<details>
  <summary>Details</summary>
Motivation: To understand how self-organizing dynamics emerge from first principles in complex systems like the brain, which is crucial for neuronal computations and artificial intelligence systems design.

Method: The method involves applying the free energy principle to a universal partitioning of random dynamical systems, obviating the need for explicitly imposed learning and inference rules. This leads to emergent, efficient, and biologically plausible inference and learning dynamics.

Result: The proposed networks favor orthogonalized attractor representations, enhancing generalization and mutual information between hidden causes and observable effects. Sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics, extending conventional Boltzmann Machines.

Conclusion: The findings present a unifying theory of self-organizing attractor networks, providing novel insights applicable to both artificial intelligence and neuroscience.

Abstract: Attractor dynamics are a hallmark of many complex systems, including the
brain. Understanding how such self-organizing dynamics emerge from first
principles is crucial for advancing our understanding of neuronal computations
and the design of artificial intelligence systems. Here we formalize how
attractor networks emerge from the free energy principle applied to a universal
partitioning of random dynamical systems. Our approach obviates the need for
explicitly imposed learning and inference rules and identifies emergent, but
efficient and biologically plausible inference and learning dynamics for such
self-organizing systems. These result in a collective, multi-level Bayesian
active inference process. Attractors on the free energy landscape encode prior
beliefs; inference integrates sensory data into posterior beliefs; and learning
fine-tunes couplings to minimize long-term surprise. Analytically and via
simulations, we establish that the proposed networks favor approximately
orthogonalized attractor representations, a consequence of simultaneously
optimizing predictive accuracy and model complexity. These attractors
efficiently span the input subspace, enhancing generalization and the mutual
information between hidden causes and observable effects. Furthermore, while
random data presentation leads to symmetric and sparse couplings, sequential
data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings
offer a unifying theory of self-organizing attractor networks, providing novel
insights for AI and neuroscience.

</details>
