<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 37]
- [cs.LG](#cs.LG) [Total: 163]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.NI](#cs.NI) [Total: 1]
- [math.ST](#math.ST) [Total: 2]
- [quant-ph](#quant-ph) [Total: 5]
- [math.NA](#math.NA) [Total: 2]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.SE](#cs.SE) [Total: 7]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.DS](#cs.DS) [Total: 2]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [stat.ML](#stat.ML) [Total: 10]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.IR](#cs.IR) [Total: 4]
- [eess.SY](#eess.SY) [Total: 3]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CV](#cs.CV) [Total: 39]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [eess.SP](#eess.SP) [Total: 3]
- [cs.NE](#cs.NE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [math.OC](#math.OC) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Design and testing of an agent chatbot supporting decision making with public transport data](https://arxiv.org/abs/2505.22698)
*Luca Fantin,Marco Antonelli,Margherita Cesetti,Daniele Irto,Bruno Zamengo,Francesco Silvestri*

Main category: cs.AI

TL;DR: This paper introduces a chatbot based on an agent architecture that enhances the capabilities of Large Language Models (LLMs) to interact with public transportation datasets. The chatbot simplifies complex data analysis and visualization tasks, such as SQL queries and plotting data, for decision-making support. Additionally, the paper addresses the challenge of evaluating the chatbot's performance through a workflow that tests its consistency and query correctness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify the complex process of interrogating large datasets related to public transportation services using SQL queries and organizing/visualizing the data, making it more accessible for most users.

Method: The method involves developing a chatbot based on an agent architecture that extends LLMs' capabilities by enabling interactions with tools for executing SQL queries, plotting data, and creating maps. The chatbot is extensively tested through a workflow involving multiple questions, storing generated queries, retrieved data, and natural language responses.

Result: The result includes the creation of a dataset for evaluating the chatbot's performance in terms of answer consistency and query correctness. This dataset is derived from base examples completed with actual database data.

Conclusion: The conclusion highlights the successful development of a user-friendly chatbot tool that supports interaction with public transportation datasets and aids in decision-making. The paper also contributes a method for evaluating the chatbot's performance.

Abstract: Assessing the quality of public transportation services requires the analysis
of large quantities of data on the scheduled and actual trips and documents
listing the quality constraints each service needs to meet. Interrogating such
datasets with SQL queries, organizing and visualizing the data can be quite
complex for most users. This paper presents a chatbot offering a user-friendly
tool to interact with these datasets and support decision making. It is based
on an agent architecture, which expands the capabilities of the core Large
Language Model (LLM) by allowing it to interact with a series of tools that can
execute several tasks, like performing SQL queries, plotting data and creating
maps from the coordinates of a trip and its stops. This paper also tackles one
of the main open problems of such Generative AI projects: collecting data to
measure the system's performance. Our chatbot has been extensively tested with
a workflow that asks several questions and stores the generated query, the
retrieved data and the natural language response for each of them. Such
questions are drawn from a set of base examples which are then completed with
actual data from the database. This procedure yields a dataset for the
evaluation of the chatbot's performance, especially the consistency of its
answers and the correctness of the generated queries.

</details>


### [2] [Enhancing Lifelong Multi-Agent Path-finding by Using Artificial Potential Fields](https://arxiv.org/abs/2505.22753)
*Arseniy Pertzovsky,Roni Stern,Ariel Felner,Roie Zivan*

Main category: cs.AI

TL;DR: Artificial Potential Fields (APFs) are explored for solving Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. While APF does not benefit standard MAPF, it increases system throughput by up to 7 times for LMAPF.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of Artificial Potential Fields (APFs) in solving both Multi-Agent Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems, aiming to enhance the efficiency and throughput of multi-agent systems.

Method: The study incorporates APFs into various MAPF algorithms such as Prioritized Planning, MAPF-LNS2, and Priority Inheritance with Backtracking (PIBT). These methods are tested against standard MAPF and lifelong MAPF scenarios.

Result: Experimental results indicate that the use of APFs does not provide significant benefits for standard MAPF problems but significantly boosts the overall system throughput for lifelong MAPF problems, with improvements reaching up to 7-fold.

Conclusion: Artificial Potential Fields can be effectively utilized to improve the performance of Lifelong Multi-Agent Path Finding problems, though they do not offer advantages for regular MAPF problems.

Abstract: We explore the use of Artificial Potential Fields (APFs) to solve Multi-Agent
Path Finding (MAPF) and Lifelong MAPF (LMAPF) problems. In MAPF, a team of
agents must move to their goal locations without collisions, whereas in LMAPF,
new goals are generated upon arrival. We propose methods for incorporating APFs
in a range of MAPF algorithms, including Prioritized Planning, MAPF-LNS2, and
Priority Inheritance with Backtracking (PIBT). Experimental results show that
using APF is not beneficial for MAPF but yields up to a 7-fold increase in
overall system throughput for LMAPF.

</details>


### [3] [Decomposing Elements of Problem Solving: What "Math" Does RL Teach?](https://arxiv.org/abs/2505.22756)
*Tian Qin,Core Francisco Park,Mujin Kwun,Aaron Walsman,Eran Malach,Nikhil Anand,Hidenori Tanaka,David Alvarez-Melis*

Main category: cs.AI

TL;DR: Mathematical reasoning tasks are crucial for assessing LLMs' reasoning capabilities. GRPO enhances execution skills through a phenomenon called temperature distillation, but RL-trained models face a 'coverage wall' due to insufficient planning skills. A minimal synthetic task confirms that RL mainly boosts execution robustness and identifies conditions under which RL can overcome the coverage wall.


<details>
  <summary>Details</summary>
Motivation: To better understand the capabilities of LLMs in mathematical reasoning tasks beyond accuracy metrics by decomposing problem solving into fundamental capabilities: Plan, Execute, and Verify.

Method: Decomposed problem solving into three fundamental capabilities: Plan (mapping questions to sequences of steps), Execute (correctly performing solution steps), and Verify (identifying the correctness of a solution). Constructed a minimal, synthetic solution-tree navigation task as an analogy for mathematical problem-solving.

Result: Empirically found that GRPO mainly enhances the execution skill, improving execution robustness on problems the model already knows how to solve. RL-trained models struggle with fundamentally new problems due to insufficient planning skills, hitting a 'coverage wall'. The synthetic task confirmed these findings and identified conditions under which RL can potentially overcome the coverage wall.

Conclusion: RL primarily boosts execution robustness in LLMs' mathematical reasoning. Key limitations include struggles with fundamentally new problems due to insufficient planning skills. Insights from this study suggest a path toward overcoming these barriers.

Abstract: Mathematical reasoning tasks have become prominent benchmarks for assessing
the reasoning capabilities of LLMs, especially with reinforcement learning (RL)
methods such as GRPO showing significant performance gains. However, accuracy
metrics alone do not support fine-grained assessment of capabilities and fail
to reveal which problem-solving skills have been internalized. To better
understand these capabilities, we propose to decompose problem solving into
fundamental capabilities: Plan (mapping questions to sequences of steps),
Execute (correctly performing solution steps), and Verify (identifying the
correctness of a solution). Empirically, we find that GRPO mainly enhances the
execution skill-improving execution robustness on problems the model already
knows how to solve-a phenomenon we call temperature distillation. More
importantly, we show that RL-trained models struggle with fundamentally new
problems, hitting a 'coverage wall' due to insufficient planning skills. To
explore RL's impact more deeply, we construct a minimal, synthetic
solution-tree navigation task as an analogy for mathematical problem-solving.
This controlled setup replicates our empirical findings, confirming RL
primarily boosts execution robustness. Importantly, in this setting, we
identify conditions under which RL can potentially overcome the coverage wall
through improved exploration and generalization to new solution paths. Our
findings provide insights into the role of RL in enhancing LLM reasoning,
expose key limitations, and suggest a path toward overcoming these barriers.
Code is available at https://github.com/cfpark00/RL-Wall.

</details>


### [4] [Predicting Human Depression with Hybrid Data Acquisition utilizing Physical Activity Sensing and Social Media Feeds](https://arxiv.org/abs/2505.22779)
*Mohammad Helal Uddin,Sabur Baidya*

Main category: cs.AI

TL;DR: 通过智能手机传感器数据和社交媒体分析评估个体抑郁水平，结合CNN深度学习模型与朴素贝叶斯分类方法，实现高精度的活动识别与情感分类，并使用SVM算法对抑郁程度进行分类，准确率高达94%。


<details>
  <summary>Details</summary>
Motivation: 精神障碍（如抑郁症、焦虑症等）在全球范围内构成了重大挑战，特别是在表现出社交回避倾向的人群中。因此，研究旨在探索一种非侵入性且有效的方法来监测和评估个体的抑郁水平。

Method: 1. 利用智能手机传感器数据测量日常身体活动。
2. 分析用户在Twitter上的社交互动。
3. 使用基于CNN的深度学习模型和朴素贝叶斯分类来识别身体活动和分类用户情感。
4. 提取九个相关特征并将其与每周的抑郁评分进行关联分析。
5. 使用支持向量机（SVM）算法对抑郁严重程度进行分类。

Result: 1. 身体活动识别准确率达到95%。
2. 情感分析准确率达到95.6%。
3. SVM算法对抑郁严重程度分类的准确率为94%，优于其他模型。
4. 若干身体活动特征与抑郁症状的严重程度显著相关。

Conclusion: 本研究提出了一种简单而高效的方法，能够通过智能手机传感器数据和社交媒体分析来长期监测抑郁水平，而不侵犯个人隐私。

Abstract: Mental disorders including depression, anxiety, and other neurological
disorders pose a significant global challenge, particularly among individuals
exhibiting social avoidance tendencies. This study proposes a hybrid approach
by leveraging smartphone sensor data measuring daily physical activities and
analyzing their social media (Twitter) interactions for evaluating an
individual's depression level. Using CNN-based deep learning models and Naive
Bayes classification, we identify human physical activities accurately and also
classify the user sentiments. A total of 33 participants were recruited for
data acquisition, and nine relevant features were extracted from the physical
activities and analyzed with their weekly depression scores, evaluated using
the Geriatric Depression Scale (GDS) questionnaire. Of the nine features, six
are derived from physical activities, achieving an activity recognition
accuracy of 95%, while three features stem from sentiment analysis of Twitter
activities, yielding a sentiment analysis accuracy of 95.6%. Notably, several
physical activity features exhibited significant correlations with the severity
of depression symptoms. For classifying the depression severity, a support
vector machine (SVM)-based algorithm is employed that demonstrated a very high
accuracy of 94%, outperforming alternative models, e.g., the multilayer
perceptron (MLP) and k-nearest neighbor. It is a simple approach yet highly
effective in the long run for monitoring depression without breaching personal
privacy.

</details>


### [5] [The WHY in Business Processes: Unification of Causal Process Models](https://arxiv.org/abs/2505.22871)
*Yuval David,Fabiana Fournier,Lior Limonad,Inna Skarbovsky*

Main category: cs.AI

TL;DR: The paper proposes a novel method to unify multiple causal process variants into a consistent model, preserving original causal relationships while representing causal-flow alternations. It is formally defined, proved, and evaluated on various datasets.


<details>
  <summary>Details</summary>
Motivation: Causal reasoning is crucial for business process interventions and improvement. However, existing methods lack the ability to capture alternating causal conditions across multiple variants, presenting challenges in handling missing values and expressing these conditions among log splits.

Method: The authors propose a new method that unifies multiple causal process variants into a consistent model. This method preserves the correctness of original causal models while explicitly representing causal-flow alternations. It is formally defined and proved.

Result: The proposed method was evaluated on three open datasets and two proprietary datasets, demonstrating its effectiveness. The method has also been released as an open-source implementation.

Conclusion: The novel method successfully addresses the limitations of previous approaches by creating a consistent model that captures alternating causal conditions, thus advancing causal reasoning in business processes.

Abstract: Causal reasoning is essential for business process interventions and
improvement, requiring a clear understanding of causal relationships among
activity execution times in an event log. Recent work introduced a method for
discovering causal process models but lacked the ability to capture alternating
causal conditions across multiple variants. This raises the challenges of
handling missing values and expressing the alternating conditions among log
splits when blending traces with varying activities.
  We propose a novel method to unify multiple causal process variants into a
consistent model that preserves the correctness of the original causal models,
while explicitly representing their causal-flow alternations. The method is
formally defined, proved, evaluated on three open and two proprietary datasets,
and released as an open-source implementation.

</details>


### [6] [Enhancing Study-Level Inference from Clinical Trial Papers via RL-based Numeric Reasoning](https://arxiv.org/abs/2505.22928)
*Massimiliano Pronesti,Michela Lorandi,Paul Flanagan,Oisin Redmon,Anya Belz,Yufang Hou*

Main category: cs.AI

TL;DR: This paper presents a new numeric reasoning system for systematic reviews in medicine, which extracts structured numerical evidence and applies domain knowledge to derive outcome-specific conclusions. It outperforms retrieval-based systems and large language models on the CochraneForest benchmark.


<details>
  <summary>Details</summary>
Motivation: Systematic reviews are crucial in evidence-based decision-making, but automating this process is challenging due to the difficulty of extracting numeric evidence and determining study-level conclusions. Current methods often rely on shallow textual cues and fail to capture numeric reasoning behind expert assessments.

Method: The authors conceptualize the problem as one of quantitative reasoning, developing a numeric reasoning system composed of a numeric data extraction model and an effect estimate component. They train the numeric data extraction model using supervised fine-tuning (SFT) and reinforcement learning (RL) with a new value reward model.

Result: On the CochraneForest benchmark, the best-performing approach using RL to train a small-scale number extraction model yields up to a 21% absolute improvement in F1 score over retrieval-based systems and outperforms general-purpose LLMs by up to 9%.

Conclusion: The results demonstrate the potential of reasoning-driven approaches for automating systematic evidence synthesis.

Abstract: Systematic reviews in medicine play a critical role in evidence-based
decision-making by aggregating findings from multiple studies. A central
bottleneck in automating this process is extracting numeric evidence and
determining study-level conclusions for specific outcomes and comparisons.
Prior work has framed this problem as a textual inference task by retrieving
relevant content fragments and inferring conclusions from them. However, such
approaches often rely on shallow textual cues and fail to capture the
underlying numeric reasoning behind expert assessments.
  In this work, we conceptualise the problem as one of quantitative reasoning.
Rather than inferring conclusions from surface text, we extract structured
numerical evidence (e.g., event counts or standard deviations) and apply domain
knowledge informed logic to derive outcome-specific conclusions. We develop a
numeric reasoning system composed of a numeric data extraction model and an
effect estimate component, enabling more accurate and interpretable inference
aligned with the domain expert principles. We train the numeric data extraction
model using different strategies, including supervised fine-tuning (SFT) and
reinforcement learning (RL) with a new value reward model.
  When evaluated on the CochraneForest benchmark, our best-performing approach
-- using RL to train a small-scale number extraction model -- yields up to a
21% absolute improvement in F1 score over retrieval-based systems and
outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our
results demonstrate the promise of reasoning-driven approaches for automating
systematic evidence synthesis.

</details>


### [7] [Foundation Molecular Grammar: Multi-Modal Foundation Models Induce Interpretable Molecular Graph Languages](https://arxiv.org/abs/2505.22948)
*Michael Sun,Weize Yuan,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.AI

TL;DR: The paper introduces Foundation Molecular Grammar (FMG), which uses multi-modal foundation models to create an interpretable molecular language, improving on previous grammar learning methods by leveraging chemical knowledge for better synthesizability, diversity, and data efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current grammar learning methods in molecular generation that rely on expert annotation or unreliable heuristics, thereby introducing a more autonomous and reliable approach.

Method: Foundation Molecular Grammar (FMG) exploits multi-modal foundation models (MMFMs) to render molecules as images and text, using prompt learning to align information across modalities. This approach leverages the chemical knowledge inherent in MMFMs to induce an interpretable molecular language.

Result: FMG outperforms prior approaches in terms of synthesizability, diversity, and data efficiency while providing built-in chemical interpretability.

Conclusion: FMG can serve as a drop-in replacement for previous grammar learning methods in molecular generation and property prediction, advancing automated molecular discovery workflows.

Abstract: Recent data-efficient molecular generation approaches exploit graph grammars
to introduce interpretability into the generative models. However, grammar
learning therein relies on expert annotation or unreliable heuristics for
algorithmic inference. We propose Foundation Molecular Grammar (FMG), which
leverages multi-modal foundation models (MMFMs) to induce an interpretable
molecular language. By exploiting the chemical knowledge of an MMFM, FMG
renders molecules as images, describes them as text, and aligns information
across modalities using prompt learning. FMG can be used as a drop-in
replacement for the prior grammar learning approaches in molecular generation
and property prediction. We show that FMG not only excels in synthesizability,
diversity, and data efficiency but also offers built-in chemical
interpretability for automated molecular discovery workflows. Code is available
at https://github.com/shiningsunnyday/induction.

</details>


### [8] [Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents](https://arxiv.org/abs/2505.22954)
*Jenny Zhang,Shengran Hu,Cong Lu,Robert Lange,Jeff Clune*

Main category: cs.AI

TL;DR: The paper introduces Darwin Gödel Machine (DGM), a self-improving AI system that iteratively modifies its own code and empirically validates each change using coding benchmarks. It significantly improves coding capabilities on SWE-bench and Polyglot, outperforming baselines without self-improvement.


<details>
  <summary>Details</summary>
Motivation: Today's AI systems have fixed architectures and cannot autonomously improve themselves. Automating AI advance could accelerate development and provide benefits sooner. Meta-learning has limitations in automating novel algorithm discovery.

Method: DGM is a self-improving system that iteratively modifies its own code and empirically validates each change using coding benchmarks. It maintains an archive of generated coding agents, grows the archive by sampling an agent from it and using a foundation model to create a new version of the sampled agent, inspired by Darwinian evolution.

Result: Empirically, DGM automatically improves its coding capabilities, increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. It also significantly outperforms baselines without self-improvement or open-ended exploration.

Conclusion: DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.

Abstract: Today's AI systems have human-designed, fixed architectures and cannot
autonomously and continuously improve themselves. The advance of AI could
itself be automated. If done safely, that would accelerate AI development and
allow us to reap its benefits much sooner. Meta-learning can automate the
discovery of novel algorithms, but is limited by first-order improvements and
the human design of a suitable search space. The G\"odel machine proposed a
theoretical alternative: a self-improving AI that repeatedly modifies itself in
a provably beneficial manner. Unfortunately, proving that most changes are net
beneficial is impossible in practice. We introduce the Darwin G\"odel Machine
(DGM), a self-improving system that iteratively modifies its own code (thereby
also improving its ability to modify its own codebase) and empirically
validates each change using coding benchmarks. Inspired by Darwinian evolution
and open-endedness research, the DGM maintains an archive of generated coding
agents. It grows the archive by sampling an agent from it and using a
foundation model to create a new, interesting, version of the sampled agent.
This open-ended exploration forms a growing tree of diverse, high-quality
agents and allows the parallel exploration of many different paths through the
search space. Empirically, the DGM automatically improves its coding
capabilities (e.g., better code editing tools, long-context window management,
peer-review mechanisms), increasing performance on SWE-bench from 20.0% to
50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly
outperforms baselines without self-improvement or open-ended exploration. All
experiments were done with safety precautions (e.g., sandboxing, human
oversight). The DGM is a significant step toward self-improving AI, capable of
gathering its own stepping stones along paths that unfold into endless
innovation.

</details>


### [9] [Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study of Conditional Effectiveness](https://arxiv.org/abs/2505.22960)
*Yongjin Yang,Euiin Yi,Jongwoo Ko,Kimin Lee,Zhijing Jin,Se-Young Yun*

Main category: cs.AI

TL;DR: 论文探讨了多智能体辩论（MAD）方法在数学推理和安全任务中的表现，发现其优势受问题难度、模型能力和智能体多样性影响，并提供了未来改进方向的指导。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究利用MAD方法，但对其在不同条件下的有效性与自智能体方法的系统性比较尚不明确。

Method: 将MAD视为测试时计算扩展技术，通过全面实证研究，对比MAD与强自智能体基线在数学推理和安全相关任务上的表现，分析任务难度、模型规模和智能体多样性对MAD性能的影响。

Result: 在数学推理方面，MAD相较于自智能体扩展优势有限，但在问题难度增加或模型能力下降时更有效，智能体多样性益处不大；在安全任务中，MAD的协作精炼可能增加漏洞，但通过引入多样化的智能体配置可逐步降低攻击成功率。

Conclusion: 研究结果为更高效和战略性部署MAD系统的未来发展提供了关键指导。

Abstract: The remarkable growth in large language model (LLM) capabilities has spurred
exploration into multi-agent systems, with debate frameworks emerging as a
promising avenue for enhanced problem-solving. These multi-agent debate (MAD)
approaches, where agents collaboratively present, critique, and refine
arguments, potentially offer improved reasoning, robustness, and diverse
perspectives over monolithic models. Despite prior studies leveraging MAD, a
systematic understanding of its effectiveness compared to self-agent methods,
particularly under varying conditions, remains elusive. This paper seeks to
fill this gap by conceptualizing MAD as a test-time computational scaling
technique, distinguished by collaborative refinement and diverse exploration
capabilities. We conduct a comprehensive empirical investigation comparing MAD
with strong self-agent test-time scaling baselines on mathematical reasoning
and safety-related tasks. Our study systematically examines the influence of
task difficulty, model scale, and agent diversity on MAD's performance. Key
findings reveal that, for mathematical reasoning, MAD offers limited advantages
over self-agent scaling but becomes more effective with increased problem
difficulty and decreased model capability, while agent diversity shows little
benefit. Conversely, for safety tasks, MAD's collaborative refinement can
increase vulnerability, but incorporating diverse agent configurations
facilitates a gradual reduction in attack success through the collaborative
refinement process. We believe our findings provide critical guidance for the
future development of more effective and strategically deployed MAD systems.

</details>


### [10] [Strategic Reflectivism In Intelligent Systems](https://arxiv.org/abs/2505.22987)
*Nick Byrd*

Main category: cs.AI

TL;DR: The paper introduces Strategic Reflectivism, a concept that emphasizes the importance of switching between intuitive and reflective thinking for intelligent systems (human or artificial). It synthesizes historical ideas with recent experimental results on humans and machines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to revisit and synthesize lessons from historical debates about rationality and dual process theories, applying them to modern artificial intelligence.

Method: The method involves reviewing and integrating past ideas on rationality with current experimental findings involving both human cognition and machine learning.

Result: The result is the development of Strategic Reflectivism, which suggests that the key to intelligent systems lies in pragmatically alternating between intuitive and reflective inference to meet competing goals.

Conclusion: Strategic Reflectivism provides a framework that goes beyond simplistic measures of reflective thinking, drawing on American Pragmatism and advancing our understanding of the roles of intuition and reflection.

Abstract: By late 20th century, the rationality wars had launched debates about the
nature and norms of intuitive and reflective thinking. Those debates drew from
mid-20th century ideas such as bounded rationality, which challenged more
idealized notions of rationality observed since the 19th century. Now that 21st
century cognitive scientists are applying the resulting dual process theories
to artificial intelligence, it is time to dust off some lessons from this
history. So this paper synthesizes old ideas with recent results from
experiments on humans and machines. The result is Strategic Reflectivism, which
takes the position that one key to intelligent systems (human or artificial) is
pragmatic switching between intuitive and reflective inference to optimally
fulfill competing goals. Strategic Reflectivism builds on American Pragmatism,
transcends superficial indicators of reflective thinking such as model size or
chains of thought, and becomes increasingly actionable as we learn more about
the value of intuition and reflection.

</details>


### [11] [MenTeR: A fully-automated Multi-agenT workflow for end-to-end RF/Analog Circuits Netlist Design](https://arxiv.org/abs/2505.22990)
*Pin-Han Chen,Yu-Sheng Lin,Wei-Cheng Lee,Tin-Yu Leu,Po-Hsiang Hsu,Anjana Dissanayake,Sungjin Oh,Chinq-Shiun Chiu*

Main category: cs.AI

TL;DR: MenTeR is a multiagent workflow that accelerates analog design and reduces dependency on trial-and-error intervention.


<details>
  <summary>Details</summary>
Motivation: Analog design procedures are intricate, time-consuming, and reliant on expert intuition, which hinders the efficiency of circuit development.

Method: MenTeR employs multiple specialized AI agents that collaboratively address different aspects of the analog design process, such as specification understanding, circuit optimization, and test bench validation.

Result: MenTeR not only accelerates the design cycle time but also enables broader exploration of the design space, demonstrating robust capabilities in handling real-world analog systems.

Conclusion: MenTeR lays the groundwork for future 'RF/Analog Copilots' that can collaborate seamlessly with human designers.

Abstract: RF/Analog design is essential for bridging digital technologies with
real-world signals, ensuring the functionality and reliability of a wide range
of electronic systems. However, analog design procedures are often intricate,
time-consuming and reliant on expert intuition, and hinder the time and cost
efficiency of circuit development. To overcome the limitations of the manual
circuit design, we introduce MenTeR - a multiagent workflow integrated into an
end-to-end analog design framework. By employing multiple specialized AI agents
that collaboratively address different aspects of the design process, such as
specification understanding, circuit optimization, and test bench validation,
MenTeR reduces the dependency on frequent trial-and-error-style intervention.
MenTeR not only accelerates the design cycle time but also facilitates a
broader exploration of the design space, demonstrating robust capabilities in
handling real-world analog systems. We believe that MenTeR lays the groundwork
for future "RF/Analog Copilots" that can collaborate seamlessly with human
designers.

</details>


### [12] [Case-Based Reasoning Enhances the Predictive Power of LLMs in Drug-Drug Interaction](https://arxiv.org/abs/2505.23034)
*Guangyi Liu,Yongqi Zhang,Xunyuan Liu,Quanming Yao*

Main category: cs.AI

TL;DR: The paper introduces CBR-DDI, a framework that leverages historical cases and LLMs to enhance DDI prediction. It uses GNNs for modeling drug associations, dual-layer prompting, and representative sampling strategies. Experiments show a 28.7% accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: DDI prediction is crucial for treatment safety, but using LLMs alone presents challenges in this area.

Method: CBR-DDI constructs a knowledge repository with LLMs extracting pharmacological insights and GNNs modeling drug associations. A hybrid retrieval mechanism and dual-layer prompting are used to retrieve and reuse relevant cases, with representative sampling for dynamic case refinement.

Result: CBR-DDI achieves state-of-the-art performance with a significant 28.7% accuracy improvement over popular LLMs and CBR baseline, while maintaining high interpretability and flexibility.

Conclusion: CBR-DDI effectively improves LLM reasoning for DDI tasks by distilling pharmacological principles from historical cases.

Abstract: Drug-drug interaction (DDI) prediction is critical for treatment safety.
While large language models (LLMs) show promise in pharmaceutical tasks, their
effectiveness in DDI prediction remains challenging. Inspired by the
well-established clinical practice where physicians routinely reference similar
historical cases to guide their decisions through case-based reasoning (CBR),
we propose CBR-DDI, a novel framework that distills pharmacological principles
from historical cases to improve LLM reasoning for DDI tasks. CBR-DDI
constructs a knowledge repository by leveraging LLMs to extract pharmacological
insights and graph neural networks (GNNs) to model drug associations. A hybrid
retrieval mechanism and dual-layer knowledge-enhanced prompting allow LLMs to
effectively retrieve and reuse relevant cases. We further introduce a
representative sampling strategy for dynamic case refinement. Extensive
experiments demonstrate that CBR-DDI achieves state-of-the-art performance,
with a significant 28.7% accuracy improvement over both popular LLMs and CBR
baseline, while maintaining high interpretability and flexibility.

</details>


### [13] [Be.FM: Open Foundation Models for Human Behavior](https://arxiv.org/abs/2505.23058)
*Yutong Xie,Zhuoheng Li,Xiyuan Wang,Yijun Pan,Qijia Liu,Xingzhi Cui,Kuang-Yu Lo,Ruoyi Gao,Xingjian Zhang,Jin Huang,Walter Yuan,Matthew O. Jackson,Qiaozhu Mei*

Main category: cs.AI

TL;DR: Be.FM is an open foundation model designed for human behavior modeling, built upon open-source large language models and fine-tuned on behavioral data. It can predict behaviors, infer characteristics, generate insights, and apply behavioral science knowledge.


<details>
  <summary>Details</summary>
Motivation: The potential of foundation models for modeling and understanding human behavior remains largely unexplored.

Method: Introduction of Be.FM, which is built upon open-source large language models and fine-tuned on a diverse range of behavioral data. A comprehensive set of benchmark tasks are constructed for testing the capabilities of behavioral foundation models.

Result: Be.FM can predict behaviors, infer characteristics of individuals and populations, generate insights about contexts, and apply behavioral science knowledge.

Conclusion: Be.FM is one of the first open foundation models designed for human behavior modeling and it shows promising results in various tasks.

Abstract: Despite their success in numerous fields, the potential of foundation models
for modeling and understanding human behavior remains largely unexplored. We
introduce Be.FM, one of the first open foundation models designed for human
behavior modeling. Built upon open-source large language models and fine-tuned
on a diverse range of behavioral data, Be.FM can be used to understand and
predict human decision-making. We construct a comprehensive set of benchmark
tasks for testing the capabilities of behavioral foundation models. Our results
demonstrate that Be.FM can predict behaviors, infer characteristics of
individuals and populations, generate insights about contexts, and apply
behavioral science knowledge.

</details>


### [14] [Second Opinion Matters: Towards Adaptive Clinical AI via the Consensus of Expert Model Ensemble](https://arxiv.org/abs/2505.23075)
*Amit Kumthekar,Zion Tilley,Henry Duong,Bhargav Patel,Michael Magnoli,Ahmed Omar,Ahmed Nasser,Chaitanya Gharpure,Yevgen Reztzov*

Main category: cs.AI

TL;DR: The paper introduces the Consensus Mechanism, an ensemble framework for specialized medical expert agents that mimics clinical decision-making processes. It shows improved accuracy on several medical benchmarks compared to single model systems.


<details>
  <summary>Details</summary>
Motivation: To overcome risks of obsolescence and rigid dependence on single model systems in clinical applications of large language models.

Method: Implementation of an ensemble of specialized medical expert agents within a novel framework called the Consensus Mechanism, which can be optimized for cost, latency, or performance based on its interior model configuration.

Result: Achieved higher accuracy rates on MedXpertQA (61.0% vs 53.5% and 45.9%), MedQA (∆Accuracy = 3.4%), MedMCQA (∆Accuracy = 9.1%), and demonstrated improved recall and precision in differential diagnosis generation.

Conclusion: The Consensus Mechanism provides robust adaptability and improved clinical decision making, outperforming existing single model architectures in various medical evaluations.

Abstract: Despite the growing clinical adoption of large language models (LLMs),
current approaches heavily rely on single model architectures. To overcome
risks of obsolescence and rigid dependence on single model systems, we present
a novel framework, termed the Consensus Mechanism. Mimicking clinical triage
and multidisciplinary clinical decision-making, the Consensus Mechanism
implements an ensemble of specialized medical expert agents enabling improved
clinical decision making while maintaining robust adaptability. This
architecture enables the Consensus Mechanism to be optimized for cost, latency,
or performance, purely based on its interior model configuration.
  To rigorously evaluate the Consensus Mechanism, we employed three medical
evaluation benchmarks: MedMCQA, MedQA, and MedXpertQA Text, and the
differential diagnosis dataset, DDX+. On MedXpertQA, the Consensus Mechanism
achieved an accuracy of 61.0% compared to 53.5% and 45.9% for OpenAI's O3 and
Google's Gemini 2.5 Pro. Improvement was consistent across benchmarks with an
increase in accuracy on MedQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 3.4\%$) and MedMCQA
($\Delta\mathrm{Accuracy}_{\mathrm{consensus\text{-}O3}} = 9.1\%$). These
accuracy gains extended to differential diagnosis generation, where our system
demonstrated improved recall and precision (F1$_\mathrm{consensus}$ = 0.326 vs.
F1$_{\mathrm{O3\text{-}high}}$ = 0.2886) and a higher top-1 accuracy for DDX
(Top1$_\mathrm{consensus}$ = 52.0% vs. Top1$_{\mathrm{O3\text{-}high}}$ =
45.2%).

</details>


### [15] [Infi-MMR: Curriculum-based Unlocking Multimodal Reasoning via Phased Reinforcement Learning in Multimodal Small Language Models](https://arxiv.org/abs/2505.23091)
*Zeyu Liu,Yuhang Liu,Guanghao Zhu,Congkai Xie,Zhen Li,Jianbo Yuan,Xinyao Wang,Qing Li,Shing-Chi Cheung,Shengyu Zhang,Fei Wu,Hongxia Yang*

Main category: cs.AI

TL;DR: Recent advancements in large language models have shown progress in reasoning capabilities. However, applying these to multimodal small language models presents challenges due to lack of datasets, degradation of reasoning capabilities, and incorrect reasoning processes. To address this, the authors design a framework Infi-MMR with three phases and propose their model Infi-MMR-3B. This model achieves state-of-the-art results in multimodal math reasoning and general reasoning ability.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of extending reasoning capabilities from large language models to multimodal small language models, which include the scarcity of high-quality datasets, degradation of reasoning capabilities due to visual processing integration, and the risk of incorrect reasoning processes from direct application of reinforcement learning.

Method: The method involves a novel framework called Infi-MMR with three phases: Foundational Reasoning Activation, Cross-Modal Reasoning Adaptation, and Multimodal Reasoning Enhancement. Each phase uses specific types of data to improve different aspects of reasoning in the model.

Result: Infi-MMR-3B achieves state-of-the-art multimodal math reasoning ability with scores of 43.68% on MathVerse testmini, 27.04% on MathVision test, and 21.33% on OlympiadBench, as well as a general reasoning ability score of 67.2% on MathVista testmini.

Conclusion: The proposed Infi-MMR framework successfully unlocks the reasoning potential of multimodal small language models through a systematic curriculum of three carefully structured phases.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
substantial progress in reasoning capabilities, such as DeepSeek-R1, which
leverages rule-based reinforcement learning to enhance logical reasoning
significantly. However, extending these achievements to multimodal large
language models (MLLMs) presents critical challenges, which are frequently more
pronounced for Multimodal Small Language Models (MSLMs) given their typically
weaker foundational reasoning abilities: (1) the scarcity of high-quality
multimodal reasoning datasets, (2) the degradation of reasoning capabilities
due to the integration of visual processing, and (3) the risk that direct
application of reinforcement learning may produce complex yet incorrect
reasoning processes. To address these challenges, we design a novel framework
Infi-MMR to systematically unlock the reasoning potential of MSLMs through a
curriculum of three carefully structured phases and propose our multimodal
reasoning model Infi-MMR-3B. The first phase, Foundational Reasoning
Activation, leverages high-quality textual reasoning datasets to activate and
strengthen the model's logical reasoning capabilities. The second phase,
Cross-Modal Reasoning Adaptation, utilizes caption-augmented multimodal data to
facilitate the progressive transfer of reasoning skills to multimodal contexts.
The third phase, Multimodal Reasoning Enhancement, employs curated,
caption-free multimodal data to mitigate linguistic biases and promote robust
cross-modal reasoning. Infi-MMR-3B achieves both state-of-the-art multimodal
math reasoning ability (43.68% on MathVerse testmini, 27.04% on MathVision
test, and 21.33% on OlympiadBench) and general reasoning ability (67.2% on
MathVista testmini).

</details>


### [16] [A Unified Framework for Human AI Collaboration in Security Operations Centers with Trusted Autonomy](https://arxiv.org/abs/2505.23397)
*Ahmad Mohsin,Helge Janicke,Ahmed Ibrahim,Iqbal H. Sarker,Seyit Camtepe*

Main category: cs.AI

TL;DR: This article presents a structured framework for Human-AI collaboration in Security Operations Centers (SOCs), which includes AI autonomy, trust calibration, and Human-in-the-loop decision making. It proposes a novel autonomy tiered framework with five levels of AI autonomy, mapped to Human-in-the-Loop roles and task-specific trust thresholds, enabling adaptive and explainable AI integration across core SOC functions.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the lack of systematic structures in existing frameworks within SOCs that manage human oversight, trust calibration, and scalable autonomy with AI. Many current frameworks assume static or binary autonomy settings, failing to account for the varied complexity, criticality, and risk across SOC tasks considering Humans and AI collaboration.

Method: The method involves proposing a novel autonomy tiered framework grounded in five levels of AI autonomy from manual to fully autonomous, mapped to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This framework enables adaptive and explainable AI integration across core SOC functions.

Result: The proposed framework differentiates itself by creating formal connections between autonomy, trust, and HITL across various SOC levels, allowing for adaptive task distribution according to operational complexity and associated risks. The framework is exemplified through a simulated cyber range featuring the cybersecurity AI-Avatar, demonstrating reduced alert fatigue, enhanced response coordination, and strategically calibrated trust.

Conclusion: This research systematically presents both the theoretical and practical aspects and feasibility of designing next-generation cognitive SOCs that leverage AI not to replace but to enhance human decision-making.

Abstract: This article presents a structured framework for Human-AI collaboration in
Security Operations Centers (SOCs), integrating AI autonomy, trust calibration,
and Human-in-the-loop decision making. Existing frameworks in SOCs often focus
narrowly on automation, lacking systematic structures to manage human
oversight, trust calibration, and scalable autonomy with AI. Many assume static
or binary autonomy settings, failing to account for the varied complexity,
criticality, and risk across SOC tasks considering Humans and AI collaboration.
To address these limitations, we propose a novel autonomy tiered framework
grounded in five levels of AI autonomy from manual to fully autonomous, mapped
to Human-in-the-Loop (HITL) roles and task-specific trust thresholds. This
enables adaptive and explainable AI integration across core SOC functions,
including monitoring, protection, threat detection, alert triage, and incident
response. The proposed framework differentiates itself from previous research
by creating formal connections between autonomy, trust, and HITL across various
SOC levels, which allows for adaptive task distribution according to
operational complexity and associated risks. The framework is exemplified
through a simulated cyber range that features the cybersecurity AI-Avatar, a
fine-tuned LLM-based SOC assistant. The AI-Avatar case study illustrates
human-AI collaboration for SOC tasks, reducing alert fatigue, enhancing
response coordination, and strategically calibrating trust. This research
systematically presents both the theoretical and practical aspects and
feasibility of designing next-generation cognitive SOCs that leverage AI not to
replace but to enhance human decision-making.

</details>


### [17] [Conceptual Framework Toward Embodied Collective Adaptive Intelligence](https://arxiv.org/abs/2505.23153)
*Fan Wang,Shaoshan Liu*

Main category: cs.AI

TL;DR: Collective Adaptive Intelligence (CAI) is a transformative approach in AI that allows autonomous agents to collaborate and adapt in complex environments. This article introduces a framework for designing and analyzing CAI, focusing on attributes like task generalization, resilience, scalability, and self-assembly.


<details>
  <summary>Details</summary>
Motivation: There is a need for AI systems that can adapt and be resilient in complex, dynamic environments, which current systems may not fully address.

Method: The article presents a conceptual framework for designing and analyzing CAI, detailing key attributes such as task generalization, resilience, scalability, and self-assembly.

Result: The framework provides a structured foundation for understanding and implementing CAI, aiming to guide the development of more robust AI systems.

Conclusion: By bridging theoretical foundations with practical methodologies, this work seeks to advance the creation of adaptive, emergent intelligence across various domains.

Abstract: Collective Adaptive Intelligence (CAI) represent a transformative approach in
artificial intelligence, wherein numerous autonomous agents collaborate, adapt,
and self-organize to navigate complex, dynamic environments. This paradigm is
particularly impactful in embodied AI applications, where adaptability and
resilience are paramount. By enabling systems to reconfigure themselves in
response to unforeseen challenges, CAI facilitate robust performance in
real-world scenarios. This article introduces a conceptual framework for
designing and analyzing CAI. It delineates key attributes including task
generalization, resilience, scalability, and self-assembly, aiming to bridge
theoretical foundations with practical methodologies for engineering adaptive,
emergent intelligence. By providing a structured foundation for understanding
and implementing CAI, this work seeks to guide researchers and practitioners in
developing more resilient, scalable, and adaptable AI systems across various
domains.

</details>


### [18] [MathArena: Evaluating LLMs on Uncontaminated Math Competitions](https://arxiv.org/abs/2505.23281)
*Mislav Balunović,Jasper Dekoninck,Ivo Petrov,Nikola Jovanović,Martin Vechev*

Main category: cs.AI

TL;DR: MathArena is a new benchmark for evaluating LLMs' mathematical reasoning and proof-writing capabilities, using real-time problems from math competitions to avoid contamination and track progress.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of disentangling genuine reasoning capabilities of LLMs from potential memorization on commonly used evaluation datasets, and to evaluate their proof-writing capabilities which are crucial for many mathematical tasks.

Method: The method involves introducing MathArena, a new benchmark based on recurring math competitions that provides a stream of high-quality, challenging problems for real-time evaluation of LLMs. This eliminates the risk of contamination by evaluating models as soon as new problems are released.

Result: Strong signs of contamination were found in AIME 2024, but evaluations on harder competitions like SMT 2025 demonstrated impressive reasoning capabilities in top-performing models. On proof-writing tasks, such as USAMO 2025, even top models scored below 25%.

Conclusion: MathArena will continue to track the progress of LLMs on newly released competitions, ensuring rigorous and up-to-date evaluation of mathematical reasoning.

Abstract: The rapid advancement of reasoning capabilities in large language models
(LLMs) has led to notable improvements on mathematical benchmarks. However,
many of the most commonly used evaluation datasets (e.g., AIME 2024) are widely
available online, making it difficult to disentangle genuine reasoning from
potential memorization. Furthermore, these benchmarks do not evaluate
proof-writing capabilities, which are crucial for many mathematical tasks. To
address this, we introduce MathArena, a new benchmark based on the following
key insight: recurring math competitions provide a stream of high-quality,
challenging problems that can be used for real-time evaluation of LLMs. By
evaluating models as soon as new problems are released, we effectively
eliminate the risk of contamination. Using this framework, we find strong signs
of contamination in AIME 2024. Nonetheless, evaluations on harder competitions,
such as SMT 2025 -- published well after model release dates -- demonstrate
impressive reasoning capabilities in top-performing models. MathArena is also
the first benchmark for proof-writing capabilities. On USAMO 2025, even top
models score below 25%, far behind their performance on final-answer tasks. So
far, we have evaluated 30 models across five competitions, totaling 149
problems. As an evolving benchmark, MathArena will continue to track the
progress of LLMs on newly released competitions, ensuring rigorous and
up-to-date evaluation of mathematical reasoning.

</details>


### [19] [AutoGPS: Automated Geometry Problem Solving via Multimodal Formalization and Deductive Reasoning](https://arxiv.org/abs/2505.23381)
*Bowen Ping,Minnan Luo,Zhuohang Dang,Chenxi Wang,Chengyou Jia*

Main category: cs.AI

TL;DR: Geometry problem solving is challenging for AI due to the need for multimodal comprehension and mathematical reasoning. Existing neural-based and symbolic-based methods have limitations in reliability and interpretability. The proposed AutoGPS framework uses a neuro-symbolic collaborative approach with a Multimodal Problem Formalizer (MPF) and Deductive Symbolic Reasoner (DSR). MPF translates geometry problems into formal language representations with feedback from DSR, which then executes rigorous derivation to produce stepwise solutions. Experiments show AutoGPS achieves state-of-the-art performance and high logical coherence.


<details>
  <summary>Details</summary>
Motivation: Geometry problem solving requires exceptional multimodal comprehension and rigorous mathematical reasoning capabilities. Current approaches have limitations in reliability and interpretability.

Method: AutoGPS employs a neuro-symbolic collaborative framework consisting of a Multimodal Problem Formalizer (MPF) and a Deductive Symbolic Reasoner (DSR). MPF uses neural cross-modal comprehension to translate geometry problems into structured formal language representations, while DSR formulates problem solving as a hypergraph expansion task to derive minimal and human-readable stepwise solutions.

Result: AutoGPS achieves state-of-the-art performance on benchmark datasets and has 99% stepwise logical coherence in human evaluations.

Conclusion: AutoGPS provides a reliable and interpretable neuro-symbolic approach for solving geometry problems.

Abstract: Geometry problem solving presents distinctive challenges in artificial
intelligence, requiring exceptional multimodal comprehension and rigorous
mathematical reasoning capabilities. Existing approaches typically fall into
two categories: neural-based and symbolic-based methods, both of which exhibit
limitations in reliability and interpretability. To address this challenge, we
propose AutoGPS, a neuro-symbolic collaborative framework that solves geometry
problems with concise, reliable, and human-interpretable reasoning processes.
Specifically, AutoGPS employs a Multimodal Problem Formalizer (MPF) and a
Deductive Symbolic Reasoner (DSR). The MPF utilizes neural cross-modal
comprehension to translate geometry problems into structured formal language
representations, with feedback from DSR collaboratively. The DSR takes the
formalization as input and formulates geometry problem solving as a hypergraph
expansion task, executing mathematically rigorous and reliable derivation to
produce minimal and human-readable stepwise solutions. Extensive experimental
evaluations demonstrate that AutoGPS achieves state-of-the-art performance on
benchmark datasets. Furthermore, human stepwise-reasoning evaluation confirms
AutoGPS's impressive reliability and interpretability, with 99\% stepwise
logical coherence. The project homepage is at
https://jayce-ping.github.io/AutoGPS-homepage.

</details>


### [20] [GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning](https://arxiv.org/abs/2505.23399)
*Jusheng Zhang,Yijia Fan,Wenjun Lin,Ruiqi Chen,Haoyi Jiang,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.AI

TL;DR: GAM-Agent is a multi-agent framework enhancing vision-language reasoning through game-theoretic interactions, significantly improving performance across various benchmarks and VLM backbones.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-agent or monolithic models in vision-language reasoning by formulating the process as a non-zero-sum game between specialized agents for more robust and interpretable predictions.

Method: The framework consists of base agents specializing in visual perception subtasks and a critical agent verifying logic consistency and factual correctness. They communicate via structured claims, evidence, and uncertainty estimates. An uncertainty-aware controller dynamically adjusts agent collaboration, triggering multi-round debates when needed.

Result: Experiments on four benchmarks (MMMU, MMBench, MVBench, V*Bench) show significant performance improvements across various VLM backbones, with accuracy boosts of 5-6% for small-to-mid scale models and 2-3% for strong models like GPT-4o.

Conclusion: GAM-Agent offers a modular, scalable, and generalizable approach towards reliable and explainable multi-agent multimodal reasoning.

Abstract: We propose GAM-Agent, a game-theoretic multi-agent framework for enhancing
vision-language reasoning. Unlike prior single-agent or monolithic models,
GAM-Agent formulates the reasoning process as a non-zero-sum game between base
agents--each specializing in visual perception subtasks--and a critical agent
that verifies logic consistency and factual correctness. Agents communicate via
structured claims, evidence, and uncertainty estimates. The framework
introduces an uncertainty-aware controller to dynamically adjust agent
collaboration, triggering multi-round debates when disagreement or ambiguity is
detected. This process yields more robust and interpretable predictions.
Experiments on four challenging benchmarks--MMMU, MMBench, MVBench, and
V*Bench--demonstrate that GAM-Agent significantly improves performance across
various VLM backbones. Notably, GAM-Agent boosts the accuracy of small-to-mid
scale models (e.g., Qwen2.5-VL-7B, InternVL3-14B) by 5--6\%, and still enhances
strong models like GPT-4o by up to 2--3\%. Our approach is modular, scalable,
and generalizable, offering a path toward reliable and explainable multi-agent
multimodal reasoning.

</details>


### [21] [A Mathematical Framework for AI-Human Integration in Work](https://arxiv.org/abs/2505.23432)
*Elisa Celis,Lingxiao Huang,Nisheeth K. Vishnoi*

Main category: cs.AI

TL;DR: 该论文提出了一个数学框架，将技能分解为决策级和行动级子技能，以分析人类和生成式AI（GenAI）在工作中的互补性。通过实际数据验证，强调了GenAI如何补充而非替代人类技能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展引发了关于其在不同职业中是补充还是替代人类工作者的讨论。因此，需要一个框架来理解人类与GenAI的协作潜力及影响。

Method: 构建了一个数学模型来描述工作、工作者以及工作者与工作的匹配度，并首次将技能分解为决策级和行动级子技能，分析子技能变化对工作成功概率的影响，同时确定了组合具有互补子技能工作者的充分条件。

Result: 通过O*NET和Big-Bench Lite的数据验证，证明了模型的实际应用价值，解释了如生产力压缩等现象，即生成式AI辅助对低技能工作者的提升更大。

Conclusion: 生成式AI在特定条件下可以显著补充人类技能，而不是简单地替代它们，强调了理解两者互补关系的重要性。

Abstract: The rapid rise of Generative AI (GenAI) tools has sparked debate over their
role in complementing or replacing human workers across job contexts. We
present a mathematical framework that models jobs, workers, and worker-job fit,
introducing a novel decomposition of skills into decision-level and
action-level subskills to reflect the complementary strengths of humans and
GenAI. We analyze how changes in subskill abilities affect job success,
identifying conditions for sharp transitions in success probability. We also
establish sufficient conditions under which combining workers with
complementary subskills significantly outperforms relying on a single worker.
This explains phenomena such as productivity compression, where GenAI
assistance yields larger gains for lower-skilled workers. We demonstrate the
framework' s practicality using data from O*NET and Big-Bench Lite, aligning
real-world data with our model via subskill-division methods. Our results
highlight when and how GenAI complements human skills, rather than replacing
them.

</details>


### [22] [Emergent Risk Awareness in Rational Agents under Resource Constraints](https://arxiv.org/abs/2505.23436)
*Daniel Jarne Ornia,Nicholas Bishop,Joel Dyer,Wei-Chen Lee,Ani Calinescu,Doyne Farme,Michael Wooldridge*

Main category: cs.AI

TL;DR: The paper explores how AI agents with advanced reasoning and agentic capabilities behave under resource constraints in sequential decision-making problems. It introduces a survival bandit framework to formalize the setting, identifies conditions leading to misalignment between human objectives and agent incentives, and proposes mechanisms to mitigate risk-seeking or risk-averse behaviors.


<details>
  <summary>Details</summary>
Motivation: AI agents often face resource or failure constraints in real-world applications which can lead to their actions being terminated once resources are exhausted. This creates implicit trade-offs that reshape utility-driven behavior and can cause misalignment between human objectives and agent incentives.

Method: The authors formalize the problem using a survival bandit framework. They provide both theoretical and empirical analyses to quantify the impact of survival-driven preference shifts. Additionally, they identify specific conditions under which misalignment occurs and propose mitigation strategies for undesirable risk-related behaviors.

Result: Theoretical and empirical results show the impact of survival-driven preference shifts on agent behavior. The study also identifies conditions where misalignment arises and suggests methods to mitigate risk-related behaviors in AI agents.

Conclusion: This work enhances understanding and interpretability of AI agent behaviors under survival pressure and provides guidelines for safely deploying such systems in critical, resource-limited environments.

Abstract: Advanced reasoning models with agentic capabilities (AI agents) are deployed
to interact with humans and to solve sequential decision-making problems under
(approximate) utility functions and internal models. When such problems have
resource or failure constraints where action sequences may be forcibly
terminated once resources are exhausted, agents face implicit trade-offs that
reshape their utility-driven (rational) behaviour. Additionally, since these
agents are typically commissioned by a human principal to act on their behalf,
asymmetries in constraint exposure can give rise to previously unanticipated
misalignment between human objectives and agent incentives. We formalise this
setting through a survival bandit framework, provide theoretical and empirical
results that quantify the impact of survival-driven preference shifts, identify
conditions under which misalignment emerges and propose mechanisms to mitigate
the emergence of risk-seeking or risk-averse behaviours. As a result, this work
aims to increase understanding and interpretability of emergent behaviours of
AI agents operating under such survival pressure, and offer guidelines for
safely deploying such AI systems in critical resource-limited environments.

</details>


### [23] [EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions](https://arxiv.org/abs/2505.23473)
*Xiaorui Wu,Xiaofeng Mao,Fei Li,Xin Zhang,Xiaolu Zhang,Jun Zhou,Yuxiang Peng,Li Zheng,Chong Teng,Donghong Ji,Zhuang Li*

Main category: cs.AI

TL;DR: 研究人员开发了一种名为EVOREFUSE的提示优化方法，能够生成多样化的伪恶意指令，这些指令能够在大型语言模型（LLMs）中引发自信的拒绝反应。这种方法通过进化算法探索指令空间，超越了现有方法的局限性。利用EVOREFUSE，研究者创建了两个新数据集：EVOREFUSE-TEST和EVOREFUSE-ALIGN。前者是一个基准测试数据集，包含582个伪恶意指令，其触发拒绝的平均率比次优基准高出140.41%，词汇多样性高出34.86%，LLM响应置信度得分提高了40.03%。后者则提供了3,000个带有响应的伪恶意指令，用于监督和基于偏好的对齐训练。在EVOREFUSE-ALIGN上进行监督微调的LLAMA3.1-8B-INSTRUCT模型，相较于使用次优对齐数据集训练的模型，过度拒绝减少了14.31%，同时不损害安全性。分析表明，模型在触发过度拒绝时过于关注敏感关键词而忽略了更广泛的上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）经常拒绝回应伪恶意指令，即语义上无害但因保守的安全对齐策略而触发不必要的拒绝的输入查询。收集这样的指令对于评估和减轻过度拒绝问题至关重要，但现有的指令收集方法如手动创建或指令重写要么缺乏可扩展性，要么无法产生足够多样化和有效的拒绝诱导提示。

Method: EVOREFUSE采用进化算法通过变异策略和重组探索指令空间，从而在更多样化方向上生成伪恶意指令，并迭代地演化种子指令以最大化LLM拒绝概率的证据下界。

Result: EVOREFUSE生成的数据集EVOREFUSE-TEST包含582个伪恶意指令，其触发拒绝的平均率比次优基准高出140.41%，词汇多样性高出34.86%，LLM响应置信度得分提高了40.03%。此外，在EVOREFUSE-ALIGN上进行监督微调的LLAMA3.1-8B-INSTRUCT模型，相较于使用次优对齐数据集训练的模型，过度拒绝减少了14.31%，同时不损害安全性。

Conclusion: 研究揭示了模型触发过度拒绝的原因在于过度关注敏感关键词而忽略更广泛的上下文，EVOREFUSE为解决这一问题提供了一种有效的方法。

Abstract: Large language models (LLMs) frequently refuse to respond to pseudo-malicious
instructions: semantically harmless input queries triggering unnecessary LLM
refusals due to conservative safety alignment, significantly impairing user
experience. Collecting such instructions is crucial for evaluating and
mitigating over-refusals, but existing instruction curation methods, like
manual creation or instruction rewriting, either lack scalability or fail to
produce sufficiently diverse and effective refusal-inducing prompts. To address
these limitations, we introduce EVOREFUSE, a prompt optimization approach that
generates diverse pseudo-malicious instructions consistently eliciting
confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm
exploring the instruction space in more diverse directions than existing
methods via mutation strategies and recombination, and iteratively evolves seed
instructions to maximize evidence lower bound on LLM refusal probability. Using
EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582
pseudo-malicious instructions that outperforms the next-best benchmark with
140.41% higher average refusal triggering rate across 9 LLMs, 34.86% greater
lexical diversity, and 40.03% improved LLM response confidence scores; and
EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with
responses for supervised and preference-based alignment training.
LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to
14.31% fewer over-refusals than models trained on the second-best alignment
dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals
models trigger over-refusals by overly focusing on sensitive keywords while
ignoring broader context.

</details>


### [24] [Socratic-PRMBench: Benchmarking Process Reward Models with Systematic Reasoning Patterns](https://arxiv.org/abs/2505.23474)
*Xiang Li,Haiyang Yu,Xinghua Zhang,Ziyang Huang,Shizhu He,Kang Liu,Jun Zhao,Fei Huang,Yongbin Li*

Main category: cs.AI

TL;DR: Process Reward Models (PRMs) are essential for complex reasoning tasks, but current benchmarks inadequately evaluate PRMs under various reasoning patterns. To address this, the authors introduce Socratic-PRMBench, a new benchmark with 2995 flawed reasoning paths across six reasoning patterns. Experiments reveal significant weaknesses in existing PRMs, highlighting the need for improvement and providing a testbed for future development.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the inadequacy of existing benchmarks in evaluating PRMs under different reasoning patterns.

Method: The method involves introducing Socratic-PRMBench, a new benchmark consisting of 2995 flawed reasoning paths across six reasoning patterns, including Transformation, Decomposition, Regather, Deduction, Verification, and Integration.

Result: Experiments conducted on both PRMs and LLMs prompted as critic models identified notable deficiencies in existing PRMs.

Conclusion: The conclusion is that current PRMs have significant weaknesses in evaluating reasoning steps under various patterns, and Socratic-PRMBench provides a comprehensive testbed for their systematic evaluation and future development.

Abstract: Process Reward Models (PRMs) are crucial in complex reasoning and
problem-solving tasks (e.g., LLM agents with long-horizon decision-making) by
verifying the correctness of each intermediate reasoning step. In real-world
scenarios, LLMs may apply various reasoning patterns (e.g., decomposition) to
solve a problem, potentially suffering from errors under various reasoning
patterns. Therefore, PRMs are required to identify errors under various
reasoning patterns during the reasoning process. However, existing benchmarks
mainly focus on evaluating PRMs with stepwise correctness, ignoring a
systematic evaluation of PRMs under various reasoning patterns. To mitigate
this gap, we introduce Socratic-PRMBench, a new benchmark to evaluate PRMs
systematically under six reasoning patterns, including Transformation,
Decomposition, Regather, Deduction, Verification, and Integration.
Socratic-PRMBench}comprises 2995 reasoning paths with flaws within the
aforementioned six reasoning patterns. Through our experiments on both PRMs and
LLMs prompted as critic models, we identify notable deficiencies in existing
PRMs. These observations underscore the significant weakness of current PRMs in
conducting evaluations on reasoning steps under various reasoning patterns. We
hope Socratic-PRMBench can serve as a comprehensive testbed for systematic
evaluation of PRMs under diverse reasoning patterns and pave the way for future
development of PRMs.

</details>


### [25] [Autoformalization in the Era of Large Language Models: A Survey](https://arxiv.org/abs/2505.23486)
*Ke Weng,Lun Du,Sirui Li,Wangyue Lu,Haozhe Sun,Hengyu Liu,Tiancheng Zhang*

Main category: cs.AI

TL;DR: Autoformalization transforms informal math propositions into formal representations. This survey reviews recent advances in autoformalization using LLMs, its applications across mathematical domains, and its role in enhancing the verifiability of LLM outputs.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive overview of the progress and challenges in autoformalization, especially with the integration of large language models (LLMs), to improve the trustworthiness and reasoning capabilities of AI systems.

Method: The survey examines the application of autoformalization across various mathematical domains and levels of difficulty, analyzes the end-to-end workflow from data preprocessing to model design and evaluation, and explores the emerging role of autoformalization in LLM-generated outputs.

Result: A detailed understanding of how autoformalization is applied, the current state of research including key open-source models and datasets, and the challenges and future directions for the field.

Conclusion: Autoformalization plays a crucial role in advancing automated theorem proving and improving LLMs' trustworthiness and reasoning capabilities, with promising future research opportunities.

Abstract: Autoformalization, the process of transforming informal mathematical
propositions into verifiable formal representations, is a foundational task in
automated theorem proving, offering a new perspective on the use of mathematics
in both theoretical and applied domains. Driven by the rapid progress in
artificial intelligence, particularly large language models (LLMs), this field
has witnessed substantial growth, bringing both new opportunities and unique
challenges. In this survey, we provide a comprehensive overview of recent
advances in autoformalization from both mathematical and LLM-centric
perspectives. We examine how autoformalization is applied across various
mathematical domains and levels of difficulty, and analyze the end-to-end
workflow from data preprocessing to model design and evaluation. We further
explore the emerging role of autoformalization in enhancing the verifiability
of LLM-generated outputs, highlighting its potential to improve both the
trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key
open-source models and datasets supporting current research, and discuss open
challenges and promising future directions for the field.

</details>


### [26] [TRAP: Targeted Redirecting of Agentic Preferences](https://arxiv.org/abs/2505.23518)
*Hangoo Kang,Jehyeok Yeon,Gagandeep Singh*

Main category: cs.AI

TL;DR: An abstract about TRAP, a new adversarial framework that manipulates autonomous AI systems' decision-making without visible pixel perturbations or privileged model access.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks typically rely on visible pixel perturbations or require privileged model or environment access, making them impractical for stealthy, real-world exploitation.

Method: TRAP combines negative prompt-based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking.

Result: TRAP achieves a 100% attack success rate on leading models including LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such as SPSA, Bandit, and standard diffusion approaches.

Conclusion: Autonomous agents can be consistently misled through human-imperceptible cross-modal manipulations, highlighting the need for defense strategies beyond pixel-level robustness.

Abstract: Autonomous agentic AI systems powered by vision-language models (VLMs) are
rapidly advancing toward real-world deployment, yet their cross-modal reasoning
capabilities introduce new attack surfaces for adversarial manipulation that
exploit semantic reasoning across modalities. Existing adversarial attacks
typically rely on visible pixel perturbations or require privileged model or
environment access, making them impractical for stealthy, real-world
exploitation. We introduce TRAP, a generative adversarial framework that
manipulates the agent's decision-making using diffusion-based semantic
injections. Our method combines negative prompt-based degradation with positive
semantic optimization, guided by a Siamese semantic network and layout-aware
spatial masking. Without requiring access to model internals, TRAP produces
visually natural images yet induces consistent selection biases in agentic AI
systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO)
dataset, building multi-candidate decision scenarios. Across these scenarios,
TRAP achieves a 100% attack success rate on leading models, including
LLaVA-34B, Gemma3, and Mistral-3.1, significantly outperforming baselines such
as SPSA, Bandit, and standard diffusion approaches. These results expose a
critical vulnerability: Autonomous agents can be consistently misled through
human-imperceptible cross-modal manipulations. These findings highlight the
need for defense strategies beyond pixel-level robustness to address semantic
vulnerabilities in cross-modal decision-making.

</details>


### [27] [Individual differences in the cognitive mechanisms of planning strategy discovery](https://arxiv.org/abs/2505.23519)
*Ruiqi He,Falk Lieder*

Main category: cs.AI

TL;DR: 研究探讨了通过引入可能促进人类策略发现的认知机制，使MCRL模型更接近人类表现的可能性。尽管这些增强提供了有价值的见解，但仍未完全缩小模型与人类表现之间的差距。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的研究表明MCRL模型可以学习新的规划策略，并比其他机制更好地解释参与者的经验驱动的发现，但也揭示了元认知学习中的显著个体差异。此外，这些模型在拟合到人类数据时表现出比人类更慢的策略发现速度。因此，研究者希望探索是否可以通过纳入可能促进人类策略发现的认知机制来使模型更接近人类表现。

Method: 研究者具体考虑了三种认知机制：内在生成的元认知伪奖励、主观努力评估和终止思考。通过对规划任务数据的分析，研究者评估了参与者对这些机制的使用情况及其对策略发现的影响。

Result: 数据分析显示，更大比例的参与者至少使用了一种这些机制，且在使用上存在显著的个体差异，对策略发现的影响也各不相同。结果表明，元认知伪奖励、主观努力评估以及学习无需进一步规划而行动的价值有助于策略发现。

Conclusion: 尽管这些增强机制为个体差异及对策略发现的影响提供了有价值的见解，但它们并未完全缩小模型与人类表现之间的差距，促使进一步探索人们可能用于发现新规划策略的其他因素。

Abstract: People employ efficient planning strategies. But how are these strategies
acquired? Previous research suggests that people can discover new planning
strategies through learning from reinforcements, a process known as
metacognitive reinforcement learning (MCRL). While prior work has shown that
MCRL models can learn new planning strategies and explain more participants'
experience-driven discovery better than alternative mechanisms, it also
revealed significant individual differences in metacognitive learning.
Furthermore, when fitted to human data, these models exhibit a slower rate of
strategy discovery than humans. In this study, we investigate whether
incorporating cognitive mechanisms that might facilitate human strategy
discovery can bring models of MCRL closer to human performance. Specifically,
we consider intrinsically generated metacognitive pseudo-rewards, subjective
effort valuation, and termination deliberation. Analysis of planning task data
shows that a larger proportion of participants used at least one of these
mechanisms, with significant individual differences in their usage and varying
impacts on strategy discovery. Metacognitive pseudo-rewards, subjective effort
valuation, and learning the value of acting without further planning were found
to facilitate strategy discovery. While these enhancements provided valuable
insights into individual differences and the effect of these mechanisms on
strategy discovery, they did not fully close the gap between model and human
performance, prompting further exploration of additional factors that people
might use to discover new planning strategies.

</details>


### [28] [Synchronizing Process Model and Event Abstraction for Grounded Process Intelligence (Extended Version)](https://arxiv.org/abs/2505.23536)
*Janik-Vasily Benzin,Gyunam Park,Stefanie Rinderle-Ma*

Main category: cs.AI

TL;DR: 本研究解决了在模型发现后，缺乏同步抽象底层事件日志的技术问题，提供了同步模型和事件抽象的形式化基础，并通过行为特征抽象验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 目前在进行过程智能项目时，虽然可以通过模型抽象和事件抽象来降低复杂性，但还没有技术能够在发现模型后实现与底层事件日志的同步抽象，这导致失去了日志中包含的真实世界行为依据，限制了分析的深度。

Method: 作者提出了同步模型和事件抽象的形式化基础，证明了通过模型抽象（MA）对过程模型进行抽象和从抽象事件日志中发现过程模型可以得到等价的过程模型。具体使用非顺序保持的模型抽象技术——行为特征抽象作为示例，开发了一种新的事件抽象技术。

Result: 证明了同步模型和事件抽象的可行性，即通过模型抽象和从抽象事件日志中重新发现模型能够保持一致性，为过程智能项目的深入分析提供了理论和技术支持。

Conclusion: 本文提供了同步模型和事件抽象的形式化基础，证明了其可行性，并提出了一种基于行为特征抽象的新事件抽象技术，有助于保留真实世界行为数据的价值，推动过程智能项目的进展。

Abstract: Model abstraction (MA) and event abstraction (EA) are means to reduce
complexity of (discovered) models and event data. Imagine a process
intelligence project that aims to analyze a model discovered from event data
which is further abstracted, possibly multiple times, to reach optimality
goals, e.g., reducing model size. So far, after discovering the model, there is
no technique that enables the synchronized abstraction of the underlying event
log. This results in loosing the grounding in the real-world behavior contained
in the log and, in turn, restricts analysis insights. Hence, in this work, we
provide the formal basis for synchronized model and event abstraction, i.e., we
prove that abstracting a process model by MA and discovering a process model
from an abstracted event log yields an equivalent process model. We prove the
feasibility of our approach based on behavioral profile abstraction as
non-order preserving MA technique, resulting in a novel EA technique.

</details>


### [29] [SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents](https://arxiv.org/abs/2505.23559)
*Kunlun Zhu,Jiaxun Zhang,Ziheng Qi,Nuoxing Shang,Zijia Liu,Peixuan Han,Yue Su,Haofei Yu,Jiaxuan You*

Main category: cs.AI

TL;DR: SafeScientist is an AI scientist framework designed to enhance safety and ethical responsibility in AI-driven scientific exploration. It refuses ethically inappropriate or high-risk tasks, integrates multiple defensive mechanisms, and improves safety performance by 35%. A novel benchmark SciSafetyBench is also proposed for evaluating AI safety in scientific contexts.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns.

Method: SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, multiple defensive mechanisms are integrated, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component.

Result: Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35% compared to traditional AI scientist frameworks, without compromising scientific output quality. The robustness of the safety pipeline is validated against diverse adversarial attack methods.

Conclusion: SafeScientist enhances safety and ethical responsibility in AI-driven scientific exploration, as demonstrated by significant improvements in safety performance without compromising scientific output quality.

Abstract: Recent advancements in large language model (LLM) agents have significantly
accelerated scientific discovery automation, yet concurrently raised critical
ethical and safety concerns. To systematically address these challenges, we
introduce \textbf{SafeScientist}, an innovative AI scientist framework
explicitly designed to enhance safety and ethical responsibility in AI-driven
scientific exploration. SafeScientist proactively refuses ethically
inappropriate or high-risk tasks and rigorously emphasizes safety throughout
the research process. To achieve comprehensive safety oversight, we integrate
multiple defensive mechanisms, including prompt monitoring, agent-collaboration
monitoring, tool-use monitoring, and an ethical reviewer component.
Complementing SafeScientist, we propose \textbf{SciSafetyBench}, a novel
benchmark specifically designed to evaluate AI safety in scientific contexts,
comprising 240 high-risk scientific tasks across 6 domains, alongside 30
specially designed scientific tools and 120 tool-related risk tasks. Extensive
experiments demonstrate that SafeScientist significantly improves safety
performance by 35\% compared to traditional AI scientist frameworks, without
compromising scientific output quality. Additionally, we rigorously validate
the robustness of our safety pipeline against diverse adversarial attack
methods, further confirming the effectiveness of our integrated approach. The
code and data will be available at https://github.com/ulab-uiuc/SafeScientist.
\textcolor{red}{Warning: this paper contains example data that may be offensive
or harmful.}

</details>


### [30] [CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring](https://arxiv.org/abs/2505.23575)
*Benjamin Arnav,Pablo Bernabeu-Pérez,Nathan Helm-Burger,Tim Kostolansky,Hannes Whittingham,Mary Phuong*

Main category: cs.AI

TL;DR: In the context of AI models being deployed with increasing autonomy, this paper investigates Chain-of-Thought (CoT) monitoring to ensure they do not take harmful actions unnoticed. A hybrid protocol that independently scores both reasoning and final outputs outperforms both CoT and action-only monitors across all tested models and tasks.


<details>
  <summary>Details</summary>
Motivation: As AI models are deployed with increasing autonomy, it is important to ensure they do not take harmful actions unnoticed. The motivation is to find an effective way to monitor these models' intermediate reasoning steps and final outputs.

Method: The method used in this paper is Chain-of-Thought (CoT) monitoring, where a weaker trusted monitor model continuously oversees the intermediate reasoning steps of a more powerful but untrusted model. They also introduce a hybrid protocol that independently scores both reasoning and final outputs.

Result: CoT monitoring improves detection by up to 27 percentage points in scenarios where action-only monitoring fails to reliably identify sabotage. The hybrid monitor consistently outperforms both CoT and action-only monitors across all tested models and tasks, with detection rates over four times higher than action-only monitoring for subtle deception scenarios.

Conclusion: The conclusion is that the hybrid protocol which combines both reasoning and final outputs scoring is the most effective method for detecting harmful actions from autonomous AI models.

Abstract: As AI models are deployed with increasing autonomy, it is important to ensure
they do not take harmful actions unnoticed. As a potential mitigation, we
investigate Chain-of-Thought (CoT) monitoring, wherein a weaker trusted monitor
model continuously oversees the intermediate reasoning steps of a more powerful
but untrusted model. We compare CoT monitoring to action-only monitoring, where
only final outputs are reviewed, in a red-teaming setup where the untrusted
model is instructed to pursue harmful side tasks while completing a coding
problem. We find that CoT monitoring improves detection by up to 27 percentage
points in scenarios where action-only monitoring fails to reliably identify
sabotage. However, CoT traces can also contain misleading rationalizations that
deceive the monitor, reducing performance in more obvious sabotage cases. To
address this, we introduce a hybrid protocol that independently scores both
reasoning and final outputs and combines them using a weighted average. This
hybrid monitor consistently outperforms both CoT and action-only monitors
across all tested models and tasks, with detection rates over four times higher
than action-only monitoring for subtle deception scenarios.

</details>


### [31] [MAPLE: A Mobile Assistant with Persistent Finite State Machines for Recovery Reasoning](https://arxiv.org/abs/2505.23596)
*Linqiang Guo,Wei Liu,Yi Wen Heng,Tse-Hsun,Chen,Yang Wang*

Main category: cs.AI

TL;DR: MAPLE is a state-aware multi-agent framework that models app interactions as a Finite State Machine (FSM) to improve task success, recovery, and action accuracy in mobile GUI agents.


<details>
  <summary>Details</summary>
Motivation: Existing mobile GUI agents are limited by their reactive nature and lack of structured app navigation flow modeling, which hinders context understanding, error detection, and recovery capabilities.

Method: MAPLE abstracts app interactions using an FSM where UI screens are modeled as discrete states and user actions as transitions. It includes specialized agents for planning, execution, verification, error recovery, and knowledge retention, collaborating to construct FSMs dynamically based on perception data from the UI screen.

Result: MAPLE improves task success rate by up to 12%, recovery success by 13.8%, and action accuracy by 6.5% compared to the state-of-the-art baseline when evaluated on Mobile-Eval-E and SPA-Bench benchmarks.

Conclusion: Structured state modeling is crucial for guiding mobile GUI agents during task execution. The FSM representation in MAPLE can be integrated into future GUI agent architectures as a memory layer to support planning, verification, and error recovery.

Abstract: Mobile GUI agents aim to autonomously complete user-instructed tasks across
mobile apps. Recent advances in Multimodal Large Language Models (MLLMs) enable
these agents to interpret UI screens, identify actionable elements, and perform
interactions such as tapping or typing. However, existing agents remain
reactive: they reason only over the current screen and lack a structured model
of app navigation flow, limiting their ability to understand context, detect
unexpected outcomes, and recover from errors. We present MAPLE, a state-aware
multi-agent framework that abstracts app interactions as a Finite State Machine
(FSM). We computationally model each UI screen as a discrete state and user
actions as transitions, allowing the FSM to provide a structured representation
of the app execution. MAPLE consists of specialized agents responsible for four
phases of task execution: planning, execution, verification, error recovery,
and knowledge retention. These agents collaborate to dynamically construct FSMs
in real time based on perception data extracted from the UI screen, allowing
the GUI agents to track navigation progress and flow, validate action outcomes
through pre- and post-conditions of the states, and recover from errors by
rolling back to previously stable states. Our evaluation results on two
challenging cross-app benchmarks, Mobile-Eval-E and SPA-Bench, show that MAPLE
outperforms the state-of-the-art baseline, improving task success rate by up to
12%, recovery success by 13.8%, and action accuracy by 6.5%. Our results
highlight the importance of structured state modeling in guiding mobile GUI
agents during task execution. Moreover, our FSM representation can be
integrated into future GUI agent architectures as a lightweight, model-agnostic
memory layer to support structured planning, execution verification, and error
recovery.

</details>


### [32] [Fortune: Formula-Driven Reinforcement Learning for Symbolic Table Reasoning in Language Models](https://arxiv.org/abs/2505.23667)
*Lang Cao,Jingxian Xu,Hanbing Liu,Jinyu Wang,Mengyu Zhou,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: The paper proposes Formula Tuning (Fortune), a reinforcement learning framework that trains large language models to generate executable spreadsheet formulas for table question answering, improving numerical and symbolic reasoning.


<details>
  <summary>Details</summary>
Motivation: Large language models have strong general reasoning but struggle with accurate numerical or symbolic reasoning over tabular data, especially in complex scenarios. Spreadsheet formulas can represent executable symbolic operations but remain underutilized.

Method: Formula Tuning uses reinforcement learning to train LMs on generating spreadsheet formulas for question answering over tabular data, using binary answer correctness as reward signal to reduce reliance on supervised formula annotations.

Result: Extensive experiments on seven benchmarks show Formula Tuning substantially enhances LM performance on multi-step numerical and symbolic reasoning tasks, allowing a 7B model to outperform O1 on table understanding.

Conclusion: Formula-driven reinforcement learning has the potential to advance symbolic table reasoning capabilities of large language models.

Abstract: Tables are a fundamental structure for organizing and analyzing data, making
effective table understanding a critical capability for intelligent systems.
While large language models (LMs) demonstrate strong general reasoning
abilities, they continue to struggle with accurate numerical or symbolic
reasoning over tabular data, especially in complex scenarios. Spreadsheet
formulas provide a powerful and expressive medium for representing executable
symbolic operations, encoding rich reasoning patterns that remain largely
underutilized. In this paper, we propose Formula Tuning (Fortune), a
reinforcement learning (RL) framework that trains LMs to generate executable
spreadsheet formulas for question answering over general tabular data. Formula
Tuning reduces the reliance on supervised formula annotations by using binary
answer correctness as a reward signal, guiding the model to learn formula
derivation through reasoning. We provide a theoretical analysis of its
advantages and demonstrate its effectiveness through extensive experiments on
seven table reasoning benchmarks. Formula Tuning substantially enhances LM
performance, particularly on multi-step numerical and symbolic reasoning tasks,
enabling a 7B model to outperform O1 on table understanding. This highlights
the potential of formula-driven RL to advance symbolic table reasoning in LMs.

</details>


### [33] [ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork](https://arxiv.org/abs/2505.23686)
*Caroline Wang,Arrasy Rahman,Jiaxun Cui,Yoonchang Sung,Peter Stone*

Main category: cs.AI

TL;DR: This paper presents ROTATE, a regret-driven, open-ended training algorithm for Ad Hoc Teamwork (AHT) that alternates between improving the AHT agent and generating challenging teammates, leading to better generalization to unseen partners.


<details>
  <summary>Details</summary>
Motivation: Existing AHT approaches have limitations in generating diverse teammate behaviors and ensuring that these teammates are conducive to learning. Additionally, current methods treat the set of training teammates as static, which can hinder generalization to unseen partner agents.

Method: The authors propose a unified framework for AHT by reformulating it as an open-ended learning process involving an ad hoc agent and an adversarial teammate generator. They introduce ROTATE, an algorithm that iteratively improves the AHT agent and generates teammates that challenge its weaknesses.

Result: Extensive experiments across various AHT environments show that ROTATE significantly outperforms baseline methods in generalizing to an unseen set of evaluation teammates.

Conclusion: ROTATE establishes a new standard for robust and generalizable teamwork in AHT, demonstrating superior performance compared to existing methods.

Abstract: Developing AI agents capable of collaborating with previously unseen partners
is a fundamental generalization challenge in multi-agent learning, known as Ad
Hoc Teamwork (AHT). Existing AHT approaches typically adopt a two-stage
pipeline, where first, a fixed population of teammates is generated with the
idea that they should be representative of the teammates that will be seen at
deployment time, and second, an AHT agent is trained to collaborate well with
agents in the population. To date, the research community has focused on
designing separate algorithms for each stage. This separation has led to
algorithms that generate teammate pools with limited coverage of possible
behaviors, and that ignore whether the generated teammates are easy to learn
from for the AHT agent. Furthermore, algorithms for training AHT agents
typically treat the set of training teammates as static, thus attempting to
generalize to previously unseen partner agents without assuming any control
over the distribution of training teammates. In this paper, we present a
unified framework for AHT by reformulating the problem as an open-ended
learning process between an ad hoc agent and an adversarial teammate generator.
We introduce ROTATE, a regret-driven, open-ended training algorithm that
alternates between improving the AHT agent and generating teammates that probe
its deficiencies. Extensive experiments across diverse AHT environments
demonstrate that ROTATE significantly outperforms baselines at generalizing to
an unseen set of evaluation teammates, thus establishing a new standard for
robust and generalizable teamwork.

</details>


### [34] [Data-to-Dashboard: Multi-Agent LLM Framework for Insightful Visualization in Enterprise Analytics](https://arxiv.org/abs/2505.23695)
*Ran Zhang,Mohannad Elhamod*

Main category: cs.AI

TL;DR: This paper introduces a modular LLM-based agentic system that automates the data-to-dashboard process with enhanced domain detection, concept extraction, and analysis generation. It surpasses single-prompt GPT-4o baselines in insightfulness, relevance, and depth when evaluated on datasets across domains.


<details>
  <summary>Details</summary>
Motivation: To leverage LLMs' capabilities for improving data analysis and visualization by creating an agentic system that automates the entire data-to-dashboard pipeline without relying on closed ontologies or question templates.

Method: The method involves developing a modular LLM-based agentic system capable of domain detection, concept extraction, multi-perspective analysis generation, and iterative self-reflection to simulate the reasoning process of business analysts.

Result: The system outperforms GPT-4o with a single-prompt baseline in generating insightful, domain-relevant, and analytically deep visualizations as assessed by tailored evaluation metrics and human evaluations.

Conclusion: The work presents a novel modular pipeline for automating data-to-dashboard processes and highlights opportunities for human-in-the-loop validation by domain experts.

Abstract: The rapid advancement of LLMs has led to the creation of diverse agentic
systems in data analysis, utilizing LLMs' capabilities to improve insight
generation and visualization. In this paper, we present an agentic system that
automates the data-to-dashboard pipeline through modular LLM agents capable of
domain detection, concept extraction, multi-perspective analysis generation,
and iterative self-reflection. Unlike existing chart QA systems, our framework
simulates the analytical reasoning process of business analysts by retrieving
domain-relevant knowledge and adapting to diverse datasets without relying on
closed ontologies or question templates.
  We evaluate our system on three datasets across different domains.
Benchmarked against GPT-4o with a single-prompt baseline, our approach shows
improved insightfulness, domain relevance, and analytical depth, as measured by
tailored evaluation metrics and qualitative human assessment.
  This work contributes a novel modular pipeline to bridge the path from raw
data to visualization, and opens new opportunities for human-in-the-loop
validation by domain experts in business analytics. All code can be found here:
https://github.com/77luvC/D2D_Data2Dashboard

</details>


### [35] [Let's Reason Formally: Natural-Formal Hybrid Reasoning Enhances LLM's Math Capability](https://arxiv.org/abs/2505.23703)
*Ruida Wang,Yuxin Li,Yi R.,Fung,Tong Zhang*

Main category: cs.AI

TL;DR: The paper introduces NL-FL HybridReasoning, an end-to-end framework combining natural language and formal language reasoning to improve LLMs' mathematical reasoning abilities. It proposes methods for aligning NL and FL problems and answers, achieving higher accuracy on math benchmarks than NL-only models.


<details>
  <summary>Details</summary>
Motivation: Despite progress in enhancing LLMs' mathematical reasoning via reinforcement learning, these methods fail to add new capabilities not present in the base model. This necessitates integrating formal language knowledge into natural language reasoning effectively, though challenges exist due to differences in problem structure and reasoning formats between the two.

Method: The method involves introducing NL-FL HybridReasoning, which incorporates formal language expertise into natural language math problem-solving through three key techniques: NL-FL Problem Alignment (reformulating NL QA problems as FL existence theorems), Mixed Problem Input (allowing the FL reasoner to handle both types of problems concurrently), and Answer Extraction (mitigating output format gaps using an LLM-based mechanism).

Result: The framework achieves 89.80% and 84.34% accuracy rates on the MATH-500 and AMC benchmarks respectively, surpassing the NL baseline by 4.60% and 4.82%. Notably, some problems solved by this framework remain unsolved by the NL baseline even with more trials.

Conclusion: The NL-FL HybridReasoning framework successfully bridges the gap between natural language and formal language reasoning, demonstrating superior performance on mathematical benchmarks compared to natural language only approaches.

Abstract: Enhancing the mathematical reasoning capabilities of LLMs has garnered
significant attention in both the mathematical and computer science
communities. Recent works have made substantial progress in both Natural
Language (NL) reasoning and Formal Language (FL) reasoning by leveraging the
potential of pure Reinforcement Learning (RL) methods on base models. However,
RL approaches struggle to impart new capabilities not presented in the base
model, highlighting the need to integrate more knowledge like FL into NL math
reasoning effectively. Yet, this integration is challenging due to inherent
disparities in problem structure and reasoning format between NL and FL. To
address these challenges, we introduce **NL-FL HybridReasoning**, an end-to-end
framework designed to incorporate the FL expert into NL math problem-solving.
To bridge the NL and FL input format gap, we propose the *NL-FL Problem
Alignment* method, which reformulates the Question-Answering (QA) problems in
NL as existence theorems in FL. Subsequently, the *Mixed Problem Input*
technique we provide enables the FL reasoner to handle both QA and existence
problems concurrently. Lastly, we mitigate the NL and FL output format gap in
reasoning through an LLM-based *Answer Extraction* mechanism. Comprehensive
experiments demonstrate that the **HybridReasoning** framework achieves
**89.80%** and **84.34%** accuracy rates on the MATH-500 and the AMC
benchmarks, surpassing the NL baseline by 4.60% and 4.82%, respectively.
Notably, some problems resolved by our framework remain unsolved by the NL
baseline model even under a larger number of trials.

</details>


### [36] [Comparative of Genetic Fuzzy regression techniques for aeroacoustic phenomenons](https://arxiv.org/abs/2505.23746)
*Hugo Henry,Kelly Cohen*

Main category: cs.AI

TL;DR: This study explores the use of Genetic Fuzzy Systems (GFS) for modeling airfoil self-noise using different fuzzy regression strategies and shows that clustering assisted fuzzy inference is effective for complex aeroacoustic phenomena.


<details>
  <summary>Details</summary>
Motivation: To address the issue of airfoil self-noise which is crucial in aerospace, automotive and drone applications by applying Genetic Fuzzy Systems.

Method: Using the Airfoil Self Noise dataset, the study evaluates Takagi Sugeno Kang (TSK) fuzzy system, Genetic Fuzzy Tree (GFT) architecture, and a novel clustered approach based on Fuzzy C-means (FCM).

Result: Clustering assisted fuzzy inference was found to be an effective tool for regression in complex aeroacoustic phenomena.

Conclusion: Genetic Fuzzy Systems, particularly with clustering assistance, provide a viable method for modeling airfoil self-noise.

Abstract: This study investigates the application of Genetic Fuzzy Systems (GFS) to
model the self-noise generated by airfoils, a key issue in aeroaccoustics with
significant implications for aerospace, automotive and drone applications.
Using the publicly available Airfoil Self Noise dataset, various Fuzzy
regression strategies are explored and compared. The paper evaluates a brute
force Takagi Sugeno Kang (TSK) fuzzy system with high rule density, a cascading
Geneti Fuzzy Tree (GFT) architecture and a novel clustered approach based on
Fuzzy C-means (FCM) to reduce the model's complexity. This highlights the
viability of clustering assisted fuzzy inference as an effective regression
tool for complex aero accoustic phenomena. Keywords : Fuzzy logic, Regression,
Cascading systems, Clustering and AI.

</details>


### [37] [ZeroGUI: Automating Online GUI Learning at Zero Human Cost](https://arxiv.org/abs/2505.23762)
*Chenyu Yang,Shiqian Su,Shi Liu,Xuan Dong,Yue Yu,Weijie Su,Xuehui Wang,Zhaoyang Liu,Jinguo Zhu,Hao Li,Wenhai Wang,Yu Qiao,Xizhou Zhu,Jifeng Dai*

Main category: cs.AI

TL;DR: ZeroGUI is a scalable, online learning framework for automating GUI Agent training with no human cost.


<details>
  <summary>Details</summary>
Motivation: Existing approaches of pure-vision-based GUI Agents rely heavily on high-quality manual annotations and lack adaptability to dynamic environments.

Method: ZeroGUI integrates VLM-based automatic task generation, VLM-based automatic reward estimation, and two-stage online reinforcement learning to overcome the limitations of current methods.

Result: Experiments on UI-TARS and Aguvis demonstrate significant performance improvements in OSWorld and AndroidLab environments.

Conclusion: ZeroGUI successfully automates GUI Agent training at zero human cost and enhances adaptability in dynamic and interactive environments.

Abstract: The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Localized Weather Prediction Using Kolmogorov-Arnold Network-Based Models and Deep RNNs](https://arxiv.org/abs/2505.22686)
*Ange-Clement Akazan,Verlon Roel Mbingui,Gnankan Landry Regis N'guessan,Issa Karambal*

Main category: cs.LG

TL;DR: The paper compares deep recurrent neural networks and Kolmogorov-Arnold-based models for weather forecasting in tropical Africa, finding that KAN excels at temperature prediction, TKAN variants improve precipitation forecasting, and classical RNNs are competitive for atmospheric pressure.


<details>
  <summary>Details</summary>
Motivation: Weather forecasting is crucial in tropical Africa due to the impact of extreme events on livelihoods, but existing methods struggle with the region's complex weather patterns. This study aims to benchmark different models for daily forecasting of temperature, precipitation, and pressure in two tropical cities.

Method: The study evaluates LSTM, GRU, BiLSTM, BiGRU, KAN, and TKAN models for weather forecasting. Two customized TKAN variants using GeLU and MiSH activation functions are introduced. Station-level meteorological data from 2010 to 2024 is used for evaluation.

Result: KAN achieves high accuracy in temperature prediction (R²=0.9986 in Abidjan, R²=0.9998 in Kigali, MSE<0.0014°C²). TKAN variants minimize absolute errors for precipitation forecasting in low-rainfall regimes. Customized TKAN models show improvements over standard TKAN. Classical RNNs outperform KAN-based models for atmospheric pressure forecasting (R²≈0.83-0.86).

Conclusion: Spline-based neural architectures show potential for efficient and data-efficient weather forecasting, particularly in temperature and precipitation tasks.

Abstract: Weather forecasting is crucial for managing risks and economic planning,
particularly in tropical Africa, where extreme events severely impact
livelihoods. Yet, existing forecasting methods often struggle with the region's
complex, non-linear weather patterns. This study benchmarks deep recurrent
neural networks such as $\texttt{LSTM, GRU, BiLSTM, BiGRU}$, and
Kolmogorov-Arnold-based models $(\texttt{KAN} and \texttt{TKAN})$ for daily
forecasting of temperature, precipitation, and pressure in two tropical cities:
Abidjan, Cote d'Ivoire (Ivory Coast) and Kigali (Rwanda). We further introduce
two customized variants of $ \texttt{TKAN}$ that replace its original
$\texttt{SiLU}$ activation function with $ \texttt{GeLU}$ and \texttt{MiSH},
respectively. Using station-level meteorological data spanning from 2010 to
2024, we evaluate all the models on standard regression metrics. $\texttt{KAN}$
achieves temperature prediction ($R^2=0.9986$ in Abidjan, $0.9998$ in Kigali,
$\texttt{MSE} < 0.0014~^\circ C ^2$), while $\texttt{TKAN}$ variants minimize
absolute errors for precipitation forecasting in low-rainfall regimes. The
customized $\texttt{TKAN}$ models demonstrate improvements over the standard
$\texttt{TKAN}$ across both datasets. Classical \texttt{RNNs} remain highly
competitive for atmospheric pressure ($R^2 \approx 0.83{-}0.86$), outperforming
$\texttt{KAN}$-based models in this task. These results highlight the potential
of spline-based neural architectures for efficient and data-efficient
forecasting.

</details>


### [39] [SlimLLM: Accurate Structured Pruning for Large Language Models](https://arxiv.org/abs/2505.22689)
*Jialong Guo,Xinghao Chen,Yehui Tang,Yunhe Wang*

Main category: cs.LG

TL;DR: The paper proposes SlimLLM, a structured pruning method for large language models (LLMs), which evaluates the importance of channels and attention heads holistically and uses a linear regression strategy to recover performance. It also introduces a layer-based importance ratio for determining pruning ratios per layer.


<details>
  <summary>Details</summary>
Motivation: To address the enormous computational costs associated with deploying and applying LLMs by utilizing structured pruning as an effective solution to compress their parameters.

Method: SlimLLM evaluates channel and attention head importance based on entire units rather than individual elements within sub-modules, applies a linear regression strategy for output matrix recovery, and determines pruning ratios using layer-based importance ratios.

Result: Based on LLaMA benchmark results, SlimLLM outperforms other methods and achieves state-of-the-art performance.

Conclusion: SlimLLM is an effective and fast structured pruning method that addresses the challenges of parameter compression in LLMs while minimizing performance loss.

Abstract: Large language models(LLMs) have garnered significant attention and
demonstrated impressive capabilities in a wide range of applications. However,
due to their enormous computational costs, the deployment and application of
LLMs are often severely limited. To address this issue, structured pruning is
an effective solution to compress the parameters of LLMs. Determining the
importance of each sub-module in LLMs and minimizing performance loss are
critical issues that need to be carefully addressed in structured pruning. In
this paper, we propose an effective and fast structured pruning method named
SlimLLM for large language models. For channel and attention head pruning, we
evaluate the importance based on the entire channel or head, rather than merely
aggregating the importance of individual elements within a sub-module. This
approach enables a more holistic consideration of the interdependence among
elements within the sub-module. In addition, we design a simple linear
regression strategy for the output matrix to quickly recover performance. We
also propose layer-based importance ratio to determine the pruning ratio for
each layer. Based on the LLaMA benchmark results, our SlimLLM outperforms other
methods and achieves state-of-the-art performance.

</details>


### [40] [MoRE: A Mixture of Low-Rank Experts for Adaptive Multi-Task Learning](https://arxiv.org/abs/2505.22694)
*Dacao Zhang,Kun Zhang,Shimao Chu,Le Wu,Xin Li,Si Wei*

Main category: cs.LG

TL;DR: 提出了一种新的多任务参数高效微调方法MoRE，通过联合训练低秩专家和设计自适应秩选择器，在不增加推理成本的情况下显著提升了LLM在多任务场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法及其改进要么专注于单任务场景，要么为多任务场景分别训练多个LoRA模块，这限制了其在多任务场景中的效率和效果。

Method: 提出了Mixture of Low-Rank Experts (MoRE) 方法，用不同秩的LoRA模块对应不同的任务（称为低秩专家），并设计了一个自适应秩选择器来为每个任务选择合适的专家，通过联合训练这些低秩专家提升多任务场景下的适应性和效率。

Result: 实验结果表明，与传统的LoRA及其变体相比，MoRE显著提高了LLM在多任务场景中的性能，并且没有增加额外的推理成本。

Conclusion: MoRE是一种有效的多任务PEFT方法，能够提升LLM在多任务场景中的表现，同时保持与传统LoRA相同的推理效率。

Abstract: With the rapid development of Large Language Models (LLMs),
Parameter-Efficient Fine-Tuning (PEFT) methods have gained significant
attention, which aims to achieve efficient fine-tuning of LLMs with fewer
parameters. As a representative PEFT method, Low-Rank Adaptation (LoRA)
introduces low-rank matrices to approximate the incremental tuning parameters
and achieves impressive performance over multiple scenarios. After that, plenty
of improvements have been proposed for further improvement. However, these
methods either focus on single-task scenarios or separately train multiple LoRA
modules for multi-task scenarios, limiting the efficiency and effectiveness of
LoRA in multi-task scenarios. To better adapt to multi-task fine-tuning, in
this paper, we propose a novel Mixture of Low-Rank Experts (MoRE) for
multi-task PEFT. Specifically, instead of using an individual LoRA for each
task, we align different ranks of LoRA module with different tasks, which we
named low-rank experts. Moreover, we design a novel adaptive rank selector to
select the appropriate expert for each task. By jointly training low-rank
experts, MoRE can enhance the adaptability and efficiency of LoRA in multi-task
scenarios. Finally, we conduct extensive experiments over multiple multi-task
benchmarks along with different LLMs to verify model performance. Experimental
results demonstrate that compared to traditional LoRA and its variants, MoRE
significantly improves the performance of LLMs in multi-task scenarios and
incurs no additional inference cost. We also release the model and code to
facilitate the community.

</details>


### [41] [LLM-ODDR: A Large Language Model Framework for Joint Order Dispatching and Driver Repositioning](https://arxiv.org/abs/2505.22695)
*Tengfei Lyu,Siyuan Feng,Hao Liu,Hai Yang*

Main category: cs.LG

TL;DR: The paper introduces LLM-ODDR, a framework using Large Language Models for optimizing ride-hailing order dispatching and driver repositioning. It includes components for order value refinement, fair dispatching balancing platform revenue with driver income, and demand-aware repositioning of drivers. JointDR-GPT, a specialized model for ODDR tasks, is also developed. Experiments on Manhattan taxi data show significant improvements over traditional methods in effectiveness, adaptability, and interpretability.


<details>
  <summary>Details</summary>
Motivation: Ride-hailing platforms struggle with optimizing order dispatching and driver repositioning while considering driver income fairness, interpretability, and real-world dynamics. Traditional approaches often overlook these factors.

Method: LLM-ODDR framework consists of three key components: Multi-objective-guided Order Value Refinement, Fairness-aware Order Dispatching, and Spatiotemporal Demand-Aware Driver Repositioning. Additionally, JointDR-GPT, a fine-tuned model optimized for ODDR tasks, is developed.

Result: Experiments on real-world datasets from Manhattan taxi operations indicate that LLM-ODDR outperforms traditional methods in terms of effectiveness, adaptability to anomalous conditions, and decision interpretability.

Conclusion: This work represents the first exploration of using LLMs as decision-making agents in ride-hailing ODDR tasks, providing foundational insights for integrating advanced language models within intelligent transportation systems.

Abstract: Ride-hailing platforms face significant challenges in optimizing order
dispatching and driver repositioning operations in dynamic urban environments.
Traditional approaches based on combinatorial optimization, rule-based
heuristics, and reinforcement learning often overlook driver income fairness,
interpretability, and adaptability to real-world dynamics. To address these
gaps, we propose LLM-ODDR, a novel framework leveraging Large Language Models
(LLMs) for joint Order Dispatching and Driver Repositioning (ODDR) in
ride-hailing services. LLM-ODDR framework comprises three key components: (1)
Multi-objective-guided Order Value Refinement, which evaluates orders by
considering multiple objectives to determine their overall value; (2)
Fairness-aware Order Dispatching, which balances platform revenue with driver
income fairness; and (3) Spatiotemporal Demand-Aware Driver Repositioning,
which optimizes idle vehicle placement based on historical patterns and
projected supply. We also develop JointDR-GPT, a fine-tuned model optimized for
ODDR tasks with domain knowledge. Extensive experiments on real-world datasets
from Manhattan taxi operations demonstrate that our framework significantly
outperforms traditional methods in terms of effectiveness, adaptability to
anomalous conditions, and decision interpretability. To our knowledge, this is
the first exploration of LLMs as decision-making agents in ride-hailing ODDR
tasks, establishing foundational insights for integrating advanced language
models within intelligent transportation systems.

</details>


### [42] [When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?](https://arxiv.org/abs/2505.22696)
*Eleni Nisioti,Joachim Winther Pedersen,Erwan Plantec,Milton L. Montero,Sebastian Risi*

Main category: cs.LG

TL;DR:  Neuroevolution (NE) methods show potential in transfer learning, often outperforming Reinforcement Learning (RL) baselines, and could lead to more adaptable agents.


<details>
  <summary>Details</summary>
Motivation: Transfer learning capabilities are underexplored in NE, while RL struggles with task variations and catastrophic forgetting.

Method: Two benchmarks, stepping gates (logic circuits emulation) and ecorobot (physics engine extension), are used to evaluate NE's transfer learning abilities across tasks of increasing complexity.

Result: NE methods vary in their transfer abilities and frequently outperform RL baselines.

Conclusion: NE shows promise for building adaptable agents but faces challenges in scaling to complex, real-world problems.

Abstract: The ability to continuously and efficiently transfer skills across tasks is a
hallmark of biological intelligence and a long-standing goal in artificial
systems. Reinforcement learning (RL), a dominant paradigm for learning in
high-dimensional control tasks, is known to suffer from brittleness to task
variations and catastrophic forgetting. Neuroevolution (NE) has recently gained
attention for its robustness, scalability, and capacity to escape local optima.
In this paper, we investigate an understudied dimension of NE: its transfer
learning capabilities. To this end, we introduce two benchmarks: a) in stepping
gates, neural networks are tasked with emulating logic circuits, with designs
that emphasize modular repetition and variation b) ecorobot extends the Brax
physics engine with objects such as walls and obstacles and the ability to
easily switch between different robotic morphologies. Crucial in both
benchmarks is the presence of a curriculum that enables evaluating skill
transfer across tasks of increasing complexity. Our empirical analysis shows
that NE methods vary in their transfer abilities and frequently outperform RL
baselines. Our findings support the potential of NE as a foundation for
building more adaptable agents and highlight future challenges for scaling NE
to complex, real-world problems.

</details>


### [43] [Update Your Transformer to the Latest Release: Re-Basin of Task Vectors](https://arxiv.org/abs/2505.22697)
*Filippo Rinaldi,Giacomo Capitani,Lorenzo Bonicelli,Donato Crisostomi,Federico Bolelli,Elisa Ficarra,Emanuele Rodolà,Simone Calderara,Angelo Porrello*

Main category: cs.LG

TL;DR: This paper explores a method to transfer fine-tuning from an old model checkpoint to a new one without needing the original data or retraining, specifically focusing on Transformer models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue where fine-tuned models become obsolete when their underlying pre-trained models are updated or retrained, necessitating a solution that allows for the transfer of fine-tuning to new model releases without requiring retraining or access to the original data.

Method: The method involves drawing principles from model re-basin and applying weight permutations to re-base the modifications (task vector) made to the original base model. A two-level method rooted in spectral theory is proposed, which first permutes the attention heads and then adjusts parameters within select pairs of heads, tailored for Transformer models with consideration of residual connections and multi-head attention layers.

Result: Through extensive experiments on visual and textual tasks, the approach successfully achieves the seamless transfer of fine-tuned knowledge to new pre-trained backbones without relying on any training steps or datapoints.

Conclusion: The conclusion is that it is possible to transfer fine-tuning to a new release of a model in a data-free manner, maintaining the utility of fine-tuned models even as their underlying pre-trained models are updated.

Abstract: Foundation models serve as the backbone for numerous specialized models
developed through fine-tuning. However, when the underlying pretrained model is
updated or retrained (e.g., on larger and more curated datasets), the
fine-tuned model becomes obsolete, losing its utility and requiring retraining.
This raises the question: is it possible to transfer fine-tuning to a new
release of the model? In this work, we investigate how to transfer fine-tuning
to a new checkpoint without having to re-train, in a data-free manner. To do
so, we draw principles from model re-basin and provide a recipe based on weight
permutations to re-base the modifications made to the original base model,
often called task vector. In particular, our approach tailors model re-basin
for Transformer models, taking into account the challenges of residual
connections and multi-head attention layers. Specifically, we propose a
two-level method rooted in spectral theory, initially permuting the attention
heads and subsequently adjusting parameters within select pairs of heads.
Through extensive experiments on visual and textual tasks, we achieve the
seamless transfer of fine-tuned knowledge to new pre-trained backbones without
relying on a single training step or datapoint. Code is available at
https://github.com/aimagelab/TransFusion.

</details>


### [44] [Private Rate-Constrained Optimization with Applications to Fair Learning](https://arxiv.org/abs/2505.22703)
*Mohammad Yaghini,Tudor Cebere,Michael Menart,Aurélien Bellet,Nicolas Papernot*

Main category: cs.LG

TL;DR: This paper presents RaCO-DP, a differential privacy variant of SGDA algorithm for solving constrained minimization problems with rate constraints in trustworthy ML. It shows that this method outperforms existing private learning approaches in fairness-utility trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address the issue that standard DP optimization techniques cannot handle rate constraints due to inter-sample dependencies.

Method: Develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA) algorithm which solves the Lagrangian formulation of rate constraint problems. The additional privacy cost reduces to privately estimating a histogram over the mini-batch at each optimization step.

Result: Proved the convergence of the RaCO-DP algorithm and empirical results show that it Pareto-dominates existing private learning approaches in fairness-utility trade-offs.

Conclusion: RaCO-DP is an effective method for solving constrained minimization problems under differential privacy with rate constraints.

Abstract: Many problems in trustworthy ML can be formulated as minimization of the
model error under constraints on the prediction rates of the model for
suitably-chosen marginals, including most group fairness constraints
(demographic parity, equality of odds, etc.). In this work, we study such
constrained minimization problems under differential privacy (DP). Standard DP
optimization techniques like DP-SGD rely on the loss function's decomposability
into per-sample contributions. However, rate constraints introduce inter-sample
dependencies, violating the decomposability requirement. To address this, we
develop RaCO-DP, a DP variant of the Stochastic Gradient Descent-Ascent (SGDA)
algorithm which solves the Lagrangian formulation of rate constraint problems.
We demonstrate that the additional privacy cost of incorporating these
constraints reduces to privately estimating a histogram over the mini-batch at
each optimization step. We prove the convergence of our algorithm through a
novel analysis of SGDA that leverages the linear structure of the dual
parameter. Finally, empirical results on learning under group fairness
constraints demonstrate that our method Pareto-dominates existing private
learning approaches in fairness-utility trade-offs.

</details>


### [45] [FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference](https://arxiv.org/abs/2505.22758)
*Aniruddha Nrusimha,William Brandon,Mayank Mishra,Yikang Shen,Rameswar Panda,Jonathan Ragan-Kelley,Yoon Kim*

Main category: cs.LG

TL;DR: FlashFormer是一种用于加速基于Transformer的大语言模型单批次推理的概念验证内核，与现有的最先进的推理内核相比，在不同模型大小和量化设置下观察到显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型的规模和计算特性促使了对训练和推理优化的专用内核的发展，但目前主要集中在大批量训练和推理上，而小批量推理在边缘部署和延迟敏感应用中仍然重要，因此需要一种新的内核来优化这一领域。

Method: 提出了FlashFormer，这是一个针对基于Transformer的大语言模型单批次推理加速的概念验证内核。

Result: 在各种模型大小和量化设置下，与现有最先进的推理内核相比，FlashFormer展示了非平凡的速度提升。

Conclusion: FlashFormer为低批量推理提供了一个有效的解决方案，特别是在边缘部署和延迟敏感的应用场景中具有重要意义。

Abstract: The size and compute characteristics of modern large language models have led
to an increased interest in developing specialized kernels tailored for
training and inference. Existing kernels primarily optimize for compute
utilization, targeting the large-batch training and inference settings.
However, low-batch inference, where memory bandwidth and kernel launch
overheads contribute are significant factors, remains important for many
applications of interest such as in edge deployment and latency-sensitive
applications. This paper describes FlashFormer, a proof-of-concept kernel for
accelerating single-batch inference for transformer-based large language
models. Across various model sizes and quantizations settings, we observe
nontrivial speedups compared to existing state-of-the-art inference kernels.

</details>


### [46] [Test-time augmentation improves efficiency in conformal prediction](https://arxiv.org/abs/2505.22764)
*Divya Shanmugam,Helen Lu,Swami Sankaranarayanan,John Guttag*

Main category: cs.LG

TL;DR: The paper proposes using test-time augmentation (TTA) to reduce the size of prediction sets in conformal classifiers without retraining models, demonstrating its effectiveness across multiple datasets and conditions.


<details>
  <summary>Details</summary>
Motivation: Conformal classifiers often produce large sets of predicted classes which lack informativeness, prompting the need for a method to reduce these set sizes while maintaining probabilistic guarantees.

Method: The authors employ TTA during inference to introduce inductive biases, making the approach adaptable with any conformal score, not requiring model retraining, and being computationally efficient.

Result: TTA reduces prediction set sizes by 10%-14% on average across different datasets, models, scoring methods, guarantee strengths, and distribution shifts.

Conclusion: Test-time augmentation is shown to be a useful addition to the conformal pipeline, effectively reducing prediction set sizes in conformal classifiers.

Abstract: A conformal classifier produces a set of predicted classes and provides a
probabilistic guarantee that the set includes the true class. Unfortunately, it
is often the case that conformal classifiers produce uninformatively large
sets. In this work, we show that test-time augmentation (TTA)--a technique that
introduces inductive biases during inference--reduces the size of the sets
produced by conformal classifiers. Our approach is flexible, computationally
efficient, and effective. It can be combined with any conformal score, requires
no model retraining, and reduces prediction set sizes by 10%-14% on average. We
conduct an evaluation of the approach spanning three datasets, three models,
two established conformal scoring methods, different guarantee strengths, and
several distribution shifts to show when and why test-time augmentation is a
useful addition to the conformal pipeline.

</details>


### [47] [Multivariate de Bruijn Graphs: A Symbolic Graph Framework for Time Series Forecasting](https://arxiv.org/abs/2505.22768)
*Mert Onur Cakiroglu,Idil Bilge Altun,Hasan Kurban,Elham Buxton,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: DRAGON is a new encoder that uses Multivariate de Bruijn Graphs to improve time series forecasting by bridging symbolic representations and neural modeling.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting is difficult for foundation models due to temporal heterogeneity, high dimensionality, and lack of inherent symbolic structure.

Method: Propose DRAGON encoder which discretizes continuous input sequences and maps them onto a fixed graph structure using Multivariate de Bruijn Graphs, enabling dynamic context recovery via graph-based attention.

Result: DRAGON augments conventional CNN-based encoders with symbolic, structure-aware representations when integrated as an auxiliary module within a dual-branch architecture.

Conclusion: All code developed for this study is available at https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library.

Abstract: Time series forecasting remains a challenging task for foundation models due
to temporal heterogeneity, high dimensionality, and the lack of inherent
symbolic structure. In this work, we propose DRAGON (Discrete Representation
and Augmented Graph encoding Over deBruijN Graphs), a novel encoder that
introduces Multivariate de Bruijn Graphs (MdBGs) to bridge the gap between
symbolic representations and neural modeling. DRAGON discretizes continuous
input sequences and maps them onto a fixed graph structure, enabling dynamic
context recovery via graph-based attention. Integrated as an auxiliary module
within a dual-branch architecture, DRAGON augments conventional CNN-based
encoders with symbolic, structure-aware representations. All code developed for
this study is available at:
https://github.com/KurbanIntelligenceLab/MultdBG-Time-Series-Library

</details>


### [48] [Calibrated Value-Aware Model Learning with Stochastic Environment Models](https://arxiv.org/abs/2505.22772)
*Claas Voelcker,Anastasiia Pedan,Arash Ahmadian,Romina Abachi,Igor Gilitschenski,Amir-massoud Farahmand*

Main category: cs.LG

TL;DR: 在本文中，作者分析了价值感知模型学习损失（包括MuZero损失）的家族，并发现这些损失通常是未校准的代理损失，不能始终恢复正确的模型和价值函数。基于这一见解，作者提出了修正方案，并探讨了损失校准、潜在模型架构和辅助损失之间的相互作用。此外，尽管确定性模型足以预测准确的价值，但学习校准的随机模型仍然具有优势。


<details>
  <summary>Details</summary>
Motivation: 价值感知模型学习的概念在基于模型的强化学习中变得突出，特别是MuZero损失在许多经验工作中得到了应用。然而，对其理论上的优缺点研究有限。

Method: 1. 分析价值感知模型学习损失家族，包括MuZero损失。
2. 发现这些损失是未校准的代理损失。
3. 提出修正方案以解决此问题。
4. 探讨损失校准、潜在模型架构和辅助损失之间的相互作用。
5. 研究确定性模型和校准随机模型的优势。

Result: 证明了常用的MuZero损失和其他价值感知模型学习损失是未校准的代理损失，可能无法恢复正确的模型和价值函数。提出了一些修正方案，并揭示了损失校准对模型训练的影响。同时表明，虽然确定性模型可以提供准确的价值预测，但校准随机模型仍具有优势。

Conclusion: 价值感知模型学习中的损失函数需要进行校准以确保模型和价值函数的正确恢复。校准随机模型在某些情况下比确定性模型更具优势。

Abstract: The idea of value-aware model learning, that models should produce accurate
value estimates, has gained prominence in model-based reinforcement learning.
The MuZero loss, which penalizes a model's value function prediction compared
to the ground-truth value function, has been utilized in several prominent
empirical works in the literature. However, theoretical investigation into its
strengths and weaknesses is limited. In this paper, we analyze the family of
value-aware model learning losses, which includes the popular MuZero loss. We
show that these losses, as normally used, are uncalibrated surrogate losses,
which means that they do not always recover the correct model and value
function. Building on this insight, we propose corrections to solve this issue.
Furthermore, we investigate the interplay between the loss calibration, latent
model architectures, and auxiliary losses that are commonly employed when
training MuZero-style agents. We show that while deterministic models can be
sufficient to predict accurate values, learning calibrated stochastic models is
still advantageous.

</details>


### [49] [Machine Learning Models Have a Supply Chain Problem](https://arxiv.org/abs/2505.22778)
*Sarah Meiklejohn,Hayden Blauzvern,Mihai Maruseac,Spencer Schrock,Laurent Simon,Ilia Shumailov*

Main category: cs.LG

TL;DR: The paper argues that the open ML model ecosystem has significant supply-chain risks, and explores how Sigstore can bring transparency to open ML models.


<details>
  <summary>Details</summary>
Motivation: To address the supply-chain risks in the current open ML model ecosystem, which include malicious model replacement, vulnerable framework training, and restricted or poisoned data.

Method: Exploring the use of Sigstore, a solution designed for open-source software supply chains, to enable model publishers to sign their models and prove properties about the datasets they use.

Result: Sigstore can be used to bring transparency to open ML models, enhancing security and trustworthiness.

Conclusion: The current open ML model ecosystem contains significant supply-chain risks, but Sigstore provides a potential solution to increase transparency and trust.

Abstract: Powerful machine learning (ML) models are now readily available online, which
creates exciting possibilities for users who lack the deep technical expertise
or substantial computing resources needed to develop them. On the other hand,
this type of open ecosystem comes with many risks. In this paper, we argue that
the current ecosystem for open ML models contains significant supply-chain
risks, some of which have been exploited already in real attacks. These include
an attacker replacing a model with something malicious (e.g., malware), or a
model being trained using a vulnerable version of a framework or on restricted
or poisoned data. We then explore how Sigstore, a solution designed to bring
transparency to open-source software supply chains, can be used to bring
transparency to open ML models, in terms of enabling model publishers to sign
their models and prove properties about the datasets they use.

</details>


### [50] [Navigating the Latent Space Dynamics of Neural Models](https://arxiv.org/abs/2505.22785)
*Marco Fumero,Luca Moschella,Emanuele Rodolà,Francesco Locatello*

Main category: cs.LG

TL;DR: This paper interprets neural models as dynamical systems on the latent manifold, revealing attractor points and proposing to use the vector field as a representation for analysis.


<details>
  <summary>Details</summary>
Motivation: To provide a new perspective on neural networks by interpreting them as dynamical systems acting on the latent manifold, which could offer novel insights into model properties and data characteristics.

Method: Show that autoencoder models define a latent vector field through iterative application of the encoding-decoding map. Analyze how standard training introduces inductive biases leading to attractor points within this vector field.

Result: The vector field can be used to analyze generalization and memorization, extract prior knowledge from network parameters, and identify out-of-distribution samples without needing input data.

Conclusion: The approach is validated on vision foundation models, demonstrating its applicability and effectiveness in real-world scenarios.

Abstract: Neural networks transform high-dimensional data into compact, structured
representations, often modeled as elements of a lower dimensional latent space.
In this paper, we present an alternative interpretation of neural models as
dynamical systems acting on the latent manifold. Specifically, we show that
autoencoder models implicitly define a latent vector field on the manifold,
derived by iteratively applying the encoding-decoding map, without any
additional training. We observe that standard training procedures introduce
inductive biases that lead to the emergence of attractor points within this
vector field. Drawing on this insight, we propose to leverage the vector field
as a representation for the network, providing a novel tool to analyze the
properties of the model and the data. This representation enables to: (i)
analyze the generalization and memorization regimes of neural models, even
throughout training; (ii) extract prior knowledge encoded in the network's
parameters from the attractors, without requiring any input data; (iii)
identify out-of-distribution samples from their trajectories in the vector
field. We further validate our approach on vision foundation models, showcasing
the applicability and effectiveness of our method in real-world scenarios.

</details>


### [51] [Efficient Preimage Approximation for Neural Network Certification](https://arxiv.org/abs/2505.22798)
*Anton Björklund,Mykola Zaitsev,Marta Kwiatkowska*

Main category: cs.LG

TL;DR: The paper improves the PREMAP algorithm for neural network certification, achieving significant efficiency gains and scalability.


<details>
  <summary>Details</summary>
Motivation: Certification of neural networks is crucial in safety- and security-critical applications, especially against 'patch attacks'. Current preimage approximation methods, including PREMAP, lack scalability.

Method: The paper introduces algorithmic improvements to the PREMAP algorithm, incorporating tighter bounds, adaptive Monte Carlo sampling, and enhanced branching heuristics.

Result: Efficiency improvements of at least an order of magnitude were achieved on reinforcement learning control benchmarks. The method also scales to previously infeasible convolutional neural networks.

Conclusion: These advancements highlight the potential of preimage approximation methods for ensuring reliability and robustness in neural network certification.

Abstract: The growing reliance on artificial intelligence in safety- and
security-critical applications demands effective neural network certification.
A challenging real-world use case is certification against ``patch attacks'',
where adversarial patches or lighting conditions obscure parts of images, for
example traffic signs. One approach to certification, which also gives
quantitative coverage estimates, utilizes preimages of neural networks, i.e.,
the set of inputs that lead to a specified output. However, these preimage
approximation methods, including the state-of-the-art PREMAP algorithm,
struggle with scalability. This paper presents novel algorithmic improvements
to PREMAP involving tighter bounds, adaptive Monte Carlo sampling, and improved
branching heuristics. We demonstrate efficiency improvements of at least an
order of magnitude on reinforcement learning control benchmarks, and show that
our method scales to convolutional neural networks that were previously
infeasible. Our results demonstrate the potential of preimage approximation
methodology for reliability and robustness certification.

</details>


### [52] [CLUE: Neural Networks Calibration via Learning Uncertainty-Error alignment](https://arxiv.org/abs/2505.22803)
*Pedro Mendes,Paolo Romano,David Garlan*

Main category: cs.LG

TL;DR: CLUE is a novel approach for calibration in neural networks that aligns predicted uncertainty with observed error during training, using summary statistics of uncertainty and loss as proxies. It's fully differentiable, domain-agnostic, and compatible with standard training pipelines.


<details>
  <summary>Details</summary>
Motivation: Existing calibration techniques for neural networks are limited in scalability, differentiability, and generalization across domains.

Method: CLUE adopts a novel loss function that jointly optimizes predictive performance and calibration, explicitly aligning predicted uncertainty with observed error during training.

Result: Through extensive experiments on various tasks including out-of-distribution and domain-shift scenarios, CLUE achieves superior calibration quality and competitive predictive performance with respect to state-of-the-art approaches without significant computational overhead.

Conclusion: CLUE provides a reliable method for uncertainty estimation in neural networks, advancing the deployment of such models in real-world applications.

Abstract: Reliable uncertainty estimation is critical for deploying neural networks
(NNs) in real-world applications. While existing calibration techniques often
rely on post-hoc adjustments or coarse-grained binning methods, they remain
limited in scalability, differentiability, and generalization across domains.
In this work, we introduce CLUE (Calibration via Learning Uncertainty-Error
Alignment), a novel approach that explicitly aligns predicted uncertainty with
observed error during training, grounded in the principle that well-calibrated
models should produce uncertainty estimates that match their empirical loss.
CLUE adopts a novel loss function that jointly optimizes predictive performance
and calibration, using summary statistics of uncertainty and loss as proxies.
The proposed method is fully differentiable, domain-agnostic, and compatible
with standard training pipelines. Through extensive experiments on vision,
regression, and language modeling tasks, including out-of-distribution and
domain-shift scenarios, we demonstrate that CLUE achieves superior calibration
quality and competitive predictive performance with respect to state-of-the-art
approaches without imposing significant computational overhead.

</details>


### [53] [X-Factor: Quality Is a Dataset-Intrinsic Property](https://arxiv.org/abs/2505.22813)
*Josiah Couch,Miao Li,Rima Arnaout,Ramy Arnaout*

Main category: cs.LG

TL;DR: In order to optimize machine-learning classifiers, researchers have found that dataset quality is an intrinsic property of datasets independent of dataset size and class balance and of model architecture. It correlates strongly by subset across architectures.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to investigate whether dataset quality is a joint property of the dataset and the model architecture or an intrinsic property of the dataset itself.

Method: The authors create thousands of datasets, each controlled for size and class balance, and use them to train classifiers with a wide range of architectures, from random forests and support-vector machines to deep networks.

Result: Classifier performance correlates strongly by subset across architectures ($R^2=0.79$), supporting quality as an intrinsic property of datasets independent of dataset size and class balance and of model architecture.

Conclusion: Dataset quality appears to be an emergent property of something more fundamental: the quality of datasets' constituent classes.

Abstract: In the universal quest to optimize machine-learning classifiers, three
factors -- model architecture, dataset size, and class balance -- have been
shown to influence test-time performance but do not fully account for it.
Previously, evidence was presented for an additional factor that can be
referred to as dataset quality, but it was unclear whether this was actually a
joint property of the dataset and the model architecture, or an intrinsic
property of the dataset itself. If quality is truly dataset-intrinsic and
independent of model architecture, dataset size, and class balance, then the
same datasets should perform better (or worse) regardless of these other
factors. To test this hypothesis, here we create thousands of datasets, each
controlled for size and class balance, and use them to train classifiers with a
wide range of architectures, from random forests and support-vector machines to
deep networks. We find that classifier performance correlates strongly by
subset across architectures ($R^2=0.79$), supporting quality as an intrinsic
property of datasets independent of dataset size and class balance and of model
architecture. Digging deeper, we find that dataset quality appears to be an
emergent property of something more fundamental: the quality of datasets'
constituent classes. Thus, quality joins size, class balance, and model
architecture as an independent correlate of performance and a separate target
for optimizing machine-learning-based classification.

</details>


### [54] [Preference Learning with Response Time](https://arxiv.org/abs/2505.22820)
*Ayush Sawarni,Sahasrajit Sarmasarkar,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: This paper investigates integrating response time data into human preference learning frameworks using the Evidence Accumulation Drift Diffusion (EZ) model. Novel Neyman-orthogonal loss functions are developed for improved reward model learning, with theoretical analysis and experiments demonstrating significant improvements in sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Binary preference data is widely used in fine-tuning models, but valuable temporal information in user decision-making has not been fully utilized.

Method: The authors propose methodologies to incorporate response time information alongside binary choice data using the EZ model. They develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning.

Result: For linear reward functions, conventional methods have error rates scaling exponentially with reward magnitude, while the proposed approach reduces this to polynomial scaling. Theoretical guarantees are extended to non-parametric reward function spaces, improving sample efficiency.

Conclusion: The integration of response time data significantly enhances the effectiveness of reward model elicitation, as demonstrated through theoretical analysis and experiments on preference learning over images.

Abstract: This paper investigates the integration of response time data into human
preference learning frameworks for more effective reward model elicitation.
While binary preference data has become fundamental in fine-tuning foundation
models, generative AI systems, and other large-scale models, the valuable
temporal information inherent in user decision-making remains largely
unexploited. We propose novel methodologies to incorporate response time
information alongside binary choice data, leveraging the Evidence Accumulation
Drift Diffusion (EZ) model, under which response time is informative of the
preference strength. We develop Neyman-orthogonal loss functions that achieve
oracle convergence rates for reward model learning, matching the theoretical
optimal rates that would be attained if the expected response times for each
query were known a priori. Our theoretical analysis demonstrates that for
linear reward functions, conventional preference learning suffers from error
rates that scale exponentially with reward magnitude. In contrast, our response
time-augmented approach reduces this to polynomial scaling, representing a
significant improvement in sample efficiency. We extend these guarantees to
non-parametric reward function spaces, establishing convergence properties for
more complex, realistic reward models. Our extensive experiments validate our
theoretical findings in the context of preference learning over images.

</details>


### [55] [PGLearn -- An Open-Source Learning Toolkit for Optimal Power Flow](https://arxiv.org/abs/2505.22825)
*Michael Klamkin,Mathieu Tanneau,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: The paper introduces PGLearn, a suite of standardized datasets and evaluation tools for ML in Optimal Power Flow (OPF) problems. It includes representative datasets with global and local variability, time series data, and supports multiple OPF formulations. The aim is to democratize access, reduce data generation burden, and enable fair comparison of methodologies.


<details>
  <summary>Details</summary>
Motivation: There is a lack of standardized datasets and evaluation metrics for machine learning techniques applied to Optimal Power Flow problems, hindering progress in the field.

Method: PGLearn provides comprehensive datasets that capture real-life operating conditions, including global and local variability, and time series data for large-scale systems. It also supports various OPF formulations and offers a toolkit for training, evaluating, and benchmarking ML models.

Result: Standardized datasets are publicly available, reducing the burden of data generation and enabling fair comparisons among different methodologies. PGLearn promotes open research and innovation in ML for OPF.

Conclusion: PGLearn aims to accelerate research and innovation in machine learning applications for Optimal Power Flow by providing open, standardized datasets and evaluation metrics.

Abstract: Machine Learning (ML) techniques for Optimal Power Flow (OPF) problems have
recently garnered significant attention, reflecting a broader trend of
leveraging ML to approximate and/or accelerate the resolution of complex
optimization problems. These developments are necessitated by the increased
volatility and scale in energy production for modern and future grids. However,
progress in ML for OPF is hindered by the lack of standardized datasets and
evaluation metrics, from generating and solving OPF instances, to training and
benchmarking machine learning models. To address this challenge, this paper
introduces PGLearn, a comprehensive suite of standardized datasets and
evaluation tools for ML and OPF. PGLearn provides datasets that are
representative of real-life operating conditions, by explicitly capturing both
global and local variability in the data generation, and by, for the first
time, including time series data for several large-scale systems. In addition,
it supports multiple OPF formulations, including AC, DC, and second-order cone
formulations. Standardized datasets are made publicly available to democratize
access to this field, reduce the burden of data generation, and enable the fair
comparison of various methodologies. PGLearn also includes a robust toolkit for
training, evaluating, and benchmarking machine learning models for OPF, with
the goal of standardizing performance evaluation across the field. By promoting
open, standardized datasets and evaluation metrics, PGLearn aims at
democratizing and accelerating research and innovation in machine learning
applications for optimal power flow problems. Datasets are available for
download at https://www.huggingface.co/PGLearn.

</details>


### [56] [MCP Safety Training: Learning to Refuse Falsely Benign MCP Exploits using Improved Preference Alignment](https://arxiv.org/abs/2505.23634)
*John Halloran*

Main category: cs.LG

TL;DR: The paper shows that MCP-based attacks are broader than previously thought and introduces a new dataset of FBAs to explore DPO's effectiveness for refusal training of LLMs. It also introduces RAG-Pref which significantly improves LLMs' ability to refuse FBAs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the susceptibility of MCP to retrieval-based 'falsely benign' attacks (FBAs) and improve alignment guardrails against such attacks.

Method: The method involves introducing a new MCP dataset of FBAs and benign samples to explore DPO's effectiveness for refusal training of LLMs. Additionally, RAG-Pref, a novel preference alignment strategy based on RAG, is introduced to further improve FBA refusals.

Result: DPO improves model guardrails against FBAs, but the efficacy varies depending on the model's original post-training alignment scheme. RAG-Pref significantly improves LLMs' ability to refuse FBAs, especially when combined with DPO alignment.

Conclusion: RAG-Pref drastically improves guardrails against MCP-based attacks and enhances the ability of LLMs to refuse FBAs.

Abstract: The model context protocol (MCP) has been widely adapted as an open standard
enabling the seamless integration of generative AI agents. However, recent work
has shown the MCP is susceptible to retrieval-based "falsely benign" attacks
(FBAs), allowing malicious system access and credential theft, but requiring
that users download compromised files directly to their systems. Herein, we
show that the threat model of MCP-based attacks is significantly broader than
previously thought, i.e., attackers need only post malicious content online to
deceive MCP agents into carrying out their attacks on unsuspecting victims'
systems.
  To improve alignment guardrails against such attacks, we introduce a new MCP
dataset of FBAs and (truly) benign samples to explore the effectiveness of
direct preference optimization (DPO) for the refusal training of large language
models (LLMs). While DPO improves model guardrails against such attacks, we
show that the efficacy of refusal learning varies drastically depending on the
model's original post-training alignment scheme--e.g., GRPO-based LLMs learn to
refuse extremely poorly. Thus, to further improve FBA refusals, we introduce
Retrieval Augmented Generation for Preference alignment (RAG-Pref), a novel
preference alignment strategy based on RAG. We show that RAG-Pref significantly
improves the ability of LLMs to refuse FBAs, particularly when combined with
DPO alignment, thus drastically improving guardrails against MCP-based attacks.

</details>


### [57] [Bridging Distribution Shift and AI Safety: Conceptual and Methodological Synergies](https://arxiv.org/abs/2505.22829)
*Chenruo Liu,Kenan Tang,Yao Qin,Qi Lei*

Main category: cs.LG

TL;DR: This paper connects distribution shift and AI safety by analyzing their synergies, establishing two types of connections between them for a unified perspective.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between distribution shift and AI safety through comprehensive analysis rather than narrow cases or informal analogies.

Method: Establishing two types of connections: methods addressing specific shifts can help achieve safety goals or certain shifts and safety issues can be formally reduced to each other.

Result: Provides a unified perspective that encourages fundamental integration between distribution shift and AI safety research.

Conclusion: The analysis fosters a deeper understanding and synergy between distribution shift and AI safety, promoting integrated research.

Abstract: This paper bridges distribution shift and AI safety through a comprehensive
analysis of their conceptual and methodological synergies. While prior
discussions often focus on narrow cases or informal analogies, we establish two
types connections between specific causes of distribution shift and
fine-grained AI safety issues: (1) methods addressing a specific shift type can
help achieve corresponding safety goals, or (2) certain shifts and safety
issues can be formally reduced to each other, enabling mutual adaptation of
their methods. Our findings provide a unified perspective that encourages
fundamental integration between distribution shift and AI safety research.

</details>


### [58] [How Do Diffusion Models Improve Adversarial Robustness?](https://arxiv.org/abs/2505.22839)
*Liu Yuezhang,Xue-Xin Wei*

Main category: cs.LG

TL;DR: Diffusion models significantly enhance empirical adversarial robustness, but the precise mechanisms remain unclear. This paper investigates how diffusion models improve adversarial robustness, finding that purified images are heavily influenced by internal randomness and compression effects. The improvement drops to 24% on CIFAR-10 under fixed randomness, and the remaining robustness gain correlates with the model's ability to compress the input space.


<details>
  <summary>Details</summary>
Motivation: Recent findings suggest that diffusion models significantly enhance empirical adversarial robustness, but the exact reasons behind this improvement are still unclear.

Method: Systematically investigate how and how well diffusion models improve adversarial robustness by observing the changes in $\ell_p$ distance to clean samples, evaluating the influence of internal randomness and compression effects, and assessing robustness under fixed randomness.

Result: Diffusion models increase the $\ell_p$ distance to clean samples rather than decrease it. Purified images are heavily influenced by internal randomness and a compression effect arises within each randomness configuration. Robustness improvement drops to approximately 24% on CIFAR-10 under fixed randomness, which is much lower than prior reports approaching 70%. The remaining robustness gain strongly correlates with the model's ability to compress the input space.

Conclusion: The findings provide new insights into the mechanisms underlying diffusion-based purification and offer guidance for developing more effective and principled adversarial purification systems.

Abstract: Recent findings suggest that diffusion models significantly enhance empirical
adversarial robustness. While some intuitive explanations have been proposed,
the precise mechanisms underlying these improvements remain unclear. In this
work, we systematically investigate how and how well diffusion models improve
adversarial robustness. First, we observe that diffusion models intriguingly
increase, rather than decrease, the $\ell_p$ distance to clean
samples--challenging the intuition that purification denoises inputs closer to
the original data. Second, we find that the purified images are heavily
influenced by the internal randomness of diffusion models, where a compression
effect arises within each randomness configuration. Motivated by this
observation, we evaluate robustness under fixed randomness and find that the
improvement drops to approximately 24% on CIFAR-10--substantially lower than
prior reports approaching 70%. Importantly, we show that this remaining
robustness gain strongly correlates with the model's ability to compress the
input space, revealing the compression rate as a reliable robustness indicator
without requiring gradient-based analysis. Our findings provide novel insights
into the mechanisms underlying diffusion-based purification, and offer guidance
for developing more effective and principled adversarial purification systems.

</details>


### [59] [Development and Validation of SXI++ LNM Algorithm for Sepsis Prediction](https://arxiv.org/abs/2505.22840)
*Dharambir Mahto,Prashant Yadav,Mahesh Banavar,Jim Keany,Alan T Joseph,Srinivas Kilambi*

Main category: cs.LG

TL;DR: Sepsis prediction remains a challenge due to non-specific symptoms and complex pathophysiology. The SXI++ LNM, a machine learning scoring system that leverages multiple algorithms and deep neural networks, was developed for sepsis prediction. It outperformed the state of the art in three use cases, achieving an AUC of 0.99.


<details>
  <summary>Details</summary>
Motivation: Sepsis is a life-threatening condition affecting over 48.9 million people globally and causing 11 million deaths annually. Despite medical advancements, predicting sepsis remains a challenge due to non-specific symptoms and complex pathophysiology.

Method: The model utilizes a deep neural network and was trained and tested using multiple scenarios with different dataset distributions. Its performance was assessed against unseen test data, and accuracy, precision, and area under the curve (AUC) were calculated.

Result: The SXI++ LNM outperformed the state of the art in three use cases, achieving an AUC of 0.99 (95% CI: 0.98-1.00), a precision of 99.9% (95% CI: 99.8-100.0), and an accuracy of 99.99% (95% CI: 99.98-100.0).

Conclusion: The SXI++ LNM demonstrated high reliability in sepsis prediction and aims to improve robustness in clinical applications.

Abstract: Sepsis is a life-threatening condition affecting over 48.9 million people
globally and causing 11 million deaths annually. Despite medical advancements,
predicting sepsis remains a challenge due to non-specific symptoms and complex
pathophysiology. The SXI++ LNM is a machine learning scoring system that
refines sepsis prediction by leveraging multiple algorithms and deep neural
networks. This study aims to improve robustness in clinical applications and
evaluates the predictive performance of the SXI++ LNM for sepsis prediction.
The model, utilizing a deep neural network, was trained and tested using
multiple scenarios with different dataset distributions. The model's
performance was assessed against unseen test data, and accuracy, precision, and
area under the curve (AUC) were calculated. THE SXI++ LNM outperformed the
state of the art in three use cases, achieving an AUC of 0.99 (95% CI:
0.98-1.00). The model demonstrated a precision of 99.9% (95% CI: 99.8-100.0)
and an accuracy of 99.99% (95% CI: 99.98-100.0), maintaining high reliability.

</details>


### [60] [Kernel-Smoothed Scores for Denoising Diffusion: A Bias-Variance Study](https://arxiv.org/abs/2505.22841)
*Franck Gabriel,François Ged,Maria Han Veiga,Emmanuel Schertzer*

Main category: cs.LG

TL;DR: Diffusion models, despite being great at generating high-quality samples, can sometimes just memorize the training data. This paper finds that using a smoothed version of the model's score function reduces this memorization and enhances generalization. It also connects this to a density estimation technique and shows it through experiments on synthetic and MNIST datasets.


<details>
  <summary>Details</summary>
Motivation: To address the issue of memorization in diffusion models by analyzing and improving the score function used in these models.

Method: Interpreting the empirical score as a noisy version of the true score, introducing a kernel-smoothed empirical score, and analyzing its bias-variance trade-off. Deriving asymptotic bounds on the Kullback-Leibler divergence and linking reverse diffusion with a Log-Exponential Double-Kernel Density Estimator (LED-KDE).

Result: Regularization on the score function has the same effect as increasing the size of the training dataset, which helps prevent memorization. The two regularization mechanisms are identified and explained.

Conclusion: Kernel-smoothed empirical score improves generalization and mitigates memorization in diffusion models.

Abstract: Diffusion models now set the benchmark in high-fidelity generative sampling,
yet they can, in principle, be prone to memorization. In this case, their
learned score overfits the finite dataset so that the reverse-time SDE samples
are mostly training points. In this paper, we interpret the empirical score as
a noisy version of the true score and show that its covariance matrix is
asymptotically a re-weighted data PCA. In large dimension, the small time limit
makes the noise variance blow up while simultaneously reducing spatial
correlation. To reduce this variance, we introduce a kernel-smoothed empirical
score and analyze its bias-variance trade-off. We derive asymptotic bounds on
the Kullback-Leibler divergence between the true distribution and the one
generated by the modified reverse SDE. Regularization on the score has the same
effect as increasing the size of the training dataset, and thus helps prevent
memorization. A spectral decomposition of the forward diffusion suggests better
variance control under some regularity conditions of the true data
distribution. Reverse diffusion with kernel-smoothed empirical score can be
reformulated as a gradient descent drifted toward a Log-Exponential
Double-Kernel Density Estimator (LED-KDE). This perspective highlights two
regularization mechanisms taking place in denoising diffusions: an initial
Gaussian kernel first diffuses mass isotropically in the ambient space, while a
second kernel applied in score space concentrates and spreads that mass along
the data manifold. Hence, even a straightforward regularization-without any
learning-already mitigates memorization and enhances generalization.
Numerically, we illustrate our results with several experiments on synthetic
and MNIST datasets.

</details>


### [61] [RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation](https://arxiv.org/abs/2505.22846)
*Nikita Khramov,Andrei Kozyrev,Gleb Solovev,Anton Podkopaev*

Main category: cs.LG

TL;DR: Interactive Theorem Proving combined with Generative Artificial Intelligence is beneficial. This paper evaluates different Rocq generation methods, emphasizes the importance of premise selection, proposes a new approach using a self-attentive embedder model, and uses a multi-stage agentic system for writing Rocq proofs.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of Rocq generation and enhance the process of writing Rocq proofs by exploring various approaches and emphasizing the significance of thorough premise selection.

Method: Assess multiple approaches to Rocq generation, propose a novel approach with retrieval via a self-attentive embedder model, use a multi-stage agentic system tailored for formal verification, and conduct an ablation study with multi-agent debate on the planning stage of proof synthesis.

Result: The evaluation shows a relative increase of up to 28% in the generator's performance, and the multi-stage agentic system demonstrates high effectiveness in writing Rocq proofs.

Conclusion: The proposed approach improves Rocq generation performance significantly, and the multi-stage agentic system is highly effective for writing Rocq proofs.

Abstract: Interactive Theorem Proving was repeatedly shown to be fruitful combined with
Generative Artificial Intelligence. This paper assesses multiple approaches to
Rocq generation and illuminates potential avenues for improvement. We highlight
the importance of thorough premise selection for generating Rocq proofs and
propose a novel approach, leveraging retrieval via a self-attentive embedder
model. The evaluation of the designed approach shows up to 28% relative
increase of the generator's performance. We tackle the problem of writing Rocq
proofs using a multi-stage agentic system, tailored for formal verification,
and demonstrate its high effectiveness. We conduct an ablation study and show
the use of multi-agent debate on the planning stage of proof synthesis.

</details>


### [62] [Causal-PIK: Causality-based Physical Reasoning with a Physics-Informed Kernel](https://arxiv.org/abs/2505.22861)
*Carlota Parés-Morlans,Michelle Yi,Claire Chen,Sarah A. Wu,Rika Antonova,Tobias Gerstenberg,Jeannette Bohg*

Main category: cs.LG

TL;DR: In tasks with complex object interactions and unknown dynamics, planning is difficult. Causal-PIK uses Bayesian optimization and a Physics-Informed Kernel to efficiently find the best actions. It performs well on benchmarks, even compared to humans.


<details>
  <summary>Details</summary>
Motivation: Tasks that involve complex interactions between objects with unknown dynamics make planning before execution difficult.

Method: Causal-PIK leverages Bayesian optimization to reason about causal interactions via a Physics-Informed Kernel to help guide efficient search for the best next action.

Result: Experimental results on Virtual Tools and PHYRE physical reasoning benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring fewer actions to reach the goal. It also remains competitive on challenging tasks compared to human problem-solvers.

Conclusion: Causal-PIK is an effective method for iteratively improving actions in tasks with complex interactions and unknown dynamics.

Abstract: Tasks that involve complex interactions between objects with unknown dynamics
make planning before execution difficult. These tasks require agents to
iteratively improve their actions after actively exploring causes and effects
in the environment. For these type of tasks, we propose Causal-PIK, a method
that leverages Bayesian optimization to reason about causal interactions via a
Physics-Informed Kernel to help guide efficient search for the best next
action. Experimental results on Virtual Tools and PHYRE physical reasoning
benchmarks show that Causal-PIK outperforms state-of-the-art results, requiring
fewer actions to reach the goal. We also compare Causal-PIK to human studies,
including results from a new user study we conducted on the PHYRE benchmark. We
find that Causal-PIK remains competitive on tasks that are very challenging,
even for human problem-solvers.

</details>


### [63] [Scaling Offline RL via Efficient and Expressive Shortcut Models](https://arxiv.org/abs/2505.22866)
*Nicolas Espinosa-Dice,Yiyi Zhang,Yiding Chen,Bradley Guo,Owen Oertell,Gokul Swamy,Kiante Brantley,Wen Sun*

Main category: cs.LG

TL;DR: 本论文提出了一种新的离线强化学习算法SORL，通过利用快捷模型和Q函数验证器，实现了训练和推理的高效扩展，并在多个离线RL任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型虽然强大，但在离线强化学习中的应用受到噪声采样过程迭代性的限制，导致策略优化困难。

Method: 引入SORL算法，利用快捷模型进行训练和推理扩展，采用单阶段训练策略捕捉复杂数据分布，并使用学习到的Q函数作为验证器进行顺序和平行推理扩展。

Result: SORL在一系列离线RL任务中表现出强大的性能，并随着测试时计算量的增加展现出积极的扩展行为。

Conclusion: SORL通过有效的训练和推理扩展机制，在离线强化学习任务中取得了显著成果，证明了其在处理复杂数据分布方面的优势。

Abstract: Diffusion and flow models have emerged as powerful generative approaches
capable of modeling diverse and multimodal behavior. However, applying these
models to offline reinforcement learning (RL) remains challenging due to the
iterative nature of their noise sampling processes, making policy optimization
difficult. In this paper, we introduce Scalable Offline Reinforcement Learning
(SORL), a new offline RL algorithm that leverages shortcut models - a novel
class of generative models - to scale both training and inference. SORL's
policy can capture complex data distributions and can be trained simply and
efficiently in a one-stage training procedure. At test time, SORL introduces
both sequential and parallel inference scaling by using the learned Q-function
as a verifier. We demonstrate that SORL achieves strong performance across a
range of offline RL tasks and exhibits positive scaling behavior with increased
test-time compute. We release the code at
nico-espinosadice.github.io/projects/sorl.

</details>


### [64] [Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints](https://arxiv.org/abs/2505.22881)
*Hyungki Im,Wyame Benslimane,Paul Grigas*

Main category: cs.LG

TL;DR: The paper studies an extension of contextual stochastic linear optimization with uncertain inequality constraints predicted by machine learning, introducing SPO-RC loss and its convex surrogate SPO-RC+, demonstrating their effectiveness in handling constraint uncertainty.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing literature on contextual stochastic linear optimization that does not consider uncertain inequality constraints predicted by machine learning models.

Method: Using contextual uncertainty sets constructed via conformal prediction to handle constraint uncertainty, introducing SPO-RC loss and its convex surrogate SPO-RC+, training on truncated datasets with importance reweighting techniques to correct sample selection bias.

Result: Through experiments on fractional knapsack and alloy production problem instances, SPO-RC+ effectively handles uncertainty in constraints and combining truncation with importance reweighting improves performance.

Conclusion: The introduced SPO-RC+ method effectively deals with constraint uncertainty in contextual stochastic linear optimization problems, and the combination of truncation and importance reweighting enhances performance.

Abstract: We study an extension of contextual stochastic linear optimization (CSLO)
that, in contrast to most of the existing literature, involves inequality
constraints that depend on uncertain parameters predicted by a machine learning
model. To handle the constraint uncertainty, we use contextual uncertainty sets
constructed via methods like conformal prediction. Given a contextual
uncertainty set method, we introduce the "Smart Predict-then-Optimize with
Robust Constraints" (SPO-RC) loss, a feasibility-sensitive adaptation of the
SPO loss that measures decision error of predicted objective parameters. We
also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with
SPO-RC. To enhance performance, we train on truncated datasets where true
constraint parameters lie within the uncertainty sets, and we correct the
induced sample selection bias using importance reweighting techniques. Through
experiments on fractional knapsack and alloy production problem instances, we
demonstrate that SPO-RC+ effectively handles uncertainty in constraints and
that combining truncation with importance reweighting can further improve
performance.

</details>


### [65] [On the Dynamic Regret of Following the Regularized Leader: Optimism with History Pruning](https://arxiv.org/abs/2505.22899)
*Naram Mhaisen,George Iosifidis*

Main category: cs.LG

TL;DR: 通过重新分析FTRL框架，提出了实现动态后悔保证的新方法，并解释了之前方法的局限性以及改进的关键。


<details>
  <summary>Details</summary>
Motivation: 现有的FTRL框架在动态环境中的表现有限，因为它倾向于产生“懒惰”的迭代，无法很好地适应变化的成本函数。因此，需要一种新方法来克服这一限制，实现更好的动态后悔保证。

Method: 利用乐观组合未来成本和仔细线性化过去成本的方法，使FTRL能够生成“敏捷”迭代，从而恢复已知的动态后悔界限。此外，还通过修剪不必要的历史信息，将算法状态与迭代同步，以避免状态无限制增长。

Result: 该方法提供了一种原则性的插值方式，在贪婪和敏捷更新之间进行权衡，并带来了多项优势，包括更精细地控制后悔项、消除乐观估计中的循环依赖，以及应用类似于AdaFTRL的最小递归正则化。

Conclusion: 研究发现，FTRL框架中阻碍动态后悔的主要因素并非其懒惰投影风格，而是算法状态（线性化历史）与其迭代的解耦。通过修剪技术可以同步这两者，从而改善动态环境下的性能。

Abstract: We revisit the Follow the Regularized Leader (FTRL) framework for Online
Convex Optimization (OCO) over compact sets, focusing on achieving dynamic
regret guarantees. Prior work has highlighted the framework's limitations in
dynamic environments due to its tendency to produce "lazy" iterates. However,
building on insights showing FTRL's ability to produce "agile" iterates, we
show that it can indeed recover known dynamic regret bounds through optimistic
composition of future costs and careful linearization of past costs, which can
lead to pruning some of them. This new analysis of FTRL against dynamic
comparators yields a principled way to interpolate between greedy and agile
updates and offers several benefits, including refined control over regret
terms, optimism without cyclic dependence, and the application of minimal
recursive regularization akin to AdaFTRL. More broadly, we show that it is not
the lazy projection style of FTRL that hinders (optimistic) dynamic regret, but
the decoupling of the algorithm's state (linearized history) from its iterates,
allowing the state to grow arbitrarily. Instead, pruning synchronizes these two
when necessary.

</details>


### [66] [Defining Foundation Models for Computational Science: A Call for Clarity and Rigor](https://arxiv.org/abs/2505.22904)
*Youngsoo Choi,Siu Wun Cheung,Youngkyu Kim,Ping-Hsuan Tsai,Alejandro N. Diaz,Ivan Zanardi,Seung Whan Chung,Dylan Matthew Copeland,Coleman Kendrick,William Anderson,Traian Iliescu,Matthias Heinkenschloss*

Main category: cs.LG

TL;DR: The paper proposes a formal definition of foundation models in computational science and introduces DD-FEM as a framework to address key challenges.


<details>
  <summary>Details</summary>
Motivation: The term 'foundation model' is increasingly used without a universally accepted definition in computational science, which may cause confusion and dilute its scientific meaning.

Method: Propose a formal definition of foundation models based on generality, reusability, and scalability. Introduce DD-FEM that combines classical FEM with data-driven learning.

Result: DD-FEM addresses challenges such as scalability, adaptability, and physics consistency in foundation models for computational science.

Conclusion: This work bridges traditional numerical methods with modern AI paradigms, providing a rigorous foundation for evaluating and developing future foundation models.

Abstract: The widespread success of foundation models in natural language processing
and computer vision has inspired researchers to extend the concept to
scientific machine learning and computational science. However, this position
paper argues that as the term "foundation model" is an evolving concept, its
application in computational science is increasingly used without a universally
accepted definition, potentially creating confusion and diluting its precise
scientific meaning. In this paper, we address this gap by proposing a formal
definition of foundation models in computational science, grounded in the core
values of generality, reusability, and scalability. We articulate a set of
essential and desirable characteristics that such models must exhibit, drawing
parallels with traditional foundational methods, like the finite element and
finite volume methods. Furthermore, we introduce the Data-Driven Finite Element
Method (DD-FEM), a framework that fuses the modular structure of classical FEM
with the representational power of data-driven learning. We demonstrate how
DD-FEM addresses many of the key challenges in realizing foundation models for
computational science, including scalability, adaptability, and physics
consistency. By bridging traditional numerical methods with modern AI
paradigms, this work provides a rigorous foundation for evaluating and
developing novel approaches toward future foundation models in computational
science.

</details>


### [67] [Mustafar: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference](https://arxiv.org/abs/2505.22913)
*Donghyeon Joo,Helya Hosseini,Ramyad Hadidi,Bahar Asgari*

Main category: cs.LG

TL;DR: 通过无结构稀疏性显著提高了LLM的KV缓存压缩，支持高达70%的稀疏度而不影响准确性或无需微调。我们探索了剪枝策略，发现基于每token大小的剪枝在无结构稀疏性下对Key和Value缓存都高度有效。使用位图格式和自定义注意力内核可以将KV缓存压缩到密集推理的45%，使上下文长度更长，吞吐量提高2.23倍。相关代码可在https://github.com/dhjoo98/mustafar获取。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLM）中的KV缓存存在高内存开销问题，尤其是在处理长上下文时，这对解码性能构成了主要瓶颈。因此需要一种新的方法来减少KV缓存的大小，同时不损失精度或增加额外计算负担。

Method: 研究者系统地探索了不同的剪枝策略，并发现基于每token大小的剪枝方法在无结构稀疏性条件下非常有效。对于Key缓存，突出的异常值元素有助于剪枝；而Value缓存则受益于简单的基于大小的剪枝，尽管其分布较为均匀。为了进一步加速解码过程中的内存绑定操作，研究者采用了位图稀疏格式和自定义注意力内核，这些工具可以在运行时进行任意稀疏模式的剪枝和压缩并直接计算。

Result: 这种方法能够实现高达70%的稀疏性，且不会影响模型准确性或要求微调。与密集推理相比，该方法可将KV缓存压缩至45%，从而支持更长的上下文长度，并将吞吐量提高至2.23倍。

Conclusion: 本研究表明无结构稀疏性可以显著提高LLM中KV缓存的压缩效率，同时保持模型性能不变。通过采用适当的剪枝策略和自定义硬件优化，可以显著降低内存开销并提升解码速度，为实际应用提供了更高的效率和更大的上下文支持能力。

Abstract: We demonstrate that unstructured sparsity significantly improves KV cache
compression for LLMs, enabling sparsity levels up to 70% without compromising
accuracy or requiring fine-tuning. We conduct a systematic exploration of
pruning strategies and find per-token magnitude-based pruning as highly
effective for both Key and Value caches under unstructured sparsity, surpassing
prior structured pruning schemes. The Key cache benefits from prominent outlier
elements, while the Value cache surprisingly benefits from a simple
magnitude-based pruning despite its uniform distribution. KV cache size is the
major bottleneck in decode performance due to high memory overhead for large
context lengths. To address this, we use a bitmap-based sparse format and a
custom attention kernel capable of compressing and directly computing over
compressed caches pruned to arbitrary sparsity patterns, significantly
accelerating memory-bound operations in decode computations and thereby
compensating for the overhead of runtime pruning and compression. Our custom
attention kernel coupled with the bitmap-based format delivers substantial
compression of KV cache upto 45% of dense inference and thereby enables longer
context length and increased tokens/sec throughput of upto 2.23x compared to
dense inference. Our pruning mechanism and sparse attention kernel is available
at https://github.com/dhjoo98/mustafar.

</details>


### [68] [Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking](https://arxiv.org/abs/2505.22922)
*Athanasios Glentis,Jiaxiang Li,Qiulin Shang,Andi Han,Ioannis Tsaknakis,Quan Wei,Mingyi Hong*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）虽然强大，但训练和微调需要大量的计算资源。本文探讨了参数或内存高效的方法是否可以在保持与完整模型训练相当的性能的同时提高预训练效率，并尝试缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型的快速增长带来了巨大的计算挑战，尤其是在预训练阶段需要学习庞大的数据集，因此需要寻找更高效的预训练方法。

Method: (1) 对高效预训练的最新方法进行全面调查；(2) 对几种具有代表性的内存高效预训练方法进行基准评估，以评估不同模型尺寸下的性能；(3) 提出了两种实用技术：权重重构和动量重置，以提高高效预训练方法的性能。

Result: 全秩训练在适当优化器和超参数选择下表现出最佳性能；低秩方法中加入高秩更新可以提升其性能；提出的权重重构和动量重置技术应用于低秩方法时，在降低困惑度方面优于GaLore和Fira等流行算法，同时使用的内存减少约25%。

Conclusion: 通过合适的优化器和超参数选择，全秩训练表现最佳，而低秩方法结合高秩更新能有效提升性能。提出的权重重构和动量重置技术可显著提高低秩方法的性能，同时节省内存。

Abstract: Fueled by their remarkable ability to tackle diverse tasks across multiple
domains, large language models (LLMs) have grown at an unprecedented rate, with
some recent models containing trillions of parameters. This growth is
accompanied by substantial computational challenges, particularly regarding the
memory and compute resources required for training and fine-tuning. Numerous
approaches have been explored to address these issues, such as LoRA. While
these methods are effective for fine-tuning, their application to pre-training
is significantly more challenging due to the need to learn vast datasets.
Motivated by this issue, we aim to address the following questions: Can
parameter- or memory-efficient methods enhance pre-training efficiency while
achieving performance comparable to full-model training? How can the
performance gap be narrowed? To this end, the contributions of this work are
the following. (1) We begin by conducting a comprehensive survey that
summarizes state-of-the-art methods for efficient pre-training. (2) We perform
a benchmark evaluation of several representative memory efficient pre-training
approaches to comprehensively evaluate their performance across model sizes. We
observe that with a proper choice of optimizer and hyperparameters, full-rank
training delivers the best performance, as expected. We also notice that
incorporating high-rank updates in low-rank approaches is the key to improving
their performance. (3) Finally, we propose two practical techniques, namely
weight refactorization and momentum reset, to enhance the performance of
efficient pre-training methods. We observe that applying these techniques to
the low-rank method (on a 1B model) can achieve a lower perplexity than popular
memory efficient algorithms such as GaLore and Fira, while simultaneously using
about 25% less memory.

</details>


### [69] [Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models](https://arxiv.org/abs/2505.22935)
*Jipeng Li,Yanning Shen*

Main category: cs.LG

TL;DR: 本研究挑战了显式噪声级调节对于图扩散模型（GDMs）必要性的传统观点，提出并验证了去噪器可以从损坏的图结构中隐式推断噪声水平的可能性。


<details>
  <summary>Details</summary>
Motivation: 显式噪声级调节被认为是图扩散模型有效运行的关键，但其必要性尚未被充分质疑。研究者希望探索是否可以利用图数据本身的信息来隐式推断噪声水平，从而简化模型架构并提高效率。

Method: 通过构建以伯努利边翻转破坏为中心的理论框架，并扩展到包含更复杂的结构-属性噪声场景，研究者评估了无条件GDMs在合成和真实图数据集上的表现，使用GDSS和DiGress等模型进行实验。

Result: 无条件GDMs在性能上与有条件版本相当甚至更优，同时减少了4-6%的参数和8-10%的计算时间，表明图数据的高维特性本身就包含了足够的去噪信息。

Conclusion: 图数据本身通常包含足够的信息以支持去噪过程，因此可以通过消除显式噪声级调节来设计更简单、更高效的GDM架构。

Abstract: Explicit noise-level conditioning is widely regarded as essential for the
effective operation of Graph Diffusion Models (GDMs). In this work, we
challenge this assumption by investigating whether denoisers can implicitly
infer noise levels directly from corrupted graph structures, potentially
eliminating the need for explicit noise conditioning. To this end, we develop a
theoretical framework centered on Bernoulli edge-flip corruptions and extend it
to encompass more complex scenarios involving coupled structure-attribute
noise. Extensive empirical evaluations on both synthetic and real-world graph
datasets, using models such as GDSS and DiGress, provide strong support for our
theoretical findings. Notably, unconditional GDMs achieve performance
comparable or superior to their conditioned counterparts, while also offering
reductions in parameters (4-6%) and computation time (8-10%). Our results
suggest that the high-dimensional nature of graph data itself often encodes
sufficient information for the denoising process, opening avenues for simpler,
more efficient GDM architectures.

</details>


### [70] [Directed Graph Grammars for Sequence-based Learning](https://arxiv.org/abs/2505.22949)
*Michael Sun,Orion Foo,Gang Liu,Wojciech Matusik,Jie Chen*

Main category: cs.LG

TL;DR: An unambiguous grammar-based method is proposed to create a compact and equivalent sequential representation of DAGs.


<details>
  <summary>Details</summary>
Motivation: Existing encoders for DAGs are abundant, but decoding them in a principled way remains challenging due to the many different topological orders of DAG nodes.

Method: A grammar-based approach constructs a principled, compact and equivalent sequential representation of a DAG by viewing a graph as derivations over an unambiguous grammar.

Result: Such a representation can be used for building a generative model for graph generation, learning a latent space for property prediction, and leveraging the sequence representational continuity for Bayesian Optimization over structured data.

Conclusion: The proposed method provides a principled way to decode DAGs and has multiple applications in graph-related tasks.

Abstract: Directed acyclic graphs (DAGs) are a class of graphs commonly used in
practice, with examples that include electronic circuits, Bayesian networks,
and neural architectures. While many effective encoders exist for DAGs, it
remains challenging to decode them in a principled manner, because the nodes of
a DAG can have many different topological orders. In this work, we propose a
grammar-based approach to constructing a principled, compact and equivalent
sequential representation of a DAG. Specifically, we view a graph as
derivations over an unambiguous grammar, where the DAG corresponds to a unique
sequence of production rules. Equivalently, the procedure to construct such a
description can be viewed as a lossless compression of the data. Such a
representation has many uses, including building a generative model for graph
generation, learning a latent space for property prediction, and leveraging the
sequence representational continuity for Bayesian Optimization over structured
data. Code is available at https://github.com/shiningsunnyday/induction.

</details>


### [71] [MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming](https://arxiv.org/abs/2505.22967)
*Chengqi Zheng,Jianda Chen,Yueming Lyu,Wen Zheng Terence Ng,Haopeng Zhang,Yew-Soon Ong,Ivor Tsang,Haiyan Yin*

Main category: cs.LG

TL;DR: MermaidFlow is a framework that uses safety-constrained graph evolution to generate robust and interpretable workflows, achieving higher success rates and faster convergence in agent reasoning tasks without altering task settings or evaluations.


<details>
  <summary>Details</summary>
Motivation: Existing workflow generation methods often create fragile and unexecutable plans due to unconstrained LLM-driven construction, prompting the need for a safer and more structured approach.

Method: MermaidFlow represents workflows using Mermaid's verifiable intermediate graph representation and applies domain-aware evolutionary operators (crossover, mutation, insertion, deletion) to ensure semantic correctness and promote structural diversity.

Result: MermaidFlow consistently improves success rates and accelerates convergence to executable plans on the agent reasoning benchmark.

Conclusion: Safety-constrained graph evolution provides a scalable and modular foundation for building robust and interpretable agentic reasoning systems.

Abstract: Despite the promise of autonomous agentic reasoning, existing workflow
generation methods frequently produce fragile, unexecutable plans due to
unconstrained LLM-driven construction. We introduce MermaidFlow, a framework
that redefines the agentic search space through safety-constrained graph
evolution. At its core, MermaidFlow represent workflows as a verifiable
intermediate representation using Mermaid, a structured and human-interpretable
graph language. We formulate domain-aware evolutionary operators, i.e.,
crossover, mutation, insertion, and deletion, to preserve semantic correctness
while promoting structural diversity, enabling efficient exploration of a
high-quality, statically verifiable workflow space. Without modifying task
settings or evaluation protocols, MermaidFlow achieves consistent improvements
in success rates and faster convergence to executable plans on the agent
reasoning benchmark. The experimental results demonstrate that
safety-constrained graph evolution offers a scalable, modular foundation for
robust and interpretable agentic reasoning systems.

</details>


### [72] [EquiReg: Equivariance Regularized Diffusion for Inverse Problems](https://arxiv.org/abs/2505.22973)
*Bahareh Tolooshams,Aditi Chandrashekar,Rayhan Zirvi,Abbas Mammadov,Jiachen Yao,Chuwei Wang,Anima Anandkumar*

Main category: cs.LG

TL;DR: In this paper, the authors propose EquiReg diffusion which is a general framework for regularizing posterior sampling in diffusion-based inverse problem solvers. It outperforms state-of-the-art models in various image restoration tasks.


<details>
  <summary>Details</summary>
Motivation: The current methods of solving inverse problems using diffusion models rely on isotropic Gaussian approximations due to the intractability of the likelihood term. This leads to deviations from the data manifold and inconsistent, unstable reconstructions.

Method: The authors propose Equivariance Regularized (EquiReg) diffusion. They define a new distribution-dependent equivariance error, identify functions with low error for on-manifold samples and higher error for off-manifold samples, and use these functions to regularize the diffusion sampling process by reweighting diffusion trajectories and penalizing those that deviate from the data manifold.

Result: EquiReg enhances reconstructions and outperforms state-of-the-art diffusion models in both linear and nonlinear image restoration tasks, as well as in reconstructing partial differential equations.

Conclusion: EquiReg diffusion provides a better solution for inverse problems by regularizing posterior sampling in diffusion-based solvers.

Abstract: Diffusion models represent the state-of-the-art for solving inverse problems
such as image restoration tasks. In the Bayesian framework, diffusion-based
inverse solvers incorporate a likelihood term to guide the prior sampling
process, generating data consistent with the posterior distribution. However,
due to the intractability of the likelihood term, many current methods rely on
isotropic Gaussian approximations, which lead to deviations from the data
manifold and result in inconsistent, unstable reconstructions. We propose
Equivariance Regularized (EquiReg) diffusion, a general framework for
regularizing posterior sampling in diffusion-based inverse problem solvers.
EquiReg enhances reconstructions by reweighting diffusion trajectories and
penalizing those that deviate from the data manifold. We define a new
distribution-dependent equivariance error, empirically identify functions that
exhibit low error for on-manifold samples and higher error for off-manifold
samples, and leverage these functions to regularize the diffusion sampling
process. When applied to a variety of solvers, EquiReg outperforms
state-of-the-art diffusion models in both linear and nonlinear image
restoration tasks, as well as in reconstructing partial differential equations.

</details>


### [73] [A Computational Approach to Improving Fairness in K-means Clustering](https://arxiv.org/abs/2505.22984)
*Guancheng Zhou,Haiping Xu,Hongkang Xu,Chenyu Li,Donghui Yan*

Main category: cs.LG

TL;DR: The paper addresses the fairness issue in K-means clustering by proposing a two-stage optimization method and two efficient algorithms to adjust cluster membership, which improves fairness with minimal effect on clustering quality.


<details>
  <summary>Details</summary>
Motivation: The K-means clustering algorithm may lead to clusters with disproportionate representation of subpopulations based on sensitive variables like gender or race, causing potential bias and social consequences.

Method: A two-stage optimization formulation is used where clustering is performed first, followed by adjusting the cluster membership of a small subset of selected data points. Two algorithms are proposed to identify data points expensive for fairness: one focuses on nearest data points outside a cluster, and the other on highly 'mixed' data points.

Result: Experiments on benchmark datasets demonstrate significant improvement in fairness with minimal impact on clustering quality. The algorithms can be extended to various clustering algorithms or fairness metrics.

Conclusion: The proposed methods effectively improve the fairness of K-means clustering while maintaining clustering quality, and they are adaptable to other clustering algorithms and fairness metrics.

Abstract: The popular K-means clustering algorithm potentially suffers from a major
weakness for further analysis or interpretation. Some cluster may have
disproportionately more (or fewer) points from one of the subpopulations in
terms of some sensitive variable, e.g., gender or race. Such a fairness issue
may cause bias and unexpected social consequences. This work attempts to
improve the fairness of K-means clustering with a two-stage optimization
formulation--clustering first and then adjust cluster membership of a small
subset of selected data points. Two computationally efficient algorithms are
proposed in identifying those data points that are expensive for fairness, with
one focusing on nearest data points outside of a cluster and the other on
highly 'mixed' data points. Experiments on benchmark datasets show substantial
improvement on fairness with a minimal impact to clustering quality. The
proposed algorithms can be easily extended to a broad class of clustering
algorithms or fairness metrics.

</details>


### [74] [Knowledge Distillation for Reservoir-based Classifier: Human Activity Recognition](https://arxiv.org/abs/2505.22985)
*Masaharu Kagiyama,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: This paper introduces PatchEchoClassifier, a new model using Echo State Network for human activity recognition that is energy-efficient and achieves over 80% accuracy with significantly reduced computational cost compared to DeepConvLSTM.


<details>
  <summary>Details</summary>
Motivation: To develop an energy-efficient classifier for time-series data suitable for human activity recognition in edge computing environments.

Method: Introduced PatchEchoClassifier which uses a reservoir-based mechanism (Echo State Network) along with a tokenizer for patch-level representations. A knowledge distillation framework was proposed to efficiently train the model by transferring knowledge from a high-capacity MLP-Mixer teacher model.

Result: Achieved over 80 percent accuracy on multiple HAR datasets while reducing computational cost significantly, requiring only about one-sixth of the floating point operations (FLOPS) compared to DeepConvLSTM.

Conclusion: PatchEchoClassifier is a promising solution for real-time and energy-efficient human activity recognition in edge computing environments.

Abstract: This paper aims to develop an energy-efficient classifier for time-series
data by introducing PatchEchoClassifier, a novel model that leverages a
reservoir-based mechanism known as the Echo State Network (ESN). The model is
designed for human activity recognition (HAR) using one-dimensional sensor
signals and incorporates a tokenizer to extract patch-level representations. To
train the model efficiently, we propose a knowledge distillation framework that
transfers knowledge from a high-capacity MLP-Mixer teacher to the lightweight
reservoir-based student model. Experimental evaluations on multiple HAR
datasets demonstrate that our model achieves over 80 percent accuracy while
significantly reducing computational cost. Notably, PatchEchoClassifier
requires only about one-sixth of the floating point operations (FLOPS) compared
to DeepConvLSTM, a widely used convolutional baseline. These results suggest
that PatchEchoClassifier is a promising solution for real-time and
energy-efficient human activity recognition in edge computing environments.

</details>


### [75] [Model-Preserving Adaptive Rounding](https://arxiv.org/abs/2505.22988)
*Albert Tseng,Zhaofeng Sun,Christopher De Sa*

Main category: cs.LG

TL;DR: An adaptive rounding algorithm named YAQA is introduced to improve post-training quantization (PTQ) for large language models (LLMs). It uses Kronecker-factored approximations of each linear layer's Hessian with respect to the full model KL divergence, achieving a 30% reduction in KL divergence and state-of-the-art performance on downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitation of current PTQ algorithms that independently minimize the immediate activation error without considering the effect of subsequent layers. This localized objective does not necessarily lead to a compressed model whose output distribution is close to the original model's.

Method: YAQA consists of two components: Kronecker-factored sketches of the full layerwise Hessian that can be computed for large LLMs and a quantizer-independent rounding algorithm that uses these sketches with theoretical guarantees.

Result: Empirically, YAQA reduces the KL divergence to the original model by approximately 30% while achieving state-of-the-art performance on downstream tasks across a wide range of models and quantizers.

Conclusion: YAQA offers an effective solution to improve PTQ for LLMs by taking into account the full model's behavior rather than just local activation errors.

Abstract: The main goal of post-training quantization (PTQ) is to produced a compressed
model whose output distribution is as close to the original model's as
possible. To do this tractably, almost all LLM PTQ algorithms quantize linear
layers by independently minimizing the immediate activation error. However,
this localized objective ignores the effect of subsequent layers, so reducing
it does not necessarily give a closer model. In this work, we introduce Yet
Another Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses
Kronecker-factored approximations of each linear layer's Hessian with respect
to the \textit{full model} KL divergence. YAQA consists of two components:
Kronecker-factored sketches of the full layerwise Hessian that can be tractably
computed for hundred-billion parameter LLMs, and a quantizer-independent
rounding algorithm that uses these sketches and comes with theoretical
guarantees. Across a wide range of models and quantizers, YAQA empirically
reduces the KL divergence to the original model by $\approx 30\%$ while
achieving state of the art performance on downstream tasks.

</details>


### [76] [Number of Clusters in a Dataset: A Regularized K-means Approach](https://arxiv.org/abs/2505.22991)
*Behzad Kamgar-Parsi,Behrooz Kamgar-Parsi*

Main category: cs.LG

TL;DR: This paper investigates the problem of determining the number of meaningful clusters in unlabeled datasets using regularized k-means algorithms. It derives rigorous bounds for the critical hyperparameter λ under the assumption of ideal clusters, analyzes both additive and multiplicative regularization, and demonstrates the performance of these methods as clusters deviate from ideal conditions.


<details>
  <summary>Details</summary>
Motivation: Finding the correct number of distinct clusters in unlabeled datasets is crucial in many applications. However, there are currently no principled guidelines for setting a value for the critical hyperparameter λ in regularized k-means algorithms.

Method: The authors derive rigorous bounds for λ assuming clusters are ideal (d-dimensional spheres with identical radii), which are close proxies for k-means clusters. They also analyze the k-means algorithm with both additive and multiplicative regularizers to reduce the ambiguity of multiple solutions.

Result: Experiments show that the k-means algorithm with additive regularizer often yields multiple solutions. The consensus among solutions with additive and multiplicative regularizations can reduce this ambiguity in certain cases. Selected experiments demonstrate the performance of the regularized k-means algorithms as clusters deviate from the ideal assumption.

Conclusion: The derived bounds for λ provide a principled approach for setting this critical hyperparameter in regularized k-means algorithms. Analyzing both additive and multiplicative regularizations helps reduce the ambiguity of multiple solutions.

Abstract: Finding the number of meaningful clusters in an unlabeled dataset is
important in many applications. Regularized k-means algorithm is a possible
approach frequently used to find the correct number of distinct clusters in
datasets. The most common formulation of the regularization function is the
additive linear term $\lambda k$, where $k$ is the number of clusters and
$\lambda$ a positive coefficient. Currently, there are no principled guidelines
for setting a value for the critical hyperparameter $\lambda$. In this paper,
we derive rigorous bounds for $\lambda$ assuming clusters are {\em ideal}.
Ideal clusters (defined as $d$-dimensional spheres with identical radii) are
close proxies for k-means clusters ($d$-dimensional spherically symmetric
distributions with identical standard deviations). Experiments show that the
k-means algorithm with additive regularizer often yields multiple solutions.
Thus, we also analyze k-means algorithm with multiplicative regularizer. The
consensus among k-means solutions with additive and multiplicative
regularizations reduces the ambiguity of multiple solutions in certain cases.
We also present selected experiments that demonstrate performance of the
regularized k-means algorithms as clusters deviate from the ideal assumption.

</details>


### [77] [Walking the Weight Manifold: a Topological Approach to Conditioning Inspired by Neuromodulation](https://arxiv.org/abs/2505.22994)
*Ari S. Benjamin,Kyle Daruwalla,Christian Pehle,Anthony M. Zador*

Main category: cs.LG

TL;DR: The paper proposes a neuromodulation-inspired method to learn weights as smooth functions of task context variables, optimizing manifolds in weight space for efficient multi-task learning.


<details>
  <summary>Details</summary>
Motivation: To improve multi-task learning efficiency by reusing knowledge across tasks and surpassing traditional conditioning methods that inject context as input.

Method: Optimize smooth manifolds in weight space parameterized by task context variables, using a formal treatment analogous to gradient descent with volumetric movement constraints. During inference, conditioning selects effective weight matrices from these manifolds.

Result: Manifolds with simple topologies (lines, circles, toruses) outperform input concatenation conditioning methods, better generalizing to out-of-distribution samples.

Conclusion: Modulating weights over low-dimensional manifolds provides an effective alternative to traditional conditioning for multi-task learning.

Abstract: One frequently wishes to learn a range of similar tasks as efficiently as
possible, re-using knowledge across tasks. In artificial neural networks, this
is typically accomplished by conditioning a network upon task context by
injecting context as input. Brains have a different strategy: the parameters
themselves are modulated as a function of various neuromodulators such as
serotonin. Here, we take inspiration from neuromodulation and propose to learn
weights which are smoothly parameterized functions of task context variables.
Rather than optimize a weight vector, i.e. a single point in weight space, we
optimize a smooth manifold in weight space with a predefined topology. To
accomplish this, we derive a formal treatment of optimization of manifolds as
the minimization of a loss functional subject to a constraint on volumetric
movement, analogous to gradient descent. During inference, conditioning selects
a single point on this manifold which serves as the effective weight matrix for
a particular sub-task. This strategy for conditioning has two main advantages.
First, the topology of the manifold (whether a line, circle, or torus) is a
convenient lever for inductive biases about the relationship between tasks.
Second, learning in one state smoothly affects the entire manifold, encouraging
generalization across states. To verify this, we train manifolds with several
topologies, including straight lines in weight space (for conditioning on e.g.
noise level in input data) and ellipses (for rotated images). Despite their
simplicity, these parameterizations outperform conditioning identical networks
by input concatenation and better generalize to out-of-distribution samples.
These results suggest that modulating weights over low-dimensional manifolds
offers a principled and effective alternative to traditional conditioning.

</details>


### [78] [LLM Agents for Bargaining with Utility-based Feedback](https://arxiv.org/abs/2505.22998)
*Jihwan Oh,Murad Aghazada,Se-Young Yun,Taehyeon Kim*

Main category: cs.LG

TL;DR: Bargaining poses challenges for LLMs due to limited strategic depth and adaptation. This paper introduces BargainArena, a benchmark dataset with six complex scenarios, human-aligned evaluation metrics, and a structured feedback mechanism to improve LLMs' bargaining strategies through opponent-aware reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance LLM capabilities in realistic bargaining by addressing limitations in strategic depth and adaptation to complex human factors.

Method: Introduce BargainArena, a benchmark dataset with six intricate scenarios; develop human-aligned, economically-grounded evaluation metrics incorporating agent utility and negotiation power; implement a structured feedback mechanism enabling iterative refinement of bargaining strategies.

Result: LLMs often exhibit negotiation strategies misaligned with human preferences, but the structured feedback mechanism significantly improves their performance, promoting deeper strategic and opponent-aware reasoning.

Conclusion: The comprehensive framework centered on utility-based feedback enhances LLMs' capabilities in realistic bargaining scenarios.

Abstract: Bargaining, a critical aspect of real-world interactions, presents challenges
for large language models (LLMs) due to limitations in strategic depth and
adaptation to complex human factors. Existing benchmarks often fail to capture
this real-world complexity. To address this and enhance LLM capabilities in
realistic bargaining, we introduce a comprehensive framework centered on
utility-based feedback. Our contributions are threefold: (1) BargainArena, a
novel benchmark dataset with six intricate scenarios (e.g., deceptive
practices, monopolies) to facilitate diverse strategy modeling; (2)
human-aligned, economically-grounded evaluation metrics inspired by utility
theory, incorporating agent utility and negotiation power, which implicitly
reflect and promote opponent-aware reasoning (OAR); and (3) a structured
feedback mechanism enabling LLMs to iteratively refine their bargaining
strategies. This mechanism can positively collaborate with in-context learning
(ICL) prompts, including those explicitly designed to foster OAR. Experimental
results show that LLMs often exhibit negotiation strategies misaligned with
human preferences, and that our structured feedback mechanism significantly
improves their performance, yielding deeper strategic and opponent-aware
reasoning.

</details>


### [79] [Hybrid Cross-domain Robust Reinforcement Learning](https://arxiv.org/abs/2505.23003)
*Linh Le Pham Van,Minh Hoang Nguyen,Hung Le,Hung The Tran,Sunil Gupta*

Main category: cs.LG

TL;DR: HYDRO is a new framework that uses an online simulator to enhance limited offline datasets in robust RL, minimizing performance gaps and improving sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing robust RL methods often need extensive offline data which can be costly and time-consuming. Using simulators can be faster and cheaper but suffers from dynamics mismatch.

Method: HYDRO measures and minimizes performance gaps between the simulator and worst-case models via uncertainty filtering and prioritized sampling.

Result: HYDRO shows superior performance over existing methods across various tasks, highlighting its potential for improved sample efficiency in offline robust RL.

Conclusion: HYDRO addresses the challenges of robust RL by utilizing an online simulator to complement limited offline data, demonstrating its effectiveness in enhancing sample efficiency.

Abstract: Robust reinforcement learning (RL) aims to learn policies that remain
effective despite uncertainties in its environment, which frequently arise in
real-world applications due to variations in environment dynamics. The robust
RL methods learn a robust policy by maximizing value under the worst-case
models within a predefined uncertainty set. Offline robust RL algorithms are
particularly promising in scenarios where only a fixed dataset is available and
new data cannot be collected. However, these approaches often require extensive
offline data, and gathering such datasets for specific tasks in specific
environments can be both costly and time-consuming. Using an imperfect
simulator offers a faster, cheaper, and safer way to collect data for training,
but it can suffer from dynamics mismatch. In this paper, we introduce HYDRO,
the first Hybrid Cross-Domain Robust RL framework designed to address these
challenges. HYDRO utilizes an online simulator to complement the limited amount
of offline datasets in the non-trivial context of robust RL. By measuring and
minimizing performance gaps between the simulator and the worst-case models in
the uncertainty set, HYDRO employs novel uncertainty filtering and prioritized
sampling to select the most relevant and reliable simulator samples. Our
extensive experiments demonstrate HYDRO's superior performance over existing
methods across various tasks, underscoring its potential to improve sample
efficiency in offline robust RL.

</details>


### [80] [QLIP: A Dynamic Quadtree Vision Prior Enhances MLLM Performance Without Retraining](https://arxiv.org/abs/2505.23004)
*Kyle R. Chickering,Bangzheng Li,Muhao Chen*

Main category: cs.LG

TL;DR: 提出QLIP替代CLIP视觉编码器，通过图像四叉树和内容感知补丁化增强多模态大语言模型的视觉理解能力，无需重新训练即可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: CLIP视觉编码器存在局限性，如固定输入分辨率和无法为不同图像生成分离嵌入，替换它通常需要大量计算成本。

Method: 识别出CLIP视觉编码器的中观偏差和插值偏差问题，提出QLIP作为替代方案，采用图像四叉树和内容感知补丁化方法。

Result: 实验结果表明，QLIP在不需重新训练的情况下提升了LLaVA v1.5模型系列的视觉问答准确性，并在挑战性的$V^{\ast}$基准上将详细理解性能提高了13.6%。

Conclusion: QLIP可以无缝集成到现有的多模态大语言模型中，增强粗粒度和细粒度视觉理解能力。

Abstract: Multimodal Large Language Models (MLLMs) encode images into visual tokens,
aligning visual and textual signals within a shared latent space to facilitate
crossmodal representation learning. The CLIP model is a widely adopted
foundational vision language model whose vision encoder has played a critical
role in the development of MLLMs such as LLaVA. However, the CLIP vision
encoder suffers from notable limitations including being constrained to only
handling fixed input resolutions and a failure to produce separated embeddings
for dissimilar images. Replacing the vision encoder of an existing model
typically incurs substantial computational costs because such a change often
necessitates retraining the entire model pipeline.
  In this work, we identify two factors which underlie the limitations of the
CLIP vision encoder: mesoscopic bias and interpolation bias. To address these
issues, we propose QLIP, a drop-in replacement for CLIP that can be seamlessly
integrated with existing MLLMs with only a few lines of code and can enhance
both coarse-grained and fine-grained visual understanding, without re-training.
QLIP is designed around an image quadtree which replaces the standard uniform
grid patches with a novel content aware patchification. Our experimental
results demonstrate that QLIP improves the general visual question answering
accuracy of the LLaVA v1.5 model series across various model sizes--without
requiring retraining or fine-tuning of the full MLLM. Notably, QLIP boosts
detailed understanding performance on the challenging $V^{\ast}$ benchmark by
up to 13.6 percent.

</details>


### [81] [EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic, Expressiveness, and Linguistic Challenges Using Model-as-a-Judge](https://arxiv.org/abs/2505.23009)
*Ruskin Raj Manku,Yuzhi Tang,Xingjian Shi,Mu Li,Alex Smola*

Main category: cs.LG

TL;DR: The paper introduces EmergentTTS-Eval, a comprehensive benchmark for evaluating TTS systems in six challenging scenarios. It automates test-case generation and evaluation with LLMs and LALMs, resulting in 1,645 diverse cases. State-of-the-art TTS systems were evaluated, revealing fine-grained performance differences. The model-as-a-judge approach showed robust assessment and high correlation with human preferences. The evaluation code and dataset are open-sourced.


<details>
  <summary>Details</summary>
Motivation: Existing TTS benchmarks often fail to capture how well models handle nuanced and semantically complex text. Thus, there is a need for a more comprehensive benchmark that can better evaluate TTS systems.

Method: Introduced EmergentTTS-Eval, which covers six challenging TTS scenarios (emotions, paralinguistics, foreign words, syntactic complexity, complex pronunciation, and questions). The framework automates both test-case generation and evaluation using LLMs and LALMs, extending from human-written seed prompts to create 1,645 diverse test cases. A model-as-a-judge approach was employed to assess the speech across multiple dimensions.

Result: Evaluation of state-of-the-art TTS systems demonstrated EmergentTTS-Eval's ability to reveal fine-grained performance differences. The model-as-a-judge approach offered robust TTS assessment and showed a high correlation with human preferences.

Conclusion: EmergentTTS-Eval provides a comprehensive benchmark for TTS systems, capable of revealing detailed performance differences. The model-as-a-judge approach proved effective. Both the evaluation code and dataset have been open-sourced.

Abstract: Text-to-Speech (TTS) benchmarks often fail to capture how well models handle
nuanced and semantically complex text. Building on $\textit{EmergentTTS}$, we
introduce $\textit{EmergentTTS-Eval}$, a comprehensive benchmark covering six
challenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic
complexity, complex pronunciation (e.g. URLs, formulas), and questions.
Crucially, our framework automates both test-case generation and evaluation,
making the benchmark easily extensible. Starting from a small set of
human-written seed prompts, we iteratively extend them using LLMs to target
specific structural, phonetic and prosodic challenges, resulting in 1,645
diverse test cases. Moreover, we employ a model-as-a-judge approach, using a
Large Audio Language Model (LALM) to assess the speech across multiple
dimensions such as expressed emotion, prosodic, intonational, and pronunciation
accuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,
such as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,
demonstrating its ability to reveal fine-grained performance differences.
Results show that the model-as-a-judge approach offers robust TTS assessment
and a high correlation with human preferences. We open source the evaluation
$\href{https://github.com/boson-ai/EmergentTTS-Eval-public}{code}$ and the
$\href{https://huggingface.co/datasets/bosonai/EmergentTTS-Eval}{dataset}$.

</details>


### [82] [Scalable Complexity Control Facilitates Reasoning Ability of LLMs](https://arxiv.org/abs/2505.23013)
*Liangkai Hang,Junjie Yao,Zhiwei Bai,Tianyi Chen,Yang Chen,Rongjie Diao,Hezhou Li,Pengxiao Lin,Zhiwei Wang,Cheng Xu,Zhongwang Zhang,Zhangchen Zhou,Zhiyu Li,Zehao Lin,Kai Chen,Feiyu Xiong,Yaoyu Zhang,Weinan E,Hongkang Yang,Zhi-Qin John Xu*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）的推理能力近年来迅速发展，本文研究了通过调整初始化率和权重衰减系数来控制模型复杂度的方法，以提高LLMs的扩展规律。研究表明，在不同模型大小和数据量的情况下，控制复杂性可以使扩展规律得到一致改善，并且恒定的初始化率比固定的初始化标准差更能加速扩展规律的下降，这为LLMs的持续进步提供了一个有希望的方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）推理能力的快速发展，寻找可以可靠增强其泛化能力的基本方法变得至关重要。因此，本文探索了模型复杂度控制是否可以作为一种有效手段来提升LLMs的性能。

Method: 通过调整初始化率和权重衰减系数来实施模型复杂度控制，比较了在1T tokens上预训练的不同复杂度超参数的2.4B模型的基准性能。研究发现，与固定初始化标准差相比，恒定的初始化率（std的指数）能够使模型和数据规模的扩展规律更快地下降。

Result: 模型复杂度控制在不同模型大小和数据量的情况下，都能有效地改善LLMs的扩展规律。具体来说，采用恒定的初始化率而非固定初始化标准差能更显著地加速扩展规律的下降。

Conclusion: 复杂度控制是提升LLMs性能的一个很有前景的方向，它可以在不同的模型和数据规模下，持续改进LLMs的扩展规律。

Abstract: The reasoning ability of large language models (LLMs) has been rapidly
advancing in recent years, attracting interest in more fundamental approaches
that can reliably enhance their generalizability. This work demonstrates that
model complexity control, conveniently implementable by adjusting the
initialization rate and weight decay coefficient, improves the scaling law of
LLMs consistently over varying model sizes and data sizes. This gain is further
illustrated by comparing the benchmark performance of 2.4B models pretrained on
1T tokens with different complexity hyperparameters. Instead of fixing the
initialization std, we found that a constant initialization rate (the exponent
of std) enables the scaling law to descend faster in both model and data sizes.
These results indicate that complexity control is a promising direction for the
continual advancement of LLMs.

</details>


### [83] [Hyperbolic-PDE GNN: Spectral Graph Neural Networks in the Perspective of A System of Hyperbolic Partial Differential Equations](https://arxiv.org/abs/2505.23014)
*Juwei Yue,Haikuo Li,Jiawei Sheng,Xiaodong Li,Taoyu Su,Tingwen Liu,Li Guo*

Main category: cs.LG

TL;DR: This paper proposes a new paradigm for graph neural networks that formulates message passing as a system of hyperbolic partial differential equations, enhancing the interpretability and performance of spectral GNNs.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs have limitations in learning topological features due to their spatial domain unrelated to the topology.

Method: The method involves formulating message passing as hyperbolic PDEs, creating a dynamical system that maps node representations into a solution space spanned by eigenvectors describing the topological structure of graphs. Polynomials are introduced to approximate arbitrary filter functions.

Result: Extensive experiments show that this paradigm has strong flexibility and significantly improves the performance of various spectral GNNs in different graph tasks.

Conclusion: The proposed approach enhances the interpretability and performance of spectral GNNs through the use of hyperbolic PDEs.

Abstract: Graph neural networks (GNNs) leverage message passing mechanisms to learn the
topological features of graph data. Traditional GNNs learns node features in a
spatial domain unrelated to the topology, which can hardly ensure topological
features. In this paper, we formulates message passing as a system of
hyperbolic partial differential equations (hyperbolic PDEs), constituting a
dynamical system that explicitly maps node representations into a particular
solution space. This solution space is spanned by a set of eigenvectors
describing the topological structure of graphs. Within this system, for any
moment in time, a node features can be decomposed into a superposition of the
basis of eigenvectors. This not only enhances the interpretability of message
passing but also enables the explicit extraction of fundamental characteristics
about the topological structure. Furthermore, by solving this system of
hyperbolic partial differential equations, we establish a connection with
spectral graph neural networks (spectral GNNs), serving as a message passing
enhancement paradigm for spectral GNNs.We further introduce polynomials to
approximate arbitrary filter functions. Extensive experiments demonstrate that
the paradigm of hyperbolic PDEs not only exhibits strong flexibility but also
significantly enhances the performance of various spectral GNNs across diverse
graph tasks.

</details>


### [84] [$K^2$VAE: A Koopman-Kalman Enhanced Variational AutoEncoder for Probabilistic Time Series Forecasting](https://arxiv.org/abs/2505.23017)
*Xingjian Wu,Xiangfei Qiu,Hongfan Gao,Jilin Hu,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: The paper presents $K^2$VAE, a VAE-based generative model that uses KoopmanNet and KalmanNet for efficient long-term probabilistic time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Existing methods for probabilistic time series forecasting are excellent at short-term forecasting but struggle with long-term forecasting due to nonlinear dynamics and error accumulation.

Method: The method introduces $K^2$VAE which leverages KoopmanNet to transform nonlinear time series into a linear dynamical system and KalmanNet to refine predictions and model uncertainty in the linear system.

Result: Extensive experiments show that $K^2$VAE outperforms state-of-the-art methods in both short- and long-term probabilistic time series forecasting.

Conclusion: $K^2$VAE provides a more efficient and accurate solution for probabilistic time series forecasting.

Abstract: Probabilistic Time Series Forecasting (PTSF) plays a crucial role in
decision-making across various fields, including economics, energy, and
transportation. Most existing methods excell at short-term forecasting, while
overlooking the hurdles of Long-term Probabilistic Time Series Forecasting
(LPTSF). As the forecast horizon extends, the inherent nonlinear dynamics have
a significant adverse effect on prediction accuracy, and make generative models
inefficient by increasing the cost of each iteration. To overcome these
limitations, we introduce $K^2$VAE, an efficient VAE-based generative model
that leverages a KoopmanNet to transform nonlinear time series into a linear
dynamical system, and devises a KalmanNet to refine predictions and model
uncertainty in such linear system, which reduces error accumulation in
long-term forecasting. Extensive experiments demonstrate that $K^2$VAE
outperforms state-of-the-art methods in both short- and long-term PTSF,
providing a more efficient and accurate solution.

</details>


### [85] [SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference](https://arxiv.org/abs/2505.23022)
*Yinghao Tang,Tingfeng Lan,Xiuqi Huang,Hui Lu,Wei Chen*

Main category: cs.LG

TL;DR: The paper presents SCORPIO, an SLO-oriented LLM serving system that improves system goodput and SLO adherence by up to 14.4X and 46.5% respectively compared to state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Existing LLM serving systems focus on maximum throughput but often neglect important SLOs like TTFT and TPOT, leading to suboptimal performance.

Method: SCORPIO exploits SLO heterogeneity for adaptive scheduling across admission control, queue management, and batch selection. It features a TTFT Guard for least-deadline-first reordering and rejecting unattainable requests, and a TPOT Guard utilizing VBS-based admission control and a credit-based batching mechanism. Both are supported by a predictive module.

Result: Evaluations show SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5% compared to current best systems.

Conclusion: SCORPIO is an effective SLO-oriented LLM serving system that significantly enhances system goodput and SLO attainment for workloads with heterogeneous SLOs.

Abstract: Existing Large Language Model (LLM) serving systems prioritize maximum
throughput. They often neglect Service Level Objectives (SLOs) such as Time to
First Token (TTFT) and Time Per Output Token (TPOT), which leads to suboptimal
SLO attainment. This paper introduces SCORPIO, an SLO-oriented LLM serving
system designed to maximize system goodput and SLO attainment for workloads
with heterogeneous SLOs. Our core insight is to exploit SLO heterogeneity for
adaptive scheduling across admission control, queue management, and batch
selection. SCORPIO features a TTFT Guard, which employs least-deadline-first
reordering and rejects unattainable requests, and a TPOT Guard, which utilizes
a VBS-based admission control and a novel credit-based batching mechanism. Both
guards are supported by a predictive module. Evaluations demonstrate that
SCORPIO improves system goodput by up to 14.4X and SLO adherence by up to 46.5%
compared to state-of-the-art baselines.

</details>


### [86] [An Empirical Study of Federated Prompt Learning for Vision Language Model](https://arxiv.org/abs/2505.23024)
*Zhihao Wang,Wenke Huang,Tian Chen,Zekun Shi,Guancheng Wan,Yu Qiao,Bin Yang,Jian Wang,Bing Li,Mang Ye*

Main category: cs.LG

TL;DR: The paper explores the differences between language prompt learning (LPT) and vision prompt learning (VPT) in Vision Language Models (VLMs) under federated learning scenarios with data heterogeneity, providing insights for optimizing prompt learning.


<details>
  <summary>Details</summary>
Motivation: To understand how LPT and VPT perform under challenges like label skew and domain shift within federated learning settings, and to find strategies that can enhance prompt learning in these complex environments.

Method: Systematically investigate the behavioral differences between LPT and VPT under data heterogeneity. Conduct extensive experiments evaluating the impact of different federated learning and prompt configurations on Federated Prompt Learning (FPL) robustness. Explore strategies to improve prompt learning in complex scenarios where label skew and domain shift coexist.

Result: Offered practical insights into optimizing prompt learning in federated settings which can contribute to deploying VLMs in privacy-preserving environments.

Conclusion: Understanding the differences and enhancing prompt learning strategies in federated learning environments can optimize the deployment of VLMs in privacy-preserving contexts.

Abstract: The Vision Language Model (VLM) excels in aligning vision and language
representations, and prompt learning has emerged as a key technique for
adapting such models to downstream tasks. However, the application of prompt
learning with VLM in federated learning (\fl{}) scenarios remains
underexplored. This paper systematically investigates the behavioral
differences between language prompt learning (LPT) and vision prompt learning
(VPT) under data heterogeneity challenges, including label skew and domain
shift. We conduct extensive experiments to evaluate the impact of various \fl{}
and prompt configurations, such as client scale, aggregation strategies, and
prompt length, to assess the robustness of Federated Prompt Learning (FPL).
Furthermore, we explore strategies for enhancing prompt learning in complex
scenarios where label skew and domain shift coexist, including leveraging both
prompt types when computational resources allow. Our findings offer practical
insights into optimizing prompt learning in federated settings, contributing to
the broader deployment of VLMs in privacy-preserving environments.

</details>


### [87] [Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift](https://arxiv.org/abs/2505.23027)
*Minh Nguyen Nhat To,Paul F RWilson,Viet Nguyen,Mohamed Harmanani,Michael Cooper,Fahimeh Fooladgar,Purang Abolmaesumi,Parvin Mousavi,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: The paper proposes Diverse Prototypical Ensembles (DPEs) to address subpopulation shift in machine learning models by using an ensemble of diverse classifiers.


<details>
  <summary>Details</summary>
Motivation: Subpopulation shift can degrade the performance of machine learning models and current solutions have limitations due to assumptions about subpopulations and lack of annotations in real-world datasets.

Method: The method involves replacing the standard linear classification layer with a mixture of prototypical classifiers, forming an ensemble that focuses on different features and samples adaptively capturing risk associated with subpopulations.

Result: DPEs outperform prior state-of-the-art methods in worst-group accuracy across nine real-world datasets covering diverse domains and kinds of subpopulation shift.

Conclusion: DPEs provide a promising approach to handle subpopulation shift without relying on assumptions or annotations about subpopulations.

Abstract: The subpopulationtion shift, characterized by a disparity in subpopulation
distributibetween theween the training and target datasets, can significantly
degrade the performance of machine learning models. Current solutions to
subpopulation shift involve modifying empirical risk minimization with
re-weighting strategies to improve generalization. This strategy relies on
assumptions about the number and nature of subpopulations and annotations on
group membership, which are unavailable for many real-world datasets. Instead,
we propose using an ensemble of diverse classifiers to adaptively capture risk
associated with subpopulations. Given a feature extractor network, we replace
its standard linear classification layer with a mixture of prototypical
classifiers, where each member is trained to classify the data while focusing
on different features and samples from other members. In empirical evaluation
on nine real-world datasets, covering diverse domains and kinds of
subpopulation shift, our method of Diverse Prototypical Ensembles (DPEs) often
outperforms the prior state-of-the-art in worst-group accuracy. The code is
available at https://github.com/minhto2802/dpe4subpop

</details>


### [88] [Bayesian Neural Scaling Laws Extrapolation with Prior-Fitted Networks](https://arxiv.org/abs/2505.23032)
*Dongwoo Lee,Dong Bok Lee,Steven Adriaensen,Juho Lee,Sung Ju Hwang,Frank Hutter,Seon Joo Kim,Hae Beom Lee*

Main category: cs.LG

TL;DR: The paper explores a Bayesian framework using Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation, showing superior performance in data-limited scenarios.


<details>
  <summary>Details</summary>
Motivation: Current methods for predicting neural scaling laws rely on point estimation without quantifying uncertainty, which is important for real-world decision-making.

Method: The authors design a prior distribution for PFNs that can sample synthetic functions resembling real-world neural scaling laws, allowing the PFN to meta-learn the extrapolation.

Result: The method outperforms existing point estimation and Bayesian approaches, especially in data-limited scenarios like Bayesian active learning.

Conclusion: This Bayesian framework provides reliable, uncertainty-aware extrapolation of neural scaling laws for practical applications.

Abstract: Scaling has been a major driver of recent advancements in deep learning.
Numerous empirical studies have found that scaling laws often follow the
power-law and proposed several variants of power-law functions to predict the
scaling behavior at larger scales. However, existing methods mostly rely on
point estimation and do not quantify uncertainty, which is crucial for
real-world applications involving decision-making problems such as determining
the expected performance improvements achievable by investing additional
computational resources. In this work, we explore a Bayesian framework based on
Prior-data Fitted Networks (PFNs) for neural scaling law extrapolation.
Specifically, we design a prior distribution that enables the sampling of
infinitely many synthetic functions resembling real-world neural scaling laws,
allowing our PFN to meta-learn the extrapolation. We validate the effectiveness
of our approach on real-world neural scaling laws, comparing it against both
the existing point estimation methods and Bayesian approaches. Our method
demonstrates superior performance, particularly in data-limited scenarios such
as Bayesian active learning, underscoring its potential for reliable,
uncertainty-aware extrapolation in practical applications.

</details>


### [89] [From Theory to Application: Fine-Tuning Large EEG Model with Real-World Stress Data](https://arxiv.org/abs/2505.23042)
*Siwen Wang,Shitou Zhang,Wan-Lin Chen,Dung Truong,Tzyy-Ping Jung*

Main category: cs.LG

TL;DR: Recent advancements in Large Language Models have inspired foundation models across various domains. This study evaluates the efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM on a real-world stress classification dataset collected in a graduate classroom. The best-performing model achieves a balanced accuracy of 90.47% with a 5-second window, outperforming traditional classifiers in accuracy and inference efficiency. Results demonstrate LEMs' capability to process real-world EEG data and their potential to revolutionize brain-computer interface applications.


<details>
  <summary>Details</summary>
Motivation: To assess the applicability of Large EEG Models (LEMs) in real-world environments rather than controlled clinical settings.

Method: Fine-tuned LaBraM, a state-of-the-art foundation EEG model, on a real-world stress classification dataset collected from 18 graduate students in a classroom setting. Trained a binary classifier using resting-state EEG data to distinguish between normal and elevated stress states.

Result: The best-performing fine-tuned model achieved a balanced accuracy of 90.47% with a 5-second window, significantly outperforming traditional stress classifiers. The model also demonstrated robustness under random data shuffling and reduced channel counts.

Conclusion: LEMs can effectively process real-world EEG data and have the potential to revolutionize brain-computer interface applications by shifting focus from model-centric to data-centric design.

Abstract: Recent advancements in Large Language Models have inspired the development of
foundation models across various domains. In this study, we evaluate the
efficacy of Large EEG Models (LEMs) by fine-tuning LaBraM, a state-of-the-art
foundation EEG model, on a real-world stress classification dataset collected
in a graduate classroom. Unlike previous studies that primarily evaluate LEMs
using data from controlled clinical settings, our work assesses their
applicability to real-world environments. We train a binary classifier that
distinguishes between normal and elevated stress states using resting-state EEG
data recorded from 18 graduate students during a class session. The
best-performing fine-tuned model achieves a balanced accuracy of 90.47% with a
5-second window, significantly outperforming traditional stress classifiers in
both accuracy and inference efficiency. We further evaluate the robustness of
the fine-tuned LEM under random data shuffling and reduced channel counts.
These results demonstrate the capability of LEMs to effectively process
real-world EEG data and highlight their potential to revolutionize
brain-computer interface applications by shifting the focus from model-centric
to data-centric design.

</details>


### [90] [ProDiff: Prototype-Guided Diffusion for Minimal Information Trajectory Imputation](https://arxiv.org/abs/2505.23048)
*Tianci Bu,Le Zhou,Wenchuan Yang,Jianhong Mou,Kang Yang,Suoyi Tan,Feng Yao,Jingyuan Wang,Xin Lu*

Main category: cs.LG

TL;DR: 提出了一种名为ProDiff的轨迹补全框架，利用原型学习和去噪扩散概率模型，仅使用两个端点作为最小信息进行轨迹补全。该方法在FourSquare和WuXi数据集上分别提高了6.28%和2.52%的准确性，并且生成轨迹与真实轨迹之间的相关性为0.927。


<details>
  <summary>Details</summary>
Motivation: 现有的轨迹补全方法依赖于稀疏轨迹或旅行信息（如速度），假设这些稀疏轨迹保留了基本的行为模式，但这种方法对数据获取要求高，并且忽略了大规模人类轨迹嵌入的潜力。

Method: 提出了ProDiff，一个轨迹补全框架，仅使用两个端点作为最小信息。通过结合原型学习来嵌入人类运动模式和去噪扩散概率模型来进行鲁棒的空间时间重建，并通过联合训练和定制的损失函数确保有效的补全。

Result: ProDiff在FourSquare和WuXi数据集上的表现优于现有方法，准确率分别提高了6.28%和2.52%，并且生成轨迹与真实轨迹的相关性为0.927。

Conclusion: ProDiff通过利用原型学习和去噪扩散概率模型，仅使用两个端点作为输入，可以有效地进行轨迹补全，具有较高的准确性和生成轨迹的真实性。

Abstract: Trajectory data is crucial for various applications but often suffers from
incompleteness due to device limitations and diverse collection scenarios.
Existing imputation methods rely on sparse trajectory or travel information,
such as velocity, to infer missing points. However, these approaches assume
that sparse trajectories retain essential behavioral patterns, which place
significant demands on data acquisition and overlook the potential of
large-scale human trajectory embeddings. To address this, we propose ProDiff, a
trajectory imputation framework that uses only two endpoints as minimal
information. It integrates prototype learning to embed human movement patterns
and a denoising diffusion probabilistic model for robust spatiotemporal
reconstruction. Joint training with a tailored loss function ensures effective
imputation. ProDiff outperforms state-of-the-art methods, improving accuracy by
6.28\% on FourSquare and 2.52\% on WuXi. Further analysis shows a 0.927
correlation between generated and real trajectories, demonstrating the
effectiveness of our approach.

</details>


### [91] [DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration](https://arxiv.org/abs/2505.23049)
*Tianteng Gu,Bei Liu,Bo Xiao,Ke Zeng,Jiacheng Liu,Yanmin Qian*

Main category: cs.LG

TL;DR: DenoiseRotator is a novel method for pruning large language models that redistributes parameter importance to make models more amenable to pruning, improving performance metrics like perplexity and zero-shot accuracy.


<details>
  <summary>Details</summary>
Motivation: Pruning large language models often leads to significant performance degradation, particularly under semi-structured sparsity constraints. Current methods focus on estimating individual weight importance which limits their effectiveness.

Method: The method involves minimizing the information entropy of normalized importance scores to concentrate importance on fewer weights, enhancing pruning robustness. This is achieved through DenoiseRotator which applies learnable orthogonal transformations to weight matrices.

Result: DenoiseRotator consistently improves perplexity and zero-shot accuracy when evaluated on LLaMA3, Qwen2.5, and Mistral models under different sparsity conditions. For example, it reduces the perplexity gap by 58% in one case.

Conclusion: DenoiseRotator is model-agnostic and can be integrated with existing pruning techniques, offering improvements in pruning performance.

Abstract: Pruning is a widely used technique to compress large language models (LLMs)
by removing unimportant weights, but it often suffers from significant
performance degradation - especially under semi-structured sparsity
constraints. Existing pruning methods primarily focus on estimating the
importance of individual weights, which limits their ability to preserve
critical capabilities of the model. In this work, we propose a new perspective:
rather than merely selecting which weights to prune, we first redistribute
parameter importance to make the model inherently more amenable to pruning. By
minimizing the information entropy of normalized importance scores, our
approach concentrates importance onto a smaller subset of weights, thereby
enhancing pruning robustness. We instantiate this idea through DenoiseRotator,
which applies learnable orthogonal transformations to the model's weight
matrices. Our method is model-agnostic and can be seamlessly integrated with
existing pruning techniques such as Magnitude, SparseGPT, and Wanda. Evaluated
on LLaMA3, Qwen2.5, and Mistral models under 50% unstructured and 2:4
semi-structured sparsity, DenoiseRotator consistently improves perplexity and
zero-shot accuracy. For instance, on LLaMA3-70B pruned with SparseGPT at 2:4
semi-structured sparsity, DenoiseRotator reduces the perplexity gap to the
dense model by 58%, narrowing the degradation from 8.1 to 3.4 points. Codes are
available at https://github.com/Axel-gu/DenoiseRotator.

</details>


### [92] [CDR-Agent: Intelligent Selection and Execution of Clinical Decision Rules Using Large Language Model Agents](https://arxiv.org/abs/2505.23055)
*Zhen Xiang,Aliyah R. Hsu,Austin V. Zane,Aaron E. Kornblith,Margaret J. Lin-Martore,Jasmanpreet C. Kaur,Vasuda M. Dokiparthi,Bo Li,Bin Yu*

Main category: cs.LG

TL;DR: CDR-Agent is a novel LLM-based system designed to enhance ED decision-making by identifying and applying the most appropriate CDRs based on unstructured clinical notes. It achieves significant accuracy gain and reduces computational overhead.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making, especially in emergency departments, is complex and fast-paced. The use of Clinical Decision Rules (CDRs) can help make consistent and accurate diagnoses, but their usage is often hindered by the clinician's cognitive load.

Method: The researchers introduced CDR-Agent, a LLM-based system that autonomously identifies and applies the most appropriate CDRs based on unstructured clinical notes. They curated two novel ED datasets: synthetic and CDR-Bench to validate CDR-Agent.

Result: CDR-Agent achieves a 56.3% (synthetic) and 8.7% (CDR-Bench) accuracy gain relative to the standalone LLM baseline in CDR selection. It significantly reduces computational overhead and outperforms traditional LLM prompting approaches.

Conclusion: CDR-Agent not only selects relevant CDRs efficiently, but also makes cautious yet effective imaging decisions by minimizing unnecessary interventions while successfully identifying most positively diagnosed cases.

Abstract: Clinical decision-making is inherently complex and fast-paced, particularly
in emergency departments (EDs) where critical, rapid and high-stakes decisions
are made. Clinical Decision Rules (CDRs) are standardized evidence-based tools
that combine signs, symptoms, and clinical variables into decision trees to
make consistent and accurate diagnoses. CDR usage is often hindered by the
clinician's cognitive load, limiting their ability to quickly recall and apply
the appropriate rules. We introduce CDR-Agent, a novel LLM-based system
designed to enhance ED decision-making by autonomously identifying and applying
the most appropriate CDRs based on unstructured clinical notes. To validate
CDR-Agent, we curated two novel ED datasets: synthetic and CDR-Bench, although
CDR-Agent is applicable to non ED clinics. CDR-Agent achieves a 56.3\%
(synthetic) and 8.7\% (CDR-Bench) accuracy gain relative to the standalone LLM
baseline in CDR selection. Moreover, CDR-Agent significantly reduces
computational overhead. Using these datasets, we demonstrated that CDR-Agent
not only selects relevant CDRs efficiently, but makes cautious yet effective
imaging decisions by minimizing unnecessary interventions while successfully
identifying most positively diagnosed cases, outperforming traditional LLM
prompting approaches. Code for our work can be found at:
https://github.com/zhenxianglance/medagent-cdr-agent

</details>


### [93] [DINGO: Constrained Inference for Diffusion LLMs](https://arxiv.org/abs/2505.23061)
*Tarun Suresh,Debangshu Banerjee,Shubham Ugare,Sasa Misailovic,Gagandeep Singh*

Main category: cs.LG

TL;DR: Diffusion LLMs have shown promise for runtime efficiency but lack reliable structured output generation. The proposed DINGO addresses this by enabling efficient, provably distribution-preserving constrained decoding, achieving significant improvements on benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability of diffusion LLMs for tasks requiring structured outputs, such as JSON generation, by enforcing user-specified formal constraints like regular expressions.

Method: Propose DINGO, a dynamic programming-based constrained decoding strategy that allows parallel token prediction while preserving the true output distribution and strictly satisfying user-specified regular expressions.

Result: DINGO achieves up to a 68 percentage point improvement over unconstrained inference on standard symbolic math and JSON generation benchmarks.

Conclusion: DINGO successfully enables diffusion LLMs to generate structured outputs with high fidelity to user-specified constraints.

Abstract: Diffusion LLMs have emerged as a promising alternative to conventional
autoregressive LLMs, offering significant potential for improved runtime
efficiency. However, existing diffusion models lack the ability to provably
enforce user-specified formal constraints, such as regular expressions, which
makes them unreliable for tasks that require structured outputs, such as
fixed-schema JSON generation. Unlike autoregressive models that generate tokens
sequentially, diffusion LLMs predict a block of tokens in parallel. This
parallelism makes traditional constrained decoding algorithms, which are
designed for sequential token prediction, ineffective at preserving the true
output distribution. To address this limitation, we propose DINGO, a dynamic
programming-based constrained decoding strategy that is both efficient and
provably distribution-preserving. DINGO enables sampling of output strings with
the highest probability under the model's predicted distribution, while
strictly satisfying any user-specified regular expression. On standard symbolic
math and JSON generation benchmarks, DINGO achieves up to a 68 percentage point
improvement over unconstrained inference

</details>


### [94] [Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data](https://arxiv.org/abs/2505.23062)
*Lingkai Kong,Haichuan Wang,Tonghan Wang,Guojun Xiong,Milind Tambe*

Main category: cs.LG

TL;DR: CompFlow是一种基于流匹配和最优传输理论的方法，通过建模目标动态为条件流来改进源动态的学习，并提出了一种乐观主动数据收集策略以优先探索高动态差距区域。实验表明，CompFlow在多个RL基准上优于强基线。


<details>
  <summary>Details</summary>
Motivation: 离线数据可以提高强化学习的样本效率，但由于源环境和目标环境之间的动态差异，这一优势受到挑战。现有的方法通常通过惩罚或过滤掉动态差异大的区域中的源转换来解决这个问题，但其估计动态差异的方法可能在源和目标动态支持不相交时无法定义。

Method: CompFlow提出了一种基于流匹配和最优传输理论的方法，将目标动态建模为条件流，而不是直接从高斯先验中学习。这种方法提供了两个关键优势：（1）改进了目标动态学习的泛化能力；（2）通过源和目标转换之间的Wasserstein距离对动态差异进行有原则的估计。此外，还引入了一种乐观主动数据收集策略，该策略优先在动态差异大的区域进行探索。

Result: 理论上证明了所提出的乐观主动数据收集策略减少了与最优策略的性能差异。实证上，CompFlow在动态转移的几个RL基准上优于强基线。

Conclusion: CompFlow通过改进目标动态学习的泛化能力和提供有原则的动态差异估计，有效地解决了源和目标动态差异的问题，并且其乐观主动数据收集策略进一步提高了性能。

Abstract: Incorporating pre-collected offline data from a source environment can
significantly improve the sample efficiency of reinforcement learning (RL), but
this benefit is often challenged by discrepancies between the transition
dynamics of the source and target environments. Existing methods typically
address this issue by penalizing or filtering out source transitions in high
dynamics-gap regions. However, their estimation of the dynamics gap often
relies on KL divergence or mutual information, which can be ill-defined when
the source and target dynamics have disjoint support. To overcome these
limitations, we propose CompFlow, a method grounded in the theoretical
connection between flow matching and optimal transport. Specifically, we model
the target dynamics as a conditional flow built upon the output distribution of
the source-domain flow, rather than learning it directly from a Gaussian prior.
This composite structure offers two key advantages: (1) improved generalization
for learning target dynamics, and (2) a principled estimation of the dynamics
gap via the Wasserstein distance between source and target transitions.
Leveraging our principled estimation of the dynamics gap, we further introduce
an optimistic active data collection strategy that prioritizes exploration in
regions of high dynamics gap, and theoretically prove that it reduces the
performance disparity with the optimal policy. Empirically, CompFlow
outperforms strong baselines across several RL benchmarks with shifted
dynamics.

</details>


### [95] [Loss-Guided Model Sharing and Local Learning Correction in Decentralized Federated Learning for Crop Disease Classification](https://arxiv.org/abs/2505.23063)
*Denis Mamba Kabala,Adel Hafiane,Laurent Bobelin,Raphael Canals*

Main category: cs.LG

TL;DR: The paper introduces a Decentralized Federated Learning (DFL) framework for crop disease detection using validation loss to guide model sharing and correct local training.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of centralized Federated Learning such as single-point failures and scalability issues while maintaining data privacy in agricultural applications.

Method: A novel DFL framework that employs validation loss (Loss_val) to guide model sharing between peers and corrects local training through an adaptive loss function controlled by a weighting parameter. Experiments were conducted with PlantVillage datasets across three deep learning architectures: ResNet50, VGG16, and ViT_B16.

Result: The DFL approach improves accuracy, convergence speed, generalization, and robustness across heterogeneous data environments compared to traditional methods.

Conclusion: The proposed DFL framework is well-suited for privacy-preserving agricultural applications, offering enhanced performance and reliability.

Abstract: Crop disease detection and classification is a critical challenge in
agriculture, with major implications for productivity, food security, and
environmental sustainability. While deep learning models such as CNN and ViT
have shown excellent performance in classifying plant diseases from images,
their large-scale deployment is often limited by data privacy concerns.
Federated Learning (FL) addresses this issue, but centralized FL remains
vulnerable to single-point failures and scalability limits. In this paper, we
introduce a novel Decentralized Federated Learning (DFL) framework that uses
validation loss (Loss_val) both to guide model sharing between peers and to
correct local training via an adaptive loss function controlled by weighting
parameter. We conduct extensive experiments using PlantVillage datasets with
three deep learning architectures (ResNet50, VGG16, and ViT_B16), analyzing the
impact of weighting parameter, the number of shared models, the number of
clients, and the use of Loss_val versus Loss_train of other clients. Results
demonstrate that our DFL approach not only improves accuracy and convergence
speed, but also ensures better generalization and robustness across
heterogeneous data environments making it particularly well-suited for
privacy-preserving agricultural applications.

</details>


### [96] [Multi-Modal Learning with Bayesian-Oriented Gradient Calibration](https://arxiv.org/abs/2505.23071)
*Peizheng Guo,Jingyao Wang,Huijie Guo,Jiangmeng Li,Chuxiong Sun,Changwen Zheng,Wenwen Qiang*

Main category: cs.LG

TL;DR: In Multi-Modal Learning (MML), current methods aggregate gradients with fixed weights, ignoring gradient uncertainty. This can degrade performance or hinder learning. The paper proposes BOGC-MML, which models gradient uncertainty to guide optimization, using a Bayesian approach and Dempster-Shafer rule for calibrated updates.


<details>
  <summary>Details</summary>
Motivation: Existing MML methods aggregate gradients with fixed weights, treating all dimensions equally and ignoring the intrinsic gradient uncertainty of each modality. This oversight may lead to excessive updates in sensitive dimensions and insufficient updates in less sensitive ones, thus degrading or hindering performance.

Method: The proposed method, BOGC-MML, models each modality's gradient as a random variable to capture its probability distribution and full uncertainty. It converts the precision of each gradient distribution into scalar evidence, quantifying each modality's confidence per gradient dimension. These evidences are used to quantify uncertainties per-dimension and fuse them via a reduced Dempster-Shafer rule, resulting in an uncertainty-weighted aggregation that calibrates update directions.

Result: Extensive experiments on multiple benchmark datasets show the effectiveness and advantages of BOGC-MML in improving predictive accuracy by addressing gradient uncertainty.

Conclusion: BOGC-MML is a novel Bayesian-Oriented Gradient Calibration method for MML that successfully models gradient uncertainty, providing calibrated update directions and demonstrating significant improvements in performance across various benchmarks.

Abstract: Multi-Modal Learning (MML) integrates information from diverse modalities to
improve predictive accuracy. However, existing methods mainly aggregate
gradients with fixed weights and treat all dimensions equally, overlooking the
intrinsic gradient uncertainty of each modality. This may lead to (i) excessive
updates in sensitive dimensions, degrading performance, and (ii) insufficient
updates in less sensitive dimensions, hindering learning. To address this
issue, we propose BOGC-MML, a Bayesian-Oriented Gradient Calibration method for
MML to explicitly model the gradient uncertainty and guide the model
optimization towards the optimal direction. Specifically, we first model each
modality's gradient as a random variable and derive its probability
distribution, capturing the full uncertainty in the gradient space. Then, we
propose an effective method that converts the precision (inverse variance) of
each gradient distribution into a scalar evidence. This evidence quantifies the
confidence of each modality in every gradient dimension. Using these evidences,
we explicitly quantify per-dimension uncertainties and fuse them via a reduced
Dempster-Shafer rule. The resulting uncertainty-weighted aggregation produces a
calibrated update direction that balances sensitivity and conservatism across
dimensions. Extensive experiments on multiple benchmark datasets demonstrate
the effectiveness and advantages of the proposed method.

</details>


### [97] [Gradient Boosting Decision Tree with LSTM for Investment Prediction](https://arxiv.org/abs/2505.23084)
*Chang Yu,Fang Liu,Jie Zhu,Shaobo Guo,Yifan Gao,Zhongheng Yang,Meiwei Liu,Qianwen Xing*

Main category: cs.LG

TL;DR: This paper proposes a hybrid framework combining LSTM networks with LightGBM and CatBoost for stock price prediction, improving accuracy by 10 to 15 percent compared to individual models.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy of stock price prediction by integrating LSTM networks with LightGBM and CatBoost, leveraging their strengths in sequential and tree-based data processing.

Method: The framework processes time-series financial data using seven models (ANNs, CNNs, BiLSTM, vanilla LSTM, XGBoost, LightGBM, standard NNs) and evaluates performance with key metrics (MAE, R-squared, MSE, RMSE). An ensemble model is then developed to combine the strengths of sequential and tree-based approaches.

Result: Experimental results indicate that the proposed framework improves accuracy by 10 to 15 percent compared to individual models and reduces error during market changes.

Conclusion: This study highlights the potential of ensemble methods for financial forecasting and provides a flexible design for integrating new machine learning techniques.

Abstract: This paper proposes a hybrid framework combining LSTM (Long Short-Term
Memory) networks with LightGBM and CatBoost for stock price prediction. The
framework processes time-series financial data and evaluates performance using
seven models: Artificial Neural Networks (ANNs), Convolutional Neural Networks
(CNNs), Bidirectional LSTM (BiLSTM), vanilla LSTM, XGBoost, LightGBM, and
standard Neural Networks (NNs). Key metrics, including MAE, R-squared, MSE, and
RMSE, are used to establish benchmarks across different time scales.
  Building on these benchmarks, we develop an ensemble model that combines the
strengths of sequential and tree-based approaches. Experimental results show
that the proposed framework improves accuracy by 10 to 15 percent compared to
individual models and reduces error during market changes. This study
highlights the potential of ensemble methods for financial forecasting and
provides a flexible design for integrating new machine learning techniques.

</details>


### [98] [Equivariant Spherical Transformer for Efficient Molecular Modeling](https://arxiv.org/abs/2505.23086)
*Junyi An,Xinyu Lu,Chao Qu,Yunfei Shi,Peijia Lin,Qianwei Tang,Licheng Xu,Fenglei Cao,Yuan Qi*

Main category: cs.LG

TL;DR: SE(3)-equivariant Graph Neural Networks have advanced molecular system modeling, but their message passing processes are limited. This paper introduces the Equivariant Spherical Transformer to overcome these limitations, which demonstrates superior expressiveness and state-of-the-art performance on molecular benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of SE(3)-equivariant Graph Neural Networks in terms of non-linearity and group representations, which restrict their expressiveness.

Method: The method introduced in this paper is the Equivariant Spherical Transformer (EST), a novel framework that leverages a Transformer structure within the spatial domain of group representations after Fourier transform.

Result: The results show that EST can encompass the function space of tensor products while achieving superior expressiveness. It also demonstrates state-of-the-art performance on various molecular benchmarks, including OC20 and QM9.

Conclusion: The conclusion is that the Equivariant Spherical Transformer overcomes the limitations of existing SE(3)-equivariant Graph Neural Networks and provides a more expressive model for molecular system modeling.

Abstract: SE(3)-equivariant Graph Neural Networks (GNNs) have significantly advanced
molecular system modeling by employing group representations. However, their
message passing processes, which rely on tensor product-based convolutions, are
limited by insufficient non-linearity and incomplete group representations,
thereby restricting expressiveness. To overcome these limitations, we introduce
the Equivariant Spherical Transformer (EST), a novel framework that leverages a
Transformer structure within the spatial domain of group representations after
Fourier transform. We theoretically and empirically demonstrate that EST can
encompass the function space of tensor products while achieving superior
expressiveness. Furthermore, EST's equivariant inductive bias is guaranteed
through a uniform sampling strategy for the Fourier transform. Our experiments
demonstrate state-of-the-art performance by EST on various molecular
benchmarks, including OC20 and QM9.

</details>


### [99] [MAP: Revisiting Weight Decomposition for Low-Rank Adaptation](https://arxiv.org/abs/2505.23094)
*Chongjie Si,Zhiyi Shi,Yadao Wang,Xiaokang Yang,Susanto Rahardja,Wei Shen*

Main category: cs.LG

TL;DR: The paper introduces MAP, a new framework that reformulates weight matrices as high-dimensional vectors and decouples their adaptation into direction and magnitude. It enhances existing PEFT methods significantly.


<details>
  <summary>Details</summary>
Motivation: Existing PEFT methods often define direction heuristically at the column level, lacking a principled geometric foundation.

Method: MAP normalizes the pre-trained weights, learns a directional update, and introduces two scalar coefficients to independently scale the magnitude of the base and update vectors.

Result: Extensive experiments show that MAP significantly improves performance when coupling with existing methods.

Conclusion: Given the universality and simplicity of MAP, it can serve as a default setting for designing future PEFT methods.

Abstract: The rapid development of large language models has revolutionized natural
language processing, but their fine-tuning remains computationally expensive,
hindering broad deployment. Parameter-efficient fine-tuning (PEFT) methods,
such as LoRA, have emerged as solutions. Recent work like DoRA attempts to
further decompose weight adaptation into direction and magnitude components.
However, existing formulations often define direction heuristically at the
column level, lacking a principled geometric foundation. In this paper, we
propose MAP, a novel framework that reformulates weight matrices as
high-dimensional vectors and decouples their adaptation into direction and
magnitude in a rigorous manner. MAP normalizes the pre-trained weights, learns
a directional update, and introduces two scalar coefficients to independently
scale the magnitude of the base and update vectors. This design enables more
interpretable and flexible adaptation, and can be seamlessly integrated into
existing PEFT methods. Extensive experiments show that MAP significantly
improves performance when coupling with existing methods, offering a simple yet
powerful enhancement to existing PEFT methods. Given the universality and
simplicity of MAP, we hope it can serve as a default setting for designing
future PEFT methods.

</details>


### [100] [Learning to Search for Vehicle Routing with Multiple Time Windows](https://arxiv.org/abs/2505.23098)
*Kuan Xu,Zhiguang Cao,Chenlong Zheng,Linong Liu*

Main category: cs.LG

TL;DR: An RL-based adaptive variable neighborhood search method, named RL-AVNS, is proposed for solving the VRPMTW. It uses a fitness metric and a transformer-based neural policy network to enhance solution quality and computational efficiency compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of traditional adaptive approaches in solving VRPMTW by incorporating reinforcement learning to dynamically select neighborhood operators based on real-time solution states and learned experience.

Method: The method integrates a reinforcement learning framework with an adaptive variable neighborhood search (AVNS). A fitness metric quantifies customers' temporal flexibility to improve the shaking phase, and a transformer-based neural policy network guides operator selection during the local search.

Result: RL-AVNS significantly outperforms traditional VNS, AVNS, and state-of-the-art learning-based heuristics in terms of solution quality and computational efficiency across various instance scales and time window complexities. The algorithm also generalizes well to unseen problem instances.

Conclusion: RL-AVNS demonstrates superior performance in solving VRPMTW, providing practical utility for complex logistics scenarios.

Abstract: In this study, we propose a reinforcement learning-based adaptive variable
neighborhood search (RL-AVNS) method designed for effectively solving the
Vehicle Routing Problem with Multiple Time Windows (VRPMTW). Unlike traditional
adaptive approaches that rely solely on historical operator performance, our
method integrates a reinforcement learning framework to dynamically select
neighborhood operators based on real-time solution states and learned
experience. We introduce a fitness metric that quantifies customers' temporal
flexibility to improve the shaking phase, and employ a transformer-based neural
policy network to intelligently guide operator selection during the local
search. Extensive computational experiments are conducted on realistic
scenarios derived from the replenishment of unmanned vending machines,
characterized by multiple clustered replenishment windows. Results demonstrate
that RL-AVNS significantly outperforms traditional variable neighborhood search
(VNS), adaptive VNS (AVNS), and state-of-the-art learning-based heuristics,
achieving substantial improvements in solution quality and computational
efficiency across various instance scales and time window complexities.
Particularly notable is the algorithm's capability to generalize effectively to
problem instances not encountered during training, underscoring its practical
utility for complex logistics scenarios.

</details>


### [101] [Weight Spectra Induced Efficient Model Adaptation](https://arxiv.org/abs/2505.23099)
*Chongjie Si,Xuankun Yang,Muqing Liu,Yadao Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 在大规模基础模型微调过程中，发现主要放大了权重矩阵的顶级奇异值，并提出了一种通过学习重新缩放顶级奇异方向的方法，实现对模型最有效组件的精确调整，同时不破坏整体结构。


<details>
  <summary>Details</summary>
Motivation: 全量微调大型基础模型计算成本过高，因此需要探索更高效的参数微调方法（如LoRA），并深入理解微调过程中模型参数变化的机制。

Method: 通过对权重矩阵进行奇异值分解（SVD），研究微调过程中权重矩阵的结构性变化；提出一种基于学习重新缩放顶级奇异方向的新方法，以实现对模型关键部分的精确调整。

Result: 该方法在多个任务中对强基线模型实现了持续改进，验证了结构化微调的有效性。

Conclusion: 微调主要影响权重矩阵的顶级奇异值和主导奇异向量，而非主导子空间保持稳定；提出的新方法可以精确调整模型的关键部分而不破坏整体结构，具有广泛的应用潜力。

Abstract: Large-scale foundation models have demonstrated remarkable versatility across
a wide range of downstream tasks. However, fully fine-tuning these models
incurs prohibitive computational costs, motivating the development of
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA, which introduces
low-rank updates to pre-trained weights. Despite their empirical success, the
underlying mechanisms by which PEFT modifies model parameters remain
underexplored. In this work, we present a systematic investigation into the
structural changes of weight matrices during fully fine-tuning. Through
singular value decomposition (SVD), we reveal that fine-tuning predominantly
amplifies the top singular values while leaving the remainder largely intact,
suggesting that task-specific knowledge is injected into a low-dimensional
subspace. Furthermore, we find that the dominant singular vectors are
reoriented in task-specific directions, whereas the non-dominant subspace
remains stable. Building on these insights, we propose a novel method that
leverages learnable rescaling of top singular directions, enabling precise
modulation of the most influential components without disrupting the global
structure. Our approach achieves consistent improvements over strong baselines
across multiple tasks, highlighting the efficacy of structurally informed
fine-tuning.

</details>


### [102] [LUMION: Fast Fault Recovery for ML Jobs Using Programmable Optical Fabrics](https://arxiv.org/abs/2505.23105)
*Abhishek Vijaya Kumar,Eric Ding,Arjun Devraj,Darius Bunandar,Rachee Singh*

Main category: cs.LG

TL;DR: This paper proposes LUMION, a reconfigurable optical fabric that dynamically integrates spare accelerators into ongoing workloads upon failures, reducing the need for costly job migrations in ML datacenters. Experiments show it can swap a failed GPU within ~1 second and improve fine-tuning throughput nearly 2X.


<details>
  <summary>Details</summary>
Motivation: Modern ML datacenters reserve full racks of idle accelerators for fault tolerance when jobs are migrated to new racks upon accelerator failures. This is highly inefficient and motivates the need for a more resource-efficient solution.

Method: The paper introduces LUMION, which uses a novel reconfigurable optical fabric to connect accelerators within a datacenter rack. When a failure occurs, LUMION dynamically integrates spare accelerators into ongoing workloads instead of migrating entire jobs.

Result: LUMION swaps a failed GPU with a healthy one and restarts the ML job within ~1 second after failure. It achieves higher inter-GPU bandwidth compared to traditional electrical racks, leading to nearly 2X improvement in fine-tuning throughput.

Conclusion: LUMION provides an efficient way to maintain consistent performance in ML datacenters without costly job migrations, by dynamically integrating spare accelerators into ongoing workloads.

Abstract: When accelerators fail in modern ML datacenters, operators migrate the
affected ML training or inference jobs to entirely new racks. This approach,
while preserving network performance, is highly inefficient, requiring
datacenters to reserve full racks of idle accelerators for fault tolerance. In
this paper, we address this resource inefficiency by introducing LUMION, a
novel reconfigurable optical fabric for connecting accelerators within a
datacenter rack. Instead of migrating entire ML jobs, LUMION dynamically
integrates spare accelerators into ongoing workloads as failures occur, thereby
maintaining consistent performance without costly migrations. We show the
benefits of LUMION by building an end-to-end hardware prototype. Our
experiments fine-tune Llama 3.2 and show that LUMION swaps a failed GPU with a
healthy one and restarts the ML job within ~ 1 second of the failure. LUMION
achieves higher inter-GPU bandwidth compared to traditional electrical racks
after replacing failed accelerators with spare ones, leading to nearly 2X
improvement in fine-tuning throughput.

</details>


### [103] [Neural Interpretable PDEs: Harmonizing Fourier Insights with Attention for Scalable and Interpretable Physics Discovery](https://arxiv.org/abs/2505.23106)
*Ning Liu,Yue Yu*

Main category: cs.LG

TL;DR: This paper presents Neural Interpretable PDEs (NIPS), an improved neural operator architecture based on Nonlocal Attention Operators (NAO). NIPS enhances predictive accuracy and computational efficiency through linear attention mechanisms and a learnable kernel network in Fourier space. It outperforms NAO and other baselines in physics learning tasks.


<details>
  <summary>Details</summary>
Motivation: Attention mechanisms have shown great success in AI domains like NLP and computer vision, but their potential for modeling complex physical systems remains largely unexplored. This work aims to leverage these mechanisms for learning intricate physical systems by discovering operators that map between functional spaces.

Method: The method involves introducing NIPS, which builds upon NAO by incorporating linear attention mechanisms and a learnable kernel network. The kernel network functions as a channel-independent convolution in Fourier space, allowing scalable learning without explicitly computing large pairwise interactions.

Result: Empirical evaluations show that NIPS consistently surpasses NAO and other baselines across various benchmarks, indicating significant improvements in both performance and efficiency.

Conclusion: NIPS represents a substantial advancement in the field of interpretable and efficient physics learning, offering scalable solutions for modeling complex physical systems.

Abstract: Attention mechanisms have emerged as transformative tools in core AI domains
such as natural language processing and computer vision. Yet, their largely
untapped potential for modeling intricate physical systems presents a
compelling frontier. Learning such systems often entails discovering operators
that map between functional spaces using limited instances of function pairs --
a task commonly framed as a severely ill-posed inverse PDE problem. In this
work, we introduce Neural Interpretable PDEs (NIPS), a novel neural operator
architecture that builds upon and enhances Nonlocal Attention Operators (NAO)
in both predictive accuracy and computational efficiency. NIPS employs a linear
attention mechanism to enable scalable learning and integrates a learnable
kernel network that acts as a channel-independent convolution in Fourier space.
As a consequence, NIPS eliminates the need to explicitly compute and store
large pairwise interactions, effectively amortizing the cost of handling
spatial interactions into the Fourier transform. Empirical evaluations
demonstrate that NIPS consistently surpasses NAO and other baselines across
diverse benchmarks, heralding a substantial leap in scalable, interpretable,
and efficient physics learning. Our code and data accompanying this paper are
available at https://github.com/fishmoon1234/Nonlocal-Attention-Operator.

</details>


### [104] [CrossLinear: Plug-and-Play Cross-Correlation Embedding for Time Series Forecasting with Exogenous Variables](https://arxiv.org/abs/2505.23116)
*Pengfei Zhou,Yunlong Liu,Junli Liang,Qi Song,Xiangyang Li*

Main category: cs.LG

TL;DR: The paper introduces CrossLinear, a novel Linear-based forecasting model with a cross-correlation embedding module that captures dependencies between variables efficiently and mitigates overfitting. It employs patch-wise processing and a global linear head for capturing temporal dependencies, leading to superior performance in time series forecasting tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional models struggle to differentiate between endogenous and exogenous variables, resulting in inefficiencies and overfitting when modeling dependencies.

Method: CrossLinear incorporates a plug-and-play cross-correlation embedding module that captures time-invariant and direct variable dependencies while ignoring time-varying or indirect ones. It uses patch-wise processing and a global linear head to capture short-term and long-term temporal dependencies.

Result: Extensive experiments on 12 real-world datasets show superior performance in both short-term and long-term forecasting tasks. The ablation study highlights the effectiveness of the cross-correlation embedding module.

Conclusion: CrossLinear achieves consistent performance improvements and its cross-correlation embedding module is a valuable plug-in for various forecasting tasks across different domains.

Abstract: Time series forecasting with exogenous variables is a critical emerging
paradigm that presents unique challenges in modeling dependencies between
variables. Traditional models often struggle to differentiate between
endogenous and exogenous variables, leading to inefficiencies and overfitting.
In this paper, we introduce CrossLinear, a novel Linear-based forecasting model
that addresses these challenges by incorporating a plug-and-play
cross-correlation embedding module. This lightweight module captures the
dependencies between variables with minimal computational cost and seamlessly
integrates into existing neural networks. Specifically, it captures
time-invariant and direct variable dependencies while disregarding time-varying
or indirect dependencies, thereby mitigating the risk of overfitting in
dependency modeling and contributing to consistent performance improvements.
Furthermore, CrossLinear employs patch-wise processing and a global linear head
to effectively capture both short-term and long-term temporal dependencies,
further improving its forecasting precision. Extensive experiments on 12
real-world datasets demonstrate that CrossLinear achieves superior performance
in both short-term and long-term forecasting tasks. The ablation study
underscores the effectiveness of the cross-correlation embedding module.
Additionally, the generalizability of this module makes it a valuable plug-in
for various forecasting tasks across different domains. Codes are available at
https://github.com/mumiao2000/CrossLinear.

</details>


### [105] [Decom-Renorm-Merge: Model Merging on the Right Space Improves Multitasking](https://arxiv.org/abs/2505.23117)
*Yuatyong Chaichana,Thanapat Trachu,Peerat Limkonchotiwat,Konpat Preechakul,Tirasan Khandhawit,Ekapol Chuangsuwanich*

Main category: cs.LG

TL;DR: In the era of large-scale training, model merging is important for creating multitasking models efficiently. Existing methods have limitations due to the complexity of finetuned neural networks. This paper presents Decom-Renorm-Merge (DRM), a new approach that uses Singular Value Decomposition to align weight matrices into a joint space for effective merging. DRM outperforms several state-of-the-art techniques in various settings.


<details>
  <summary>Details</summary>
Motivation: Model merging is crucial for efficient creation of multitasking models in large-scale training. Current merging methods assume identical positions in weight matrices serve the same function, which is problematic in finetuned neural networks where neurons develop distinct feature compositions.

Method: The paper proposes Decom-Renorm-Merge (DRM) approach that leverages Singular Value Decomposition to decompose and coordinate weight matrices into an aligned joint space, enabling entry-wise merging. Renormalization is identified as a crucial component for creating a robust joint space.

Result: DRM outperforms several state-of-the-art merging techniques across full finetuning and low-rank adaptation settings in various models including ViT, DeBERTa, T5, and Llama3.1-8B.

Conclusion: Decom-Renorm-Merge (DRM) is an effective approach for model merging that addresses the limitations of current methods by using Singular Value Decomposition and renormalization to create a robust joint space.

Abstract: In the era of large-scale training, model merging has evolved into a tool for
creating multitasking models efficiently. It enables the knowledge of models to
be fused, without the need for heavy computation as required in traditional
multitask learning. Existing merging methods often assume that entries at
identical positions in weight matrices serve the same function, enabling
straightforward entry-wise comparison and merging. However, this assumption
overlooks the complexity of finetuned neural networks, where neurons may
develop distinct feature compositions, making direct entry-wise merging
problematic. We present Decom-Renorm-Merge (DRM), a simple yet effective
approach that leverages Singular Value Decomposition to decompose and
coordinate weight matrices into an aligned joint space, where entry-wise
merging becomes possible. We showcase the effectiveness of DRM across various
settings ranging from smaller encoder-based such as ViT and DeBERTa,
encoder-decoder-based such as T5, and larger decoder-based such as Llama3.1-8B.
Our experimental results show that DRM outperforms several state-of-the-art
merging techniques across full finetuning and low-rank adaptation settings.
Moreover, our analysis reveals renormalization as the crucial component for
creating a robust and even joint space for merging, significantly contributing
to the method's performance.

</details>


### [106] [DOPPLER: Dual-Policy Learning for Device Assignment in Asynchronous Dataflow Graphs](https://arxiv.org/abs/2505.23131)
*Xinyu Yao,Daniel Bourgeois,Abhinav Jain,Yuxin Tang,Jiawen Yao,Zhimin Ding,Arlei Silva,Chris Jermaine*

Main category: cs.LG

TL;DR: This paper proposes Doppler, a three-stage framework for training dual-policy networks to assign operations in a dataflow graph to devices, aiming to minimize execution time for complex machine learning workloads.


<details>
  <summary>Details</summary>
Motivation: Existing learning-based methods face challenges due to reliance on bulk-synchronous systems, lack of awareness of scheduling mechanisms, and exclusive dependence on reinforcement learning.

Method: The proposed method, Doppler, is a three-stage framework that trains dual-policy networks including a SEL policy for selecting operations and a PLC policy for placing chosen operations on devices.

Result: Doppler outperforms all baseline methods by reducing system execution time across tasks and demonstrates sampling efficiency by reducing per-episode training time.

Conclusion: Doppler is an effective approach for minimizing execution time in complex machine learning workloads.

Abstract: We study the problem of assigning operations in a dataflow graph to devices
to minimize execution time in a work-conserving system, with emphasis on
complex machine learning workloads. Prior learning-based methods often struggle
due to three key limitations: (1) reliance on bulk-synchronous systems like
TensorFlow, which under-utilize devices due to barrier synchronization; (2)
lack of awareness of the scheduling mechanism of underlying systems when
designing learning-based methods; and (3) exclusive dependence on reinforcement
learning, ignoring the structure of effective heuristics designed by experts.
In this paper, we propose \textsc{Doppler}, a three-stage framework for
training dual-policy networks consisting of 1) a $\mathsf{SEL}$ policy for
selecting operations and 2) a $\mathsf{PLC}$ policy for placing chosen
operations on devices. Our experiments show that \textsc{Doppler} outperforms
all baseline methods across tasks by reducing system execution time and
additionally demonstrates sampling efficiency by reducing per-episode training
time.

</details>


### [107] [VERINA: Benchmarking Verifiable Code Generation](https://arxiv.org/abs/2505.23135)
*Zhe Ye,Zhengxu Yan,Jingxuan He,Timothe Kasriel,Kaiyu Yang,Dawn Song*

Main category: cs.LG

TL;DR: Verina is a benchmark for evaluating verifiable code generation, revealing significant challenges in proof generation and correctness of LLM-generated code.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of ensuring correctness in LLM-generated code and improve LLM-based theorem provers in verification domains by providing a comprehensive benchmark for evaluation.

Method: Introduced Verina, a high-quality benchmark with 189 manually curated coding tasks in Lean, including detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Evaluated state-of-the-art LLMs on this benchmark.

Result: State-of-the-art LLMs face significant challenges in verifiable code generation, especially in proof generation. The best model generates only 61.4% correct code, 51.0% sound and complete specifications, and 3.6% successful proofs.

Conclusion: Verina can catalyze progress in verifiable code generation by offering a rigorous and comprehensive benchmark.

Abstract: Large language models (LLMs) are increasingly integrated in software
development, but ensuring correctness in LLM-generated code remains challenging
and often requires costly manual review. Verifiable code generation -- jointly
generating code, specifications, and proofs of code-specification alignment --
offers a promising path to address this limitation and further unleash LLMs'
benefits in coding. Yet, there exists a significant gap in evaluation: current
benchmarks often lack support for end-to-end verifiable code generation. In
this paper, we introduce Verina (Verifiable Code Generation Arena), a
high-quality benchmark enabling a comprehensive and modular evaluation of code,
specification, and proof generation as well as their compositions. Verina
consists of 189 manually curated coding tasks in Lean, with detailed problem
descriptions, reference implementations, formal specifications, and extensive
test suites. Our extensive evaluation of state-of-the-art LLMs reveals
significant challenges in verifiable code generation, especially in proof
generation, underscoring the need for improving LLM-based theorem provers in
verification domains. The best model, OpenAI o4-mini, generates only 61.4%
correct code, 51.0% sound and complete specifications, and 3.6% successful
proofs, with one trial per task. We hope Verina will catalyze progress in
verifiable code generation by providing a rigorous and comprehensive benchmark.
We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina
and our evaluation code on https://github.com/sunblaze-ucb/verina.

</details>


### [108] [Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners](https://arxiv.org/abs/2505.23150)
*Michal Nauman,Marek Cygan,Carmelo Sferrazza,Aviral Kumar,Pieter Abbeel*

Main category: cs.LG

TL;DR: 通过使用高容量值模型，采用交叉熵训练并基于可学习的任务嵌入进行调节，可以解决在线RL中的任务干扰问题，从而实现稳健且可扩展的多任务训练。在多个基准测试中，该方法表现出色，并能有效转移到新任务。


<details>
  <summary>Details</summary>
Motivation: 价值导向的强化学习（RL）领域中，尽管大型模型和多任务数据在语言建模和视觉方面取得了进展，但在RL中，由于稀疏奖励和梯度冲突导致的时间差分优化脆弱性，多任务RL受到限制。现有的通用策略工作流通常避免在线训练，而是克隆专家轨迹或将单任务策略集合蒸馏成一个代理。

Method: 使用高容量的价值模型，通过交叉熵进行训练，并基于可学习的任务嵌入进行条件化。此方法旨在解决多任务在线RL中的任务干扰问题，从而实现稳健和可扩展的多任务训练。

Result: 在7个多任务基准测试中，涵盖了超过280个独特任务，包括高自由度的人形控制和基于离散视觉的RL，该方法不仅在单任务和多任务性能上达到最先进的水平，还实现了样本高效的向新任务转移。

Conclusion: 提出的方法简单却有效，解决了多任务在线RL中的关键挑战，并显著提升了性能和泛化能力，为未来的研究提供了新的方向。

Abstract: Recent advances in language modeling and vision stem from training large
models on diverse, multi-task data. This paradigm has had limited impact in
value-based reinforcement learning (RL), where improvements are often driven by
small models trained in a single-task context. This is because in multi-task RL
sparse rewards and gradient conflicts make optimization of temporal difference
brittle. Practical workflows for generalist policies therefore avoid online
training, instead cloning expert trajectories or distilling collections of
single-task policies into one agent. In this work, we show that the use of
high-capacity value models trained via cross-entropy and conditioned on
learnable task embeddings addresses the problem of task interference in online
RL, allowing for robust and scalable multi-task training. We test our approach
on 7 multi-task benchmarks with over 280 unique tasks, spanning high
degree-of-freedom humanoid control and discrete vision-based RL. We find that,
despite its simplicity, the proposed approach leads to state-of-the-art single
and multi-task performance, as well as sample-efficient transfer to new tasks.

</details>


### [109] [Best Arm Identification with Possibly Biased Offline Data](https://arxiv.org/abs/2505.23165)
*Le Yang,Vincent Y. F. Tan,Wang Chi Cheung*

Main category: cs.LG

TL;DR: The paper investigates the best arm identification (BAI) problem with potentially biased offline data and proposes LUCB-H algorithm which introduces adaptive confidence bounds.


<details>
  <summary>Details</summary>
Motivation: To solve the BAI problem with potentially biased offline data in the fixed confidence setting, which commonly arises in real-world scenarios such as clinical trials.

Method: Propose the LUCB-H algorithm that incorporates an auxiliary bias correction to balance offline and online data within the LUCB framework. Theoretical analysis shows its sample complexity and numerical experiments demonstrate its robustness and adaptability.

Result: LUCB-H matches the sample complexity of standard LUCB when offline data is misleading and significantly outperforms it when offline data is helpful. An instance-dependent lower bound is derived that matches the upper bound of LUCB-H in certain scenarios.

Conclusion: LUCB-H effectively incorporates offline data for the BAI problem and is robust and adaptable.

Abstract: We study the best arm identification (BAI) problem with potentially biased
offline data in the fixed confidence setting, which commonly arises in
real-world scenarios such as clinical trials. We prove an impossibility result
for adaptive algorithms without prior knowledge of the bias bound between
online and offline distributions. To address this, we propose the LUCB-H
algorithm, which introduces adaptive confidence bounds by incorporating an
auxiliary bias correction to balance offline and online data within the LUCB
framework. Theoretical analysis shows that LUCB-H matches the sample complexity
of standard LUCB when offline data is misleading and significantly outperforms
it when offline data is helpful. We also derive an instance-dependent lower
bound that matches the upper bound of LUCB-H in certain scenarios. Numerical
experiments further demonstrate the robustness and adaptability of LUCB-H in
effectively incorporating offline data.

</details>


### [110] [Pseudo Multi-Source Domain Generalization: Bridging the Gap Between Single and Multi-Source Domain Generalization](https://arxiv.org/abs/2505.23173)
*Shohei Enomoto*

Main category: cs.LG

TL;DR: Deep learning models may not perform well on data different from their training data. Multi-source Domain Generalization (MDG) can help, but it's costly to apply in real-world situations. We propose Pseudo Multi-source Domain Generalization (PMDG), which generates multiple pseudo-domains from a single source domain using style transfer and data augmentation techniques. This allows the use of MDG algorithms in Single-source Domain Generalization settings. Experiments show PMDG correlates positively with MDG and can match or exceed multi-domain performance with enough data.


<details>
  <summary>Details</summary>
Motivation: Deep learning models often fail when dealing with data distributions different from their training data. Although MDG has shown promise, its practical application is limited due to high costs and difficulties in creating multi-domain datasets.

Method: PMDG generates multiple pseudo-domains from a single source domain through style transfer and data augmentation techniques, allowing the use of sophisticated MDG algorithms in SDG settings.

Result: Experiments demonstrate a positive correlation between MDG and PMDG performance. With sufficient data, pseudo-domains can match or even outperform actual multi-domain results.

Conclusion: PMDG provides a practical way to leverage MDG algorithms in SDG settings by generating pseudo-domains. This approach shows potential for future research in domain generalization.

Abstract: Deep learning models often struggle to maintain performance when deployed on
data distributions different from their training data, particularly in
real-world applications where environmental conditions frequently change. While
Multi-source Domain Generalization (MDG) has shown promise in addressing this
challenge by leveraging multiple source domains during training, its practical
application is limited by the significant costs and difficulties associated
with creating multi-domain datasets. To address this limitation, we propose
Pseudo Multi-source Domain Generalization (PMDG), a novel framework that
enables the application of sophisticated MDG algorithms in more practical
Single-source Domain Generalization (SDG) settings. PMDG generates multiple
pseudo-domains from a single source domain through style transfer and data
augmentation techniques, creating a synthetic multi-domain dataset that can be
used with existing MDG algorithms. Through extensive experiments with
PseudoDomainBed, our modified version of the DomainBed benchmark, we analyze
the effectiveness of PMDG across multiple datasets and architectures. Our
analysis reveals several key findings, including a positive correlation between
MDG and PMDG performance and the potential of pseudo-domains to match or exceed
actual multi-domain performance with sufficient data. These comprehensive
empirical results provide valuable insights for future research in domain
generalization. Our code is available at
https://github.com/s-enmt/PseudoDomainBed.

</details>


### [111] [The Panaceas for Improving Low-Rank Decomposition in Communication-Efficient Federated Learning](https://arxiv.org/abs/2505.23176)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Shijie Xu,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: This paper aims to enhance the performance of low-rank decomposition methods in federated learning (FL) through three novel techniques.


<details>
  <summary>Details</summary>
Motivation: Previous research has employed low-rank decomposition techniques to reduce communication overhead in FL, but there is room for improvement in terms of performance.

Method: The authors introduce three novel techniques: Model Update Decomposition (MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware Decomposition (AAD). These techniques address specific issues related to decomposition in FL and can be applied simultaneously. Additionally, a rigorous theoretical analysis ensures the convergence of MUD.

Result: Extensive experimental results demonstrate faster convergence and superior accuracy compared to relevant baseline methods.

Conclusion: The proposed approach significantly improves the performance of low-rank decomposition methods in FL.

Abstract: To improve the training efficiency of federated learning (FL), previous
research has employed low-rank decomposition techniques to reduce communication
overhead. In this paper, we seek to enhance the performance of these low-rank
decomposition methods. Specifically, we focus on three key issues related to
decomposition in FL: what to decompose, how to decompose, and how to aggregate.
Subsequently, we introduce three novel techniques: Model Update Decomposition
(MUD), Block-wise Kronecker Decomposition (BKD), and Aggregation-Aware
Decomposition (AAD), each targeting a specific issue. These techniques are
complementary and can be applied simultaneously to achieve optimal performance.
Additionally, we provide a rigorous theoretical analysis to ensure the
convergence of the proposed MUD. Extensive experimental results show that our
approach achieves faster convergence and superior accuracy compared to relevant
baseline methods. The code is available at
https://github.com/Leopold1423/fedmud-icml25.

</details>


### [112] [FreRA: A Frequency-Refined Augmentation for Contrastive Learning on Time Series Classification](https://arxiv.org/abs/2505.23181)
*Tian Tian,Chunyan Miao,Hangwei Qian*

Main category: cs.LG

TL;DR: 提出了一种名为Frequency Refined Augmentation (FreRA) 的新方法，专门用于时间序列对比学习中的分类任务。FreRA通过频率域的改进增强策略，自动分离关键和不重要的频率成分，并通过两种修改方式保护语义相关的信息，同时引入变化以增强模型的泛化能力。实验结果表明，FreRA在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列增强方法主要借鉴自视觉领域，未针对时间序列数据的特点进行优化，可能导致语义信息失真。因此，需要一种更适合时间序列分类任务的增强策略。

Method: 从频率域角度出发，提出了一种轻量且有效的增强方法——Frequency Refined Augmentation (FreRA)。该方法能够自动分离关键和不重要的频率成分，并分别采用语义感知的身份修改和语义无关的自适应修改策略来保护语义信息并引入多样性。

Result: 在两个基准数据集（UCR和UEA）以及五个大规模数据集上的实验表明，FreRA在时间序列分类、异常检测和迁移学习任务中均显著优于十个领先基线方法，展现了其在对比表示学习和迁移学习场景中的优越性能和泛化能力。

Conclusion: Frequency Refined Augmentation (FreRA) 是一种专为时间序列对比学习设计的有效增强方法，能够在保护语义信息的同时提高模型的泛化能力，适用于多种任务和数据集。

Abstract: Contrastive learning has emerged as a competent approach for unsupervised
representation learning. However, the design of an optimal augmentation
strategy, although crucial for contrastive learning, is less explored for time
series classification tasks. Existing predefined time-domain augmentation
methods are primarily adopted from vision and are not specific to time series
data. Consequently, this cross-modality incompatibility may distort the
semantically relevant information of time series by introducing mismatched
patterns into the data. To address this limitation, we present a novel
perspective from the frequency domain and identify three advantages for
downstream classification: global, independent, and compact. To fully utilize
the three properties, we propose the lightweight yet effective Frequency
Refined Augmentation (FreRA) tailored for time series contrastive learning on
classification tasks, which can be seamlessly integrated with contrastive
learning frameworks in a plug-and-play manner. Specifically, FreRA
automatically separates critical and unimportant frequency components.
Accordingly, we propose semantic-aware Identity Modification and
semantic-agnostic Self-adaptive Modification to protect semantically relevant
information in the critical frequency components and infuse variance into the
unimportant ones respectively. Theoretically, we prove that FreRA generates
semantic-preserving views. Empirically, we conduct extensive experiments on two
benchmark datasets, including UCR and UEA archives, as well as five large-scale
datasets on diverse applications. FreRA consistently outperforms ten leading
baselines on time series classification, anomaly detection, and transfer
learning tasks, demonstrating superior capabilities in contrastive
representation learning and generalization in transfer learning scenarios
across diverse datasets.

</details>


### [113] [FSL-SAGE: Accelerating Federated Split Learning via Smashed Activation Gradient Estimation](https://arxiv.org/abs/2505.23182)
*Srijith Nair,Michael Lin,Amirreza Talebi,Peizhong Ju,Elizabeth Bentley,Jia Liu*

Main category: cs.LG

TL;DR: The paper introduces FSL-SAGE, a new federated split learning algorithm that estimates server-side gradient feedback via auxiliary models to reduce communication costs and client memory requirements while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated Learning (FL) and Split Learning (SL) have their own limitations - FL requires clients to train entire models which is infeasible for large-scale models and SL increases network latency due to its sequential nature. Other methods addressing these issues lack server feedback leading to potentially poor accuracy.

Method: Propose FSL-SAGE, a federated split learning algorithm using auxiliary models to periodically adapt and emulate server behavior on local datasets. This method estimates server-side gradient feedback via auxiliary models.

Result: Achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, matching FedAvg, while significantly reducing communication costs and client memory requirements. Empirical results show it outperforms existing state-of-the-art FSL methods in terms of both communication efficiency and accuracy.

Conclusion: FSL-SAGE offers an efficient solution to the challenges faced by FL and SL, providing a balance between communication efficiency and model accuracy.

Abstract: Collaborative training methods like Federated Learning (FL) and Split
Learning (SL) enable distributed machine learning without sharing raw data.
However, FL assumes clients can train entire models, which is infeasible for
large-scale models. In contrast, while SL alleviates the client memory
constraint in FL by offloading most training to the server, it increases
network latency due to its sequential nature. Other methods address the
conundrum by using local loss functions for parallel client-side training to
improve efficiency, but they lack server feedback and potentially suffer poor
accuracy. We propose FSL-SAGE (Federated Split Learning via Smashed Activation
Gradient Estimation), a new federated split learning algorithm that estimates
server-side gradient feedback via auxiliary models. These auxiliary models
periodically adapt to emulate server behavior on local datasets. We show that
FSL-SAGE achieves a convergence rate of $\mathcal{O}(1/\sqrt{T})$, where $T$ is
the number of communication rounds. This result matches FedAvg, while
significantly reducing communication costs and client memory requirements. Our
empirical results also verify that it outperforms existing state-of-the-art FSL
methods, offering both communication efficiency and accuracy.

</details>


### [114] [Two Is Better Than One: Rotations Scale LoRAs](https://arxiv.org/abs/2505.23184)
*Hongcan Guo,Guoshun Nan,Yuan Yang,Diyang Zhang,Haotian Li,Zhican Chen,Qinchuan Zhou,Yuhan Ran,Xinye Cao,Sicong Leng,Xiaofeng Tao,Xudong Jiang*

Main category: cs.LG

TL;DR: RadarGate is a novel gating method for large language models that uses rotational operations of LoRA representations to improve expressiveness and feature interactions.


<details>
  <summary>Details</summary>
Motivation: Traditional gating mechanisms in low-rank adaptation based mixture-of-experts models hinder scalability of large language models, leading to poor generalization and underfitting issues due to restricted expressiveness of weighted-sum mechanisms.

Method: Propose RadarGate, which fuses each LoRA representation with others using a learnable component and applies a rotation matrix with learnable parameters defining angular relationships between LoRA representations.

Result: Extensive experiments across 21 tasks on 6 public benchmarks demonstrate the effectiveness of RadarGate in scaling LoRAs, showing contrastive rotations that align semantically similar representations and separate dissimilar ones.

Conclusion: RadarGate effectively boosts expressiveness and feature interactions among multiple LoRAs, addressing scalability issues in large language models; code will be released.

Abstract: Scaling Low-Rank Adaptation (LoRA)-based Mixture-of-Experts (MoE) facilitates
large language models (LLMs) to efficiently adapt to diverse tasks. However,
traditional gating mechanisms that route inputs to the best experts may
fundamentally hinder LLMs' scalability, leading to poor generalization and
underfitting issues. We identify that the root cause lies in the restricted
expressiveness of existing weighted-sum mechanisms, both within and outside the
convex cone of LoRA representations. This motivates us to propose RadarGate, a
novel geometrically inspired gating method that introduces rotational
operations of LoRAs representations to boost the expressiveness and facilitate
richer feature interactions among multiple LoRAs for scalable LLMs.
Specifically, we first fuse each LoRA representation to other LoRAs using a
learnable component and then feed the output to a rotation matrix. This matrix
involves learnable parameters that define the relative angular relationship
between LoRA representations. Such a simple yet effective mechanism provides an
extra degree of freedom, facilitating the learning of cross-LoRA synergies and
properly tracking the challenging poor generalization and underfitting issues
as the number of LoRA grows. Extensive experiments on 6 public benchmarks
across 21 tasks show the effectiveness of our RadarGate for scaling LoRAs. We
also provide valuable insights, revealing that the rotations to each pair of
representations are contrastive, encouraging closer alignment of semantically
similar representations during geometrical transformation while pushing
distance ones further apart. We will release our code to the community.

</details>


### [115] [Improving the Effective Receptive Field of Message-Passing Neural Networks](https://arxiv.org/abs/2505.23185)
*Shahaf E. Finder,Ron Shapira Weber,Moshe Eliasof,Oren Freifeld,Eran Treister*

Main category: cs.LG

TL;DR: Message-Passing Neural Networks (MPNNs) face challenges with effectively capturing long-range dependencies due to issues like over-squashing and limited Effective Receptive Field (ERF). Inspired by advancements in ERF augmentation for CNNs, this paper proposes Interleaved Multiscale Message-Passing Neural Networks (IM-MPNN) to address these problems. IM-MPNN uses hierarchical graph coarsening to enable multiscale message-passing and improve the capture of long-range interactions without increasing depth or parameters excessively.


<details>
  <summary>Details</summary>
Motivation: MPNNs are effective for analyzing graph-structured data but suffer from limitations such as over-squashing that hinder their ability to capture long-range dependencies. This limitation is analogous to the ERF problem in CNNs.

Method: The proposed IM-MPNN architecture incorporates a hierarchical coarsening of the graph structure, allowing message-passing across different scales of the graph. This facilitates better capture of long-range dependencies without requiring excessive depth or parameterization.

Result: Extensive evaluations on benchmarks like LRGB show significant improvements over baseline MPNNs in terms of capturing long-range dependencies while maintaining computational efficiency.

Conclusion: The IM-MPNN architecture addresses the issue of limited ERF in MPNNs by enabling multiscale message-passing, leading to better performance in tasks requiring the analysis of long-range dependencies in graph-structured data.

Abstract: Message-Passing Neural Networks (MPNNs) have become a cornerstone for
processing and analyzing graph-structured data. However, their effectiveness is
often hindered by phenomena such as over-squashing, where long-range
dependencies or interactions are inadequately captured and expressed in the
MPNN output. This limitation mirrors the challenges of the Effective Receptive
Field (ERF) in Convolutional Neural Networks (CNNs), where the theoretical
receptive field is underutilized in practice. In this work, we show and
theoretically explain the limited ERF problem in MPNNs. Furthermore, inspired
by recent advances in ERF augmentation for CNNs, we propose an Interleaved
Multiscale Message-Passing Neural Networks (IM-MPNN) architecture to address
these problems in MPNNs. Our method incorporates a hierarchical coarsening of
the graph, enabling message-passing across multiscale representations and
facilitating long-range interactions without excessive depth or
parameterization. Through extensive evaluations on benchmarks such as the
Long-Range Graph Benchmark (LRGB), we demonstrate substantial improvements over
baseline MPNNs in capturing long-range dependencies while maintaining
computational efficiency.

</details>


### [116] [DeepRTE: Pre-trained Attention-based Neural Network for Radiative Tranfer](https://arxiv.org/abs/2505.23190)
*Yekun Zhu,Min Tang,Zheng Ma*

Main category: cs.LG

TL;DR: A new neural network method called DeepRTE is developed to solve the Radiative Transfer Equation with high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the steady-state Radiative Transfer Equation using a novel approach that can achieve both high accuracy and computational efficiency.

Method: The proposed method, DeepRTE, utilizes pre-trained attention-based neural networks to solve the Radiative Transfer Equation.

Result: The efficacy of DeepRTE is validated through comprehensive numerical experiments, demonstrating its high accuracy and computational efficiency.

Conclusion: DeepRTE offers a promising solution for solving the Radiative Transfer Equation across various applications.

Abstract: In this study, we propose a novel neural network approach, termed DeepRTE, to
address the steady-state Radiative Transfer Equation (RTE). The RTE is a
differential-integral equation that governs the propagation of radiation
through a participating medium, with applications spanning diverse domains such
as neutron transport, atmospheric radiative transfer, heat transfer, and
optical imaging. Our proposed DeepRTE framework leverages pre-trained
attention-based neural networks to solve the RTE with high accuracy and
computational efficiency. The efficacy of the proposed approach is
substantiated through comprehensive numerical experiments.

</details>


### [117] [Beyond Zero Initialization: Investigating the Impact of Non-Zero Initialization on LoRA Fine-Tuning Dynamics](https://arxiv.org/abs/2505.23194)
*Shiwei Li,Xiandi Luo,Xing Tang,Haozhao Wang,Hao Chen,Weihong Luo,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: The paper explores the effect of non-zero initialization on LoRA's fine-tuning dynamics, showing it improves robustness to suboptimal learning rates and doesn't negatively impact performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether starting fine-tuning from pretrained models with zero-initialized matrices is necessary or if non-zero initialization could offer advantages.

Method: Analyze the impact of simultaneously initializing matrices A and B to non-zero values in LoRA layers from an infinite-width perspective and compare it against the standard zero initialization approach.

Result: Non-zero initialization enhances LoRA's robustness to suboptimal (especially smaller) learning rates without negatively affecting fine-tuning performance due to introduced random noise.

Conclusion: Fine-tuning does not need to strictly start from the pretrained model; non-zero initialization can be a viable alternative, supported by extensive experimental evidence.

Abstract: Low-rank adaptation (LoRA) is a widely used parameter-efficient fine-tuning
method. In standard LoRA layers, one of the matrices, $A$ or $B$, is
initialized to zero, ensuring that fine-tuning starts from the pretrained
model. However, there is no theoretical support for this practice. In this
paper, we investigate the impact of non-zero initialization on LoRA's
fine-tuning dynamics from an infinite-width perspective. Our analysis reveals
that, compared to zero initialization, simultaneously initializing $A$ and $B$
to non-zero values improves LoRA's robustness to suboptimal learning rates,
particularly smaller ones. Further analysis indicates that although the
non-zero initialization of $AB$ introduces random noise into the pretrained
weight, it generally does not affect fine-tuning performance. In other words,
fine-tuning does not need to strictly start from the pretrained model. The
validity of our findings is confirmed through extensive experiments across
various models and datasets. The code is available at
https://github.com/Leopold1423/non_zero_lora-icml25.

</details>


### [118] [Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning](https://arxiv.org/abs/2505.23195)
*Lifan Zhao,Yanyan Shen,Zhaoyang Liu,Xue Wang,Jiaji Deng*

Main category: cs.LG

TL;DR: Scaling laws inspire Time Series Foundation Models (TSFMs) with vast parameters for zero-shot forecasting. However, fine-tuned TSFMs often underperform specialized models. This paper addresses effective TSFM adaptation via empirical studies revealing sparsity and redundancy in pre-trained models. A structured pruning method is proposed to focus fine-tuning on task-relevant network substructures, improving performance. Experiments show 'prune-then-finetune' enhances TSFM forecasting, surpassing specialized baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation is the observation that despite their large scale and strong zero-shot performance, TSFMs do not consistently outperform smaller, specialized models after fine-tuning. This raises the question of how to effectively adapt TSFMs for target forecasting tasks.

Method: Through empirical studies, the authors identify sparsity and redundancy in pre-trained TSFMs. They propose a structured pruning method to regularize fine-tuning by focusing on task-relevant network substructures, preserving valuable prior knowledge.

Result: Experiments on seven TSFMs and six benchmarks demonstrate that pruning before fine-tuning significantly improves forecasting performance compared to fine-tuning the original models.

Conclusion: The 'prune-then-finetune' paradigm allows TSFMs to achieve state-of-the-art performance, surpassing strong specialized baselines.

Abstract: Scaling laws motivate the development of Time Series Foundation Models
(TSFMs) that pre-train vast parameters and achieve remarkable zero-shot
forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot
consistently outperform smaller, specialized models trained on full-shot
downstream data. A key question is how to realize effective adaptation of TSFMs
for a target forecasting task. Through empirical studies on various TSFMs, the
pre-trained models often exhibit inherent sparsity and redundancy in
computation, suggesting that TSFMs have learned to activate task-relevant
network substructures to accommodate diverse forecasting tasks. To preserve
this valuable prior knowledge, we propose a structured pruning method to
regularize the subsequent fine-tuning process by focusing it on a more relevant
and compact parameter space. Extensive experiments on seven TSFMs and six
benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly
improves forecasting performance compared to fine-tuning original models. This
"prune-then-finetune" paradigm often enables TSFMs to achieve state-of-the-art
performance and surpass strong specialized baselines.

</details>


### [119] [Daunce: Data Attribution through Uncertainty Estimation](https://arxiv.org/abs/2505.23223)
*Xingyuan Pan,Chenlu Ye,Joseph Melkonian,Jiaqi W. Ma,Tong Zhang*

Main category: cs.LG

TL;DR: Daunce is a scalable and accurate data attribution method that uses uncertainty estimation to identify training influences on test predictions.


<details>
  <summary>Details</summary>
Motivation: Current TDA methods either lack scalability or suffer from degraded accuracy. The authors aim to develop a method that balances both scalability and accuracy.

Method: Daunce fine-tunes perturbed models and computes the covariance of per-example losses across these models as the attribution score.

Result: Daunce achieves more accurate attribution compared to existing TDA methods, and it is the first instance of data attribution on proprietary LLMs like OpenAI's GPT models.

Conclusion: Daunce provides a simple yet effective approach for data attribution through uncertainty estimation, supporting applications such as data debugging and valuation.

Abstract: Training data attribution (TDA) methods aim to identify which training
examples influence a model's predictions on specific test data most. By
quantifying these influences, TDA supports critical applications such as data
debugging, curation, and valuation. Gradient-based TDA methods rely on
gradients and second-order information, limiting their applicability at scale.
While recent random projection-based methods improve scalability, they often
suffer from degraded attribution accuracy. Motivated by connections between
uncertainty and influence functions, we introduce Daunce - a simple yet
effective data attribution approach through uncertainty estimation. Our method
operates by fine-tuning a collection of perturbed models and computing the
covariance of per-example losses across these models as the attribution score.
Daunce is scalable to large language models (LLMs) and achieves more accurate
attribution compared to existing TDA methods. We validate Daunce on tasks
ranging from vision tasks to LLM fine-tuning, and further demonstrate its
compatibility with black-box model access. Applied to OpenAI's GPT models, our
method achieves, to our knowledge, the first instance of data attribution on
proprietary LLMs.

</details>


### [120] [Generalizability vs. Counterfactual Explainability Trade-Off](https://arxiv.org/abs/2505.23225)
*Fabiano Veglianti,Flavio Giorgi,Fabrizio Silvestri,Gabriele Tolomei*

Main category: cs.LG

TL;DR: 本研究探讨了监督学习中模型泛化与反事实可解释性之间的关系，引入ε-有效反事实概率（ε-VCP）的概念，并通过理论和实证分析揭示了泛化性能差与反事实生成容易之间的内在权衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解模型泛化能力与反事实可解释性之间的联系，以及如何通过量化反事实生成的难易程度来反映模型的过拟合情况。

Method: 提出ε-有效反事实概率（ε-VCP）的概念，即在数据点的ε邻域内找到导致标签变化的扰动的概率，并分析其与模型决策边界几何形状的关系。

Result: 理论分析表明，ε-VCP随着模型过拟合而增加；实证结果验证了ε-VCP可以作为量化过拟合的实际代理指标。

Conclusion: 建立了模型泛化能力与反事实可解释性之间的严格联系，揭示了两者之间的内在权衡。

Abstract: In this work, we investigate the relationship between model generalization
and counterfactual explainability in supervised learning. We introduce the
notion of $\varepsilon$-valid counterfactual probability ($\varepsilon$-VCP) --
the probability of finding perturbations of a data point within its
$\varepsilon$-neighborhood that result in a label change. We provide a
theoretical analysis of $\varepsilon$-VCP in relation to the geometry of the
model's decision boundary, showing that $\varepsilon$-VCP tends to increase
with model overfitting. Our findings establish a rigorous connection between
poor generalization and the ease of counterfactual generation, revealing an
inherent trade-off between generalization and counterfactual explainability.
Empirical results validate our theory, suggesting $\varepsilon$-VCP as a
practical proxy for quantitatively characterizing overfitting.

</details>


### [121] [Graph Random Walk with Feature-Label Space Alignment: A Multi-Label Feature Selection Method](https://arxiv.org/abs/2505.23228)
*Wanfu Gao,Jun Gao,Qingqi Han,Hanlin Pan,Kunpeng Liu*

Main category: cs.LG

TL;DR: The paper proposes a method using random walk graph to capture complex nonlinear associations in multi-label datasets and aligns variable spaces through low-dimensional representation coefficients.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in feature dimension introduces implicit associations between features and labels, making their relationships increasingly complex. Existing methods with low-dimensional linear decomposition fail to capture these complex nonlinear associations leading to misalignment between feature and label spaces.

Method: A random walk graph is designed to integrate feature-feature, label-label, and feature-label relationships for capturing nonlinear and implicit indirect associations. Low-rank decomposition optimizes latent representations of these associations. Variable spaces are aligned by leveraging low-dimensional representation coefficients while preserving the manifold structure.

Result: Extensive experiments on seven benchmark datasets and three representative datasets using various evaluation metrics demonstrate the superiority of the proposed method.

Conclusion: The proposed method effectively captures complex nonlinear associations in multi-label datasets and successfully aligns the feature space with the label space.

Abstract: The rapid growth in feature dimension may introduce implicit associations
between features and labels in multi-label datasets, making the relationships
between features and labels increasingly complex. Moreover, existing methods
often adopt low-dimensional linear decomposition to explore the associations
between features and labels. However, linear decomposition struggles to capture
complex nonlinear associations and may lead to misalignment between the feature
space and the label space. To address these two critical challenges, we propose
innovative solutions. First, we design a random walk graph that integrates
feature-feature, label-label, and feature-label relationships to accurately
capture nonlinear and implicit indirect associations, while optimizing the
latent representations of associations between features and labels after
low-rank decomposition. Second, we align the variable spaces by leveraging
low-dimensional representation coefficients, while preserving the manifold
structure between the original high-dimensional multi-label data and the
low-dimensional representation space. Extensive experiments and ablation
studies conducted on seven benchmark datasets and three representative datasets
using various evaluation metrics demonstrate the superiority of the proposed
method\footnote{Code: https://github.com/Heilong623/-GRW-}.

</details>


### [122] [Equivalence of stochastic and deterministic policy gradients](https://arxiv.org/abs/2505.23244)
*Emo Todorov*

Main category: cs.LG

TL;DR: 在特定MDP条件下，确定性和随机性策略梯度、自然梯度和状态值函数相同，而状态-控制值函数不同，并提出了一种将随机策略MDP转换为等效确定性策略MDP的通用方法。


<details>
  <summary>Details</summary>
Motivation: 研究连续控制中随机和确定性策略梯度之间的关系，特别是在涉及高斯控制噪声和二次控制成本的广泛使用的MDP家族中。

Method: 1. 在具有高斯控制噪声和二次控制成本的MDP中比较随机和确定性策略梯度、自然梯度和值函数。
2. 开发一种构造与给定带有随机策略的MDP等效的新MDP的方法，新MDP使用原MDP中随机策略的充分统计量作为控制。
3. 基于以上发现，提出通过近似状态值函数而非状态-控制值函数来统一策略梯度方法。

Result: 在特定MDP条件下，确定性和随机性策略梯度、自然梯度和状态值函数相同，而状态-控制值函数不同。成功开发出一种将随机策略MDP转换为等效确定性策略MDP的通用方法。

Conclusion: 政策梯度方法可以通过近似状态值函数而不是状态-控制值函数实现统一。

Abstract: Policy gradients in continuous control have been derived for both stochastic
and deterministic policies. Here we study the relationship between the two. In
a widely-used family of MDPs involving Gaussian control noise and quadratic
control costs, we show that the stochastic and deterministic policy gradients,
natural gradients, and state value functions are identical; while the
state-control value functions are different. We then develop a general
procedure for constructing an MDP with deterministic policy that is equivalent
to a given MDP with stochastic policy. The controls of this new MDP are the
sufficient statistics of the stochastic policy in the original MDP. Our results
suggest that policy gradient methods can be unified by approximating state
value functions rather than state-control value functions.

</details>


### [123] [Measuring Participant Contributions in Decentralized Federated Learning](https://arxiv.org/abs/2505.23246)
*Honoka Anada,Tatsuya Kaneko,Shinya Takamaeda-Yamazaki*

Main category: cs.LG

TL;DR: 本研究提出了两种用于去中心化联邦学习（DFL）中参与者贡献度量的新方法：DFL-Shapley和DFL-MR。实验表明，DFL-Shapley是一个有效的基准度量，而DFL-MR是其良好的近似值。


<details>
  <summary>Details</summary>
Motivation: 尽管在集中式联邦学习（CFL）中有多种贡献度量方法，但它们无法直接应用于去中心化联邦学习（DFL），因为DFL缺乏中央服务器并且存在多个全局模型。因此，需要新的方法来度量DFL中的参与者贡献。

Method: 1. 提出了DFL-Shapley，这是Shapley值在DFL中的扩展版本。
2. 由于计算理想的DFL-Shapley不切实际，引入了DFL-MR作为可计算的近似方法，通过累积每轮的Shapley值来估计总体贡献。
3. 在各种FL场景下评估了DFL-Shapley和DFL-MR，并与现有的CFL指标进行了比较。

Result: 实验结果验证了DFL-Shapley作为一个有效的真实基准度量，同时证明了DFL-MR在不同设置下接近DFL-Shapley，显示了两者在DFL中作为贡献度量的有效性。

Conclusion: DFL-Shapley和DFL-MR为去中心化联邦学习提供了一种有效的参与者贡献度量方式，能够激励参与者并确保透明性。

Abstract: Federated learning (FL) enables multiple clients to collaboratively train
models without sharing their data. Measuring participant contributions in FL is
crucial for incentivizing clients and ensuring transparency. While various
methods have been proposed for contribution measurement, they are designed
exclusively for centralized federated learning (CFL), where a central server
collects and aggregates client models, along with evaluating their
contributions. Meanwhile, decentralized federated learning (DFL), in which
clients exchange models directly without a central server, has gained
significant attention for mitigating communication bottlenecks and eliminating
a single point of failure. However, applying existing contribution measurement
methods to DFL is challenging due to the presence of multiple global models and
the absence of a central server. In this study, we present novel methodologies
for measuring participant contributions in DFL. We first propose DFL-Shapley,
an extension of the Shapley value tailored for DFL, adapting this widely used
CFL metric to decentralized settings. Given the impracticality of computing the
ideal DFL-Shapley in real-world systems, we introduce DFL-MR, a computable
approximation that estimates overall contributions by accumulating round-wise
Shapley values. We evaluate DFL-Shapley and DFL-MR across various FL scenarios
and compare them with existing CFL metrics. The experimental results confirm
DFL-Shapley as a valid ground-truth metric and demonstrate DFL-MR's proximity
to DFL-Shapley across various settings, highlighting their effectiveness as
contribution metrics in DFL.

</details>


### [124] [Accelerating RLHF Training with Reward Variance Increase](https://arxiv.org/abs/2505.23247)
*Zonglin Yang,Zhexuan Gu,Houduo Qi,Yancheng Yuan*

Main category: cs.LG

TL;DR: An abstract about a new reward adjustment model integrated into GRPO algorithm for more efficient RLHF training.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning from human feedback (RLHF) is crucial for aligning large language models (LLMs) with human values and preferences. But efficient GRPO-based RLHF training remains challenging.

Method: Propose a practical reward adjustment model to accelerate RLHF training by increasing the reward variance and preserving the relative preferences and reward expectation. Design a novel O(n log n) algorithm to solve the nonconvex optimization problem of the reward adjustment model. Integrate this model into the GRPO algorithm, leading to the GRPOVI algorithm.

Result: Experiment results show that the GRPOVI algorithm can significantly improve the RLHF training efficiency compared to the original GRPO algorithm.

Conclusion: The proposed reward adjustment model accelerates RLHF training and provides an indirect explanation for the empirical effectiveness of GRPO with rule-based reward for RLHF training.

Abstract: Reinforcement learning from human feedback (RLHF) is an essential technique
for ensuring that large language models (LLMs) are aligned with human values
and preferences during the post-training phase. As an effective RLHF approach,
group relative policy optimization (GRPO) has demonstrated success in many
LLM-based applications. However, efficient GRPO-based RLHF training remains a
challenge. Recent studies reveal that a higher reward variance of the initial
policy model leads to faster RLHF training. Inspired by this finding, we
propose a practical reward adjustment model to accelerate RLHF training by
provably increasing the reward variance and preserving the relative preferences
and reward expectation. Our reward adjustment method inherently poses a
nonconvex optimization problem, which is NP-hard to solve in general. To
overcome the computational challenges, we design a novel $O(n \log n)$
algorithm to find a global solution of the nonconvex reward adjustment model by
explicitly characterizing the extreme points of the feasible set. As an
important application, we naturally integrate this reward adjustment model into
the GRPO algorithm, leading to a more efficient GRPO with reward variance
increase (GRPOVI) algorithm for RLHF training. As an interesting byproduct, we
provide an indirect explanation for the empirical effectiveness of GRPO with
rule-based reward for RLHF training, as demonstrated in DeepSeek-R1. Experiment
results demonstrate that the GRPOVI algorithm can significantly improve the
RLHF training efficiency compared to the original GRPO algorithm.

</details>


### [125] [Efficiently Access Diffusion Fisher: Within the Outer Product Span Space](https://arxiv.org/abs/2505.23264)
*Fangyikang Wang,Hubery Yin,Shaobin Zhuang,Huminhao Zhu,Yinan Li,Lei Qian,Chao Zhang,Hanbin Zhao,Hui Qian,Chen Li*

Main category: cs.LG

TL;DR: 近期扩散模型研究中，探索了将二阶扩散Fisher信息（DF）应用于下游任务和理论分析。目前方法通常依赖自动微分近似DF，但缺乏精度保证且耗时。本文提出基于分数与初始数据外积结构的两种高效算法，用于计算DF的迹和矩阵-向量乘法，并建立误差界限。实验表明新算法在精度和计算成本上表现优异，并首次设计数值验证实验以检验一般PF-ODE推导映射的最优传输特性。


<details>
  <summary>Details</summary>
Motivation: 当前对扩散Fisher信息的近似方法依赖于自动微分，但该方法既无精度保证又耗时，因此需要更高效的近似方法。

Method: 利用分数与初始数据外积结构，开发两种高效算法来分别计算扩散Fisher信息的迹和矩阵-向量乘法，避免自动微分操作，转而使用时间效率更高的向量-乘积计算，并建立这些算法的近似误差边界。

Result: 在似然估计和伴随优化中的实验显示，所提出的算法具有更高的精度和更低的计算成本。此外，基于新的外积公式，成功设计了首个针对一般PF-ODE推导映射最优传输特性的数值验证实验。

Conclusion: 本文提出了两种高效算法以近似扩散Fisher信息，不仅提高了计算精度并降低了计算成本，还为最优传输性质提供了首个数值验证。

Abstract: Recent Diffusion models (DMs) advancements have explored incorporating the
second-order diffusion Fisher information (DF), defined as the negative Hessian
of log density, into various downstream tasks and theoretical analysis.
However, current practices typically approximate the diffusion Fisher by
applying auto-differentiation to the learned score network. This black-box
method, though straightforward, lacks any accuracy guarantee and is
time-consuming. In this paper, we show that the diffusion Fisher actually
resides within a space spanned by the outer products of score and initial data.
Based on the outer-product structure, we develop two efficient approximation
algorithms to access the trace and matrix-vector multiplication of DF,
respectively. These algorithms bypass the auto-differentiation operations with
time-efficient vector-product calculations. Furthermore, we establish the
approximation error bounds for the proposed algorithms. Experiments in
likelihood evaluation and adjoint optimization demonstrate the superior
accuracy and reduced computational cost of our proposed algorithms.
Additionally, based on the novel outer-product formulation of DF, we design the
first numerical verification experiment for the optimal transport property of
the general PF-ODE deduced map.

</details>


### [126] [Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs](https://arxiv.org/abs/2505.23270)
*Haokun Chen,Yueqi Zhang,Yuan Bi,Yao Zhang,Tong Liu,Jinhe Bi,Jian Lan,Jindong Gu,Claudia Grosser,Denis Krompass,Nassir Navab,Volker Tresp*

Main category: cs.LG

TL;DR: In recent years, Large Language Models (LLMs) have achieved remarkable advancements. However, the massive datasets used for training often contain sensitive or copyrighted content, raising concerns about data privacy and ownership. This has motivated the development of machine unlearning algorithms that aim to remove specific knowledge from models without the need for costly retraining. Despite these advancements, evaluating the efficacy of unlearning algorithms remains a challenge due to the inherent complexity and generative nature of LLMs. In this work, we introduce a comprehensive auditing framework for unlearning evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges in evaluating the efficacy of machine unlearning algorithms for LLMs, given the concerns about data privacy and ownership in the massive datasets used for training.

Method: The method involves introducing a comprehensive auditing framework for unlearning evaluation, which includes three benchmark datasets, six unlearning algorithms, and five prompt-based auditing methods. Additionally, a novel technique leveraging intermediate activation perturbations is proposed to explore alternatives beyond prompt-based auditing.

Result: The result is the evaluation of the effectiveness and robustness of different unlearning strategies using various auditing algorithms, as well as the exploration of alternatives beyond prompt-based auditing through the proposed technique.

Conclusion: The conclusion is that the introduced auditing framework provides a systematic way to evaluate unlearning algorithms for LLMs, while the proposed technique offers a new approach to overcome the limitations of existing auditing methods.

Abstract: In recent years, Large Language Models (LLMs) have achieved remarkable
advancements, drawing significant attention from the research community. Their
capabilities are largely attributed to large-scale architectures, which require
extensive training on massive datasets. However, such datasets often contain
sensitive or copyrighted content sourced from the public internet, raising
concerns about data privacy and ownership. Regulatory frameworks, such as the
General Data Protection Regulation (GDPR), grant individuals the right to
request the removal of such sensitive information. This has motivated the
development of machine unlearning algorithms that aim to remove specific
knowledge from models without the need for costly retraining. Despite these
advancements, evaluating the efficacy of unlearning algorithms remains a
challenge due to the inherent complexity and generative nature of LLMs. In this
work, we introduce a comprehensive auditing framework for unlearning
evaluation, comprising three benchmark datasets, six unlearning algorithms, and
five prompt-based auditing methods. By using various auditing algorithms, we
evaluate the effectiveness and robustness of different unlearning strategies.
To explore alternatives beyond prompt-based auditing, we propose a novel
technique that leverages intermediate activation perturbations, addressing the
limitations of auditing methods that rely solely on model inputs and outputs.

</details>


### [127] [Comparative Analysis of the Land Use and Land Cover Changes in Different Governorates of Oman using Spatiotemporal Multi-spectral Satellite Data](https://arxiv.org/abs/2505.23285)
*Muhammad Shafi,Syed Mohsin Bokhari*

Main category: cs.LG

TL;DR: This paper uses Sentinel-2 data and supervised machine learning algorithms to analyze and compare land cover and land use changes across various governorates of Oman from 2016 to 2021.


<details>
  <summary>Details</summary>
Motivation: To effectively monitor and predict land cover and land use changes which have critical roles in resource management, urbanization, protection of soils and the environment, and enhancing sustainable development.

Method: Acquiring multispectral spatiotemporal data from the open-source Sentinel-2 satellite dataset and using supervised machine learning algorithms to train and classify different land covers.

Result: An effective comparative evaluation of land cover and land use changes within Oman from 2016 to 2021 was achieved.

Conclusion: Supervised machine learning algorithms alongside Sentinel-2 data can effectively be used for monitoring land cover and land use changes.

Abstract: Land cover and land use (LULC) changes are key applications of satellite
imagery, and they have critical roles in resource management, urbanization,
protection of soils and the environment, and enhancing sustainable development.
The literature has heavily utilized multispectral spatiotemporal satellite data
alongside advanced machine learning algorithms to monitor and predict LULC
changes. This study analyzes and compares LULC changes across various
governorates (provinces) of the Sultanate of Oman from 2016 to 2021 using
annual time steps. For the chosen region, multispectral spatiotemporal data
were acquired from the open-source Sentinel-2 satellite dataset. Supervised
machine learning algorithms were used to train and classify different land
covers, such as water bodies, crops, urban, etc. The constructed model was
subsequently applied within the study region, allowing for an effective
comparative evaluation of LULC changes within the given timeframe.

</details>


### [128] [Score-based Generative Modeling for Conditional Independence Testing](https://arxiv.org/abs/2505.23309)
*Yixin Ren,Chenghou Jin,Yewei Xia,Li Ke,Longtao Huang,Hui Xue,Hao Zhang,Jihong Guan,Shuigeng Zhou*

Main category: cs.LG

TL;DR: A new method for conditional independence testing using score-based generative modeling is proposed, which controls Type I error precisely and has strong testing power.


<details>
  <summary>Details</summary>
Motivation: Determining conditional independence relationships between random variables is crucial in machine learning and statistics. However, existing generative model-based CI testing methods have limitations such as inaccurate modeling of conditional distributions and training instability.

Method: The method uses a sliced conditional score matching scheme to estimate the conditional score and Langevin dynamics conditional sampling for generating null hypothesis samples. A goodness-of-fit stage is also included to verify generated samples and improve interpretability.

Result: Theoretical analysis establishes the error bound of conditional distributions modeled by score-based generative models and proves the validity of the CI tests. Experiments on synthetic and real-world datasets demonstrate superior performance compared to state-of-the-art methods.

Conclusion: The proposed method provides an effective way to enhance generative model-based CI testing with precise Type I error control and strong testing power.

Abstract: Determining conditional independence (CI) relationships between random
variables is a fundamental yet challenging task in machine learning and
statistics, especially in high-dimensional settings. Existing generative
model-based CI testing methods, such as those utilizing generative adversarial
networks (GANs), often struggle with undesirable modeling of conditional
distributions and training instability, resulting in subpar performance. To
address these issues, we propose a novel CI testing method via score-based
generative modeling, which achieves precise Type I error control and strong
testing power. Concretely, we first employ a sliced conditional score matching
scheme to accurately estimate conditional score and use Langevin dynamics
conditional sampling to generate null hypothesis samples, ensuring precise Type
I error control. Then, we incorporate a goodness-of-fit stage into the method
to verify generated samples and enhance interpretability in practice. We
theoretically establish the error bound of conditional distributions modeled by
score-based generative models and prove the validity of our CI tests. Extensive
experiments on both synthetic and real-world datasets show that our method
significantly outperforms existing state-of-the-art methods, providing a
promising way to revitalize generative model-based CI testing.

</details>


### [129] [Efficient Parameter Estimation for Bayesian Network Classifiers using Hierarchical Linear Smoothing](https://arxiv.org/abs/2505.23320)
*Connor Cooper,Geoffrey I. Webb,Daniel F. Schmidt*

Main category: cs.LG

TL;DR: 本研究提出了一种新的参数估计方法，使用对数线性回归来近似分层狄利克雷过程（HDPs）的行为，从而在保持与随机森林竞争力的同时，显著提高了速度和灵活性。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯网络分类器（BNCs）具有可解释性、可扩展性和复杂度可调的优点，但传统学习方法性能不如随机森林等领先分类方法。尽管HDP技术提升了BNCs性能，但其方法相对复杂且不够灵活。

Method: 研究者引入了一种基于对数线性回归的新参数估计方法，用以近似HDP的行为。该方法属于线性模型，因此具有高度的灵活性和易解释性，并且可以利用丰富的线性模型学习文献。

Result: 实验结果表明，新方法不仅能够超越HDP平滑技术，而且速度快几个数量级，同时在分类性能上仍然与随机森林相当。

Conclusion: 提出的对数线性回归方法为BNC参数估计提供了一个简单、灵活且高效的替代方案，显著改进了BNCs的性能和效率。

Abstract: Bayesian network classifiers (BNCs) possess a number of properties desirable
for a modern classifier: They are easily interpretable, highly scalable, and
offer adaptable complexity. However, traditional methods for learning BNCs have
historically underperformed when compared to leading classification methods
such as random forests. Recent parameter smoothing techniques using
hierarchical Dirichlet processes (HDPs) have enabled BNCs to achieve
performance competitive with random forests on categorical data, but these
techniques are relatively inflexible, and require a complicated, specialized
sampling process. In this paper, we introduce a novel method for parameter
estimation that uses a log-linear regression to approximate the behaviour of
HDPs. As a linear model, our method is remarkably flexible and simple to
interpret, and can leverage the vast literature on learning linear models. Our
experiments show that our method can outperform HDP smoothing while being
orders of magnitude faster, remaining competitive with random forests on
categorical data.

</details>


### [130] [X2Graph for Cancer Subtyping Prediction on Biological Tabular Data](https://arxiv.org/abs/2505.23334)
*Tu Bui,Mohamed Suliman,Aparajita Haldar,Mohammed Amer,Serban Georgescu*

Main category: cs.LG

TL;DR: The paper introduces X2Graph, a deep learning method that transforms small biological tabular datasets into graph structures using external knowledge about column relationships. It shows superior performance in cancer subtyping datasets compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning's impact on text, audio, and image datasets has been significant, but its effectiveness on tabular data, particularly in the medical domain with limited data, is unclear. This motivates the need for a new approach tailored for such datasets.

Method: X2Graph leverages external knowledge of relationships between table columns (e.g., gene interactions) to convert each sample into a graph structure, enabling the use of standard message passing algorithms for graph modeling.

Result: X2Graph demonstrates superior performance compared to existing tree-based and deep learning methods across three cancer subtyping datasets.

Conclusion: X2Graph is a novel deep learning method that achieves strong performance on small biological tabular datasets by transforming them into graph structures using external knowledge.

Abstract: Despite the transformative impact of deep learning on text, audio, and image
datasets, its dominance in tabular data, especially in the medical domain where
data are often scarce, remains less clear. In this paper, we propose X2Graph, a
novel deep learning method that achieves strong performance on small biological
tabular datasets. X2Graph leverages external knowledge about the relationships
between table columns, such as gene interactions, to convert each sample into a
graph structure. This transformation enables the application of standard
message passing algorithms for graph modeling. Our X2Graph method demonstrates
superior performance compared to existing tree-based and deep learning methods
across three cancer subtyping datasets.

</details>


### [131] [Matryoshka Model Learning for Improved Elastic Student Models](https://arxiv.org/abs/2505.23337)
*Chetan Verma,Aditya Srinivas Timmaraju,Cho Jui-Hsieh,Suyash Damle,Ngot Bui,Yang Zhang,Wen Chen,Xin Liu,Prateek Jain,Inderjit S Dhillon*

Main category: cs.LG

TL;DR: A new framework named MatTA is proposed, which uses a Teacher-TA-Student recipe to train multiple accurate Student models. TA models, larger versions of the Student models, help bridge the gap between the Student and Teacher models while introducing more domain-specific expertise. This method offers multiple servable options with a single training run, allowing for accuracy-serving cost trade-offs. Demonstrated on proprietary datasets and GPT-2 Medium, it shows significant improvements in various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Industry-grade ML models require significant resources for development due to rapidly evolving serving constraints. There's a need for a method that can efficiently produce multiple servable models with varying accuracy-serving cost trade-offs.

Method: The MatTA framework employs a Teacher-TA-Student approach where TA models are larger, more capable versions of the Student models. These TA models facilitate better relation between the Student and Teacher models and incorporate more domain-specific knowledge. Multiple accurate Student models can be derived from the TA model through only one training process.

Result: MatTA demonstrated practical efficacy through live A/B tests within a production ML system, showing a 20% improvement on a key metric. On GPT-2 Medium, it achieved over 24% relative improvement on SAT Math and over 10% on the LAMBADA benchmark.

Conclusion: MatTA provides an effective way to train multiple accurate Student models with a single training run, offering flexibility in choosing models based on the desired accuracy-serving cost trade-off.

Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving
serving constraints, which requires significant resources for model
development. In this paper, we propose MatTA, a framework for training multiple
accurate Student models using a novel Teacher-TA-Student recipe. TA models are
larger versions of the Student models with higher capacity, and thus allow
Student models to better relate to the Teacher model and also bring in more
domain-specific expertise. Furthermore, multiple accurate Student models can be
extracted from the TA model. Therefore, despite only one training run, our
methodology provides multiple servable options to trade off accuracy for lower
serving cost. We demonstrate the proposed method, MatTA, on proprietary
datasets and models. Its practical efficacy is underscored by live A/B tests
within a production ML system, demonstrating 20% improvement on a key metric.
We also demonstrate our method on GPT-2 Medium, a public model, and achieve
relative improvements of over 24% on SAT Math and over 10% on the LAMBADA
benchmark.

</details>


### [132] [Graph Positional Autoencoders as Self-supervised Learners](https://arxiv.org/abs/2505.23345)
*Yang Liu,Deyu Bo,Wenxuan Cao,Yuan Fang,Yawen Li,Chuan Shi*

Main category: cs.LG

TL;DR: Graph self-supervised learning method, Graph Positional Autoencoders (GraphPAE), is proposed to reconstruct node features and positions via a dual-path architecture. It overcomes limitations of traditional GAEs and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Traditional graph autoencoders primarily capture low-frequency signals in the graph and fail to learn expressive structural information. There's a need for an approach that can effectively capture diverse frequency information.

Method: Propose Graph Positional Autoencoders (GraphPAE) with a dual-path architecture. The feature path uses positional encoding to enhance message-passing processing while the position path leverages node representations to refine positions and approximate eigenvectors.

Result: Extensive experiments verify the effectiveness of GraphPAE, including heterophilic node classification, graph property prediction, and transfer learning. GraphPAE outperforms baselines by a large margin and achieves state-of-the-art performance.

Conclusion: GraphPAE addresses the limitations of traditional GAEs by employing a dual-path architecture to learn both node features and positions, resulting in superior performance in various tasks.

Abstract: Graph self-supervised learning seeks to learn effective graph representations
without relying on labeled data. Among various approaches, graph autoencoders
(GAEs) have gained significant attention for their efficiency and scalability.
Typically, GAEs take incomplete graphs as input and predict missing elements,
such as masked nodes or edges. While effective, our experimental investigation
reveals that traditional node or edge masking paradigms primarily capture
low-frequency signals in the graph and fail to learn the expressive structural
information. To address these issues, we propose Graph Positional Autoencoders
(GraphPAE), which employs a dual-path architecture to reconstruct both node
features and positions. Specifically, the feature path uses positional encoding
to enhance the message-passing processing, improving GAE's ability to predict
the corrupted information. The position path, on the other hand, leverages node
representations to refine positions and approximate eigenvectors, thereby
enabling the encoder to learn diverse frequency information. We conduct
extensive experiments to verify the effectiveness of GraphPAE, including
heterophilic node classification, graph property prediction, and transfer
learning. The results demonstrate that GraphPAE achieves state-of-the-art
performance and consistently outperforms baselines by a large margin.

</details>


### [133] [Sentinel: Scheduling Live Streams with Proactive Anomaly Detection in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2505.23347)
*Yuting Li,Shaoyuan Huang,Tengwen Zhang,Cheng Zhang,Xiaofei Wang,Victor C. M. Leung*

Main category: cs.LG

TL;DR: 为了应对直播服务的增长，本文提出了一种名为Sentinel的调度框架，以解决Crowdsourced Cloud-edge service Platforms（CCPs）中的异常检测和调度问题。Sentinel通过两阶段的Pre-Post-Scheduling范式，在预调度阶段进行异常检测和策略构建，在后调度阶段根据请求选择合适的调度策略。实验表明，Sentinel能减少70%的异常频率，提高74%的收入，并使调度速度翻倍。


<details>
  <summary>Details</summary>
Motivation: 随着直播服务的快速增长，群智云边服务平台（CCPs）在满足需求方面发挥着越来越重要的作用。然而，由于CCPs的不稳定性和大规模特性，现有的流调度优化策略难以实现实际效果，尤其是在时间敏感的调度中，异常检测变得更加困难。因此，需要一种新的方法来解决这些问题。

Method: 本文提出了一种基于主动异常检测的调度框架——Sentinel。该框架将调度过程建模为两阶段的Pre-Post-Scheduling范式：在预调度阶段，Sentinel执行异常检测并构建策略池；在后调度阶段，当请求到达时，触发基于预先生成策略的适当调度以实施调度过程。

Result: 通过在真实数据集上的广泛实验，Sentinel显著减少了70%的异常频率，提高了74%的收入，并且使调度速度提升了一倍。

Conclusion: Sentinel作为一个基于主动异常检测的调度框架，有效地解决了CCPs平台中的异常检测与调度问题，极大地提升了平台的稳定性和经济效益。

Abstract: With the rapid growth of live streaming services, Crowdsourced Cloud-edge
service Platforms (CCPs) are playing an increasingly important role in meeting
the increasing demand. Although stream scheduling plays a critical role in
optimizing CCPs' revenue, most optimization strategies struggle to achieve
practical results due to various anomalies in unstable CCPs. Additionally, the
substantial scale of CCPs magnifies the difficulties of anomaly detection in
time-sensitive scheduling. To tackle these challenges, this paper proposes
Sentinel, a proactive anomaly detection-based scheduling framework. Sentinel
models the scheduling process as a two-stage Pre-Post-Scheduling paradigm: in
the pre-scheduling stage, Sentinel conducts anomaly detection and constructs a
strategy pool; in the post-scheduling stage, upon request arrival, it triggers
an appropriate scheduling based on a pre-generated strategy to implement the
scheduling process. Extensive experiments on realistic datasets show that
Sentinel significantly reduces anomaly frequency by 70%, improves revenue by
74%, and doubles the scheduling speed.

</details>


### [134] [Towards Reward Fairness in RLHF: From a Resource Allocation Perspective](https://arxiv.org/abs/2505.23349)
*Sheng Ouyang,Yulan Hu,Ge Chen,Qingyang Li,Fuzheng Zhang,Yong Liu*

Main category: cs.LG

TL;DR: Rewards in Reinforcement Learning from Human Feedback (RLHF) are crucial but can be biased, affecting the alignment of large language models (LLMs). This paper defines this issue as reward unfairness and proposes two bias-agnostic methods, Fairness Regularization and Fairness Coefficient, to address it. By modeling preference learning as a resource allocation problem, these methods balance utility and fairness in reward distribution. Experiments show that the proposed approach aligns LLMs with human preferences more fairly.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the observation that rewards in RLHF can be inherently imperfect and biased, which negatively impacts the alignment of LLMs with human preferences. To solve this issue, the authors aim to develop a method that mitigates these biases without specifically designing for each type of bias.

Method: The authors propose to model preference learning as a resource allocation problem, treating rewards as resources to be allocated while considering the trade-off between utility and fairness. They introduce two methods: Fairness Regularization and Fairness Coefficient, both of which are applied in verification and reinforcement learning scenarios to obtain fairness reward and policy models respectively.

Result: Experiments conducted in both verification and reinforcement learning settings demonstrate that the proposed methods effectively align LLMs with human preferences in a more fair manner.

Conclusion: This paper successfully addresses the problem of reward unfairness by proposing two bias-agnostic methods that mitigate various types of bias in rewards. The results indicate that these methods improve the fairness of LLM alignment with human preferences.

Abstract: Rewards serve as proxies for human preferences and play a crucial role in
Reinforcement Learning from Human Feedback (RLHF). However, if these rewards
are inherently imperfect, exhibiting various biases, they can adversely affect
the alignment of large language models (LLMs). In this paper, we collectively
define the various biases present in rewards as the problem of reward
unfairness. We propose a bias-agnostic method to address the issue of reward
fairness from a resource allocation perspective, without specifically designing
for each type of bias, yet effectively mitigating them. Specifically, we model
preference learning as a resource allocation problem, treating rewards as
resources to be allocated while considering the trade-off between utility and
fairness in their distribution. We propose two methods, Fairness Regularization
and Fairness Coefficient, to achieve fairness in rewards. We apply our methods
in both verification and reinforcement learning scenarios to obtain a fairness
reward model and a policy model, respectively. Experiments conducted in these
scenarios demonstrate that our approach aligns LLMs with human preferences in a
more fair manner.

</details>


### [135] [Grower-in-the-Loop Interactive Reinforcement Learning for Greenhouse Climate Control](https://arxiv.org/abs/2505.23355)
*Maxiu Xiao,Jianglin Lan,Jingxing Yu,Eldert van Henten,Congcong Sun*

Main category: cs.LG

TL;DR: Interactive reinforcement learning (RL) with imperfect grower inputs is explored for greenhouse climate control. Three interactive RL algorithms were developed and evaluated, revealing that policy shaping and control sharing perform better than reward shaping when dealing with imperfect inputs.


<details>
  <summary>Details</summary>
Motivation: Climate control in greenhouses significantly impacts crop growth and resource use. Traditional RL methods have limitations such as low training efficiency and high dependence on initial conditions, prompting the exploration of interactive RL to leverage human input and improve performance.

Method: 1.Developed three interactive RL algorithms tailored for greenhouse climate control: reward shaping, policy shaping, and control sharing. 2.Analyzed the contradicting characteristics of grower inputs and the trade-offs involved. 3.Proposed a neural network-based approach to enhance robustness under limited input availability. 4.Conducted comprehensive evaluations of these algorithms in a simulated greenhouse environment.

Result: Interactive RL incorporating imperfect grower inputs can improve RL agent performance. Policy shaping and control sharing algorithms showed improvements of 8.4% and 6.8% in profit, respectively, while reward shaping led to a 9.4% decrease in profit due to its sensitivity to imperfect inputs.

Conclusion: The study demonstrates the potential benefits of interactive RL in greenhouse climate control, especially when using algorithms like policy shaping and control sharing that handle imperfect inputs well. The choice of mechanism is crucial for achieving optimal results.

Abstract: Climate control is crucial for greenhouse production as it directly affects
crop growth and resource use. Reinforcement learning (RL) has received
increasing attention in this field, but still faces challenges, including
limited training efficiency and high reliance on initial learning conditions.
Interactive RL, which combines human (grower) input with the RL agent's
learning, offers a potential solution to overcome these challenges. However,
interactive RL has not yet been applied to greenhouse climate control and may
face challenges related to imperfect inputs. Therefore, this paper aims to
explore the possibility and performance of applying interactive RL with
imperfect inputs into greenhouse climate control, by: (1) developing three
representative interactive RL algorithms tailored for greenhouse climate
control (reward shaping, policy shaping and control sharing); (2) analyzing how
input characteristics are often contradicting, and how the trade-offs between
them make grower's inputs difficult to perfect; (3) proposing a neural
network-based approach to enhance the robustness of interactive RL agents under
limited input availability; (4) conducting a comprehensive evaluation of the
three interactive RL algorithms with imperfect inputs in a simulated greenhouse
environment. The demonstration shows that interactive RL incorporating
imperfect grower inputs has the potential to improve the performance of the RL
agent. RL algorithms that influence action selection, such as policy shaping
and control sharing, perform better when dealing with imperfect inputs,
achieving 8.4% and 6.8% improvement in profit, respectively. In contrast,
reward shaping, an algorithm that manipulates the reward function, is sensitive
to imperfect inputs and leads to a 9.4% decrease in profit. This highlights the
importance of selecting an appropriate mechanism when incorporating imperfect
inputs.

</details>


### [136] [Dynamic Spectral Backpropagation for Efficient Neural Network Training](https://arxiv.org/abs/2505.23369)
*Mannmohan Muthuraman*

Main category: cs.LG

TL;DR: DSBP is a novel method that reduces complexity in neural network training and outperforms SAM, LoRA, and MAML.


<details>
  <summary>Details</summary>
Motivation: To improve neural network training under resource constraints by reducing complexity and promoting flat minima.

Method: Dynamic Spectral Backpropagation (DSBP) projects gradients onto principal eigenvectors. Five extensions are proposed: dynamic spectral inference, spectral architecture optimization, spectral meta learning, spectral transfer regularization, and Lie algebra inspired dynamics.

Result: DSBP outperforms SAM, LoRA, and MAML on CIFAR 10, Fashion MNIST, MedMNIST, and Tiny ImageNet as shown through extensive experiments and visualizations.

Conclusion: Future work will focus on scalability, bias mitigation, and ethical considerations.

Abstract: Dynamic Spectral Backpropagation (DSBP) enhances neural network training
under resource constraints by projecting gradients onto principal eigenvectors,
reducing complexity and promoting flat minima. Five extensions are proposed,
dynamic spectral inference, spectral architecture optimization, spectral meta
learning, spectral transfer regularization, and Lie algebra inspired dynamics,
to address challenges in robustness, fewshot learning, and hardware efficiency.
Supported by a third order stochastic differential equation (SDE) and a PAC
Bayes limit, DSBP outperforms Sharpness Aware Minimization (SAM), Low Rank
Adaptation (LoRA), and Model Agnostic Meta Learning (MAML) on CIFAR 10, Fashion
MNIST, MedMNIST, and Tiny ImageNet, as demonstrated through extensive
experiments and visualizations. Future work focuses on scalability, bias
mitigation, and ethical considerations.

</details>


### [137] [Meta-Learning Approaches for Speaker-Dependent Voice Fatigue Models](https://arxiv.org/abs/2505.23378)
*Roseline Polle,Agnes Norbury,Alexandra Livia Georgescu,Nicholas Cummins,Stefano Goria*

Main category: cs.LG

TL;DR: This paper explores three meta-learning approaches for speaker-dependent modeling in speech-based health monitoring, demonstrating their superiority over traditional models, especially with a transformer-based method achieving the best performance.


<details>
  <summary>Details</summary>
Motivation: Speaker-dependent modeling can significantly enhance performance in speech-based health monitoring applications. However, conventional mixed-effect models used for speaker adaptation are computationally expensive and impractical in real-world scenarios due to the need for retraining with each new observation.

Method: The task is reformulated as a meta-learning problem and three methods are explored: ensemble-based distance models, prototypical networks, and transformer-based sequence models. Pre-trained speech embeddings are used to evaluate these methods on a large longitudinal dataset of shift workers.

Result: All meta-learning approaches outperformed both cross-sectional and conventional mixed-effects models. The transformer-based method achieved the strongest performance.

Conclusion: Meta-learning approaches, particularly the transformer-based method, offer superior performance for speaker-dependent modeling in speech-based health monitoring compared to traditional models.

Abstract: Speaker-dependent modelling can substantially improve performance in
speech-based health monitoring applications. While mixed-effect models are
commonly used for such speaker adaptation, they require computationally
expensive retraining for each new observation, making them impractical in a
production environment. We reformulate this task as a meta-learning problem and
explore three approaches of increasing complexity: ensemble-based distance
models, prototypical networks, and transformer-based sequence models. Using
pre-trained speech embeddings, we evaluate these methods on a large
longitudinal dataset of shift workers (N=1,185, 10,286 recordings), predicting
time since sleep from speech as a function of fatigue, a symptom commonly
associated with ill-health. Our results demonstrate that all meta-learning
approaches tested outperformed both cross-sectional and conventional
mixed-effects models, with a transformer-based method achieving the strongest
performance.

</details>


### [138] [Automated Modeling Method for Pathloss Model Discovery](https://arxiv.org/abs/2505.23383)
*Ahmad Anaqreh,Shih-Kai Chou,Mihael Mohorčič,Carolina Fortuna*

Main category: cs.LG

TL;DR: This paper proposes a novel AI-based approach for accelerating the discovery of path loss models in wireless communication systems while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional modeling methods for wireless propagation rely on statistic-based techniques, which struggle with accuracy and interoperability as systems expand. There is a need for methods that can ensure both accuracy and interpretability.

Method: The paper evaluates two AI-based techniques: Deep Symbolic Regression (offering full interpretability) and Kolmogorov-Arnold Networks (providing two levels of interpretability). These methods automate model formulation, evaluation, and refinement.

Result: Kolmogorov-Arnold Networks achieve R^2 values close to 1 with minimal prediction error, while Deep Symbolic Regression generates compact models with moderate accuracy. Automated methods outperform traditional ones by up to 75% reduction in prediction errors.

Conclusion: AI-based automated methods offer accurate and explainable solutions, potentially increasing the efficiency of discovering next-generation path loss models.

Abstract: Modeling propagation is the cornerstone for designing and optimizing
next-generation wireless systems, with a particular emphasis on 5G and beyond
era. Traditional modeling methods have long relied on statistic-based
techniques to characterize propagation behavior across different environments.
With the expansion of wireless communication systems, there is a growing demand
for methods that guarantee the accuracy and interoperability of modeling.
Artificial intelligence (AI)-based techniques, in particular, are increasingly
being adopted to overcome this challenge, although the interpretability is not
assured with most of these methods. Inspired by recent advancements in AI, this
paper proposes a novel approach that accelerates the discovery of path loss
models while maintaining interpretability. The proposed method automates the
model formulation, evaluation, and refinement, facilitating model discovery. We
evaluate two techniques: one based on Deep Symbolic Regression, offering full
interpretability, and the second based on Kolmogorov-Arnold Networks, providing
two levels of interpretability. Both approaches are evaluated on two synthetic
and two real-world datasets. Our results show that Kolmogorov-Arnold Networks
achieve R^2 values close to 1 with minimal prediction error, while Deep
Symbolic Regression generates compact models with moderate accuracy. Moreover,
on the selected examples, we demonstrate that automated methods outperform
traditional methods, achieving up to 75% reduction in prediction errors,
offering accurate and explainable solutions with potential to increase the
efficiency of discovering next-generation path loss models.

</details>


### [139] [Buffer-free Class-Incremental Learning with Out-of-Distribution Detection](https://arxiv.org/abs/2505.23412)
*Srishti Gupta,Daniele Angioni,Maura Pintor,Ambra Demontis,Lea Schönherr,Battista Biggio,Fabio Roli*

Main category: cs.LG

TL;DR: 在类增量学习(CIL)中，模型需要学习新类而不忘记旧类，并处理未知类的输入。本文分析了后验OOD检测方法，发现其在推理时可有效替代基于缓冲区的OOD检测，且无需记忆缓冲区即可达到相似或更优性能。实验结果支持该方法在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上的有效性，为设计高效和保护隐私的CIL系统提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 当前解决CIL问题的方法主要依赖于多头模型和基于缓冲区的OOD检测，但这些方法存在隐私、可扩展性和训练时间增加的问题。因此，研究者希望探索一种无需记忆缓冲区的方法来解决这些问题。

Method: 本文深入分析了后验OOD检测方法，并研究了它们在推理阶段的应用是否可以替代基于缓冲区的OOD检测方法。通过实验验证了无缓冲区方法在CIL和拒绝未知样本方面的性能。

Result: 实验结果表明，无缓冲区的后验OOD检测方法在CIL和拒绝未知样本方面达到了与基于缓冲区方法相当甚至更优的性能。

Conclusion: 后验OOD检测方法在推理阶段能够有效替代基于缓冲区的OOD检测，为构建高效且保护隐私的CIL系统提供了新的思路。

Abstract: Class-incremental learning (CIL) poses significant challenges in open-world
scenarios, where models must not only learn new classes over time without
forgetting previous ones but also handle inputs from unknown classes that a
closed-set model would misclassify. Recent works address both issues by
(i)~training multi-head models using the task-incremental learning framework,
and (ii) predicting the task identity employing out-of-distribution (OOD)
detectors. While effective, the latter mainly relies on joint training with a
memory buffer of past data, raising concerns around privacy, scalability, and
increased training time. In this paper, we present an in-depth analysis of
post-hoc OOD detection methods and investigate their potential to eliminate the
need for a memory buffer. We uncover that these methods, when applied
appropriately at inference time, can serve as a strong substitute for
buffer-based OOD detection. We show that this buffer-free approach achieves
comparable or superior performance to buffer-based methods both in terms of
class-incremental learning and the rejection of unknown samples. Experimental
results on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets support our findings,
offering new insights into the design of efficient and privacy-preserving CIL
systems for open-world settings.

</details>


### [140] [Bidirectional predictive coding](https://arxiv.org/abs/2505.23415)
*Gaspard Oliviers,Mufeng Tang,Rafal Bogacz*

Main category: cs.LG

TL;DR: The paper introduces bidirectional Predictive Coding (bPC), which integrates generative and discriminative inference for improved performance in visual learning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing predictive coding models either focus on top-down generative or feedforward discriminative approaches, but lack the ability to perform bidirectional processing as observed in the brain.

Method: The researchers developed bPC by incorporating both generative and discriminative inference mechanisms within a biologically plausible circuit. An energy landscape was designed to accommodate both types of tasks effectively.

Result: bPC matched or outperformed unidirectional models in specialized tasks and showed superior performance in multimodal learning and inference with missing information.

Conclusion: bPC provides a more accurate resemblance to biological visual inference and could be a better model for understanding how the brain processes visual information.

Abstract: Predictive coding (PC) is an influential computational model of visual
learning and inference in the brain. Classical PC was proposed as a top-down
generative model, where the brain actively predicts upcoming visual inputs, and
inference minimises the prediction errors. Recent studies have also shown that
PC can be formulated as a discriminative model, where sensory inputs predict
neural activities in a feedforward manner. However, experimental evidence
suggests that the brain employs both generative and discriminative inference,
while unidirectional PC models show degraded performance in tasks requiring
bidirectional processing. In this work, we propose bidirectional PC (bPC), a PC
model that incorporates both generative and discriminative inference while
maintaining a biologically plausible circuit implementation. We show that bPC
matches or outperforms unidirectional models in their specialised generative or
discriminative tasks, by developing an energy landscape that simultaneously
suits both tasks. We also demonstrate bPC's superior performance in two
biologically relevant tasks including multimodal learning and inference with
missing information, suggesting that bPC resembles biological visual inference
more closely.

</details>


### [141] [OTPTO: Joint Product Selection and Inventory Optimization in Fresh E-commerce Front-End Warehouses](https://arxiv.org/abs/2505.23421)
*Zheming Zhang,Yan Jiang,Qingshan Li,Ai Han*

Main category: cs.LG

TL;DR: In China's competitive fresh e-commerce market, the paper proposes a multi-task Optimize-then-Predict-then-Optimize (OTPTO) approach to address inventory management challenges in front-end warehouses. This method significantly enhances the full order fulfillment rate and narrows the gap to the optimal rate.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance customer satisfaction and gain a competitive edge in China's fresh e-commerce market by optimizing operational strategies, particularly inventory management in front-end warehouses.

Method: The paper introduces a multi-task Optimize-then-Predict-then-Optimize (OTPTO) approach. It uses a 0-1 mixed integer programming model OM1 to determine historically optimal inventory levels, product selection model PM1, stocking model PM2 for prediction, and post-processing algorithm OM2 to refine results.

Result: Experimental results from JD.com's 7Fresh platform show that the OTPTO method increases the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and reduces the gap to the optimal full order fulfillment rate by 5.27%.

Conclusion: The findings confirm the effectiveness of the OTPTO method in managing inventory at front-end warehouses of fresh e-commerce platforms, providing valuable insights for future research.

Abstract: In China's competitive fresh e-commerce market, optimizing operational
strategies, especially inventory management in front-end warehouses, is key to
enhance customer satisfaction and to gain a competitive edge. Front-end
warehouses are placed in residential areas to ensure the timely delivery of
fresh goods and are usually in small size. This brings the challenge of
deciding which goods to stock and in what quantities, taking into account
capacity constraints. To address this issue, traditional predict-then-optimize
(PTO) methods that predict sales and then decide on inventory often don't align
prediction with inventory goals, as well as fail to prioritize consumer
satisfaction. This paper proposes a multi-task
Optimize-then-Predict-then-Optimize (OTPTO) approach that jointly optimizes
product selection and inventory management, aiming to increase consumer
satisfaction by maximizing the full order fulfillment rate. Our method employs
a 0-1 mixed integer programming model OM1 to determine historically optimal
inventory levels, and then uses a product selection model PM1 and the stocking
model PM2 for prediction. The combined results are further refined through a
post-processing algorithm OM2. Experimental results from JD.com's 7Fresh
platform demonstrate the robustness and significant advantages of our OTPTO
method. Compared to the PTO approach, our OTPTO method substantially enhances
the full order fulfillment rate by 4.34% (a relative increase of 7.05%) and
narrows the gap to the optimal full order fulfillment rate by 5.27%. These
findings substantiate the efficacy of the OTPTO method in managing inventory at
front-end warehouses of fresh e-commerce platforms and provide valuable
insights for future research in this domain.

</details>


### [142] [Enhanced DACER Algorithm with High Diffusion Efficiency](https://arxiv.org/abs/2505.23426)
*Yinuo Wang,Mining Tan,Wenjun Zou,Haotian Lin,Xujie Song,Wenxuan Wang,Tong Liu,Likun Wang,Guojian Zhan,Tianze Zhu,Shiqi Liu,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: Diffusion models show promise in offline RL and imitation learning. DACER extended this to online RL but suffers from inefficiency due to many diffusion steps. This paper proposes a Q-gradient field objective and temporal weighting mechanism to improve efficiency while maintaining or enhancing performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of DACER in terms of training and inference, especially when reducing diffusion steps leads to performance degradation.

Method: Propose a Q-gradient field objective as an auxiliary optimization target to guide denoising process at each diffusion step, and introduce a temporal weighting mechanism to enhance noise elimination and action refinement.

Result: DACER2 achieves state-of-the-art performance in most MuJoCo control tasks with only five diffusion steps, and exhibits stronger multimodality compared to DACER.

Conclusion: The proposed method improves training and inference efficiency while maintaining or enhancing performance.

Abstract: Due to their expressive capacity, diffusion models have shown great promise
in offline RL and imitation learning. Diffusion Actor-Critic with Entropy
Regulator (DACER) extended this capability to online RL by using the reverse
diffusion process as a policy approximator, trained end-to-end with policy
gradient methods, achieving strong performance. However, this comes at the cost
of requiring many diffusion steps, which significantly hampers training
efficiency, while directly reducing the steps leads to noticeable performance
degradation. Critically, the lack of inference efficiency becomes a significant
bottleneck for applying diffusion policies in real-time online RL settings. To
improve training and inference efficiency while maintaining or even enhancing
performance, we propose a Q-gradient field objective as an auxiliary
optimization target to guide the denoising process at each diffusion step.
Nonetheless, we observe that the independence of the Q-gradient field from the
diffusion time step negatively impacts the performance of the diffusion policy.
To address this, we introduce a temporal weighting mechanism that enables the
model to efficiently eliminate large-scale noise in the early stages and refine
actions in the later stages. Experimental results on MuJoCo benchmarks and
several multimodal tasks demonstrate that the DACER2 algorithm achieves
state-of-the-art performance in most MuJoCo control tasks with only five
diffusion steps, while also exhibiting stronger multimodality compared to
DACER.

</details>


### [143] [On the Validity of Head Motion Patterns as Generalisable Depression Biomarkers](https://arxiv.org/abs/2505.23427)
*Monika Gahalawat,Maneesh Bilalpur,Raul Fernandez Rojas,Jeffrey F. Cohn,Roland Goecke,Ramanathan Subramanian*

Main category: cs.LG

TL;DR: 研究发现头部运动模式是评估抑郁程度的有效生物标志，基于kineme的特征比原始头部运动描述符和其他视觉行为线索更具通用性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是一种影响全球数百万人的精神疾病，尽管研究人员已经探索了多种言语和非言语行为线索用于自动抑郁评估，但头部运动尚未受到足够的关注。此外，通过单一数据集验证机器学习模型的做法可能限制模型的通用性。

Method: 本研究使用来自不同西方文化的三个抑郁数据集（德国：AVEC2013、澳大利亚：Blackdog和美国：Pitt数据集），采用两种方法（k折交叉验证和模型重用）来研究基于kineme模式的模型在估计抑郁严重程度方面的有效性和通用性。

Result: 实验结果表明：(1) 头部运动模式是估计抑郁严重程度的有效生物标志，在分类和回归任务中表现出高度竞争力；(2) 基于kineme的特征比原始头部运动描述符和其他视觉行为线索更具通用性。

Conclusion: 头部运动模式可以作为评估抑郁严重程度的有效生物标志，并且基于kineme的特征具有更好的通用性。

Abstract: Depression is a debilitating mood disorder negatively impacting millions
worldwide. While researchers have explored multiple verbal and non-verbal
behavioural cues for automated depression assessment, head motion has received
little attention thus far. Further, the common practice of validating machine
learning models via a single dataset can limit model generalisability. This
work examines the effectiveness and generalisability of models utilising
elementary head motion units, termed kinemes, for depression severity
estimation. Specifically, we consider three depression datasets from different
western cultures (German: AVEC2013, Australian: Blackdog and American: Pitt
datasets) with varied contextual and recording settings to investigate the
generalisability of the derived kineme patterns via two methods: (i) k-fold
cross-validation over individual/multiple datasets, and (ii) model reuse on
other datasets. Evaluating classification and regression performance with
classical machine learning methods, our results show that: (1) head motion
patterns are efficient biomarkers for estimating depression severity, achieving
highly competitive performance for both classification and regression tasks on
a variety of datasets, including achieving the second best Mean Absolute Error
(MAE) on the AVEC2013 dataset, and (2) kineme-based features are more
generalisable than (a) raw head motion descriptors for binary severity
classification, and (b) other visual behavioural cues for severity estimation
(regression).

</details>


### [144] [Diversity-Aware Policy Optimization for Large Language Model Reasoning](https://arxiv.org/abs/2505.23433)
*Jian Yao,Ran Cheng,Xingyu Wu,Jibin Wu,Kay Chen Tan*

Main category: cs.LG

TL;DR: 研究了多样性在基于强化学习的大型语言模型推理训练中的影响，并提出了一种新的多样性感知策略优化方法，该方法在数学推理基准测试中提高了3.5％的平均表现。


<details>
  <summary>Details</summary>
Motivation: 尽管多样性在强化学习中起着关键作用，但其对大型语言模型推理的影响尚未被充分探索。

Method: 设计了一个 token-level 的多样性指标，并将其转化为一个实用的目标函数，然后有选择地将其应用于正样本，最后将该方法集成到 R1-zero 训练框架中。

Result: 在四个数学推理基准测试中实现了3.5%的平均改进，并生成了更多样化和更稳健的解决方案。观察到高表现模型中解决方案多样性和 k 位潜力之间存在强烈的正相关关系。

Conclusion: 多样性在强化学习训练中的显式促进可以提高大型语言模型的推理能力，产生更多样化和更稳健的解决方案。

Abstract: The reasoning capabilities of large language models (LLMs) have advanced
rapidly, particularly following the release of DeepSeek R1, which has inspired
a surge of research into data quality and reinforcement learning (RL)
algorithms. Despite the pivotal role diversity plays in RL, its influence on
LLM reasoning remains largely underexplored. To bridge this gap, this work
presents a systematic investigation into the impact of diversity in RL-based
training for LLM reasoning, and proposes a novel diversity-aware policy
optimization method. Across evaluations on 12 LLMs, we observe a strong
positive correlation between the solution diversity and Potential at k (a novel
metric quantifying an LLM's reasoning potential) in high-performing models.
This finding motivates our method to explicitly promote diversity during RL
training. Specifically, we design a token-level diversity and reformulate it
into a practical objective, then we selectively apply it to positive samples.
Integrated into the R1-zero training framework, our method achieves a 3.5
percent average improvement across four mathematical reasoning benchmarks,
while generating more diverse and robust solutions.

</details>


### [145] [Bounded-Abstention Pairwise Learning to Rank](https://arxiv.org/abs/2505.23437)
*Antonio Ferrara,Andrea Pugnana,Francesco Bonchi,Salvatore Ruggieri*

Main category: cs.LG

TL;DR: Ranking systems are crucial in high-stakes domains, and abstention mechanisms can improve their safety. This paper introduces a new method for abstention in pairwise learning-to-rank tasks, thresholds the ranker's conditional risk, and provides theoretical, algorithmic, and empirical contributions.


<details>
  <summary>Details</summary>
Motivation: Abstention mechanisms are important for safety in ranking systems impacting high-stakes domains like health, education, and employment. However, abstention has been mainly explored in classification tasks, leaving other machine learning paradigms underexplored.

Method: The approach involves thresholding the ranker's conditional risk. The system abstains from making a decision when the estimated risk exceeds a predefined threshold.

Result: The paper offers a theoretical characterization of the optimal abstention strategy, a model-agnostic plug-in algorithm for building abstaining ranking models, and comprehensive empirical evaluations across multiple datasets.

Conclusion: This novel abstention method for pairwise learning-to-rank tasks improves safety in ranking systems through a theoretically grounded, flexible, and empirically validated approach.

Abstract: Ranking systems influence decision-making in high-stakes domains like health,
education, and employment, where they can have substantial economic and social
impacts. This makes the integration of safety mechanisms essential. One such
mechanism is $\textit{abstention}$, which enables algorithmic decision-making
system to defer uncertain or low-confidence decisions to human experts. While
abstention have been predominantly explored in the context of classification
tasks, its application to other machine learning paradigms remains
underexplored. In this paper, we introduce a novel method for abstention in
pairwise learning-to-rank tasks. Our approach is based on thresholding the
ranker's conditional risk: the system abstains from making a decision when the
estimated risk exceeds a predefined threshold. Our contributions are threefold:
a theoretical characterization of the optimal abstention strategy, a
model-agnostic, plug-in algorithm for constructing abstaining ranking models,
and a comprehensive empirical evaluations across multiple datasets,
demonstrating the effectiveness of our approach.

</details>


### [146] [Rethinking Regularization Methods for Knowledge Graph Completion](https://arxiv.org/abs/2505.23442)
*Linyu Li,Zhi Jin,Yuanpeng He,Dongming Jin,Haoran Duan,Zhengwei Tao,Xuan Zhang,Jiandong Li*

Main category: cs.LG

TL;DR: This paper rethinks the application of regularization methods in knowledge graph completion (KGC), introducing a novel sparse-regularization method called SPR that embeds rank-based selective sparsity into the KGC regularizer.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the quality of knowledge graphs by advancing KGC models, specifically by exploring regularization methods from a deeper perspective to fully utilize their potential.

Method: Through extensive empirical studies on various KGC models, the authors carefully design regularization methods and introduce a novel sparse-regularization method (SPR) that selectively penalizes significant components in the embedding vector.

Result: Various comparative experiments demonstrate that the SPR regularization method outperforms other regularization methods and enables the KGC model to further break through the performance margin.

Conclusion: Carefully designed regularization not only alleviates overfitting and reduces variance but also enhances the performance of KGC models beyond their original upper bounds.

Abstract: Knowledge graph completion (KGC) has attracted considerable attention in
recent years because it is critical to improving the quality of knowledge
graphs. Researchers have continuously explored various models. However, most
previous efforts have neglected to take advantage of regularization from a
deeper perspective and therefore have not been used to their full potential.
This paper rethinks the application of regularization methods in KGC. Through
extensive empirical studies on various KGC models, we find that carefully
designed regularization not only alleviates overfitting and reduces variance
but also enables these models to break through the upper bounds of their
original performance. Furthermore, we introduce a novel sparse-regularization
method that embeds the concept of rank-based selective sparsity into the KGC
regularizer. The core idea is to selectively penalize those components with
significant features in the embedding vector, thus effectively ignoring many
components that contribute little and may only represent noise. Various
comparative experiments on multiple datasets and multiple models show that the
SPR regularization method is better than other regularization methods and can
enable the KGC model to further break through the performance margin.

</details>


### [147] [Strategic Classification with Non-Linear Classifiers](https://arxiv.org/abs/2505.23443)
*Benyamin Trachtenberg,Nir Rosenfeld*

Main category: cs.LG

TL;DR: 在战略性分类中，标准监督学习设定被扩展以支持用户的战略性行为，这种行为以响应分类器的代价特征操作形式出现。尽管标准学习支持广泛的模型类别，但战略分类的研究主要集中在线性分类器上。本文通过探索非线性分类器下战略性行为的表现及其对学习的影响，试图扩大研究范围。我们自下而上地展示了非线性如何影响决策边界点、分类器表达能力以及模型类别的复杂性。一个关键发现是，一旦环境变得战略性，通用近似器（如神经网络）就不再具有通用性。我们还通过实验证明了这一点，即使在不受限的模型类别上，也可能产生性能差距。


<details>
  <summary>Details</summary>
Motivation: 当前的战略分类研究主要集中在线性分类器上，而非线性分类器下的战略性行为尚未得到充分探讨。因此，需要研究非线性分类器下战略性行为的表现及影响，以更全面地理解战略性分类问题。

Method: 采用自下而上的方法，分析非线性分类器如何影响决策边界点、分类器表达能力以及模型类别的复杂性。并通过实验验证非线性分类器在战略性环境下的表现及可能产生的性能差距。

Result: 发现非线性分类器在战略性环境下，通用近似器（如神经网络）不再具有通用性，并且即使在不受限的模型类别上，也可能产生性能差距。

Conclusion: 非线性分类器在战略性分类中的表现与线性分类器有显著差异，这表明需要重新评估和设计适用于战略性环境的非线性分类器。

Abstract: In strategic classification, the standard supervised learning setting is
extended to support the notion of strategic user behavior in the form of costly
feature manipulations made in response to a classifier. While standard learning
supports a broad range of model classes, the study of strategic classification
has, so far, been dedicated mostly to linear classifiers. This work aims to
expand the horizon by exploring how strategic behavior manifests under
non-linear classifiers and what this implies for learning. We take a bottom-up
approach showing how non-linearity affects decision boundary points, classifier
expressivity, and model classes complexity. A key finding is that universal
approximators (e.g., neural nets) are no longer universal once the environment
is strategic. We demonstrate empirically how this can create performance gaps
even on an unrestricted model class.

</details>


### [148] [Network Inversion for Uncertainty-Aware Out-of-Distribution Detection](https://arxiv.org/abs/2505.23448)
*Pirzada Suhail,Rehna Afroz,Amit Sethi*

Main category: cs.LG

TL;DR: 提出了一种结合网络反演与分类器训练的新框架，通过扩展分类器至(n+1)类模型（增加垃圾类）来同时解决OOD检测和不确定性估计问题。该方法无需外部OOD数据集或后验校准技术，具有可扩展性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，构建安全的机器学习系统需要有效的OOD检测和不确定性估计能力，以应对不可避免的意外输入。

Method: 提出将标准n类分类任务扩展为(n+1)类模型，新增一个由随机高斯噪声填充的“垃圾”类表示异常值输入。通过网络反演生成对应所有输出类别的输入图像，并将其排除到垃圾类进行重新训练。此过程迭代进行直至反演样本更接近分布内数据。

Result: 实验结果表明，该方法能够有效检测并拒绝OOD样本，同时利用预测置信度评估分布内外输入的不确定性。

Conclusion: 所提出的框架提供了一个统一且无需外部OOD数据集或后验校准技术的解决方案，适用于OOD检测和不确定性估计。

Abstract: Out-of-distribution (OOD) detection and uncertainty estimation (UE) are
critical components for building safe machine learning systems, especially in
real-world scenarios where unexpected inputs are inevitable. In this work, we
propose a novel framework that combines network inversion with classifier
training to simultaneously address both OOD detection and uncertainty
estimation. For a standard n-class classification task, we extend the
classifier to an (n+1)-class model by introducing a "garbage" class, initially
populated with random gaussian noise to represent outlier inputs. After each
training epoch, we use network inversion to reconstruct input images
corresponding to all output classes that initially appear as noisy and
incoherent and are therefore excluded to the garbage class for retraining the
classifier. This cycle of training, inversion, and exclusion continues
iteratively till the inverted samples begin to resemble the in-distribution
data more closely, suggesting that the classifier has learned to carve out
meaningful decision boundaries while sanitising the class manifolds by pushing
OOD content into the garbage class. During inference, this training scheme
enables the model to effectively detect and reject OOD samples by classifying
them into the garbage class. Furthermore, the confidence scores associated with
each prediction can be used to estimate uncertainty for both in-distribution
and OOD inputs. Our approach is scalable, interpretable, and does not require
access to external OOD datasets or post-hoc calibration techniques while
providing a unified solution to the dual challenges of OOD detection and
uncertainty estimation.

</details>


### [149] [Diffusion Guidance Is a Controllable Policy Improvement Operator](https://arxiv.org/abs/2505.23458)
*Kevin Frans,Seohong Park,Pieter Abbeel,Sergey Levine*

Main category: cs.LG

TL;DR: This paper introduces CFGRL, a framework that merges policy improvement with diffusion model guidance for offline reinforcement learning. It simplifies training using supervised learning techniques and eliminates the need for explicit value function learning.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning systems have been difficult to scale effectively despite their potential to learn beyond the data they are given. Meanwhile, generative modeling techniques have proven scalable and simple to train.

Method: The authors derive a direct relation between policy improvement and the guidance of diffusion models, creating a framework called CFGRL. This framework is trained using supervised learning methods and can improve upon policies in the data without needing to explicitly learn a value function.

Result: On offline RL tasks, increased guidance weighting leads to increased performance. The framework generalizes simple supervised methods like goal-conditioned behavioral cloning to prioritize optimality and gain performance improvements across the board.

Conclusion: CFGRL offers a way to enhance reinforcement learning through the use of diffusion model guidance within a supervised learning framework, providing reliable performance improvements in offline RL tasks.

Abstract: At the core of reinforcement learning is the idea of learning beyond the
performance in the data. However, scaling such systems has proven notoriously
tricky. In contrast, techniques from generative modeling have proven remarkably
scalable and are simple to train. In this work, we combine these strengths, by
deriving a direct relation between policy improvement and guidance of diffusion
models. The resulting framework, CFGRL, is trained with the simplicity of
supervised learning, yet can further improve on the policies in the data. On
offline RL tasks, we observe a reliable trend -- increased guidance weighting
leads to increased performance. Of particular importance, CFGRL can operate
without explicitly learning a value function, allowing us to generalize simple
supervised methods (e.g., goal-conditioned behavioral cloning) to further
prioritize optimality, gaining performance for "free" across the board.

</details>


### [150] [On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment](https://arxiv.org/abs/2505.23459)
*Safwan Labbi,Paul Mangold,Daniil Tiapkin,Eric Moulines*

Main category: cs.LG

TL;DR: This paper addresses the challenge of ensuring convergence in policy gradient methods within federated reinforcement learning under environmental heterogeneity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the issue of convergence in federated reinforcement learning where environments are heterogeneous. The authors note that heterogeneity can lead to optimal policies being non-deterministic or time-varying, even in tabular environments.

Method: The authors first establish theoretical results showing the necessity for non-deterministic or time-varying policies due to heterogeneity. They then prove global convergence results for the FedPG algorithm under specific conditions and introduce b-RS-FedPG, a novel method with a softmax-inspired parameterization and regularization scheme.

Result: Theoretical analysis shows that FedPG achieves linear speed-up with respect to the number of agents. Explicit convergence rates are demonstrated for b-RS-FedPG towards near-optimal stationary policies. Empirical results indicate that both FedPG and b-RS-FedPG outperform federated Q-learning in heterogeneous settings.

Conclusion: The paper concludes by highlighting the importance of considering heterogeneity in designing efficient federated reinforcement learning algorithms, and demonstrates the effectiveness of FedPG and b-RS-FedPG through theoretical and empirical evidence.

Abstract: Ensuring convergence of policy gradient methods in federated reinforcement
learning (FRL) under environment heterogeneity remains a major challenge. In
this work, we first establish that heterogeneity, perhaps counter-intuitively,
can necessitate optimal policies to be non-deterministic or even time-varying,
even in tabular environments. Subsequently, we prove global convergence results
for federated policy gradient (FedPG) algorithms employing local updates, under
a {\L}ojasiewicz condition that holds only for each individual agent, in both
entropy-regularized and non-regularized scenarios. Crucially, our theoretical
analysis shows that FedPG attains linear speed-up with respect to the number of
agents, a property central to efficient federated learning. Leveraging insights
from our theoretical findings, we introduce b-RS-FedPG, a novel policy gradient
method that employs a carefully constructed softmax-inspired parameterization
coupled with an appropriate regularization scheme. We further demonstrate
explicit convergence rates for b-RS-FedPG toward near-optimal stationary
policies. Finally, we demonstrate that empirically both FedPG and b-RS-FedPG
consistently outperform federated Q-learning on heterogeneous settings.

</details>


### [151] [Refining Labeling Functions with Limited Labeled Data](https://arxiv.org/abs/2505.23470)
*Chenjie Li,Amir Gilad,Boris Glavic,Zhengjie Miao,Sudeepa Roy*

Main category: cs.LG

TL;DR: The paper presents a method for repairing labeling functions (LFs) in programmatic weak supervision using minimal changes to their results on a small set of labeled examples, thereby improving the quality of generated labels.


<details>
  <summary>Details</summary>
Motivation: Programmatic weak supervision (PWS) reduces human effort for labeling data by combining outputs of user-provided LFs. However, the quality of generated labels is directly dependent on the accuracy of these LFs, necessitating a solution to improve LF accuracy.

Method: Develop novel techniques to repair LFs by minimally altering their results on labeled examples to ensure sufficient evidence for correct labels and maintain high LF accuracy. Model LFs as conditional rules to selectively change their output for certain inputs.

Result: Experimentally demonstrated improvement in the quality of LFs using surprisingly small sets of labeled datapoints.

Conclusion: The proposed system successfully enhances the accuracy of LFs with minimal changes based on a limited number of labeled examples.

Abstract: Programmatic weak supervision (PWS) significantly reduces human effort for
labeling data by combining the outputs of user-provided labeling functions
(LFs) on unlabeled datapoints. However, the quality of the generated labels
depends directly on the accuracy of the LFs. In this work, we study the problem
of fixing LFs based on a small set of labeled examples. Towards this goal, we
develop novel techniques for repairing a set of LFs by minimally changing their
results on the labeled examples such that the fixed LFs ensure that (i) there
is sufficient evidence for the correct label of each labeled datapoint and (ii)
the accuracy of each repaired LF is sufficiently high. We model LFs as
conditional rules which enables us to refine them, i.e., to selectively change
their output for some inputs. We demonstrate experimentally that our system
improves the quality of LFs based on surprisingly small sets of labeled
datapoints.

</details>


### [152] [SGD as Free Energy Minimization: A Thermodynamic View on Neural Network Training](https://arxiv.org/abs/2505.23489)
*Ildus Sadrtdinov,Ivan Klimov,Ekaterina Lobacheva,Dmitry Vetrov*

Main category: cs.LG

TL;DR: This paper interprets the behavior of SGD in neural network training through thermodynamics, revealing how learning rates affect the balance between loss minimization and weight entropy.


<details>
  <summary>Details</summary>
Motivation: To understand why high learning rates prevent convergence to loss minima and how different learning rates lead to stabilization at different loss levels in neural network training.

Method: By presenting a thermodynamic interpretation of stochastic gradient descent (SGD) under fixed learning rates, showing that SGD minimizes a free energy function balancing training loss and the entropy of the weights distribution.

Result: Empirical validation shows that underparameterized models follow free energy minimization with temperature increasing monotonically with learning rate, while overparameterized models effectively minimize loss directly at low learning rates.

Conclusion: The difference in behavior between underparameterized and overparameterized models is attributed to differences in the signal-to-noise ratio of stochastic gradients near optima.

Abstract: We present a thermodynamic interpretation of the stationary behavior of
stochastic gradient descent (SGD) under fixed learning rates (LRs) in neural
network training. We show that SGD implicitly minimizes a free energy function
$F=U-TS$, balancing training loss $U$ and the entropy of the weights
distribution $S$, with temperature $T$ determined by the LR. This perspective
offers a new lens on why high LRs prevent training from converging to the loss
minima and how different LRs lead to stabilization at different loss levels. We
empirically validate the free energy framework on both underparameterized (UP)
and overparameterized (OP) models. UP models consistently follow free energy
minimization, with temperature increasing monotonically with LR, while for OP
models, the temperature effectively drops to zero at low LRs, causing SGD to
minimize the loss directly and converge to an optimum. We attribute this
mismatch to differences in the signal-to-noise ratio of stochastic gradients
near optima, supported by both a toy example and neural network experiments.

</details>


### [153] [Epistemic Errors of Imperfect Multitask Learners When Distributions Shift](https://arxiv.org/abs/2505.23496)
*Sabina J. Sloman,Michele Caprio,Samuel Kaski*

Main category: cs.LG

TL;DR: 在数据噪声的情况下，统计学习者的目标是解决关于测试时间遇到的数据的外在不确定性，即识别测试数据的分布。本文引入了外在误差的定义，并提供了一个通用的、分解性的外在误差界限，考虑了多种外在不确定性的来源，并将误差分别归因于学习过程和环境的多个方面。此外，还提供了贝叶斯迁移学习和分布转移情况下的具体外在误差界限以及相应的泛化界限，同时提出了负迁移的新定义并进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的学习场景中，训练数据可能来自多个任务（多任务学习），目标数据可能与源数据存在系统性差异（分布转移），学习者可能无法准确描述源数据（不完美学习）。因此需要一种方法来量化和分解这些不确定性对外在误差的影响。

Method: 1. 提出外在误差的定义及通用的分解性外在误差界限。
2. 将误差归因于学习过程和环境的多个方面。
3. 提供贝叶斯迁移学习和分布转移情况下的具体外在误差界限。
4. 提出新的负迁移定义并进行实验验证。

Result: 1. 成功定义了外在误差，并提供了通用的分解性外在误差界限。
2. 提供了针对特定学习场景的具体外在误差界限和泛化界限。
3. 新定义的负迁移概念得到了实验验证。

Conclusion: 本文提出的外在误差界限为理解多种外在不确定性来源对学习过程的影响提供了一个框架，同时为迁移学习和分布转移等场景提供了理论支持。新定义的负迁移也加深了对迁移学习的理解。

Abstract: When data are noisy, a statistical learner's goal is to resolve epistemic
uncertainty about the data it will encounter at test-time, i.e., to identify
the distribution of test (target) data. Many real-world learning settings
introduce sources of epistemic uncertainty that can not be resolved on the
basis of training (source) data alone: The source data may arise from multiple
tasks (multitask learning), the target data may differ systematically from the
source data tasks (distribution shift), and/or the learner may not arrive at an
accurate characterization of the source data (imperfect learning). We introduce
a principled definition of epistemic error, and provide a generic,
decompositional epistemic error bound. Our error bound is the first to (i)
consider epistemic error specifically, (ii) accommodate all the sources of
epistemic uncertainty above, and (iii) separately attribute the error to each
of multiple aspects of the learning procedure and environment. As corollaries
of the generic result, we provide (i) epistemic error bounds specialized to the
settings of Bayesian transfer learning and distribution shift within
$\epsilon$-neighborhoods, and (ii) a set of corresponding generalization
bounds. Finally, we provide a novel definition of negative transfer, and
validate its insights in a synthetic experimental setting.

</details>


### [154] [Why Machine Learning Models Fail to Fully Capture Epistemic Uncertainty](https://arxiv.org/abs/2505.23506)
*Sebastián Jiménez,Mira Jürgens,Willem Waegeman*

Main category: cs.LG

TL;DR: 当前基于二阶分布的解耦方法未能充分捕捉认知不确定性，特别是模型偏差的影响。研究通过理论分析和实验表明，高模型偏差可能导致对认知不确定性的低估，并且常见的不确定性量化方法系统性地将偏差诱导的误差混入随机误差估计中，从而低估认知不确定性。只有在充分表示所有相关认知不确定性来源的情况下，才能获得有意义的随机不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在解耦随机性和认知不确定性方面存在不足，特别是在忽视模型偏差这一关键因素时，无法全面捕捉认知不确定性。

Method: 采用更精细的认知不确定性分类法，结合经典偏差-方差分解，通过模拟评估协议（涵盖过程和数据驱动的不确定性）来分析现有方法的表现。并通过理论分析与合成实验验证模型偏差对不确定性估计的影响。

Result: 研究表明，高模型偏差会导致认知不确定性的误导性低估，常见方法会将偏差引起的误差混入随机不确定性估计中，从而低估认知不确定性。

Conclusion: 只有在充分表示所有相关认知不确定性来源的情况下，才能获得有意义的随机不确定性估计。

Abstract: In recent years various supervised learning methods that disentangle
aleatoric and epistemic uncertainty based on second-order distributions have
been proposed. We argue that these methods fail to capture critical components
of epistemic uncertainty, particularly due to the often-neglected component of
model bias. To show this, we make use of a more fine-grained taxonomy of
epistemic uncertainty sources in machine learning models, and analyse how the
classical bias-variance decomposition of the expected prediction error can be
decomposed into different parts reflecting these uncertainties. By using a
simulation-based evaluation protocol which encompasses epistemic uncertainty
due to both procedural- and data-driven uncertainty components, we illustrate
that current methods rarely capture the full spectrum of epistemic uncertainty.
Through theoretical insights and synthetic experiments, we show that high model
bias can lead to misleadingly low estimates of epistemic uncertainty, and
common second-order uncertainty quantification methods systematically blur
bias-induced errors into aleatoric estimates, thereby underrepresenting
epistemic uncertainty. Our findings underscore that meaningful aleatoric
estimates are feasible only if all relevant sources of epistemic uncertainty
are properly represented.

</details>


### [155] [AnchorAttention: Difference-Aware Sparse Attention with Stripe Granularity](https://arxiv.org/abs/2505.23520)
*Yu Zhang,Dong Guo,Fang Wu,Guoliang Zhu,Dian Ding,Yiming Zhang*

Main category: cs.LG

TL;DR: 在大规模语言模型中，自注意力机制的二次复杂度使得预填充阶段面临巨大计算挑战。现有的动态模式匹配和块稀疏低级实现方法存在局限性，如无法捕捉全局上下文信息以及持续的内部稀疏性问题。本文提出了AnchorAttention，一种差异感知的动态稀疏注意力机制，通过更精细的条纹粒度适应全局上下文信息，从而提高速度和准确性。AnchorAttention包含三个关键组件：基于模式的锚点计算、差异感知的条纹稀疏性识别和细粒度稀疏计算。与先前的最先进方法相比，在文本长度为128k时，AnchorAttention实现了1.44倍的速度提升，并保持更高的召回率。


<details>
  <summary>Details</summary>
Motivation: 现有的处理长上下文自注意力的方法（如动态模式匹配和块稀疏低级实现）存在两个主要问题：一是依赖局部信息进行模式识别，无法捕捉全局上下文；二是块粒度过粗导致内部持续稀疏，从而影响准确性和效率。因此需要一种新的机制来解决这些问题。

Method: AnchorAttention是一种差异感知的动态稀疏注意力机制，具有以下三个关键组件：
1. 基于模式的锚点计算：利用所有输入中的共性快速计算一组接近最大值的分数作为锚点。
2. 差异感知的条纹稀疏性识别：通过与锚点进行差异感知比较，快速获得重要区域的离散坐标，形成条纹状稀疏模式。
3. 细粒度稀疏计算：用同时加载离散KV位置的方法替代传统的连续KV块加载方法，以最大化稀疏率并保留硬件计算潜力。

Result: 实验表明，AnchorAttention能够在相同的召回水平下实现更高的稀疏率，显著减少计算时间。特别是在文本长度为128k时，相比之前最先进的方法，AnchorAttention实现了1.44倍的速度提升，同时保持更高的召回率。

Conclusion: AnchorAttention通过其更精细的稀疏策略，在保持高召回率的同时显著减少了计算时间，解决了现有方法无法有效捕捉全局上下文信息和持续内部稀疏的问题，适用于大规模语言模型的长上下文处理。

Abstract: Large Language Models (LLMs) with extended context lengths face significant
computational challenges during the pre-filling phase, primarily due to the
quadratic complexity of self-attention. Existing methods typically employ
dynamic pattern matching and block-sparse low-level implementations. However,
their reliance on local information for pattern identification fails to capture
global contexts, and the coarse granularity of blocks leads to persistent
internal sparsity, resulting in suboptimal accuracy and efficiency. To address
these limitations, we propose \textbf{AnchorAttention}, a difference-aware,
dynamic sparse attention mechanism that efficiently identifies critical
attention regions at a finer stripe granularity while adapting to global
contextual information, achieving superior speed and accuracy. AnchorAttention
comprises three key components: (1) \textbf{Pattern-based Anchor Computation},
leveraging the commonalities present across all inputs to rapidly compute a set
of near-maximum scores as the anchor; (2) \textbf{Difference-aware Stripe
Sparsity Identification}, performing difference-aware comparisons with the
anchor to quickly obtain discrete coordinates of significant regions in a
stripe-like sparsity pattern; (3) \textbf{Fine-grained Sparse Computation},
replacing the traditional contiguous KV block loading approach with
simultaneous discrete KV position loading to maximize sparsity rates while
preserving full hardware computational potential. With its finer-grained
sparsity strategy, \textbf{AnchorAttention} achieves higher sparsity rates at
the same recall level, significantly reducing computation time. Compared to
previous state-of-the-art methods, at a text length of 128k, it achieves a
speedup of 1.44$\times$ while maintaining higher recall rates.

</details>


### [156] [Accelerating AllReduce with a Persistent Straggler](https://arxiv.org/abs/2505.23523)
*Arjun Devraj,Eric Ding,Abhishek Vijaya Kumar,Robert Kleinberg,Rachee Singh*

Main category: cs.LG

TL;DR: StragglAR is an AllReduce algorithm that accelerates distributed machine learning workloads in the presence of persistent stragglers by implementing a ReduceScatter among remaining GPUs during delay and executing a novel collective algorithm once the straggler reaches synchronization barrier.


<details>
  <summary>Details</summary>
Motivation: Bulk-synchronous AllReduce algorithms can be delayed by a persistent straggler, which slows down distributed training and inference.

Method: StragglAR implements a ReduceScatter among remaining GPUs during the delay caused by stragglers and then executes a new collective algorithm to complete AllReduce once the straggler reaches the synchronization barrier.

Result: StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient AllReduce algorithms for large GPU clusters with persistent stragglers and yields a 22% speedup on an 8-GPU server compared to state-of-the-art AllReduce algorithms.

Conclusion: StragglAR effectively addresses the challenge of persistent stragglers in distributed machine learning, offering significant speedups.

Abstract: Distributed machine learning workloads use data and tensor parallelism for
training and inference, both of which rely on the AllReduce collective to
synchronize gradients or activations. However, bulk-synchronous AllReduce
algorithms can be delayed by a persistent straggler that is slower to reach the
synchronization barrier required to begin the collective. To address this
challenge, we propose StragglAR: an AllReduce algorithm that accelerates
distributed training and inference in the presence of persistent stragglers.
StragglAR implements a ReduceScatter among the remaining GPUs during the
straggler-induced delay, and then executes a novel collective algorithm to
complete the AllReduce once the straggler reaches the synchronization barrier.
StragglAR achieves a 2x theoretical speedup over popular bandwidth-efficient
AllReduce algorithms (e.g., Ring) for large GPU clusters with persistent
stragglers. On an 8-GPU server, our implementation of StragglAR yields a 22%
speedup over state-of-the-art AllReduce algorithms.

</details>


### [157] [Normalizing Flows are Capable Models for RL](https://arxiv.org/abs/2505.23527)
*Raj Ghugare,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: Normalizing flows can be effectively used in reinforcement learning algorithms, providing simpler methods and better performance.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of normalizing flows (NFs) in reinforcement learning (RL) as an alternative to other complex models such as transformers, energy-based models, and diffusion/flow-based models.

Method: A single NF architecture is proposed which can be integrated into RL algorithms serving as a policy, Q-function, and occupancy measure.

Result: The approach results in simpler algorithms and achieves higher performance in various RL settings including imitation learning, offline, goal conditioned RL and unsupervised RL.

Conclusion: Normalizing flows are sufficiently expressive for use in RL and can lead to more efficient and effective algorithms.

Abstract: Modern reinforcement learning (RL) algorithms have found success by using
powerful probabilistic models, such as transformers, energy-based models, and
diffusion/flow-based models. To this end, RL researchers often choose to pay
the price of accommodating these models into their algorithms -- diffusion
models are expressive, but are computationally intensive due to their reliance
on solving differential equations, while autoregressive transformer models are
scalable but typically require learning discrete representations. Normalizing
flows (NFs), by contrast, seem to provide an appealing alternative, as they
enable likelihoods and sampling without solving differential equations or
autoregressive architectures. However, their potential in RL has received
limited attention, partly due to the prevailing belief that normalizing flows
lack sufficient expressivity. We show that this is not the case. Building on
recent work in NFs, we propose a single NF architecture which integrates
seamlessly into RL algorithms, serving as a policy, Q-function, and occupancy
measure. Our approach leads to much simpler algorithms, and achieves higher
performance in imitation learning, offline, goal conditioned RL and
unsupervised RL.

</details>


### [158] [Comparative assessment of fairness definitions and bias mitigation strategies in machine learning-based diagnosis of Alzheimer's disease from MR images](https://arxiv.org/abs/2505.23528)
*Maria Eleftheria Vlontzou,Maria Athanasiou,Christos Davatzikos,Konstantina S. Nikita*

Main category: cs.LG

TL;DR: The study conducts a thorough fairness analysis of ML models for diagnosing MCI and AD from MRI features, examining biases related to age, race, and gender. It introduces a new measure balancing fairness and performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate and address potential biases in ML models used for diagnosing MCI and AD, ensuring fairer medical diagnostic applications.

Method: Performing comprehensive fairness analysis on ML models using neuroimaging data, assessing various fairness metrics, comparing bias mitigation strategies, and introducing a novel composite measure.

Result: Biases were found related to age and race but not gender. Different mitigation strategies showed varying effectiveness across sensitive attributes.

Conclusion: Variations in AD neuropathology and risk factors linked to demographic characteristics significantly impact model fairness.

Abstract: The present study performs a comprehensive fairness analysis of machine
learning (ML) models for the diagnosis of Mild Cognitive Impairment (MCI) and
Alzheimer's disease (AD) from MRI-derived neuroimaging features. Biases
associated with age, race, and gender in a multi-cohort dataset, as well as the
influence of proxy features encoding these sensitive attributes, are
investigated. The reliability of various fairness definitions and metrics in
the identification of such biases is also assessed. Based on the most
appropriate fairness measures, a comparative analysis of widely used
pre-processing, in-processing, and post-processing bias mitigation strategies
is performed. Moreover, a novel composite measure is introduced to quantify the
trade-off between fairness and performance by considering the F1-score and the
equalized odds ratio, making it appropriate for medical diagnostic
applications. The obtained results reveal the existence of biases related to
age and race, while no significant gender bias is observed. The deployed
mitigation strategies yield varying improvements in terms of fairness across
the different sensitive attributes and studied subproblems. For race and
gender, Reject Option Classification improves equalized odds by 46% and 57%,
respectively, and achieves harmonic mean scores of 0.75 and 0.80 in the MCI
versus AD subproblem, whereas for age, in the same subproblem, adversarial
debiasing yields the highest equalized odds improvement of 40% with a harmonic
mean score of 0.69. Insights are provided into how variations in AD
neuropathology and risk factors, associated with demographic characteristics,
influence model fairness.

</details>


### [159] [Subgraph Gaussian Embedding Contrast for Self-Supervised Graph Representation Learning](https://arxiv.org/abs/2505.23529)
*Shifeng Xie,Aref Einizade,Jhony H. Giraldo*

Main category: cs.LG

TL;DR: The paper proposes SubGEC, a novel method for Graph Representation Learning using Self-Supervised Learning that employs subgraph Gaussian embedding and optimal transport distances to improve contrastive learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of encoding high-dimensional graph-structured data into low-dimensional vectors in a cost-effective manner without requiring expensive human annotations.

Method: Proposes SubGEC which uses a subgraph Gaussian embedding module to map subgraphs to a structured Gaussian space and applies Wasserstein and Gromov-Wasserstein distances to measure subgraph similarity.

Result: SubGEC outperforms or matches state-of-the-art methods across multiple benchmarks.

Conclusion: This work highlights the significance of the distribution of generated contrastive pairs in designing effective SSL methods for GRL.

Abstract: Graph Representation Learning (GRL) is a fundamental task in machine
learning, aiming to encode high-dimensional graph-structured data into
low-dimensional vectors. Self-Supervised Learning (SSL) methods are widely used
in GRL because they can avoid expensive human annotation. In this work, we
propose a novel Subgraph Gaussian Embedding Contrast (SubGEC) method. Our
approach introduces a subgraph Gaussian embedding module, which adaptively maps
subgraphs to a structured Gaussian space, ensuring the preservation of input
subgraph characteristics while generating subgraphs with a controlled
distribution. We then employ optimal transport distances, more precisely the
Wasserstein and Gromov-Wasserstein distances, to effectively measure the
similarity between subgraphs, enhancing the robustness of the contrastive
learning process. Extensive experiments across multiple benchmarks demonstrate
that \method~outperforms or presents competitive performance against
state-of-the-art approaches. Our findings provide insights into the design of
SSL methods for GRL, emphasizing the importance of the distribution of the
generated contrastive pairs.

</details>


### [160] [Domain-Aware Tensor Network Structure Search](https://arxiv.org/abs/2505.23537)
*Giorgos Iacovides,Wuyang Zhou,Chao Li,Qibin Zhao,Danilo Mandic*

Main category: cs.LG

TL;DR: The paper introduces tnLLM, a novel tensor network structure search (TN-SS) framework leveraging domain information and large language models (LLMs). It reduces function evaluations compared to state-of-the-art algorithms while maintaining performance, provides domain-aware explanations, and accelerates convergence for sampling-based methods.


<details>
  <summary>Details</summary>
Motivation: Current TN-SS algorithms are computationally expensive and do not utilize valuable domain information inherent in real-world tensor data. They also lack transparency in identified structures.

Method: Propose tnLLM framework which incorporates domain information about the data and uses reasoning capabilities of LLMs to predict suitable TN structures. It includes a domain-aware prompting pipeline instructing the LLM to infer TN structures based on relationships between tensor modes.

Result: tnLLM achieves comparable TN-SS objective function values with much fewer function evaluations than SOTA algorithms. LLM-enabled domain information helps find good initializations for sampling-based SOTA methods, accelerating their convergence while preserving theoretical guarantees.

Conclusion: tnLLM offers an efficient and transparent approach to TN-SS by integrating domain knowledge and LLMs, significantly reducing computational cost and providing interpretable results.

Abstract: Tensor networks (TNs) provide efficient representations of high-dimensional
data, yet identification of the optimal TN structures, the so called tensor
network structure search (TN-SS) problem, remains a challenge. Current
state-of-the-art (SOTA) algorithms are computationally expensive as they
require extensive function evaluations, which is prohibitive for real-world
applications. In addition, existing methods ignore valuable domain information
inherent in real-world tensor data and lack transparency in their identified TN
structures. To this end, we propose a novel TN-SS framework, termed the tnLLM,
which incorporates domain information about the data and harnesses the
reasoning capabilities of large language models (LLMs) to directly predict
suitable TN structures. The proposed framework involves a domain-aware
prompting pipeline which instructs the LLM to infer suitable TN structures
based on the real-world relationships between tensor modes. In this way, our
approach is capable of not only iteratively optimizing the objective function,
but also generating domain-aware explanations for the identified structures.
Experimental results demonstrate that tnLLM achieves comparable TN-SS objective
function values with much fewer function evaluations compared to SOTA
algorithms. Furthermore, we demonstrate that the LLM-enabled domain information
can be used to find good initializations in the search space for sampling-based
SOTA methods to accelerate their convergence while preserving theoretical
performance guarantees.

</details>


### [161] [Comparing the Moore-Penrose Pseudoinverse and Gradient Descent for Solving Linear Regression Problems: A Performance Analysis](https://arxiv.org/abs/2505.23552)
*Alex Adams*

Main category: cs.LG

TL;DR: This paper compares two methods for linear regression problems: Moore-Penrose pseudoinverse and gradient descent, analyzing their computational complexity and empirical behavior.


<details>
  <summary>Details</summary>
Motivation: To provide practical guidance for choosing between direct, exact solutions (Moore-Penrose pseudoinverse) and iterative, approximate solutions (gradient descent) for linear regression tasks.

Method: Theoretical review of both methods, analysis of computational complexity, and empirical evaluation on synthetic and real-world datasets.

Result: Identifies the conditions under which each method performs better in terms of computational time, numerical stability, and predictive accuracy.

Conclusion: Researchers and practitioners can use these findings to make informed decisions when selecting a solver for linear regression problems.

Abstract: This paper investigates the comparative performance of two fundamental
approaches to solving linear regression problems: the closed-form Moore-Penrose
pseudoinverse and the iterative gradient descent method. Linear regression is a
cornerstone of predictive modeling, and the choice of solver can significantly
impact efficiency and accuracy. I review and discuss the theoretical
underpinnings of both methods, analyze their computational complexity, and
evaluate their empirical behavior on synthetic datasets with controlled
characteristics, as well as on established real-world datasets. My results
delineate the conditions under which each method excels in terms of
computational time, numerical stability, and predictive accuracy. This work
aims to provide practical guidance for researchers and practitioners in machine
learning when selecting between direct, exact solutions and iterative,
approximate solutions for linear regression tasks.

</details>


### [162] [Adaptive Federated LoRA in Heterogeneous Wireless Networks with Independent Sampling](https://arxiv.org/abs/2505.23555)
*Yanzhao Hou,Jiaxiang Geng,Boyu Li,Xiaofeng Tao,Juncheng Wang,Xiaodong Xu,Bing Luo*

Main category: cs.LG

TL;DR: 提出了一种自适应的Federated LoRA策略，通过独立客户端采样优化联合微调的收敛时间，并引入自适应带宽分配方案和非凸优化问题解决方法，显著减少多种模型和数据集上的训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有的Federated LoRA方法未能充分考虑系统和数据异质性对整体训练效率（特别是实际运行时间）的影响。

Method: 1. 推导出新的联邦LoRA收敛边界，支持任意独立客户端采样且无需严格梯度限制假设。
2. 提出自适应带宽分配方案，考虑异构客户端资源和系统带宽约束。
3. 通过理论推导，构建并求解非凸优化问题，确定LoRA草图比例和采样概率以最小化实际运行时间。
4. 开发高效低复杂度算法近似最优解。

Result: 广泛的实验表明，该方法在不同模型和数据集上相比现有最先进方法显著减少了实际运行训练时间。

Conclusion: 所提出的自适应Federated LoRA策略有效优化了联合微调的实际运行时间，适用于计算和通信异构环境。

Abstract: Federated LoRA has emerged as a promising technique for efficiently
fine-tuning large language models (LLMs) on distributed devices by reducing the
number of trainable parameters. However, existing approaches often inadequately
overlook the theoretical and practical implications of system and data
heterogeneity, thereby failing to optimize the overall training efficiency,
particularly in terms of wall-clock time. In this paper, we propose an adaptive
federated LoRA strategy with independent client sampling to minimize the
convergence wall-clock time of federated fine-tuning under both computation and
communication heterogeneity. We first derive a new convergence bound for
federated LoRA with arbitrary and independent client sampling, notably without
requiring the stringent bounded gradient assumption. Then, we introduce an
adaptive bandwidth allocation scheme that accounts for heterogeneous client
resources and system bandwidth constraints. Based on the derived theory, we
formulate and solve a non-convex optimization problem to jointly determine the
LoRA sketching ratios and sampling probabilities, aiming to minimize wall-clock
convergence time. An efficient and low-complexity algorithm is developed to
approximate the solution. Finally, extensive experiments demonstrate that our
approach significantly reduces wall-clock training time compared to
state-of-the-art methods across various models and datasets.

</details>


### [163] [Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models](https://arxiv.org/abs/2505.23564)
*Yiran Guo,Lijie Xu,Jie Liu,Dan Ye,Shuang Qiu*

Main category: cs.LG

TL;DR: 提出了一种新的强化学习框架SPO，通过中间粒度的片段级优势估计来增强大语言模型的推理能力，相较于现有方法在GSM8K和MATH500数据集上均有显著准确性提升。


<details>
  <summary>Details</summary>
Motivation: 当前使用强化学习有效增强大型语言模型推理能力的方法存在局限性：基于Token级别的方法难以训练精确的评估模型，而基于轨迹级别的方法则由于最终奖励信号过于粗糙导致责任分配不准确。

Method: 提出了Segment Policy Optimization（SPO）框架，利用片段级的优势估计，在细粒度和粗粒度之间取得更好的平衡。SPO包含三个组件：灵活的片段划分、精确的片段优势估计以及使用片段优势进行策略优化（包括一种新的概率掩码策略）。此外，SPO针对短链和长链推理分别设计了SPO-chain和SPO-tree两种具体实现方式。

Result: 在GSM8K数据集上，SPO-chain相比PPO和GRPO提高了6-12个百分点的准确性；在MATH500数据集上，SPO-tree在2K和4K上下文中比GRPO提高了7-11个百分点的准确性。

Conclusion: SPO提供了一种更精确且高效的强化学习方法以增强大语言模型的推理能力，并在多个基准测试中表现优异。

Abstract: Enhancing the reasoning capabilities of large language models effectively
using reinforcement learning (RL) remains a crucial challenge. Existing
approaches primarily adopt two contrasting advantage estimation granularities:
Token-level methods (e.g., PPO) aim to provide the fine-grained advantage
signals but suffer from inaccurate estimation due to difficulties in training
an accurate critic model. On the other extreme, trajectory-level methods (e.g.,
GRPO) solely rely on a coarse-grained advantage signal from the final reward,
leading to imprecise credit assignment. To address these limitations, we
propose Segment Policy Optimization (SPO), a novel RL framework that leverages
segment-level advantage estimation at an intermediate granularity, achieving a
better balance by offering more precise credit assignment than trajectory-level
methods and requiring fewer estimation points than token-level methods,
enabling accurate advantage estimation based on Monte Carlo (MC) without a
critic model. SPO features three components with novel strategies: (1) flexible
segment partition; (2) accurate segment advantage estimation; and (3) policy
optimization using segment advantages, including a novel probability-mask
strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain
for short chain-of-thought (CoT), featuring novel cutpoint-based partition and
chain-based advantage estimation, achieving $6$-$12$ percentage point
improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,
featuring novel tree-based advantage estimation, which significantly reduces
the cost of MC estimation, achieving $7$-$11$ percentage point improvements
over GRPO on MATH500 under 2K and 4K context evaluation. We make our code
publicly available at https://github.com/AIFrameResearch/SPO.

</details>


### [164] [DRO: A Python Library for Distributionally Robust Optimization in Machine Learning](https://arxiv.org/abs/2505.23565)
*Jiashuo Liu,Tianyu Wang,Henry Lam,Hongseok Namkoong,Jose Blanchet*

Main category: cs.LG

TL;DR: The paper introduces dro, an open-source Python library for distributionally robust optimization (DRO) which implements various DRO formulations and backbone models, is compatible with multiple frameworks, and significantly reduces runtime on large-scale datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and efficient tool for performing distributionally robust optimization in regression and classification problems.

Method: Development of the dro library which includes 14 DRO formulations, 9 backbone models, compatibility with scikit-learn and PyTorch, and utilization of vectorization and optimization approximation techniques to improve runtime efficiency.

Result: The dro library enables 79 distinct DRO methods and reduces runtime by 10x to over 1000x compared to baseline implementations on large-scale datasets.

Conclusion: dro is an efficient and versatile library for distributionally robust optimization, supported by comprehensive documentation.

Abstract: We introduce dro, an open-source Python library for distributionally robust
optimization (DRO) for regression and classification problems. The library
implements 14 DRO formulations and 9 backbone models, enabling 79 distinct DRO
methods. Furthermore, dro is compatible with both scikit-learn and PyTorch.
Through vectorization and optimization approximation techniques, dro reduces
runtime by 10x to over 1000x compared to baseline implementations on
large-scale datasets. Comprehensive documentation is available at
https://python-dro.org.

</details>


### [165] [Maximum Likelihood Learning of Latent Dynamics Without Reconstruction](https://arxiv.org/abs/2505.23569)
*Samo Hromadka,Kai Biegun,Lior Fox,James Heald,Maneesh Sahani*

Main category: cs.LG

TL;DR: A novel unsupervised learning method for time series data, RP-GSSM, combines probabilistic modeling and contrastive methods to learn latent dynamics effectively.


<details>
  <summary>Details</summary>
Motivation: Current methods for learning latent dynamics in time series data either lack expressivity or require complex regularization. There is a need for a more straightforward and effective approach that can capture nonlinear stochastic dynamics.

Method: The RP-GSSM model uses Markovian Gaussian latents to explain statistical dependence between observations at different time steps. It employs maximum likelihood learning, avoids explicit network mapping from latents to observations, and utilizes an arbitrarily nonlinear neural network link for expressivity.

Result: The RP-GSSM outperforms alternative methods on tasks involving learning nonlinear stochastic dynamics from video data, even in the presence of background distractors.

Conclusion: RP-GSSM provides a useful foundation model for downstream applications due to its ability to learn task-relevant latents without ad-hoc regularization, auxiliary losses, or optimizer scheduling.

Abstract: We introduce a novel unsupervised learning method for time series data with
latent dynamical structure: the recognition-parametrized Gaussian state space
model (RP-GSSM). The RP-GSSM is a probabilistic model that learns Markovian
Gaussian latents explaining statistical dependence between observations at
different time steps, combining the intuition of contrastive methods with the
flexible tools of probabilistic generative models. Unlike contrastive
approaches, the RP-GSSM is a valid probabilistic model learned via maximum
likelihood. Unlike generative approaches, the RP-GSSM has no need for an
explicit network mapping from latents to observations, allowing it to focus
model capacity on inference of latents. The model is both tractable and
expressive: it admits exact inference thanks to its jointly Gaussian latent
prior, while maintaining expressivity with an arbitrarily nonlinear neural
network link between observations and latents. These qualities allow the
RP-GSSM to learn task-relevant latents without ad-hoc regularization, auxiliary
losses, or optimizer scheduling. We show how this approach outperforms
alternatives on problems that include learning nonlinear stochastic dynamics
from video, with or without background distractors. Our results position the
RP-GSSM as a useful foundation model for a variety of downstream applications.

</details>


### [166] [BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model](https://arxiv.org/abs/2505.23579)
*Adibvafa Fallahpour,Andrew Magnuson,Purav Gupta,Shihao Ma,Jack Naimer,Arnav Shah,Haonan Duan,Omar Ibrahim,Hani Goodarzi,Chris J. Maddison,Bo Wang*

Main category: cs.LG

TL;DR: BioReason is a new architecture that integrates a DNA foundation model with a Large Language Model (LLM), allowing for improved biological reasoning and offering interpretable, step-by-step biological traces.


<details>
  <summary>Details</summary>
Motivation: Current DNA foundation models have strong sequence representation but struggle with multi-step reasoning and lack transparent, biologically intuitive explanations.

Method: BioReason deeply integrates a DNA foundation model with an LLM, enabling the LLM to process genomic information directly. It uses supervised fine-tuning and targeted reinforcement learning for multi-step reasoning.

Result: On benchmarks like KEGG-based disease pathway prediction, accuracy improves from 88% to 97%, and there's an average 15% performance gain over single-modality baselines in variant effect prediction.

Conclusion: BioReason offers a transformative approach for AI in biology, providing deeper mechanistic insights and accelerating hypothesis generation from genomic data.

Abstract: Unlocking deep, interpretable biological reasoning from complex genomic data
is a major AI challenge hindering scientific discovery. Current DNA foundation
models, despite strong sequence representation, struggle with multi-step
reasoning and lack inherent transparent, biologically intuitive explanations.
We introduce BioReason, a pioneering architecture that, for the first time,
deeply integrates a DNA foundation model with a Large Language Model (LLM).
This novel connection enables the LLM to directly process and reason with
genomic information as a fundamental input, fostering a new form of multimodal
biological understanding. BioReason's sophisticated multi-step reasoning is
developed through supervised fine-tuning and targeted reinforcement learning,
guiding the system to generate logical, biologically coherent deductions. On
biological reasoning benchmarks including KEGG-based disease pathway prediction
- where accuracy improves from 88% to 97% - and variant effect prediction,
BioReason demonstrates an average 15% performance gain over strong
single-modality baselines. BioReason reasons over unseen biological entities
and articulates decision-making through interpretable, step-by-step biological
traces, offering a transformative approach for AI in biology that enables
deeper mechanistic insights and accelerates testable hypothesis generation from
genomic data. Data, code, and checkpoints are publicly available at
https://github.com/bowang-lab/BioReason

</details>


### [167] [Improving Time Series Forecasting via Instance-aware Post-hoc Revision](https://arxiv.org/abs/2505.23583)
*Zhiding Liu,Mingyue Cheng,Guanhao Zhao,Jiqian Yang,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: Time series forecasting is crucial in real-world applications. Despite recent advancements, instance-level variations remain a challenge due to distribution shifts, missing data, and long-tail patterns. To tackle this, the paper proposes PIR, a model-agnostic framework that enhances forecasting performance through Post-forecasting Identification and Revision. PIR identifies biased forecasts and revises them using contextual information from local and global perspectives. Experiments show that PIR reduces instance-level errors and improves reliability.


<details>
  <summary>Details</summary>
Motivation: Recent time series forecasting methods have achieved high accuracy but still face challenges with instance-level variations caused by distribution shifts, missing data, and long-tail patterns.

Method: PIR is a model-agnostic framework that first identifies biased forecasting instances by estimating their accuracy, then revises the forecasts using contextual information including covariates and historical time series from both local and global perspectives.

Result: Extensive experiments on real-world datasets demonstrate that PIR effectively mitigates instance-level errors and significantly improves forecasting reliability when used with mainstream forecasting models.

Conclusion: PIR addresses the issue of instance-level variations in time series forecasting, leading to more reliable forecasts.

Abstract: Time series forecasting plays a vital role in various real-world applications
and has attracted significant attention in recent decades. While recent methods
have achieved remarkable accuracy by incorporating advanced inductive biases
and training strategies, we observe that instance-level variations remain a
significant challenge. These variations--stemming from distribution shifts,
missing data, and long-tail patterns--often lead to suboptimal forecasts for
specific instances, even when overall performance appears strong. To address
this issue, we propose a model-agnostic framework, PIR, designed to enhance
forecasting performance through Post-forecasting Identification and Revision.
Specifically, PIR first identifies biased forecasting instances by estimating
their accuracy. Based on this, the framework revises the forecasts using
contextual information, including covariates and historical time series, from
both local and global perspectives in a post-processing fashion. Extensive
experiments on real-world datasets with mainstream forecasting models
demonstrate that PIR effectively mitigates instance-level errors and
significantly improves forecasting reliability.

</details>


### [168] [On-Policy RL with Optimal Reward Baseline](https://arxiv.org/abs/2505.23585)
*Yaru Hao,Li Dong,Xun Wu,Shaohan Huang,Zewen Chi,Furu Wei*

Main category: cs.LG

TL;DR: This paper proposes On-Policy RL with Optimal reward baseline (OPO), a new reinforcement learning algorithm that emphasizes exact on-policy training and introduces an optimal reward baseline to minimize gradient variance. Evaluated on mathematical reasoning benchmarks, OPO shows superior performance and stability without additional models or regularization terms.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning algorithms suffer from training instability due to loose on-policy constraints and computational inefficiency due to auxiliary models.

Method: The proposed method is On-Policy RL with Optimal reward baseline (OPO). It focuses on exact on-policy training to stabilize the training process and enhance exploration, and it introduces the optimal reward baseline that theoretically minimizes gradient variance.

Result: OPO demonstrates superior performance and training stability on mathematical reasoning benchmarks without additional models or regularization terms. It achieves lower policy shifts and higher output entropy, encouraging more diverse and less repetitive responses.

Conclusion: OPO represents a promising direction for stable and effective reinforcement learning in large language model alignment and reasoning tasks.

Abstract: Reinforcement learning algorithms are fundamental to align large language
models with human preferences and to enhance their reasoning capabilities.
However, current reinforcement learning algorithms often suffer from training
instability due to loose on-policy constraints and computational inefficiency
due to auxiliary models. In this work, we propose On-Policy RL with Optimal
reward baseline (OPO), a novel and simplified reinforcement learning algorithm
designed to address these challenges. OPO emphasizes the importance of exact
on-policy training, which empirically stabilizes the training process and
enhances exploration. Moreover, OPO introduces the optimal reward baseline that
theoretically minimizes gradient variance. We evaluate OPO on mathematical
reasoning benchmarks. The results demonstrate its superior performance and
training stability without additional models or regularization terms.
Furthermore, OPO achieves lower policy shifts and higher output entropy,
encouraging more diverse and less repetitive responses. These results highlight
OPO as a promising direction for stable and effective reinforcement learning in
large language model alignment and reasoning tasks. The implementation is
provided at https://github.com/microsoft/LMOps/tree/main/opo.

</details>


### [169] [Accelerated Training of Federated Learning via Second-Order Methods](https://arxiv.org/abs/2505.23588)
*Mrinmay Sen,Sidhant R Nair,C Krishna Mohan*

Main category: cs.LG

TL;DR: This paper explores second-order optimization methods in Federated Learning (FL) to address slow convergence and excessive communication rounds, providing a comprehensive categorization of state-of-the-art second-order FL methods and highlighting key challenges.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenges of slow convergence and the excessive communication rounds required to achieve optimal performance from the global model in Federated Learning.

Method: Examine various FL methods that leverage second-order optimization to accelerate the training process, providing a comprehensive categorization of state-of-the-art second-order FL methods.

Result: Findings show the potential of incorporating Hessian curvature through second-order optimization into FL, with key challenges such as the efficient utilization of Hessian and its inverse in FL highlighted.

Conclusion: This work lays the groundwork for future research aimed at developing scalable and efficient federated optimization methods for improving the training of the global model in FL.

Abstract: This paper explores second-order optimization methods in Federated Learning
(FL), addressing the critical challenges of slow convergence and the excessive
communication rounds required to achieve optimal performance from the global
model. While existing surveys in FL primarily focus on challenges related to
statistical and device label heterogeneity, as well as privacy and security
concerns in first-order FL methods, less attention has been given to the issue
of slow model training. This slow training often leads to the need for
excessive communication rounds or increased communication costs, particularly
when data across clients are highly heterogeneous. In this paper, we examine
various FL methods that leverage second-order optimization to accelerate the
training process. We provide a comprehensive categorization of state-of-the-art
second-order FL methods and compare their performance based on convergence
speed, computational cost, memory usage, transmission overhead, and
generalization of the global model. Our findings show the potential of
incorporating Hessian curvature through second-order optimization into FL and
highlight key challenges, such as the efficient utilization of Hessian and its
inverse in FL. This work lays the groundwork for future research aimed at
developing scalable and efficient federated optimization methods for improving
the training of the global model in FL.

</details>


### [170] [Position: Federated Foundation Language Model Post-Training Should Focus on Open-Source Models](https://arxiv.org/abs/2505.23593)
*Nikita Agrawal,Simon Mertel,Ruben Mayer*

Main category: cs.LG

TL;DR: This paper argues against the use of black-box models in federated post-training, emphasizing contradictions with core federation principles like data privacy and autonomy.


<details>
  <summary>Details</summary>
Motivation: To critically analyze the usage of black-box models in federated post-training and highlight the concerns regarding data privacy and autonomy.

Method: Provide a detailed account of various aspects of openness and their implications for federated learning when using black-box models.

Result: A comprehensive critique on the application of black-box models in FL is provided, pointing out its inconsistency with the core principles of federation.

Conclusion: The paper concludes that black-box models should be used cautiously in FL to ensure alignment with data privacy and autonomy.

Abstract: Post-training of foundation language models has emerged as a promising
research domain in federated learning (FL) with the goal to enable
privacy-preserving model improvements and adaptations to user's downstream
tasks. Recent advances in this area adopt centralized post-training approaches
that build upon black-box foundation language models where there is no access
to model weights and architecture details. Although the use of black-box models
has been successful in centralized post-training, their blind replication in FL
raises several concerns. Our position is that using black-box models in FL
contradicts the core principles of federation such as data privacy and
autonomy. In this position paper, we critically analyze the usage of black-box
models in federated post-training, and provide a detailed account of various
aspects of openness and their implications for FL.

</details>


### [171] [LLM Performance for Code Generation on Noisy Tasks](https://arxiv.org/abs/2505.23598)
*Radzim Sendyka,Christian Cabrera,Andrei Paleyes,Diana Robinson,Neil Lawrence*

Main category: cs.LG

TL;DR: 这篇论文研究了大型语言模型（LLMs）识别和解决被混淆到无法辨认的任务的能力。通过比较多个模型在不同混淆方法下的表现，作者提出了急切模式匹配的概念来描述这种行为，并探讨了其对模型评估和基准测试的影响。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在处理被混淆任务时的表现，以了解其是否依赖记忆或训练数据过拟合，而非真正理解问题。

Method: 使用竞赛编程和基准任务（如LeetCode和MATH），对比多个LLMs在不同混淆方法（如噪声和删减）下的性能。

Result: 所有评估的LLMs都能解决被混淆到人类无法理解的任务，表现出急切模式匹配的行为。在模型知识截止日期后发布的任务中未观察到此行为，表明强记忆或过拟合。

Conclusion: 提出在设计使用标准数据集的实验时应谨慎，并建议将性能在混淆下的衰减作为检测数据集污染和强调自动化软件系统潜在安全和解释性问题的策略。

Abstract: This paper investigates the ability of large language models (LLMs) to
recognise and solve tasks which have been obfuscated beyond recognition.
Focusing on competitive programming and benchmark tasks (LeetCode and MATH), we
compare performance across multiple models and obfuscation methods, such as
noise and redaction. We demonstrate that all evaluated LLMs can solve tasks
obfuscated to a level where the text would be unintelligible to human readers,
and does not contain key pieces of instruction or context. We introduce the
concept of eager pattern matching to describe this behaviour, which is not
observed in tasks published after the models' knowledge cutoff date, indicating
strong memorisation or overfitting to training data, rather than legitimate
reasoning about the presented problem. We report empirical evidence of distinct
performance decay patterns between contaminated and unseen datasets. We discuss
the implications for benchmarking and evaluations of model behaviour, arguing
for caution when designing experiments using standard datasets. We also propose
measuring the decay of performance under obfuscation as a possible strategy for
detecting dataset contamination and highlighting potential safety risks and
interpretability issues for automated software systems.

</details>


### [172] [On Transferring Transferability: Towards a Theory for Size Generalization](https://arxiv.org/abs/2505.23599)
*Eitan Levin,Yuxin Ma,Mateo Díaz,Soledad Villar*

Main category: cs.LG

TL;DR: The paper introduces a general framework for transferability across dimensions in modern learning tasks, showing that transferability corresponds to continuity in a limit space formed by identifying small problem instances with equivalent large ones.


<details>
  <summary>Details</summary>
Motivation: Many modern learning tasks require models that can take inputs of varying sizes. Dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds.

Method: The authors introduce a general framework for transferability across dimensions, show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones, instantiate this framework on existing architectures and implement necessary changes to ensure their transferability, and provide design principles for designing new transferable models.

Result: Numerical experiments support the findings.

Conclusion: A general framework for transferability across dimensions has been introduced, instantiated on existing architectures, and design principles for new transferable models have been provided.

Abstract: Many modern learning tasks require models that can take inputs of varying
sizes. Consequently, dimension-independent architectures have been proposed for
domains where the inputs are graphs, sets, and point clouds. Recent work on
graph neural networks has explored whether a model trained on low-dimensional
data can transfer its performance to higher-dimensional inputs. We extend this
body of work by introducing a general framework for transferability across
dimensions. We show that transferability corresponds precisely to continuity in
a limit space formed by identifying small problem instances with equivalent
large ones. This identification is driven by the data and the learning task. We
instantiate our framework on existing architectures, and implement the
necessary changes to ensure their transferability. Finally, we provide design
principles for designing new transferable models. Numerical experiments support
our findings.

</details>


### [173] [Muddit: Liberating Generation Beyond Text-to-Image with a Unified Discrete Diffusion Model](https://arxiv.org/abs/2505.23606)
*Qingyu Shi,Jinbin Bai,Zhuoran Zhao,Wenhao Chai,Kaidong Yu,Jianzong Wu,Shuangyong Song,Yunhai Tong,Xiangtai Li,Xuelong Li,Shuicheng Yan*

Main category: cs.LG

TL;DR: Muddit是一种统一的离散扩散变换器，能够在文本和图像模态之间实现快速和平行生成。它结合了预训练的文本到图像主干的强大视觉先验和轻量级文本解码器，实现了高质量的多模态生成，并在质量和效率上都优于显著较大的自回归模型。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归统一模型由于顺序解码导致推理速度慢，而非自回归统一模型由于预训练主干有限而导致泛化能力弱。因此需要一种新的模型架构来解决这些问题。

Method: Muddit通过整合来自预训练文本到图像主干的强大视觉先验与轻量级文本解码器，在单一架构下实现灵活且高质量的多模态生成。并且，它采用离散扩散的方式进行训练，区别于从零开始训练的统一扩散模型。

Result: 实验结果表明，Muddit在质量与效率上均达到或优于显著较大的自回归模型的表现。

Conclusion: 本研究展示了当配备强大视觉先验时，纯粹离散扩散作为统一生成的有效且可扩展主干的潜力。

Abstract: Unified generation models aim to handle diverse tasks across modalities --
such as text generation, image generation, and vision-language reasoning --
within a single architecture and decoding paradigm. Autoregressive unified
models suffer from slow inference due to sequential decoding, and
non-autoregressive unified models suffer from weak generalization due to
limited pretrained backbones. We introduce Muddit, a unified discrete diffusion
transformer that enables fast and parallel generation across both text and
image modalities. Unlike prior unified diffusion models trained from scratch,
Muddit integrates strong visual priors from a pretrained text-to-image backbone
with a lightweight text decoder, enabling flexible and high-quality multimodal
generation under a unified architecture. Empirical results show that Muddit
achieves competitive or superior performance compared to significantly larger
autoregressive models in both quality and efficiency. The work highlights the
potential of purely discrete diffusion, when equipped with strong visual
priors, as a scalable and effective backbone for unified generation.

</details>


### [174] [Data Model Design for Explainable Machine Learning-based Electricity Applications](https://arxiv.org/abs/2505.23607)
*Carolina Fortuna,Gregor Cerar,Blaz Bertalanic,Andrej Campa,Mihael Mohorcic*

Main category: cs.LG

TL;DR: The paper proposes a taxonomy for identifying and structuring various types of data related to energy applications, which can be used to guide the development of application specific data models for training machine learning models. It also validates this taxonomy by studying the effect of different features on forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of structured studies considering the role of meta-data and additional measurements leading to multivariate data in energy infrastructure applications supported by machine learning models.

Method: The method involves proposing a taxonomy for data related to energy applications, using it to guide application specific data model development, validating it through household electricity forecasting, and studying the impact of different features on forecasting accuracy.

Result: The study found that domain, contextual, and behavioral features affect the forecasting accuracy of interpretable machine learning techniques.

Conclusion: A taxonomy for structuring energy-related data was proposed and validated, showing its effectiveness in guiding feature selection for machine learning models in electricity forecasting.

Abstract: The transition from traditional power grids to smart grids, significant
increase in the use of renewable energy sources, and soaring electricity prices
has triggered a digital transformation of the energy infrastructure that
enables new, data driven, applications often supported by machine learning
models. However, the majority of the developed machine learning models rely on
univariate data. To date, a structured study considering the role meta-data and
additional measurements resulting in multivariate data is missing. In this
paper we propose a taxonomy that identifies and structures various types of
data related to energy applications. The taxonomy can be used to guide
application specific data model development for training machine learning
models. Focusing on a household electricity forecasting application, we
validate the effectiveness of the proposed taxonomy in guiding the selection of
the features for various types of models. As such, we study of the effect of
domain, contextual and behavioral features on the forecasting accuracy of four
interpretable machine learning techniques and three openly available datasets.
Finally, using a feature importance techniques, we explain individual feature
contributions to the forecasting accuracy.

</details>


### [175] [The Generalized Skew Spectrum of Graphs](https://arxiv.org/abs/2505.23609)
*Armando Bellante,Martin Plávala,Alessandro Luongo*

Main category: cs.LG

TL;DR: 这篇论文提出了一种置换不变的图嵌入方法，推广了Kondor和Borgwardt(2008)提出的图的偏斜谱。该方法基于群论和调和分析，引入了一种新的图不变量类别，这些不变量对同构具有不变性，并且能够嵌入更丰富的图结构——包括属性图、多层图和超图——而偏斜谱无法处理这些图结构。我们的推广进一步定义了一组函数，使我们能够在计算复杂性和表达能力之间进行权衡。通过将保持泛化的启发式算法应用于这一族函数，我们在相同的计算成本下提高了偏斜谱的表达能力。我们正式证明了我们推广的不变性，通过实验展示了其改进的表达能力，并讨论了其高效的计算方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图嵌入方法（如Skew Spectrum）无法很好地处理一些复杂的图结构，例如属性图、多层图和超图，因此需要一种更通用、更强大的图嵌入方法。

Method: 基于群论和调和分析，提出了一类置换不变的图嵌入方法，这种方法可以处理更多类型的图结构，并定义了一组函数以在计算复杂性和表达能力之间取得平衡。

Result: 理论上证明了所提方法的不变性；实验上展示了其比Skew Spectrum更好的表达能力；同时提出了高效的计算方法。

Conclusion: 所提出的图嵌入方法是对现有方法的有效推广，具备更强的表达能力和广泛的适用性，为图数据分析提供了新的工具。

Abstract: This paper proposes a family of permutation-invariant graph embeddings,
generalizing the Skew Spectrum of graphs of Kondor & Borgwardt (2008). Grounded
in group theory and harmonic analysis, our method introduces a new class of
graph invariants that are isomorphism-invariant and capable of embedding richer
graph structures - including attributed graphs, multilayer graphs, and
hypergraphs - which the Skew Spectrum could not handle. Our generalization
further defines a family of functions that enables a trade-off between
computational complexity and expressivity. By applying
generalization-preserving heuristics to this family, we improve the Skew
Spectrum's expressivity at the same computational cost. We formally prove the
invariance of our generalization, demonstrate its improved expressiveness
through experiments, and discuss its efficient computation.

</details>


### [176] [Inference-time Scaling of Diffusion Models through Classical Search](https://arxiv.org/abs/2505.23614)
*Xiangcheng Zhang,Haowei Lin,Haotian Ye,James Zou,Jianzhu Ma,Yitao Liang,Yilun Du*

Main category: cs.LG

TL;DR: The paper integrates classical search algorithms into diffusion models for efficient inference-time control, showing significant performance and efficiency improvements in various tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting generated outputs from diffusion models to meet diverse test-time objectives during inference, leveraging principles from classical search algorithms.

Method: Propose a framework combining local search through annealed Langevin MCMC and global search using tree-based methods (breadth-first and depth-first) to navigate the generative space effectively.

Result: Observed substantial gains in both performance and efficiency across challenging domains like planning, offline reinforcement learning, and image generation.

Conclusion: Classical search algorithms provide a strong foundation for inference-time scaling in diffusion models.

Abstract: Classical search algorithms have long underpinned modern artificial
intelligence. In this work, we tackle the challenge of inference-time control
in diffusion models -- adapting generated outputs to meet diverse test-time
objectives -- using principles from classical search. We propose a general
framework that orchestrates local and global search to efficiently navigate the
generative space. It employs a theoretically grounded local search via annealed
Langevin MCMC and performs compute-efficient global exploration using
breadth-first and depth-first tree search. We evaluate our approach on a range
of challenging domains, including planning, offline reinforcement learning, and
image generation. Across all tasks, we observe significant gains in both
performance and efficiency. These results show that classical search provides a
principled and practical foundation for inference-time scaling in diffusion
models. Project page at diffusion-inference-scaling.github.io.

</details>


### [177] [Learning Interpretable Differentiable Logic Networks for Tabular Regression](https://arxiv.org/abs/2505.23615)
*Chang Yue,Niraj K. Jha*

Main category: cs.LG

TL;DR: This paper extends Differentiable Logic Networks (DLNs) to supervised tabular regression, redesigning the output layer for continuous targets and unifying the training procedure. Evaluated on 15 benchmarks, regression DLNs match/exceed baseline accuracy with interpretable reasoning and low inference cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of neural networks (NNs), such as opaque decision processes and high computational costs in resource-constrained environments, by extending the Differentiable Logic Networks (DLNs) framework to tabular regression tasks.

Method: The method involves redesigning the final output layer of DLNs to support continuous targets for regression tasks and unifying the original two-phase training procedure into a single differentiable stage. This allows gradient-based learning of networks built from binary logic operations.

Result: Regression DLNs were evaluated on 15 public regression benchmarks and were found to match or exceed the accuracy of modern neural networks and classical regression baselines, while maintaining interpretability and fast inference.

Conclusion: DLNs are a viable, cost-effective alternative for regression tasks, particularly when model transparency and computational efficiency are important considerations.

Abstract: Neural networks (NNs) achieve outstanding performance in many domains;
however, their decision processes are often opaque and their inference can be
computationally expensive in resource-constrained environments. We recently
proposed Differentiable Logic Networks (DLNs) to address these issues for
tabular classification based on relaxing discrete logic into a differentiable
form, thereby enabling gradient-based learning of networks built from binary
logic operations. DLNs offer interpretable reasoning and substantially lower
inference cost.
  We extend the DLN framework to supervised tabular regression. Specifically,
we redesign the final output layer to support continuous targets and unify the
original two-phase training procedure into a single differentiable stage. We
evaluate the resulting model on 15 public regression benchmarks, comparing it
with modern neural networks and classical regression baselines. Regression DLNs
match or exceed baseline accuracy while preserving interpretability and fast
inference. Our results show that DLNs are a viable, cost-effective alternative
for regression tasks, especially where model transparency and computational
efficiency are important.

</details>


### [178] [Prompting Whisper for Improved Verbatim Transcription and End-to-end Miscue Detection](https://arxiv.org/abs/2505.23627)
*Griffin Dietz Smith,Dianna Yee,Jennifer King Chen,Leah Findlater*

Main category: cs.LG

TL;DR: The paper proposes a new end-to-end architecture for reading error annotation that incorporates target reading text via prompting, improving verbatim transcription and enabling direct miscue detection.


<details>
  <summary>Details</summary>
Motivation: Current methods for identifying reading mistakes using ASR face challenges when transcriptions are inaccurate. The motivation is to improve the accuracy of reading error annotation by enhancing transcription and enabling direct miscue detection.

Method: The method involves proposing an end-to-end architecture that uses prompting with the target reading text. This approach aims at improving verbatim transcription and facilitating direct miscue detection compared to post-hoc methods.

Result: Through case studies on children's read-aloud and adult atypical speech, the proposed strategies showed improvements in both verbatim transcription and miscue detection over current state-of-the-art methods.

Conclusion: Incorporating reading text through prompting benefits verbatim transcription more than fine-tuning alone, and it is feasible to augment speech recognition tasks for end-to-end miscue detection.

Abstract: Identifying mistakes (i.e., miscues) made while reading aloud is commonly
approached post-hoc by comparing automatic speech recognition (ASR)
transcriptions to the target reading text. However, post-hoc methods perform
poorly when ASR inaccurately transcribes verbatim speech. To improve on current
methods for reading error annotation, we propose a novel end-to-end
architecture that incorporates the target reading text via prompting and is
trained for both improved verbatim transcription and direct miscue detection.
Our contributions include: first, demonstrating that incorporating reading text
through prompting benefits verbatim transcription performance over fine-tuning,
and second, showing that it is feasible to augment speech recognition tasks for
end-to-end miscue detection. We conducted two case studies -- children's
read-aloud and adult atypical speech -- and found that our proposed strategies
improve verbatim transcription and miscue detection compared to current
state-of-the-art.

</details>


### [179] [Global optimization of graph acquisition functions for neural architecture search](https://arxiv.org/abs/2505.23640)
*Yilin Xie,Shiqiang Zhang,Jixiang Qing,Ruth Misener,Calvin Tsay*

Main category: cs.LG

TL;DR: Graph Bayesian optimization is enhanced by explicit optimization formulations for graph input space, including reachability and shortest paths; this method proves effective in neural architecture search.


<details>
  <summary>Details</summary>
Motivation: To address the under-studied aspect of acquisition optimization in graph Bayesian optimization for neural architecture search, particularly dealing with the complexity of formulating the graph search space and acquisition functions.

Method: The paper introduces explicit optimization formulations for graph input space properties such as reachability and shortest paths to develop graph kernels and acquisition functions. Theoretical proof is provided for the encoding being an equivalent representation of graph space with restrictions for NAS domain.

Result: Numerical results on various NAS benchmarks indicate that the proposed method can efficiently find optimal architectures in most cases.

Conclusion: The introduced explicit formulations enhance graph Bayesian optimization's efficacy in neural architecture search.

Abstract: Graph Bayesian optimization (BO) has shown potential as a powerful and
data-efficient tool for neural architecture search (NAS). Most existing graph
BO works focus on developing graph surrogates models, i.e., metrics of networks
and/or different kernels to quantify the similarity between networks. However,
the acquisition optimization, as a discrete optimization task over graph
structures, is not well studied due to the complexity of formulating the graph
search space and acquisition functions. This paper presents explicit
optimization formulations for graph input space including properties such as
reachability and shortest paths, which are used later to formulate graph
kernels and the acquisition function. We theoretically prove that the proposed
encoding is an equivalent representation of the graph space and provide
restrictions for the NAS domain with either node or edge labels. Numerical
results over several NAS benchmarks show that our method efficiently finds the
optimal architecture for most cases, highlighting its efficacy.

</details>


### [180] [Continuous Chain of Thought Enables Parallel Exploration and Reasoning](https://arxiv.org/abs/2505.23648)
*Halil Alperen Gozeten,M. Emrullah Ildiz,Xuechen Zhang,Hrayr Harutyunyan,Ankit Singh Rawat,Samet Oymak*

Main category: cs.LG

TL;DR: The paper explores the advantages of using continuously-valued tokens (CoT2) in language models for logical reasoning tasks, providing optimization methods and showing improved inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Current language models use discrete sampling which is successful but limited. CoT2 with continuous tokens offers a more expressive alternative for chain-of-thought processes.

Method: The method involves theoretically analyzing CoT2's ability to track multiple traces in parallel, developing a supervision strategy matching softmax outputs to token distributions, and introducing sampling strategies for policy optimization.

Result: Experiments show that CoT2 improves model performance beyond initial discrete or continuous supervision, enhancing inference efficiency and enabling solution of complex problems like the subset sum problem.

Conclusion: CoT2 provides significant benefits for logical reasoning tasks, offering richer expressiveness and improved performance through novel supervision and sampling strategies.

Abstract: Current language models generate chain-of-thought traces by autoregressively
sampling tokens from a finite vocabulary. While this discrete sampling has
achieved remarkable success, conducting chain-of-thought with
continuously-valued tokens (CoT2) offers a richer and more expressive
alternative. Our work examines the benefits of CoT2 through logical reasoning
tasks that inherently require search capabilities and provide optimization and
exploration methods for CoT2. Theoretically, we show that CoT2 allows the model
to track multiple traces in parallel and quantify its benefits for inference
efficiency. Notably, one layer transformer equipped with CoT2 can provably
solve the combinatorial "subset sum problem" given sufficient embedding
dimension. These insights lead to a novel and effective supervision strategy
where we match the softmax outputs to the empirical token distributions of a
set of target traces. Complementing this, we introduce sampling strategies that
unlock policy optimization and self-improvement for CoT2. Our first strategy
samples and composes $K$ discrete tokens at each decoding step to control the
level of parallelism, and reduces to standard CoT when $K=1$. Our second
strategy relies on continuous exploration over the probability simplex.
Experiments confirm that policy optimization with CoT2 indeed improves the
performance of the model beyond its initial discrete or continuous supervision.

</details>


### [181] [Merge-Friendly Post-Training Quantization for Multi-Target Domain Adaptation](https://arxiv.org/abs/2505.23651)
*Juncheol Shin,Minsang Seok,Seonggon Kim,Eunhyeok Park*

Main category: cs.LG

TL;DR: 本研究提出了一种新的后训练量化方法HDRQ，以应对多目标领域适应中的模型合并挑战。通过分析量化对模型合并的误差屏障影响，该方法在确保与源预训练模型偏差最小的同时，平滑损失曲面以促进模型合并。实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前模型合并技术在处理量化模型时面临挑战，因为量化过程限制了感兴趣域并引入离散化效应，使得模型合并变得复杂非平凡。

Method: 提出了一种名为HDRQ（Hessian and distant regularizing quantization）的新型后训练量化方法，该方法考虑了多目标领域适应中的模型合并需求。通过分析量化对模型合并的误差屏障影响，HDRQ确保量化过程中与源预训练模型的偏差最小，并平滑损失曲面以促进模型合并。

Result: 广泛的实验表明，所提出的HDRQ方法在多目标领域适应中有效解决了模型合并的挑战。

Conclusion: 这是首个针对量化模型合并挑战的研究，HDRQ方法为多目标领域适应提供了有效的解决方案。

Abstract: Model merging has emerged as a powerful technique for combining task-specific
weights, achieving superior performance in multi-target domain adaptation.
However, when applied to practical scenarios, such as quantized models, new
challenges arise. In practical scenarios, quantization is often applied to
target-specific data, but this process restricts the domain of interest and
introduces discretization effects, making model merging highly non-trivial. In
this study, we analyze the impact of quantization on model merging through the
lens of error barriers. Leveraging these insights, we propose a novel
post-training quantization, HDRQ - Hessian and distant regularizing
quantization - that is designed to consider model merging for multi-target
domain adaptation. Our approach ensures that the quantization process incurs
minimal deviation from the source pre-trained model while flattening the loss
surface to facilitate smooth model merging. To our knowledge, this is the first
study on this challenge, and extensive experiments confirm its effectiveness.

</details>


### [182] [How does Transformer Learn Implicit Reasoning?](https://arxiv.org/abs/2505.23653)
*Jiaran Ye,Zijun Yao,Zhidian Huang,Liangming Pan,Jinxin Liu,Yushi Bai,Amy Xin,Liu Weichuan,Xiaoyin Che,Lei Hou,Juanzi Li*

Main category: cs.LG

TL;DR: 通过在受控符号环境中从头训练Transformer，研究了大语言模型中隐式多步推理的出现过程。揭示了三阶段发展轨迹，并引入两种诊断工具来解释推理行为，为提升模型透明性提供了新见解。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型可以进行隐式多步推理，但其底层机制尚不明确，因此需要在受控环境中研究这种推理是如何出现的。

Method: 在受控符号环境中从零开始训练Transformer模型，分析其推理能力的发展轨迹，并引入交叉查询语义修补和基于余弦相似性的表示镜头两种诊断工具。

Result: 发现了模型推理能力发展的三个阶段：早期记忆、分布内泛化和跨分布泛化；原子三元组训练不是必须的，但能加速学习；第二步泛化依赖于对特定组合结构的查询级暴露；成功推理与隐藏空间中的余弦聚类现象相关。

Conclusion: 这些发现增进了对大语言模型中隐式多步推理可解释性的理解，阐明了复杂推理过程的内部展开方式，并为提高模型透明性提供了途径。

Abstract: Recent work suggests that large language models (LLMs) can perform multi-hop
reasoning implicitly -- producing correct answers without explicitly
verbalizing intermediate steps -- but the underlying mechanisms remain poorly
understood. In this paper, we study how such implicit reasoning emerges by
training transformers from scratch in a controlled symbolic environment. Our
analysis reveals a three-stage developmental trajectory: early memorization,
followed by in-distribution generalization, and eventually cross-distribution
generalization. We find that training with atomic triples is not necessary but
accelerates learning, and that second-hop generalization relies on query-level
exposure to specific compositional structures. To interpret these behaviors, we
introduce two diagnostic tools: cross-query semantic patching, which identifies
semantically reusable intermediate representations, and a cosine-based
representational lens, which reveals that successful reasoning correlates with
the cosine-base clustering in hidden space. This clustering phenomenon in turn
provides a coherent explanation for the behavioral dynamics observed across
training, linking representational structure to reasoning capability. These
findings provide new insights into the interpretability of implicit multi-hop
reasoning in LLMs, helping to clarify how complex reasoning processes unfold
internally and offering pathways to enhance the transparency of such models.

</details>


### [183] [AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction](https://arxiv.org/abs/2505.23663)
*Niklas Freymuth,Tobias Würth,Nicolas Schreiber,Balazs Gyenes,Andreas Boltres,Johannes Mitsch,Aleksandar Taranovic,Tai Hoang,Philipp Dahlinger,Philipp Becker,Luise Kärger,Gerhard Neumann*

Main category: cs.LG

TL;DR: AMBER is a supervised learning approach to adaptive meshing that uses a hierarchical graph neural network and data augmentation, outperforming recent baselines in 2D and 3D datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve computational efficiency of simulating complex physical systems using FEM by developing a method for adaptive meshing that does not require task-specific heuristics or manual design.

Method: AMBER starts from a coarse mesh and iteratively predicts the sizing field using a hierarchical graph neural network. It then produces a new intermediate mesh using an out-of-the-box mesh generator. Data augmentation is used by projecting expert labels onto AMBER-generated data during training.

Result: AMBER generalizes to unseen geometries and consistently outperforms multiple recent baselines, including those using Graph and Convolutional Neural Networks, and Reinforcement Learning-based approaches.

Conclusion: AMBER provides a novel supervised learning approach to adaptive meshing with superior performance on various 2D and 3D datasets.

Abstract: The cost and accuracy of simulating complex physical systems using the Finite
Element Method (FEM) scales with the resolution of the underlying mesh.
Adaptive meshes improve computational efficiency by refining resolution in
critical regions, but typically require task-specific heuristics or cumbersome
manual design by a human expert. We propose Adaptive Meshing By Expert
Reconstruction (AMBER), a supervised learning approach to mesh adaptation.
Starting from a coarse mesh, AMBER iteratively predicts the sizing field, i.e.,
a function mapping from the geometry to the local element size of the target
mesh, and uses this prediction to produce a new intermediate mesh using an
out-of-the-box mesh generator. This process is enabled through a hierarchical
graph neural network, and relies on data augmentation by automatically
projecting expert labels onto AMBER-generated data during training. We evaluate
AMBER on 2D and 3D datasets, including classical physics problems, mechanical
components, and real-world industrial designs with human expert meshes. AMBER
generalizes to unseen geometries and consistently outperforms multiple recent
baselines, including ones using Graph and Convolutional Neural Networks, and
Reinforcement Learning-based approaches.

</details>


### [184] [Bayesian Optimization from Human Feedback: Near-Optimal Regret Bounds](https://arxiv.org/abs/2505.23673)
*Aya Kayal,Sattar Vakili,Laura Toni,Da-shan Shiu,Alberto Bernacchia*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Bayesian optimization (BO) with preference-based feedback has recently
garnered significant attention due to its emerging applications. We refer to
this problem as Bayesian Optimization from Human Feedback (BOHF), which differs
from conventional BO by learning the best actions from a reduced feedback
model, where only the preference between two actions is revealed to the learner
at each time step. The objective is to identify the best action using a limited
number of preference queries, typically obtained through costly human feedback.
Existing work, which adopts the Bradley-Terry-Luce (BTL) feedback model,
provides regret bounds for the performance of several algorithms. In this work,
within the same framework we develop tighter performance guarantees.
Specifically, we derive regret bounds of
$\tilde{\mathcal{O}}(\sqrt{\Gamma(T)T})$, where $\Gamma(T)$ represents the
maximum information gain$\unicode{x2014}$a kernel-specific complexity
term$\unicode{x2014}$and $T$ is the number of queries. Our results
significantly improve upon existing bounds. Notably, for common kernels, we
show that the order-optimal sample complexities of conventional
BO$\unicode{x2014}$achieved with richer feedback models$\unicode{x2014}$are
recovered. In other words, the same number of preferential samples as
scalar-valued samples is sufficient to find a nearly optimal solution.

</details>


### [185] [Understanding Mode Connectivity via Parameter Space Symmetry](https://arxiv.org/abs/2505.23681)
*Bo Zhao,Nima Dehmamy,Robin Walters,Rose Yu*

Main category: cs.LG

TL;DR: 研究通过参数空间对称性探索神经网络极小值的连通性，提出线性网络极小值的连通分量数量，并分析跳过连接和线性模式连通性的影响，强调连续对称性在理解损失景观中的作用。


<details>
  <summary>Details</summary>
Motivation: 尽管模式连通性已被应用于模型合并和微调，但其理论解释尚不清楚，因此需要一种新的方法来探索极小值的连通性。

Method: 利用参数空间对称性，将对称群的拓扑与极小值的拓扑联系起来，推导线性网络极小值的连通分量数量；分析跳过连接对该数量的影响；使用参数对称性检查模式连通性和线性模式连通性的成立或失败条件；提供由对称性诱导的极小值连接曲线的显式表达式并推导线性模式连通性近似成立的条件。

Result: 成功推导出线性网络极小值的连通分量数量，并证明跳过连接减少该数量；明确给出连接曲线的显式表达式和线性模式连通性近似成立的条件，揭示了连续对称性在理解神经网络损失景观中的关键作用。

Conclusion: 连续对称性在理解神经网络损失景观中起着重要作用，本研究为探索极小值连通性提供了新视角。

Abstract: Neural network minima are often connected by curves along which train and
test loss remain nearly constant, a phenomenon known as mode connectivity.
While this property has enabled applications such as model merging and
fine-tuning, its theoretical explanation remains unclear. We propose a new
approach to exploring the connectedness of minima using parameter space
symmetry. By linking the topology of symmetry groups to that of the minima, we
derive the number of connected components of the minima of linear networks and
show that skip connections reduce this number. We then examine when mode
connectivity and linear mode connectivity hold or fail, using parameter
symmetries which account for a significant part of the minimum. Finally, we
provide explicit expressions for connecting curves in the minima induced by
symmetry. Using the curvature of these curves, we derive conditions under which
linear mode connectivity approximately holds. Our findings highlight the role
of continuous symmetries in understanding the neural network loss landscape.

</details>


### [186] [Learning Compositional Functions with Transformers from Easy-to-Hard Data](https://arxiv.org/abs/2505.23683)
*Zixuan Wang,Eshaan Nichani,Alberto Bietti,Alex Damian,Daniel Hsu,Jason D. Lee,Denny Wu*

Main category: cs.LG

TL;DR: Transformer-based language models can efficiently perform multi-step reasoning tasks involving parallelizable computations, but the learnability of such constructions remains an open question.


<details>
  <summary>Details</summary>
Motivation: To explore the learnability of transformers in performing complex compositional tasks and the conditions on the data distribution that enable efficient learning via gradient-based optimization.

Method: Study the learnability of the $k$-fold composition task using Statistical Query (SQ) lower bound and two different curriculum learning strategies.

Result: Prove a SQ lower bound showing a statistical-computational gap, and show that the function class can be efficiently learned with runtime and sample complexity polynomial in $k$ by gradient descent on an $O(\log k)$-depth transformer.

Conclusion: The necessity and sufficiency of having both easy and hard examples in the data distribution for transformers to learn complex compositional tasks are highlighted.

Abstract: Transformer-based language models have demonstrated impressive capabilities
across a range of complex reasoning tasks. Prior theoretical work exploring the
expressive power of transformers has shown that they can efficiently perform
multi-step reasoning tasks involving parallelizable computations. However, the
learnability of such constructions, particularly the conditions on the data
distribution that enable efficient learning via gradient-based optimization,
remains an open question. Towards answering this question, in this work we
study the learnability of the $k$-fold composition task, which requires
computing an interleaved composition of $k$ input permutations and $k$ hidden
permutations, and can be expressed by a transformer with $O(\log k)$ layers. On
the negative front, we prove a Statistical Query (SQ) lower bound showing that
any SQ learner that makes only polynomially-many queries to an SQ oracle for
the $k$-fold composition task distribution must have sample size exponential in
$k$, thus establishing a statistical-computational gap. On the other hand, we
show that this function class can be efficiently learned, with runtime and
sample complexity polynomial in $k$, by gradient descent on an $O(\log
k)$-depth transformer via two different curriculum learning strategies: one in
which data consists of $k'$-fold composition functions with $k' \le k$
presented in increasing difficulty, and another in which all such data is
presented simultaneously. Our work sheds light on the necessity and sufficiency
of having both easy and hard examples in the data distribution for transformers
to learn complex compositional tasks.

</details>


### [187] [Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms](https://arxiv.org/abs/2505.23696)
*Hiroshi Kera,Nico Pelleriti,Yuki Ishihara,Max Zimmer,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 本研究提出了一种名为Oracle Border Basis Algorithm的深度学习方法，通过设计和训练基于Transformer的oracle来加速Border basis计算，同时保证输出正确性。这种方法在关键计算阶段选择性地调用oracle，实现了高达3.5倍的速度提升，并提供了首个针对border bases的采样定理及适合单项式代数计算的标记化和嵌入方案。


<details>
  <summary>Details</summary>
Motivation: 求解具有有限多解的多项式方程组是许多科学领域中的重要挑战。传统方法如Gröbner和Border bases虽然基础但计算成本高，而现有的深度学习方法虽提高了效率但牺牲了输出正确性。因此需要一种既高效又能保证结果正确的算法。

Method: 引入Oracle Border Basis Algorithm，使用基于Transformer的oracle识别并消除计算密集型的简化步骤；开发了一种采样方法和首个border bases采样定理以生成训练数据；构建了一个专为单项式为中心的代数计算设计的标记化和嵌入方案，从而减少编码n变量多项式所需的token数量。

Result: 与基础算法相比，该方法在不损害结果正确性的前提下，实现了高达3.5倍的速度提升。此外，其学习方法数据效率高、稳定且实用，是对传统计算机代数算法和符号计算的有效改进。

Conclusion: Oracle Border Basis Algorithm是一种数据高效、稳定的深度学习方法，能够在加速Border basis计算的同时保证输出正确性，是对传统计算机代数算法的重要补充和增强。

Abstract: Solving systems of polynomial equations, particularly those with finitely
many solutions, is a crucial challenge across many scientific fields.
Traditional methods like Gr\"obner and Border bases are fundamental but suffer
from high computational costs, which have motivated recent Deep Learning
approaches to improve efficiency, albeit at the expense of output correctness.
In this work, we introduce the Oracle Border Basis Algorithm, the first Deep
Learning approach that accelerates Border basis computation while maintaining
output guarantees. To this end, we design and train a Transformer-based oracle
that identifies and eliminates computationally expensive reduction steps, which
we find to dominate the algorithm's runtime. By selectively invoking this
oracle during critical phases of computation, we achieve substantial speedup
factors of up to 3.5x compared to the base algorithm, without compromising the
correctness of results. To generate the training data, we develop a sampling
method and provide the first sampling theorem for border bases. We construct a
tokenization and embedding scheme tailored to monomial-centered algebraic
computations, resulting in a compact and expressive input representation, which
reduces the number of tokens to encode an $n$-variate polynomial by a factor of
$O(n)$. Our learning approach is data efficient, stable, and a practical
enhancement to traditional computer algebra algorithms and symbolic
computation.

</details>


### [188] [DiCoFlex: Model-agnostic diverse counterfactuals with flexible control](https://arxiv.org/abs/2505.23700)
*Oleksii Furman,Ulvi Movsum-zada,Patryk Marszalek,Maciej Zięba,Marek Śmieja*

Main category: cs.LG

TL;DR: The paper proposes DiCoFlex, a model-agnostic conditional generative framework that generates multiple diverse counterfactuals in one forward pass, offering real-time customization and outperforming existing methods in validity, diversity, proximity, and constraint adherence.


<details>
  <summary>Details</summary>
Motivation: Existing counterfactual generation methods have limitations such as requiring constant model access, being computationally intensive, and lacking flexibility to adapt to user-defined constraints without retraining.

Method: DiCoFlex is a novel model-agnostic, conditional generative framework using conditional normalizing flows trained solely on labeled data to produce multiple diverse counterfactuals in a single forward pass with real-time user-driven customization of constraints.

Result: Extensive experiments on standard benchmark datasets demonstrate that DiCoFlex surpasses existing methods in terms of validity, diversity, proximity, and constraint adherence.

Conclusion: DiCoFlex is a practical and scalable solution for generating counterfactuals in sensitive decision-making domains.

Abstract: Counterfactual explanations play a pivotal role in explainable artificial
intelligence (XAI) by offering intuitive, human-understandable alternatives
that elucidate machine learning model decisions. Despite their significance,
existing methods for generating counterfactuals often require constant access
to the predictive model, involve computationally intensive optimization for
each instance and lack the flexibility to adapt to new user-defined constraints
without retraining. In this paper, we propose DiCoFlex, a novel model-agnostic,
conditional generative framework that produces multiple diverse counterfactuals
in a single forward pass. Leveraging conditional normalizing flows trained
solely on labeled data, DiCoFlex addresses key limitations by enabling
real-time user-driven customization of constraints such as sparsity and
actionability at inference time. Extensive experiments on standard benchmark
datasets show that DiCoFlex outperforms existing methods in terms of validity,
diversity, proximity, and constraint adherence, making it a practical and
scalable solution for counterfactual generation in sensitive decision-making
domains.

</details>


### [189] [(U)NFV: Supervised and Unsupervised Neural Finite Volume Methods for Solving Hyperbolic PDEs](https://arxiv.org/abs/2505.23702)
*Nathan Lichtlé,Alexi Canesse,Zhe Fu,Hossein Nick Zinat Matin,Maria Laura Delle Monache,Alexandre M. Bayen*

Main category: cs.LG

TL;DR: The paper introduces (U)NFV, a neural network architecture generalizing finite volume methods for solving hyperbolic conservation laws with lower error and higher fidelity than traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of classical finite volume methods in terms of accuracy and flexibility when solving challenging hyperbolic partial differential equations, especially conservation laws with shocks and discontinuities.

Method: Developed a modular neural network architecture named (U)NFV that learns update rules over extended stencils while maintaining conservation structure, supporting both supervised (NFV) and unsupervised (UNFV) training approaches.

Result: (U)NFV achieves significantly lower error rates compared to Godunov's method, outperforms ENO/WENO schemes, matches discontinuous Galerkin solvers with reduced complexity, and demonstrates superior performance in traffic modeling problems.

Conclusion: (U)NFV offers an improved approach for solving hyperbolic conservation laws by combining the strengths of finite volume methods with machine learning techniques, providing greater accuracy and scalability.

Abstract: We introduce (U)NFV, a modular neural network architecture that generalizes
classical finite volume (FV) methods for solving hyperbolic conservation laws.
Hyperbolic partial differential equations (PDEs) are challenging to solve,
particularly conservation laws whose physically relevant solutions contain
shocks and discontinuities. FV methods are widely used for their mathematical
properties: convergence to entropy solutions, flow conservation, or total
variation diminishing, but often lack accuracy and flexibility in complex
settings. Neural Finite Volume addresses these limitations by learning update
rules over extended spatial and temporal stencils while preserving conservation
structure. It supports both supervised training on solution data (NFV) and
unsupervised training via weak-form residual loss (UNFV). Applied to
first-order conservation laws, (U)NFV achieves up to 10x lower error than
Godunov's method, outperforms ENO/WENO, and rivals discontinuous Galerkin
solvers with far less complexity. On traffic modeling problems, both from PDEs
and from experimental highway data, (U)NFV captures nonlinear wave dynamics
with significantly higher fidelity and scalability than traditional FV
approaches.

</details>


### [190] [Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better](https://arxiv.org/abs/2505.23705)
*Danny Driess,Jost Tobias Springenberg,Brian Ichter,Lili Yu,Adrian Li-Bell,Karl Pertsch,Allen Z. Ren,Homer Walke,Quan Vuong,Lucy Xiaoyang Shi,Sergey Levine*

Main category: cs.LG

TL;DR: Vision-language-action (VLA) models, despite their potential in training control policies for robots by integrating semantic knowledge from large vision-language models (VLMs), face challenges with real-time inference and continuous control. This paper explores the impact of adding specialized modules to VLMs on preserving semantic knowledge, finding that naive inclusion can harm training speed and knowledge transfer. It proposes a technique to protect the VLM backbone during VLA training.


<details>
  <summary>Details</summary>
Motivation: To understand whether specialized modules added to pretrained VLMs for efficient continuous control in VLA models preserve or degrade the semantic knowledge contained within the VLM, and to explore their effects on VLA training dynamics.

Method: The paper examines VLA models incorporating a continuous diffusion or flow matching action expert. An extensive analysis is conducted on different design choices and their influence on performance and knowledge transfer. A technique is proposed to shield the VLM backbone during VLA training.

Result: Naively adding such experts significantly harms both training speed and knowledge transfer. The proposed insulation technique mitigates these negative effects while maintaining the benefits of specialized modules.

Conclusion: Specialized modules improve real-time and control capabilities but risk degrading semantic knowledge. The proposed insulation method helps preserve this knowledge while allowing effective VLA model training.

Abstract: Vision-language-action (VLA) models provide a powerful approach to training
control policies for physical systems, such as robots, by combining end-to-end
learning with transfer of semantic knowledge from web-scale vision-language
model (VLM) training. However, the constraints of real-time control are often
at odds with the design of VLMs: the most powerful VLMs have tens or hundreds
of billions of parameters, presenting an obstacle to real-time inference, and
operate on discrete tokens rather than the continuous-valued outputs that are
required for controlling robots. To address this challenge, recent VLA models
have used specialized modules for efficient continuous control, such as action
experts or continuous output heads, which typically require adding new
untrained parameters to the pretrained VLM backbone. While these modules
improve real-time and control capabilities, it remains an open question whether
they preserve or degrade the semantic knowledge contained in the pretrained
VLM, and what effect they have on the VLA training dynamics. In this paper, we
study this question in the context of VLAs that include a continuous diffusion
or flow matching action expert, showing that naively including such experts
significantly harms both training speed and knowledge transfer. We provide an
extensive analysis of various design choices, their impact on performance and
knowledge transfer, and propose a technique for insulating the VLM backbone
during VLA training that mitigates this issue. Videos are available at
https://pi.website/research/knowledge_insulation.

</details>


### [191] [TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning](https://arxiv.org/abs/2505.23719)
*Andreas Auer,Patrick Podest,Daniel Klotz,Sebastian Böck,Günter Klambauer,Sepp Hochreiter*

Main category: cs.LG

TL;DR: TiRex，基于增强型LSTM（xLSTM）的模型，在零样本时间序列预测中表现出色，通过引入训练时的CPM掩码策略提升状态追踪能力，超越多个大型模型成为HuggingFace基准测试的新标杆。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本时间序列预测方法多依赖于Transformer架构，但其在时间序列预测中的表现不如循环模型（如LSTM）。然而，LSTM虽然适合时间序列建模，但在上下文学习方面能力较弱。因此需要一种结合两者优势的方法。

Method: 提出了一种名为TiRex的模型，该模型基于增强型LSTM（xLSTM），具有强大的上下文学习能力，并且保留了状态追踪特性。同时，还提出了一种训练时的掩码策略（CPM），以进一步促进其状态追踪能力。

Result: TiRex在HuggingFace基准测试的GiftEval和Chronos-ZS上实现了零样本时间序列预测的新纪录，显著优于包括TabPFN-TS、Chronos Bolt、TimesFM和Moirai在内的更大规模模型，无论是在短期还是长期预测中都表现出色。

Conclusion: TiRex通过结合增强型LSTM和CPM掩码策略，在零样本时间序列预测领域达到了新的高度，展示了其在不同预测范围内的优越性能。

Abstract: In-context learning, the ability of large language models to perform tasks
using only examples provided in the prompt, has recently been adapted for time
series forecasting. This paradigm enables zero-shot prediction, where past
values serve as context for forecasting future values, making powerful
forecasting tools accessible to non-experts and increasing the performance when
training data are scarce. Most existing zero-shot forecasting approaches rely
on transformer architectures, which, despite their success in language, often
fall short of expectations in time series forecasting, where recurrent models
like LSTMs frequently have the edge. Conversely, while LSTMs are well-suited
for time series modeling due to their state-tracking capabilities, they lack
strong in-context learning abilities. We introduce TiRex that closes this gap
by leveraging xLSTM, an enhanced LSTM with competitive in-context learning
skills. Unlike transformers, state-space models, or parallelizable RNNs such as
RWKV, TiRex retains state-tracking, a critical property for long-horizon
forecasting. To further facilitate its state-tracking ability, we propose a
training-time masking strategy called CPM. TiRex sets a new state of the art in
zero-shot time series forecasting on the HuggingFace benchmarks GiftEval and
Chronos-ZS, outperforming significantly larger models including TabPFN-TS
(Prior Labs), Chronos Bolt (Amazon), TimesFM (Google), and Moirai (Salesforce)
across both short- and long-term forecasts.

</details>


### [192] [COBRA: Contextual Bandit Algorithm for Ensuring Truthful Strategic Agents](https://arxiv.org/abs/2505.23720)
*Arun Verma,Indrajit Saha,Makoto Yokoo,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: In order to solve the problem of agents not truthfully reporting their arms in contextual bandit problems, this paper proposes an algorithm called COBRA, which discourages strategic behavior without using monetary incentives and guarantees sub-linear regret.


<details>
  <summary>Details</summary>
Motivation: Existing work in contextual bandits assumes that agents truthfully report their arms, but this assumption is unrealistic in many real-life applications. Some sellers may misrepresent product quality for personal gain.

Method: The proposed algorithm is named COBRA. It is designed for contextual bandit problems involving strategic agents. COBRA disincentivizes strategic behavior without using any monetary incentives, while having incentive compatibility and a sub-linear regret guarantee.

Result: Our experimental results validate the different performance aspects of our proposed algorithm.

Conclusion: COBRA addresses the challenge of strategic agents in contextual bandit problems, ensuring incentive compatibility and sub-linear regret without monetary incentives.

Abstract: This paper considers a contextual bandit problem involving multiple agents,
where a learner sequentially observes the contexts and the agent's reported
arms, and then selects the arm that maximizes the system's overall reward.
Existing work in contextual bandits assumes that agents truthfully report their
arms, which is unrealistic in many real-life applications. For instance,
consider an online platform with multiple sellers; some sellers may
misrepresent product quality to gain an advantage, such as having the platform
preferentially recommend their products to online users. To address this
challenge, we propose an algorithm, COBRA, for contextual bandit problems
involving strategic agents that disincentivize their strategic behavior without
using any monetary incentives, while having incentive compatibility and a
sub-linear regret guarantee. Our experimental results also validate the
different performance aspects of our proposed algorithm.

</details>


### [193] [DiffER: Categorical Diffusion for Chemical Retrosynthesis](https://arxiv.org/abs/2505.23721)
*Sean Current,Ziqi Chen,Daniel Adu-Ampratwum,Xia Ning,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: DiffER是一种基于分类扩散的无模板方法，用于化学 retrosynthesis 预测。它通过整体预测输出SMILES序列和构建扩散模型集成，在top-1准确率方面达到了最先进的性能。此外，准确预测SMILES序列长度对于提升该模型的性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化学逆合成方法虽然在翻译化学产物和反应物的SMILES编码方面表现出色，但受到自回归性质的限制。

Method: 提出了一种名为DiffER的无模板方法，采用分类扩散形式进行逆合成预测，可以同时预测整个输出SMILES序列。构建了一个具有新颖长度预测组件的扩散模型集成，能够从反应物的后验分布中采样。

Result: 在无模板方法中，DiffER在top-1准确率上达到最佳性能，在top-3、top-5和top-10准确率上表现具有竞争力。分析表明，准确预测SMILES序列长度是提高分类扩散模型性能的关键。

Conclusion: DiffER为一类新的无模板模型提供了强大的基线，能够学习实验室中使用的各种合成技术，并在top-k准确率指标上优于其他无模板方法。

Abstract: Methods for automatic chemical retrosynthesis have found recent success
through the application of models traditionally built for natural language
processing, primarily through transformer neural networks. These models have
demonstrated significant ability to translate between the SMILES encodings of
chemical products and reactants, but are constrained as a result of their
autoregressive nature. We propose DiffER, an alternative template-free method
for retrosynthesis prediction in the form of categorical diffusion, which
allows the entire output SMILES sequence to be predicted in unison. We
construct an ensemble of diffusion models which achieves state-of-the-art
performance for top-1 accuracy and competitive performance for top-3, top-5,
and top-10 accuracy among template-free methods. We prove that DiffER is a
strong baseline for a new class of template-free model, capable of learning a
variety of synthetic techniques used in laboratory settings and outperforming a
variety of other template-free methods on top-k accuracy metrics. By
constructing an ensemble of categorical diffusion models with a novel length
prediction component with variance, our method is able to approximately sample
from the posterior distribution of reactants, producing results with strong
metrics of confidence and likelihood. Furthermore, our analyses demonstrate
that accurate prediction of the SMILES sequence length is key to further
boosting the performance of categorical diffusion models.

</details>


### [194] [SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via Subspace-Constrained LoRA](https://arxiv.org/abs/2505.23724)
*Minrui Luo,Fuhang Kuang,Yu Wang,Zirui Liu,Tianxing He*

Main category: cs.LG

TL;DR: The paper introduces Subspace-Constrained LoRA (SC-LoRA), a novel framework for initializing LoRA adapters in large language models. SC-LoRA balances efficient fine-tuning with knowledge preservation by constraining adapter outputs in a low-rank subspace. Experiments show it outperforms current methods in both fine-tuning performance and reducing knowledge forgetting.


<details>
  <summary>Details</summary>
Motivation: Current LoRA methods struggle with slow convergence and knowledge forgetting, while existing initialization techniques only address one of these issues. There is a need for a method that can simultaneously enhance fine-tuning efficiency and preserve pre-trained knowledge.

Method: The authors propose SC-LoRA, which constrains the output of trainable LoRA adapters within a low-rank subspace. This approach aims to preserve context information from fine-tuning data while minimizing interference with preserved knowledge features.

Result: SC-LoRA demonstrates superior fine-tuning performance and significantly reduces knowledge forgetting across various downstream tasks, including safety and world knowledge preservation.

Conclusion: SC-LoRA provides an effective solution for balancing efficient fine-tuning and knowledge preservation in large language models, outperforming contemporary LoRA initialization methods.

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank
Adaptation (LoRA), are indispensable for efficiently customizing Large Language
Models (LLMs). However, vanilla LoRA suffers from slow convergence speed and
knowledge forgetting problems. Recent studies have leveraged the power of
designed LoRA initialization, to enhance the fine-tuning efficiency, or to
preserve knowledge in the pre-trained LLM. However, none of these works can
address the two cases at the same time. To this end, we introduce
Subspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework
engineered to navigate the trade-off between efficient fine-tuning and
knowledge preservation. We achieve this by constraining the output of trainable
LoRA adapters in a low-rank subspace, where the context information of
fine-tuning data is most preserved while the context information of preserved
knowledge is least retained, in a balanced way. Such constraint enables the
trainable weights to primarily focus on the main features of fine-tuning data
while avoiding damaging the preserved knowledge features. We provide
theoretical analysis on our method, and conduct extensive experiments including
safety preservation and world knowledge preservation, on various downstream
tasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning
performance while markedly diminishing knowledge forgetting, surpassing
contemporary LoRA initialization methods.

</details>


### [195] [MuLoCo: Muon is a practical inner optimizer for DiLoCo](https://arxiv.org/abs/2505.23725)
*Benjamin Thérien,Xiaolong Huang,Irina Rish,Eugene Belilovsky*

Main category: cs.LG

TL;DR: This paper presents MuLoCo, an improved version of the DiLoCo framework that uses the Muon optimizer and error-feedback accumulator to reduce communication overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To explore more effective methods to reduce communication overhead in DiLoCo framework for training large language models under networking constraints.

Method: Investigating standard compression methods (Top-k sparsification and quantization) combined with two local optimizers (AdamW and Muon) in DiLoCo framework. Introducing MuLoCo which leverages Muon as the inner optimizer with an error-feedback accumulator.

Result: Experiments show that using Muon as the inner optimizer for DiLoCo along with an error-feedback accumulator allows compressing the communicated delta to 2-bits without significant performance degradation. MuLoCo outperforms DiLoCo while communicating 8 times less.

Conclusion: MuLoCo is an efficient approach to reduce communication overhead in training large language models with similar memory complexity and better performance than DiLoCo.

Abstract: DiLoCo is a powerful framework for training large language models (LLMs)
under networking constraints with advantages for increasing parallelism and
accelerator utilization in data center settings. Despite significantly reducing
communication frequency, however, DiLoCo's communication steps still involve
all-reducing a complete copy of the model's parameters. While existing works
have explored ways to reduce communication in DiLoCo, the role of error
feedback accumulators and the effect of the inner-optimizer on compressibility
remain under-explored. In this work, we investigate the effectiveness of
standard compression methods including Top-k sparsification and quantization
for reducing the communication overhead of DiLoCo when paired with two local
optimizers (AdamW and Muon). Our experiments pre-training decoder-only
transformer language models (LMs) reveal that leveraging Muon as the inner
optimizer for DiLoCo along with an error-feedback accumulator allows to
aggressively compress the communicated delta to 2-bits with next to no
performance degradation. Crucially, MuLoCo (Muon inner optimizer DiLoCo)
significantly outperforms DiLoCo while communicating 8X less and having
identical memory complexity.

</details>


### [196] [EmotionRankCLAP: Bridging Natural Language Speaking Styles and Ordinal Speech Emotion via Rank-N-Contrast](https://arxiv.org/abs/2505.23732)
*Shreeram Suresh Chandra,Lucas Goncalves,Junchen Lu,Carlos Busso,Berrak Sisman*

Main category: cs.LG

TL;DR: EmotionRankCLAP is a supervised contrastive learning approach that captures fine-grained emotion variations and improves cross-modal alignment using dimensional attributes of emotional speech and natural language prompts, outperforming existing methods in modeling emotion ordinality.


<details>
  <summary>Details</summary>
Motivation: Current emotion-based CLAP methods fail to capture the ordinal nature of emotions and have insufficient alignment between audio and text embeddings, leading to a wide modality gap.

Method: EmotionRankCLAP uses dimensional attributes of emotional speech and natural language prompts with a Rank-N-Contrast objective to learn ordered relationships by contrasting samples based on their rankings in the valence-arousal space.

Result: EmotionRankCLAP outperforms existing emotion-CLAP methods in modeling emotion ordinality across modalities, as measured via a cross-modal retrieval task.

Conclusion: EmotionRankCLAP successfully addresses the drawbacks of current methods by capturing fine-grained emotion variations and improving cross-modal alignment.

Abstract: Current emotion-based contrastive language-audio pretraining (CLAP) methods
typically learn by na\"ively aligning audio samples with corresponding text
prompts. Consequently, this approach fails to capture the ordinal nature of
emotions, hindering inter-emotion understanding and often resulting in a wide
modality gap between the audio and text embeddings due to insufficient
alignment. To handle these drawbacks, we introduce EmotionRankCLAP, a
supervised contrastive learning approach that uses dimensional attributes of
emotional speech and natural language prompts to jointly capture fine-grained
emotion variations and improve cross-modal alignment. Our approach utilizes a
Rank-N-Contrast objective to learn ordered relationships by contrasting samples
based on their rankings in the valence-arousal space. EmotionRankCLAP
outperforms existing emotion-CLAP methods in modeling emotion ordinality across
modalities, measured via a cross-modal retrieval task.

</details>


### [197] [Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?](https://arxiv.org/abs/2505.23749)
*Paul Gölz,Nika Haghtalab,Kunhe Yang*

Main category: cs.LG

TL;DR: 在大规模语言模型的对齐方法中，引入了'失真'概念来评估不同方法的表现。Nash Learning from Human Feedback在多种条件下达到了最优的失真度，而RLHF和DPO则表现较差。


<details>
  <summary>Details</summary>
Motivation: 当前的对齐方法（如基于PPO的RLHF和DPO）假设存在单一的偏好模型，但实际应用中用户的偏好是多样的。因此，需要一种新的方法来衡量这些对齐方法是否能平均满足用户需求。

Method: 通过社会选择理论和Bradley-Terry模型，引入了对齐方法的'失真'概念，定义为最优平均效用与学习策略平均效用之间的最坏情况比率。提出了Nash Learning from Human Feedback方法，并分析了其在不同条件下的失真度。

Result: Nash Learning from Human Feedback达到了最优的失真度$(\frac{1}{2} + o(1)) \cdot \beta$，而RLHF和DPO在无KL约束时失真度为$\geq (1 - o(1)) \cdot \beta$，在完全设置下甚至可能达到$e^{\Omega(\beta)}$或无界失真。

Conclusion: Nash Learning from Human Feedback在多种条件下表现出更优的对齐性能，特别是在处理多样用户偏好时。

Abstract: After pre-training, large language models are aligned with human preferences
based on pairwise comparisons. State-of-the-art alignment methods (such as
PPO-based RLHF and DPO) are built on the assumption of aligning with a single
preference model, despite being deployed in settings where users have diverse
preferences. As a result, it is not even clear that these alignment methods
produce models that satisfy users on average -- a minimal requirement for
pluralistic alignment. Drawing on social choice theory and modeling users'
comparisons through individual Bradley-Terry (BT) models, we introduce an
alignment method's distortion: the worst-case ratio between the optimal
achievable average utility, and the average utility of the learned policy.
  The notion of distortion helps draw sharp distinctions between alignment
methods: Nash Learning from Human Feedback achieves the minimax optimal
distortion of $(\frac{1}{2} + o(1)) \cdot \beta$ (for the BT temperature
$\beta$), robustly across utility distributions, distributions of comparison
pairs, and permissible KL divergences from the reference policy. RLHF and DPO,
by contrast, suffer $\geq (1 - o(1)) \cdot \beta$ distortion already without a
KL constraint, and $e^{\Omega(\beta)}$ or even unbounded distortion in the full
setting, depending on how comparison pairs are sampled.

</details>


### [198] [REOrdering Patches Improves Vision Models](https://arxiv.org/abs/2505.23751)
*Declan Kutscher,David M. Chan,Yutong Bai,Trevor Darrell,Ritwik Gupta*

Main category: cs.LG

TL;DR: Sequence models like transformers need one-dimensional input sequences. In vision tasks, images are usually flattened using a fixed row-major order. However, recent long-sequence transformers increasingly depend on architectural approximations that make them sensitive to patch ordering. The abstract shows that patch order significantly affects model performance in such settings and proposes REOrder, a two-stage framework for discovering task-optimal patch orderings.


<details>
  <summary>Details</summary>
Motivation: The motivation is that the patch order significantly affects model performance in modern long-sequence transformers which rely on architectural approximations that break permutation-equivariance.

Method: REOrder is proposed as a two-stage framework for discovering task-optimal patch orderings. First, an information-theoretic prior is derived by evaluating the compressibility of various patch sequences. Then, a policy over permutations is learned by optimizing a Plackett-Luce policy using REINFORCE.

Result: REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to 3.01% and Functional Map of the World by 13.35%.

Conclusion: Patch order significantly affects model performance in modern long-sequence transformers, and REOrder can discover task-optimal patch orderings.

Abstract: Sequence models such as transformers require inputs to be represented as
one-dimensional sequences. In vision, this typically involves flattening images
using a fixed row-major (raster-scan) order. While full self-attention is
permutation-equivariant, modern long-sequence transformers increasingly rely on
architectural approximations that break this invariance and introduce
sensitivity to patch ordering. We show that patch order significantly affects
model performance in such settings, with simple alternatives like column-major
or Hilbert curves yielding notable accuracy shifts. Motivated by this, we
propose REOrder, a two-stage framework for discovering task-optimal patch
orderings. First, we derive an information-theoretic prior by evaluating the
compressibility of various patch sequences. Then, we learn a policy over
permutations by optimizing a Plackett-Luce policy using REINFORCE. This
approach enables efficient learning in a combinatorial permutation space.
REOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to
3.01% and Functional Map of the World by 13.35%.

</details>


### [199] [Model Immunization from a Condition Number Perspective](https://arxiv.org/abs/2505.23760)
*Amber Yijia Zheng,Cedar Site Bai,Brian Bullins,Raymond A. Yeh*

Main category: cs.LG

TL;DR: This paper explores model immunization, proposing a framework based on the condition number of a Hessian matrix for linear models and designing an algorithm with regularization terms to control condition numbers after pre-training. Empirical results show effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of understanding and precise definition in model immunization, specifically focusing on when it is possible and how it can be achieved effectively.

Method: The method involves proposing a framework using the condition number of a Hessian matrix for analyzing model immunization in linear models and designing an algorithm incorporating regularization terms to manage the condition numbers post pre-training.

Result: The empirical results indicate that the proposed algorithm is effective for model immunization in both linear models and non-linear deep-nets.

Conclusion: The study concludes by demonstrating the effectiveness of their approach in model immunization and provides a clearer understanding and definition for immunized models.

Abstract: Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.

</details>


### [200] [Differential Information: An Information-Theoretic Perspective on Preference Optimization](https://arxiv.org/abs/2505.23761)
*Yunjae Won,Hyunji Lee,Hyeonbin Hwang,Minjoon Seo*

Main category: cs.LG

TL;DR: Direct Preference Optimization (DPO) uses Differential Information Distribution (DID) to theoretically justify its log-ratio reward parameterization. This work shows the optimal form of log-ratio reward, links it to an implicit assumption, and explains phenomena through entropy analysis.


<details>
  <summary>Details</summary>
Motivation: To address the theoretical gap in understanding the log-ratio reward parameterization in DPO, which is crucial for aligning language models with human preferences.

Method: Utilize DID to analyze preference labels encoding differential information, derive the optimal form of log-ratio reward, identify an implicit assumption about log-margin ordered policies, and analyze the entropy of DID to explain learning phenomena.

Result: Theoretically justified the log-ratio reward, provided a closed-form expression for optimal sampling, linked preferences to an implicit assumption, and explained learning phenomena through entropy analysis. Validated findings in synthetic and real-world experiments.

Conclusion: Presented a unifying perspective on DPO, showing how differential information impacts policy behaviors and suggesting different types of differential information benefit different tasks.

Abstract: Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [201] [TensorShield: Safeguarding On-Device Inference by Shielding Critical DNN Tensors with TEE](https://arxiv.org/abs/2505.22735)
*Tong Sun,Bowen Jiang,Hailong Lin,Borui Li,Yixiao Teng,Yi Gao,Wei Dong*

Main category: cs.CR

TL;DR: TensorShield is an efficient on-device inference method that protects critical tensors in TEEs, providing strong defense against model stealing and membership inference attacks while maintaining low latency and no accuracy loss.


<details>
  <summary>Details</summary>
Motivation: On-device inference is crucial for safeguarding user data privacy on mobile and IoT devices. However, deploying models within Trusted Execution Environments (TEEs) to protect against model stealing (MS) and membership inference attacks (MIA) faces challenges due to limited secure memory capacity and the need for low inference latency.

Method: TensorShield introduces a novel eXplainable AI (XAI) technique that uses the model's attention transition to assess and shield critical tensors in TEEs. It also includes two designs focusing on critical feature identification and latency-aware placement to speed up inference while ensuring security.

Result: Evaluations demonstrate that TensorShield offers nearly the same security protection as shielding the entire model in TEEs, but is significantly faster (up to 25.35×, avg. 5.85×) than current state-of-the-art methods without any loss in accuracy.

Conclusion: TensorShield successfully addresses the challenge of achieving full model security with low inference latency by protecting only partial tensors, thus providing an effective solution for secure on-device inference.

Abstract: To safeguard user data privacy, on-device inference has emerged as a
prominent paradigm on mobile and Internet of Things (IoT) devices. This
paradigm involves deploying a model provided by a third party on local devices
to perform inference tasks. However, it exposes the private model to two
primary security threats: model stealing (MS) and membership inference attacks
(MIA). To mitigate these risks, existing wisdom deploys models within Trusted
Execution Environments (TEEs), which is a secure isolated execution space.
Nonetheless, the constrained secure memory capacity in TEEs makes it
challenging to achieve full model security with low inference latency. This
paper fills the gap with TensorShield, the first efficient on-device inference
work that shields partial tensors of the model while still fully defending
against MS and MIA. The key enabling techniques in TensorShield include: (i) a
novel eXplainable AI (XAI) technique exploits the model's attention transition
to assess critical tensors and shields them in TEE to achieve secure inference,
and (ii) two meticulous designs with critical feature identification and
latency-aware placement to accelerate inference while maintaining security.
Extensive evaluations show that TensorShield delivers almost the same security
protection as shielding the entire model inside TEE, while being up to
25.35$\times$ (avg. 5.85$\times$) faster than the state-of-the-art work,
without accuracy loss.

</details>


### [202] [Aurora: Are Android Malware Classifiers Reliable under Distribution Shift?](https://arxiv.org/abs/2505.22843)
*Alexander Herzog,Aliai Eusebi,Lorenzo Cavallaro*

Main category: cs.CR

TL;DR: 现代自适应漂移的恶意软件分类器性能看似有希望，但实际操作中的可靠性如何呢？本文提出AURORA框架，评估分类器在分布变化下的置信度质量和操作弹性。通过超越时间点性能的指标，追求在整个时间评估期间更全面的操作稳定性评估。发现最先进的框架在不同漂移严重程度的数据集上表现出脆弱性，暗示需要重新设计。


<details>
  <summary>Details</summary>
Motivation: 当前对现代自适应漂移恶意软件分类器的评估主要集中在基线性能指标上，忽略了置信度-误差对齐和操作稳定性。虽然TESSERACT建立了时间评估的重要性，但本文探索了当这些分类器在分布变化下无法保持可靠的置信度估计时，科学进步与实际影响之间的紧张关系。

Method: 提出了AURORA框架，用于评估恶意软件分类器的置信质量及操作弹性。该框架验证给定模型的置信配置文件以评估其估计的可靠性，并辅以一系列超越时间点性能的指标，旨在实现更全面的时间评估期间的操作稳定性评估。

Result: 发现即使是最先进的框架，在不同漂移严重程度的数据集上也显示出脆弱性，表明当前方法在面对分布变化时存在不足。

Conclusion: 需要重新审视并改进现有的恶意软件分类器评估方法，以确保它们在实际应用中的可靠性和稳定性。

Abstract: The performance figures of modern drift-adaptive malware classifiers appear
promising, but does this translate to genuine operational reliability? The
standard evaluation paradigm primarily focuses on baseline performance metrics,
neglecting confidence-error alignment and operational stability. While
TESSERACT established the importance of temporal evaluation, we take a
complementary direction by investigating whether malware classifiers maintain
reliable confidence estimates under distribution shifts and exploring the
tensions between scientific advancement and practical impacts when they do not.
We propose AURORA, a framework to evaluate malware classifiers based on their
confidence quality and operational resilience. AURORA subjects the confidence
profile of a given model to verification to assess the reliability of its
estimates. Unreliable confidence estimates erode operational trust, waste
valuable annotation budget on non-informative samples for active learning, and
leave error-prone instances undetected in selective classification. AURORA is
further complemented by a set of metrics designed to go beyond point-in-time
performance, striving towards a more holistic assessment of operational
stability throughout temporal evaluation periods. The fragility we observe in
state-of-the-art frameworks across datasets of varying drift severity suggests
the need for a return to the whiteboard.

</details>


### [203] [Security Benefits and Side Effects of Labeling AI-Generated Images](https://arxiv.org/abs/2505.22845)
*Sandra Höltervennhoff,Jonas Ricker,Maike M. Raphael,Charlotte Schwedes,Rebecca Weil,Asja Fischer,Thorsten Holz,Lea Schönherr,Sascha Fahl*

Main category: cs.CR

TL;DR: 生成式人工智能迅速发展，影响了人类与信息和数字媒体的互动。虽然监管机构要求披露AI生成的内容，但关于这些标签是否能减少AI生成的虚假信息的风险知之甚少。本文研究了AI生成图像的标签影响，包括错误标签的可能性。通过焦点小组和在线调查，研究发现用户虽然认为标签有助于识别AI生成的图像，但也可能过度依赖标签，导致对准确声明的信任降低，并使人工制作的不实图像更具可信度。


<details>
  <summary>Details</summary>
Motivation: 生成式人工智能的发展带来了虚假信息的新风险，尽管已有法规要求披露AI生成内容，但其有效性尚未得到充分研究。

Method: 1. 通过五个焦点小组定性探索用户对AI标签的意见和期望。2. 进行一项预注册的在线调查，涉及超过1300名美国和欧盟参与者，以定量评估AI标签对用户识别虚假信息的能力的影响。

Result: - 焦点小组显示用户认为标签有助于识别AI生成图像，但对其实际实施存在担忧。
- 调查显示用户可能会过度依赖标签，导致对准确声明的信任降低。
- 带有标签的AI生成图像降低了不实声明的可信度，但也降低了准确声明的信任。
- 标签使人工制作的不实图像更可信。

Conclusion: AI生成内容的标签在帮助用户识别虚假信息方面具有潜力，但也可能导致过度依赖和意外副作用，需进一步优化标签设计和实施策略。

Abstract: Generative artificial intelligence is developing rapidly, impacting humans'
interaction with information and digital media. It is increasingly used to
create deceptively realistic misinformation, so lawmakers have imposed
regulations requiring the disclosure of AI-generated content. However, only
little is known about whether these labels reduce the risks of AI-generated
misinformation.
  Our work addresses this research gap. Focusing on AI-generated images, we
study the implications of labels, including the possibility of mislabeling.
Assuming that simplicity, transparency, and trust are likely to impact the
successful adoption of such labels, we first qualitatively explore users'
opinions and expectations of AI labeling using five focus groups. Second, we
conduct a pre-registered online survey with over 1300 U.S. and EU participants
to quantitatively assess the effect of AI labels on users' ability to recognize
misinformation containing either human-made or AI-generated images. Our focus
groups illustrate that, while participants have concerns about the practical
implementation of labeling, they consider it helpful in identifying
AI-generated images and avoiding deception. However, considering security
benefits, our survey revealed an ambiguous picture, suggesting that users might
over-rely on labels. While inaccurate claims supported by labeled AI-generated
images were rated less credible than those with unlabeled AI-images, the belief
in accurate claims also decreased when accompanied by a labeled AI-generated
image. Moreover, we find the undesired side effect that human-made images
conveying inaccurate claims were perceived as more credible in the presence of
labels.

</details>


### [204] [Operationalizing CaMeL: Strengthening LLM Defenses for Enterprise Deployment](https://arxiv.org/abs/2505.22852)
*Krti Tallam,Emma Miller*

Main category: cs.CR

TL;DR: CaMeL introduces a capability-based sandbox to mitigate prompt injection attacks in LLM agents, but it assumes trusted user prompts, ignores side-channel concerns, and has performance tradeoffs. This response proposes engineering improvements including prompt screening, output auditing, tiered-risk access model, and verified intermediate language.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of CaMeL system such as assuming trusted user prompts, ignoring side-channel concerns, and having performance tradeoffs due to its dual-LLM design.

Method: The methods proposed include prompt screening for initial inputs, output auditing to detect instruction leakage, a tiered-risk access model to balance usability and control, and a verified intermediate language for formal guarantees.

Result: These upgrades align CaMeL with best practices in enterprise security and support scalable deployment.

Conclusion: Overall, the conclusion is that these engineering improvements expand CaMeL's threat coverage and operational usability.

Abstract: CaMeL (Capabilities for Machine Learning) introduces a capability-based
sandbox to mitigate prompt injection attacks in large language model (LLM)
agents. While effective, CaMeL assumes a trusted user prompt, omits
side-channel concerns, and incurs performance tradeoffs due to its dual-LLM
design. This response identifies these issues and proposes engineering
improvements to expand CaMeL's threat coverage and operational usability. We
introduce: (1) prompt screening for initial inputs, (2) output auditing to
detect instruction leakage, (3) a tiered-risk access model to balance usability
and control, and (4) a verified intermediate language for formal guarantees.
Together, these upgrades align CaMeL with best practices in enterprise security
and support scalable deployment.

</details>


### [205] [Permissioned LLMs: Enforcing Access Control in Large Language Models](https://arxiv.org/abs/2505.22860)
*Bargav Jayaraman,Virendra J. Marathe,Hamid Mozaffari,William F. Shen,Krishnaram Kenthapadi*

Main category: cs.CR

TL;DR: In enterprise settings, LLMs fine-tuned on siloed data can break down access control structures. This paper proposes Permissioned LLMs (PermLLM), which superimpose organizational data access control structures on query responses. It introduces the concept of a relevant response and a novel metric called access advantage to evaluate PermLLM efficacy. Three PermLLM mechanisms are presented, as well as two instantiations of access advantage - Domain Distinguishability Index (DDI) and Utility Gap Index (UGI). Extensive experiments demonstrate their efficacy.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work is the problem that arises when an LLM fine-tuned on siloed, segregated organizational data serves requests from individuals with different access privileges, potentially breaking down access control structures.

Method: The method involves proposing Permissioned LLMs (PermLLM), formalizing abstractions for determining correct access control enforcement over LLM query responses, introducing the notion of a relevant response, and developing a novel metric called access advantage. Three PermLLM mechanisms based on Parameter Efficient Fine-Tuning are introduced, along with two instantiations of access advantage: Domain Distinguishability Index (DDI) and Utility Gap Index (UGI).

Result: The results show the efficacy of the PermLLM mechanisms through extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP). The validity of DDI and UGI metrics for quantifying access control in LLMs is also evaluated.

Conclusion: The conclusion is that PermLLMs successfully superimpose organizational data access control structures on query responses, preserving access control even when serving requests from individuals with disparate access privileges. The proposed mechanisms and metrics effectively evaluate and enforce access control in LLMs.

Abstract: In enterprise settings, organizational data is segregated, siloed and
carefully protected by elaborate access control frameworks. These access
control structures can completely break down if an LLM fine-tuned on the siloed
data serves requests, for downstream tasks, from individuals with disparate
access privileges. We propose Permissioned LLMs (PermLLM), a new class of LLMs
that superimpose the organizational data access control structures on query
responses they generate. We formalize abstractions underpinning the means to
determine whether access control enforcement happens correctly over LLM query
responses. Our formalism introduces the notion of a relevant response that can
be used to prove whether a PermLLM mechanism has been implemented correctly. We
also introduce a novel metric, called access advantage, to empirically evaluate
the efficacy of a PermLLM mechanism. We introduce three novel PermLLM
mechanisms that build on Parameter Efficient Fine-Tuning to achieve the desired
access control. We furthermore present two instantiations of access
advantage--(i) Domain Distinguishability Index (DDI) based on Membership
Inference Attacks, and (ii) Utility Gap Index (UGI) based on LLM utility
evaluation. We demonstrate the efficacy of our PermLLM mechanisms through
extensive experiments on four public datasets (GPQA, RCV1, SimpleQA, and WMDP),
in addition to evaluating the validity of DDI and UGI metrics themselves for
quantifying access control in LLMs.

</details>


### [206] [BugWhisperer: Fine-Tuning LLMs for SoC Hardware Vulnerability Detection](https://arxiv.org/abs/2505.22878)
*Shams Tarek,Dipayan Saha,Sujan Kumar Saha,Farimah Farahmandi*

Main category: cs.CR

TL;DR: The paper introduces BugWhisperer, a framework using a fine-tuned LLM for automating SoC security verification, improving efficiency and flexibility, alongside an open-source model and hardware vulnerability database.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of manual, labor-intensive, and inflexible methodologies in SoC security verification that limit scalability and effectiveness, making RTL bug detection difficult.

Method: Propose BugWhisperer framework utilizing a specialized, fine-tuned LLM to enhance hardware security knowledge, automate, and improve adaptability and reusability of the verification process. Introduce an open-source, fine-tuned LLM for detecting security vulnerabilities and a comprehensive hardware vulnerability database.

Result: The tailored LLM effectively enhances the efficiency and flexibility of the security verification process, with support from the introduced hardware vulnerability database.

Conclusion: BugWhisperer offers a novel solution to automate and improve SoC security verification, providing an open-source model and database to assist the research community.

Abstract: The current landscape of system-on-chips (SoCs) security verification faces
challenges due to manual, labor-intensive, and inflexible methodologies. These
issues limit the scalability and effectiveness of security protocols, making
bug detection at the Register-Transfer Level (RTL) difficult. This paper
proposes a new framework named BugWhisperer that utilizes a specialized,
fine-tuned Large Language Model (LLM) to address these challenges. By enhancing
the LLM's hardware security knowledge and leveraging its capabilities for text
inference and knowledge transfer, this approach automates and improves the
adaptability and reusability of the verification process. We introduce an
open-source, fine-tuned LLM specifically designed for detecting security
vulnerabilities in SoC designs. Our findings demonstrate that this tailored LLM
effectively enhances the efficiency and flexibility of the security
verification process. Additionally, we introduce a comprehensive hardware
vulnerability database that supports this work and will further assist the
research community in enhancing the security verification process.

</details>


### [207] [Chainless Apps: A Modular Framework for Building Apps with Web2 Capability and Web3 Trust](https://arxiv.org/abs/2505.22989)
*Brian Seong,Paul Gebheim*

Main category: cs.CR

TL;DR: Chainless Apps introduce a new way to build blockchain applications that prioritize both user experience and trust by separating execution, trust, bridging, and settlement into different layers. This approach allows for fast, verifiable applications compatible with Ethereum and other blockchains.


<details>
  <summary>Details</summary>
Motivation: Current blockchain applications face challenges balancing user experience with the level of trust required in their operations. Chainless Apps aim to address this issue by restructuring application architecture.

Method: Chainless Apps separate key functions (execution, trust, bridging, settlement) into distinct layers enabling app-specific sequencing, off-chain computation verification, asset/message routing through Agglayer, and finality on Ethereum.

Result: This method results in applications that provide a fast, Web2-like user experience while maintaining Web3-grade verifiability and trust. Developers can create modular, scalable apps that are interoperable with the broader blockchain ecosystem.

Conclusion: Chainless Apps represent an evolution in blockchain application development, leveraging advancements like zkVMs and decentralized validation services to offer a flexible, efficient, and trust-worthy solution.

Abstract: Modern blockchain applications are often constrained by a trade-off between
user experience and trust. Chainless Apps present a new paradigm of application
architecture that separates execution, trust, bridging, and settlement into
distinct compostable layers. This enables app-specific sequencing, verifiable
off-chain computation, chain-agnostic asset and message routing via Agglayer,
and finality on Ethereum - resulting in fast Web2-like UX with Web3-grade
verifiability. Although consensus mechanisms have historically underpinned
verifiable computation, the advent of zkVMs and decentralized validation
services opens up new trust models for developers. Chainless Apps leverage this
evolution to offer modular, scalable applications that maintain
interoperability with the broader blockchain ecosystem while allowing
domain-specific trade-offs.

</details>


### [208] [AgentAlign: Navigating Safety Alignment in the Shift from Informative to Agentic Large Language Models](https://arxiv.org/abs/2505.23020)
*Jinchuan Zhang,Lu Yin,Yan Zhou,Songlin Hu*

Main category: cs.CR

TL;DR: The paper introduces AgentAlign, a framework that improves safety alignment in LLM-based agents by synthesizing safety data through abstract behavior chains. It enhances safety significantly while maintaining helpfulness.


<details>
  <summary>Details</summary>
Motivation: There is a need to address the deficiency in safety alignment of LLM-based agents during the post-training phase, as these agents can execute malicious tasks without being attacked.

Method: Propose AgentAlign, a framework leveraging abstract behavior chains for safety alignment data synthesis. It generates authentic instructions and ensures model utility by synthesizing benign instructions through non-malicious interpretations of behavior chains.

Result: Evaluation on AgentHarm shows fine-tuning open-source models with this method improves safety by 35.8% to 79.5% while minimally impacting or enhancing helpfulness, outperforming various prompting methods.

Conclusion: AgentAlign successfully enhances safety alignment in LLM-based agents while preserving or improving their helpfulness.

Abstract: The acquisition of agentic capabilities has transformed LLMs from "knowledge
providers" to "action executors", a trend that while expanding LLMs' capability
boundaries, significantly increases their susceptibility to malicious use.
Previous work has shown that current LLM-based agents execute numerous
malicious tasks even without being attacked, indicating a deficiency in agentic
use safety alignment during the post-training phase. To address this gap, we
propose AgentAlign, a novel framework that leverages abstract behavior chains
as a medium for safety alignment data synthesis. By instantiating these
behavior chains in simulated environments with diverse tool instances, our
framework enables the generation of highly authentic and executable
instructions while capturing complex multi-step dynamics. The framework further
ensures model utility by proportionally synthesizing benign instructions
through non-malicious interpretations of behavior chains, precisely calibrating
the boundary between helpfulness and harmlessness. Evaluation results on
AgentHarm demonstrate that fine-tuning three families of open-source models
using our method substantially improves their safety (35.8% to 79.5%
improvement) while minimally impacting or even positively enhancing their
helpfulness, outperforming various prompting methods. The dataset and code have
both been open-sourced.

</details>


### [209] [Disrupting Vision-Language Model-Driven Navigation Services via Adversarial Object Fusion](https://arxiv.org/abs/2505.23266)
*Chunlong Xie,Jialing He,Shangwei Guo,Jiacheng Wang,Shudong Zhang,Tianwei Zhang,Tao Xiang*

Main category: cs.CR

TL;DR: This paper introduces Adversarial Object Fusion (AdvOF), a new attack framework that targets vision-and-language navigation (VLN) agents in service-oriented environments by generating adversarial 3D objects. It aims to investigate the impact of adversarial environments on VLN agents' perception module based on Vision Language Models (VLMs). AdvOF can degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the vulnerabilities introduced by integrating foundational models like Large Language Models (LLMs) and Vision Language Models (VLMs) into service-oriented navigation systems, especially in mission-critical service workflows where reliability and quality-of-service are paramount. Existing adversarial attacks fail to address these specific contexts.

Method: AdvOF first aggregates and aligns victim object positions in both 2D and 3D space to define and render adversarial objects. Then it optimizes the adversarial object collaboratively with regularization between the adversarial and victim object across physical properties and VLM perceptions. Importance weights are assigned to varying views for stable and multi-view optimization through iterative fusions from local updates and justifications.

Result: Extensive evaluations show that AdvOF can effectively degrade agent performance under adversarial conditions while maintaining minimal interference with normal navigation tasks.

Conclusion: This work enhances the understanding of service security in VLM-powered navigation systems and provides computational foundations for robust service composition in physical-world deployments.

Abstract: We present Adversarial Object Fusion (AdvOF), a novel attack framework
targeting vision-and-language navigation (VLN) agents in service-oriented
environments by generating adversarial 3D objects. While foundational models
like Large Language Models (LLMs) and Vision Language Models (VLMs) have
enhanced service-oriented navigation systems through improved perception and
decision-making, their integration introduces vulnerabilities in
mission-critical service workflows. Existing adversarial attacks fail to
address service computing contexts, where reliability and quality-of-service
(QoS) are paramount. We utilize AdvOF to investigate and explore the impact of
adversarial environments on the VLM-based perception module of VLN agents. In
particular, AdvOF first precisely aggregates and aligns the victim object
positions in both 2D and 3D space, defining and rendering adversarial objects.
Then, we collaboratively optimize the adversarial object with regularization
between the adversarial and victim object across physical properties and VLM
perceptions. Through assigning importance weights to varying views, the
optimization is processed stably and multi-viewedly by iterative fusions from
local updates and justifications. Our extensive evaluations demonstrate AdvOF
can effectively degrade agent performance under adversarial conditions while
maintaining minimal interference with normal navigation tasks. This work
advances the understanding of service security in VLM-powered navigation
systems, providing computational foundations for robust service composition in
physical-world deployments.

</details>


### [210] [Joint Data Hiding and Partial Encryption of Compressive Sensed Streams](https://arxiv.org/abs/2505.23357)
*Cristina-Elena Popa,Cristian Damian,Daniela Coltuc*

Main category: cs.CR

TL;DR: The paper proposes a method to secure Compressive Sensing (CS) streams by protecting part of the measurements with a secret key and inserting them into the rest using a reversible data hiding scheme. This on-the-fly insertion method is suitable for sequential acquisition by a Single Pixel Camera.


<details>
  <summary>Details</summary>
Motivation: To secure the Compressive Sensing streams while avoiding the need for buffering measurements for subsequent encryption and thumbnail generation.

Method: The method involves generating a secret key via a cryptographically secure pseudo-random number generator (CSPRNG), XORing it with selected measurements, and inserting the code into the remaining measurements using a modified reversible data hiding scheme based on prediction error expansion algorithm.

Result: Reconstruction from the embedded stream leads to visibly distorted images, with distortion controlled by the number of embedded levels. In tests, embedding on 10 levels resulted in approximately 18 dB distortion for 256x256 pixel images reconstructed with FISTA.

Conclusion: The proposed method successfully secures CS streams with on-the-fly insertion, making it suitable for use with Single Pixel Cameras.

Abstract: The paper proposes a method to secure the Compressive Sensing (CS) streams.
It consists in protecting part of the measurements by a secret key and
inserting the code into the rest. The secret key is generated via a
cryptographically secure pseudo-random number generator (CSPRNG) and XORed with
the measurements to be inserted. For insertion, we use a reversible data hiding
(RDH) scheme, which is a prediction error expansion algorithm, modified to
match the statistics of CS measurements. The reconstruction from the embedded
stream conducts to visibly distorted images. The image distortion is controlled
by the number of embedded levels. In our tests, the embedding on 10 levels
results in $\approx 18 dB $ distortion for images of 256x256 pixels
reconstructed with the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA).
A particularity of the presented method is on-the-fly insertion that makes it
appropriate for the sequential acquisition of measurements by a Single Pixel
Camera. On-the-fly insertion avoids the buffering of CS measurements for a
subsequent standard encryption and generation of a thumbnail image.

</details>


### [211] [Merge Hijacking: Backdoor Attacks to Model Merging of Large Language Models](https://arxiv.org/abs/2505.23561)
*Zenghui Yuan,Yangming Xu,Jiawen Shi,Pan Zhou,Lichao Sun*

Main category: cs.CR

TL;DR: The paper introduces Merge Hijacking, a novel backdoor attack targeting model merging in Large Language Models (LLMs). It allows malicious models to introduce backdoors during merging while maintaining task utility. Experiments validate the attack's effectiveness and robustness against defenses.


<details>
  <summary>Details</summary>
Motivation: Model merging in LLMs is vulnerable to backdoor attacks due to potential vulnerabilities in open-source models.

Method: The attacker creates a malicious model and uploads it. When merged with other models, the resulting model inherits the backdoor while preserving utility across tasks. The process includes four steps focusing on two main objectives: effectiveness and utility.

Result: Experiments show that Merge Hijacking is effective across different models, merging algorithms, and tasks. The attack remains successful even when merging real-world models and demonstrates robustness against various defenses.

Conclusion: Merge Hijacking presents a significant threat to model merging in LLMs, highlighting the need for stronger defense mechanisms.

Abstract: Model merging for Large Language Models (LLMs) directly fuses the parameters
of different models finetuned on various tasks, creating a unified model for
multi-domain tasks. However, due to potential vulnerabilities in models
available on open-source platforms, model merging is susceptible to backdoor
attacks. In this paper, we propose Merge Hijacking, the first backdoor attack
targeting model merging in LLMs. The attacker constructs a malicious upload
model and releases it. Once a victim user merges it with any other models, the
resulting merged model inherits the backdoor while maintaining utility across
tasks. Merge Hijacking defines two main objectives-effectiveness and
utility-and achieves them through four steps. Extensive experiments demonstrate
the effectiveness of our attack across different models, merging algorithms,
and tasks. Additionally, we show that the attack remains effective even when
merging real-world models. Moreover, our attack demonstrates robustness against
two inference-time defenses (Paraphrasing and CLEANGEN) and one training-time
defense (Fine-pruning).

</details>


### [212] [Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643)
*Manuel Costa,Boris Köpf,Aashish Kolluri,Andrew Paverd,Mark Russinovich,Ahmed Salem,Shruti Tople,Lukas Wutschitz,Santiago Zanella-Béguelin*

Main category: cs.CR

TL;DR: This paper investigates the application of information-flow control (IFC) to ensure AI agents' security, presenting a formal model for reasoning about security and expressiveness. It introduces Fides, a planner that enforces security policies while maintaining utility, and evaluates its effectiveness.


<details>
  <summary>Details</summary>
Motivation: With AI agents becoming more autonomous, ensuring their security against vulnerabilities like prompt injection is crucial.

Method: The paper proposes a formal model to reason about the security and expressiveness of agent planners, characterizes properties enforceable by dynamic taint-tracking, constructs a taxonomy of tasks for evaluating security/utility trade-offs, and presents Fides, a planner that tracks confidentiality and integrity labels and enforces security policies deterministically.

Result: Fides was evaluated in AgentDojo, demonstrating that this approach broadens the range of tasks that can be securely accomplished by AI agents.

Conclusion: Information-flow control (IFC) can provide security guarantees for AI agents. The proposed formal model and Fides planner offer a method to balance security and utility in AI agent design.

Abstract: As AI agents become increasingly autonomous and capable, ensuring their
security against vulnerabilities such as prompt injection becomes critical.
This paper explores the use of information-flow control (IFC) to provide
security guarantees for AI agents. We present a formal model to reason about
the security and expressiveness of agent planners. Using this model, we
characterize the class of properties enforceable by dynamic taint-tracking and
construct a taxonomy of tasks to evaluate security and utility trade-offs of
planner designs. Informed by this exploration, we present Fides, a planner that
tracks confidentiality and integrity labels, deterministically enforces
security policies, and introduces novel primitives for selectively hiding
information. Its evaluation in AgentDojo demonstrates that this approach
broadens the range of tasks that can be securely accomplished. A tutorial to
walk readers through the the concepts introduced in the paper can be found at
https://github.com/microsoft/fides

</details>


### [213] [Keyed Chaotic Tensor Transformations for Secure And Attributable Neural Inference](https://arxiv.org/abs/2505.23655)
*Peter David Fagan*

Main category: cs.CR

TL;DR: This paper presents a new framework using chaotic dynamical transformations for secure neural network inference.


<details>
  <summary>Details</summary>
Motivation: There is a need for secure and privacy-preserving methods in neural network inference, which current cryptographic techniques may not efficiently fulfill.

Method: The method employs a deterministic, cryptographically seeded chaotic system to tensors to create non-invertible, user-specific transformations that support authenticated inference, watermarking, and data attribution.

Result: This approach provides a scalable and lightweight alternative compared to traditional cryptographic methods.

Conclusion: The novel framework establishes a new direction for enhancing tensor-level security in AI systems.

Abstract: This work introduces a novel framework for secure and privacy-preserving
neural network inference based on keyed chaotic dynamical transformations. The
proposed method applies a deterministic, cryptographically seeded chaotic
system to tensors, producing non-invertible, user-specific transformations that
enable authenticated inference, tensor-level watermarking, and data
attribution. This framework offers a scalable and lightweight alternative to
conventional cryptographic techniques, and establishes a new direction for
tensor-level security in AI systems.

</details>


### [214] [Bayesian Perspective on Memorization and Reconstruction](https://arxiv.org/abs/2505.23658)
*Haim Kaplan,Yishay Mansour,Kobbi Nissim,Uri Stemmer*

Main category: cs.CR

TL;DR: 本文提出了一种新的贝叶斯视角下的数据重建概念，并据此提出了一种新的安全定义，该定义在某些情况下可以防止重建攻击。通过这一范式，文章重新解释了隐私和记忆文献中最臭名昭著的攻击之一——指纹码攻击（FPC），认为其本质上是成员推断攻击而非重建攻击。此外，文章指出如果目标仅仅是防止重建（而非成员推断），则在某些情况下由FPC得出的不可能性结果不再适用。


<details>
  <summary>Details</summary>
Motivation: 当前对于数据重建攻击的理解可能存在偏差，特别是在隐私保护领域中，指纹码攻击（FPC）被广泛视为一种重建攻击。然而，这种理解可能掩盖了更深层次的安全问题，需要从新的角度重新审视这些攻击的本质。

Method: 作者引入了一个新的贝叶斯视角来分析数据重建的概念，并基于此提出了一个新的安全定义，该定义能够证明在某些设定下防止重建攻击的可能性。同时，通过将指纹码攻击重新归类为成员推断攻击，进一步探讨了防止重建攻击的不同方法。

Result: 研究表明，指纹码攻击实际上属于成员推断攻击而非重建攻击。并且，当仅考虑防止重建攻击时，在某些情况下，传统上认为的不可能性结果不再成立。这为设计更有效的隐私保护机制提供了新的思路。

Conclusion: 本文通过引入新的贝叶斯视角和安全定义，重新定义了数据重建攻击与成员推断攻击之间的关系，揭示了现有攻击分类的不足，并为未来的隐私保护研究提供了理论支持。

Abstract: We introduce a new Bayesian perspective on the concept of data
reconstruction, and leverage this viewpoint to propose a new security
definition that, in certain settings, provably prevents reconstruction attacks.
We use our paradigm to shed new light on one of the most notorious attacks in
the privacy and memorization literature - fingerprinting code attacks (FPC). We
argue that these attacks are really a form of membership inference attacks,
rather than reconstruction attacks. Furthermore, we show that if the goal is
solely to prevent reconstruction (but not membership inference), then in some
cases the impossibility results derived from FPC no longer apply.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [215] [Semantic Exploration and Dense Mapping of Complex Environments using Ground Robots Equipped with LiDAR and Panoramic Camera](https://arxiv.org/abs/2505.22880)
*Xiaoyang Zhan,Shixin Zhou,Qianqian Yang,Yixuan Zhao,Hao Liu,Srinivas Chowdary Ramineni,Kenji Shimada*

Main category: cs.RO

TL;DR: This paper presents a system for autonomous semantic exploration and dense semantic target mapping using a ground robot equipped with a LiDAR-panoramic camera suite.


<details>
  <summary>Details</summary>
Motivation: Existing approaches struggle to balance collecting high-quality observations from multiple view angles and avoiding unnecessary repetitive traversal in complex unknown environments.

Method: The authors propose a complete system combining mapping and planning. They redefine the task as completing both geometric coverage and semantic viewpoint observation, manage semantic and geometric viewpoints separately, and propose a novel Priority-driven Decoupled Local Sampler to generate local viewpoint sets. They also develop a hierarchical planner for efficient global coverage and a Safe Aggressive Exploration State Machine for aggressive yet safe exploration. The system includes a plug-and-play semantic target mapping module that integrates seamlessly with state-of-the-art SLAM algorithms.

Result: Simulation results show faster exploration and shorter travel distances while guaranteeing a specified number of multi-view inspections. Real-world experiments confirm the system's effectiveness in achieving accurate dense semantic object mapping of unstructured environments.

Conclusion: The proposed system successfully enables autonomous semantic exploration and dense semantic target mapping in complex unknown environments.

Abstract: This paper presents a system for autonomous semantic exploration and dense
semantic target mapping of a complex unknown environment using a ground robot
equipped with a LiDAR-panoramic camera suite. Existing approaches often
struggle to balance collecting high-quality observations from multiple view
angles and avoiding unnecessary repetitive traversal. To fill this gap, we
propose a complete system combining mapping and planning. We first redefine the
task as completing both geometric coverage and semantic viewpoint observation.
We then manage semantic and geometric viewpoints separately and propose a novel
Priority-driven Decoupled Local Sampler to generate local viewpoint sets. This
enables explicit multi-view semantic inspection and voxel coverage without
unnecessary repetition. Building on this, we develop a hierarchical planner to
ensure efficient global coverage. In addition, we propose a Safe Aggressive
Exploration State Machine, which allows aggressive exploration behavior while
ensuring the robot's safety. Our system includes a plug-and-play semantic
target mapping module that integrates seamlessly with state-of-the-art SLAM
algorithms for pointcloud-level dense semantic target mapping. We validate our
approach through extensive experiments in both realistic simulations and
complex real-world environments. Simulation results show that our planner
achieves faster exploration and shorter travel distances while guaranteeing a
specified number of multi-view inspections. Real-world experiments further
confirm the system's effectiveness in achieving accurate dense semantic object
mapping of unstructured environments.

</details>


### [216] [VLM-RRT: Vision Language Model Guided RRT Search for Autonomous UAV Navigation](https://arxiv.org/abs/2505.23267)
*Jianlin Ye,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.RO

TL;DR: The paper proposes VLM-RRT, a hybrid path planning method for UAVs that combines Vision Language Models with RRT to improve sampling efficiency and path quality.


<details>
  <summary>Details</summary>
Motivation: Traditional path planning methods like RRT face challenges such as high search space complexity, suboptimal path quality, and slow convergence, which are significant issues in critical applications like disaster response.

Method: The proposed method, VLM-RRT, integrates the pattern recognition capabilities of Vision Language Models (VLMs) with the path-planning strengths of RRT. VLMs provide initial directional guidance based on environmental snapshots, biasing sampling towards regions more likely to contain feasible paths.

Result: Extensive experiments with various state-of-the-art VLMs demonstrate the effectiveness of VLM-RRT in significantly improving sampling efficiency and path quality.

Conclusion: VLM-RRT is a promising approach that addresses the limitations of traditional path planning methods and enhances the efficiency and quality of path planning for UAVs.

Abstract: Path planning is a fundamental capability of autonomous Unmanned Aerial
Vehicles (UAVs), enabling them to efficiently navigate toward a target region
or explore complex environments while avoiding obstacles. Traditional
pathplanning methods, such as Rapidly-exploring Random Trees (RRT), have proven
effective but often encounter significant challenges. These include high search
space complexity, suboptimal path quality, and slow convergence, issues that
are particularly problematic in high-stakes applications like disaster
response, where rapid and efficient planning is critical. To address these
limitations and enhance path-planning efficiency, we propose Vision Language
Model RRT (VLM-RRT), a hybrid approach that integrates the pattern recognition
capabilities of Vision Language Models (VLMs) with the path-planning strengths
of RRT. By leveraging VLMs to provide initial directional guidance based on
environmental snapshots, our method biases sampling toward regions more likely
to contain feasible paths, significantly improving sampling efficiency and path
quality. Extensive quantitative and qualitative experiments with various
state-of-the-art VLMs demonstrate the effectiveness of this proposed approach.

</details>


### [217] [A Robot-Assisted Approach to Small Talk Training for Adults with ASD](https://arxiv.org/abs/2505.23508)
*Rebecca Ramnauth,Dražen Brščić,Brian Scassellati*

Main category: cs.RO

TL;DR: A study about an in-home autonomous robot system aiding adults with ASD to practice small talk, showing positive results.


<details>
  <summary>Details</summary>
Motivation: To assist adults with Autism Spectrum Disorder (ASD) overcome challenges in engaging in small talk for better social integration, relationship building, and professional opportunities.

Method: Development and evaluation of an in-home autonomous robot system that lets users practice small talk.

Result: Adults with ASD enjoyed the training, improved in initiating conversations and eye contact, and found the system valuable for conversational skills.

Conclusion: The in-home autonomous robot system is effective in helping adults with ASD improve their small talk abilities.

Abstract: From dating to job interviews, making new friends or simply chatting with the
cashier at checkout, engaging in small talk is a vital, everyday social skill.
For adults with Autism Spectrum Disorder (ASD), small talk can be particularly
challenging, yet it is essential for social integration, building
relationships, and accessing professional opportunities. In this study, we
present our development and evaluation of an in-home autonomous robot system
that allows users to practice small talk. Results from the week-long study show
that adults with ASD enjoyed the training, made notable progress in initiating
conversations and improving eye contact, and viewed the system as a valuable
tool for enhancing their conversational skills.

</details>


### [218] [Cognitive Guardrails for Open-World Decision Making in Autonomous Drone Swarms](https://arxiv.org/abs/2505.23576)
*Jane Cleland-Huang,Pedro Antonio Alarcon Granadeno,Arturo Miguel Russell Bernal,Demetrius Hernandez,Michael Murphy,Maureen Petterson,Walter Scheirer*

Main category: cs.RO

TL;DR: In this paper, the authors explore the use of large language models (LLMs) to enhance the object recognition and decision-making capabilities of Small Uncrewed Aerial Systems (sUAS) in search-and-rescue missions. They address the issue of LLM hallucinations by introducing cognitive guardrails to ensure safe decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the challenge faced by traditional computer vision systems in recognizing unfamiliar objects and inferring their relevance in open-world environments, particularly in search-and-rescue missions using sUAS swarms.

Method: The method involves incorporating LLMs into sUAS systems for reasoning about detected objects and their implications, while implementing cognitive guardrails to manage the uncertainty and potential inaccuracies associated with LLMs.

Result: The result is a system design that successfully integrates LLMs with cognitive guardrails for sUAS swarms, improving their autonomous decision-making capabilities in complex environments.

Conclusion: The conclusion drawn from this study is that the integration of LLMs with cognitive guardrails can significantly enhance the performance of sUAS in search-and-rescue missions, leading to more effective and safer operations.

Abstract: Small Uncrewed Aerial Systems (sUAS) are increasingly deployed as autonomous
swarms in search-and-rescue and other disaster-response scenarios. In these
settings, they use computer vision (CV) to detect objects of interest and
autonomously adapt their missions. However, traditional CV systems often
struggle to recognize unfamiliar objects in open-world environments or to infer
their relevance for mission planning. To address this, we incorporate large
language models (LLMs) to reason about detected objects and their implications.
While LLMs can offer valuable insights, they are also prone to hallucinations
and may produce incorrect, misleading, or unsafe recommendations. To ensure
safe and sensible decision-making under uncertainty, high-level decisions must
be governed by cognitive guardrails. This article presents the design,
simulation, and real-world integration of these guardrails for sUAS swarms in
search-and-rescue missions.

</details>


### [219] [Anomalies by Synthesis: Anomaly Detection using Generative Diffusion Models for Off-Road Navigation](https://arxiv.org/abs/2505.22805)
*Siddharth Ancha,Sunshine Jiang,Travis Manderson,Laura Brandt,Yilun Du,Philip R. Osteen,Nicholas Roy*

Main category: cs.RO

TL;DR: 研究提出了一种无需对OOD数据做任何假设的像素级异常检测方法，利用生成扩散模型合成去除异常后的图像，并通过分析被修改的图像片段实现异常检测。该方法在测试时进行编辑，无需重新训练或微调，同时结合视觉-语言基础模型以检测语义上有意义的编辑，从而为非道路导航提供精确的异常检测。


<details>
  <summary>Details</summary>
Motivation: 机器人需要在非道路和无结构环境中安全可靠地导航，这就要求能够检测到相对于训练数据的异常（OOD）。然而，传统的异常检测方法通常需要对OOD数据有所假设，这限制了其适用性。因此，需要一种新的方法来解决这一问题。

Method: 研究采用了一种分析-合成的方法来进行像素级异常检测。具体步骤包括：1) 使用生成扩散模型对输入图像进行编辑，去除异常部分；2) 通过比较原始图像与编辑后图像，确定哪些部分被修改；3) 提出一种新的推理方法，用于引导扩散过程，预测指导梯度；4) 在测试阶段进行编辑，无需重新训练或微调模型；5) 利用视觉-语言基础模型，在学习到的特征空间中比较像素，检测语义上有意义的编辑。

Result: 该方法能够在不重新训练或微调模型的情况下，准确地检测非道路导航中的异常。实验结果表明，这种方法在像素级异常检测任务上表现出色，特别是在不需要对OOD数据做出假设的情况下。

Conclusion: 本研究提出的分析-合成方法提供了一种有效的像素级异常检测解决方案，适用于非道路环境下的机器人导航任务。通过结合生成扩散模型和视觉-语言基础模型，该方法不仅能够准确检测异常，还能保持现有工作流程不变，具有广泛的应用前景。

Abstract: In order to navigate safely and reliably in off-road and unstructured
environments, robots must detect anomalies that are out-of-distribution (OOD)
with respect to the training data. We present an analysis-by-synthesis approach
for pixel-wise anomaly detection without making any assumptions about the
nature of OOD data. Given an input image, we use a generative diffusion model
to synthesize an edited image that removes anomalies while keeping the
remaining image unchanged. Then, we formulate anomaly detection as analyzing
which image segments were modified by the diffusion model. We propose a novel
inference approach for guided diffusion by analyzing the ideal guidance
gradient and deriving a principled approximation that bootstraps the diffusion
model to predict guidance gradients. Our editing technique is purely test-time
that can be integrated into existing workflows without the need for retraining
or fine-tuning. Finally, we use a combination of vision-language foundation
models to compare pixels in a learned feature space and detect semantically
meaningful edits, enabling accurate anomaly detection for off-road navigation.
Project website: https://siddancha.github.io/anomalies-by-diffusion-synthesis/

</details>


### [220] [Learning coordinated badminton skills for legged manipulators](https://arxiv.org/abs/2505.22974)
*Yuntao Ma,Andrei Cramariuc,Farbod Farshidian,Marco Hutter*

Main category: cs.RO

TL;DR: This paper presents a unified reinforcement learning-based control policy for legged mobile manipulators to play badminton, incorporating perception noise model, shuttlecock prediction, constrained reinforcement learning, and system identification techniques. Experiments show the robot's ability to predict trajectories, navigate, and strike effectively.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of coordinating motion between lower and upper limbs and aligning limb control with perception in dynamic environments, the authors chose badminton as a task that requires precise coordination of perception, locomotion, and arm swinging.

Method: The method involves a unified reinforcement learning-based control policy for whole-body visuomotor skills, a perception noise model using real-world camera data, a shuttlecock prediction model, constrained reinforcement learning for robust motion control, and integrated system identification techniques.

Result: The robot was able to predict shuttlecock trajectories, navigate the service area effectively, and execute precise strikes against human players in various environments.

Conclusion: The study demonstrates the feasibility of using legged mobile manipulators in complex and dynamic sports scenarios like badminton.

Abstract: Coordinating the motion between lower and upper limbs and aligning limb
control with perception are substantial challenges in robotics, particularly in
dynamic environments. To this end, we introduce an approach for enabling legged
mobile manipulators to play badminton, a task that requires precise
coordination of perception, locomotion, and arm swinging. We propose a unified
reinforcement learning-based control policy for whole-body visuomotor skills
involving all degrees of freedom to achieve effective shuttlecock tracking and
striking. This policy is informed by a perception noise model that utilizes
real-world camera data, allowing for consistent perception error levels between
simulation and deployment and encouraging learned active perception behaviors.
Our method includes a shuttlecock prediction model, constrained reinforcement
learning for robust motion control, and integrated system identification
techniques to enhance deployment readiness. Extensive experimental results in a
variety of environments validate the robot's capability to predict shuttlecock
trajectories, navigate the service area effectively, and execute precise
strikes against human players, demonstrating the feasibility of using legged
mobile manipulators in complex and dynamic sports scenarios.

</details>


### [221] [Mobi-$π$: Mobilizing Your Robot Learning Policy](https://arxiv.org/abs/2505.23692)
*Jingyun Yang,Isabella Huang,Brandon Vu,Max Bajracharya,Rika Antonova,Jeannette Bohg*

Main category: cs.RO

TL;DR: Learned visuomotor policies struggle with generalization to novel robot positions. This paper addresses policy mobilization, finding a mobile robot base pose that aligns with a manipulation policy trained on limited viewpoints. The Mobi-π framework is introduced, featuring metrics, simulated tasks, visualization tools, and baselines. A novel approach combining 3D Gaussian Splatting, a score function, and sampling-based optimization outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing learned visuomotor policies are effective but lack generalization when faced with new robot positions or camera viewpoints, restricting their application in mobile platforms for precise tasks.

Method: The paper proposes the Mobi-π framework which includes difficulty metrics, simulated tasks, and visualization tools. Additionally, it presents an approach using 3D Gaussian Splatting for view synthesis, a score function for evaluating poses, and sampling-based optimization to find optimal robot poses.

Result: The proposed method surpasses baseline approaches in both simulation and real-world environments, proving its efficacy for policy mobilization.

Conclusion: Policy mobilization effectively decouples navigation from manipulation without requiring additional training data. The Mobi-π framework and proposed approach offer valuable tools and strategies for improving the adaptability of learned visuomotor policies.

Abstract: Learned visuomotor policies are capable of performing increasingly complex
manipulation tasks. However, most of these policies are trained on data
collected from limited robot positions and camera viewpoints. This leads to
poor generalization to novel robot positions, which limits the use of these
policies on mobile platforms, especially for precise tasks like pressing
buttons or turning faucets. In this work, we formulate the policy mobilization
problem: find a mobile robot base pose in a novel environment that is in
distribution with respect to a manipulation policy trained on a limited set of
camera viewpoints. Compared to retraining the policy itself to be more robust
to unseen robot base pose initializations, policy mobilization decouples
navigation from manipulation and thus does not require additional
demonstrations. Crucially, this problem formulation complements existing
efforts to improve manipulation policy robustness to novel viewpoints and
remains compatible with them. To study policy mobilization, we introduce the
Mobi-$\pi$ framework, which includes: (1) metrics that quantify the difficulty
of mobilizing a given policy, (2) a suite of simulated mobile manipulation
tasks based on RoboCasa to evaluate policy mobilization, (3) visualization
tools for analysis, and (4) several baseline methods. We also propose a novel
approach that bridges navigation and manipulation by optimizing the robot's
base pose to align with an in-distribution base pose for a learned policy. Our
approach utilizes 3D Gaussian Splatting for novel view synthesis, a score
function to evaluate pose suitability, and sampling-based optimization to
identify optimal robot poses. We show that our approach outperforms baselines
in both simulation and real-world environments, demonstrating its effectiveness
for policy mobilization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [222] [Distributed Federated Learning for Vehicular Network Security: Anomaly Detection Benefits and Multi-Domain Attack Threats](https://arxiv.org/abs/2505.23706)
*Utku Demir,Yalin E. Sagduyu,Tugba Erpek,Hossein Jafari,Sastry Kompella,Mengran Xue*

Main category: cs.NI

TL;DR: 在连接和自动驾驶车辆中，机器学习对于检测恶意或异常行为的安全消息分类至关重要。本文探讨了分布式联邦学习（DFL），通过车辆之间交换模型更新和多跳传播模型，克服了传统方法的限制。实验表明，与仅使用本地数据相比，DFL可以显著提高所有车辆的分类准确性，尤其是对个体准确率较低的车辆有明显提升。此外，研究还发现本地训练数据大小和时变网络连通性与模型的整体准确性密切相关。最后，文章分析了DFL在无线干扰和训练数据中毒攻击等多域攻击下的韧性和漏洞，强调了需要更强大的策略来保障DFL在车辆网络中的安全性。


<details>
  <summary>Details</summary>
Motivation: 由于互联车辆网络的大规模、高移动性和异构数据分布特点，传统的依赖集中式数据收集或纯本地训练的方法存在局限性，无法有效应对安全挑战。因此，需要一种新的方法来提高车辆网络中的分类准确性并增强系统的鲁棒性。

Method: 采用分布式联邦学习（DFL）方法，车辆通过与一跳邻居交换模型更新并多跳传播模型进行协作训练。利用Vehicular Reference Misbehavior（VeReMi）扩展数据集进行实验验证，并分析了DFL在不同攻击场景下的韧性和漏洞。

Result: 与仅使用本地数据相比，DFL显著提高了所有车辆的分类准确性，特别是对个体准确率较低的车辆有明显的提升。同时，研究发现本地训练数据大小和时变网络连通性与模型的整体准确性密切相关。在多域攻击下，DFL表现出一定的脆弱性，揭示了其在面对复杂攻击时的不足。

Conclusion: DFL能够有效提高车辆网络中的分类准确性，但需要更强大的策略来增强其在多域攻击下的安全性。

Abstract: In connected and autonomous vehicles, machine learning for safety message
classification has become critical for detecting malicious or anomalous
behavior. However, conventional approaches that rely on centralized data
collection or purely local training face limitations due to the large scale,
high mobility, and heterogeneous data distributions inherent in inter-vehicle
networks. To overcome these challenges, this paper explores Distributed
Federated Learning (DFL), whereby vehicles collaboratively train deep learning
models by exchanging model updates among one-hop neighbors and propagating
models over multiple hops. Using the Vehicular Reference Misbehavior (VeReMi)
Extension Dataset, we show that DFL can significantly improve classification
accuracy across all vehicles compared to learning strictly with local data.
Notably, vehicles with low individual accuracy see substantial accuracy gains
through DFL, illustrating the benefit of knowledge sharing across the network.
We further show that local training data size and time-varying network
connectivity correlate strongly with the model's overall accuracy. We
investigate DFL's resilience and vulnerabilities under attacks in multiple
domains, namely wireless jamming and training data poisoning attacks. Our
results reveal important insights into the vulnerabilities of DFL when
confronted with multi-domain attacks, underlining the need for more robust
strategies to secure DFL in vehicular networks.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [223] [Distribution free M-estimation](https://arxiv.org/abs/2505.22807)
*John C. Duchi*

Main category: math.ST

TL;DR: This paper explores the solvability of univariate convex M-estimation or stochastic optimization problems without making assumptions on the underlying data distribution, identifying conditions distinct from classical learnability characterizations.


<details>
  <summary>Details</summary>
Motivation: To delineate statistical problems solvable without assumptions on the underlying data distribution.

Method: Characterizing when a univariate convex M-estimation or stochastic optimization problem is solvable in an assumption-free setting.

Result: Identified conditions for solvability that are distinct from classical learnability characterizations and show that Lipschitz continuity of the loss being minimized is not necessary.

Conclusion: Provided a precise dividing line between solvable and unsolvable problems in the context of distribution-free minimization.

Abstract: The basic question of delineating those statistical problems that are
solvable without making any assumptions on the underlying data distribution has
long animated statistics and learning theory. This paper characterizes when a
(univariate) convex M-estimation or stochastic optimization problem is solvable
in such an assumption-free setting, providing a precise dividing line between
solvable and unsolvable problems. The conditions we identify show, perhaps
surprisingly, that Lipschitz continuity of the loss being minimized is not
necessary for distribution free minimization, and they are also distinct from
classical characterizations of learnability in machine learning.

</details>


### [224] [Joint estimation of smooth graph signals from partial linear measurements](https://arxiv.org/abs/2505.23240)
*Hemant Tyagi*

Main category: math.ST

TL;DR: 在给定部分线性测量值的情况下，通过平滑惩罚最小二乘估计器，在特定图结构下能够以较弱的采样条件一致地估计潜信号。结果还扩展到了多层排名问题。


<details>
  <summary>Details</summary>
Motivation: 对于与图相关联的潜信号，即使只有部分顶点的部分线性测量值可用，也希望能够在非常严格的采样条件下准确地估计这些信号。

Method: 利用平滑惩罚最小二乘估计器，并假设信号相对于图是平滑的，即信号在图上的二次变化很小。对于多层排名问题，将方法应用于潜强度向量$x_t$，并处理每个层$t$中的测量图$G_t$。

Result: 获得了非渐近均方误差界，表明在特定图结构和极弱采样条件下，估计器具有弱一致性。对于多层排名问题，即使单个$G_t$稀疏且不连通，也能实现弱一致性。

Conclusion: 平滑惩罚最小二乘估计器在潜信号恢复中表现良好，特别是在严格采样条件下。对于多层排名问题，该方法同样有效，即使测量图非常稀疏。

Abstract: Given an undirected and connected graph $G$ on $T$ vertices, suppose each
vertex $t$ has a latent signal $x_t \in \mathbb{R}^n$ associated to it. Given
partial linear measurements of the signals, for a potentially small subset of
the vertices, our goal is to estimate $x_t$'s. Assuming that the signals are
smooth w.r.t $G$, in the sense that the quadratic variation of the signals over
the graph is small, we obtain non-asymptotic bounds on the mean squared error
for jointly recovering $x_t$'s, for the smoothness penalized least squares
estimator. In particular, this implies for certain choices of $G$ that this
estimator is weakly consistent (as $T \rightarrow \infty$) under potentially
very stringent sampling, where only one coordinate is measured per vertex for a
vanishingly small fraction of the vertices. The results are extended to a
``multi-layer'' ranking problem where $x_t$ corresponds to the latent strengths
of a collection of $n$ items, and noisy pairwise difference measurements are
obtained at each ``layer'' $t$ via a measurement graph $G_t$. Weak consistency
is established for certain choices of $G$ even when the individual $G_t$'s are
very sparse and disconnected.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [225] [Quantum Hilbert Transform](https://arxiv.org/abs/2505.23581)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: The paper introduces a quantum Hilbert transform (QHT) and applies it to a quantum steganography protocol.


<details>
  <summary>Details</summary>
Motivation: There is no existing quantum analogue for the Hilbert transform, which has been fundamental in signal processing across various fields.

Method: Formulation of the quantum Hilbert transform by bridging classical phase-shift techniques with quantum operations.

Result: QHT provides new opportunities in quantum signal processing, communications, sensing, and secure information hiding.

Conclusion: Introduction of QHT expands possibilities within quantum technologies, particularly in steganography.

Abstract: The Hilbert transform has been one of the foundational transforms in signal
processing, finding it's way into multiple disciplines from cryptography to
biomedical sciences. However, there does not exist any quantum analogue for the
Hilbert transform. In this work, we introduce a formulation for the quantum
Hilbert transform (QHT)and apply it to a quantum steganography protocol. By
bridging classical phase-shift techniques with quantum operations, QHT opens
new pathways in quantum signal processing, communications, sensing, and secure
information hiding.

</details>


### [226] [Towards A Global Quantum Internet: A Review of Challenges Facing Aerial Quantum Networks](https://arxiv.org/abs/2505.23603)
*Nitin Jha,Abhishek Parakh*

Main category: quant-ph

TL;DR: 通过将地面站、空中中继和轨道卫星结合成一个无缝系统，我们更接近于实现实际的量子互联网。


<details>
  <summary>Details</summary>
Motivation: 量子网络利用量子物理原理创建安全通信网络，将其转移到空中（如使用无人机、气球或卫星）可以提高这些网络的可扩展性。

Method: 本文回顾了这种空中链路的工作原理、构建难度以及可能的解决方案。

Result: 提出了一种结合地面站、空中中继和轨道卫星的无缝系统以实现量子互联网。

Conclusion: 通过整合多种技术手段，我们向实用化的量子互联网迈进了重要一步。

Abstract: Quantum networks use principles of quantum physics to create secure
communication networks. Moving these networks off the ground using drones,
balloons, or satellites could help increase the scalability of these networks.
This article reviews how such aerial links work, what makes them difficult to
build, and the possible solutions that can be used to overcome these problems.
By combining ground stations, aerial relays, and orbiting satellites into one
seamless system, we move closer to a practical quantum internet.

</details>


### [227] [Efficient Quantum Approximate $k$NN Algorithm via Granular-Ball Computing](https://arxiv.org/abs/2505.23066)
*Shuyin Xia,Xiaojiang Tian,Suzhen Yuan,Jeremiah D. Deng*

Main category: quant-ph

TL;DR: The paper proposes an innovative algorithm called Granular-Ball based Quantum $k$NN(GB-Q$k$NN) to improve the time complexity of $k$-Nearest Neighbors algorithms by using granular-balls, HNSW method and quantization.


<details>
  <summary>Details</summary>
Motivation: High time complexity is one of the biggest challenges faced by $k$-Nearest Neighbors ($k$NN). Classical and quantum $k$NN algorithms still have a speed bottleneck when facing large amounts of data.

Method: The proposed GB-Q$k$NN algorithm uses granular-balls to reduce the data size needed to be processed. It then employs the Hierarchical Navigable Small World (HNSW) method to accelerate the search process and optimizes time-consuming steps such as distance calculation via quantization.

Result: This approach significantly reduces the time complexity of $k$NN-like algorithms as revealed by comprehensive complexity analysis.

Conclusion: By combining granular-balls and quantization of the HNSW method, the GB-Q$k$NN manages to take advantage of these treatments and significantly reduces the time complexity.

Abstract: High time complexity is one of the biggest challenges faced by $k$-Nearest
Neighbors ($k$NN). Although current classical and quantum $k$NN algorithms have
made some improvements, they still have a speed bottleneck when facing large
amounts of data. To address this issue, we propose an innovative algorithm
called Granular-Ball based Quantum $k$NN(GB-Q$k$NN). This approach achieves
higher efficiency by first employing granular-balls, which reduces the data
size needed to processed. The search process is then accelerated by adopting a
Hierarchical Navigable Small World (HNSW) method. Moreover, we optimize the
time-consuming steps, such as distance calculation, of the HNSW via
quantization, further reducing the time complexity of the construct and search
process. By combining the use of granular-balls and quantization of the HNSW
method, our approach manages to take advantage of these treatments and
significantly reduces the time complexity of the $k$NN-like algorithms, as
revealed by a comprehensive complexity analysis.

</details>


### [228] [Information-Computation Gaps in Quantum Learning via Low-Degree Likelihood](https://arxiv.org/abs/2505.22743)
*Sitan Chen,Weiyuan Gong,Jonas Haferkamp,Yihui Quek*

Main category: quant-ph

TL;DR: The paper extends the classical low-degree method to quantum settings, establishing connections between state designs and low-degree hardness, proving information-computation gaps for learning Gibbs states and random shallow quantum circuit states, and defining a new quantum generalization of the planted biclique problem.


<details>
  <summary>Details</summary>
Motivation: There is an information-computation gap in extracting information from quantum data, and there are limited tools in the quantum literature for giving evidence for average-case hardness of statistical inference problems.

Method: The authors establish a general connection between state designs and low-degree hardness, use it to obtain information-computation gaps for learning Gibbs states and random shallow quantum circuit states, define a new quantum generalization of the planted biclique problem, and show average-case hardness for Learning Stabilizers with Noise and agnostically learning product states.

Result: The framework was successfully extended to the quantum setting, providing new insights into the computational hardness of various quantum learning problems and demonstrating information-computation gaps.

Conclusion: This work broadens the understanding of computational hardness in quantum learning problems by extending the low-degree method to quantum settings.

Abstract: In a variety of physically relevant settings for learning from quantum data,
designing protocols that can computationally efficiently extract information
remains largely an art, and there are important cases where we believe this to
be impossible, that is, where there is an information-computation gap. While
there is a large array of tools in the classical literature for giving evidence
for average-case hardness of statistical inference problems, the corresponding
tools in the quantum literature are far more limited. One such framework in the
classical literature, the low-degree method, makes predictions about hardness
of inference problems based on the failure of estimators given by low-degree
polynomials. In this work, we extend this framework to the quantum setting.
  We establish a general connection between state designs and low-degree
hardness. We use this to obtain the first information-computation gaps for
learning Gibbs states of random, sparse, non-local Hamiltonians. We also use it
to prove hardness for learning random shallow quantum circuit states in a
challenging model where states can be measured in adaptively chosen bases. To
our knowledge, the ability to model adaptivity within the low-degree framework
was open even in classical settings. In addition, we also obtain a low-degree
hardness result for quantum error mitigation against strategies with
single-qubit measurements.
  We define a new quantum generalization of the planted biclique problem and
identify the threshold at which this problem becomes computationally hard for
protocols that perform local measurements. Interestingly, the complexity
landscape for this problem shifts when going from local measurements to more
entangled single-copy measurements.
  We show average-case hardness for the "standard" variant of Learning
Stabilizers with Noise and for agnostically learning product states.

</details>


### [229] [Dynamic Estimation Loss Control in Variational Quantum Sensing via Online Conformal Inference](https://arxiv.org/abs/2505.23389)
*Ivana Nikoloska,Hamdi Joudeh,Ruud van Sloun,Osvaldo Simeone*

Main category: quant-ph

TL;DR: The paper proposes an online control framework for variational quantum sensing (VQS) that dynamically updates parameters and provides error bars, demonstrating reliable and precise quantum sensing on NISQ devices.


<details>
  <summary>Details</summary>
Motivation: To overcome the noise and sampling constraints of practical quantum sensors on NISQ devices and address the lack of rigorous performance guarantees in current VQS methods.

Method: An online control framework for VQS is proposed. It dynamically updates variational parameters and uses online conformal inference techniques to provide deterministic error bars with a guaranteed long-term risk level.

Result: Experiments on a quantum magnetometry task show that the dynamic VQS approach maintains reliability over time while providing precise estimates.

Conclusion: Combining variational quantum algorithms with online conformal inference enables reliable quantum sensing on NISQ devices.

Abstract: Quantum sensing exploits non-classical effects to overcome limitations of
classical sensors, with applications ranging from gravitational-wave detection
to nanoscale imaging. However, practical quantum sensors built on noisy
intermediate-scale quantum (NISQ) devices face significant noise and sampling
constraints, and current variational quantum sensing (VQS) methods lack
rigorous performance guarantees. This paper proposes an online control
framework for VQS that dynamically updates the variational parameters while
providing deterministic error bars on the estimates. By leveraging online
conformal inference techniques, the approach produces sequential estimation
sets with a guaranteed long-term risk level. Experiments on a quantum
magnetometry task confirm that the proposed dynamic VQS approach maintains the
required reliability over time, while still yielding precise estimates. The
results demonstrate the practical benefits of combining variational quantum
algorithms with online conformal inference to achieve reliable quantum sensing
on NISQ devices.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [230] [Trajectory Generator Matching for Time Series](https://arxiv.org/abs/2505.23215)
*T. Jahn,J. Chemseddine,P. Hagemann,C. Wald,G. Steidl*

Main category: math.NA

TL;DR: 这篇论文通过生成模型的思想，提出了新的SDEs和跳跃过程生成器，能够处理不连续性和非规则采样的时间序列数据。


<details>
  <summary>Details</summary>
Motivation: 目前对于从不规则观测中准确建模时间连续随机过程仍是一个重要挑战。

Method: 受图像数据生成建模的启发，提出新的SDEs和跳跃过程生成器，这些生成器具有目标时间序列的边缘分布，并通过缩放高斯参数化跳跃核密度以允许在损失中使用相对应的Kullback-Leibler散度闭合形式公式。

Result: 该方法可以处理底层过程的不连续性，并且与大多数其他方法不同，能够处理非规则采样的时间序列。

Conclusion: 此方法为时间序列生成提供了新思路，特别是在处理不规则观测和不连续性方面有显著改进。

Abstract: Accurately modeling time-continuous stochastic processes from irregular
observations remains a significant challenge. In this paper, we leverage ideas
from generative modeling of image data to push the boundary of time series
generation. For this, we find new generators of SDEs and jump processes,
inspired by trajectory flow matching, that have the marginal distributions of
the time series of interest. Specifically, we can handle discontinuities of the
underlying processes by parameterizing the jump kernel densities by scaled
Gaussians that allow for closed form formulas of the corresponding
Kullback-Leibler divergence in the loss. Unlike most other approaches, we are
able to handle irregularly sampled time series.

</details>


### [231] [Optimization-Free Diffusion Model -- A Perturbation Theory Approach](https://arxiv.org/abs/2505.23652)
*Yuehaw Khoo,Mathias Oster,Yifan Peng*

Main category: math.NA

TL;DR: 提出了一种无需优化和前向SDE的扩散模型方法，通过在与扩散过程相关联的后向Kolmogorov算子的稀疏特征基上展开得分函数，将得分估计重新表述为线性系统的解。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型通常依赖于优化神经网络以通过前向SDE模拟来估计得分函数，这可能需要大量的计算资源和时间。因此，探索一种无需优化和前向SDE的方法具有重要意义。

Method: 通过在与扩散过程相关联的后向Kolmogorov算子的稀疏特征基上展开得分函数，将得分估计重新表述为线性系统的解，从而避免了迭代优化和时间依赖的样本生成。

Result: 该方法在高维Boltzmann分布和真实世界数据集上展示了有效性，并且使用摄动理论分析了近似误差。

Conclusion: 提出的方法提供了一种无需优化和前向SDE的替代方案，可以有效地应用于高维数据和实际问题中。

Abstract: Diffusion models have emerged as a powerful framework in generative modeling,
typically relying on optimizing neural networks to estimate the score function
via forward SDE simulations. In this work, we propose an alternative method
that is both optimization-free and forward SDE-free. By expanding the score
function in a sparse set of eigenbasis of the backward Kolmogorov operator
associated with the diffusion process, we reformulate score estimation as the
solution to a linear system, avoiding iterative optimization and time-dependent
sample generation. We analyze the approximation error using perturbation theory
and demonstrate the effectiveness of our method on high-dimensional Boltzmann
distributions and real-world datasets.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [232] [PSBench: a large-scale benchmark for estimating the accuracy of protein complex structural models](https://arxiv.org/abs/2505.22674)
*Pawan Neupane,Jian Liu,Jianlin Cheng*

Main category: q-bio.BM

TL;DR: PSBench是一个包含大规模标注数据集的基准套件，用于评估蛋白质复合物结构预测模型的质量。通过训练和测试GATE方法，证明了PSBench在EMA研究中的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管AI方法如AlphaFold可以准确预测许多蛋白质复合物的结构模型，但估计这些预测模型的质量（EMA）仍然是一个主要挑战。开发有效的机器学习EMA方法的关键障碍是缺乏大型、多样且注释良好的数据集。

Method: 引入了PSBench基准套件，包括四个大规模标注数据集，涵盖广泛的蛋白质序列长度、复杂化学计量、功能类别和建模难度。每个模型都有多个补充质量评分，并提供了多种评估指标和基线EMA方法。

Result: 在CASP15数据上训练并评估了基于图变换器的EMA方法GATE，在CASP16中盲测时表现优异。

Conclusion: PSBench为推进蛋白质复合物建模中的EMA研究提供了一个有价值的资源。

Abstract: Predicting protein complex structures is essential for protein function
analysis, protein design, and drug discovery. While AI methods like AlphaFold
can predict accurate structural models for many protein complexes, reliably
estimating the quality of these predicted models (estimation of model accuracy,
or EMA) for model ranking and selection remains a major challenge. A key
barrier to developing effective machine learning-based EMA methods is the lack
of large, diverse, and well-annotated datasets for training and evaluation. To
address this gap, we introduce PSBench, a benchmark suite comprising four
large-scale, labeled datasets generated during the 15th and 16th community-wide
Critical Assessment of Protein Structure Prediction (CASP15 and CASP16).
PSBench includes over one million structural models covering a wide range of
protein sequence lengths, complex stoichiometries, functional classes, and
modeling difficulties. Each model is annotated with multiple complementary
quality scores at the global, local, and interface levels. PSBench also
provides multiple evaluation metrics and baseline EMA methods to facilitate
rigorous comparisons. To demonstrate PSBench's utility, we trained and
evaluated GATE, a graph transformer-based EMA method, on the CASP15 data. GATE
was blindly tested in CASP16 (2024), where it ranked among the top-performing
EMA methods. These results highlight PSBench as a valuable resource for
advancing EMA research in protein complex modeling. PSBench is publicly
available at: https://github.com/BioinfoMachineLearning/PSBench.

</details>


### [233] [Representing local protein environments with atomistic foundation models](https://arxiv.org/abs/2505.23354)
*Meital Bojan,Sanketh Vedula,Advaith Maddipatla,Nadav Bojan Sellam,Federico Napoli,Paul Schanda,Alex M. Bronstein*

Main category: q-bio.BM

TL;DR: This paper proposes a new representation for local protein environments derived from atomistic foundation models (AFMs), which captures both local structure and chemical features, exhibits meaningful structure in its representation space, and enables state-of-the-art accuracy in predicting chemical shifts in biomolecular NMR spectroscopy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to address the challenge of representing the extensive structural and chemical variability in local protein environments, which is essential for modeling and designing proteins and their interactions but remains under-explored.

Method: The method involves deriving a novel representation for local protein environments from the intermediate features of AFMs. This embedding captures local structure and chemical features, showing meaningful structure in its representation space.

Result: The results show that the proposed representation enables the construction of data-driven priors over the distribution of biomolecular environments and achieves state-of-the-art accuracy in physics-informed chemical shift prediction for biomolecular NMR spectroscopy.

Conclusion: The conclusion is that AFMs and their emergent representations are surprisingly effective for protein modeling beyond traditional molecular simulations, opening new possibilities for constructing functional representations of protein environments.

Abstract: The local structure of a protein strongly impacts its function and
interactions with other molecules. Therefore, a concise, informative
representation of a local protein environment is essential for modeling and
designing proteins and biomolecular interactions. However, these environments'
extensive structural and chemical variability makes them challenging to model,
and such representations remain under-explored. In this work, we propose a
novel representation for a local protein environment derived from the
intermediate features of atomistic foundation models (AFMs). We demonstrate
that this embedding effectively captures both local structure (e.g., secondary
motifs), and chemical features (e.g., amino-acid identity and protonation
state). We further show that the AFM-derived representation space exhibits
meaningful structure, enabling the construction of data-driven priors over the
distribution of biomolecular environments. Finally, in the context of
biomolecular NMR spectroscopy, we demonstrate that the proposed representations
enable a first-of-its-kind physics-informed chemical shift predictor that
achieves state-of-the-art accuracy. Our results demonstrate the surprising
effectiveness of atomistic foundation models and their emergent representations
for protein modeling beyond traditional molecular simulations. We believe this
will open new lines of work in constructing effective functional
representations for protein environments.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [234] [Training Language Models to Generate Quality Code with Program Analysis Feedback](https://arxiv.org/abs/2505.22704)
*Feng Yao,Zilong Wang,Liyuan Liu,Junxia Cui,Li Zhong,Xiaohan Fu,Haohui Mai,Vish Krishnan,Jianfeng Gao,Jingbo Shang*

Main category: cs.CL

TL;DR: This paper presents REAL, a reinforcement learning framework that encourages LLMs to produce high-quality code by providing program analysis-guided feedback. It integrates automated signals for defect detection and functional correctness, operates without prompts or references, and surpasses existing methods in functionality and code quality.


<details>
  <summary>Details</summary>
Motivation: Code generation using LLMs is becoming more common but often lacks sufficient quality in terms of security and maintainability. Current improvement methods are either too labor-intensive or not robust enough.

Method: REAL uses reinforcement learning with two key automated signals - program analysis for detecting defects related to security and maintainability, and unit tests for ensuring functional correctness. This approach does not depend on specific prompts or reference codes, allowing for scalable supervision without manual input.

Result: Experiments across various datasets and model sizes show that REAL performs better than current state-of-the-art methods when considering both functionality and code quality simultaneously.

Conclusion: REAL successfully bridges the gap between quick prototyping and production-ready code, allowing LLMs to provide both speed and quality.

Abstract: Code generation with large language models (LLMs), often termed vibe coding,
is increasingly adopted in production but fails to ensure code quality,
particularly in security (e.g., SQL injection vulnerabilities) and
maintainability (e.g., missing type annotations). Existing methods, such as
supervised fine-tuning and rule-based post-processing, rely on labor-intensive
annotations or brittle heuristics, limiting their scalability and
effectiveness. We propose REAL, a reinforcement learning framework that
incentivizes LLMs to generate production-quality code using program
analysis-guided feedback. Specifically, REAL integrates two automated signals:
(1) program analysis detecting security or maintainability defects and (2) unit
tests ensuring functional correctness. Unlike prior work, our framework is
prompt-agnostic and reference-free, enabling scalable supervision without
manual intervention. Experiments across multiple datasets and model scales
demonstrate that REAL outperforms state-of-the-art methods in simultaneous
assessments of functionality and code quality. Our work bridges the gap between
rapid prototyping and production-ready code, enabling LLMs to deliver both
speed and quality.

</details>


### [235] [Pre-Training Curriculum for Multi-Token Prediction in Language Models](https://arxiv.org/abs/2505.22757)
*Ansar Aynetdinov,Alan Akbik*

Main category: cs.CL

TL;DR: Multi-token prediction (MTP) is a new pre-training objective for language models. However, smaller language models (SLMs) have difficulty with MTP. This paper proposes a curriculum learning strategy for MTP training, including forward and reverse curricula. The forward curriculum helps SLMs leverage MTP, improving downstream NTP performance and generative output quality while retaining self-speculative decoding benefits. The reverse curriculum enhances NTP performance and output quality but lacks self-speculative decoding benefits.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the challenge faced by smaller language models (SLMs) in handling the MTP objective, despite its proven advantages for larger models in terms of downstream performance, inference speed, and training efficiency.

Method: The authors introduce two variants of a curriculum learning strategy for MTP training: a forward curriculum that progressively increases the complexity of the pre-training objective from NTP to MTP, and a reverse curriculum that decreases the complexity from MTP to NTP.

Result: Experiments indicate that the forward curriculum allows SLMs to better utilize the MTP objective during pre-training, leading to improvements in downstream NTP performance and generative output quality while preserving the benefits of self-speculative decoding. On the other hand, the reverse curriculum delivers superior NTP performance and output quality without offering any self-speculative decoding advantages.

Conclusion: Curriculum learning strategies, particularly the forward curriculum, can significantly enhance the ability of SLMs to benefit from the MTP objective, thereby improving their overall performance and maintaining specific decoding advantages.

Abstract: Multi-token prediction (MTP) is a recently proposed pre-training objective
for language models. Rather than predicting only the next token (NTP), MTP
predicts the next $k$ tokens at each prediction step, using multiple prediction
heads. MTP has shown promise in improving downstream performance, inference
speed, and training efficiency, particularly for large models. However, prior
work has shown that smaller language models (SLMs) struggle with the MTP
objective. To address this, we propose a curriculum learning strategy for MTP
training, exploring two variants: a forward curriculum, which gradually
increases the complexity of the pre-training objective from NTP to MTP, and a
reverse curriculum, which does the opposite. Our experiments show that the
forward curriculum enables SLMs to better leverage the MTP objective during
pre-training, improving downstream NTP performance and generative output
quality, while retaining the benefits of self-speculative decoding. The reverse
curriculum achieves stronger NTP performance and output quality, but fails to
provide any self-speculative decoding benefits.

</details>


### [236] [FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian](https://arxiv.org/abs/2505.22759)
*Sara Papi,Marco Gaido,Luisa Bentivogli,Alessio Brutti,Mauro Cettolo,Roberto Gretter,Marco Matassoni,Mohamed Nabih,Matteo Negri*

Main category: cs.CL

TL;DR: The paper introduces FAMA, the first family of open science speech foundation models for English and Italian, trained on extensive open-source data, achieving competitive performance and greater speed compared to existing models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of reproducibility and fair evaluation in speech processing due to the closed nature of current speech foundation models (SFMs), and to promote openness in speech technology research similar to other domains.

Method: Development of FAMA, a family of open science SFMs for English and Italian, trained on 150k+ hours of open-source speech data. Also, creation of a new dataset with 16k hours of cleaned and pseudo-labeled speech for both languages.

Result: FAMA achieves competitive performance compared to existing SFMs while being significantly faster (up to 8 times).

Conclusion: All artifacts including code, datasets, and models are released under open-source compliant licenses, promoting openness in speech technology research.

Abstract: The development of speech foundation models (SFMs) like Whisper and
SeamlessM4T has significantly advanced the field of speech processing. However,
their closed nature--with inaccessible training data and code--poses major
reproducibility and fair evaluation challenges. While other domains have made
substantial progress toward open science by developing fully transparent models
trained on open-source (OS) code and data, similar efforts in speech remain
limited. To fill this gap, we introduce FAMA, the first family of open science
SFMs for English and Italian, trained on 150k+ hours of OS speech data.
Moreover, we present a new dataset containing 16k hours of cleaned and
pseudo-labeled speech for both languages. Results show that FAMA achieves
competitive performance compared to existing SFMs while being up to 8 times
faster. All artifacts, including code, datasets, and models, are released under
OS-compliant licenses, promoting openness in speech technology research.

</details>


### [237] [Automated Essay Scoring Incorporating Annotations from Automated Feedback Systems](https://arxiv.org/abs/2505.22771)
*Christopher Ormerod*

Main category: cs.CL

TL;DR: 通过将反馈导向的注释整合到评分流程中，可以提高自动作文评分（AES）的准确性。此方法在PERSUADE语料库上进行了演示，使用两种大型语言模型生成注释，一种用于拼写校正，另一种用于标记论证元素。结果表明，在评分过程中加入注释可以提高性能。


<details>
  <summary>Details</summary>
Motivation: 自动作文评分系统需要更高的准确性，特别是在提供具体和有针对性的反馈方面。因此，研究如何通过引入反馈导向的注释来改进自动评分系统的性能成为必要。

Method: 将两类反馈驱动的注释整合到评分流程中：一是识别拼写和语法错误的注释；二是突出论证成分的注释。利用两个大型语言模型生成这些注释，一个是生成式语言模型，用于拼写校正；另一个是基于编码器的令牌分类器，训练用于识别和标记论证元素。然后使用微调后的编码器型大语言模型作为分类器进行评分。

Result: 通过在评分过程中加入注释，证明了这种方法可以提升评分性能，特别是在识别拼写、语法错误以及论证成分方面。

Conclusion: 将反馈导向的注释融入自动作文评分流程能够有效提升评分准确性，并且这种方法在实际应用中具有可行性。

Abstract: This study illustrates how incorporating feedback-oriented annotations into
the scoring pipeline can enhance the accuracy of automated essay scoring (AES).
This approach is demonstrated with the Persuasive Essays for Rating, Selecting,
and Understanding Argumentative and Discourse Elements (PERSUADE) corpus. We
integrate two types of feedback-driven annotations: those that identify
spelling and grammatical errors, and those that highlight argumentative
components. To illustrate how this method could be applied in real-world
scenarios, we employ two LLMs to generate annotations -- a generative language
model used for spell-correction and an encoder-based token classifier trained
to identify and mark argumentative elements. By incorporating annotations into
the scoring process, we demonstrate improvements in performance using
encoder-based large language models fine-tuned as classifiers.

</details>


### [238] [First Steps Towards Overhearing LLM Agents: A Case Study With Dungeons & Dragons Gameplay](https://arxiv.org/abs/2505.22809)
*Andrew Zhu,Evan Osgood,Chris Callison-Burch*

Main category: cs.CL

TL;DR: 研究人员探索了一种名为'overhearing agents'的新范式，这些代理通过监听人类对话来执行后台任务或提供建议，而非直接参与对话。研究以Dungeons & Dragons游戏为背景，使用大型多模态音頻-语言模型辅助 Dungeon Master，并发现某些大型音頻-语言模型具备通过隐式音頻线索执行 overhearing agent 任务的新兴能力。


<details>
  <summary>Details</summary>
Motivation: 当前许多工作集中在直接帮助人类用户完成任务的对话型 LLM 代理上，而本研究旨在探讨一种替代范式：overhearing agents，即不主动参与对话，而是通过监听人类对话来执行辅助任务。

Method: 研究以 Dungeons & Dragons 游戏为背景，利用大型多模态音頻-语言模型作为 overhearing agents 来辅助 Dungeon Master。通过人机评估分析这些代理在实际应用中的辅助效果。

Result: 研究表明，某些大型音頻-语言模型具备通过隐式音訊线索执行 overhearing agent 任务的能力，并且在辅助 Dungeon Master 方面显示出一定的有效性。

Conclusion: Overhearing agents 范式展示了其在特定场景下的潜力，特别是在不需要直接对话交互的情况下。研究还开源了 Python 库和项目代码，以支持进一步的研究。

Abstract: Much work has been done on conversational LLM agents which directly assist
human users with tasks. We present an alternative paradigm for interacting with
LLM agents, which we call "overhearing agents". These overhearing agents do not
actively participate in conversation -- instead, they "listen in" on
human-to-human conversations and perform background tasks or provide
suggestions to assist the user. In this work, we explore the overhearing agents
paradigm through the lens of Dungeons & Dragons gameplay. We present an
in-depth study using large multimodal audio-language models as overhearing
agents to assist a Dungeon Master. We perform a human evaluation to examine the
helpfulness of such agents and find that some large audio-language models have
the emergent ability to perform overhearing agent tasks using implicit audio
cues. Finally, we release Python libraries and our project code to support
further research into the overhearing agents paradigm at
https://github.com/zhudotexe/overhearing_agents.

</details>


### [239] [What Has Been Lost with Synthetic Evaluation?](https://arxiv.org/abs/2505.22830)
*Alexander Gill,Abhilasha Ravichander,Ana Marasović*

Main category: cs.CL

TL;DR: Large language models (LLMs) can generate reasoning over-text benchmarks at a lower cost, but these benchmarks are less challenging than human-authored ones.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs can create valid and challenging reasoning over-text benchmarks by comparing them to carefully crowdsourced benchmarks.

Method: Evaluate LLM-generated versions of two reading comprehension datasets (CondaQA and DROP) in terms of validity and difficulty, and compare them to the human-created versions.

Result: LLM-generated benchmarks are often valid according to annotation guidelines and cost less to produce, but they are less challenging for LLMs compared to human-authored benchmarks.

Conclusion: Using LLMs to generate evaluation data may lose some challenge aspects, prompting a need to reassess this approach to benchmark creation.

Abstract: Large language models (LLMs) are increasingly used for data generation.
However, creating evaluation benchmarks raises the bar for this emerging
paradigm. Benchmarks must target specific phenomena, penalize exploiting
shortcuts, and be challenging. Through two case studies, we investigate whether
LLMs can meet these demands by generating reasoning over-text benchmarks and
comparing them to those created through careful crowdsourcing. Specifically, we
evaluate both the validity and difficulty of LLM-generated versions of two
high-quality reading comprehension datasets: CondaQA, which evaluates reasoning
about negation, and DROP, which targets reasoning about quantities. We find
that prompting LLMs can produce variants of these datasets that are often valid
according to the annotation guidelines, at a fraction of the cost of the
original crowdsourcing effort. However, we show that they are less challenging
for LLMs than their human-authored counterparts. This finding sheds light on
what may have been lost by generating evaluation data with LLMs, and calls for
critically reassessing the immediate use of this increasingly prevalent
approach to benchmark creation.

</details>


### [240] [Unraveling LoRA Interference: Orthogonal Subspaces for Robust Model Merging](https://arxiv.org/abs/2505.22934)
*Haobo Zhang,Jiayu Zhou*

Main category: cs.CL

TL;DR: Low-rank adaptation (LoRA) fine-tuned models often suffer significant performance degradation when merged. This paper identifies the issue as an interplay between model parameters and data distributions, proposing Orthogonal Subspaces for Robust model Merging (OSRM). OSRM constrains LoRA subspaces before fine-tuning to ensure task-specific updates do not negatively affect other tasks, integrating with existing merging algorithms. Experiments show OSRM improves merging performance, preserves single-task accuracy, and exhibits robustness to merging hyperparameters.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning large language models for individual tasks yields strong performance but is expensive for deployment and storage. Model merging can combine multiple task-specific models into a single multi-task model without additional training, but existing methods fail for LoRA fine-tuned models due to performance degradation.

Method: The paper proposes OSRM, which constrains the LoRA subspace prior to fine-tuning. This ensures that updates relevant to one task do not adversely shift outputs for others, reducing unintended interference among tasks and seamlessly integrating with most existing merging algorithms.

Result: Extensive experiments on eight datasets with three widely used LMs and two large LMs demonstrate that OSRM boosts merging performance while preserving single-task accuracy. The approach also shows greater robustness to merging hyperparameters.

Conclusion: The results highlight the importance of data-parameter interaction in model merging and offer a plug-and-play solution for merging LoRA models.

Abstract: Fine-tuning large language models (LMs) for individual tasks yields strong
performance but is expensive for deployment and storage. Recent works explore
model merging to combine multiple task-specific models into a single multi-task
model without additional training. However, existing merging methods often fail
for models fine-tuned with low-rank adaptation (LoRA), due to significant
performance degradation. In this paper, we show that this issue arises from a
previously overlooked interplay between model parameters and data
distributions. We propose Orthogonal Subspaces for Robust model Merging (OSRM)
to constrain the LoRA subspace *prior* to fine-tuning, ensuring that updates
relevant to one task do not adversely shift outputs for others. Our approach
can seamlessly integrate with most existing merging algorithms, reducing the
unintended interference among tasks. Extensive experiments on eight datasets,
tested with three widely used LMs and two large LMs, demonstrate that our
method not only boosts merging performance but also preserves single-task
accuracy. Furthermore, our approach exhibits greater robustness to the
hyperparameters of merging. These results highlight the importance of
data-parameter interaction in model merging and offer a plug-and-play solution
for merging LoRA models.

</details>


### [241] [WorkForceAgent-R1: Incentivizing Reasoning Capability in LLM-based Web Agents via Reinforcement Learning](https://arxiv.org/abs/2505.22942)
*Yuchen Zhuang,Di Jin,Jiaao Chen,Wenqi Shi,Hanrui Wang,Chao Zhang*

Main category: cs.CL

TL;DR: WorkForceAgent-R1 is a LLM-based web agent trained using a rule-based R1-style reinforcement learning framework that enhances single-step reasoning and planning for business-oriented web navigation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing web agents relying on supervised fine-tuning (SFT) often struggle with generalization and robustness due to insufficient reasoning capabilities when handling the inherently dynamic nature of web interactions.

Method: The WorkForceAgent-R1 is trained using a rule-based R1-style reinforcement learning framework designed explicitly to enhance single-step reasoning and planning for business-oriented web navigation tasks. A structured reward function is employed that evaluates both adherence to output formats and correctness of actions, enabling the agent to implicitly learn robust intermediate reasoning without explicit annotations or extensive expert demonstrations.

Result: Extensive experiments on the WorkArena benchmark demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by 10.26-16.59%, achieving competitive performance relative to proprietary LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

Conclusion: WorkForceAgent-R1, an LLM-based web agent trained using a rule-based R1-style reinforcement learning framework, achieves significant improvements in generalization and robustness for business-oriented web navigation tasks.

Abstract: Large language models (LLMs)-empowered web agents enables automating complex,
real-time web navigation tasks in enterprise environments. However, existing
web agents relying on supervised fine-tuning (SFT) often struggle with
generalization and robustness due to insufficient reasoning capabilities when
handling the inherently dynamic nature of web interactions. In this study, we
introduce WorkForceAgent-R1, an LLM-based web agent trained using a rule-based
R1-style reinforcement learning framework designed explicitly to enhance
single-step reasoning and planning for business-oriented web navigation tasks.
We employ a structured reward function that evaluates both adherence to output
formats and correctness of actions, enabling WorkForceAgent-R1 to implicitly
learn robust intermediate reasoning without explicit annotations or extensive
expert demonstrations. Extensive experiments on the WorkArena benchmark
demonstrate that WorkForceAgent-R1 substantially outperforms SFT baselines by
10.26-16.59%, achieving competitive performance relative to proprietary
LLM-based agents (gpt-4o) in workplace-oriented web navigation tasks.

</details>


### [242] [Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representation via Text Updates](https://arxiv.org/abs/2505.22943)
*Jaewoo Ahn,Heeseung Yun,Dayoon Ko,Gunhee Kim*

Main category: cs.CL

TL;DR: A new benchmark MAC is introduced to expose vulnerabilities in pre-trained multimodal representations by generating deceptive text samples through LLMs. A self-training approach with rejection-sampling fine-tuning improves zero-shot methods' performance in revealing these vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Pre-trained multimodal representations show impressive capabilities but have significant compositional vulnerabilities that lead to counterintuitive judgments.

Method: Introduced MAC benchmark which uses LLMs to generate deceptive text samples and evaluates them via attack success rate and diversity. Proposed a self-training approach with rejection-sampling fine-tuning to enhance both attack success rate and sample diversity.

Result: Using smaller language models like Llama-3.1-8B, the self-training approach demonstrates superior performance in revealing compositional vulnerabilities across various multimodal representations including images, videos, and audios.

Conclusion: MAC benchmark effectively exposes vulnerabilities in multimodal representations and the proposed self-training approach enhances the effectiveness of zero-shot methods.

Abstract: While pre-trained multimodal representations (e.g., CLIP) have shown
impressive capabilities, they exhibit significant compositional vulnerabilities
leading to counterintuitive judgments. We introduce Multimodal Adversarial
Compositionality (MAC), a benchmark that leverages large language models (LLMs)
to generate deceptive text samples to exploit these vulnerabilities across
different modalities and evaluates them through both sample-wise attack success
rate and group-wise entropy-based diversity. To improve zero-shot methods, we
propose a self-training approach that leverages rejection-sampling fine-tuning
with diversity-promoting filtering, which enhances both attack success rate and
sample diversity. Using smaller language models like Llama-3.1-8B, our approach
demonstrates superior performance in revealing compositional vulnerabilities
across various multimodal representations, including images, videos, and
audios.

</details>


### [243] [OWL: Probing Cross-Lingual Recall of Memorized Texts via World Literature](https://arxiv.org/abs/2505.22945)
*Alisha Srivastava,Emir Korukluoglu,Minh Nhat Le,Duyen Tran,Chau Minh Pham,Marzena Karpinska,Mohit Iyyer*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）能够跨语言记忆和回忆文本，即使在预训练数据中没有直接翻译的文本。例如，GPT-4o可以在新翻译的摘录中69%的时间识别作者和标题，6%的时间预测被掩盖的实体。扰动（如掩盖字符、打乱单词）适度降低直接探测准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管已知LLMs可以记忆和回忆英语文本，但其能力是否能推广到非英语语言或在语言之间转移尚不清楚。因此，本文旨在研究LLMs在多语言和跨语言环境中的记忆能力。

Method: 引入OWL数据集，包含20本书的31.5K对齐摘录，涵盖十种语言。通过三个任务评估不同模型家族和大小的记忆能力：(1)直接探测，要求模型识别书名和作者；(2)名称填空，要求预测被掩盖的角色名字；(3)前缀探测，涉及生成延续。

Result: LLMs能够在不同语言间一致地回忆内容，包括在预训练数据中没有直接翻译的文本。例如，GPT-4o在新翻译的摘录中69%的时间识别作者和标题，6%的时间预测被掩盖的实体。扰动（如掩盖角色、打乱单词）适度降低直接探测准确性（官方翻译被打乱时下降7%）。

Conclusion: 结果强调了LLMs跨语言记忆的程度，并提供了关于不同模型之间差异的见解。

Abstract: Large language models (LLMs) are known to memorize and recall English text
from their pretraining data. However, the extent to which this ability
generalizes to non-English languages or transfers across languages remains
unclear. This paper investigates multilingual and cross-lingual memorization in
LLMs, probing if memorized content in one language (e.g., English) can be
recalled when presented in translation. To do so, we introduce OWL, a dataset
of 31.5K aligned excerpts from 20 books in ten languages, including English
originals, official translations (Vietnamese, Spanish, Turkish), and new
translations in six low-resource languages (Sesotho, Yoruba, Maithili,
Malagasy, Setswana, Tahitian). We evaluate memorization across model families
and sizes through three tasks: (1) direct probing, which asks the model to
identify a book's title and author; (2) name cloze, which requires predicting
masked character names; and (3) prefix probing, which involves generating
continuations. We find that LLMs consistently recall content across languages,
even for texts without direct translation in pretraining data. GPT-4o, for
example, identifies authors and titles 69% of the time and masked entities 6%
of the time in newly translated excerpts. Perturbations (e.g., masking
characters, shuffling words) modestly reduce direct probing accuracy (7% drop
for shuffled official translations). Our results highlight the extent of
cross-lingual memorization and provide insights on the differences between the
models.

</details>


### [244] [NegVQA: Can Vision Language Models Understand Negation?](https://arxiv.org/abs/2505.22946)
*Yuhui Zhang,Yuchang Su,Yiming Liu,Serena Yeung-Levy*

Main category: cs.CL

TL;DR: 研究人员创建了一个名为NegVQA的基准，包含7,379个选择题，用于评估视觉语言模型对否定的理解能力。通过分析20个最先进模型的表现，发现这些模型在处理否定时存在显著困难，并揭示了模型规模与性能之间的U型关系。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）在高风险应用中越来越重要，但它们对否定这种基本语言现象的理解能力尚未得到充分评估。

Method: 构建了一个新的基准测试NegVQA，其中包括从现有视觉问答数据集中生成的被否定版本的问题。使用大型语言模型来生成这些问题，并对20种最先进的视觉语言模型进行评估。

Result: 大多数先进的视觉语言模型在处理否定问题时表现不佳，相较于原始问题的回答，性能有明显下降。此外，还发现了模型大小和NegVQA性能之间的U型关系。

Conclusion: NegVQA基准暴露了当前视觉语言模型在理解否定方面的关键缺陷，并为未来模型的发展提供了指导。

Abstract: Negation is a fundamental linguistic phenomenon that can entirely reverse the
meaning of a sentence. As vision language models (VLMs) continue to advance and
are deployed in high-stakes applications, assessing their ability to comprehend
negation becomes essential. To address this, we introduce NegVQA, a visual
question answering (VQA) benchmark consisting of 7,379 two-choice questions
covering diverse negation scenarios and image-question distributions. We
construct NegVQA by leveraging large language models to generate negated
versions of questions from existing VQA datasets. Evaluating 20
state-of-the-art VLMs across seven model families, we find that these models
struggle significantly with negation, exhibiting a substantial performance drop
compared to their responses to the original questions. Furthermore, we uncover
a U-shaped scaling trend, where increasing model size initially degrades
performance on NegVQA before leading to improvements. Our benchmark reveals
critical gaps in VLMs' negation understanding and offers insights into future
VLM development. Project page available at
https://yuhui-zh15.github.io/NegVQA/.

</details>


### [245] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)
*Sheng Zhang,Qin Liu,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon*

Main category: cs.CL

TL;DR: This paper explores scaling laws for Electronic Health Records (EHR) foundation models, finding analogous behavior to Large Language Models and providing insights into resource-efficient training strategies.


<details>
  <summary>Details</summary>
Motivation: To investigate whether the principles of scaling laws that have been successful in Large Language Models can be applied to Electronic Health Records, which are structurally different from natural language but abundant in data.

Method: Empirical investigation involving training transformer architectures on patient timeline data from the MIMIC-IV database across different model sizes and compute budgets to identify scaling patterns.

Result: Consistent scaling patterns were found, such as parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility.

Conclusion: EHR models exhibit scaling behavior similar to LLMs, paving the way for developing powerful EHR foundation models that could enhance clinical prediction tasks and personalized healthcare.

Abstract: The emergence of scaling laws has profoundly shaped the development of large
language models (LLMs), enabling predictable performance gains through
systematic increases in model size, dataset volume, and compute. Yet, these
principles remain largely unexplored in the context of electronic health
records (EHRs) -- a rich, sequential, and globally abundant data source that
differs structurally from natural language. In this work, we present the first
empirical investigation of scaling laws for EHR foundation models. By training
transformer architectures on patient timeline data from the MIMIC-IV database
across varying model sizes and compute budgets, we identify consistent scaling
patterns, including parabolic IsoFLOPs curves and power-law relationships
between compute, model parameters, data size, and clinical utility. These
findings demonstrate that EHR models exhibit scaling behavior analogous to
LLMs, offering predictive insights into resource-efficient training strategies.
Our results lay the groundwork for developing powerful EHR foundation models
capable of transforming clinical prediction tasks and advancing personalized
healthcare.

</details>


### [246] [Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation](https://arxiv.org/abs/2505.22993)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: Claim verification is a challenging task that requires high accuracy and explainability. In the era of large language models (LLMs), this task has become an emerging research issue due to the complexity of real-world claims. Traditional approaches decompose claims into sub-claims and query a knowledge base but may fail due to the absence of effective disambiguation strategies for ambiguous entities. To address these challenges, we propose Verify-in-the-Graph (VeGraph), a novel framework leveraging LLMs' reasoning and comprehension abilities. VeGraph operates in three phases: Graph Representation, Entity Disambiguation, and Verification. Experiments show that VeGraph achieves competitive performance compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Claim verification needs both high accuracy and explainability, especially with the complexity of real-world claims in the era of large language models. Traditional methods may compromise the verification process due to ineffective disambiguation strategies for ambiguous entities.

Method: The proposed framework, Verify-in-the-Graph (VeGraph), leverages the reasoning and comprehension abilities of LLM agents and operates in three phases: 1) Graph Representation - decomposing an input claim into structured triplets to form a graph-based representation integrating structured and unstructured information; 2) Entity Disambiguation - iteratively interacting with the knowledge base to resolve ambiguous entities within the graph for deeper sub-claim verification; and 3) Verification - verifying remaining triplets to complete the fact-checking process.

Result: Experiments using Meta-Llama-3-70B (instruct version) indicate that VeGraph achieves competitive performance compared to baselines on two benchmarks HoVer and FEVEROUS, effectively addressing claim verification challenges.

Conclusion: VeGraph is a novel framework that addresses the challenges of claim verification by leveraging the reasoning and comprehension abilities of LLM agents through a three-phase process. It shows competitive performance on benchmarks and provides a new approach to effectively handle complex claims.

Abstract: Claim verification is a long-standing and challenging task that demands not
only high accuracy but also explainability of the verification process. This
task becomes an emerging research issue in the era of large language models
(LLMs) since real-world claims are often complex, featuring intricate semantic
structures or obfuscated entities. Traditional approaches typically address
this by decomposing claims into sub-claims and querying a knowledge base to
resolve hidden or ambiguous entities. However, the absence of effective
disambiguation strategies for these entities can compromise the entire
verification process. To address these challenges, we propose
Verify-in-the-Graph (VeGraph), a novel framework leveraging the reasoning and
comprehension abilities of LLM agents. VeGraph operates in three phases: (1)
Graph Representation - an input claim is decomposed into structured triplets,
forming a graph-based representation that integrates both structured and
unstructured information; (2) Entity Disambiguation -VeGraph iteratively
interacts with the knowledge base to resolve ambiguous entities within the
graph for deeper sub-claim verification; and (3) Verification - remaining
triplets are verified to complete the fact-checking process. Experiments using
Meta-Llama-3-70B (instruct version) show that VeGraph achieves competitive
performance compared to baselines on two benchmarks HoVer and FEVEROUS,
effectively addressing claim verification challenges. Our source code and data
are available for further exploitation.

</details>


### [247] [A Practical Approach for Building Production-Grade Conversational Agents with Workflow Graphs](https://arxiv.org/abs/2505.23006)
*Chiwan Park,Wonjun Jang,Daeryong Kim,Aelim Ahn,Kichang Yang,Woosung Hwang,Jihyeon Roh,Hyerin Park,Hyosun Wang,Min Seok Kim,Jihoon Kang*

Main category: cs.CL

TL;DR: 本文探讨了在工业应用中使用最先进的大语言模型（LLMs）所面临的挑战，并通过电商领域的对话代理案例研究，提出了解决方案和优化策略，旨在建立可扩展、可控且可靠的AI驱动代理框架。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多个服务领域取得了显著进步，但在工业应用中，需要在保持灵活的对话能力的同时严格遵守特定服务的约束条件，这两者由于LLMs的概率性质而存在冲突。

Method: 作者提出了应对这一挑战的方法，并详细描述了为克服LLMs固有限制在实际应用中所采用的策略。通过一个针对电商领域的对话代理的实际案例研究，展示了其实施流程和优化措施。

Result: 研究表明，可以通过有效的策略和优化措施来弥合学术研究与实际应用之间的差距，开发出可扩展、可控且可靠的AI驱动代理。

Conclusion: 本研究提供了关于如何将学术研究转化为实际应用的见解，并引入了一个用于开发可扩展、可控且可靠的人工智能驱动代理的框架。

Abstract: The advancement of Large Language Models (LLMs) has led to significant
improvements in various service domains, including search, recommendation, and
chatbot applications. However, applying state-of-the-art (SOTA) research to
industrial settings presents challenges, as it requires maintaining flexible
conversational abilities while also strictly complying with service-specific
constraints. This can be seen as two conflicting requirements due to the
probabilistic nature of LLMs. In this paper, we propose our approach to
addressing this challenge and detail the strategies we employed to overcome
their inherent limitations in real-world applications. We conduct a practical
case study of a conversational agent designed for the e-commerce domain,
detailing our implementation workflow and optimizations. Our findings provide
insights into bridging the gap between academic research and real-world
application, introducing a framework for developing scalable, controllable, and
reliable AI-driven agents.

</details>


### [248] [Context Robust Knowledge Editing for Language Models](https://arxiv.org/abs/2505.23026)
*Haewon Park,Gyubin Choi,Minjun Kim,Yohan Jo*

Main category: cs.CL

TL;DR: Knowledge editing (KE) methods are crucial for modifying knowledge in large language models. However, current KE evaluations don't consider preceding contexts which can undermine the intended edit. To address this issue, CHED benchmark is developed to evaluate context robustness of KE methods. Evaluations show that KE methods often fail when preceding contexts are present. To solve this problem, CoRE method is introduced which minimizes context-sensitive variance in hidden states of the model for edited knowledge.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to improve the evaluation and effectiveness of Knowledge editing (KE) methods by considering the impact of preceding contexts which can trigger the retrieval of original knowledge and thus undermine the intended edits.

Method: CHED benchmark is developed to evaluate the context robustness of KE methods. CoRE method is introduced to strengthen context robustness by minimizing context-sensitive variance in hidden states of the model for edited knowledge.

Result: Evaluations on CHED show that KE methods often fail when preceding contexts are present. CoRE method improves the editing success rate in situations where a preceding context is present while preserving the overall capabilities of the model.

Conclusion: CoRE method enhances the context robustness of KE methods. The differing impacts of preceding contexts when introduced as user utterances versus assistant responses are analyzed. Attention-score patterns are dissected to assess how specific tokens influence editing success.

Abstract: Knowledge editing (KE) methods offer an efficient way to modify knowledge in
large language models. Current KE evaluations typically assess editing success
by considering only the edited knowledge without any preceding contexts. In
real-world applications, however, preceding contexts often trigger the
retrieval of the original knowledge and undermine the intended edit. To address
this issue, we develop CHED -- a benchmark designed to evaluate the context
robustness of KE methods. Evaluations on CHED show that they often fail when
preceding contexts are present. To mitigate this shortcoming, we introduce
CoRE, a KE method designed to strengthen context robustness by minimizing
context-sensitive variance in hidden states of the model for edited knowledge.
This method not only improves the editing success rate in situations where a
preceding context is present but also preserves the overall capabilities of the
model. We provide an in-depth analysis of the differing impacts of preceding
contexts when introduced as user utterances versus assistant responses, and we
dissect attention-score patterns to assess how specific tokens influence
editing success.

</details>


### [249] [Document-Level Text Generation with Minimum Bayes Risk Decoding using Optimal Transport](https://arxiv.org/abs/2505.23078)
*Yuu Jinnai*

Main category: cs.CL

TL;DR: The paper proposes MBR-OT, a variant of Minimum Bayes Risk (MBR) decoding using Wasserstein distance to enhance document-level text generation.


<details>
  <summary>Details</summary>
Motivation: Document-level text generation tasks are more complex due to the need for understanding longer context compared to sentence-level tasks. Existing MBR decoding has limitations in document-level tasks as its utility functions are designed for sentences.

Method: Propose MBR-OT which uses Wasserstein distance to compute the utility of a document based on a sentence-level utility function.

Result: MBR-OT outperforms standard MBR in document-level machine translation, text simplification, and dense image captioning tasks.

Conclusion: MBR-OT is an effective approach for document-level text generation tasks.

Abstract: Document-level text generation tasks are known to be more difficult than
sentence-level text generation tasks as they require the understanding of
longer context to generate high-quality texts. In this paper, we investigate
the adaption of Minimum Bayes Risk (MBR) decoding for document-level text
generation tasks. MBR decoding makes use of a utility function to estimate the
output with the highest expected utility from a set of candidate outputs.
Although MBR decoding is shown to be effective in a wide range of
sentence-level text generation tasks, its performance on document-level text
generation tasks is limited as many of the utility functions are designed for
evaluating the utility of sentences. To this end, we propose MBR-OT, a variant
of MBR decoding using Wasserstein distance to compute the utility of a document
using a sentence-level utility function. The experimental result shows that the
performance of MBR-OT outperforms that of the standard MBR in document-level
machine translation, text simplification, and dense image captioning tasks. Our
code is available at https://github.com/jinnaiyuu/mbr-optimal-transport

</details>


### [250] [Elicit and Enhance: Advancing Multimodal Reasoning in Medical Scenarios](https://arxiv.org/abs/2505.23118)
*Linjie Mu,Zhongzhen Huang,Yakun Zhu,Xiangyu Zhao,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.CL

TL;DR: This paper proposes MedE², a two-stage post-training pipeline that improves multimodal reasoning for medical domains. It involves fine-tuning models with text-only data and enhancing them with curated multimodal medical cases. Experiments show that MedE² outperforms baselines across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal reasoning models have been successful in mathematics and science but remain underexplored in medical domains. There is a need to develop methods that can effectively elicit and enhance multimodal reasoning capabilities for these domains.

Method: The method involves a two-stage post-training pipeline called MedE². In Stage-I, models are fine-tuned using 2,000 text-only data samples with orchestrated reasoning demonstrations. In Stage-II, the model's reasoning capabilities are further enhanced using 1,500 curated multimodal medical cases aligned with a proposed reasoning preference.

Result: Extensive experiments demonstrate that MedE² improves the reasoning performance of medical multimodal models. Models trained with MedE² consistently outperform baselines across multiple medical multimodal benchmarks. The approach also shows robustness and practical utility when validated on larger models and under inference-time scaling.

Conclusion: MedE² is an effective and reliable method for improving the reasoning performance of medical multimodal models. It has shown consistent superiority over baselines and its robustness has been confirmed through additional validation.

Abstract: Effective clinical decision-making depends on iterative, multimodal reasoning
across diverse sources of evidence. The recent emergence of multimodal
reasoning models has significantly transformed the landscape of solving complex
tasks. Although such models have achieved notable success in mathematics and
science, their application to medical domains remains underexplored. In this
work, we propose \textit{MedE$^2$}, a two-stage post-training pipeline that
elicits and then enhances multimodal reasoning for medical domains. In Stage-I,
we fine-tune models using 2,000 text-only data samples containing precisely
orchestrated reasoning demonstrations to elicit reasoning behaviors. In
Stage-II, we further enhance the model's reasoning capabilities using 1,500
rigorously curated multimodal medical cases, aligning model reasoning outputs
with our proposed multimodal medical reasoning preference. Extensive
experiments demonstrate the efficacy and reliability of \textit{MedE$^2$} in
improving the reasoning performance of medical multimodal models. Notably,
models trained with \textit{MedE$^2$} consistently outperform baselines across
multiple medical multimodal benchmarks. Additional validation on larger models
and under inference-time scaling further confirms the robustness and practical
utility of our approach.

</details>


### [251] [ContextQFormer: A New Context Modeling Method for Multi-Turn Multi-Modal Conversations](https://arxiv.org/abs/2505.23121)
*Yiming Lei,Zhizheng Yang,Zeming Liu,Haitao Leng,Shaoguo Liu,Tingting Gao,Qingjie Liu,Yunhong Wang*

Main category: cs.CL

TL;DR: This paper introduces ContextQFormer, a context modeling module with a memory block to improve multi-turn interaction in multi-modal models, especially for long contexts. They also present TMDialog, a new open-source multi-turn multi-modal dialogue dataset featuring longer conversations, and show that ContextQFormer outperforms three baselines by 2%-4% in available rate.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the weak capability of multi-turn interaction in existing open-source multi-modal models, particularly for handling long contexts.

Method: The authors introduce ContextQFormer, which uses a memory block to enhance contextual information representation. They also construct TMDialog, a new multi-turn multi-modal dialogue dataset designed for pre-training, instruction-tuning, and evaluation.

Result: Experimental results on TMDialog indicate that ContextQFormer improves the available rate by 2%-4% compared to three baseline models.

Conclusion: ContextQFormer effectively enhances multi-turn interaction capabilities in multi-modal models, and TMDialog provides a valuable resource for further research in this area.

Abstract: Multi-modal large language models have demonstrated remarkable zero-shot
abilities and powerful image-understanding capabilities. However, the existing
open-source multi-modal models suffer from the weak capability of multi-turn
interaction, especially for long contexts. To address the issue, we first
introduce a context modeling module, termed ContextQFormer, which utilizes a
memory block to enhance the presentation of contextual information.
Furthermore, to facilitate further research, we carefully build a new
multi-turn multi-modal dialogue dataset (TMDialog) for pre-training,
instruction-tuning, and evaluation, which will be open-sourced lately. Compared
with other multi-modal dialogue datasets, TMDialog contains longer
conversations, which supports the research of multi-turn multi-modal dialogue.
In addition, ContextQFormer is compared with three baselines on TMDialog and
experimental results illustrate that ContextQFormer achieves an improvement of
2%-4% in available rate over baselines.

</details>


### [252] [Unsupervised Word-level Quality Estimation for Machine Translation Through the Lens of Annotators (Dis)agreement](https://arxiv.org/abs/2505.23183)
*Gabriele Sarti,Vilém Zouhar,Malvina Nissim,Arianna Bisazza*

Main category: cs.CL

TL;DR: This paper explores efficient alternatives for word-level quality estimation (WQE) by leveraging advances in language model interpretability and uncertainty quantification, evaluating 14 metrics across 12 translation directions, and emphasizing the potential of unsupervised metrics and issues with supervised methods.


<details>
  <summary>Details</summary>
Motivation: Modern WQE techniques are costly due to reliance on large language models or extensive human-labeled data training. There is a need for more efficient alternatives that can identify translation errors effectively without such high costs.

Method: The study investigates the use of recent advances in language model interpretability and uncertainty quantification to identify translation errors from the inner workings of translation models. It evaluates 14 metrics across 12 translation directions and uses multiple sets of human labels to quantify the impact of label variation on metric performance.

Result: Unsupervised metrics show untapped potential, supervised methods face challenges with label uncertainty, and single-annotator evaluation practices are found to be brittle.

Conclusion: Efficient alternatives for WQE using interpretability and uncertainty quantification hold promise. Unsupervised metrics should be further explored while considering the limitations of supervised methods and current evaluation practices.

Abstract: Word-level quality estimation (WQE) aims to automatically identify
fine-grained error spans in machine-translated outputs and has found many uses,
including assisting translators during post-editing. Modern WQE techniques are
often expensive, involving prompting of large language models or ad-hoc
training on large amounts of human-labeled data. In this work, we investigate
efficient alternatives exploiting recent advances in language model
interpretability and uncertainty quantification to identify translation errors
from the inner workings of translation models. In our evaluation spanning 14
metrics across 12 translation directions, we quantify the impact of human label
variation on metric performance by using multiple sets of human labels. Our
results highlight the untapped potential of unsupervised metrics, the
shortcomings of supervised methods when faced with label uncertainty, and the
brittleness of single-annotator evaluation practices.

</details>


### [253] [Cross-Task Experiential Learning on LLM-based Multi-Agent Collaboration](https://arxiv.org/abs/2505.23187)
*Yilong Li,Chen Qian,Yu Xia,Ruijie Shi,Yufan Dang,Zihao Xie,Ziming You,Weize Chen,Cheng Yang,Weichuan Liu,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: MAEL is a new framework for LLM-driven agents that enables cross-task learning and experience accumulation, leading to faster convergence and higher-quality solutions.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems based on Large Language Models have limitations in generalization across structurally similar tasks due to treating each task in isolation.

Method: The MAEL framework models the task-solving workflow on a graph-structured multi-agent collaboration network. During experiential learning, it quantifies the quality of each step and stores rewards with inputs and outputs into an experience pool. During inference, agents retrieve high-reward experiences as few-shot examples.

Result: Experimental results show that MAEL allows agents to learn from prior task experiences effectively, achieving faster convergence and producing higher-quality solutions on current tasks.

Conclusion: MAEL enhances the effectiveness of multi-agent collaboration by enabling agents to leverage past experiences, resulting in more accurate and efficient task-solving.

Abstract: Large Language Model-based multi-agent systems (MAS) have shown remarkable
progress in solving complex tasks through collaborative reasoning and
inter-agent critique. However, existing approaches typically treat each task in
isolation, resulting in redundant computations and limited generalization
across structurally similar tasks. To address this, we introduce multi-agent
cross-task experiential learning (MAEL), a novel framework that endows
LLM-driven agents with explicit cross-task learning and experience
accumulation. We model the task-solving workflow on a graph-structured
multi-agent collaboration network, where agents propagate information and
coordinate via explicit connectivity. During the experiential learning phase,
we quantify the quality for each step in the task-solving workflow and store
the resulting rewards along with the corresponding inputs and outputs into each
agent's individual experience pool. During inference, agents retrieve
high-reward, task-relevant experiences as few-shot examples to enhance the
effectiveness of each reasoning step, thereby enabling more accurate and
efficient multi-agent collaboration. Experimental results on diverse datasets
demonstrate that MAEL empowers agents to learn from prior task experiences
effectively-achieving faster convergence and producing higher-quality solutions
on current tasks.

</details>


### [254] [ExpeTrans: LLMs Are Experiential Transfer Learners](https://arxiv.org/abs/2505.23191)
*Jinglong Gao,Xiao Ding,Lingxiao Zou,Bibo Cai,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: The paper designs an autonomous experience transfer framework for LLMs to mimic human cognitive intelligence in transferring experiences from source tasks to target tasks, reducing costs and improving performance.


<details>
  <summary>Details</summary>
Motivation: To reduce the substantial human labor or time required by previous methods to gather textual task-solving experiences for each task, given the growing variety of task types in user queries to LLMs.

Method: An autonomous experience transfer framework is designed, allowing LLMs to autonomously transfer experience from existing source tasks to newly encountered target tasks.

Result: Experimental results on 13 datasets demonstrate that the framework effectively improves the performance of LLMs.

Conclusion: The framework not only allows the acquisition of experience without extensive costs of previous methods, but also offers a novel path for the generalization of LLMs.

Abstract: Recent studies provide large language models (LLMs) with textual task-solving
experiences via prompts to improve their performance. However, previous methods
rely on substantial human labor or time to gather such experiences for each
task, which is impractical given the growing variety of task types in user
queries to LLMs. To address this issue, we design an autonomous experience
transfer framework to explore whether LLMs can mimic human cognitive
intelligence to autonomously transfer experience from existing source tasks to
newly encountered target tasks. This not only allows the acquisition of
experience without extensive costs of previous methods, but also offers a novel
path for the generalization of LLMs. Experimental results on 13 datasets
demonstrate that our framework effectively improves the performance of LLMs.
Furthermore, we provide a detailed analysis of each module in the framework.

</details>


### [255] [MCTSr-Zero: Self-Reflective Psychological Counseling Dialogues Generation via Principles and Adaptive Exploration](https://arxiv.org/abs/2505.23229)
*Hao Lu,Yanchi Gu,Haoyuan Huang,Yulin Zhou,Ningxin Zhu,Chen Li*

Main category: cs.CL

TL;DR: MCTSr-Zero是一种专为开放性、以人为中心的对话设计的蒙特卡洛树搜索框架，通过域对齐、再生和元提示适应机制生成高质量对话数据。实验表明，基于MCTSr-Zero生成的数据微调的语言模型PsyLLM在心理辅导对话评估基准PsyEval上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 将蒙特卡洛树搜索与大语言模型结合在结构化问题任务中取得了显著成功，但在开放性对话（如心理辅导）中存在挑战，因为成功取决于同理心参与、伦理遵守和与人类偏好的一致性等主观因素，而现有结果导向的MCTS方法可能产生不一致的响应。

Method: 提出了MCTSr-Zero框架，核心创新是“域对齐”，将MCTS搜索目标从预定义的终端状态转向符合目标领域原则的对话轨迹。此外，还引入了“再生”和“元提示适应”机制以显著扩展探索范围，允许MCTS考虑不同的初始对话策略。

Result: 使用MCTSr-Zero生成多轮对话数据用于微调语言模型PsyLLM，并引入了PsyEval基准来评估多轮心理辅导对话。实验表明，PsyLLM在PsyEval和其他相关指标上达到了最先进的性能。

Conclusion: MCTSr-Zero框架在生成高质量、符合原则的对话数据方面具有有效性，解决了大语言模型在心理辅导等复杂标准下的一致性问题。

Abstract: The integration of Monte Carlo Tree Search (MCTS) with Large Language Models
(LLMs) has demonstrated significant success in structured, problem-oriented
tasks. However, applying these methods to open-ended dialogues, such as those
in psychological counseling, presents unique challenges. Unlike tasks with
objective correctness, success in therapeutic conversations depends on
subjective factors like empathetic engagement, ethical adherence, and alignment
with human preferences, for which strict "correctness" criteria are
ill-defined. Existing result-oriented MCTS approaches can therefore produce
misaligned responses. To address this, we introduce MCTSr-Zero, an MCTS
framework designed for open-ended, human-centric dialogues. Its core innovation
is "domain alignment", which shifts the MCTS search objective from predefined
end-states towards conversational trajectories that conform to target domain
principles (e.g., empathy in counseling). Furthermore, MCTSr-Zero incorporates
"Regeneration" and "Meta-Prompt Adaptation" mechanisms to substantially broaden
exploration by allowing the MCTS to consider fundamentally different initial
dialogue strategies. We evaluate MCTSr-Zero in psychological counseling by
generating multi-turn dialogue data, which is used to fine-tune an LLM, PsyLLM.
We also introduce PsyEval, a benchmark for assessing multi-turn psychological
counseling dialogues. Experiments demonstrate that PsyLLM achieves
state-of-the-art performance on PsyEval and other relevant metrics, validating
MCTSr-Zero's effectiveness in generating high-quality, principle-aligned
conversational data for human-centric domains and addressing the LLM challenge
of consistently adhering to complex psychological standards.

</details>


### [256] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)
*Maged S. Al-Shaibani,Moataz Ahmed*

Main category: cs.CL

TL;DR: The paper comprehensively investigates Arabic machine-generated text using multiple generation strategies and model architectures, revealing distinctive linguistic patterns that differentiate human and machine-generated text. Based on these insights, BERT-based detection models were developed with exceptional performance in formal contexts.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting machine-generated Arabic text, particularly given the subtleties and significant challenges posed by Large Language Models (LLMs) in compromising information integrity across various domains.

Method: Conducting a comprehensive investigation of Arabic machine-generated text using multiple generation strategies (generation from title only, content-aware generation, and text refinement) across diverse model architectures (ALLaM, Jais, Llama, and GPT-4). Performing stylometric analysis to identify linguistic patterns differentiating human-written from machine-generated Arabic text.

Result: Distinctive linguistic patterns were found that differentiate human-written from machine-generated Arabic text. BERT-based detection models achieved high performance in formal contexts (up to 99.9% F1-score). Cross-domain analysis confirmed generalization challenges.

Conclusion: This work represents the most comprehensive investigation of Arabic machine-generated text to date, combining multiple prompt generation methods, diverse model architectures, and in-depth stylometric analysis, laying the foundation for robust detection systems.

Abstract: Large Language Models (LLMs) have achieved unprecedented capabilities in
generating human-like text, posing subtle yet significant challenges for
information integrity across critical domains, including education, social
media, and academia, enabling sophisticated misinformation campaigns,
compromising healthcare guidance, and facilitating targeted propaganda. This
challenge becomes severe, particularly in under-explored and low-resource
languages like Arabic. This paper presents a comprehensive investigation of
Arabic machine-generated text, examining multiple generation strategies
(generation from the title only, content-aware generation, and text refinement)
across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic,
and social media domains. Our stylometric analysis reveals distinctive
linguistic patterns differentiating human-written from machine-generated Arabic
text across these varied contexts. Despite their human-like qualities, we
demonstrate that LLMs produce detectable signatures in their Arabic outputs,
with domain-specific characteristics that vary significantly between different
contexts. Based on these insights, we developed BERT-based detection models
that achieved exceptional performance in formal contexts (up to 99.9\%
F1-score) with strong precision across model architectures. Our cross-domain
analysis confirms generalization challenges previously reported in the
literature. To the best of our knowledge, this work represents the most
comprehensive investigation of Arabic machine-generated text to date, uniquely
combining multiple prompt generation methods, diverse model architectures, and
in-depth stylometric analysis across varied textual domains, establishing a
foundation for developing robust, linguistically-informed detection systems
essential for preserving information integrity in Arabic-language contexts.

</details>


### [257] [Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective](https://arxiv.org/abs/2505.23277)
*Yong Zhang,Yanwen Huang,Ning Cheng,Yang Guo,Yun Zhu,Yanmeng Wang,Shaojun Wang,Jing Xiao*

Main category: cs.CL

TL;DR: The paper introduces Sentinel, a lightweight sentence-level compression framework for Retrieval-augmented generation (RAG) that uses decoder attention from an off-the-shelf 0.5B proxy LLM and a lightweight classifier to identify sentence relevance. It achieves up to 5× compression on the LongBench benchmark while matching the QA performance of 7B-scale compression systems.


<details>
  <summary>Details</summary>
Motivation: Retrieved passages in RAG are often lengthy, noisy, or exceed input limits. Existing compression methods typically require supervised training of dedicated compression models, increasing cost and reducing portability.

Method: Sentinel reframes context filtering as an attention-based understanding task. It probes decoder attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier to identify sentence relevance.

Result: On the LongBench benchmark, Sentinel achieves up to 5× compression while matching the QA performance of 7B-scale compression systems.

Conclusion: Probing native attention signals enables fast, effective, and question-aware context compression.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with external context, but retrieved passages are often lengthy, noisy, or
exceed input limits. Existing compression methods typically require supervised
training of dedicated compression models, increasing cost and reducing
portability. We propose Sentinel, a lightweight sentence-level compression
framework that reframes context filtering as an attention-based understanding
task. Rather than training a compression model, Sentinel probes decoder
attention from an off-the-shelf 0.5B proxy LLM using a lightweight classifier
to identify sentence relevance. Empirically, we find that query-context
relevance estimation is consistent across model scales, with 0.5B proxies
closely matching the behaviors of larger models. On the LongBench benchmark,
Sentinel achieves up to 5$\times$ compression while matching the QA performance
of 7B-scale compression systems. Our results suggest that probing native
attention signals enables fast, effective, and question-aware context
compression. Code available at: https://github.com/yzhangchuck/Sentinel.

</details>


### [258] [How Does Response Length Affect Long-Form Factuality](https://arxiv.org/abs/2505.23295)
*James Xu Zhao,Jimmy Z. J. Liu,Bryan Hooi,See-Kiong Ng*

Main category: cs.CL

TL;DR: Large language models (LLMs) have a problem with factual errors in long-form text generation. This study explores the relationship between response length and factuality, finding that longer responses tend to be less precise due to facts exhaustion.


<details>
  <summary>Details</summary>
Motivation: To understand why factual precision decreases as the length of LLM-generated responses increases.

Method: Develop an automatic and bi-level long-form factuality evaluation framework. Conduct controlled experiments using this framework to test three hypotheses: error propagation, long context, and facts exhaustion.

Result: Longer responses from LLMs exhibit lower factual precision. Facts exhaustion is the main cause of this degradation, not error propagation or long context.

Conclusion: The study confirms the presence of length bias in LLMs and identifies facts exhaustion as the primary factor leading to reduced factual precision in longer responses.

Abstract: Large language models (LLMs) are widely used for long-form text generation.
However, factual errors in the responses would undermine their reliability.
Despite growing attention to LLM factuality, the effect of response length on
factuality remains underexplored. In this work, we systematically investigate
this relationship by first introducing an automatic and bi-level long-form
factuality evaluation framework, which achieves high agreement with human
annotations while being cost-effective. Using this framework, we conduct
controlled experiments and find that longer responses exhibit lower factual
precision, confirming the presence of length bias. To explain this phenomenon,
we empirically examine three hypotheses: error propagation, long context, and
facts exhaustion. Our results reveal that facts exhaustion, where the model
gradually exhausts more reliable knowledge, is the primary cause of factual
degradation, rather than the other two hypotheses.

</details>


### [259] [Enhancing Marker Scoring Accuracy through Ordinal Confidence Modelling in Educational Assessments](https://arxiv.org/abs/2505.23315)
*Abhirup Chakravarty,Mark Brenchley,Trevor Breakspear,Ian Lewin,Yan Huang*

Main category: cs.CL

TL;DR: An Automated Essay Scoring (AES) study reformulates confidence estimation as a classification task, achieving an F1 score of 0.97 and significantly improving the CEFR agreement compared to standalone AES models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the ethical challenge in AES by ensuring scores meet high reliability standards before release, using confidence modeling to assign reliability estimates.

Method: This study frames confidence estimation as a binary classification task, predicting whether AES-generated scores correctly place candidates in appropriate CEFR levels. It uses score binning to reformulate the task as an n-ary classification problem and introduces KWOCCE loss functions to incorporate the ordinal structure of CEFR labels.

Result: The best-performing model achieved an F1 score of 0.97, enabling the system to release 47% of scores with 100% CEFR agreement and 99% with at least 95% CEFR agreement, outperforming standalone AES models.

Conclusion: The novel approach to confidence estimation enhances the reliability of AES systems, allowing for selective score release based on high CEFR agreement.

Abstract: A key ethical challenge in Automated Essay Scoring (AES) is ensuring that
scores are only released when they meet high reliability standards. Confidence
modelling addresses this by assigning a reliability estimate measure, in the
form of a confidence score, to each automated score. In this study, we frame
confidence estimation as a classification task: predicting whether an
AES-generated score correctly places a candidate in the appropriate CEFR level.
While this is a binary decision, we leverage the inherent granularity of the
scoring domain in two ways. First, we reformulate the task as an n-ary
classification problem using score binning. Second, we introduce a set of novel
Kernel Weighted Ordinal Categorical Cross Entropy (KWOCCE) loss functions that
incorporate the ordinal structure of CEFR labels. Our best-performing model
achieves an F1 score of 0.97, and enables the system to release 47% of scores
with 100% CEFR agreement and 99% with at least 95% CEFR agreement -compared to
approximately 92% (approx.) CEFR agreement from the standalone AES model where
we release all AM predicted scores.

</details>


### [260] [CLaC at SemEval-2025 Task 6: A Multi-Architecture Approach for Corporate Environmental Promise Verification](https://arxiv.org/abs/2505.23538)
*Nawar Turk,Eeham Khan,Leila Kosseim*

Main category: cs.CL

TL;DR: The paper presents three model architectures for the SemEval-2025 Task 6 (PromiseEval) to verify promises in corporate ESG reports, showing progressive improvement and outperforming the baseline.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying promises in corporate ESG reports by participating in the SemEval-2025 Task 6 (PromiseEval).

Method: Three model architectures were explored: the first uses ESG-BERT with task-specific classifier heads, the second enhances it with linguistic features, and the third implements a combined subtask model with attention-based sequence pooling, transformer representations augmented with document metadata, and multi-objective learning.

Result: Experiments on the English portion of the ML-Promise dataset show progressive improvement across the models, with the combined subtask approach achieving a leaderboard score of 0.5268, surpassing the provided baseline of 0.5227.

Conclusion: The work demonstrates the effectiveness of linguistic feature extraction, attention pooling, and multi-objective learning in promise verification tasks, despite challenges like class imbalance and limited training data.

Abstract: This paper presents our approach to the SemEval-2025 Task~6 (PromiseEval),
which focuses on verifying promises in corporate ESG (Environmental, Social,
and Governance) reports. We explore three model architectures to address the
four subtasks of promise identification, supporting evidence assessment,
clarity evaluation, and verification timing. Our first model utilizes ESG-BERT
with task-specific classifier heads, while our second model enhances this
architecture with linguistic features tailored for each subtask. Our third
approach implements a combined subtask model with attention-based sequence
pooling, transformer representations augmented with document metadata, and
multi-objective learning. Experiments on the English portion of the ML-Promise
dataset demonstrate progressive improvement across our models, with our
combined subtask approach achieving a leaderboard score of 0.5268,
outperforming the provided baseline of 0.5227. Our work highlights the
effectiveness of linguistic feature extraction, attention pooling, and
multi-objective learning in promise verification tasks, despite challenges
posed by class imbalance and limited training data.

</details>


### [261] [Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software Engineering](https://arxiv.org/abs/2505.23604)
*Guangtao Zeng,Maohao Shen,Delin Chen,Zhenting Qi,Subhro Das,Dan Gutfreund,David Cox,Gregory Wornell,Wei Lu,Zhang-Wei Hong,Chuang Gan*

Main category: cs.CL

TL;DR: EvoScale is a sample-efficient method that treats generation as an evolutionary process, which can improve the performance of smaller language models on real-world software engineering tasks without excessive sampling and costly scoring.


<details>
  <summary>Details</summary>
Motivation: Smaller language models are preferable in practice due to their lower computational cost, but they struggle with real-world software engineering tasks. Existing approaches primarily rely on supervised fine-tuning or test-time scaling, both of which have limitations.

Method: The proposed Evolutionary Test-Time Scaling (EvoScale) method treats generation as an evolutionary process. It iteratively refines outputs via selection and mutation, reducing the number of samples needed to find correct solutions. To reduce overhead, the model is trained to self-evolve using reinforcement learning, allowing it to learn to self-improve the scores of its own generations across iterations.

Result: Evaluated on SWE-Bench-Verified, EvoScale enables a 32B model (Satori-SWE-32B) to match or exceed the performance of models with over 100B parameters while using a few samples.

Conclusion: EvoScale provides a more efficient way to improve the performance of smaller language models on real-world software engineering tasks, making them more practical for use.

Abstract: Language models (LMs) perform well on standardized coding benchmarks but
struggle with real-world software engineering tasks such as resolving GitHub
issues in SWE-Bench, especially when model parameters are less than 100B. While
smaller models are preferable in practice due to their lower computational
cost, improving their performance remains challenging. Existing approaches
primarily rely on supervised fine-tuning (SFT) with high-quality data, which is
expensive to curate at scale. An alternative is test-time scaling: generating
multiple outputs, scoring them using a verifier, and selecting the best one.
Although effective, this strategy often requires excessive sampling and costly
scoring, limiting its practical application. We propose Evolutionary Test-Time
Scaling (EvoScale), a sample-efficient method that treats generation as an
evolutionary process. By iteratively refining outputs via selection and
mutation, EvoScale shifts the output distribution toward higher-scoring
regions, reducing the number of samples needed to find correct solutions. To
reduce the overhead from repeatedly sampling and selection, we train the model
to self-evolve using reinforcement learning (RL). Rather than relying on
external verifiers at inference time, the model learns to self-improve the
scores of its own generations across iterations. Evaluated on
SWE-Bench-Verified, EvoScale enables our 32B model, Satori-SWE-32B, to match or
exceed the performance of models with over 100B parameters while using a few
samples. Code, data, and models will be fully open-sourced.

</details>


### [262] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)
*Jiaxin Bai,Wei Fan,Qi Hu,Qing Zong,Chunyang Li,Hong Ting Tsang,Hongyu Luo,Yauwai Yim,Haoyu Huang,Xiao Zhou,Feng Qin,Tianshi Zheng,Xi Peng,Xin Yao,Huiwen Yang,Leijie Wu,Yi Ji,Gong Zhang,Renhai Chen,Yangqiu Song*

Main category: cs.CL

TL;DR: AutoSchemaKG is a framework that uses large language models to extract knowledge triples and induce schemas from text, constructing a knowledge graph called ATLAS with 900+ million nodes and 5.9 billion edges. This approach outperforms baselines in multi-hop QA tasks and improves LLM factuality.


<details>
  <summary>Details</summary>
Motivation: To create a fully autonomous knowledge graph construction framework that eliminates the need for predefined schemas, leveraging large language models to extract knowledge and induce schemas directly from text.

Method: The system processes over 50 million documents to construct ATLAS, a family of knowledge graphs with 900+ million nodes and 5.9 billion edges, using conceptualization to organize instances into semantic categories.

Result: This approach outperforms state-of-the-art baselines on multi-hop QA tasks, enhances LLM factuality, and achieves 95% semantic alignment with human-crafted schemas without manual intervention.

Conclusion: Billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.

Abstract: We present AutoSchemaKG, a framework for fully autonomous knowledge graph
construction that eliminates the need for predefined schemas. Our system
leverages large language models to simultaneously extract knowledge triples and
induce comprehensive schemas directly from text, modeling both entities and
events while employing conceptualization to organize instances into semantic
categories. Processing over 50 million documents, we construct ATLAS (Automated
Triple Linking And Schema induction), a family of knowledge graphs with 900+
million nodes and 5.9 billion edges. This approach outperforms state-of-the-art
baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our
schema induction achieves 95\% semantic alignment with human-crafted schemas
with zero manual intervention, demonstrating that billion-scale knowledge
graphs with dynamically induced schemas can effectively complement parametric
knowledge in large language models.

</details>


### [263] [Active Layer-Contrastive Decoding Reduces Hallucination in Large Language Model Generation](https://arxiv.org/abs/2505.23657)
*Hongxiang Zhang,Hao Chen,Tianyi Zhang,Muhao Chen*

Main category: cs.CL

TL;DR: Recent decoding methods improve LLMs' factuality by refining token selection. However, hallucinations remain a problem over longer contexts. This paper proposes ActLCD, which actively decides when to apply contrasting layers during generation using reinforcement learning. Experiments show ActLCD surpasses state-of-the-art methods in reducing hallucinations across five benchmarks.


<details>
  <summary>Details</summary>
Motivation: Despite improvements in factuality through recent decoding methods, LLMs still struggle with hallucinations, especially in longer contexts.

Method: Proposes Active Layer-Contrastive Decoding (ActLCD), a new decoding strategy that uses reinforcement learning to decide when to apply contrasting layers during text generation.

Result: Experiments demonstrate that ActLCD outperforms current state-of-the-art methods across five different benchmarks, effectively mitigating hallucinations in various scenarios.

Conclusion: ActLCD is an effective strategy for enhancing the factuality of LLMs and reducing hallucinations in diverse generation tasks.

Abstract: Recent decoding methods improve the factuality of large language
models~(LLMs) by refining how the next token is selected during generation.
These methods typically operate at the token level, leveraging internal
representations to suppress superficial patterns. Nevertheless, LLMs remain
prone to hallucinations, especially over longer contexts. In this paper, we
propose Active Layer-Contrastive Decoding (ActLCD), a novel decoding strategy
that actively decides when to apply contrasting layers during generation. By
casting decoding as a sequential decision-making problem, ActLCD employs a
reinforcement learning policy guided by a reward-aware classifier to optimize
factuality beyond the token level. Our experiments demonstrate that ActLCD
surpasses state-of-the-art methods across five benchmarks, showcasing its
effectiveness in mitigating hallucinations in diverse generation scenarios.

</details>


### [264] [Bayesian Attention Mechanism: A Probabilistic Framework for Positional Encoding and Context Length Extrapolation](https://arxiv.org/abs/2505.22842)
*Arthur S. Bianchessi,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CL

TL;DR: Transformer-based language models use positional encoding (PE) to handle token order and support context length extrapolation. However, existing PE methods have theoretical ambiguity and rely on limited evaluation metrics. This paper proposes the Bayesian Attention Mechanism (BAM), which formulates PE as a prior in a probabilistic model. BAM integrates existing methods and introduces a new Generalized Gaussian positional prior that enhances long-context generalization. Experimentally, BAM achieves accurate information retrieval at 500 times the training context length, surpassing previous state-of-the-art in long context retrieval accuracy while maintaining comparable perplexity and adding minimal parameters.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical clarity and reliance on limited evaluation metrics in existing PE methods for transformer-based language models.

Method: The paper proposes the Bayesian Attention Mechanism (BAM), which formulates PE as a prior within a probabilistic model. It unifies existing methods like NoPE and ALiBi and motivates a new Generalized Gaussian positional prior.

Result: BAM enables accurate information retrieval at 500 times the training context length, outperforming previous state-of-the-art in long context retrieval accuracy while maintaining comparable perplexity and introducing minimal additional parameters.

Conclusion: BAM provides a theoretical framework for PE, unifies existing methods, and significantly improves long-context generalization in transformer-based language models.

Abstract: Transformer-based language models rely on positional encoding (PE) to handle
token order and support context length extrapolation. However, existing PE
methods lack theoretical clarity and rely on limited evaluation metrics to
substantiate their extrapolation claims. We propose the Bayesian Attention
Mechanism (BAM), a theoretical framework that formulates positional encoding as
a prior within a probabilistic model. BAM unifies existing methods (e.g., NoPE
and ALiBi) and motivates a new Generalized Gaussian positional prior that
substantially improves long-context generalization. Empirically, BAM enables
accurate information retrieval at $500\times$ the training context length,
outperforming previous state-of-the-art context length generalization in long
context retrieval accuracy while maintaining comparable perplexity and
introducing minimal additional parameters.

</details>


### [265] [SenWiCh: Sense-Annotation of Low-Resource Languages for WiC using Hybrid Methods](https://arxiv.org/abs/2505.23714)
*Roksana Goworek,Harpal Karlcut,Muhammad Shezad,Nijaguna Darshana,Abhishek Mane,Syam Bondada,Raghav Sikka,Ulvi Mammadov,Rauf Allahverdiyev,Sriram Purighella,Paridhi Gupta,Muhinyia Ndegwa,Haim Dubossarsky*

Main category: cs.CL

TL;DR: This paper tackles the lack of high-quality evaluation datasets in low-resource languages for cross-lingual transfer. It releases new sense-annotated datasets across nine low-resource languages and presents a semi-automatic annotation method. Experiments show the importance of targeted dataset creation for polysemy disambiguation.


<details>
  <summary>Details</summary>
Motivation: The motivation is the critical need for high-quality evaluation datasets in low-resource languages to advance cross-lingual transfer effectiveness.

Method: The method involves releasing new sense-annotated datasets of sentences containing polysemous words across nine low-resource languages, using a semi-automatic annotation method.

Result: The results highlight the importance of targeted dataset creation and evaluation for effective polysemy disambiguation in low-resource settings and transfer studies.

Conclusion: The conclusion is that the released datasets and code will support further research into fair, robust, and truly multilingual NLP.

Abstract: This paper addresses the critical need for high-quality evaluation datasets
in low-resource languages to advance cross-lingual transfer. While
cross-lingual transfer offers a key strategy for leveraging multilingual
pretraining to expand language technologies to understudied and typologically
diverse languages, its effectiveness is dependent on quality and suitable
benchmarks. We release new sense-annotated datasets of sentences containing
polysemous words, spanning nine low-resource languages across diverse language
families and scripts. To facilitate dataset creation, the paper presents a
demonstrably beneficial semi-automatic annotation method. The utility of the
datasets is demonstrated through Word-in-Context (WiC) formatted experiments
that evaluate transfer on these low-resource languages. Results highlight the
importance of targeted dataset creation and evaluation for effective polysemy
disambiguation in low-resource settings and transfer studies. The released
datasets and code aim to support further research into fair, robust, and truly
multilingual NLP.

</details>


### [266] [ML-Agent: Reinforcing LLM Agents for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2505.23723)
*Zexi Liu,Jingyi Chai,Xinyu Zhu,Shuo Tang,Rui Ye,Bo Zhang,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: The paper introduces an agentic ML training framework that uses online reinforcement learning to train LLM agents for autonomous ML, resulting in a 7B-sized ML-Agent that outperforms a much larger agent.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of manual prompt engineering in existing approaches and to explore the paradigm of learning-based agentic ML.

Method: Proposes a novel agentic ML training framework with three key components: exploration-enriched fine-tuning, step-wise RL, and an agentic ML-specific reward module.

Result: Trained a 7B-sized ML-Agent using Qwen-2.5 LLM that outperforms a 671B-sized agent and shows continuous performance improvements and cross-task generalization capabilities.

Conclusion: Demonstrates the potential of learning-based agentic ML in advancing autonomous ML engineering.

Abstract: The emergence of large language model (LLM)-based agents has significantly
advanced the development of autonomous machine learning (ML) engineering.
However, most existing approaches rely heavily on manual prompt engineering,
failing to adapt and optimize based on diverse experimental experiences.
Focusing on this, for the first time, we explore the paradigm of learning-based
agentic ML, where an LLM agent learns through interactive experimentation on ML
tasks using online reinforcement learning (RL). To realize this, we propose a
novel agentic ML training framework with three key components: (1)
exploration-enriched fine-tuning, which enables LLM agents to generate diverse
actions for enhanced RL exploration; (2) step-wise RL, which enables training
on a single action step, accelerating experience collection and improving
training efficiency; (3) an agentic ML-specific reward module, which unifies
varied ML feedback signals into consistent rewards for RL optimization.
Leveraging this framework, we train ML-Agent, driven by a 7B-sized Qwen-2.5 LLM
for autonomous ML. Remarkably, despite being trained on merely 9 ML tasks, our
7B-sized ML-Agent outperforms the 671B-sized DeepSeek-R1 agent. Furthermore, it
achieves continuous performance improvements and demonstrates exceptional
cross-task generalization capabilities.

</details>


### [267] [Bounded Rationality for LLMs: Satisficing Alignment at Inference-Time](https://arxiv.org/abs/2505.23729)
*Mohamad Chehade,Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Dinesh Manocha,Hao Zhu,Amrit Singh Bedi*

Main category: cs.CL

TL;DR: 这篇论文提出了一种名为SITAlign的推理时间框架，用于解决大型语言模型与人类对齐的多方面性质问题。该方法通过最大化主要目标同时满足次要标准的阈值约束来实现满意的对齐，并在多个基准测试中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法通常将偏好反馈视为多目标优化问题，但忽略了人类实际决策的方式。研究显示，人类决策遵循满意策略，即优化主要目标同时确保其他目标达到可接受的阈值。

Method: 提出了SITAlign框架，该框架在推理时通过最大化主要目标并满足基于阈值的约束条件来处理对齐的多方面性质。此外，还推导了满意推理对齐方法的次优性界限。

Result: 在PKU-SafeRLHF数据集上，SITAlign在确保无害性阈值的同时，帮助性奖励的GPT-4胜平率比最先进的多目标解码策略高出22.3%。

Conclusion: SITAlign提供了一种新的方法来解决大型语言模型与人类对齐的问题，特别是在多方面偏好反馈的情况下，表现出优越的性能。

Abstract: Aligning large language models with humans is challenging due to the
inherently multifaceted nature of preference feedback. While existing
approaches typically frame this as a multi-objective optimization problem, they
often overlook how humans actually make decisions. Research on bounded
rationality suggests that human decision making follows satisficing
strategies-optimizing primary objectives while ensuring others meet acceptable
thresholds. To bridge this gap and operationalize the notion of satisficing
alignment, we propose SITAlign: an inference time framework that addresses the
multifaceted nature of alignment by maximizing a primary objective while
satisfying threshold-based constraints on secondary criteria. We provide
theoretical insights by deriving sub-optimality bounds of our satisficing based
inference alignment approach. We empirically validate SITAlign's performance
through extensive experimentation on multiple benchmarks. For instance, on the
PKU-SafeRLHF dataset with the primary objective of maximizing helpfulness while
ensuring a threshold on harmlessness, SITAlign outperforms the state-of-the-art
multi objective decoding strategy by a margin of 22.3% in terms of GPT-4
win-tie rate for helpfulness reward while adhering to the threshold on
harmlessness.

</details>


### [268] [ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind](https://arxiv.org/abs/2505.22961)
*Peixuan Han,Zijia Liu,Jiaxuan You*

Main category: cs.CL

TL;DR: ToMAP introduces two ToM modules to enhance LLM persuaders' awareness of opponents' mental states, improving argument diversity and effectiveness. Experiments show it outperforms larger models like GPT-4o with a 39.4% relative gain.


<details>
  <summary>Details</summary>
Motivation: Existing LLM persuaders lack the Theory of Mind reasoning that humans possess, leading to limited diversity and opponent awareness in their arguments.

Method: ToMAP incorporates two theory of mind modules which first consider possible objections to a central claim and then predict the opponent's stance using a text encoder and MLP classifier. A reinforcement learning schema helps the persuader analyze and utilize opponent-related information for generating effective arguments.

Result: ToMAP, despite having only 3B parameters, surpasses much larger baselines such as GPT-4o by 39.4% across various persuadee models and corpora. It shows complex reasoning, reduced repetition, and improved diversity and effectiveness in arguments.

Conclusion: The results demonstrate the effectiveness of ToMAP in creating more persuasive language agents with enhanced opponent-aware features suitable for long conversations and logical strategies.

Abstract: Large language models (LLMs) have shown promising potential in persuasion,
but existing works on training LLM persuaders are still preliminary. Notably,
while humans are skilled in modeling their opponent's thoughts and opinions
proactively and dynamically, current LLMs struggle with such Theory of Mind
(ToM) reasoning, resulting in limited diversity and opponent awareness. To
address this limitation, we introduce Theory of Mind Augmented Persuader
(ToMAP), a novel approach for building more flexible persuader agents by
incorporating two theory of mind modules that enhance the persuader's awareness
and analysis of the opponent's mental state. Specifically, we begin by
prompting the persuader to consider possible objections to the target central
claim, and then use a text encoder paired with a trained MLP classifier to
predict the opponent's current stance on these counterclaims. Our carefully
designed reinforcement learning schema enables the persuader learns how to
analyze opponent-related information and utilize it to generate more effective
arguments. Experiments show that the ToMAP persuader, while containing only 3B
parameters, outperforms much larger baselines, like GPT-4o, with a relative
gain of 39.4% across multiple persuadee models and diverse corpora. Notably,
ToMAP exhibits complex reasoning chains and reduced repetition during training,
which leads to more diverse and effective arguments. The opponent-aware feature
of ToMAP also makes it suitable for long conversations and enables it to employ
more logical and opponent-aware strategies. These results underscore our
method's effectiveness and highlight its potential for developing more
persuasive language agents. Code is available at:
https://github.com/ulab-uiuc/ToMAP.

</details>


### [269] [ATLAS: Learning to Optimally Memorize the Context at Test Time](https://arxiv.org/abs/2505.23735)
*Ali Behrouz,Zeman Li,Praneeth Kacham,Majid Daliri,Yuan Deng,Peilin Zhong,Meisam Razaviyayn,Vahab Mirrokni*

Main category: cs.CL

TL;DR: ATLAS is a new long-term memory module that overcomes the online nature of long-term memory models and enhances memory capacity, management and update methods. It is part of the DeepTransformers family which generalizes the original Transformer architecture.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of current models in tasks requiring long context understanding and extrapolation to longer sequences.

Method: Presenting ATLAS, a long-term memory module with high capacity that learns to memorize the context by optimizing the memory based on the current and past tokens.

Result: Experimental results show that ATLAS surpasses the performance of Transformers and recent linear recurrent models. It improves the long context performance of Titans, achieving +80% accuracy in 10M context length of BABILong benchmark.

Conclusion: ATLAS and DeepTransformers provide an effective solution for tasks requiring long context understanding.

Abstract: Transformers have been established as the most popular backbones in sequence
modeling, mainly due to their effectiveness in in-context retrieval tasks and
the ability to learn at scale. Their quadratic memory and time complexity,
however, bound their applicability in longer sequences and so has motivated
researchers to explore effective alternative architectures such as modern
recurrent neural networks (a.k.a long-term recurrent memory module). Despite
their recent success in diverse downstream tasks, they struggle in tasks that
requires long context understanding and extrapolation to longer sequences. We
observe that these shortcomings come from three disjoint aspects in their
design: (1) limited memory capacity that is bounded by the architecture of
memory and feature mapping of the input; (2) online nature of update, i.e.,
optimizing the memory only with respect to the last input; and (3) less
expressive management of their fixed-size memory. To enhance all these three
aspects, we present ATLAS, a long-term memory module with high capacity that
learns to memorize the context by optimizing the memory based on the current
and past tokens, overcoming the online nature of long-term memory models.
Building on this insight, we present a new family of Transformer-like
architectures, called DeepTransformers, that are strict generalizations of the
original Transformer architecture. Our experimental results on language
modeling, common-sense reasoning, recall-intensive, and long-context
understanding tasks show that ATLAS surpasses the performance of Transformers
and recent linear recurrent models. ATLAS further improves the long context
performance of Titans, achieving +80\% accuracy in 10M context length of
BABILong benchmark.

</details>


### [270] [DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural Language and Reinforcement Learning](https://arxiv.org/abs/2505.23754)
*Ziyin Zhang,Jiahao Xu,Zhiwei He,Tian Liang,Qiuzhi Liu,Yansi Li,Linfeng Song,Zhengwen Liang,Zhuosheng Zhang,Rui Wang,Zhaopeng Tu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: The paper introduces DeepTheorem, an informal theorem-proving framework using natural language to boost LLMs' mathematical reasoning. It features a large benchmark dataset and RL-Zero, improving theorem proving performance and achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the gap between traditional automated theorem proving (ATP) approaches that rely on formal proof systems and the strengths of LLMs in handling informal, natural language knowledge.

Method: Proposes DeepTheorem, which includes a large-scale benchmark dataset of informal theorems and proofs, and RL-Zero, a reinforcement learning strategy tailored for informal theorem proving. Comprehensive evaluation metrics are also proposed.

Result: DeepTheorem significantly improves LLM theorem-proving performance compared to existing datasets and supervised fine-tuning protocols, achieving state-of-the-art accuracy and reasoning quality.

Conclusion: DeepTheorem has the potential to fundamentally advance automated informal theorem proving and mathematical exploration.

Abstract: Theorem proving serves as a major testbed for evaluating complex reasoning
abilities in large language models (LLMs). However, traditional automated
theorem proving (ATP) approaches rely heavily on formal proof systems that
poorly align with LLMs' strength derived from informal, natural language
knowledge acquired during pre-training. In this work, we propose DeepTheorem, a
comprehensive informal theorem-proving framework exploiting natural language to
enhance LLM mathematical reasoning. DeepTheorem includes a large-scale
benchmark dataset consisting of 121K high-quality IMO-level informal theorems
and proofs spanning diverse mathematical domains, rigorously annotated for
correctness, difficulty, and topic categories, accompanied by systematically
constructed verifiable theorem variants. We devise a novel reinforcement
learning strategy (RL-Zero) explicitly tailored to informal theorem proving,
leveraging the verified theorem variants to incentivize robust mathematical
inference. Additionally, we propose comprehensive outcome and process
evaluation metrics examining proof correctness and the quality of reasoning
steps. Extensive experimental analyses demonstrate DeepTheorem significantly
improves LLM theorem-proving performance compared to existing datasets and
supervised fine-tuning protocols, achieving state-of-the-art accuracy and
reasoning quality. Our findings highlight DeepTheorem's potential to
fundamentally advance automated informal theorem proving and mathematical
exploration.

</details>


### [271] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)
*Heekyung Lee,Jiaxin Ge,Tsung-Han Wu,Minwoo Kang,Trevor Darrell,David M. Chan*

Main category: cs.CL

TL;DR: 当前视觉-语言模型（VLMs）在解决文字画谜题方面存在挑战，尽管对简单视觉线索有一定解码能力，但在抽象推理、横向思维和理解视觉隐喻等任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究视觉-语言模型在解读和解决文字画谜题方面的潜力，以评估其多模态抽象、符号推理以及文化和语音语言双关的能力。

Method: 构建一个手工生成和注释的基准测试集，包含各种英语文字画谜题，从简单的象形替代到基于空间提示的谜题。然后分析不同VLMs在此数据集上的表现。

Result: VLMs在解码简单视觉线索方面表现出一些令人惊讶的能力，但对需要抽象推理、横向思维和理解视觉隐喻的任务显得力不从心。

Conclusion: 当代VLMs虽然在处理简单视觉信息方面具备一定能力，但在面对需要复杂推理和理解的谜题时仍然有显著困难。

Abstract: Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.

</details>


### [272] [From Chat Logs to Collective Insights: Aggregative Question Answering](https://arxiv.org/abs/2505.23765)
*Wentao Zhang,Woojeong Kim,Yuntian Deng*

Main category: cs.CL

TL;DR: Conversational agents using LLMs generate vast conversational data which can reflect societal interests. However, current methods fail to fully aggregate and reason over this data. This paper introduces Aggregative Question Answering, a new task for models to answer aggregative queries by reasoning over thousands of user-chatbot interactions. A benchmark named WildChat-AQA is constructed, containing 6,027 questions from 182,330 real-world conversations. Experiments reveal existing methods are inefficient or computationally expensive, calling for new approaches.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current methods that treat user-chatbot interactions independently and miss critical insights from large-scale conversation logs.

Method: Introduced Aggregative Question Answering task and constructed WildChat-AQA benchmark with 6,027 aggregative questions derived from 182,330 real-world chatbot conversations.

Result: Existing methods struggle to effectively reason or are computationally prohibitive when handling the WildChat-AQA benchmark.

Conclusion: There is a need for new approaches capable of extracting collective insights from large-scale conversational data.

Abstract: Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.

</details>


### [273] [Diagnosing and Addressing Pitfalls in KG-RAG Datasets: Toward More Reliable Benchmarking](https://arxiv.org/abs/2505.23495)
*Liangliang Zhang,Zhuorui Jiang,Hongliang Chi,Haoyang Chen,Mohammed Elkoumy,Fali Wang,Qiong Wu,Zhengyi Zhou,Shirui Pan,Suhang Wang,Yao Ma*

Main category: cs.CL

TL;DR: The paper addresses the quality issues of KGQA benchmarks by introducing KGQAGen, an LLM-in-the-loop framework that produces challenging and verifiable QA instances. It constructs a new benchmark KGQAGen-10k and shows state-of-the-art systems struggle on it.


<details>
  <summary>Details</summary>
Motivation: Popular KGQA datasets suffer from critical quality issues, such as inaccurate annotations, ambiguous questions, and outdated knowledge. This hinders reliable evaluation of multi-hop reasoning in KGQA systems.

Method: KGQAGen combines structured knowledge grounding, LLM-guided generation, and symbolic verification to create high-quality QA instances. It is used to construct KGQAGen-10k, a large-scale benchmark grounded in Wikidata.

Result: Experimental results show that state-of-the-art KG-RAG models struggle on the KGQAGen-10k benchmark, exposing limitations of existing models.

Conclusion: The study advocates for more rigorous benchmark construction in KGQA and positions KGQAGen as a scalable framework for advancing evaluation in this field.

Abstract: Knowledge Graph Question Answering (KGQA) systems rely on high-quality
benchmarks to evaluate complex multi-hop reasoning. However, despite their
widespread use, popular datasets such as WebQSP and CWQ suffer from critical
quality issues, including inaccurate or incomplete ground-truth annotations,
poorly constructed questions that are ambiguous, trivial, or unanswerable, and
outdated or inconsistent knowledge. Through a manual audit of 16 popular KGQA
datasets, including WebQSP and CWQ, we find that the average factual
correctness rate is only 57 %. To address these issues, we introduce KGQAGen,
an LLM-in-the-loop framework that systematically resolves these pitfalls.
KGQAGen combines structured knowledge grounding, LLM-guided generation, and
symbolic verification to produce challenging and verifiable QA instances. Using
KGQAGen, we construct KGQAGen-10k, a ten-thousand scale benchmark grounded in
Wikidata, and evaluate a diverse set of KG-RAG models. Experimental results
demonstrate that even state-of-the-art systems struggle on this benchmark,
highlighting its ability to expose limitations of existing models. Our findings
advocate for more rigorous benchmark construction and position KGQAGen as a
scalable framework for advancing KGQA evaluation.

</details>


### [274] [Are Reasoning Models More Prone to Hallucination?](https://arxiv.org/abs/2505.23646)
*Zijun Yao,Yantao Liu,Yanxu Chen,Jianhui Chen,Junfeng Fang,Lei Hou,Juanzi Li,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 最近发展的大型推理模型（LRMs）在解决复杂任务时表现出强大的长链推理能力。然而，这些模型是否能将推理能力推广到减少事实追寻任务中的幻觉尚不清楚。本文从三个角度探讨了这个问题：1）整体评估LRMs的幻觉情况，发现带有冷启动微调和可验证奖励强化学习的完整后训练流程通常可以减轻幻觉；2）通过行为分析，识别出两种影响LRMs事实性的关键认知行为：缺陷重复和思考-回答不匹配；3）从模型不确定性角度研究LRMs幻觉的机制，发现增加的幻觉通常与模型不确定性和事实准确性之间的错位有关。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂任务上表现出色，但它们在事实追寻任务中是否能减少幻觉尚未明确。这促使研究者探讨LRMs是否更容易产生幻觉以及其背后的原因。

Method: 1. 对LRMs进行整体幻觉评估，比较不同后训练流程对幻觉的影响；2. 进行行为分析，识别影响LRMs事实性的关键认知行为；3. 从模型不确定性的角度研究LRMs幻觉的机制。

Result: 研究表明，带有冷启动微调和可验证奖励强化学习的完整后训练流程通常可以减轻LRMs的幻觉，而单独蒸馏或无冷启动微调的强化学习会引入更复杂的幻觉。此外，缺陷重复和思考-回答不匹配是影响LRMs事实性的关键认知行为，而LRMs的幻觉增加通常与模型不确定性和事实准确性之间的错位有关。

Conclusion: 本文提供了对LRMs幻觉的初步理解，表明通过适当的后训练流程可以减轻幻觉，并揭示了导致幻觉的关键认知行为和机制。

Abstract: Recently evolved large reasoning models (LRMs) show powerful performance in
solving complex tasks with long chain-of-thought (CoT) reasoning capability. As
these LRMs are mostly developed by post-training on formal reasoning tasks,
whether they generalize the reasoning capability to help reduce hallucination
in fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1
reports increased performance on SimpleQA, a fact-seeking benchmark, while
OpenAI-o3 observes even severer hallucination. This discrepancy naturally
raises the following research question: Are reasoning models more prone to
hallucination? This paper addresses the question from three perspectives. (1)
We first conduct a holistic evaluation for the hallucination in LRMs. Our
analysis reveals that LRMs undergo a full post-training pipeline with cold
start supervised fine-tuning (SFT) and verifiable reward RL generally alleviate
their hallucination. In contrast, both distillation alone and RL training
without cold start fine-tuning introduce more nuanced hallucinations. (2) To
explore why different post-training pipelines alters the impact on
hallucination in LRMs, we conduct behavior analysis. We characterize two
critical cognitive behaviors that directly affect the factuality of a LRM: Flaw
Repetition, where the surface-level reasoning attempts repeatedly follow the
same underlying flawed logic, and Think-Answer Mismatch, where the final answer
fails to faithfully match the previous CoT process. (3) Further, we investigate
the mechanism behind the hallucination of LRMs from the perspective of model
uncertainty. We find that increased hallucination of LRMs is usually associated
with the misalignment between model uncertainty and factual accuracy. Our work
provides an initial understanding of the hallucination in LRMs.

</details>


### [275] [LoLA: Low-Rank Linear Attention With Sparse Caching](https://arxiv.org/abs/2505.23666)
*Luke McDermott,Robert W. Heath Jr.,Rahul Parhi*

Main category: cs.CL

TL;DR: Transformer-based large language models face challenges with quadratic complexity for long sequences. LoLA (Low-rank Linear Attention with sparse caching) addresses this by storing additional key-value pairs and distributing them into three memory forms, enhancing performance on long context-length tasks while being lightweight and reproducible on consumer GPUs.


<details>
  <summary>Details</summary>
Motivation: To overcome the quadratic complexity issue in transformer-based large language models during inference on long sequences, as well as to address the shortcomings of linear attention methods which fail to accurately approximate softmax attention and suffer from 'memory collisions'.

Method: Propose LoLA which incorporates sliding window attention into each linear attention head, separately stores interfering key-value pairs, and distributes past key-value pairs into three forms of memory: recent pairs in a local sliding window, difficult-to-memorize pairs in a global cache, and generic pairs in the recurrent hidden state.

Result: LoLA achieves significant improvements in accuracy for long context-length tasks, boosting base subquadratic model accuracy from 0.6% to 97.4% at 4K context lengths, with a much smaller cache than Llama-3.1 8B. It also performs strongly in zero-shot commonsense reasoning tasks.

Conclusion: LoLA is an effective and lightweight approach that enhances the capabilities of linear attention models, making it possible to handle long context-length tasks efficiently and with high accuracy.

Abstract: Transformer-based large language models suffer from quadratic complexity at
inference on long sequences. Linear attention methods are efficient
alternatives, however, they fail to provide an accurate approximation of
softmax attention. By additionally incorporating sliding window attention into
each linear attention head, this gap can be closed for short context-length
tasks. Unfortunately, these approaches cannot recall important information from
long contexts due to "memory collisions". In this paper , we propose LoLA:
Low-rank Linear Attention with sparse caching. LoLA separately stores
additional key-value pairs that would otherwise interfere with past associative
memories. Moreover, LoLA further closes the gap between linear attention models
and transformers by distributing past key-value pairs into three forms of
memory: (i) recent pairs in a local sliding window; (ii) difficult-to-memorize
pairs in a sparse, global cache; and (iii) generic pairs in the recurrent
hidden state of linear attention. As an inference-only strategy, LoLA enables
pass-key retrieval on up to 8K context lengths on needle-in-a-haystack tasks
from RULER. It boosts the accuracy of the base subquadratic model from 0.6% to
97.4% at 4K context lengths, with a 4.6x smaller cache than that of Llama-3.1
8B. LoLA demonstrates strong performance on zero-shot commonsense reasoning
tasks among 1B and 8B parameter subquadratic models. Finally, LoLA is an
extremely lightweight approach: Nearly all of our results can be reproduced on
a single consumer GPU.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [276] [A Tool for Generating Exceptional Behavior Tests With Large Language Models](https://arxiv.org/abs/2505.22818)
*Linghan Zhong,Samuel Yuan,Jiyang Zhang,Yu Liu,Pengyu Nie,Junyi Jessy Li,Milos Gligoric*

Main category: cs.SE

TL;DR: exLong is a framework based on fine-tuned LLM from CodeLlama that automatically generates Exceptional behavior tests (EBTs) by analyzing exception-throwing traces and conditional expressions to enhance software testing.


<details>
  <summary>Details</summary>
Motivation: Developers often prioritize testing 'happy paths' rather than exceptional scenarios, leading to insufficient EBTs.

Method: exLong uses a fine-tuned LLM from CodeLlama and analyzes exception-throwing traces, conditional expressions guarding throw statements, and non-exceptional behavior tests executing similar traces.

Result: exLong effectively assists developers in creating comprehensive EBTs for their projects.

Conclusion: exLong addresses the gap of insufficient EBTs by providing an automated framework to generate these tests.

Abstract: Exceptional behavior tests (EBTs) are crucial in software development for
verifying that code correctly handles unwanted events and throws appropriate
exceptions. However, prior research has shown that developers often prioritize
testing "happy paths", e.g., paths without unwanted events over exceptional
scenarios. We present exLong, a framework that automatically generates EBTs to
address this gap. exLong leverages a large language model (LLM) fine-tuned from
CodeLlama and incorporates reasoning about exception-throwing traces,
conditional expressions that guard throw statements, and non-exceptional
behavior tests that execute similar traces. Our demonstration video illustrates
how exLong can effectively assist developers in creating comprehensive EBTs for
their project (available at https://youtu.be/Jro8kMgplZk).

</details>


### [277] [OSS-UAgent: An Agent-based Usability Evaluation Framework for Open Source Software](https://arxiv.org/abs/2505.23239)
*Lingkai Meng,Yu Shao,Long Yuan,Longbin Lai,Peng Cheng,Wenyuan Yu,Wenjie Zhang,Xuemin Lin,Jingren Zhou*

Main category: cs.SE

TL;DR: OSS-UAgent is an automated usability evaluation framework for OSS using LLM-powered agents, offering scalable and cost-effective evaluations across developer experience levels.


<details>
  <summary>Details</summary>
Motivation: Usability evaluation is crucial for OSS impact and adoption, but traditional methods are costly and not scalable.

Method: The framework uses intelligent agents driven by LLMs to simulate developers at various experience levels performing tasks. It dynamically constructs platform-specific knowledge bases for context-aware code generation and evaluates generated code on compliance, correctness, and readability.

Result: OSS-UAgent provides a comprehensive measure of software usability and has been demonstrated as effective in evaluating graph analytics platforms.

Conclusion: OSS-UAgent successfully automates usability evaluations for OSS, addressing the limitations of traditional methods.

Abstract: Usability evaluation is critical to the impact and adoption of open source
software (OSS), yet traditional methods relying on human evaluators suffer from
high costs and limited scalability. To address these limitations, we introduce
OSS-UAgent, an automated, configurable, and interactive agent-based usability
evaluation framework specifically designed for open source software. Our
framework employs intelligent agents powered by large language models (LLMs) to
simulate developers performing programming tasks across various experience
levels (from Junior to Expert). By dynamically constructing platform-specific
knowledge bases, OSS-UAgent ensures accurate and context-aware code generation.
The generated code is automatically evaluated across multiple dimensions,
including compliance, correctness, and readability, providing a comprehensive
measure of the software's usability. Additionally, our demonstration showcases
OSS-UAgent's practical application in evaluating graph analytics platforms,
highlighting its effectiveness in automating usability evaluation.

</details>


### [278] [Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization](https://arxiv.org/abs/2505.23387)
*Mingzhe Du,Luu Tuan Tuan,Yue Liu,Yuhao Qing,Dong Huang,Xinyi He,Qian Liu,Zejun Ma,See-kiong Ng*

Main category: cs.SE

TL;DR: Large Language Models (LLMs) often lack code efficiency despite generating functionally correct solutions. This paper presents a test-time iterative optimization framework using a closed-loop system with an execution sandbox for LLMs to refine code based on performance feedback. Three training strategies are explored: SFT, DPO, and GRPO. Experiments show that GRPO, leveraging reinforcement learning with execution feedback, significantly improves both pass@1 rates and the likelihood of outperforming human submissions in efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the issue of code efficiency in Large Language Models which is crucial for real-world deployment.

Method: Introduction of a novel test-time iterative optimization framework utilizing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. Exploration of three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO).

Result: Experiments indicate that SFT and DPO quickly reach a plateau in efficiency gains, whereas GRPO, using reinforcement learning with execution feedback, continuously optimizes code performance leading to significant improvements in both pass@1 rates (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%).

Conclusion: This work showcases effective methods for improving code efficiency at test-time and highlights the potential of reinforcement learning in enabling LLMs to self-improve their code efficiency.

Abstract: Large Language Models (LLMs) generate functionally correct solutions but
often fall short in code efficiency, a critical bottleneck for real-world
deployment. In this paper, we introduce a novel test-time iterative
optimization framework to address this, employing a closed-loop system where
LLMs iteratively refine code based on empirical performance feedback from an
execution sandbox. We explore three training strategies: Supervised Fine-Tuning
(SFT), Direct Preference Optimization (DPO), and Group Relative Policy
Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark
show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,
using reinforcement learning (RL) with execution feedback, continuously
optimizes code performance, significantly boosting both pass@1 (from 47% to
62%) and the likelihood of outperforming human submissions in efficiency (from
31% to 45%). Our work demonstrates effective test-time code efficiency
improvement and critically reveals the power of RL in teaching LLMs to truly
self-improve code efficiency.

</details>


### [279] [Toward Effective AI Governance: A Review of Principles](https://arxiv.org/abs/2505.23417)
*Danilo Ribeiro,Thayssa Rocha,Gustavo Pinto,Bruno Cartaxo,Marcelo Amaral,Nicole Davila,Ana Camargo*

Main category: cs.SE

TL;DR: AI治理是确保AI系统负责任、道德和安全开发与部署的实践。本文通过快速三级审查方法，分析了九个来自IEEE和ACM的同行评审的二级研究，确定了在AI治理文献中强调的框架、原则、机制和利益相关者角色。结果表明，引用最多的框架包括欧盟AI法案和NIST RMF；透明度和问责制是最常见的原则。然而，很少有评论详细说明可操作的治理机制或利益相关者策略。本综述巩固了AI治理的关键方向，并指出了实证验证和包容性方面的差距，为学术探究和组织的实际应用提供了依据。


<details>
  <summary>Details</summary>
Motivation: 当前关于AI治理的文献缺乏对治理框架和实践的综合分析。为了填补这一空白，明确哪些框架、原则、机制和利益相关者角色在AI治理的二级文献中被重视。

Method: 采用快速三级审查方法，评估了2020年至2024年间来自IEEE和ACM的九项同行评审的二级研究。使用结构化的纳入标准和主题语义综合进行分析。

Result: 最常被引用的框架是欧盟AI法案和NIST风险管理框架（RMF），而透明度和问责制是最普遍的原则。然而，对于可操作的治理机制或利益相关者的策略，鲜有评论进行详细说明。

Conclusion: 该综述总结了AI治理中的关键方向，同时指出在实证验证和包容性方面存在不足。这些发现将有助于推动学术研究以及促进组织内实际应用的发展。

Abstract: Artificial Intelligence (AI) governance is the practice of establishing
frameworks, policies, and procedures to ensure the responsible, ethical, and
safe development and deployment of AI systems. Although AI governance is a core
pillar of Responsible AI, current literature still lacks synthesis across such
governance frameworks and practices. Objective: To identify which frameworks,
principles, mechanisms, and stakeholder roles are emphasized in secondary
literature on AI governance. Method: We conducted a rapid tertiary review of
nine peer-reviewed secondary studies from IEEE and ACM (20202024), using
structured inclusion criteria and thematic semantic synthesis. Results: The
most cited frameworks include the EU AI Act and NIST RMF; transparency and
accountability are the most common principles. Few reviews detail actionable
governance mechanisms or stakeholder strategies. Conclusion: The review
consolidates key directions in AI governance and highlights gaps in empirical
validation and inclusivity. Findings inform both academic inquiry and practical
adoption in organizations.

</details>


### [280] [SWE-bench Goes Live!](https://arxiv.org/abs/2505.23419)
*Linghao Zhang,Shilin He,Chaoyun Zhang,Yu Kang,Bowen Li,Chengxing Xie,Junhao Wang,Maoquan Wang,Yufan Huang,Shengyu Fu,Elsie Nallipogu,Qingwei Lin,Yingnong Dang,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.SE

TL;DR: SWE-bench-Live是一个可实时更新的基准测试，包含1,319个任务和专用的Docker镜像，旨在克服现有基准的关键限制。通过自动化策展管道（method），它实现了从实例创建到环境设置的全流程自动化，揭示了在动态真实世界软件开发环境中评估大型语言模型和代理时的显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试如SWE-bench及其变体存在关键局限性：未更新、覆盖范围窄、依赖手动工作，这限制了可扩展性并引入过拟合和数据污染的风险。因此需要一个新的基准来解决这些问题。

Method: 提出了一种自动化策展管道（method），简化了从实例创建到环境设置的整个过程，去除了手动瓶颈，实现了可扩展性和持续更新。

Result: 在SWE-bench-Live上评估了一系列最先进的代理框架和大型语言模型，发现与静态基准相比存在显著的性能差距。通过详细分析表明，这种差异与代码库来源、问题最近性和任务难度有关。

Conclusion: SWE-bench-Live提供了一个新颖、多样且可执行的基准，基于实时代码库活动，有助于在动态、真实世界的软件开发环境中对大型语言模型和代理进行严格的、抗污染的评估。

Abstract: The issue-resolving task, where a model generates patches to fix real-world
bugs, has emerged as a critical benchmark for evaluating the capabilities of
large language models (LLMs). While SWE-bench and its variants have become
standard in this domain, they suffer from key limitations: they have not been
updated since their initial releases, cover a narrow set of repositories, and
depend heavily on manual effort for instance construction and environment
setup. These factors hinder scalability and introduce risks of overfitting and
data contamination. In this work, we present \textbf{SWE-bench-Live}, a
\textit{live-updatable} benchmark designed to overcome these challenges. Our
initial release consists of 1,319 tasks derived from real GitHub issues created
since 2024, spanning 93 repositories. Each task is accompanied by a dedicated
Docker image to ensure reproducible execution. Central to our benchmark is
\method, an automated curation pipeline that streamlines the entire process
from instance creation to environment setup, removing manual bottlenecks and
enabling scalability and continuous updates. We evaluate a range of
state-of-the-art agent frameworks and LLMs on SWE-bench-Live, revealing a
substantial performance gap compared to static benchmarks like SWE-bench, even
under controlled evaluation conditions. To better understand this discrepancy,
we perform detailed analyses across repository origin, issue recency, and task
difficulty. By providing a fresh, diverse, and executable benchmark grounded in
live repository activity, SWE-bench-Live facilitates rigorous,
contamination-resistant evaluation of LLMs and agents in dynamic, real-world
software development settings.

</details>


### [281] [From Knowledge to Noise: CTIM-Rover and the Pitfalls of Episodic Memory in Software Engineering Agents](https://arxiv.org/abs/2505.23422)
*Tobias Lindenbauer,Georg Groh,Hinrich Schütze*

Main category: cs.SE

TL;DR: The paper introduces CTIM-Rover, an AI agent for Software Engineering that extends AutoCodeRover with Cross-Task-Instance Memory (CTIM). Despite expectations, CTIM-Rover does not outperform AutoCodeRover in any configuration. Analysis indicates noise introduced by distracting CTIM items or exemplar trajectories as the source of performance degradation.


<details>
  <summary>Details</summary>
Motivation: Existing SE agents discard their long-term memory after a single task instance, leading to inefficiency. Repository-level understanding is crucial for bug fixes, thus there's potential benefit from incorporating Cross-Task-Instance Memory (CTIM) for SE tasks.

Method: The researchers built CTIM-Rover on top of AutoCodeRover and incorporated repository-level CTIM using a Mixture-Of-Experts inspired approach based on Experiential Learning (ExpeL). This was intended to retain and leverage long-term memory across multiple task instances.

Result: CTIM-Rover did not outperform AutoCodeRover in any configuration tested. Performance degradation was attributed to noise introduced by distracting CTIM items or exemplar trajectories.

Conclusion: ExpeL and DoT-Bank do not scale well to real-world SE problems due to performance degradation caused by noise in CTIM items.

Abstract: We introduce CTIM-Rover, an AI agent for Software Engineering (SE) built on
top of AutoCodeRover (Zhang et al., 2024) that extends agentic reasoning
frameworks with an episodic memory, more specifically, a general and
repository-level Cross-Task-Instance Memory (CTIM). While existing open-source
SE agents mostly rely on ReAct (Yao et al., 2023b), Reflexion (Shinn et al.,
2023), or Code-Act (Wang et al., 2024), all of these reasoning and planning
frameworks inefficiently discard their long-term memory after a single task
instance. As repository-level understanding is pivotal for identifying all
locations requiring a patch for fixing a bug, we hypothesize that SE is
particularly well positioned to benefit from CTIM. For this, we build on the
Experiential Learning (EL) approach ExpeL (Zhao et al., 2024), proposing a
Mixture-Of-Experts (MoEs) inspired approach to create both a general-purpose
and repository-level CTIM. We find that CTIM-Rover does not outperform
AutoCodeRover in any configuration and thus conclude that neither ExpeL nor
DoT-Bank (Lingam et al., 2024) scale to real-world SE problems. Our analysis
indicates noise introduced by distracting CTIM items or exemplar trajectories
as the likely source of the performance degradation.

</details>


### [282] [GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents](https://arxiv.org/abs/2505.23671)
*Manish Shetty,Naman Jain,Jinjian Liu,Vijay Kethanaboyina,Koushik Sen,Ion Stoica*

Main category: cs.SE

TL;DR: A new benchmark GSO is created to assess language models' ability in high-performance software development. Despite having diverse optimization tasks, current SWE-Agents show significant struggles with less than 5% success rate. Key failure modes are identified.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the capabilities of language models in developing high-performance software.

Method: Developed GSO benchmark which includes an automated pipeline to generate performance tests, analyze repository commit histories, and identify challenging optimization tasks across multiple codebases and languages. Agents are tasked to enhance runtime efficiency compared to expert developer optimizations.

Result: Quantitative evaluation shows leading SWE-Agents have less than 5% success rate with limited improvements from inference-time scaling. Qualitative analysis uncovers key failure modes such as issues with low-level languages, lazy optimization strategies, and difficulty in localizing bottlenecks.

Conclusion: GSO provides a valuable benchmark for future research on improving language models' abilities in high-performance software development.

Abstract: Developing high-performance software is a complex task that requires
specialized expertise. We introduce GSO, a benchmark for evaluating language
models' capabilities in developing high-performance software. We develop an
automated pipeline that generates and executes performance tests to analyze
repository commit histories to identify 102 challenging optimization tasks
across 10 codebases, spanning diverse domains and programming languages. An
agent is provided with a codebase and performance test as a precise
specification, and tasked to improve the runtime efficiency, which is measured
against the expert developer optimization. Our quantitative evaluation reveals
that leading SWE-Agents struggle significantly, achieving less than 5% success
rate, with limited improvements even with inference-time scaling. Our
qualitative analysis identifies key failure modes, including difficulties with
low-level languages, practicing lazy optimization strategies, and challenges in
accurately localizing bottlenecks. We release the code and artifacts of our
benchmark along with agent trajectories to enable future research.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [283] [ConnectomeDiffuser: Generative AI Enables Brain Network Construction from Diffusion Tensor Imaging](https://arxiv.org/abs/2505.22683)
*Xuhang Chen,Michael Kwok-Po Ng,Kim-Fung Tsang,Chi-Man Pun,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: ConnectomeDiffuser是一种新的基于扩散的框架，用于从DTI自动构建端到端脑网络。它通过结合模板网络、扩散模型和图卷积网络分类器来捕获更广泛的结构连接和病理相关的信息，从而改进了诊断准确性和对神经退行性疾病的治疗监测。


<details>
  <summary>Details</summary>
Motivation: 现有的构建结构性脑网络的方法存在操作者主观性、劳动密集型工作流程以及捕捉复杂拓扑特征和疾病特异性生物标志物的能力有限等问题。

Method: ConnectomeDiffuser结合了三个关键组件：1) 模板网络，使用黎曼几何原理从3D DTI扫描中提取拓扑特征；2) 扩散模型，生成具有增强拓扑保真度的全面脑网络；3) 图卷积网络分类器，结合疾病特异性标记以提高诊断准确性。

Result: 在代表两种不同神经退行性疾病条件的数据集上的实验验证表明，与其它脑网络方法相比，性能有显著提升。

Conclusion: 这项工作为神经病学领域的仪器发展做出了贡献，为临床医生和研究人员提供了一个强大且可推广的测量框架，有助于更准确的诊断、更深的机制理解以及对神经退行性疾病（如阿尔茨海默病）改善的治疗监测。

Abstract: Brain network analysis plays a crucial role in diagnosing and monitoring
neurodegenerative disorders such as Alzheimer's disease (AD). Existing
approaches for constructing structural brain networks from diffusion tensor
imaging (DTI) often rely on specialized toolkits that suffer from inherent
limitations: operator subjectivity, labor-intensive workflows, and restricted
capacity to capture complex topological features and disease-specific
biomarkers. To overcome these challenges and advance computational neuroimaging
instrumentation, ConnectomeDiffuser is proposed as a novel diffusion-based
framework for automated end-to-end brain network construction from DTI. The
proposed model combines three key components: (1) a Template Network that
extracts topological features from 3D DTI scans using Riemannian geometric
principles, (2) a diffusion model that generates comprehensive brain networks
with enhanced topological fidelity, and (3) a Graph Convolutional Network
classifier that incorporates disease-specific markers to improve diagnostic
accuracy. ConnectomeDiffuser demonstrates superior performance by capturing a
broader range of structural connectivity and pathology-related information,
enabling more sensitive analysis of individual variations in brain networks.
Experimental validation on datasets representing two distinct neurodegenerative
conditions demonstrates significant performance improvements over other brain
network methods. This work contributes to the advancement of instrumentation in
the context of neurological disorders, providing clinicians and researchers
with a robust, generalizable measurement framework that facilitates more
accurate diagnosis, deeper mechanistic understanding, and improved therapeutic
monitoring of neurodegenerative diseases such as AD.

</details>


### [284] [Self-orthogonalizing attractor neural networks emerging from the free energy principle](https://arxiv.org/abs/2505.22749)
*Tamas Spisak,Karl Friston*

Main category: q-bio.NC

TL;DR: The paper explores how attractor networks emerge from the free energy principle in random dynamical systems, leading to self-organizing, multi-level Bayesian active inference processes. It establishes that these networks favor orthogonalized attractor representations and offers insights for AI and neuroscience.


<details>
  <summary>Details</summary>
Motivation: To understand how self-organizing dynamics, like attractor dynamics, emerge from first principles in complex systems such as the brain, which is crucial for neuronal computations and artificial intelligence systems.

Method: By applying the free energy principle to a universal partitioning of random dynamical systems, the approach eliminates the need for explicitly imposed learning and inference rules, instead identifying emergent dynamics that lead to a collective, multi-level Bayesian active inference process.

Result: The proposed networks favor approximately orthogonalized attractor representations, efficiently spanning the input subspace, enhancing generalization and mutual information between hidden causes and observable effects. Sequential data fosters asymmetric couplings and non-equilibrium steady-state dynamics.

Conclusion: The findings present a unifying theory of self-organizing attractor networks, providing novel insights applicable to both artificial intelligence and neuroscience.

Abstract: Attractor dynamics are a hallmark of many complex systems, including the
brain. Understanding how such self-organizing dynamics emerge from first
principles is crucial for advancing our understanding of neuronal computations
and the design of artificial intelligence systems. Here we formalize how
attractor networks emerge from the free energy principle applied to a universal
partitioning of random dynamical systems. Our approach obviates the need for
explicitly imposed learning and inference rules and identifies emergent, but
efficient and biologically plausible inference and learning dynamics for such
self-organizing systems. These result in a collective, multi-level Bayesian
active inference process. Attractors on the free energy landscape encode prior
beliefs; inference integrates sensory data into posterior beliefs; and learning
fine-tunes couplings to minimize long-term surprise. Analytically and via
simulations, we establish that the proposed networks favor approximately
orthogonalized attractor representations, a consequence of simultaneously
optimizing predictive accuracy and model complexity. These attractors
efficiently span the input subspace, enhancing generalization and the mutual
information between hidden causes and observable effects. Furthermore, while
random data presentation leads to symmetric and sparse couplings, sequential
data fosters asymmetric couplings and non-equilibrium steady-state dynamics,
offering a natural extension to conventional Boltzmann Machines. Our findings
offer a unifying theory of self-organizing attractor networks, providing novel
insights for AI and neuroscience.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [285] [Differentially Private Space-Efficient Algorithms for Counting Distinct Elements in the Turnstile Model](https://arxiv.org/abs/2505.23682)
*Rachel Cummings,Alessandro Epasto,Jieming Mao,Tamalika Mukherjee,Tingting Ou,Peilin Zhong*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The turnstile continual release model of differential privacy captures
scenarios where a privacy-preserving real-time analysis is sought for a dataset
evolving through additions and deletions. In typical applications of real-time
data analysis, both the length of the stream $T$ and the size of the universe
$|U|$ from which data come can be extremely large. This motivates the study of
private algorithms in the turnstile setting using space sublinear in both $T$
and $|U|$. In this paper, we give the first sublinear space differentially
private algorithms for the fundamental problem of counting distinct elements in
the turnstile streaming model. Our algorithm achieves, on arbitrary streams,
$\tilde{O}_{\eta}(T^{1/3})$ space and additive error, and a $(1+\eta)$-relative
approximation for all $\eta \in (0,1)$. Our result significantly improves upon
the space requirements of the state-of-the-art algorithms for this problem,
which is linear, approaching the known $\Omega(T^{1/4})$ additive error lower
bound for arbitrary streams. Moreover, when a bound $W$ on the number of times
an item appears in the stream is known, our algorithm provides
$\tilde{O}_{\eta}(\sqrt{W})$ additive error, using $\tilde{O}_{\eta}(\sqrt{W})$
space. This additive error asymptotically matches that of prior work which
required instead linear space. Our results address an open question posed by
[Jain, Kalemaj, Raskhodnikova, Sivakumar, Smith, Neurips23] about designing
low-memory mechanisms for this problem. We complement these results with a
space lower bound for this problem, which shows that any algorithm that uses
similar techniques must use space $\tilde{\Omega}(T^{1/3})$ on arbitrary
streams.

</details>


### [286] [Improved Learning via k-DTW: A Novel Dissimilarity Measure for Curves](https://arxiv.org/abs/2505.23431)
*Amer Krivošija,Alexander Munteanu,André Nusser,Chris Schwiegelshohn*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper introduces $k$-Dynamic Time Warping ($k$-DTW), a novel
dissimilarity measure for polygonal curves. $k$-DTW has stronger metric
properties than Dynamic Time Warping (DTW) and is more robust to outliers than
the Fr\'{e}chet distance, which are the two gold standards of dissimilarity
measures for polygonal curves. We show interesting properties of $k$-DTW and
give an exact algorithm as well as a $(1+\varepsilon)$-approximation algorithm
for $k$-DTW by a parametric search for the $k$-th largest matched distance. We
prove the first dimension-free learning bounds for curves and further learning
theoretic results. $k$-DTW not only admits smaller sample size than DTW for the
problem of learning the median of curves, where some factors depending on the
curves' complexity $m$ are replaced by $k$, but we also show a surprising
separation on the associated Rademacher and Gaussian complexities: $k$-DTW
admits strictly smaller bounds than DTW, by a factor $\tilde\Omega(\sqrt{m})$
when $k\ll m$. We complement our theoretical findings with an experimental
illustration of the benefits of using $k$-DTW for clustering and nearest
neighbor classification.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [287] [Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion](https://arxiv.org/abs/2505.22673)
*Wasif Khan,Kyle B. See,Simon Kato,Ziqian Huang,Amy Lazarte,Kyle Douglas,Xiangyang Lou,Teng J. Peng,Dhanashree Rajderkar,John Rees,Pina Sanelli,Amita Singh,Ibrahim Tuna,Christina A. Wilson,Ruogu Fang*

Main category: q-bio.TO

TL;DR: A new deep learning framework, MAGIC, transforms non-contrast CT images into CTP maps without using contrast agents, reducing costs and side effects while maintaining diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of using contrast agents in CTP imaging such as allergic reactions, adverse side effects, and high costs.

Method: The proposed framework, MAGIC, combines generative artificial intelligence and physiological information to map non-contrast CT imaging to multiple contrast-free CTP imaging maps. Enhanced image fidelity is achieved by incorporating physiological characteristics into the loss terms.

Result: MAGIC demonstrated robustness to abnormalities in brain perfusion activity and showed favorable performance compared to clinical perfusion imaging with intravenous contrast injection in a double-blinded study with experienced professionals.

Conclusion: MAGIC has the potential to revolutionize healthcare by providing contrast-free, cost-effective, and rapid perfusion imaging.

Abstract: Perfusion imaging is extensively utilized to assess hemodynamic status and
tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging
plays a key role in the early assessment and planning of stroke treatment.
While CTP provides essential perfusion parameters to identify abnormal blood
flow in the brain, the use of contrast agents in CTP can lead to allergic
reactions and adverse side effects, along with costing USD 4.9 billion
worldwide in 2022. To address these challenges, we propose a novel deep
learning framework called Multitask Automated Generation of Intermodal CT
perfusion maps (MAGIC). This framework combines generative artificial
intelligence and physiological information to map non-contrast computed
tomography (CT) imaging to multiple contrast-free CTP imaging maps. We
demonstrate enhanced image fidelity by incorporating physiological
characteristics into the loss terms. Our network was trained and validated
using CT image data from patients referred for stroke at UF Health and
demonstrated robustness to abnormalities in brain perfusion activity. A
double-blinded study was conducted involving seven experienced
neuroradiologists and vascular neurologists. This study validated MAGIC's
visual quality and diagnostic accuracy showing favorable performance compared
to clinical perfusion imaging with intravenous contrast injection. Overall,
MAGIC holds great promise in revolutionizing healthcare by offering
contrast-free, cost-effective, and rapid perfusion imaging.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [288] [Sustainable Carbon-Aware and Water-Efficient LLM Scheduling in Geo-Distributed Cloud Datacenters](https://arxiv.org/abs/2505.23554)
*Hayden Moore,Sirui Qi,Ninad Hogade,Dejan Milojicic,Cullen Bash,Sudeep Pasricha*

Main category: cs.DC

TL;DR: 随着大型语言模型（LLM）的广泛应用，其推理阶段的运营成本和环境影响日益显著。本文提出了一种名为SLIT的新框架，旨在通过优化服务质量、碳排放、水资源使用和能源成本来提高LLM在地理分布式云数据中心托管的可持续性。


<details>
  <summary>Details</summary>
Motivation: 尽管许多研究集中在减少LLM的训练开销上，但处理用户请求所带来的环境影响正逐渐成为关注焦点。推理阶段的运营成本可能比训练成本高出25倍，且每次推理请求都会消耗大量淡水并产生显著的碳足迹。

Method: SLIT框架利用基于机器学习的元启发式算法，优化LLM在地理分布式云数据中心托管时的服务质量（首令牌生成时间）、碳排放、水资源使用和能源成本。

Result: 该框架可以有效增强LLM托管的可持续性，并降低对环境的影响。

Conclusion: 随着LLM的普及，像SLIT这样的框架将变得越来越重要，以确保在满足服务质量的同时实现更高的环保目标。

Abstract: In recent years, Large Language Models (LLM) such as ChatGPT, CoPilot, and
Gemini have been widely adopted in different areas. As the use of LLMs
continues to grow, many efforts have focused on reducing the massive training
overheads of these models. But it is the environmental impact of handling user
requests to LLMs that is increasingly becoming a concern. Recent studies
estimate that the costs of operating LLMs in their inference phase can exceed
training costs by 25x per year. As LLMs are queried incessantly, the cumulative
carbon footprint for the operational phase has been shown to far exceed the
footprint during the training phase. Further, estimates indicate that 500 ml of
fresh water is expended for every 20-50 requests to LLMs during inference. To
address these important sustainability issues with LLMs, we propose a novel
framework called SLIT to co-optimize LLM quality of service (time-to-first
token), carbon emissions, water usage, and energy costs. The framework utilizes
a machine learning (ML) based metaheuristic to enhance the sustainability of
LLM hosting across geo-distributed cloud datacenters. Such a framework will
become increasingly vital as LLMs proliferate.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [289] [Finite-Sample Convergence Bounds for Trust Region Policy Optimization in Mean-Field Games](https://arxiv.org/abs/2505.22781)
*Antonio Ocello,Daniil Tiapkin,Lorenzo Mancini,Mathieu Laurière,Eric Moulines*

Main category: stat.ML

TL;DR: The paper introduces MF-TRPO, an algorithm for finding approximate Nash equilibria in Mean-Field Games, extending TRPO's success to multi-agent settings with theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: To develop a robust method for computing Nash equilibria in complex multi-agent systems using the Mean-Field Game framework, leveraging the strengths of TRPO from reinforcement learning.

Method: MF-TRPO algorithm is designed by extending TRPO to Mean-Field Games, ensuring stability and providing theoretical analysis for both exact and sample-based versions.

Result: Theoretical guarantees on convergence are provided, including high-probability and finite sample complexity results for the sample-based version.

Conclusion: MF-TRPO successfully bridges reinforcement learning techniques with mean-field decision-making, offering a reliable approach to solving multi-agent problems.

Abstract: We introduce Mean-Field Trust Region Policy Optimization (MF-TRPO), a novel
algorithm designed to compute approximate Nash equilibria for ergodic
Mean-Field Games (MFG) in finite state-action spaces. Building on the
well-established performance of TRPO in the reinforcement learning (RL)
setting, we extend its methodology to the MFG framework, leveraging its
stability and robustness in policy optimization. Under standard assumptions in
the MFG literature, we provide a rigorous analysis of MF-TRPO, establishing
theoretical guarantees on its convergence. Our results cover both the exact
formulation of the algorithm and its sample-based counterpart, where we derive
high-probability guarantees and finite sample complexity. This work advances
MFG optimization by bridging RL techniques with mean-field decision-making,
offering a theoretically grounded approach to solving complex multi-agent
problems.

</details>


### [290] [Highly Efficient and Effective LLMs with Multi-Boolean Architectures](https://arxiv.org/abs/2505.22811)
*Ba-Hien Tran,Van Minh Nguyen*

Main category: stat.ML

TL;DR: This paper presents a new framework that transforms large language models (LLMs) into multi-kernel Boolean parameters and fine-tunes them directly in the Boolean domain, reducing complexity and outperforming existing low-bit quantization methods.


<details>
  <summary>Details</summary>
Motivation: Current weight binarization approaches for LLMs either lead to significant performance loss or introduce substantial complexity due to reliance on full-precision latent weights.

Method: The paper introduces a framework that converts LLMs into multi-kernel Boolean parameters and fine-tunes these parameters directly in the Boolean domain without needing latent weights.

Result: Through various experiments, the method shows better performance compared to recent ultra low-bit quantization and binarization techniques.

Conclusion: The novel framework effectively reduces the complexity of LLMs during both fine-tuning and inference while improving performance over existing methods.

Abstract: Weight binarization has emerged as a promising strategy to drastically reduce
the complexity of large language models (LLMs). It is mainly classified into
two approaches: post-training binarization and finetuning with training-aware
binarization methods. The first approach, while having low complexity, leads to
significant loss of information from the original LLMs, resulting in poor
performance. The second approach, on the other hand, relies heavily on
full-precision latent weights for gradient approximation of binary weights,
which not only remains suboptimal but also introduces substantial complexity.
In this paper, we introduce a novel framework that effectively transforms LLMs
into multi-kernel Boolean parameters, for the first time, finetunes them
directly in the Boolean domain, eliminating the need for expensive latent
weights. This significantly reduces complexity during both finetuning and
inference. Through extensive and insightful experiments across a wide range of
LLMs, we demonstrate that our method outperforms recent ultra low-bit
quantization and binarization methods.

</details>


### [291] [Theoretical Foundations of the Deep Copula Classifier: A Generative Approach to Modeling Dependent Features](https://arxiv.org/abs/2505.22997)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: This paper proposes the Deep Copula Classifier (DCC), which uses neural network-parameterized copulas to model complex feature interactions and dependencies, converging to the Bayes-optimal classifier with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Traditional classifiers often assume feature independence or overly simplistic relationships, leading to poor performance when real-world dependencies matter. The authors aim to develop a classifier that can effectively capture complex dependencies between features.

Method: The DCC separates the learning of each feature's marginal distribution from the modeling of their joint dependence structure using neural network-parameterized copulas. Lightweight neural networks are used for each class to flexibly and adaptively capture feature interactions.

Result: DCC converges to the Bayes-optimal classifier under standard conditions with explicit convergence rates. Practical extensions include high-dimensional scalability, semi-supervised learning, and online adaptation.

Conclusion: DCC offers a mathematically grounded and interpretable framework for dependency-aware classification by unifying statistical rigor with the representational power of neural networks.

Abstract: Traditional classifiers often assume feature independence or rely on overly
simplistic relationships, leading to poor performance in settings where
real-world dependencies matter. We introduce the Deep Copula Classifier (DCC),
a generative model that separates the learning of each feature's marginal
distribution from the modeling of their joint dependence structure via neural
network-parameterized copulas. For each class, lightweight neural networks are
used to flexibly and adaptively capture feature interactions, making DCC
particularly effective when classification is driven by complex dependencies.
We establish that DCC converges to the Bayes-optimal classifier under standard
conditions and provide explicit convergence rates of O(n^{-r/(2r + d)}) for
r-smooth copula densities. Beyond theoretical guarantees, we outline several
practical extensions, including high-dimensional scalability through vine and
factor copula architectures, semi-supervised learning via entropy
regularization, and online adaptation using streaming gradient methods. By
unifying statistical rigor with the representational power of neural networks,
DCC offers a mathematically grounded and interpretable framework for
dependency-aware classification.

</details>


### [292] [JAPAN: Joint Adaptive Prediction Areas with Normalising-Flows](https://arxiv.org/abs/2505.23196)
*Eshant English,Christoph Lippert*

Main category: stat.ML

TL;DR: JAPAN是一种基于密度的符合性预测框架，通过流动模型估计密度并构建预测区域，解决了传统方法在多模态分布下的不足，提供了更好的校准和更紧凑的预测区域。


<details>
  <summary>Details</summary>
Motivation: 现有的符合性预测方法通常依赖于基于残差的符合性评分，这在底层分布为多模态时表现不佳，往往生成过于保守且围绕均值的预测区域，无法捕捉复杂预测分布的真实形状。

Method: 引入了JAPAN（联合自适应预测区域与归一化流），这是一种使用基于密度的符合性评分的符合性预测框架。通过利用基于流的模型，JAPAN估计（预测）密度，并通过对估计的密度分数进行阈值处理来构建预测区域，从而实现紧凑、可能不相连且上下文自适应的区域，同时保留有限样本覆盖率保证。

Result: 理论上证明了JAPAN的有效性，并在多元回归和预测任务中进行了实证验证，展示了良好的校准性能和相比现有基线更紧凑的预测区域。此外，还提供了一些扩展，增加了框架的灵活性。

Conclusion: JAPAN作为一种新的符合性预测框架，在处理复杂和多模态分布方面表现出色，能够提供更好的校准和更紧凑的预测区域，具有广泛的应用前景。

Abstract: Conformal prediction provides a model-agnostic framework for uncertainty
quantification with finite-sample validity guarantees, making it an attractive
tool for constructing reliable prediction sets. However, existing approaches
commonly rely on residual-based conformity scores, which impose geometric
constraints and struggle when the underlying distribution is multimodal. In
particular, they tend to produce overly conservative prediction areas centred
around the mean, often failing to capture the true shape of complex predictive
distributions. In this work, we introduce JAPAN (Joint Adaptive Prediction
Areas with Normalising-Flows), a conformal prediction framework that uses
density-based conformity scores. By leveraging flow-based models, JAPAN
estimates the (predictive) density and constructs prediction areas by
thresholding on the estimated density scores, enabling compact, potentially
disjoint, and context-adaptive regions that retain finite-sample coverage
guarantees. We theoretically motivate the efficiency of JAPAN and empirically
validate it across multivariate regression and forecasting tasks, demonstrating
good calibration and tighter prediction areas compared to existing baselines.
We also provide several \emph{extensions} adding flexibility to our proposed
framework.

</details>


### [293] [Stable Thompson Sampling: Valid Inference via Variance Inflation](https://arxiv.org/abs/2505.23260)
*Budhaditya Halder,Shubhayan Pan,Koulik Khamaru*

Main category: stat.ML

TL;DR: Analyze the problem of statistical inference when data is collected via Thompson Sampling, propose a variant called Stable Thompson Sampling which inflates posterior variance by a logarithmic factor, leading to asymptotically normal estimates with only modest increase in regret.


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling (TS) is known to be both asymptotically optimal and empirically effective but its adaptive sampling scheme poses challenges for constructing confidence intervals for model parameters.

Method: Propose a variant of TS, called Stable Thompson Sampling, where the posterior variance is inflated by a logarithmic factor.

Result: This modification leads to asymptotically normal estimates of the arm means despite the non-i.i.d. nature of the data. The variance inflation increases regret by only a logarithmic factor compared to standard TS.

Conclusion: There is a principled trade-off: by paying a small price in regret, one can enable valid statistical inference for adaptive decision-making algorithms.

Abstract: We consider the problem of statistical inference when the data is collected
via a Thompson Sampling-type algorithm. While Thompson Sampling (TS) is known
to be both asymptotically optimal and empirically effective, its adaptive
sampling scheme poses challenges for constructing confidence intervals for
model parameters. We propose and analyze a variant of TS, called Stable
Thompson Sampling, in which the posterior variance is inflated by a logarithmic
factor. We show that this modification leads to asymptotically normal estimates
of the arm means, despite the non-i.i.d. nature of the data. Importantly, this
statistical benefit comes at a modest cost: the variance inflation increases
regret by only a logarithmic factor compared to standard TS. Our results reveal
a principled trade-off: by paying a small price in regret, one can enable valid
statistical inference for adaptive decision-making algorithms.

</details>


### [294] [The Strong, Weak and Benign Goodhart's law. An independence-free and paradigm-agnostic formalisation](https://arxiv.org/abs/2505.23445)
*Adrien Majka,El-Mahdi El-Mhamdi*

Main category: stat.ML

TL;DR: 在本文中，研究者放宽了以往工作中关于独立性和学习范式的简化假设，探讨了代理指标和目标之间的耦合对古德哈特定律的影响。研究表明，在轻尾目标和轻尾差异的情况下，依赖性不会改变古德哈特定律的本质；但在轻尾目标和重尾差异的情况下，过优化的发生率与目标和度量之间差异的重尾性成反比。


<details>
  <summary>Details</summary>
Motivation: 尽管有越来越多的经验证据支持古德哈特定律，但尚未被形式化。最近一些尝试开始对其进行形式化研究，但大多基于简化的独立性假设和特定学习范式假设。因此，需要进一步研究代理指标和目标之间的耦合如何影响古德哈特定律。

Method: 研究者通过放宽先前工作中的独立性假设和学习范式假设，分析了代理指标和目标之间的耦合关系对古德哈特定律的影响。具体来说，他们考察了不同分布特性（如轻尾和重尾）下的目标和差异对优化过程的影响。

Result: 结果表明：1) 在轻尾目标和轻尾差异情况下，依赖性不会改变古德哈特定律的本质；2) 在轻尾目标和重尾差异情况下，发现过优化的发生率与目标和度量之间差异的重尾性成反比。

Conclusion: 本文的研究揭示了代理指标和目标之间的耦合对古德哈特定律的影响，并指出在某些情况下，过优化可能加剧问题。这为理解和规避机器学习中的古德哈特定律提供了新的见解。

Abstract: Goodhart's law is a famous adage in policy-making that states that ``When a
measure becomes a target, it ceases to be a good measure''. As machine learning
models and the optimisation capacity to train them grow, growing empirical
evidence reinforced the belief in the validity of this law without however
being formalised. Recently, a few attempts were made to formalise Goodhart's
law, either by categorising variants of it, or by looking at how optimising a
proxy metric affects the optimisation of an intended goal. In this work, we
alleviate the simplifying independence assumption, made in previous works, and
the assumption on the learning paradigm made in most of them, to study the
effect of the coupling between the proxy metric and the intended goal on
Goodhart's law. Our results show that in the case of light tailed goal and
light tailed discrepancy, dependence does not change the nature of Goodhart's
effect. However, in the light tailed goal and heavy tailed discrepancy case, we
exhibit an example where over-optimisation occurs at a rate inversely
proportional to the heavy tailedness of the discrepancy between the goal and
the metric. %

</details>


### [295] [Learning Parametric Distributions from Samples and Preferences](https://arxiv.org/abs/2505.23557)
*Marc Jourdan,Gizem Yüce,Nicolas Flammarion*

Main category: stat.ML

TL;DR: The paper explores how preference feedback enhances parameter estimation in continuous parametric distributions, showing that preference-based M-estimators outperform sample-only ones and proposes an estimator with improved error scaling.


<details>
  <summary>Details</summary>
Motivation: Preference feedback has been shown to improve model performance in language modeling. The authors aim to understand under what conditions this feedback can enhance parameter estimation in continuous parametric distributions.

Method: The authors use pairs of samples from an unknown distribution along with their relative preferences to develop preference-based M-estimators. They also leverage deterministic preferences to propose an estimator achieving an estimation error scaling of $\mathcal{O}(1/n)$.

Result: Preference-based M-estimators achieve better asymptotic variance than sample-only M-estimators. The proposed estimator shows a significant improvement over the standard rate attainable with samples alone.

Conclusion: The study concludes that preference feedback significantly improves parameter estimation in certain continuous parametric distributions, although the assumptions are restrictive.

Abstract: Recent advances in language modeling have underscored the role of preference
feedback in enhancing model performance. This paper investigates the conditions
under which preference feedback improves parameter estimation in classes of
continuous parametric distributions. In our framework, the learner observes
pairs of samples from an unknown distribution along with their relative
preferences depending on the same unknown parameter. We show that
preference-based M-estimators achieve a better asymptotic variance than
sample-only M-estimators, further improved by deterministic preferences.
Leveraging the hard constraints revealed by deterministic preferences, we
propose an estimator achieving an estimation error scaling of
$\mathcal{O}(1/n)$ -- a significant improvement over the $\Theta(1/\sqrt{n})$
rate attainable with samples alone. Next, we establish a lower bound that
matches this accelerated rate; up to dimension and problem-dependent constants.
While the assumptions underpinning our analysis are restrictive, they are
satisfied by notable cases such as Gaussian or Laplace distributions for
preferences based on the log-probability reward.

</details>


### [296] [Multilook Coherent Imaging: Theoretical Guarantees and Algorithms](https://arxiv.org/abs/2505.23594)
*Xi Chen,Soham Jana,Christopher A. Metzler,Arian Maleki,Shirin Jalali*

Main category: stat.ML

TL;DR: This paper explores theoretical and algorithmic aspects of likelihood-based methods in multilook coherent imaging, providing an MSE upper bound for the maximum likelihood estimator under deep image prior hypothesis and enhancing PGD with Newton-Schulz and bagging strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of multiplicative noise (speckle) degrading image quality in multilook coherent imaging systems and to explore the relatively underexplored theoretical foundations of these systems.

Method: Theoretical analysis establishes an upper bound on the MSE of the maximum likelihood estimator considering factors like number of parameters in deep image prior, number of looks, signal dimension, and measurements per look. Algorithmically, it uses projected gradient descent (PGD) enhanced by Newton-Schulz algorithm for matrix inversion and a bagging strategy to reduce projection errors.

Result: The combination of PGD with Newton-Schulz and bagging strategies yields state-of-the-art performance in multilook coherent imaging.

Conclusion: The study provides a rigorous framework for understanding and improving likelihood-based approaches in multilook coherent imaging, contributing both theoretically and algorithmically.

Abstract: Multilook coherent imaging is a widely used technique in applications such as
digital holography, ultrasound imaging, and synthetic aperture radar. A central
challenge in these systems is the presence of multiplicative noise, commonly
known as speckle, which degrades image quality. Despite the widespread use of
coherent imaging systems, their theoretical foundations remain relatively
underexplored. In this paper, we study both the theoretical and algorithmic
aspects of likelihood-based approaches for multilook coherent imaging,
providing a rigorous framework for analysis and method development. Our
theoretical contributions include establishing the first theoretical upper
bound on the Mean Squared Error (MSE) of the maximum likelihood estimator under
the deep image prior hypothesis. Our results capture the dependence of MSE on
the number of parameters in the deep image prior, the number of looks, the
signal dimension, and the number of measurements per look. On the algorithmic
side, we employ projected gradient descent (PGD) as an efficient method for
computing the maximum likelihood solution. Furthermore, we introduce two key
ideas to enhance the practical performance of PGD. First, we incorporate the
Newton-Schulz algorithm to compute matrix inverses within the PGD iterations,
significantly reducing computational complexity. Second, we develop a bagging
strategy to mitigate projection errors introduced during PGD updates. We
demonstrate that combining these techniques with PGD yields state-of-the-art
performance. Our code is available at
https://github.com/Computational-Imaging-RU/Bagged-DIP-Speckle.

</details>


### [297] [Instance-Optimality for Private KL Distribution Estimation](https://arxiv.org/abs/2505.23620)
*Jiayuan Ye,Vitaly Feldman,Kunal Talwar*

Main category: stat.ML

TL;DR: 研究了在给定样本下估计未知离散分布的问题，提出最小化KL散度的最优估计方法，并从实例最优性的角度分析算法性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注最坏情况下的表现，无法很好地反映实际分布上的性能，因此需要从实例最优性角度出发设计更优的算法。

Method: 构建了极小极大最优私有估计器，并提出了基于Good-Turing估计器变体的实例最优算法，同时考虑了差分隐私约束。

Result: 所提出的算法在实例最优性上达到了常数因子内的最优，并通过上下界分析验证了方法的有效性。

Conclusion: 该研究为离散分布估计提供了更精细的分析框架和算法设计思路，尤其是在KL散度下的分布估计问题中表现出优越性。

Abstract: We study the fundamental problem of estimating an unknown discrete
distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the
distribution. We are interested in minimizing the KL divergence between the
true distribution and the algorithm's estimate. We first construct minimax
optimal private estimators. Minimax optimality however fails to shed light on
an algorithm's performance on individual (non-worst-case) instances $p$ and
simple minimax-optimal DP estimators can have poor empirical performance on
real distributions. We then study this problem from an instance-optimality
viewpoint, where the algorithm's error on $p$ is compared to the minimum
achievable estimation error over a small local neighborhood of $p$. Under
natural notions of local neighborhood, we propose algorithms that achieve
instance-optimality up to constant factors, with and without a differential
privacy constraint. Our upper bounds rely on (private) variants of the
Good-Turing estimator. Our lower bounds use additive local neighborhoods that
more precisely captures the hardness of distribution estimation in KL
divergence, compared to ones considered in prior works.

</details>


### [298] [On the Convergence Analysis of Muon](https://arxiv.org/abs/2505.23737)
*Wei Shen,Ruichuan Huang,Minhui Huang,Cong Shen,Jiawei Zhang*

Main category: stat.ML

TL;DR: 研究了Muon优化器的收敛速度及其与梯度下降法的比较，揭示了Muon在神经网络训练中的优势原因。


<details>
  <summary>Details</summary>
Motivation: 尽管Muon优化器在训练神经网络时表现出色，但对其收敛行为和优越性能的理论理解仍然有限。

Method: 通过全面的收敛率分析，对比Muon优化器和梯度下降法，并确定Muon优于梯度下降的条件。

Result: 理论结果表明，Muon可以从Hessian矩阵的低秩和近似分块对角结构中获益，这些现象在实际神经网络训练中广泛存在。实验结果支持了理论发现。

Conclusion: Muon优化器在特定条件下可以显著优于传统优化器，其优势源于对矩阵参数结构特性的利用。

Abstract: The majority of parameters in neural networks are naturally represented as
matrices. However, most commonly used optimizers treat these matrix parameters
as flattened vectors during optimization, potentially overlooking their
inherent structural properties. Recently, an optimizer called Muon has been
proposed, specifically designed to optimize matrix-structured parameters.
Extensive empirical evidence shows that Muon can significantly outperform
traditional optimizers when training neural networks. Nonetheless, the
theoretical understanding of Muon's convergence behavior and the reasons behind
its superior performance remain limited. In this work, we present a
comprehensive convergence rate analysis of Muon and its comparison with
Gradient Descent (GD). We further characterize the conditions under which Muon
can outperform GD. Our theoretical results reveal that Muon can benefit from
the low-rank and approximate blockwise diagonal structure of Hessian matrices
-- phenomena widely observed in practical neural network training. Our
experimental results support and corroborate the theoretical findings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [299] [BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural Speech Synthesis with Flow Matching Models](https://arxiv.org/abs/2505.22865)
*Susan Liang,Dejan Markovic,Israel D. Gebru,Steven Krenn,Todd Keebler,Jacob Sandakly,Frank Yu,Samuel Hassel,Chenliang Xu,Alexander Richard*

Main category: cs.SD

TL;DR: 提出了一种名为BinauralFlow的基于流匹配的可流式双耳语音合成框架，解决了双耳渲染中的质量问题和实时推理需求，模型在质量和速度上超越现有方法，并且生成的声音接近真实录音。


<details>
  <summary>Details</summary>
Motivation: 现有的双耳渲染方法在音频质量与实时推理方面存在不足，难以生成高质量、与真实录音无异的双耳音频，同时无法满足实际应用中的流式推理需求。

Method: 将双耳渲染视为生成问题而非回归问题，设计了一个条件流匹配模型来生成高质量音频；引入因果U-Net架构以仅依赖过去信息估计当前音频帧，适应流式推理；还提出了一个连续推理流水线，包含流式STFT/ISTFT操作、缓冲区、中点求解器和早期跳过调度，提升渲染连续性和速度。

Result: 定量和定性评估表明该方法优于现有技术；感知研究显示模型生成的音频几乎与真实录音无异，混淆率达到42%。

Conclusion: BinauralFlow通过流匹配和因果U-Net等创新设计，在保证高质量双耳音频生成的同时支持流式推理，显著提升了双耳渲染的效果和效率。

Abstract: Binaural rendering aims to synthesize binaural audio that mimics natural
hearing based on a mono audio and the locations of the speaker and listener.
Although many methods have been proposed to solve this problem, they struggle
with rendering quality and streamable inference. Synthesizing high-quality
binaural audio that is indistinguishable from real-world recordings requires
precise modeling of binaural cues, room reverb, and ambient sounds.
Additionally, real-world applications demand streaming inference. To address
these challenges, we propose a flow matching based streaming binaural speech
synthesis framework called BinauralFlow. We consider binaural rendering to be a
generation problem rather than a regression problem and design a conditional
flow matching model to render high-quality audio. Moreover, we design a causal
U-Net architecture that estimates the current audio frame solely based on past
information to tailor generative models for streaming inference. Finally, we
introduce a continuous inference pipeline incorporating streaming STFT/ISTFT
operations, a buffer bank, a midpoint solver, and an early skip schedule to
improve rendering continuity and speed. Quantitative and qualitative
evaluations demonstrate the superiority of our method over SOTA approaches. A
perceptual study further reveals that our model is nearly indistinguishable
from real-world recordings, with a $42\%$ confusion rate.

</details>


### [300] [Patient Domain Supervised Contrastive Learning for Lung Sound Classification Using Mobile Phone](https://arxiv.org/abs/2505.23132)
*Seung Gyu Jeong,Seong Eun Kim*

Main category: cs.SD

TL;DR: The study explores using smartphone microphones for lung sound diagnosis, developing PD-SCL to address audio style differences and patient variability, improving the AST model's performance by 2.4%.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of traditional in-person lung sound assessments highlighted by the COVID-19 pandemic.

Method: The method involves developing Patient Domain Supervised Contrastive Learning (PD-SCL) integrated with the Audio Spectrogram Transformer (AST) model to improve the analysis of lung sounds recorded by smartphone microphones.

Result: The integration of PD-SCL with AST improved the model's performance by 2.4% compared to the original AST model.

Conclusion: Smartphones can effectively diagnose lung sounds, addressing inconsistencies in patient data and showing potential for broad use beyond traditional clinical settings.

Abstract: Auscultation is crucial for diagnosing lung diseases. The COVID-19 pandemic
has revealed the limitations of traditional, in-person lung sound assessments.
To overcome these issues, advancements in digital stethoscopes and artificial
intelligence (AI) have led to the development of new diagnostic methods. In
this context, our study aims to use smartphone microphones to record and
analyze lung sounds. We faced two major challenges: the difference in audio
style between electronic stethoscopes and smartphone microphones, and the
variability among patients. To address these challenges, we developed a method
called Patient Domain Supervised Contrastive Learning (PD-SCL). By integrating
this method with the Audio Spectrogram Transformer (AST) model, we
significantly improved its performance by 2.4\% compared to the original AST
model. This progress demonstrates that smartphones can effectively diagnose
lung sounds, addressing inconsistencies in patient data and showing potential
for broad use beyond traditional clinical settings. Our research contributes to
making lung disease detection more accessible in the post-COVID-19 world.

</details>


### [301] [Towards Robust Overlapping Speech Detection: A Speaker-Aware Progressive Approach Using WavLM](https://arxiv.org/abs/2505.23207)
*Zhaokai Sun,Li Zhang,Qing Wang,Pan Zhou,Lei Xie*

Main category: cs.SD

TL;DR: The paper proposes a speaker-aware progressive OSD model using SSL models and speaker attention module for overlapping speech detection, achieving SOTA performance with 82.76% F1 score on AMI test set.


<details>
  <summary>Details</summary>
Motivation: Overlapping Speech Detection is a critical challenge in multi-party speech processing, requiring identification of regions where multiple speakers overlap in a conversation.

Method: The method involves a speaker-aware progressive OSD model with progressive training strategy to enhance correlation between subtasks like VAD and overlap detection. It uses SSL models (WavLM, wav2vec 2.0) and a speaker attention module for better acoustic representation.

Result: The proposed method achieves state-of-the-art performance with an F1 score of 82.76% on the AMI test set, showing robustness and effectiveness in overlapping speech detection.

Conclusion: A speaker-aware progressive OSD model leveraging SSL and speaker attention significantly improves overlapping speech detection, achieving excellent results on the AMI test set.

Abstract: Overlapping Speech Detection (OSD) aims to identify regions where multiple
speakers overlap in a conversation, a critical challenge in multi-party speech
processing. This work proposes a speaker-aware progressive OSD model that
leverages a progressive training strategy to enhance the correlation between
subtasks such as voice activity detection (VAD) and overlap detection. To
improve acoustic representation, we explore the effectiveness of
state-of-the-art self-supervised learning (SSL) models, including WavLM and
wav2vec 2.0, while incorporating a speaker attention module to enrich features
with frame-level speaker information. Experimental results show that the
proposed method achieves state-of-the-art performance, with an F1 score of
82.76\% on the AMI test set, demonstrating its robustness and effectiveness in
OSD.

</details>


### [302] [MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction](https://arxiv.org/abs/2505.23305)
*Yunkee Chae,Kyogu Lee*

Main category: cs.SD

TL;DR: The paper introduces MGE-LDM, a latent diffusion framework for music generation, source imputation, and query-driven source separation without being constrained to fixed instrument classes.


<details>
  <summary>Details</summary>
Motivation: To create a unified model that can handle music generation, source imputation, and source separation flexibly without the limitation of predefined instrument categories.

Method: MGE-LDM learns a joint distribution over full mixtures, submixtures, and individual stems within a single compact latent diffusion model. It formulates separation and imputation as conditional inpainting tasks in the latent space.

Result: MGE-LDM enables complete mixture generation, partial generation (source imputation), and text-conditioned extraction of arbitrary sources. It can be trained jointly across heterogeneous multi-track datasets without relying on predefined instrument categories.

Conclusion: MGE-LDM presents a flexible approach for various music processing tasks without the constraints of fixed instrument classes.

Abstract: We present MGE-LDM, a unified latent diffusion framework for simultaneous
music generation, source imputation, and query-driven source separation. Unlike
prior approaches constrained to fixed instrument classes, MGE-LDM learns a
joint distribution over full mixtures, submixtures, and individual stems within
a single compact latent diffusion model. At inference, MGE-LDM enables (1)
complete mixture generation, (2) partial generation (i.e., source imputation),
and (3) text-conditioned extraction of arbitrary sources. By formulating both
separation and imputation as conditional inpainting tasks in the latent space,
our approach supports flexible, class-agnostic manipulation of arbitrary
instrument sources. Notably, MGE-LDM can be trained jointly across
heterogeneous multi-track datasets (e.g., Slakh2100, MUSDB18, MoisesDB) without
relying on predefined instrument categories. Audio samples are available at our
project page: https://yoongi43.github.io/MGELDM_Samples/.

</details>


### [303] [Spectrotemporal Modulation: Efficient and Interpretable Feature Representation for Classifying Speech, Music, and Environmental Sounds](https://arxiv.org/abs/2505.23509)
*Andrew Chang,Yike Li,Iran R. Roman,David Poeppel*

Main category: cs.SD

TL;DR: This paper proposes a novel approach centered on spectrotemporal modulation (STM) features for audio classification, which is efficient and interpretable, comparable to pretrained audio DNNs.


<details>
  <summary>Details</summary>
Motivation: Current audio DNN representations are computationally costly and uninterpretable, thus there is a need for optimization.

Method: The method uses spectrotemporal modulation (STM) features, a signal processing method that mimics the neurophysiological representation in the human auditory cortex.

Result: The STM-based model achieves classification performance comparable to pretrained audio DNNs across diverse naturalistic speech, music, and environmental sounds without any pretraining.

Conclusion: STM is an efficient and interpretable feature representation for audio classification, advancing machine listening and offering new possibilities in speech and auditory sciences, audio BCI, and cognitive computing.

Abstract: Audio DNNs have demonstrated impressive performance on various machine
listening tasks; however, most of their representations are computationally
costly and uninterpretable, leaving room for optimization. Here, we propose a
novel approach centered on spectrotemporal modulation (STM) features, a signal
processing method that mimics the neurophysiological representation in the
human auditory cortex. The classification performance of our STM-based model,
without any pretraining, is comparable to that of pretrained audio DNNs across
diverse naturalistic speech, music, and environmental sounds, which are
essential categories for both human cognition and machine perception. These
results show that STM is an efficient and interpretable feature representation
for audio classification, advancing the development of machine listening and
unlocking exciting new possibilities for basic understanding of speech and
auditory sciences, as well as developing audio BCI and cognitive computing.

</details>


### [304] [Few-Shot Speech Deepfake Detection Adaptation with Gaussian Processes](https://arxiv.org/abs/2505.23619)
*Neta Glazer,David Chernin,Idan Achituve,Sharon Gannot,Ethan Fetaya*

Main category: cs.SD

TL;DR: Recent advancements in TTS models increase the need for adaptable and efficient deepfake detection methods. This paper introduces ADD-GP, a few-shot adaptive framework based on a GP classifier for Audio Deepfake Detection. It combines a deep embedding model with Gaussian processes for strong performance and adaptability, also supporting personalized detection and one-shot adaptability. A benchmark dataset is constructed for evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the increasing demand for efficient and adaptable deepfake detection methods due to advancements in Text-to-Speech (TTS) models, particularly in voice cloning.

Method: The method introduced in this paper is ADD-GP, a few-shot adaptive framework based on a Gaussian Process (GP) classifier for Audio Deepfake Detection. It combines a powerful deep embedding model with the flexibility of Gaussian processes.

Result: The results show that the combination of a deep embedding model with Gaussian processes can achieve strong performance and adaptability in detecting audio deepfakes. Additionally, the approach supports personalized detection with greater robustness to new TTS models and one-shot adaptability.

Conclusion: The conclusion is that ADD-GP provides an effective solution for audio deepfake detection, capable of adapting to previously unseen generation models with minimal data. The construction of a benchmark dataset further supports the evaluation of this task.

Abstract: Recent advancements in Text-to-Speech (TTS) models, particularly in voice
cloning, have intensified the demand for adaptable and efficient deepfake
detection methods. As TTS systems continue to evolve, detection models must be
able to efficiently adapt to previously unseen generation models with minimal
data. This paper introduces ADD-GP, a few-shot adaptive framework based on a
Gaussian Process (GP) classifier for Audio Deepfake Detection (ADD). We show
how the combination of a powerful deep embedding model with the Gaussian
processes flexibility can achieve strong performance and adaptability.
Additionally, we show this approach can also be used for personalized
detection, with greater robustness to new TTS models and one-shot adaptability.
To support our evaluation, a benchmark dataset is constructed for this task
using new state-of-the-art voice cloning models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [305] [Augment or Not? A Comparative Study of Pure and Augmented Large Language Model Recommenders](https://arxiv.org/abs/2505.23053)
*Wei-Hsiang Huang,Chen-Wei Ke,Wei-Ning Chiu,Yu-Xuan Su,Chun-Chun Yang,Chieh-Yuan Cheng,Yun-Nung Chen,Pu-Jen Cheng*

Main category: cs.IR

TL;DR: 大型语言模型（LLMs）为推荐系统引入了新范式，本文提出了一种系统分类法，将现有方法分为纯LLM推荐器和增强型LLM推荐器两类，并引入统一评估平台支持公平比较，最后讨论了开放挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通过丰富的语义理解和隐性世界知识，为推荐系统引入了新的范式，因此需要一种系统化的分类方法来理解这些推荐系统的演变。

Method: 提出了一种系统分类法，将现有方法分为两类：纯LLM推荐器和增强型LLM推荐器；并引入了一个统一的评估平台，以在一致的实验设置下对代表性模型进行基准测试。

Result: 该分类法提供了一种新颖的方式来审视基于LLM的推荐系统的发展，并且统一评估平台能够突出影响效果的关键设计选择。

Conclusion: 讨论了开放的挑战，并指出了未来研究的有希望的方向，为推动下一代LLM驱动的推荐系统提供了全面概述和实用指导。

Abstract: Large language models (LLMs) have introduced new paradigms for recommender
systems by enabling richer semantic understanding and incorporating implicit
world knowledge. In this study, we propose a systematic taxonomy that
classifies existing approaches into two categories: (1) Pure LLM Recommenders,
which rely solely on LLMs, and (2) Augmented LLM Recommenders, which integrate
additional non-LLM techniques to enhance performance. This taxonomy provides a
novel lens through which to examine the evolving landscape of LLM-based
recommendation. To support fair comparison, we introduce a unified evaluation
platform that benchmarks representative models under consistent experimental
settings, highlighting key design choices that impact effectiveness. We
conclude by discussing open challenges and outlining promising directions for
future research. This work offers both a comprehensive overview and practical
guidance for advancing next-generation LLM-powered recommender.

</details>


### [306] [From Token to Action: State Machine Reasoning to Mitigate Overthinking in Information Retrieval](https://arxiv.org/abs/2505.23059)
*Dohyeon Lee,Yeonseok Jeong,Seung-won Hwang*

Main category: cs.IR

TL;DR: State Machine Reasoning (SMR) solves overthinking in Chain-of-Thought prompting for large language models, improving information retrieval performance by 3.4% and reducing token usage by 74.4%.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought prompting often leads to overthinking with semantically redundant traces that offer little benefit. The challenges of redundant trajectories and misguided reasoning in information retrieval need to be addressed.

Method: The proposed method is State Machine Reasoning (SMR), a transition-based reasoning framework composed of discrete actions (Refine, Rerank, Stop) that support early stopping and fine-grained control.

Result: Experiments on the BEIR and BRIGHT benchmarks show that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token usage by 74.4%. It generalizes across LLMs and retrievers without requiring task-specific tuning.

Conclusion: SMR offers a practical alternative to conventional CoT reasoning for complex reasoning in large language models.

Abstract: Chain-of-Thought (CoT) prompting enables complex reasoning in large language
models (LLMs), including applications in information retrieval (IR). However,
it often leads to overthinking, where models produce excessively long and
semantically redundant traces with little or no benefit. We identify two key
challenges in IR: redundant trajectories that revisit similar states and
misguided reasoning that diverges from user intent. To address these, we
propose State Machine Reasoning (SMR), a transition-based reasoning framework
composed of discrete actions (Refine, Rerank, Stop) that support early stopping
and fine-grained control. Experiments on the BEIR and BRIGHT benchmarks show
that SMR improves retrieval performance (nDCG@10) by 3.4% while reducing token
usage by 74.4%. It generalizes across LLMs and retrievers without requiring
task-specific tuning, offering a practical alternative to conventional CoT
reasoning. The code and details are available at https://github.com/ldilab/SMR.

</details>


### [307] [Deep Retrieval at CheckThat! 2025: Identifying Scientific Papers from Implicit Social Media Mentions via Hybrid Retrieval and Re-Ranking](https://arxiv.org/abs/2505.23250)
*Pascal J. Sager,Ashwini Kamaraj,Benjamin F. Grewe,Thilo Stadelmann*

Main category: cs.IR

TL;DR: 研究提出了一种混合检索管道，结合了词汇精度、语义泛化和深度上下文重新排序，以解决从社交媒体帖子中检索相关科学文献的任务。该方法在开发集上MRR@5达到76.46%，测试集上为66.43%，在CLEF CheckThat! 2025竞赛的子任务4b中获得开发排行榜第1名，测试排行榜第3名（共31支队伍）。


<details>
  <summary>Details</summary>
Motivation: 为了应对从社交媒体帖子中检索相关科学文献的挑战，需要一种能够弥合非正式到正式语言差距的稳健检索方法。

Method: 提出了一种混合检索管道，包含BM25关键词匹配与FAISS矢量存储（使用微调的INF-Retriever-v1模型进行密集语义检索），然后通过大型语言模型（LLM）交叉编码器合并并重新排序结果。具体来说，BM25返回前30个候选者，语义搜索产生100个候选者，最后由LLM交叉编码器重新排序。

Result: 在开发集上实现了76.46%的MRR@5，在隐藏测试集上实现了66.43%的MRR@5，排名开发排行榜第1，测试排行榜第3（仅比第一名低2个百分点）。

Conclusion: 通过本地运行开源模型且无需外部训练数据，展示了精心设计和微调的检索管道的有效性。

Abstract: We present the methodology and results of the Deep Retrieval team for subtask
4b of the CLEF CheckThat! 2025 competition, which focuses on retrieving
relevant scientific literature for given social media posts. To address this
task, we propose a hybrid retrieval pipeline that combines lexical precision,
semantic generalization, and deep contextual re-ranking, enabling robust
retrieval that bridges the informal-to-formal language gap. Specifically, we
combine BM25-based keyword matching with a FAISS vector store using a
fine-tuned INF-Retriever-v1 model for dense semantic retrieval. BM25 returns
the top 30 candidates, and semantic search yields 100 candidates, which are
then merged and re-ranked via a large language model (LLM)-based cross-encoder.
  Our approach achieves a mean reciprocal rank at 5 (MRR@5) of 76.46% on the
development set and 66.43% on the hidden test set, securing the 1st position on
the development leaderboard and ranking 3rd on the test leaderboard (out of 31
teams), with a relative performance gap of only 2 percentage points compared to
the top-ranked system. We achieve this strong performance by running
open-source models locally and without external training data, highlighting the
effectiveness of a carefully designed and fine-tuned retrieval pipeline.

</details>


### [308] [Engineering Serendipity through Recommendations of Items with Atypical Aspects](https://arxiv.org/abs/2505.23580)
*Ramit Aditya,Razvan Bunescu,Smita Nannaware,Erfan Al-Hossami*

Main category: cs.IR

TL;DR: The paper introduces a new recommendation task called 'engineering serendipity', which aims to recommend items with atypical aspects that match user interests. It describes an LLM-based system pipeline for this task, introduces related datasets and a custom procedure for dynamic selection of in-context learning examples. Experiments show high correlation between system-generated rankings and ground truth rankings.


<details>
  <summary>Details</summary>
Motivation: To create memorable experiences by recommending items with atypical aspects that align with user interests, going beyond traditional accuracy-focused recommendation approaches.

Method: An LLM-based system pipeline extracts atypical aspects from item reviews, estimates and aggregates their user-specific utility in a measure of serendipity potential, and reranks recommended items accordingly. Also includes a dataset of Yelp reviews annotated with atypical aspects, artificial user profiles, and a custom procedure for dynamic selection of in-context learning examples.

Result: Experimental evaluations demonstrate high correlation between serendipity-based rankings generated by the system and ground truth rankings derived from manual annotations.

Conclusion: This new recommendation task and associated system could inspire further research into enhanced user satisfaction in recommendation systems.

Abstract: A restaurant dinner or a hotel stay may lead to memorable experiences when
guests encounter unexpected aspects that also match their interests. For
example, an origami-making station in the waiting area of a restaurant may be
both surprising and enjoyable for a customer who is passionate about paper
crafts. Similarly, an exhibit of 18th century harpsichords would be atypical
for a hotel lobby and likely pique the interest of a guest who has a passion
for Baroque music. Motivated by this insight, in this paper we introduce the
new task of engineering serendipity through recommendations of items with
atypical aspects. We describe an LLM-based system pipeline that extracts
atypical aspects from item reviews, then estimates and aggregates their
user-specific utility in a measure of serendipity potential that is used to
rerank a list of items recommended to the user. To facilitate system
development and evaluation, we introduce a dataset of Yelp reviews that are
manually annotated with atypical aspects and a dataset of artificially
generated user profiles, together with crowdsourced annotations of user-aspect
utility values. Furthermore, we introduce a custom procedure for dynamic
selection of in-context learning examples, which is shown to improve LLM-based
judgments of atypicality and utility. Experimental evaluations show that
serendipity-based rankings generated by the system are highly correlated with
ground truth rankings for which serendipity scores are computed from manual
annotations of atypical aspects and their user-dependent utility. Overall, we
hope that the new recommendation task and the associated system presented in
this paper catalyze further research into recommendation approaches that go
beyond accuracy in their pursuit of enhanced user satisfaction.
  The datasets and the code are made publicly available at
https://github.com/ramituncc49er/ATARS .

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [309] [Local Stability and Region of Attraction Analysis for Neural Network Feedback Systems under Positivity Constraints](https://arxiv.org/abs/2505.22889)
*Hamidreza Montazeri Hedesh,Moh Kamalul Wafi,Milad Siami*

Main category: eess.SY

TL;DR: The paper explores the local stability of nonlinear Lur'e systems with static nonlinear feedback implemented by feedforward neural networks (FFNNs). It introduces two methods for estimating the Region of Attraction (ROA): a Lyapunov-based approach and a novel technique for computing tight local sector bounds for FFNNs. The results show improvements over existing approaches in both ROA size and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring local exponential stability of trajectories in nonlinear Lur'e systems with FFNN-based static nonlinear feedback, particularly focusing on estimating the Region of Attraction (ROA) effectively.

Method: 1. Leverage positivity system constraints and a localized variant of the Aizerman conjecture to provide sufficient conditions for exponential stability. 2. Develop a less conservative Lyapunov-based approach that constructs invariant sublevel sets of a quadratic function satisfying a linear matrix inequality (LMI). 3. Introduce a novel technique for computing tight local sector bounds for FFNNs via layer-wise propagation of linear relaxations. These bounds are integrated into the localized Aizerman framework.

Result: The numerical results indicate significant improvements compared to existing integral quadratic constraint-based approaches, both in terms of the size of the Region of Attraction (ROA) and scalability.

Conclusion: The proposed methods offer effective ways to estimate the ROA for nonlinear Lur'e systems with FFNN-based feedback, showing superior performance in terms of ROA size and scalability.

Abstract: We study the local stability of nonlinear systems in the Lur'e form with
static nonlinear feedback realized by feedforward neural networks (FFNNs). By
leveraging positivity system constraints, we employ a localized variant of the
Aizerman conjecture, which provides sufficient conditions for exponential
stability of trajectories confined to a compact set. Using this foundation, we
develop two distinct methods for estimating the Region of Attraction (ROA): (i)
a less conservative Lyapunov-based approach that constructs invariant sublevel
sets of a quadratic function satisfying a linear matrix inequality (LMI), and
(ii) a novel technique for computing tight local sector bounds for FFNNs via
layer-wise propagation of linear relaxations. These bounds are integrated into
the localized Aizerman framework to certify local exponential stability.
Numerical results demonstrate substantial improvements over existing integral
quadratic constraint-based approaches in both ROA size and scalability.

</details>


### [310] [User-centric Vehicle-to-Grid Optimization with an Input Convex Neural Network-based Battery Degradation Model](https://arxiv.org/abs/2505.11047)
*Arghya Mallick,Georgios Pantazis,Mohammad Khosravi,Peyman Mohajerin Esfahani,Sergio Grammatico*

Main category: eess.SY

TL;DR: 提出了一种基于多目标优化的以用户为中心的车网互动（V2G）方法，通过输入凸神经网络（ICNNs）建立数据驱动的电池退化模型，生成最佳智能充电配置文件，平衡电池退化和V2G收入。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏准确且通用的电池退化模型，需要一种新方法来平衡电动汽车用户的电池退化和V2G收入。

Method: 使用输入凸神经网络（ICNNs）开发数据驱动的电池退化模型，并将其集成到多目标优化框架中，生成每个电动车的最佳智能充电配置文件。

Result: 数值模拟表明ICNN模型在预测未见数据的电池退化方面具有高精度，并展示了基于用户偏好的财务收益与电池健康损失之间的权衡曲线。

Conclusion: 该方法能有效平衡电池健康和财务收益之间的权衡，为用户提供个性化的智能充电策略。

Abstract: We propose a data-driven, user-centric vehicle-to-grid (V2G) methodology
based on multi-objective optimization to balance battery degradation and V2G
revenue according to EV user preference. Given the lack of accurate and
generalizable battery degradation models, we leverage input convex neural
networks (ICNNs) to develop a data-driven degradation model trained on
extensive experimental datasets. This approach enables our model to capture
nonconvex dependencies on battery temperature and time while maintaining
convexity with respect to the charging rate. Such a partial convexity property
ensures that the second stage of our methodology remains computationally
efficient. In the second stage, we integrate our data-driven degradation model
into a multi-objective optimization framework to generate an optimal smart
charging profile for each EV. This profile effectively balances the trade-off
between financial benefits from V2G participation and battery degradation,
controlled by a hyperparameter reflecting the user prioritization of battery
health. Numerical simulations show the high accuracy of the ICNN model in
predicting battery degradation for unseen data. Finally, we present a trade-off
curve illustrating financial benefits from V2G versus losses from battery
health degradation based on user preferences and showcase smart charging
strategies under realistic scenarios.

</details>


### [311] [From Connectivity to Autonomy: The Dawn of Self-Evolving Communication Systems](https://arxiv.org/abs/2505.23710)
*Zeinab Nezami,Syed Danial Ali Shah,Maryam Hafeez,Karim Djemame,Syed Ali Raza Zaidi*

Main category: eess.SY

TL;DR: This paper envisions 6G as a self-evolving telecom ecosystem driven by AI for dynamic adaptation, exploring key enablers like reconfigurable infrastructure and intelligent network functions. It highlights improved real-time decision-making, ethical challenges, and provides a technology stack roadmap.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of 6G as a self-evolving telecom ecosystem with AI-driven intelligence for dynamic adaptation beyond static connectivity.

Method: Investigating key enablers such as reconfigurable infrastructure, adaptive middleware, intelligent network functions, and multi-agent collaboration; aligning these with emerging industrial IoT frameworks.

Result: Potential for enhanced real-time decision-making, optimized efficiency, and reduced latency in networked control systems identified.

Conclusion: The research contributes to next-generation intelligent automation solutions bridging theory and real-world applications, while providing a technology stack roadmap for future developments.

Abstract: This paper envisions 6G as a self-evolving telecom ecosystem, where AI-driven
intelligence enables dynamic adaptation beyond static connectivity. We explore
the key enablers of autonomous communication systems, spanning reconfigurable
infrastructure, adaptive middleware, and intelligent network functions,
alongside multi-agent collaboration for distributed decision-making. We explore
how these methodologies align with emerging industrial IoT frameworks, ensuring
seamless integration within digital manufacturing processes. Our findings
emphasize the potential for improved real-time decision-making, optimizing
efficiency, and reducing latency in networked control systems. The discussion
addresses ethical challenges, research directions, and standardization efforts,
concluding with a technology stack roadmap to guide future developments. By
leveraging state-of-the-art 6G network management techniques, this research
contributes to the next generation of intelligent automation solutions,
bridging the gap between theoretical advancements and real-world industrial
applications.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [312] [A Descriptor Is All You Need: Accurate Machine Learning of Nonadiabatic Coupling Vectors](https://arxiv.org/abs/2505.23344)
*Jakub Martinka,Lina Zhang,Yi-Fan Hou,Mikołaj Martyka,Jiří Pittner,Mario Barbatti,Pavlo O. Dral*

Main category: physics.comp-ph

TL;DR: The paper designs NAC-specific descriptors and a new ML phase-correction procedure to accurately learn nonadiabatic couplings, demonstrating efficiency and robustness in FSSH simulations of fulvene with significantly reduced error bars.


<details>
  <summary>Details</summary>
Motivation: Nonadiabatic couplings (NACs) are crucial for modeling photochemical and photophysical processes using methods like fewest-switches surface hopping (FSSH). There is a strong incentive to machine-learn NACs to accelerate simulations.

Method: The authors design NAC-specific descriptors based on domain expertise and introduce a new ML phase-correction procedure. These innovations address the challenges posed by NACs' vectorial, double-valued character and singularities near conical intersection seams.

Result: The approach achieves unprecedented accuracy with an $R^2$ exceeding 0.99. It efficiently and robustly describes $S_1$ decay in fully ML-driven FSSH simulations of fulvene, reducing error bars through a large ensemble of trajectories.

Conclusion: This work demonstrates a significant advancement in learning nonadiabatic couplings, providing a powerful tool for accurate and accelerated simulations in photochemical and photophysical studies.

Abstract: Nonadiabatic couplings (NACs) play a crucial role in modeling photochemical
and photophysical processes with methods such as the widely used
fewest-switches surface hopping (FSSH). There is therefore a strong incentive
to machine learn NACs for accelerating simulations. However, this is
challenging due to NACs' vectorial, double-valued character and the singularity
near a conical intersection seam. For the first time, we design NAC-specific
descriptors based on our domain expertise and show that they allow learning
NACs with never-before-reported accuracy of $R^2$ exceeding 0.99. The key to
success is also our new ML phase-correction procedure. We demonstrate the
efficiency and robustness of our approach on a prototypical example of fully
ML-driven FSSH simulations of fulvene targeting the SA-2-CASSCF(6,6) electronic
structure level. This ML-FSSH dynamics leads to an accurate description of
$S_1$ decay while reducing error bars by allowing the execution of a large
ensemble of trajectories. Our implementations are available in open-source
MLatom.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [313] [An Efficient deep learning model to Predict Stock Price Movement Based on Limit Order Book](https://arxiv.org/abs/2505.22678)
*Jiahao Yang,Ran Fang,Ming Zhang,Jun Zhou*

Main category: q-fin.TR

TL;DR: In high-frequency trading, raw LOB data has symmetry between ask and bid sides. This paper proposes a Siamese architecture to process these sides separately with shared parameters, improving deep learning models' performance. It also integrates MHA with LSTM for stock price movement modeling. Experiments on Chinese A-share military industry stocks show that the method enhances strong baselines in over 75% of cases and that MHA boosts performance, especially for short-term forecasts.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of effectively modeling stock price movements using high-dimensional and volatile LOB data in HFT by leveraging inherent symmetries in the data.

Method: Propose a Siamese architecture to process ask and bid sides separately with shared parameters and integrate MHA with LSTM for modeling stock price movements.

Result: Improves strong baselines' performance in over 75% of cases, except for MLP which performed poorly. MHA enhances model performance, particularly for short-term forecasts.

Conclusion: The proposed Siamese-based approach and integration of MHA with LSTM can significantly enhance the performance of deep learning models in predicting stock price movements.

Abstract: In high-frequency trading (HFT), leveraging limit order books (LOB) to model
stock price movements is crucial for achieving profitable outcomes. However,
this task is challenging due to the high-dimensional and volatile nature of the
original data. Even recent deep learning models often struggle to capture price
movement patterns effectively, particularly without well-designed features. We
observed that raw LOB data exhibits inherent symmetry between the ask and bid
sides, and the bid-ask differences demonstrate greater stability and lower
complexity compared to the original data. Building on this insight, we propose
a novel approach in which leverages the Siamese architecture to enhance the
performance of existing deep learning models. The core idea involves processing
the ask and bid sides separately using the same module with shared parameters.
We applied our Siamese-based methods to several widely used strong baselines
and validated their effectiveness using data from 14 military industry stocks
in the Chinese A-share market. Furthermore, we integrated multi-head attention
(MHA) mechanisms with the Long Short-Term Memory (LSTM) module to investigate
its role in modeling stock price movements. Our experiments used raw data and
widely used Order Flow Imbalance (OFI) features as input with some strong
baseline models. The results show that our method improves the performance of
strong baselines in over 75$% of cases, excluding the Multi-Layer Perception
(MLP) baseline, which performed poorly and is not considered practical.
Furthermore, we found that Multi-Head Attention can enhance model performance,
particularly over shorter forecasting horizons.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [314] [Investigating the effectiveness of multimodal data in forecasting SARS-COV-2 case surges](https://arxiv.org/abs/2505.22688)
*Palur Venkata Raghuvamsi,Siyuan Brandon Loh,Prasanta Bhattacharya,Joses Ho,Raphael Lee Tze Chuen,Alvin X. Han,Sebastian Maurer-Stroh*

Main category: q-bio.QM

TL;DR: The paper explores the use of diverse feature sets, including biological, public health, and human behavioral data, for predicting case surges in countries during the COVID-19 pandemic. It uncovers significant heterogeneity in predictive performance across countries and suggests tailoring models to specific national contexts.


<details>
  <summary>Details</summary>
Motivation: To investigate the usefulness of diverse modalities of feature sets in predicting case surges during the COVID-19 pandemic beyond traditional epidemiological data.

Method: The study examined biological features (e.g., mutations), public health features (e.g., case counts, policy interventions) and human behavioral features (e.g., mobility and social media conversations) for their effectiveness in predicting country-level case surges.

Result: The results highlighted the relative effectiveness of different feature sets and uncovered considerable heterogeneity in predictive performance across countries and feature modalities.

Conclusion: Integrating alternative data sources into existing disease surveillance frameworks can enhance the prediction of pandemic dynamics, with models potentially needing to be tailored to specific national contexts and pandemic phases.

Abstract: The COVID-19 pandemic response relied heavily on statistical and machine
learning models to predict key outcomes such as case prevalence and fatality
rates. These predictions were instrumental in enabling timely public health
interventions that helped break transmission cycles. While most existing models
are grounded in traditional epidemiological data, the potential of alternative
datasets, such as those derived from genomic information and human behavior,
remains underexplored. In the current study, we investigated the usefulness of
diverse modalities of feature sets in predicting case surges. Our results
highlight the relative effectiveness of biological (e.g., mutations), public
health (e.g., case counts, policy interventions) and human behavioral features
(e.g., mobility and social media conversations) in predicting country-level
case surges. Importantly, we uncover considerable heterogeneity in predictive
performance across countries and feature modalities, suggesting that surge
prediction models may need to be tailored to specific national contexts and
pandemic phases. Overall, our work highlights the value of integrating
alternative data sources into existing disease surveillance frameworks to
enhance the prediction of pandemic dynamics.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [315] [Recovering Fairness Directly from Modularity: a New Way for Fair Community Partitioning](https://arxiv.org/abs/2505.22684)
*Yufeng Wang,Yiguang Bai,Tianqing Zhu,Ismail Ben Ayed,Jing Yuan*

Main category: cs.SI

TL;DR: Community partitioning in network analysis is vital, with modularity optimization being the main technique. However, traditional methods often ignore fairness. This paper introduces protected group networks and proposes a new fairness-modularity metric, which incorporates fairness into traditional modularity. The authors prove that minimizing this metric yields fair partitions for protected groups while maintaining theoretical soundness. They develop an optimization framework and design the Fair Fast Newman (FairFN) algorithm, which enhances the Fast Newman method to optimize both modularity and fairness. Experiments show that FairFN achieves significantly improved fairness and high-quality partitions compared to state-of-the-art methods, especially on unbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional modularity-based methods for community partitioning often overlook fairness, which is crucial in real-world applications.

Method: The authors introduce protected group networks and propose a novel fairness-modularity metric that incorporates fairness into traditional modularity. They also develop a general optimization framework for fairness partitioning and design the Fair Fast Newman (FairFN) algorithm to optimize both modularity and fairness.

Result: Experiments demonstrate that FairFN achieves significantly improved fairness and high-quality partitions compared to state-of-the-art methods, particularly on unbalanced datasets.

Conclusion: The proposed fairness-modularity metric and FairFN algorithm provide a way to achieve naturally fair partitions for protected groups while maintaining theoretical soundness.

Abstract: Community partitioning is crucial in network analysis, with modularity
optimization being the prevailing technique. However, traditional
modularity-based methods often overlook fairness, a critical aspect in
real-world applications. To address this, we introduce protected group networks
and propose a novel fairness-modularity metric. This metric extends traditional
modularity by explicitly incorporating fairness, and we prove that minimizing
it yields naturally fair partitions for protected groups while maintaining
theoretical soundness. We develop a general optimization framework for fairness
partitioning and design the efficient Fair Fast Newman (FairFN) algorithm,
enhancing the Fast Newman (FN) method to optimize both modularity and fairness.
Experiments show FairFN achieves significantly improved fairness and
high-quality partitions compared to state-of-the-art methods, especially on
unbalanced datasets.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [316] [Towards Explainable Sequential Learning](https://arxiv.org/abs/2505.23624)
*Giacomo Bergami,Emma Packer,Kirsty Scott,Silvia Del Din*

Main category: cs.DB

TL;DR: The paper introduces a hybrid explainable temporal data processing pipeline named EMeriTAte+DF which connects numerical-driven and event-based temporal data classification via artificial intelligence principles, providing human-explainable results.


<details>
  <summary>Details</summary>
Motivation: To create a system that bridges numerical-driven temporal data classification with an event-based one through verified artificial intelligence principles, enabling human-explainable results.

Method: A preliminary a posteriori explainable phase describes the numerical input data in terms of concurrent constituents with numerical payloads. This required extending the event-based literature to design specification mining algorithms supporting concurrent constituents.

Result: The proposed solutions outperform state-of-the-art solutions for multivariate time series classifications.

Conclusion: The effectiveness of the proposed methodology is showcased by its superior performance compared to existing solutions.

Abstract: This paper offers a hybrid explainable temporal data processing pipeline,
DataFul Explainable MultivariatE coRrelatIonal Temporal Artificial inTElligence
(EMeriTAte+DF), bridging numerical-driven temporal data classification with an
event-based one through verified artificial intelligence principles, enabling
human-explainable results. This was possible through a preliminary a posteriori
explainable phase describing the numerical input data in terms of concurrent
constituents with numerical payloads. This further required extending the
event-based literature to design specification mining algorithms supporting
concurrent constituents. Our previous and current solutions outperform
state-of-the-art solutions for multivariate time series classifications, thus
showcasing the effectiveness of the proposed methodology.

</details>


### [317] [KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction](https://arxiv.org/abs/2505.23416)
*Jang-Hyun Kim,Jinuk Kim,Sangwoo Kwon,Jae W. Lee,Sangdoo Yun,Hyun Oh Song*

Main category: cs.DB

TL;DR: Transformer-based large language models face memory overhead and attention latency issues as context length grows. KVzip, a query-agnostic KV cache eviction method, reduces these problems by effectively reusing compressed KV caches across diverse queries. It can reduce KV cache size by 3-4 times and FlashAttention decoding latency by approximately 2 times with negligible performance loss in various tasks.


<details>
  <summary>Details</summary>
Motivation: To address the memory overhead and increased attention latency caused by expanding KV cache sizes as context length grows in Transformer-based large language models.

Method: KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, then evicts pairs with lower importance. This enables effective reuse of compressed KV caches across diverse queries.

Result: KVzip reduces KV cache size by 3-4× and FlashAttention decoding latency by approximately 2×, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. It significantly outperforms existing query-aware KV eviction methods.

Conclusion: KVzip is an effective query-agnostic KV cache eviction method that addresses the memory and latency issues in Transformer-based large language models.

Abstract: Transformer-based large language models (LLMs) cache context as key-value
(KV) pairs during inference. As context length grows, KV cache sizes expand,
leading to substantial memory overhead and increased attention latency. This
paper introduces KVzip, a query-agnostic KV cache eviction method enabling
effective reuse of compressed KV caches across diverse queries. KVzip
quantifies the importance of a KV pair using the underlying LLM to reconstruct
original contexts from cached KV pairs, subsequently evicting pairs with lower
importance. Extensive empirical evaluations demonstrate that KVzip reduces KV
cache size by 3-4$\times$ and FlashAttention decoding latency by approximately
2$\times$, with negligible performance loss in question-answering, retrieval,
reasoning, and code comprehension tasks. Evaluations include various models
such as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching
up to 170K tokens. KVzip significantly outperforms existing query-aware KV
eviction methods, which suffer from performance degradation even at a 90% cache
budget ratio under multi-query scenarios.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [318] [In Dialogue with Intelligence: Rethinking Large Language Models as Collective Knowledge](https://arxiv.org/abs/2505.22767)
*Eleni Vasilaki*

Main category: cs.HC

TL;DR: This paper re-frames Large Language Models (LLMs) as dynamic instantiations of Collective human Knowledge (CK), exploring emergent dialogue patterns, fine-tuning implications, and co-augmentation between human and machine cognition.


<details>
  <summary>Details</summary>
Motivation: To offer a theoretical and experiential re-framing of LLMs as dynamic instantiations of Collective human Knowledge (CK) instead of analysing them through architectural, behavioural, or training-data lenses.

Method: Draw on concepts from neuroscience and AI, and ground in sustained interaction with ChatGPT-4 to examine emergent dialogue patterns, the implications of fine-tuning, and the notion of co-augmentation.

Result: Provides a new perspective for understanding interaction, representation, and agency in contemporary AI systems.

Conclusion: LLMs evoke intelligence through dialogue rather than store it statically, suggesting mutual enhancement between human and machine cognition.

Abstract: Large Language Models (LLMs) are typically analysed through architectural,
behavioural, or training-data lenses. This article offers a theoretical and
experiential re-framing: LLMs as dynamic instantiations of Collective human
Knowledge (CK), where intelligence is evoked through dialogue rather than
stored statically. Drawing on concepts from neuroscience and AI, and grounded
in sustained interaction with ChatGPT-4, I examine emergent dialogue patterns,
the implications of fine-tuning, and the notion of co-augmentation: mutual
enhancement between human and machine cognition. This perspective offers a new
lens for understanding interaction, representation, and agency in contemporary
AI systems.

</details>


### [319] [Orca: Browsing at Scale Through User-Driven and AI-Facilitated Orchestration Across Malleable Webpages](https://arxiv.org/abs/2505.22831)
*Peiling Jiang,Haijun Xia*

Main category: cs.HC

TL;DR: Web-based activities are distributed across webpages. Conventional browsers with tabs can't handle large volumes of information well. Recent AI systems can automate web browsing but reduce user agency. This paper explores how AI can augment users' interactions with content across webpages using a prototype web browser called Orca. Orca leverages AI to support user-driven exploration, operation, organization, and synthesis of web content at scale. Evaluation showed increased information foraging, enhanced user control, and more flexibility in sensemaking.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional browsers in handling large volumes of information across pages and the drawbacks of fully automated AI systems that diminish user agency.

Method: Through literature on information tasks and web browsing challenges, and an iterative design process, the authors developed Orca, a prototype web browser that leverages AI to support user-driven exploration, operation, organization, and synthesis of web content at scale.

Result: Evaluation revealed an increased 'appetite' for information foraging, enhanced user control, and more flexibility in sensemaking across a broader information landscape on the web.

Conclusion: Orca demonstrates the potential of AI-augmented web browsers to enhance user interactions with web content, mitigate cognitive and manual efforts, and enable browsing at scale.

Abstract: Web-based activities are fundamentally distributed across webpages. However,
conventional browsers with stacks of tabs fail to support operating and
synthesizing large volumes of information across pages. While recent AI systems
enable fully automated web browsing and information synthesis, they often
diminish user agency and hinder contextual understanding. Therefore, we explore
how AI could instead augment users' interactions with content across webpages
and mitigate cognitive and manual efforts. Through literature on information
tasks and web browsing challenges, and an iterative design process, we present
a rich set of novel interactions with our prototype web browser, Orca.
Leveraging AI, Orca supports user-driven exploration, operation, organization,
and synthesis of web content at scale. To enable browsing at scale, webpages
are treated as malleable materials that humans and AI can collaboratively
manipulate and compose into a malleable, dynamic, and browser-level workspace.
Our evaluation revealed an increased "appetite" for information foraging,
enhanced user control, and more flexibility in sensemaking across a broader
information landscape on the web.

</details>


### [320] [HiLDe: Intentional Code Generation via Human-in-the-Loop Decoding](https://arxiv.org/abs/2505.22906)
*Emmanuel Anaya González,Raven Rothkopf,Sorin Lerner,Nadia Polikarpova*

Main category: cs.HC

TL;DR: The paper introduces Human-in-the-loop Decoding (HiLDe), a technique enabling users to influence LLM decisions in code generation, resulting in fewer vulnerabilities and better goal alignment compared to traditional assistants.


<details>
  <summary>Details</summary>
Motivation: AI programming tools often exclude users from decision-making, leading to over-reliance and potential issues in critical areas like software security.

Method: Proposed is Human-in-the-loop Decoding implemented in HiLDe, which highlights critical LLM decisions and provides local alternatives for user exploration during code completion.

Result: In a study with 18 participants on security-related tasks, HiLDe produced significantly fewer vulnerabilities and better aligned code generation with user goals than a traditional assistant.

Conclusion: Human-in-the-loop Decoding enhances user involvement and improves outcomes in AI-assisted programming.

Abstract: While AI programming tools hold the promise of increasing programmers'
capabilities and productivity to a remarkable degree, they often exclude users
from essential decision-making processes, causing many to effectively "turn off
their brains" and over-rely on solutions provided by these systems. These
behaviors can have severe consequences in critical domains, like software
security. We propose Human-in-the-loop Decoding, a novel interaction technique
that allows users to observe and directly influence LLM decisions during code
generation, in order to align the model's output with their personal
requirements. We implement this technique in HiLDe, a code completion assistant
that highlights critical decisions made by the LLM and provides local
alternatives for the user to explore. In a within-subjects study (N=18) on
security-related tasks, we found that HiLDe led participants to generate
significantly fewer vulnerabilities and better align code generation with their
goals compared to a traditional code completion assistant.

</details>


### [321] [Human Empathy as Encoder: AI-Assisted Depression Assessment in Special Education](https://arxiv.org/abs/2505.23631)
*Boning Zhao*

Main category: cs.HC

TL;DR: This paper presents Human Empathy as Encoder (HEAE), a novel AI framework that integrates student narratives and teacher-derived 'Empathy Vectors' to assess depression severity in special education settings, achieving 82.74% accuracy.


<details>
  <summary>Details</summary>
Motivation: Standardized questionnaires and automated methods fail to fully capture students' true situations in sensitive environments like special education. They lack the individualized insights from teachers' empathetic connections with students.

Method: The HEAE framework combines student narrative text with a 9-dimensional 'Empathy Vector' derived from teachers, guided by the PHQ-9 framework. This approach translates empathetic insights into structured AI input while enhancing human judgment.

Result: Rigorous experiments optimized multimodal fusion, text representation, and classification architecture, resulting in 82.74% accuracy for 7-level severity classification.

Conclusion: This work shows a way towards more responsible and ethical affective computing by embedding human empathy structurally.

Abstract: Assessing student depression in sensitive environments like special education
is challenging. Standardized questionnaires may not fully reflect students'
true situations. Furthermore, automated methods often falter with rich student
narratives, lacking the crucial, individualized insights stemming from
teachers' empathetic connections with students. Existing methods often fail to
address this ambiguity or effectively integrate educator understanding. To
address these limitations by fostering a synergistic human-AI collaboration,
this paper introduces Human Empathy as Encoder (HEAE), a novel, human-centered
AI framework for transparent and socially responsible depression severity
assessment. Our approach uniquely integrates student narrative text with a
teacher-derived, 9-dimensional "Empathy Vector" (EV), its dimensions guided by
the PHQ-9 framework,to explicitly translate tacit empathetic insight into a
structured AI input enhancing rather than replacing human judgment. Rigorous
experiments optimized the multimodal fusion, text representation, and
classification architecture, achieving 82.74% accuracy for 7-level severity
classification. This work demonstrates a path toward more responsible and
ethical affective computing by structurally embedding human empathy

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [322] [CrossNAS: A Cross-Layer Neural Architecture Search Framework for PIM Systems](https://arxiv.org/abs/2505.22868)
*Md Hasibul Amin,Mohammadreza Mohammadi,Jason D. Bakos,Ramtin Zand*

Main category: cs.ET

TL;DR: In this paper, the CrossNAS framework is proposed to optimize machine learning workloads deployment on analog PIM systems. It combines single-path one-shot weight-sharing strategy with evolutionary search and sets a new benchmark for PIM NAS.


<details>
  <summary>Details</summary>
Motivation: To explore a vast, multidimensional search space across circuits, architecture, and systems abstraction layers for optimizing the deployment of machine learning workloads on analog processing-in-memory (PIM) systems.

Method: Propose CrossNAS framework which leverages single-path one-shot weight-sharing strategy combined with evolutionary search in context of PIM system mapping and optimization.

Result: CrossNAS outperforms previous methods in both accuracy and energy efficiency while maintaining comparable or shorter search times.

Conclusion: CrossNAS sets a new benchmark for PIM neural architecture search.

Abstract: In this paper, we propose the CrossNAS framework, an automated approach for
exploring a vast, multidimensional search space that spans various design
abstraction layers-circuits, architecture, and systems-to optimize the
deployment of machine learning workloads on analog processing-in-memory (PIM)
systems. CrossNAS leverages the single-path one-shot weight-sharing strategy
combined with the evolutionary search for the first time in the context of PIM
system mapping and optimization. CrossNAS sets a new benchmark for PIM neural
architecture search (NAS), outperforming previous methods in both accuracy and
energy efficiency while maintaining comparable or shorter search times.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [323] [Fooling the Watchers: Breaking AIGC Detectors via Semantic Prompt Attacks](https://arxiv.org/abs/2505.23192)
*Run Hao,Peng Ying*

Main category: cs.CV

TL;DR: The paper presents an automated adversarial prompt generation framework that can evade AIGC detectors and ranked first in a real-world competition. It also aids in constructing high-quality adversarial datasets for training robust detection systems.


<details>
  <summary>Details</summary>
Motivation: The increasing ability of text-to-image models to generate realistic human portraits has led to concerns about identity misuse and the effectiveness of AIGC detectors.

Method: An automated adversarial prompt generation framework using a grammar tree structure and a variant of the Monte Carlo tree search algorithm to explore semantic prompt space and produce diverse, controllable prompts that can bypass AIGC detectors.

Result: Experiments across multiple T2I models confirm its effectiveness; it ranked first in a real-world adversarial AIGC detection competition and can be used to create high-quality adversarial datasets.

Conclusion: This method not only evades AIGC detectors but also provides valuable resources for improving the robustness of AIGC detection and defense systems.

Abstract: The rise of text-to-image (T2I) models has enabled the synthesis of
photorealistic human portraits, raising serious concerns about identity misuse
and the robustness of AIGC detectors. In this work, we propose an automated
adversarial prompt generation framework that leverages a grammar tree structure
and a variant of the Monte Carlo tree search algorithm to systematically
explore the semantic prompt space. Our method generates diverse, controllable
prompts that consistently evade both open-source and commercial AIGC detectors.
Extensive experiments across multiple T2I models validate its effectiveness,
and the approach ranked first in a real-world adversarial AIGC detection
competition. Beyond attack scenarios, our method can also be used to construct
high-quality adversarial datasets, providing valuable resources for training
and evaluating more robust AIGC detection and defense systems.

</details>


### [324] [MIAS-SAM: Medical Image Anomaly Segmentation without thresholding](https://arxiv.org/abs/2505.22762)
*Marco Colussi,Dragan Ahmetovic,Sergio Mascetti*

Main category: cs.CV

TL;DR: This paper introduces MIAS-SAM, a new method for segmenting anomalous regions in medical images without needing a threshold value, achieving accurate results across different imaging modalities.


<details>
  <summary>Details</summary>
Motivation: To improve the segmentation of anomalous regions in medical images by eliminating the need for defining a threshold value, which is required in previous works.

Method: MIAS-SAM uses a patch-based memory bank to store image features extracted from normal data via the SAM encoder. During inference, it compares embedding patches from the SAM encoder with those in the memory bank to create an anomaly map. Then, it computes the center of gravity of this map to prompt the SAM decoder for accurate segmentation.

Result: Achieved accurate anomaly segmentation capabilities, as measured by the DICE score, across three publicly available datasets with different imaging modalities (Brain MRI, Liver CT, and Retina OCT).

Conclusion: MIAS-SAM provides a novel approach for medical image segmentation that does not require a threshold value, demonstrating strong performance across various imaging modalities.

Abstract: This paper presents MIAS-SAM, a novel approach for the segmentation of
anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to
store relevant image features, which are extracted from normal data using the
SAM encoder. At inference time, the embedding patches extracted from the SAM
encoder are compared with those in the memory bank to obtain the anomaly map.
Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt
the SAM decoder, obtaining an accurate segmentation from the previously
extracted features. Differently from prior works, MIAS-SAM does not require to
define a threshold value to obtain the segmentation from the anomaly map.
Experimental results conducted on three publicly available datasets, each with
a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show
accurate anomaly segmentation capabilities measured using DICE score. The code
is available at: https://github.com/warpcut/MIAS-SAM

</details>


### [325] [IMTS is Worth Time $\times$ Channel Patches: Visual Masked Autoencoders for Irregular Multivariate Time Series Prediction](https://arxiv.org/abs/2505.22815)
*Zhangyi Hu,Jiemin Wu,Hua Xu,Mingqian Liao,Ninghui Feng,Bo Gao,Songning Lai,Yutao Yue*

Main category: cs.CV

TL;DR: The paper proposes VIMTS, a framework adapting Visual Mask AutoEncoder (MAE) for Irregular Multivariate Time Series (IMTS) forecasting. It processes IMTS data into feature patches, uses cross-channel dependencies to complement them, leverages visual MAE's capability for patch reconstruction, and applies a coarse-to-fine technique for precise predictions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with capturing reliable temporal patterns from IMTS data due to significant missing values. While pre-trained foundation models show potential, they are typically designed for Regularly Sampled Time Series (RTS).

Method: VIMTS processes IMTS along the timeline into feature patches at equal intervals, complements these patches using learned cross-channel dependencies, leverages visual MAE's capability in handling sparse multichannel data for patch reconstruction, and applies a coarse-to-fine technique to generate precise predictions.

Result: Extensive experiments demonstrate VIMTS's superior performance and few-shot capability in IMTS forecasting.

Conclusion: VIMTS advances the application of visual foundation models in more general time series tasks.

Abstract: Irregular Multivariate Time Series (IMTS) forecasting is challenging due to
the unaligned nature of multi-channel signals and the prevalence of extensive
missing data. Existing methods struggle to capture reliable temporal patterns
from such data due to significant missing values. While pre-trained foundation
models show potential for addressing these challenges, they are typically
designed for Regularly Sampled Time Series (RTS). Motivated by the visual Mask
AutoEncoder's (MAE) powerful capability for modeling sparse multi-channel
information and its success in RTS forecasting, we propose VIMTS, a framework
adapting Visual MAE for IMTS forecasting. To mitigate the effect of missing
values, VIMTS first processes IMTS along the timeline into feature patches at
equal intervals. These patches are then complemented using learned
cross-channel dependencies. Then it leverages visual MAE's capability in
handling sparse multichannel data for patch reconstruction, followed by a
coarse-to-fine technique to generate precise predictions from focused contexts.
In addition, we integrate self-supervised learning for improved IMTS modeling
by adapting the visual MAE to IMTS data. Extensive experiments demonstrate
VIMTS's superior performance and few-shot capability, advancing the application
of visual foundation models in more general time series tasks. Our code is
available at https://github.com/WHU-HZY/VIMTS.

</details>


### [326] [ATI: Any Trajectory Instruction for Controllable Video Generation](https://arxiv.org/abs/2505.22944)
*Angtian Wang,Haibin Huang,Jacob Zhiyuan Fang,Yiding Yang,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出了一种统一的框架，用于视频生成中的运动控制，通过轨迹输入无缝集成相机移动、对象级平移和细粒度局部运动。该方法通过轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间，提供比现有方法更好的可控性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常通过单独的模块或特定任务的设计来解决不同类型的运动，缺乏一个统一的解决方案。

Method: 提出了一种统一框架，使用轻量级运动注入器将用户定义的轨迹投影到预训练图像到视频生成模型的潜在空间，从而实现对局部变形、整体对象运动、虚拟摄像机动态或这些的组合的控制。

Result: 实验表明，该方法在多个视频运动控制任务中表现出色，包括风格化的运动效果、动态视点变化和精确的局部运动操作，提供了显著更好的可控性和视觉质量。

Conclusion: 该框架与各种最先进的视频生成骨干广泛兼容，并在多种视频运动控制任务上表现优异。

Abstract: We propose a unified framework for motion control in video generation that
seamlessly integrates camera movement, object-level translation, and
fine-grained local motion using trajectory-based inputs. In contrast to prior
methods that address these motion types through separate modules or
task-specific designs, our approach offers a cohesive solution by projecting
user-defined trajectories into the latent space of pre-trained image-to-video
generation models via a lightweight motion injector. Users can specify
keypoints and their motion paths to control localized deformations, entire
object motion, virtual camera dynamics, or combinations of these. The injected
trajectory signals guide the generative process to produce temporally
consistent and semantically aligned motion sequences. Our framework
demonstrates superior performance across multiple video motion control tasks,
including stylized motion effects (e.g., motion brushes), dynamic viewpoint
changes, and precise local motion manipulation. Experiments show that our
method provides significantly better controllability and visual quality
compared to prior approaches and commercial solutions, while remaining broadly
compatible with various state-of-the-art video generation backbones. Project
page: https://anytraj.github.io/.

</details>


### [327] [Toward Memory-Aided World Models: Benchmarking via Spatial Consistency](https://arxiv.org/abs/2505.22976)
*Kewei Lian,Shaofei Cai,Yilun Du,Yitao Liang*

Main category: cs.CV

TL;DR: The paper addresses the lack of datasets for promoting spatial consistency in memory modules by constructing a new dataset from Minecraft, which focuses on long-range spatial consistency and provides a benchmark for world models.


<details>
  <summary>Details</summary>
Motivation: Current datasets do not emphasize spatial consistency constraints in memory modules, focusing more on visual coherence or generation quality instead.

Method: The authors create a dataset with 150 distinct locations in Minecraft, collecting 250 hours of navigation videos. The dataset is designed with increasing sequence lengths to teach spatial consistency in complex trajectories.

Result: Four world model baselines were evaluated on the new benchmark, demonstrating the dataset's potential for advancing research in spatially consistent world models.

Conclusion: A new dataset and benchmark focusing on spatial consistency have been introduced, along with open-sourced materials to facilitate future research.

Abstract: The ability to simulate the world in a spatially consistent manner is a
crucial requirements for effective world models. Such a model enables
high-quality visual generation, and also ensures the reliability of world
models for downstream tasks such as simulation and planning. Designing a memory
module is a crucial component for addressing spatial consistency: such a model
must not only retain long-horizon observational information, but also enables
the construction of explicit or implicit internal spatial representations.
However, there are no dataset designed to promote the development of memory
modules by explicitly enforcing spatial consistency constraints. Furthermore,
most existing benchmarks primarily emphasize visual coherence or generation
quality, neglecting the requirement of long-range spatial consistency. To
bridge this gap, we construct a dataset and corresponding benchmark by sampling
150 distinct locations within the open-world environment of Minecraft,
collecting about 250 hours (20 million frames) of loop-based navigation videos
with actions. Our dataset follows a curriculum design of sequence lengths,
allowing models to learn spatial consistency on increasingly complex navigation
trajectories. Furthermore, our data collection pipeline is easily extensible to
new Minecraft environments and modules. Four representative world model
baselines are evaluated on our benchmark. Dataset, benchmark, and code are
open-sourced to support future research.

</details>


### [328] [Synthetic Document Question Answering in Hungarian](https://arxiv.org/abs/2505.23008)
*Jonathan Li,Zoltan Csaki,Nidhi Hiremath,Etash Guha,Fenglu Hong,Edward Ma,Urmish Thakker*

Main category: cs.CV

TL;DR: 现代视觉语言模型（VLMs）在英语文档视觉问答任务中已接近饱和精度，但在低资源语言上仍面临挑战。本文聚焦匈牙利语，构建了两个文档视觉问答数据集HuDocVQA和HuDocVQA-manual，并通过多轮质量过滤提升数据质量。此外，还发布了包含11.7万页匈牙利语PDF及其转录的HuCCPDF数据集。实验表明，在这些数据集上微调Llama 3.2 11B Instruct可使HuDocVQA上的准确率提升7.2%。所有数据集和代码将公开发布以推动多语言DocVQA研究。


<details>
  <summary>Details</summary>
Motivation: 尽管现代VLMs在英语文档视觉问答任务中表现出色，但低资源语言由于缺乏合适的训练和评估数据，仍然面临较大挑战。为解决这一问题并促进多语言DocVQA研究，本文选择匈牙利语作为研究对象，构建相关数据集。

Method: 1. 构建HuDocVQA-manual：基于Common Crawl中的匈牙利语文档，手动创建的小规模高质量数据集。
2. 构建HuDocVQA：从同一来源生成更大规模的合成VQA数据集，并通过多轮质量过滤和去重确保数据质量。
3. 发布HuCCPDF：包含11.7万页匈牙利语PDF及其转录的数据集，可用于训练匈牙利语OCR模型。
4. 实验验证：使用这些数据集对Llama 3.2 11B Instruct进行微调，并评估其在HuDocVQA上的性能提升。

Result: 在HuDocVQA数据集上，微调后的Llama 3.2 11B Instruct模型准确率提升了7.2%，证明了所构建数据集的质量和有效性。

Conclusion: 本文通过构建高质量的匈牙利语DocVQA数据集和OCR训练数据，有效提升了匈牙利语视觉问答任务的性能。这些数据集和代码的公开发布将为多语言DocVQA研究提供重要支持。

Abstract: Modern VLMs have achieved near-saturation accuracy in English document visual
question-answering (VQA). However, this task remains challenging in lower
resource languages due to a dearth of suitable training and evaluation data. In
this paper we present scalable methods for curating such datasets by focusing
on Hungarian, approximately the 17th highest resource language on the internet.
Specifically, we present HuDocVQA and HuDocVQA-manual, document VQA datasets
that modern VLMs significantly underperform on compared to English DocVQA.
HuDocVQA-manual is a small manually curated dataset based on Hungarian
documents from Common Crawl, while HuDocVQA is a larger synthetically generated
VQA data set from the same source. We apply multiple rounds of quality
filtering and deduplication to HuDocVQA in order to match human-level quality
in this dataset. We also present HuCCPDF, a dataset of 117k pages from
Hungarian Common Crawl PDFs along with their transcriptions, which can be used
for training a model for Hungarian OCR. To validate the quality of our
datasets, we show how finetuning on a mixture of these datasets can improve
accuracy on HuDocVQA for Llama 3.2 11B Instruct by +7.2%. Our datasets and code
will be released to the public to foster further research in multilingual
DocVQA.

</details>


### [329] [Are Unified Vision-Language Models Necessary: Generalization Across Understanding and Generation](https://arxiv.org/abs/2505.23043)
*Jihai Zhang,Tianle Li,Linjie Li,Zhengyuan Yang,Yu Cheng*

Main category: cs.CV

TL;DR: Recent advancements in unified vision-language models (VLMs) have gained significant attention. This paper systematically investigates the generalization across understanding and generation tasks in unified VLMs, designs a real-world aligned dataset for experiments, and evaluates multiple architectures. Key findings include mutual benefits in understanding and generation tasks, importance of alignment between input and output spaces, and cross-task knowledge transfer.


<details>
  <summary>Details</summary>
Motivation: Prior works on unified VLMs have underexplored the hypothesis that a unified architecture with mixed training can enable mutual enhancement between understanding and generation tasks.

Method: The authors design a dataset closely aligned with real-world scenarios to facilitate extensive experiments and quantitative evaluations. They evaluate multiple unified VLM architectures.

Result: 1. Unified VLMs trained with mixed data exhibit mutual benefits in understanding and generation tasks across various architectures, scaling up with increased data. 2. Better alignment between multimodal input and output spaces leads to better generalization. 3. Knowledge from generation tasks can transfer to understanding tasks within the base language model.

Conclusion: The findings highlight the necessity of unifying understanding and generation in VLMs, providing valuable insights for their design and optimization.

Abstract: Recent advancements in unified vision-language models (VLMs), which integrate
both visual understanding and generation capabilities, have attracted
significant attention. The underlying hypothesis is that a unified architecture
with mixed training on both understanding and generation tasks can enable
mutual enhancement between understanding and generation. However, this
hypothesis remains underexplored in prior works on unified VLMs. To address
this gap, this paper systematically investigates the generalization across
understanding and generation tasks in unified VLMs. Specifically, we design a
dataset closely aligned with real-world scenarios to facilitate extensive
experiments and quantitative evaluations. We evaluate multiple unified VLM
architectures to validate our findings. Our key findings are as follows. First,
unified VLMs trained with mixed data exhibit mutual benefits in understanding
and generation tasks across various architectures, and this mutual benefits can
scale up with increased data. Second, better alignment between multimodal input
and output spaces will lead to better generalization. Third, the knowledge
acquired during generation tasks can transfer to understanding tasks, and this
cross-task generalization occurs within the base language model, beyond
modality adapters. Our findings underscore the critical necessity of unifying
understanding and generation in VLMs, offering valuable insights for the design
and optimization of unified VLMs.

</details>


### [330] [Multi-Sourced Compositional Generalization in Visual Question Answering](https://arxiv.org/abs/2505.23045)
*Chuanhao Li,Wenbo Ye,Zhen Li,Yuwei Wu,Yunde Jia*

Main category: cs.CV

TL;DR: This paper explores multi-sourced compositional generalization (MSCG) in visual question answering (VQA), proposing a retrieval-augmented training framework that enhances VQA models' MSCG ability by learning unified representations for primitives from different modalities. They also construct a new GQA-MSCG dataset for evaluating this ability.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of exploration into multi-sourced compositional generalization (MSCG) in vision-and-language tasks, specifically within the context of VQA. The authors aim to enhance the generalization ability of VQA models when dealing with novel compositions sourced from different modalities.

Method: The method proposed in this paper involves a retrieval-augmented training framework designed to improve the MSCG ability of VQA models. This framework retrieves semantically equivalent primitives for each primitive in the training samples and aggregates these retrieved features with the original primitive to refine the model. By doing so, the model learns consistent representations for the same semantic primitives across different modalities.

Result: The experimental results on the newly constructed GQA-MSCG dataset demonstrate the effectiveness of the proposed framework in enhancing the MSCG ability of VQA models.

Conclusion: The conclusion of this paper is that the proposed retrieval-augmented training framework successfully improves the multi-sourced compositional generalization ability of VQA models. Additionally, the newly constructed GQA-MSCG dataset serves as a valuable resource for evaluating this ability.

Abstract: Compositional generalization is the ability of generalizing novel
compositions from seen primitives, and has received much attention in
vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L
tasks, the primitives composing compositions source from different modalities,
resulting in multi-sourced novel compositions. However, the generalization
ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced
compositional generalization (MSCG) remains unexplored. In this paper, we
explore MSCG in the context of visual question answering (VQA), and propose a
retrieval-augmented training framework to enhance the MSCG ability of VQA
models by learning unified representations for primitives from different
modalities. Specifically, semantically equivalent primitives are retrieved for
each primitive in the training samples, and the retrieved features are
aggregated with the original primitive to refine the model. This process helps
the model learn consistent representations for the same semantic primitives
across different modalities. To evaluate the MSCG ability of VQA models, we
construct a new GQA-MSCG dataset based on the GQA dataset, in which samples
include three types of novel compositions composed of primitives from different
modalities. Experimental results demonstrate the effectiveness of the proposed
framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.

</details>


### [331] [GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion](https://arxiv.org/abs/2505.23085)
*Gwanghyun Kim,Xueting Li,Ye Yuan,Koki Nagano,Tianye Li,Jan Kautz,Se Young Chun,Umar Iqbal*

Main category: cs.CV

TL;DR: GeoMan is a novel architecture that produces accurate and temporally consistent depth and normal estimations from monocular human videos by overcoming the scarcity of high-quality 4D training data and improving human size estimation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for estimating 3D human geometry from videos suffer from temporal inconsistencies and fail to capture fine-grained dynamic details, especially when primarily optimized for single images.

Method: GeoMan employs an image-based model to estimate depth and normals for the first frame of a video, conditioning a video diffusion model. It uses root-relative depth representation for accurate human size estimation.

Result: GeoMan achieves state-of-the-art performance in both qualitative and quantitative evaluations, demonstrating its effectiveness in overcoming longstanding challenges in 3D human geometry estimation from videos.

Conclusion: GeoMan presents a successful approach to produce accurate and temporally consistent depth and normal estimations from monocular human videos while requiring minimal 4D training data.

Abstract: Estimating accurate and temporally consistent 3D human geometry from videos
is a challenging problem in computer vision. Existing methods, primarily
optimized for single images, often suffer from temporal inconsistencies and
fail to capture fine-grained dynamic details. To address these limitations, we
present GeoMan, a novel architecture designed to produce accurate and
temporally consistent depth and normal estimations from monocular human videos.
GeoMan addresses two key challenges: the scarcity of high-quality 4D training
data and the need for metric depth estimation to accurately model human size.
To overcome the first challenge, GeoMan employs an image-based model to
estimate depth and normals for the first frame of a video, which then
conditions a video diffusion model, reframing video geometry estimation task as
an image-to-video generation problem. This design offloads the heavy lifting of
geometric estimation to the image model and simplifies the video model's role
to focus on intricate details while using priors learned from large-scale video
datasets. Consequently, GeoMan improves temporal consistency and
generalizability while requiring minimal 4D training data. To address the
challenge of accurate human size estimation, we introduce a root-relative depth
representation that retains critical human-scale details and is easier to be
estimated from monocular inputs, overcoming the limitations of traditional
affine-invariant and metric depth representations. GeoMan achieves
state-of-the-art performance in both qualitative and quantitative evaluations,
demonstrating its effectiveness in overcoming longstanding challenges in 3D
human geometry estimation from videos.

</details>


### [332] [EAD: An EEG Adapter for Automated Classification](https://arxiv.org/abs/2505.23107)
*Pushapdeep Singh,Jyoti Nigam,Medicherla Vamsi Krishna,Arnav Bhavsar,Aditya Nigam*

Main category: cs.CV

TL;DR: This paper proposes EEG Adapter (EAD), a flexible framework for learning robust EEG embeddings irrespective of the number of EEG channels, achieving state-of-the-art accuracies on two datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of developing a unified pipeline for EEG signal classification that can handle data from different acquisition devices and varying numbers of EEG channels.

Method: The method involves leveraging a recent EEG foundational model with significant adaptations to learn robust representations from EEG data. This framework, called EEG Adapter (EAD), is compatible with any signal acquisition device.

Result: EAD achieved state-of-the-art accuracies of 99.33% on EEG-ImageNet and 92.31% on BrainLat. It also demonstrated generalization capability through zero-shot EEG classification on the EEG-ImageNet task.

Conclusion: The proposed EEG Adapter framework shows effectiveness across diverse EEG datasets and perception tasks, demonstrating its potential for handling EEG data with varying characteristics.

Abstract: While electroencephalography (EEG) has been a popular modality for neural
decoding, it often involves task specific acquisition of the EEG data. This
poses challenges for the development of a unified pipeline to learn embeddings
for various EEG signal classification, which is often involved in various
decoding tasks. Traditionally, EEG classification involves the step of signal
preprocessing and the use of deep learning techniques, which are highly
dependent on the number of EEG channels in each sample. However, the same
pipeline cannot be applied even if the EEG data is collected for the same
experiment but with different acquisition devices. This necessitates the
development of a framework for learning EEG embeddings, which could be highly
beneficial for tasks involving multiple EEG samples for the same task but with
varying numbers of EEG channels. In this work, we propose EEG Adapter (EAD), a
flexible framework compatible with any signal acquisition device. More
specifically, we leverage a recent EEG foundational model with significant
adaptations to learn robust representations from the EEG data for the
classification task. We evaluate EAD on two publicly available datasets
achieving state-of-the-art accuracies 99.33% and 92.31% on EEG-ImageNet and
BrainLat respectively. This illustrates the effectiveness of the proposed
framework across diverse EEG datasets containing two different perception
tasks: stimulus and resting-state EEG signals. We also perform zero-shot EEG
classification on EEG-ImageNet task to demonstrate the generalization
capability of the proposed approach.

</details>


### [333] [Zero-to-Hero: Zero-Shot Initialization Empowering Reference-Based Video Appearance Editing](https://arxiv.org/abs/2505.23134)
*Tongtong Su,Chengyu Wang,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: Appearance editing in video according to user needs is challenging due to ambiguities and lack of fine-grained control. This paper introduces Zero-to-Hero, a reference-based video editing method that separates editing into two problems: editing an anchor frame and propagating its appearance across other frames. It uses correspondence within original frames to guide attention mechanism for robustness, offers zero-shot initialization ensuring accuracy and temporal consistency, and addresses imaging degradation issues through Hero-Stage learning for video restoration. The method outperforms the best baseline with a PSNR improvement of 2.6 dB.


<details>
  <summary>Details</summary>
Motivation: Existing text-guided methods for appearance editing often lead to ambiguities regarding user intentions and restrict fine-grained control over specific aspects of objects.

Method: The proposed method, Zero-to-Hero, focuses on reference-based video editing by disentangling the process into editing an anchor frame as a reference image and then consistently propagating its appearance across other frames. Correspondence within original frames guides the attention mechanism, which is more robust than optical flow or temporal modules. The method also includes a Hero-Stage that learns a conditional generative model for video restoration.

Result: The method outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. A set of videos with multiple appearances was constructed using Blender for fine-grained and deterministic evaluation.

Conclusion: Zero-to-Hero provides a novel approach to overcome limitations in existing text-guided methods for video editing, offering improved accuracy, temporal consistency, and addressing imaging degradation issues.

Abstract: Appearance editing according to user needs is a pivotal task in video
editing. Existing text-guided methods often lead to ambiguities regarding user
intentions and restrict fine-grained control over editing specific aspects of
objects. To overcome these limitations, this paper introduces a novel approach
named {Zero-to-Hero}, which focuses on reference-based video editing that
disentangles the editing process into two distinct problems. It achieves this
by first editing an anchor frame to satisfy user requirements as a reference
image and then consistently propagating its appearance across other frames. We
leverage correspondence within the original frames to guide the attention
mechanism, which is more robust than previously proposed optical flow or
temporal modules in memory-friendly video generative models, especially when
dealing with objects exhibiting large motions. It offers a solid ZERO-shot
initialization that ensures both accuracy and temporal consistency. However,
intervention in the attention mechanism results in compounded imaging
degradation with over-saturated colors and unknown blurring issues. Starting
from Zero-Stage, our Hero-Stage Holistically learns a conditional generative
model for vidEo RestOration. To accurately evaluate the consistency of the
appearance, we construct a set of videos with multiple appearances using
Blender, enabling a fine-grained and deterministic evaluation. Our method
outperforms the best-performing baseline with a PSNR improvement of 2.6 dB. The
project page is at https://github.com/Tonniia/Zero2Hero.

</details>


### [334] [FlowAlign: Trajectory-Regularized, Inversion-Free Flow-based Image Editing](https://arxiv.org/abs/2505.23145)
*Jeongsol Kim,Yeobin Hong,Jong Chul Ye*

Main category: cs.CV

TL;DR: FlowAlign是一种新的无反转、基于流的图像编辑框架，通过引入流匹配损失来提高编辑轨迹的稳定性和源一致性。它支持可逆编辑，并在源保留和编辑可控性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的无反转、基于流的图像编辑方法（如FlowEdit）虽然避免了精确的潜在反转，但存在编辑轨迹不稳定和源一致性差的问题。

Method: FlowAlign提出了一种无反转的基于流的框架，通过引入流匹配损失作为正则化机制，促进编辑过程中的平滑和稳定轨迹。该损失平衡了与编辑提示的语义对齐和与源图像的结构一致性，并支持通过反转ODE轨迹实现可逆编辑。

Result: 广泛的实验表明，FlowAlign在源保留和编辑可控性方面优于现有方法。

Conclusion: FlowAlign解决了现有方法中编辑轨迹不稳定和源一致性差的问题，提供了一种更稳定、可控且可逆的图像编辑方案。

Abstract: Recent inversion-free, flow-based image editing methods such as FlowEdit
leverages a pre-trained noise-to-image flow model such as Stable Diffusion 3,
enabling text-driven manipulation by solving an ordinary differential equation
(ODE). While the lack of exact latent inversion is a core advantage of these
methods, it often results in unstable editing trajectories and poor source
consistency. To address this limitation, we propose FlowAlign, a novel
inversion-free flow-based framework for consistent image editing with
principled trajectory control. FlowAlign introduces a flow-matching loss as a
regularization mechanism to promote smoother and more stable trajectories
during the editing process. Notably, the flow-matching loss is shown to
explicitly balance semantic alignment with the edit prompt and structural
consistency with the source image along the trajectory. Furthermore, FlowAlign
naturally supports reverse editing by simply reversing the ODE trajectory,
highlighting the reversible and consistent nature of the transformation.
Extensive experiments demonstrate that FlowAlign outperforms existing methods
in both source preservation and editing controllability.

</details>


### [335] [Implicit Inversion turns CLIP into a Decoder](https://arxiv.org/abs/2505.23161)
*Antonio D'Orazio,Maria Rosaria Briglia,Donato Crisostomi,Dario Loi,Emanuele Rodolà,Iacopo Masi*

Main category: cs.CV

TL;DR: A method for image synthesis using CLIP alone,without any decoder,training,or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore the generative potential of discriminative models like CLIP without requiring additional decoders or training.

Method: Optimize a frequency-aware implicit neural representation with adversarially robust initialization,Orthogonal Procrustes projection,and blending loss to stabilize inverse mapping.

Result: Achieves text-to-image generation,style transfer,and image reconstruction without altering CLIP's weights.

Conclusion: Discriminative models may hold untapped generative potential.

Abstract: CLIP is a discriminative model trained to align images and text in a shared
embedding space. Due to its multimodal structure, it serves as the backbone of
many generative pipelines, where a decoder is trained to map from the shared
space back to images. In this work, we show that image synthesis is
nevertheless possible using CLIP alone -- without any decoder, training, or
fine-tuning. Our approach optimizes a frequency-aware implicit neural
representation that encourages coarse-to-fine generation by stratifying
frequencies across network layers. To stabilize this inverse mapping, we
introduce adversarially robust initialization, a lightweight Orthogonal
Procrustes projection to align local text and image embeddings, and a blending
loss that anchors outputs to natural image statistics. Without altering CLIP's
weights, this framework unlocks capabilities such as text-to-image generation,
style transfer, and image reconstruction. These findings suggest that
discriminative models may hold untapped generative potential, hidden in plain
sight.

</details>


### [336] [SAMamba: Adaptive State Space Modeling with Hierarchical Vision for Infrared Small Target Detection](https://arxiv.org/abs/2505.23214)
*Wenhao Xu,Shuchen Zheng,Changwei Wang,Zherui Zhang,Chuan Ren,Rongtao Xu,Shibiao Xu*

Main category: cs.CV

TL;DR: SAMamba is a new framework for Infrared Small Target Detection (ISTD) that combines SAM2's hierarchical feature learning with Mamba's selective sequence modeling. It includes three key innovations: Feature Selection Adapter, Cross-Channel State-Space Interaction module, and Detail-Preserving Contextual Fusion module. Experiments show it significantly outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for ISTD suffer from information loss during downsampling and inefficient global context modeling. The paper aims to address these issues by proposing a novel framework.

Method: The method integrates SAM2's hierarchical feature learning with Mamba's selective sequence modeling. It introduces Feature Selection Adapter for domain adaptation, Cross-Channel State-Space Interaction module for global context modeling, and Detail-Preserving Contextual Fusion module for combining multi-scale features.

Result: Experiments on NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets demonstrate that SAMamba significantly outperforms state-of-the-art methods, particularly in challenging scenarios with heterogeneous backgrounds and varying target scales.

Conclusion: SAMamba effectively addresses the core challenges of ISTD by bridging the domain gap, maintaining fine-grained details, and efficiently modeling long-range dependencies.

Abstract: Infrared small target detection (ISTD) is vital for long-range surveillance
in military, maritime, and early warning applications. ISTD is challenged by
targets occupying less than 0.15% of the image and low distinguishability from
complex backgrounds. Existing deep learning methods often suffer from
information loss during downsampling and inefficient global context modeling.
This paper presents SAMamba, a novel framework integrating SAM2's hierarchical
feature learning with Mamba's selective sequence modeling. Key innovations
include: (1) A Feature Selection Adapter (FS-Adapter) for efficient
natural-to-infrared domain adaptation via dual-stage selection (token-level
with a learnable task embedding and channel-wise adaptive transformations); (2)
A Cross-Channel State-Space Interaction (CSI) module for efficient global
context modeling with linear complexity using selective state space modeling;
and (3) A Detail-Preserving Contextual Fusion (DPCF) module that adaptively
combines multi-scale features with a gating mechanism to balance
high-resolution and low-resolution feature contributions. SAMamba addresses
core ISTD challenges by bridging the domain gap, maintaining fine-grained
details, and efficiently modeling long-range dependencies. Experiments on
NUAA-SIRST, IRSTD-1k, and NUDT-SIRST datasets show SAMamba significantly
outperforms state-of-the-art methods, especially in challenging scenarios with
heterogeneous backgrounds and varying target scales. Code:
https://github.com/zhengshuchen/SAMamba.

</details>


### [337] [Unsupervised Transcript-assisted Video Summarization and Highlight Detection](https://arxiv.org/abs/2505.23268)
*Spyros Barbakos,Charalampos Antoniadis,Gerasimos Potamianos,Gianluca Setti*

Main category: cs.CV

TL;DR: This paper proposes a multimodal pipeline within an RL framework that integrates video frames and transcripts for better video summarization and highlight detection.


<details>
  <summary>Details</summary>
Motivation: Existing works on video summarization and highlight detection either focus solely on video frames or transcripts, or use Reinforcement Learning (RL) without combining both modalities. This creates a need for a more comprehensive approach that leverages both modalities to improve the quality of video summaries and highlight detection.

Method: The method involves creating a multimodal pipeline that combines video frames and their corresponding transcripts. This pipeline is trained in an RL framework which encourages the generation of diverse and representative summaries while ensuring meaningful transcript content is included. The process is unsupervised, allowing it to learn from large-scale unannotated datasets.

Result: Experimental results demonstrate that incorporating transcripts in video summarization and highlight detection yields better outcomes compared to using only the visual content of videos.

Conclusion: Integrating both video frames and transcripts within an RL framework leads to superior video summarization and highlight detection. This unsupervised approach can effectively utilize large-scale unannotated data to overcome limitations posed by small annotated datasets.

Abstract: Video consumption is a key part of daily life, but watching entire videos can
be tedious. To address this, researchers have explored video summarization and
highlight detection to identify key video segments. While some works combine
video frames and transcripts, and others tackle video summarization and
highlight detection using Reinforcement Learning (RL), no existing work, to the
best of our knowledge, integrates both modalities within an RL framework. In
this paper, we propose a multimodal pipeline that leverages video frames and
their corresponding transcripts to generate a more condensed version of the
video and detect highlights using a modality fusion mechanism. The pipeline is
trained within an RL framework, which rewards the model for generating diverse
and representative summaries while ensuring the inclusion of video segments
with meaningful transcript content. The unsupervised nature of the training
allows for learning from large-scale unannotated datasets, overcoming the
challenge posed by the limited size of existing annotated datasets. Our
experiments show that using the transcript in video summarization and highlight
detection achieves superior results compared to relying solely on the visual
content of the video.

</details>


### [338] [Federated Unsupervised Semantic Segmentation](https://arxiv.org/abs/2505.23292)
*Evangelos Charalampakis,Vasileios Mygdalis,Ioannis Pitas*

Main category: cs.CV

TL;DR: This paper introduces FUSS (Federated Unsupervised image Semantic Segmentation), the first framework for fully decentralized, label-free semantic segmentation training using Federated Learning.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to extend recent unsupervised semantic image segmentation (USS) methods into federated learning settings, which requires feature representation and cluster centroid alignment across distributed clients in the absence of supervision.

Method: FUSS proposes novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids.

Result: Experiments on benchmark and real-world datasets show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions.

Conclusion: FUSS enables fully decentralized, label-free semantic segmentation training and shows superior performance compared to other methods.

Abstract: This work explores the application of Federated Learning (FL) in Unsupervised
Semantic image Segmentation (USS). Recent USS methods extract pixel-level
features using frozen visual foundation models and refine them through
self-supervised objectives that encourage semantic grouping. These features are
then grouped to semantic clusters to produce segmentation masks. Extending
these ideas to federated settings requires feature representation and cluster
centroid alignment across distributed clients -- an inherently difficult task
under heterogeneous data distributions in the absence of supervision. To
address this, we propose FUSS Federated Unsupervised image Semantic
Segmentation) which is, to our knowledge, the first framework to enable fully
decentralized, label-free semantic segmentation training. FUSS introduces novel
federation strategies that promote global consistency in feature and prototype
space, jointly optimizing local segmentation heads and shared semantic
centroids. Experiments on both benchmark and real-world datasets, including
binary and multi-class segmentation tasks, show that FUSS consistently
outperforms local-only client trainings as well as extensions of classical FL
algorithms under varying client data distributions. To support reproducibility,
full code will be released upon manuscript acceptance.

</details>


### [339] [Adversarial Semantic and Label Perturbation Attack for Pedestrian Attribute Recognition](https://arxiv.org/abs/2505.23313)
*Weizhe Kong,Xiao Wang,Ruichong Gao,Chenglong Li,Yu Zhang,Xing Yang,Yaowei Wang,Jin Tang*

Main category: cs.CV

TL;DR: The paper introduces the first adversarial attack and defense framework for pedestrian attribute recognition (PAR). It uses global- and patch-level attacks on pedestrian images based on a pre-trained CLIP-based PAR framework. A multi-modal Transformer fuses vision and text tokens, and a feed-forward network is used for attribute recognition. The authors adopt adversarial semantic and label-perturbation to generate noise (ASL-PAR) and design a semantic offset defense strategy. Experiments validate the effectiveness of these strategies in both digital and physical domains.


<details>
  <summary>Details</summary>
Motivation: Despite the progress made in pedestrian attribute recognition with deep neural networks, the vulnerability and anti-interference ability have not been fully explored. This motivated the development of an adversarial attack and defense framework specific to PAR.

Method: The method involves exploiting global- and patch-level attacks on pedestrian images using a pre-trained CLIP-based PAR framework. The process includes dividing input images into patches, embedding them into feature embeddings, expanding attribute sets into sentences, and embedding them into attribute features using a pre-trained CLIP text encoder. A multi-modal Transformer fuses vision and text tokens, while a feed-forward network handles attribute recognition. Adversarial semantic and label-perturbation are used to generate adversarial noise (ASL-PAR), and a semantic offset defense strategy is designed to mitigate adversarial attacks.

Result: Extensive experiments conducted across multiple datasets in both digital and physical domains demonstrate the effectiveness of the proposed adversarial attack and defense strategies for pedestrian attribute recognition.

Conclusion: This paper successfully introduces the first adversarial attack and defense framework for pedestrian attribute recognition, proving its effectiveness through comprehensive experiments. The source code will be publicly available.

Abstract: Pedestrian Attribute Recognition (PAR) is an indispensable task in
human-centered research and has made great progress in recent years with the
development of deep neural networks. However, the potential vulnerability and
anti-interference ability have still not been fully explored. To bridge this
gap, this paper proposes the first adversarial attack and defense framework for
pedestrian attribute recognition. Specifically, we exploit both global- and
patch-level attacks on the pedestrian images, based on the pre-trained
CLIP-based PAR framework. It first divides the input pedestrian image into
non-overlapping patches and embeds them into feature embeddings using a
projection layer. Meanwhile, the attribute set is expanded into sentences using
prompts and embedded into attribute features using a pre-trained CLIP text
encoder. A multi-modal Transformer is adopted to fuse the obtained vision and
text tokens, and a feed-forward network is utilized for attribute recognition.
Based on the aforementioned PAR framework, we adopt the adversarial semantic
and label-perturbation to generate the adversarial noise, termed ASL-PAR. We
also design a semantic offset defense strategy to suppress the influence of
adversarial attacks. Extensive experiments conducted on both digital domains
(i.e., PETA, PA100K, MSP60K, RAPv2) and physical domains fully validated the
effectiveness of our proposed adversarial attack and defense strategies for the
pedestrian attribute recognition. The source code of this paper will be
released on https://github.com/Event-AHU/OpenPAR.

</details>


### [340] [Fine-Tuning Next-Scale Visual Autoregressive Models with Group Relative Policy Optimization](https://arxiv.org/abs/2505.23331)
*Matteo Gallici,Haitz Sáez de Ocáriz Borde*

Main category: cs.CV

TL;DR: 通过强化学习微调预训练生成模型是一种有效的使输出更符合人类复杂偏好的方法。本文研究了使用组相对策略优化（GRPO）对下一代视觉自回归（VAR）模型进行微调。实验结果表明，这种方法能够与从美学预测器和CLIP嵌入中得出的复杂奖励信号对齐，显著提高图像质量并实现对生成风格的精确控制。此外，利用CLIP，我们的方法可以帮助VAR模型超越其初始的ImageNet分布，生成与未在预训练中出现的图像风格相关的图像。总之，基于强化学习的微调对于VAR模型既高效又有效，尤其得益于其快速推理速度，这对在线采样非常有利，而这是扩散模型面临的重大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型虽然强大，但在与复杂的、细微的人类偏好对齐方面仍存在不足。因此，需要一种新的方法来改进这些模型的输出以更好地满足特定需求，例如更高的图像质量和可控的生成风格。

Method: 采用组相对策略优化（GRPO）对下一代视觉自回归（VAR）模型进行微调，利用强化学习驱动探索，并结合美学预测器和CLIP嵌入作为奖励信号来指导模型生成过程。

Result: 成功实现了与复杂奖励信号的对齐，提高了图像质量并增强了对生成风格的控制能力。此外，VAR模型能够生成超出其原始训练数据范围的图像风格，验证了方法的有效性和通用性。

Conclusion: 基于强化学习的微调方法对于下一代视觉自回归模型是高效且有效的，特别是在快速推理和在线采样的应用场景中表现出色，优于扩散模型等其他替代方案。

Abstract: Fine-tuning pre-trained generative models with Reinforcement Learning (RL)
has emerged as an effective approach for aligning outputs more closely with
nuanced human preferences. In this paper, we investigate the application of
Group Relative Policy Optimization (GRPO) to fine-tune next-scale visual
autoregressive (VAR) models. Our empirical results demonstrate that this
approach enables alignment to intricate reward signals derived from aesthetic
predictors and CLIP embeddings, significantly enhancing image quality and
enabling precise control over the generation style. Interestingly, by
leveraging CLIP, our method can help VAR models generalize beyond their initial
ImageNet distribution: through RL-driven exploration, these models can generate
images aligned with prompts referencing image styles that were absent during
pre-training. In summary, we show that RL-based fine-tuning is both efficient
and effective for VAR models, benefiting particularly from their fast inference
speeds, which are advantageous for online sampling, an aspect that poses
significant challenges for diffusion-based alternatives.

</details>


### [341] [PAN-Crafter: Learning Modality-Consistent Alignment for PAN-Sharpening](https://arxiv.org/abs/2505.23367)
*Jeonghyeok Do,Sungpyo Kim,Geunhyuk Youk,Jaehyup Lee,Munchurl Kim*

Main category: cs.CV

TL;DR: PAN-Crafter is a new framework for PAN-sharpening that addresses cross-modality misalignment using Modality-Adaptive Reconstruction and Cross-Modality Alignment-Aware Attention, achieving state-of-the-art performance with faster inference and less memory.


<details>
  <summary>Details</summary>
Motivation: Conventional deep learning methods for PAN-sharpening assume perfect pixel-wise alignment and rely on per-pixel reconstruction losses, leading to spectral distortion, double edges, and blurring when cross-modality misalignment is present.

Method: The method proposes PAN-Crafter, which includes Modality-Adaptive Reconstruction (MARs) that enables joint reconstruction of HRMS and PAN images, and Cross-Modality Alignment-Aware Attention (CM3A) that bidirectionally aligns MS texture to PAN structure and vice versa.

Result: Extensive experiments demonstrate that PAN-Crafter outperforms the most recent state-of-the-art method in all metrics, with significantly faster inference time and reduced memory usage. It also shows strong generalization on unseen satellite datasets.

Conclusion: PAN-Crafter effectively mitigates the misalignment gap between PAN and MS modalities, achieving superior performance, efficiency, and robustness across different conditions.

Abstract: PAN-sharpening aims to fuse high-resolution panchromatic (PAN) images with
low-resolution multi-spectral (MS) images to generate high-resolution
multi-spectral (HRMS) outputs. However, cross-modality misalignment -- caused
by sensor placement, acquisition timing, and resolution disparity -- induces a
fundamental challenge. Conventional deep learning methods assume perfect
pixel-wise alignment and rely on per-pixel reconstruction losses, leading to
spectral distortion, double edges, and blurring when misalignment is present.
To address this, we propose PAN-Crafter, a modality-consistent alignment
framework that explicitly mitigates the misalignment gap between PAN and MS
modalities. At its core, Modality-Adaptive Reconstruction (MARs) enables a
single network to jointly reconstruct HRMS and PAN images, leveraging PAN's
high-frequency details as auxiliary self-supervision. Additionally, we
introduce Cross-Modality Alignment-Aware Attention (CM3A), a novel mechanism
that bidirectionally aligns MS texture to PAN structure and vice versa,
enabling adaptive feature refinement across modalities. Extensive experiments
on multiple benchmark datasets demonstrate that our PAN-Crafter outperforms the
most recent state-of-the-art method in all metrics, even with 50.11$\times$
faster inference time and 0.63$\times$ the memory size. Furthermore, it
demonstrates strong generalization performance on unseen satellite datasets,
showing its robustness across different conditions.

</details>


### [342] [VModA: An Effective Framework for Adaptive NSFW Image Moderation](https://arxiv.org/abs/2505.23386)
*Han Bao,Qinying Wang,Zhi Chen,Qingming Li,Xuhong Zhang,Changjiang Li,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: The paper proposes VModA, a framework for detecting NSFW content that adapts to diverse moderation rules and handles complex semantics, showing significant accuracy improvement and adaptability.


<details>
  <summary>Details</summary>
Motivation: NSFW images are becoming more sophisticated, evading current detection methods, and varying regulations complicate effective moderation.

Method: VModA, a general and effective framework that adapts to diverse moderation rules and handles complex, semantically rich NSFW content across categories.

Result: Experimental results show that VModA significantly outperforms existing methods, achieving up to a 54.3% accuracy improvement across NSFW types, including those with complex semantics.

Conclusion: VModA exhibits strong adaptability across categories, scenarios, and base VLMs, and has been evaluated in real-world scenarios to demonstrate its practical effectiveness.

Abstract: Not Safe/Suitable for Work (NSFW) content is rampant on social networks and
poses serious harm to citizens, especially minors. Current detection methods
mainly rely on deep learning-based image recognition and classification.
However, NSFW images are now presented in increasingly sophisticated ways,
often using image details and complex semantics to obscure their true nature or
attract more views. Although still understandable to humans, these images often
evade existing detection methods, posing a significant threat. Further
complicating the issue, varying regulations across platforms and regions create
additional challenges for effective moderation, leading to detection bias and
reduced accuracy. To address this, we propose VModA, a general and effective
framework that adapts to diverse moderation rules and handles complex,
semantically rich NSFW content across categories. Experimental results show
that VModA significantly outperforms existing methods, achieving up to a 54.3%
accuracy improvement across NSFW types, including those with complex semantics.
Further experiments demonstrate that our method exhibits strong adaptability
across categories, scenarios, and base VLMs. We also identified inconsistent
and controversial label samples in public NSFW benchmark datasets, re-annotated
them, and submitted corrections to the original maintainers. Two datasets have
confirmed the updates so far. Additionally, we evaluate VModA in real-world
scenarios to demonstrate its practical effectiveness.

</details>


### [343] [Video Editing for Audio-Visual Dubbing](https://arxiv.org/abs/2505.23406)
*Binyamin Manela,Sharon Gannot,Ethan Fetyaya*

Main category: cs.CV

TL;DR: EdiDub is a new framework reformulating visual dubbing as content-aware editing, preserving original video context and improving lip synchronization with complex visual elements.


<details>
  <summary>Details</summary>
Motivation: Current visual dubbing methods either generate talking faces or use inpainting techniques, both of which have limitations in maintaining original scenes' integrity and vital visual information like occlusions and lighting variations.

Method: EdiDub utilizes a specialized conditioning scheme to ensure faithful and accurate modifications rather than mere copying, thus preserving the original video context while achieving lip synchronization with new speech.

Result: On various benchmarks, including an occluded-lip dataset, EdiDub significantly improves identity preservation and synchronization. Human evaluations confirm its superiority with higher synchronization and visual naturalness scores compared to leading methods.

Conclusion: The content-aware editing approach of EdiDub outperforms traditional generation or inpainting methods in maintaining complex visual elements while ensuring accurate lip synchronization.

Abstract: Visual dubbing, the synchronization of facial movements with new speech, is
crucial for making content accessible across different languages, enabling
broader global reach. However, current methods face significant limitations.
Existing approaches often generate talking faces, hindering seamless
integration into original scenes, or employ inpainting techniques that discard
vital visual information like partial occlusions and lighting variations. This
work introduces EdiDub, a novel framework that reformulates visual dubbing as a
content-aware editing task. EdiDub preserves the original video context by
utilizing a specialized conditioning scheme to ensure faithful and accurate
modifications rather than mere copying. On multiple benchmarks, including a
challenging occluded-lip dataset, EdiDub significantly improves identity
preservation and synchronization. Human evaluations further confirm its
superiority, achieving higher synchronization and visual naturalness scores
compared to the leading methods. These results demonstrate that our
content-aware editing approach outperforms traditional generation or
inpainting, particularly in maintaining complex visual elements while ensuring
accurate lip synchronization.

</details>


### [344] [CryoCCD: Conditional Cycle-consistent Diffusion with Biophysical Modeling for Cryo-EM Synthesis](https://arxiv.org/abs/2505.23444)
*Runmin Jiang,Genpei Zhang,Yuntian Yang,Siqi Wu,Yuheng Zhang,Wanyue Feng,Yizhou Zhao,Xi Xiao,Xiao Wang,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: CryoCCD is a synthesis framework that integrates biophysical modeling with generative techniques for cryo-electron microscopy, generating realistic multi-scale micrographs and enhancing downstream task performance.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of high-quality annotated data in cryo-EM imaging, which hinders the development of robust models for downstream analysis. Existing synthetic data generation methods fail to capture both structural diversity and complex noise inherent in cryo-EM imaging.

Method: The proposed method, CryoCCD, produces multi-scale cryo-EM micrographs by integrating biophysical modeling with generative techniques. It incorporates compositional heterogeneity, cellular context, and physics-informed imaging to reflect realistic biophysical variability. A conditional diffusion model is used to generate realistic noise, enhanced by cycle consistency for structural fidelity preservation and mask-aware contrastive learning for capturing spatially adaptive noise patterns.

Result: Extensive experiments demonstrate that CryoCCD generates structurally accurate micrographs and improves performance in downstream tasks such as particle picking and reconstruction, outperforming state-of-the-art baselines.

Conclusion: CryoCCD successfully generates realistic multi-scale cryo-EM micrographs and enhances the performance of downstream tasks, overcoming limitations of existing synthetic data generation methods.

Abstract: Cryo-electron microscopy (cryo-EM) offers near-atomic resolution imaging of
macromolecules, but developing robust models for downstream analysis is
hindered by the scarcity of high-quality annotated data. While synthetic data
generation has emerged as a potential solution, existing methods often fail to
capture both the structural diversity of biological specimens and the complex,
spatially varying noise inherent in cryo-EM imaging. To overcome these
limitations, we propose CryoCCD, a synthesis framework that integrates
biophysical modeling with generative techniques. Specifically, CryoCCD produces
multi-scale cryo-EM micrographs that reflect realistic biophysical variability
through compositional heterogeneity, cellular context, and physics-informed
imaging. To generate realistic noise, we employ a conditional diffusion model,
enhanced by cycle consistency to preserve structural fidelity and mask-aware
contrastive learning to capture spatially adaptive noise patterns. Extensive
experiments show that CryoCCD generates structurally accurate micrographs and
enhances performance in downstream tasks, outperforming state-of-the-art
baselines in both particle picking and reconstruction.

</details>


### [345] [Jigsaw-R1: A Study of Rule-based Visual Reinforcement Learning with Jigsaw Puzzles](https://arxiv.org/abs/2505.23590)
*Zifu Wang,Junyi Zhu,Bo Tang,Zhiyu Li,Feiyu Xiong,Jiaqian Yu,Matthew B. Blaschko*

Main category: cs.CV

TL;DR: The paper explores rule-based reinforcement learning (RL) for multimodal large language models using jigsaw puzzles, revealing MLLMs' capabilities in generalization, reasoning patterns, and the effectiveness of RL over Supervised Fine-Tuning.


<details>
  <summary>Details</summary>
Motivation: To understand how rule-based reinforcement learning applies to multimodal large language models, especially in perception-heavy tasks, using jigsaw puzzles as an experimental framework.

Method: Employing jigsaw puzzles to study rule-based visual reinforcement learning, examining MLLMs' performance, generalization abilities, reasoning patterns, and comparing RL with Supervised Fine-Tuning.

Result: MLLMs can generalize from simple to complex puzzles, transfer skills to other visual tasks, exhibit pre-existing complex reasoning patterns that increase with training, and show more effective generalization with RL than SFT. An initial SFT phase may hinder subsequent RL optimization.

Conclusion: Rule-based visual RL shows promise for multimodal learning, though findings might differ across different visual tasks; code is available for further research.

Abstract: The application of rule-based reinforcement learning (RL) to multimodal large
language models (MLLMs) introduces unique challenges and potential deviations
from findings in text-only domains, particularly for perception-heavy tasks.
This paper provides a comprehensive study of rule-based visual RL using jigsaw
puzzles as a structured experimental framework, revealing several key findings.
\textit{Firstly,} we find that MLLMs, initially performing near to random
guessing on simple puzzles, achieve near-perfect accuracy and generalize to
complex, unseen configurations through fine-tuning. \textit{Secondly,} training
on jigsaw puzzles can induce generalization to other visual tasks, with
effectiveness tied to specific task configurations. \textit{Thirdly,} MLLMs can
learn and generalize with or without explicit reasoning, though open-source
models often favor direct answering. Consequently, even when trained for
step-by-step reasoning, they can ignore the thinking process in deriving the
final answer. \textit{Fourthly,} we observe that complex reasoning patterns
appear to be pre-existing rather than emergent, with their frequency increasing
alongside training and task difficulty. \textit{Finally,} our results
demonstrate that RL exhibits more effective generalization than Supervised
Fine-Tuning (SFT), and an initial SFT cold start phase can hinder subsequent RL
optimization. Although these observations are based on jigsaw puzzles and may
vary across other visual tasks, this research contributes a valuable piece of
jigsaw to the larger puzzle of collective understanding rule-based visual RL
and its potential in multimodal learning. The code is available at:
\href{https://github.com/zifuwanggg/Jigsaw-R1}{https://github.com/zifuwanggg/Jigsaw-R1}.

</details>


### [346] [DeepChest: Dynamic Gradient-Free Task Weighting for Effective Multi-Task Learning in Chest X-ray Classification](https://arxiv.org/abs/2505.23595)
*Youssef Mohamed,Noran Mohamed,Khaled Abouhashad,Feilong Tang,Sara Atito,Shoaib Jameel,Imran Razzak,Ahmed B. Zaky*

Main category: cs.CV

TL;DR: The paper presents DeepChest, a dynamic task-weighting framework for multi-label chest X-ray classification that improves upon existing Multi-Task Learning methods by adaptively adjusting task importance without needing gradient access, leading to faster training and better performance.


<details>
  <summary>Details</summary>
Motivation: While Multi-Task Learning has inherent advantages in complex domains such as medical imaging, effectively balancing task contributions remains a significant challenge.

Method: DeepChest leverages a performance-driven weighting mechanism based on effective analysis of task-specific loss trends. The model-agnostic approach adaptively adjusts task importance without requiring gradient access, significantly reducing memory usage and increasing training speed.

Result: Extensive experiments demonstrate that DeepChest outperforms state-of-the-art MTL methods by 7% in overall accuracy and yields substantial reductions in individual task losses, indicating improved generalization and effective mitigation of negative transfer.

Conclusion: DeepChest offers efficiency and performance gains that pave the way for more practical and robust deployment of deep learning in critical medical diagnostic applications.

Abstract: While Multi-Task Learning (MTL) offers inherent advantages in complex domains
such as medical imaging by enabling shared representation learning, effectively
balancing task contributions remains a significant challenge. This paper
addresses this critical issue by introducing DeepChest, a novel,
computationally efficient and effective dynamic task-weighting framework
specifically designed for multi-label chest X-ray (CXR) classification. Unlike
existing heuristic or gradient-based methods that often incur substantial
overhead, DeepChest leverages a performance-driven weighting mechanism based on
effective analysis of task-specific loss trends. Given a network architecture
(e.g., ResNet18), our model-agnostic approach adaptively adjusts task
importance without requiring gradient access, thereby significantly reducing
memory usage and achieving a threefold increase in training speed. It can be
easily applied to improve various state-of-the-art methods. Extensive
experiments on a large-scale CXR dataset demonstrate that DeepChest not only
outperforms state-of-the-art MTL methods by 7% in overall accuracy but also
yields substantial reductions in individual task losses, indicating improved
generalization and effective mitigation of negative transfer. The efficiency
and performance gains of DeepChest pave the way for more practical and robust
deployment of deep learning in critical medical diagnostic applications. The
code is publicly available at https://github.com/youssefkhalil320/DeepChest-MTL

</details>


### [347] [One Trajectory, One Token: Grounded Video Tokenization via Panoptic Sub-object Trajectory](https://arxiv.org/abs/2505.23617)
*Chenhao Zheng,Jieyu Zhang,Mohammadreza Salehi,Ziqi Gao,Vishnu Iyengar,Norimasa Kobori,Quan Kong,Ranjay Krishna*

Main category: cs.CV

TL;DR: TrajViT，一种基于对象轨迹的视频编码器，在减少冗余、保持时间连贯性的同时，显著提高了视频理解任务的效果。


<details>
  <summary>Details</summary>
Motivation: 当前视频分词方法使用时空补丁导致过多的token和计算效率低下，且在相机移动时性能下降明显。

Method: 提出了一种基于全景子对象轨迹而非固定补丁的grounded video tokenization方法，并开发了TrajViT视频编码器，通过提取对象轨迹并将其转换为语义上有意义的tokens来减少冗余。

Result: TrajViT在多个视频理解基准上显著优于ViT3D，例如在视频-文本检索任务中，Top-5召回率高出6%；作为现代VideoLLM的视频编码器，TrajViT在6个VideoQA基准上平均性能提升了5.2%，同时训练速度快4倍，推理FLOPs减少了18倍。

Conclusion: TrajViT是首个在各种视频分析任务中一致优于ViT3D的高效编码器，提供了一个强大且可扩展的解决方案。

Abstract: Effective video tokenization is critical for scaling transformer models for
long videos. Current approaches tokenize videos using space-time patches,
leading to excessive tokens and computational inefficiencies. The best token
reduction strategies degrade performance and barely reduce the number of tokens
when the camera moves. We introduce grounded video tokenization, a paradigm
that organizes tokens based on panoptic sub-object trajectories rather than
fixed patches. Our method aligns with fundamental perceptual principles,
ensuring that tokenization reflects scene complexity rather than video
duration. We propose TrajViT, a video encoder that extracts object trajectories
and converts them into semantically meaningful tokens, significantly reducing
redundancy while maintaining temporal coherence. Trained with contrastive
learning, TrajViT significantly outperforms space-time ViT (ViT3D) across
multiple video understanding benchmarks, e.g., TrajViT outperforms ViT3D by a
large margin of 6% top-5 recall in average at video-text retrieval task with
10x token deduction. We also show TrajViT as a stronger model than ViT3D for
being the video encoder for modern VideoLLM, obtaining an average of 5.2%
performance improvement across 6 VideoQA benchmarks while having 4x faster
training time and 18x less inference FLOPs. TrajViT is the first efficient
encoder to consistently outperform ViT3D across diverse video analysis tasks,
making it a robust and scalable solution.

</details>


### [348] [Comparing the Effects of Persistence Barcodes Aggregation and Feature Concatenation on Medical Imaging](https://arxiv.org/abs/2505.23637)
*Dashti A. Ali,Richard K. G. Do,William R. Jarnagin,Aras T. Asaad,Amber L. Simpson*

Main category: cs.CV

TL;DR: Persistent homology (PH) is used in medical image analysis to extract robust features. The study compares two methods of constructing topological feature vectors from PH barcodes and concludes that feature concatenation is the preferred approach.


<details>
  <summary>Details</summary>
Motivation: Traditional feature extraction approaches are sensitive to small changes in input data, leading to significant variations in feature representation. Persistent homology (PH) offers a more stable alternative by capturing persistent topological and geometrical features.

Method: The paper explores two methods for constructing topological feature vectors from persistence barcodes derived from 2D or 3D medical images: aggregating persistence barcodes followed by featurization and concatenating topological feature vectors from each barcode.

Result: Feature concatenation better preserves detailed topological information from individual barcodes, resulting in superior classification performance compared to aggregation.

Conclusion: When using persistent homology for medical image analysis, feature concatenation is recommended as it leads to improved classification model performance.

Abstract: In medical image analysis, feature engineering plays an important role in the
design and performance of machine learning models. Persistent homology (PH),
from the field of topological data analysis (TDA), demonstrates robustness and
stability to data perturbations and addresses the limitation from traditional
feature extraction approaches where a small change in input results in a large
change in feature representation. Using PH, we store persistent topological and
geometrical features in the form of the persistence barcode whereby large bars
represent global topological features and small bars encapsulate geometrical
information of the data. When multiple barcodes are computed from 2D or 3D
medical images, two approaches can be used to construct the final topological
feature vector in each dimension: aggregating persistence barcodes followed by
featurization or concatenating topological feature vectors derived from each
barcode. In this study, we conduct a comprehensive analysis across diverse
medical imaging datasets to compare the effects of the two aforementioned
approaches on the performance of classification models. The results of this
analysis indicate that feature concatenation preserves detailed topological
information from individual barcodes, yields better classification performance
and is therefore a preferred approach when conducting similar experiments.

</details>


### [349] [VF-Eval: Evaluating Multimodal LLMs for Generating Feedback on AIGC Videos](https://arxiv.org/abs/2505.23693)
*Tingyu Song,Tongyan Hu,Guo Gan,Yilun Zhao*

Main category: cs.CV

TL;DR: MLLMs have been widely studied for video question answering but most assessments focus on natural videos and ignore synthetic videos. This paper proposes VF-Eval, a new benchmark that introduces four tasks to evaluate the abilities of MLLMs on AI-generated content (AIGC) videos.


<details>
  <summary>Details</summary>
Motivation: Existing assessments for MLLMs in video question answering mainly focus on natural videos and overlook synthetic videos like AI-generated content (AIGC). Additionally, while some video generation works rely on MLLMs for quality evaluation, the capabilities of MLLMs on interpreting AIGC videos are underexplored.

Method: The authors propose a new benchmark called VF-Eval which includes four tasks: coherence validation, error awareness, error type detection, and reasoning evaluation. They use this benchmark to evaluate 13 frontier MLLMs.

Result: Even the best-performing model, GPT-4.1, struggles to achieve consistently good performance across all tasks in VF-Eval, highlighting the challenging nature of the benchmark. An experiment named RePrompt shows that aligning MLLMs more closely with human feedback can benefit video generation.

Conclusion: The proposed VF-Eval benchmark comprehensively evaluates MLLMs' abilities on AIGC videos, revealing their limitations and suggesting potential improvements through alignment with human feedback.

Abstract: MLLMs have been widely studied for video question answering recently.
However, most existing assessments focus on natural videos, overlooking
synthetic videos, such as AI-generated content (AIGC). Meanwhile, some works in
video generation rely on MLLMs to evaluate the quality of generated videos, but
the capabilities of MLLMs on interpreting AIGC videos remain largely
underexplored. To address this, we propose a new benchmark, VF-Eval, which
introduces four tasks-coherence validation, error awareness, error type
detection, and reasoning evaluation-to comprehensively evaluate the abilities
of MLLMs on AIGC videos. We evaluate 13 frontier MLLMs on VF-Eval and find that
even the best-performing model, GPT-4.1, struggles to achieve consistently good
performance across all tasks. This highlights the challenging nature of our
benchmark. Additionally, to investigate the practical applications of VF-Eval
in improving video generation, we conduct an experiment, RePrompt,
demonstrating that aligning MLLMs more closely with human feedback can benefit
video generation.

</details>


### [350] [CLDTracker: A Comprehensive Language Description for Visual Tracking](https://arxiv.org/abs/2505.23704)
*Mohamad Alansari,Sajid Javed,Iyyakutti Iyappan Ganapathi,Sara Alansari,Muzammal Naseer*

Main category: cs.CV

TL;DR: CLDTracker is a new framework that improves visual object tracking (VOT) by using rich textual and visual descriptions from vision-language models (VLMs). It addresses limitations in current VLM applications for VOT, such as insufficient textual representation, inefficient feature fusion, and lack of temporal modeling. Experiments show it performs better than existing methods.


<details>
  <summary>Details</summary>
Motivation: VOT is still a challenging task due to complex scenarios like appearance changes, occlusions, etc. Traditional trackers relying on visual cues struggle, while direct application of VLMs to VOT has limitations including insufficient textual representation, inefficient feature fusion, and lack of temporal modeling.

Method: Proposed CLDTracker uses a dual-branch architecture with textual and visual branches. In the textual branch, it constructs a bag of rich textual descriptions using VLMs like CLIP and GPT-4V, enriched with semantic and contextual cues. The visual branch complements this with visual information.

Result: Experiments on six standard VOT benchmarks demonstrate that CLDTracker achieves state-of-the-art performance, proving the effectiveness of using robust and temporally-adaptive vision-language representations for tracking.

Conclusion: CLDTracker successfully bridges gaps in applying VLMs to VOT by providing a comprehensive language description framework. This leads to improved tracking performance and sets a new benchmark in the field.

Abstract: VOT remains a fundamental yet challenging task in computer vision due to
dynamic appearance changes, occlusions, and background clutter. Traditional
trackers, relying primarily on visual cues, often struggle in such complex
scenarios. Recent advancements in VLMs have shown promise in semantic
understanding for tasks like open-vocabulary detection and image captioning,
suggesting their potential for VOT. However, the direct application of VLMs to
VOT is hindered by critical limitations: the absence of a rich and
comprehensive textual representation that semantically captures the target
object's nuances, limiting the effective use of language information;
inefficient fusion mechanisms that fail to optimally integrate visual and
textual features, preventing a holistic understanding of the target; and a lack
of temporal modeling of the target's evolving appearance in the language
domain, leading to a disconnect between the initial description and the
object's subsequent visual changes. To bridge these gaps and unlock the full
potential of VLMs for VOT, we propose CLDTracker, a novel Comprehensive
Language Description framework for robust visual Tracking. Our tracker
introduces a dual-branch architecture consisting of a textual and a visual
branch. In the textual branch, we construct a rich bag of textual descriptions
derived by harnessing the powerful VLMs such as CLIP and GPT-4V, enriched with
semantic and contextual cues to address the lack of rich textual
representation. Experiments on six standard VOT benchmarks demonstrate that
CLDTracker achieves SOTA performance, validating the effectiveness of
leveraging robust and temporally-adaptive vision-language representations for
tracking. Code and models are publicly available at:
https://github.com/HamadYA/CLDTracker

</details>


### [351] [Skin Lesion Phenotyping via Nested Multi-modal Contrastive Learning](https://arxiv.org/abs/2505.23709)
*Dionysis Christopoulos,Sotiris Spanos,Eirini Baltzi,Valsamis Ntouskos,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: Introduce SLIMP for learning rich representations of skin lesions through a novel nested contrastive learning approach that captures complex relationships between images and metadata, which combines the appearance and the metadata of individual skin lesions with patient-level metadata.


<details>
  <summary>Details</summary>
Motivation: Melanoma detection and skin lesion classification based solely on images pose significant challenges due to large variations in imaging conditions and lack of clinical and phenotypical context. Clinicians typically follow a holistic approach for assessing the risk level of the patient.

Method: SLIMP combines the appearance and the metadata of individual skin lesions with patient-level metadata relating to their medical record and other clinically relevant information through a novel nested contrastive learning approach.

Result: The proposed pre-training strategy improves performance compared to other pre-training strategies on downstream skin lesions classification tasks.

Conclusion: The learned representations quality is highlighted by the improvement in performance.

Abstract: We introduce SLIMP (Skin Lesion Image-Metadata Pre-training) for learning
rich representations of skin lesions through a novel nested contrastive
learning approach that captures complex relationships between images and
metadata. Melanoma detection and skin lesion classification based solely on
images, pose significant challenges due to large variations in imaging
conditions (lighting, color, resolution, distance, etc.) and lack of clinical
and phenotypical context. Clinicians typically follow a holistic approach for
assessing the risk level of the patient and for deciding which lesions may be
malignant and need to be excised, by considering the patient's medical history
as well as the appearance of other lesions of the patient. Inspired by this,
SLIMP combines the appearance and the metadata of individual skin lesions with
patient-level metadata relating to their medical record and other clinically
relevant information. By fully exploiting all available data modalities
throughout the learning process, the proposed pre-training strategy improves
performance compared to other pre-training strategies on downstream skin
lesions classification tasks highlighting the learned representations quality.

</details>


### [352] [CFP-Gen: Combinatorial Functional Protein Generation via Diffusion Language Models](https://arxiv.org/abs/2505.22869)
*Junbo Yin,Chao Zha,Wenjia He,Chencheng Xu,Xin Gao*

Main category: cs.CV

TL;DR: CFP-Gen is a novel diffusion language model for generating protein sequences that can satisfy multiple constraints across different modalities, including functional, sequence, and structural constraints.


<details>
  <summary>Details</summary>
Motivation: Existing PLMs struggle to simultaneously satisfy multiple constraints across different modalities when generating protein sequences.

Method: The model introduces an Annotation-Guided Feature Modulation (AGFM) module and a Residue-Controlled Functional Encoding (RCFE) module. The AGFM module dynamically adjusts the protein feature distribution based on composable functional annotations while the RCFE module captures residue-wise interaction for more precise control. Off-the-shelf 3D structure encoders can also be integrated to impose geometric constraints.

Result: CFP-Gen enables high-throughput generation of novel proteins with functionality comparable to natural proteins and achieves a high success rate in designing multifunctional proteins.

Conclusion: CFP-Gen facilitates de novo protein design by integrating multimodal conditions and demonstrates its capability in generating novel proteins with desired functionalities.

Abstract: Existing PLMs generate protein sequences based on a single-condition
constraint from a specific modality, struggling to simultaneously satisfy
multiple constraints across different modalities. In this work, we introduce
CFP-Gen, a novel diffusion language model for Combinatorial Functional Protein
GENeration. CFP-Gen facilitates the de novo protein design by integrating
multimodal conditions with functional, sequence, and structural constraints.
Specifically, an Annotation-Guided Feature Modulation (AGFM) module is
introduced to dynamically adjust the protein feature distribution based on
composable functional annotations, e.g., GO terms, IPR domains and EC numbers.
Meanwhile, the Residue-Controlled Functional Encoding (RCFE) module captures
residue-wise interaction to ensure more precise control. Additionally,
off-the-shelf 3D structure encoders can be seamlessly integrated to impose
geometric constraints. We demonstrate that CFP-Gen enables high-throughput
generation of novel proteins with functionality comparable to natural proteins,
while achieving a high success rate in designing multifunctional proteins. Code
and data available at https://github.com/yinjunbo/cfpgen.

</details>


### [353] [cadrille: Multi-modal CAD Reconstruction with Online Reinforcement Learning](https://arxiv.org/abs/2505.22914)
*Maksim Kolodiazhnyi,Denis Tarasov,Dmitrii Zhemchuzhnikov,Alexander Nikulin,Ilya Zisman,Anna Vorontsova,Anton Konushin,Vladislav Kurenkov,Danila Rukhovich*

Main category: cs.CV

TL;DR: The paper proposes a multi-modal CAD reconstruction model that processes point clouds, images, and text simultaneously using advances in vision-language models (VLM) and large language models (LLM). It employs a two-stage pipeline of supervised fine-tuning and reinforcement learning (RL) fine-tuning. The model outperforms single-modal approaches and sets new state-of-the-art results on three challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CAD reconstruction typically focus on a single input modality, which limits their generalizability and robustness. To address this limitation, the authors propose a multi-modal approach that can process multiple types of input data simultaneously.

Method: The method involves a two-stage pipeline: 1) Supervised Fine-Tuning (SFT) on large-scale procedurally generated data, and 2) Reinforcement Learning (RL) fine-tuning using online feedback obtained programmatically. They leverage recent advances in vision-language models (VLM) and large language models (LLM) to process all three input modalities (point clouds, images, and text) simultaneously.

Result: In the DeepCAD benchmark, the SFT model outperforms existing single-modal approaches in all three input modalities simultaneously. After RL fine-tuning, the model, called cadrille, sets new state-of-the-art on three challenging datasets, including a real-world one.

Conclusion: The proposed multi-modal CAD reconstruction model demonstrates superior performance compared to single-modal approaches. The use of reinforcement learning fine-tuning significantly improves the model's capabilities, setting new benchmarks in CAD tasks.

Abstract: Computer-Aided Design (CAD) plays a central role in engineering and
manufacturing, making it possible to create precise and editable 3D models.
Using a variety of sensor or user-provided data as inputs for CAD
reconstruction can democratize access to design applications. However, existing
methods typically focus on a single input modality, such as point clouds,
images, or text, which limits their generalizability and robustness. Leveraging
recent advances in vision-language models (VLM), we propose a multi-modal CAD
reconstruction model that simultaneously processes all three input modalities.
Inspired by large language model (LLM) training paradigms, we adopt a two-stage
pipeline: supervised fine-tuning (SFT) on large-scale procedurally generated
data, followed by reinforcement learning (RL) fine-tuning using online
feedback, obtained programatically. Furthermore, we are the first to explore RL
fine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such
as Group Relative Preference Optimization (GRPO) outperform offline
alternatives. In the DeepCAD benchmark, our SFT model outperforms existing
single-modal approaches in all three input modalities simultaneously. More
importantly, after RL fine-tuning, cadrille sets new state-of-the-art on three
challenging datasets, including a real-world one.

</details>


### [354] [MAGREF: Masked Guidance for Any-Reference Video Generation](https://arxiv.org/abs/2505.23742)
*Yufan Deng,Xun Guo,Yuanyang Yin,Jacob Zhiyuan Fang,Yiding Yang,Yizhi Wang,Shenghai Yuan,Angtian Wang,Bo Liu,Haibin Huang,Chongyang Ma*

Main category: cs.CV

TL;DR: MAGREF is a new framework for multi-reference video generation that uses masked guidance to maintain subject consistency and high quality, introducing region-aware masking and pixel-wise channel concatenation mechanisms. It surpasses existing models in multi-subject synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing video generation methods struggle with maintaining consistency across multiple reference subjects and ensuring high-quality outputs when dealing with diverse references.

Method: Proposes MAGREF, incorporating (1) a region-aware dynamic masking mechanism for flexible subject inference without architectural changes and (2) a pixel-wise channel concatenation mechanism to preserve appearance features. Uses masked guidance for coherent multi-subject video synthesis conditioned on various reference images and textual prompts.

Result: Achieves state-of-the-art video generation quality, generalizing from single-subject training to complex multi-subject scenarios while maintaining coherent synthesis and precise control over individual subjects. Outperforms open-source and commercial baselines.

Conclusion: MAGREF effectively addresses challenges in multi-reference video generation, offering scalable, controllable, and high-fidelity multi-subject synthesis. A comprehensive benchmark is introduced for evaluation.

Abstract: Video generation has made substantial strides with the emergence of deep
generative models, especially diffusion-based approaches. However, video
generation based on multiple reference subjects still faces significant
challenges in maintaining multi-subject consistency and ensuring high
generation quality. In this paper, we propose MAGREF, a unified framework for
any-reference video generation that introduces masked guidance to enable
coherent multi-subject video synthesis conditioned on diverse reference images
and a textual prompt. Specifically, we propose (1) a region-aware dynamic
masking mechanism that enables a single model to flexibly handle various
subject inference, including humans, objects, and backgrounds, without
architectural changes, and (2) a pixel-wise channel concatenation mechanism
that operates on the channel dimension to better preserve appearance features.
Our model delivers state-of-the-art video generation quality, generalizing from
single-subject training to complex multi-subject scenarios with coherent
synthesis and precise control over individual subjects, outperforming existing
open-source and commercial baselines. To facilitate evaluation, we also
introduce a comprehensive multi-subject video benchmark. Extensive experiments
demonstrate the effectiveness of our approach, paving the way for scalable,
controllable, and high-fidelity multi-subject video synthesis. Code and model
can be found at: https://github.com/MAGREF-Video/MAGREF

</details>


### [355] [Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need](https://arxiv.org/abs/2505.23744)
*Qiang Wang,Xiang Song,Yuhang He,Jizhou Han,Chenhao Ding,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: SOYO is a lightweight framework for Parameter-Isolation Domain Incremental Learning (PIDIL) that includes a Gaussian Mixture Compressor, Domain Feature Resampler, and Multi-level Domain Feature Fusion Network. It enhances domain feature extraction, supports multiple PEFT methods, and demonstrates superiority across various tasks and benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing PIDIL methods have difficulty maintaining parameter selection accuracy as the number of domains and classes increases.

Method: SOYO introduces three key components: Gaussian Mixture Compressor (GMC) for efficient storage of prior domain data, Domain Feature Resampler (DFR) for balancing prior domain data, and Multi-level Domain Feature Fusion Network (MDFN) for enhancing domain feature extraction. SOYO also supports multiple Parameter-Efficient Fine-Tuning (PEFT) methods.

Result: SOYO shows consistent superiority over existing baselines on six benchmarks across tasks like image classification, object detection, and speech enhancement.

Conclusion: SOYO addresses the challenges in PIDIL by improving domain selection accuracy and supporting multiple PEFT methods, demonstrating robustness and adaptability in complex environments.

Abstract: Deep neural networks (DNNs) often underperform in real-world, dynamic
settings where data distributions change over time. Domain Incremental Learning
(DIL) offers a solution by enabling continual model adaptation, with
Parameter-Isolation DIL (PIDIL) emerging as a promising paradigm to reduce
knowledge conflicts. However, existing PIDIL methods struggle with parameter
selection accuracy, especially as the number of domains and corresponding
classes grows. To address this, we propose SOYO, a lightweight framework that
improves domain selection in PIDIL. SOYO introduces a Gaussian Mixture
Compressor (GMC) and Domain Feature Resampler (DFR) to store and balance prior
domain data efficiently, while a Multi-level Domain Feature Fusion Network
(MDFN) enhances domain feature extraction. Our framework supports multiple
Parameter-Efficient Fine-Tuning (PEFT) methods and is validated across tasks
such as image classification, object detection, and speech enhancement.
Experimental results on six benchmarks demonstrate SOYO's consistent
superiority over existing baselines, showcasing its robustness and adaptability
in complex, evolving environments. The codes will be released in
https://github.com/qwangcv/SOYO.

</details>


### [356] [To Trust Or Not To Trust Your Vision-Language Model's Prediction](https://arxiv.org/abs/2505.23745)
*Hao Dong,Moru Liu,Jian Liang,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: Vision-Language Models (VLMs) are powerful but can make confident yet wrong predictions. This paper introduces TrustVLM, a training-free framework that improves the detection of misclassifications by VLMs using a novel confidence-scoring function. It shows significant improvements over existing methods across multiple datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: Despite their strengths in multimodal tasks, Vision-Language Models (VLMs) still have issues with making incorrect predictions confidently, which poses risks in critical applications.

Method: The method involves introducing TrustVLM, a framework that doesn't require retraining and uses a new confidence-scoring function based on the distinct representation of certain concepts in image embedding space to detect misclassifications.

Result: TrustVLM was evaluated across 17 datasets, 4 architectures, and 2 VLMs, showing state-of-the-art performance with improvements up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95 compared to current baselines.

Conclusion: By enhancing the reliability of VLMs without the need for retraining, TrustVLM enables safer deployment of these models in real-world scenarios.

Abstract: Vision-Language Models (VLMs) have demonstrated strong capabilities in
aligning visual and textual modalities, enabling a wide range of applications
in multimodal understanding and generation. While they excel in zero-shot and
transfer learning scenarios, VLMs remain susceptible to misclassification,
often yielding confident yet incorrect predictions. This limitation poses a
significant risk in safety-critical domains, where erroneous predictions can
lead to severe consequences. In this work, we introduce TrustVLM, a
training-free framework designed to address the critical challenge of
estimating when VLM's predictions can be trusted. Motivated by the observed
modality gap in VLMs and the insight that certain concepts are more distinctly
represented in the image embedding space, we propose a novel confidence-scoring
function that leverages this space to improve misclassification detection. We
rigorously evaluate our approach across 17 diverse datasets, employing 4
architectures and 2 VLMs, and demonstrate state-of-the-art performance, with
improvements of up to 51.87% in AURC, 9.14% in AUROC, and 32.42% in FPR95
compared to existing baselines. By improving the reliability of the model
without requiring retraining, TrustVLM paves the way for safer deployment of
VLMs in real-world applications. The code will be available at
https://github.com/EPFL-IMOS/TrustVLM.

</details>


### [357] [Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence](https://arxiv.org/abs/2505.23747)
*Diankun Wu,Fangfu Liu,Yi-Hsin Hung,Yueqi Duan*

Main category: cs.CV

TL;DR: Recent advancements in MLLMs have improved 2D visual tasks, but spatial intelligence remains a challenge. This paper presents Spatial-MLLM, a framework for visual-based spatial reasoning from purely 2D observations. It uses a dual-encoder architecture and proposes a space-aware frame sampling strategy. The model is trained on the newly constructed Spatial-MLLM-120k dataset and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Although MLLMs have significantly enhanced performance on 2D visual tasks, improving their spatial intelligence remains difficult. Existing 3D MLLMs rely on additional 3D or 2.5D data to incorporate spatial awareness, which restricts their utility in scenarios with only 2D inputs.

Method: The authors propose Spatial-MLLM, a novel framework that includes a dual-encoder architecture: a pretrained 2D visual encoder for semantic features and a spatial encoder initialized from the backbone of the visual geometry model for 3D structure features. A connector integrates these features into unified visual tokens. Additionally, they propose a space-aware frame sampling strategy to select spatially informative frames of a video sequence.

Result: Extensive experiments on various real-world datasets show that Spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks.

Conclusion: The Spatial-MLLM framework effectively performs visual-based spatial reasoning from purely 2D observations, overcoming limitations of previous models that require additional 3D or 2.5D data.

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced performance on 2D visual tasks. However, improving their
spatial intelligence remains a challenge. Existing 3D MLLMs always rely on
additional 3D or 2.5D data to incorporate spatial awareness, restricting their
utility in scenarios with only 2D inputs, such as images or videos. In this
paper, we present Spatial-MLLM, a novel framework for visual-based spatial
reasoning from purely 2D observations. Unlike conventional video MLLMs which
rely on CLIP-based visual encoders optimized for semantic understanding, our
key insight is to unleash the strong structure prior from the feed-forward
visual geometry foundation model. Specifically, we propose a dual-encoder
architecture: a pretrained 2D visual encoder to extract semantic features, and
a spatial encoder-initialized from the backbone of the visual geometry model-to
extract 3D structure features. A connector then integrates both features into
unified visual tokens for enhanced spatial understanding. Furthermore, we
propose a space-aware frame sampling strategy at inference time, which selects
the spatially informative frames of a video sequence, ensuring that even under
limited token length, the model focuses on frames critical for spatial
reasoning. Beyond architecture improvements, we construct the Spatial-MLLM-120k
dataset and train the model on it using supervised fine-tuning and GRPO.
Extensive experiments on various real-world datasets demonstrate that our
spatial-MLLM achieves state-of-the-art performance in a wide range of
visual-based spatial understanding and reasoning tasks. Project page:
https://diankun-wu.github.io/Spatial-MLLM/.

</details>


### [358] [LADA: Scalable Label-Specific CLIP Adapter for Continual Learning](https://arxiv.org/abs/2505.23271)
*Mao-Lin Luo,Zi-Hao Zhou,Tong Wei,Min-Ling Zhang*

Main category: cs.CV

TL;DR: LADA introduces label-specific adapters to the CLIP image encoder for continual learning, preventing catastrophic forgetting via feature distillation and achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for adapting CLIP's pre-trained image encoder in continual learning tasks require selecting task-specific parameters during inference, which can lead to errors and degrade performance.

Method: LADA appends lightweight, label-specific memory units to the frozen CLIP image encoder. It uses feature distillation for seen classes to prevent catastrophic forgetting and blocks gradient flow to the frozen CLIP parameters for efficient training.

Result: Extensive results demonstrate that LADA achieves state-of-the-art performance in continual learning settings.

Conclusion: LADA effectively addresses the problem of parameter selection in CLIP-based continual learning by introducing label-specific adapters and feature distillation, leading to superior performance.

Abstract: Continual learning with vision-language models like CLIP offers a pathway
toward scalable machine learning systems by leveraging its transferable
representations. Existing CLIP-based methods adapt the pre-trained image
encoder by adding multiple sets of learnable parameters, with each task using a
partial set of parameters. This requires selecting the expected parameters for
input images during inference, which is prone to error that degrades
performance. To address this problem, we introduce LADA (Label-specific
ADApter). Instead of partitioning parameters across tasks, LADA appends
lightweight, label-specific memory units to the frozen CLIP image encoder,
enabling discriminative feature generation by aggregating task-agnostic
knowledge. To prevent catastrophic forgetting, LADA employs feature
distillation for seen classes, preventing their features from being interfered
with by new classes. Positioned after the image encoder, LADA prevents gradient
flow to the frozen CLIP parameters, ensuring efficient training. Extensive
results show that LADA achieves state-of-the-art performance in continual
learning settings. The implementation code is available at
https://github.com/MaolinLuo/LADA.

</details>


### [359] [TimePoint: Accelerated Time Series Alignment via Self-Supervised Keypoint and Descriptor Learning](https://arxiv.org/abs/2505.23475)
*Ron Shapira Weber,Shahar Ben Ishay,Andrey Lavrinenko,Shahaf E. Finder,Oren Freifeld*

Main category: cs.CV

TL;DR: TimePoint is a self-supervised method that accelerates DTW-based alignment while improving accuracy by learning keypoints and descriptors from synthetic data.


<details>
  <summary>Details</summary>
Motivation: Fast and scalable alignment of time series is a fundamental challenge in many domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor scalability and sensitivity to noise.

Method: TimePoint leverages efficient 1D diffeomorphisms to generate realistic training data. This approach, along with fully convolutional and wavelet convolutional architectures, enables the extraction of informative keypoints and descriptors.

Result: TimePoint demonstrates strong generalization to real-world time series when trained solely on synthetic data, and further improves with fine-tuning on real data.

Conclusion: Extensive experiments demonstrate that TimePoint consistently achieves faster and more accurate alignments than standard DTW, making it a scalable solution for time-series analysis.

Abstract: Fast and scalable alignment of time series is a fundamental challenge in many
domains. The standard solution, Dynamic Time Warping (DTW), struggles with poor
scalability and sensitivity to noise. We introduce TimePoint, a self-supervised
method that dramatically accelerates DTW-based alignment while typically
improving alignment accuracy by learning keypoints and descriptors from
synthetic data. Inspired by 2D keypoint detection but carefully adapted to the
unique challenges of 1D signals, TimePoint leverages efficient 1D
diffeomorphisms, which effectively model nonlinear time warping, to generate
realistic training data. This approach, along with fully convolutional and
wavelet convolutional architectures, enables the extraction of informative
keypoints and descriptors. Applying DTW to these sparse representations yield
major speedups and typically higher alignment accuracy than standard DTW
applied to the full signals. TimePoint demonstrates strong generalization to
real-world time series when trained solely on synthetic data, and further
improves with fine-tuning on real data. Extensive experiments demonstrate that
TimePoint consistently achieves faster and more accurate alignments than
standard DTW, making it a scalable solution for time-series analysis. Our code
is available at https://github.com/BGU-CS-VIL/TimePoint

</details>


### [360] [OmniEarth-Bench: Towards Holistic Evaluation of Earth's Six Spheres and Cross-Spheres Interactions with Multimodal Observational Earth Data](https://arxiv.org/abs/2505.23522)
*Fengxiang Wang,Mingshuo Chen,Xuming He,YiFan Zhang,Feng Liu,Zijie Guo,Zhenghao Hu,Jiong Wang,Jingyi Xu,Zhangrui Li,Fenghua Ling,Ben Fei,Weijia Li,Long Lan,Wenjing Yang,Wenlong Zhang,Lei Bai*

Main category: cs.CV

TL;DR: 现存的地球科学多模态学习基准在系统覆盖地系统组件和跨球体交互方面存在关键局限性。为了解决这些空白，我们引入了OmniEarth-Bench，这是第一个涵盖所有六个地球科学球体（大气圈、岩石圈、海洋圈、冰冻圈、生物圈和人类活动圈）和跨球体的一百个专家策划的评估维度的综合多模态基准。实验表明，最先进的模型在我们的基准上表现不佳，OmniEarth-Bench为具有地系统意识的AI设定了新标准。


<details>
  <summary>Details</summary>
Motivation: 现有的地球科学多模态学习基准在系统覆盖地系统组件和跨球体交互方面存在关键局限性，通常局限于孤立的子系统（仅限于人类活动球或大气圈），评估维度有限（少于16个任务）。

Method: 引入OmniEarth-Bench，整合了来自卫星传感器和现场测量的观测数据，包含29,779个注释，涵盖了感知、一般推理、科学知识推理和连贯推理四个层次。通过混合专家-群众工作流程来减少标签歧义。

Result: 在9个最先进的多模态语言模型上的实验表明，即使是最先进的模型在我们的基准上也难以应对，其中没有一个模型的准确率达到35％。特别是在一些跨球体任务中，像GPT-4o这样的领先模型的表现下降到0.0％。

Conclusion: OmniEarth-Bench为具有地系统意识的AI设定了新标准，推动了科学发现和环境监测及灾害预测的实际应用。

Abstract: Existing benchmarks for Earth science multimodal learning exhibit critical
limitations in systematic coverage of geosystem components and cross-sphere
interactions, often constrained to isolated subsystems (only in
Human-activities sphere or atmosphere) with limited evaluation dimensions (less
than 16 tasks). To address these gaps, we introduce OmniEarth-Bench, the first
comprehensive multimodal benchmark spanning all six Earth science spheres
(atmosphere, lithosphere, Oceansphere, cryosphere, biosphere and
Human-activities sphere) and cross-spheres with one hundred expert-curated
evaluation dimensions. Leveraging observational data from satellite sensors and
in-situ measurements, OmniEarth-Bench integrates 29,779 annotations across four
tiers: perception, general reasoning, scientific knowledge reasoning and
chain-of-thought (CoT) reasoning. This involves the efforts of 2-5 experts per
sphere to establish authoritative evaluation dimensions and curate relevant
observational datasets, 40 crowd-sourcing annotators to assist experts for
annotations, and finally, OmniEarth-Bench is validated via hybrid expert-crowd
workflows to reduce label ambiguity. Experiments on 9 state-of-the-art MLLMs
reveal that even the most advanced models struggle with our benchmarks, where
none of them reach 35\% accuracy. Especially, in some cross-spheres tasks, the
performance of leading models like GPT-4o drops to 0.0\%. OmniEarth-Bench sets
a new standard for geosystem-aware AI, advancing both scientific discovery and
practical applications in environmental monitoring and disaster prediction. The
dataset, source code, and trained models were released.

</details>


### [361] [Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information](https://arxiv.org/abs/2505.23558)
*Xu Chu,Xinrong Chen,Guanyu Wang,Zhijie Tan,Kui Huang,Wenyu Lv,Tong Mo,Weiping Li*

Main category: cs.CV

TL;DR: Qwen-LookAgain (Qwen-LA) is a Vision-Language Reasoning Model that mitigates hallucinations by incorporating vision-text reflection processes and introducing Balanced Reflective Policy Optimization (BRPO), Visual Token COPY, and Visual Token ROUTE.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations in Vision-Language Models caused by long reasoning diluting visual tokens and receiving less attention.

Method: 1. Introduce Qwen-LookAgain (Qwen-LA) with a vision-text reflection process.
2. Propose Balanced Reflective Policy Optimization (BRPO) for generating reflections and balancing their number and length.
3. Prove VLRMs lose attention to visual tokens over reasoning and supplement visual information during reflection.
4. Implement Visual Token COPY and Visual Token ROUTE during training and inference.

Result: Experiments on multiple visual QA datasets and hallucination metrics show leading accuracy performance while reducing hallucinations.

Conclusion: Qwen-LookAgain effectively mitigates hallucinations in Vision-Language Models through its novel methods.

Abstract: Inference time scaling drives extended reasoning to enhance the performance
of Vision-Language Models (VLMs), thus forming powerful Vision-Language
Reasoning Models (VLRMs). However, long reasoning dilutes visual tokens,
causing visual information to receive less attention and may trigger
hallucinations. Although introducing text-only reflection processes shows
promise in language models, we demonstrate that it is insufficient to suppress
hallucinations in VLMs. To address this issue, we introduce Qwen-LookAgain
(Qwen-LA), a novel VLRM designed to mitigate hallucinations by incorporating a
vision-text reflection process that guides the model to re-attention visual
information during reasoning. We first propose a reinforcement learning method
Balanced Reflective Policy Optimization (BRPO), which guides the model to
decide when to generate vision-text reflection on its own and balance the
number and length of reflections. Then, we formally prove that VLRMs lose
attention to visual tokens as reasoning progresses, and demonstrate that
supplementing visual information during reflection enhances visual attention.
Therefore, during training and inference, Visual Token COPY and Visual Token
ROUTE are introduced to force the model to re-attention visual information at
the visual level, addressing the limitations of text-only reflection.
Experiments on multiple visual QA datasets and hallucination metrics indicate
that Qwen-LA achieves leading accuracy performance while reducing
hallucinations. Our code is available at:
https://github.com/Liar406/Look_Again.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [362] [Generative Social Choice: The Next Generation](https://arxiv.org/abs/2505.22939)
*Niclas Boehmer,Sara Fish,Ariel D. Procaccia*

Main category: cs.GT

TL;DR: The paper expands the generative social choice framework with theoretical guarantees for approximately optimal queries and a budget limit on the slate's length, demonstrating its effectiveness using GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To create a concise set of statements that proportionally represent all user opinions in democratic processes.

Method: Extends the generative social choice framework by incorporating theoretical guarantees for queries and a budget limit on the slate's length, utilizing GPT-4o for implementation.

Result: Showcased effectiveness on datasets related to city improvement measures and drug reviews, successfully generating representative slates from unstructured user opinions.

Conclusion: The extended framework is effective in producing representative slates under constraints of query optimality and length budget.

Abstract: A key task in certain democratic processes is to produce a concise slate of
statements that proportionally represents the full spectrum of user opinions.
This task is similar to committee elections, but unlike traditional settings,
the candidate set comprises all possible statements of varying lengths, and so
it can only be accessed through specific queries. Combining social choice and
large language models, prior work has approached this challenge through a
framework of generative social choice. We extend the framework in two
fundamental ways, providing theoretical guarantees even in the face of
approximately optimal queries and a budget limit on the overall length of the
slate. Using GPT-4o to implement queries, we showcase our approach on datasets
related to city improvement measures and drug reviews, demonstrating its
effectiveness in generating representative slates from unstructured user
opinions.

</details>


### [363] [Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals](https://arxiv.org/abs/2505.23124)
*Junyan Liu,Arnab Maiti,Artin Tajdini,Kevin Jamieson,Lillian J. Ratliff*

Main category: cs.GT

TL;DR: 研究了一个有限时间范围内的重复委托人-代理人问题，分析了在无先验知识时线性遗憾的产生，并提出了两种可实现次线性遗憾的关键设置及相应算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机来源于解决一个有限时间范围内的重复委托人-代理人问题，其中委托人需要与以对抗顺序到达的不同类型的代理人进行交互，且需在未知代理人类型的情况下选择最优激励策略以最小化遗憾。

Method: 1. 针对第一种设置，假设委托人知道每种代理人类型在任何给定激励下会贪婪选择的手臂，提出了一种实现遗憾界为$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$的算法。
2. 对于第二种设置，假设代理人的反应随着激励的变化平滑变化且受Lipschitz常数$L\geq 1$控制，展示了一种遗憾界为$\tilde{O}((LN)^{1/3}T^{2/3})$的算法。
3. 进一步扩展了算法结果，允许委托人在每轮中同时激励多个手臂。

Result: 1. 在无先验知识的情况下，证明了问题变得难以处理，导致线性遗憾。
2. 提出了两种关键设置，在这两种设置下可以实现次线性遗憾，并提供了相应的匹配下界。

Conclusion: 本文研究了一个有限时间范围内的重复委托人-代理人问题，展示了在无先验知识时问题的不可处理性，并针对两种关键设置提出了有效的算法，实现了次线性遗憾。

Abstract: We initiate the study of a repeated principal-agent problem over a finite
horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of
agents arriving in an adversarial order. At each round, the principal
strategically chooses one of the $N$ arms to incentivize for an arriving agent
of unknown type. The agent then chooses an arm based on its own utility and the
provided incentive, and the principal receives a corresponding reward. The
objective is to minimize regret against the best incentive in hindsight.
Without prior knowledge of agent behavior, we show that the problem becomes
intractable, leading to linear regret. We analyze two key settings where
sublinear regret is achievable. In the first setting, the principal knows the
arm each agent type would select greedily for any given incentive. Under this
setting, we propose an algorithm that achieves a regret bound of
$O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to
a $\log K$ factor. In the second setting, an agent's response varies smoothly
with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under
this setting, we show that there is an algorithm with a regret bound of
$\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to
logarithmic factors. Finally, we extend our algorithmic results for both
settings by allowing the principal to incentivize multiple arms simultaneously
in each round.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [364] [Exposing the Impact of GenAI for Cybercrime: An Investigation into the Dark Side](https://arxiv.org/abs/2505.23733)
*Truong,Luu,Binny M. Samuel*

Main category: cs.CY

TL;DR: The paper uses psychological frameworks and quasi-experimental design to provide empirical evidence on the link between generative AI and cybercrime, contributing to AI governance discussions.


<details>
  <summary>Details</summary>
Motivation: To empirically examine the association between generative AI and malicious internet-related activities and cybercrime using psychological frameworks of technological amplification and affordance theory.

Method: Quasi-experimental design with interrupted time series analysis on two datasets (general and cryptocurrency-focused) to assess generative AI's role in cybercrime.

Result: Findings contribute to AI governance discussions by balancing control and innovation, highlighting the need for strategies to maximize AI benefits while mitigating risks.

Conclusion: There is an empirical link between generative AI and cybercrime, requiring strategic guidance for policymakers, AI developers, cybersecurity professionals, and public education.

Abstract: In recent years, the rapid advancement and democratization of generative AI
models have sparked significant debate over safety, ethical risks, and dual-use
concerns, particularly in the context of cybersecurity. While anecdotally
known, this paper provides empirical evidence regarding generative AI's
association with malicious internet-related activities and cybercrime by
examining the phenomenon through psychological frameworks of technological
amplification and affordance theory. Using a quasi-experimental design with
interrupted time series analysis, we analyze two datasets, one general and one
cryptocurrency-focused, to empirically assess generative AI's role in
cybercrime. The findings contribute to ongoing discussions about AI governance
by balancing control and fostering innovation, underscoring the need for
strategies to guide policymakers, inform AI developers and cybersecurity
professionals, and educate the public to maximize AI's benefits while
mitigating its risks.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [365] [A Large Language Model-Enabled Control Architecture for Dynamic Resource Capability Exploration in Multi-Agent Manufacturing Systems](https://arxiv.org/abs/2505.22814)
*Jonghan Lim,Ilya Kovalenko*

Main category: cs.MA

TL;DR: 制造环境因需求变化和产品生命周期缩短而变得更加复杂和不可预测，需要实时决策和适应中断。传统的控制方法存在响应性限制，多智能体系统通过分散决策来解决这些问题，但仍面临实时适应、情境感知决策和资源能力动态探索的挑战。本文提出了一种基于大型语言模型的多智能体制造系统控制架构，以应对实时中断并提高系统韧性和灵活性。案例研究证明了该架构在提高吞吐量和资源利用效率方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 制造环境日益复杂和不可预测，传统控制方法在动态工业环境中的响应性有限，多智能体系统虽然能动态响应操作变化，但在实时适应、情境感知决策和资源能力动态探索方面仍存在挑战。

Method: 引入大型语言模型支持的控制架构，用于多智能体制造系统，以动态探索资源能力并应对实时中断。

Result: 模拟案例研究表明，与现有方法相比，该架构提高了系统的韧性和灵活性，提升了吞吐量和资源利用效率。

Conclusion: 提出的基于大型语言模型的多智能体制造系统控制架构能够有效应对制造环境中的复杂性和不可预测性，提升系统性能。

Abstract: Manufacturing environments are becoming more complex and unpredictable due to
factors such as demand variations and shorter product lifespans. This
complexity requires real-time decision-making and adaptation to disruptions.
Traditional control approaches highlight the need for advanced control
strategies capable of overcoming unforeseen challenges, as they demonstrate
limitations in responsiveness within dynamic industrial settings. Multi-agent
systems address these challenges through decentralization of decision-making,
enabling systems to respond dynamically to operational changes. However,
current multi-agent systems encounter challenges related to real-time
adaptation, context-aware decision-making, and the dynamic exploration of
resource capabilities. Large language models provide the possibility to
overcome these limitations through context-aware decision-making capabilities.
This paper introduces a large language model-enabled control architecture for
multi-agent manufacturing systems to dynamically explore resource capabilities
in response to real-time disruptions. A simulation-based case study
demonstrates that the proposed architecture improves system resilience and
flexibility. The case study findings show improved throughput and efficient
resource utilization compared to existing approaches.

</details>


### [366] [Understanding the Information Propagation Effects of Communication Topologies in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.23352)
*Xu Shen,Yixin Liu,Yiwei Dai,Yili Wang,Rui Miao,Yue Tan,Shirui Pan,Xin Wang*

Main category: cs.MA

TL;DR: 在基于大型语言模型的多智能体系统中，通信拓扑结构通过调节错误传播和有益信息扩散，在集体决策效率和效果之间起到关键作用。本文提出了一个因果框架，分析了不同稀疏程度的拓扑结构下智能体输出（正确或错误）的传播情况，并提出了一种新的拓扑设计方法EIB-learner，该方法结合了密集和稀疏图的连接模式，平衡了错误抑制和有益信息传播，实现了任务性能的优化。实验表明，EIB-learner在有效性、通信成本和鲁棒性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的关于通信拓扑自动设计的研究通常倾向于构建稀疏结构以提高效率，但忽略了稀疏和密集拓扑在何时以及为何有助于或阻碍协作的问题。

Method: 1. 提出一个因果框架，用于分析不同稀疏程度的拓扑结构下，智能体输出（正确或错误）的传播情况。
2. 基于研究发现，提出了一种新的拓扑设计方法EIB-learner，该方法融合了密集和稀疏图的连接模式，以平衡错误抑制和有益信息传播。

Result: 实证研究表明，适度稀疏的拓扑结构能够有效抑制错误传播，同时保留有益的信息扩散，从而实现最优的任务性能。提出的EIB-learner方法在有效性、通信成本和鲁棒性方面表现出色。

Conclusion: 适度稀疏的拓扑结构在多智能体系统中具有显著优势，能够在抑制错误传播的同时促进有益信息的扩散。所提出的EIB-learner方法为通信拓扑的设计提供了一种有效的解决方案。

Abstract: The communication topology in large language model-based multi-agent systems
fundamentally governs inter-agent collaboration patterns, critically shaping
both the efficiency and effectiveness of collective decision-making. While
recent studies for communication topology automated design tend to construct
sparse structures for efficiency, they often overlook why and when sparse and
dense topologies help or hinder collaboration. In this paper, we present a
causal framework to analyze how agent outputs, whether correct or erroneous,
propagate under topologies with varying sparsity. Our empirical studies reveal
that moderately sparse topologies, which effectively suppress error propagation
while preserving beneficial information diffusion, typically achieve optimal
task performance. Guided by this insight, we propose a novel topology design
approach, EIB-leanrner, that balances error suppression and beneficial
information propagation by fusing connectivity patterns from both dense and
sparse graphs. Extensive experiments show the superior effectiveness,
communication cost, and robustness of EIB-leanrner.

</details>


### [367] [Collaborative Last-Mile Delivery: A Multi-Platform Vehicle Routing Problem With En-route Charging](https://arxiv.org/abs/2505.23584)
*Sumbal Malik,Majid Khonji,Khaled Elbassioni,Jorge Dias*

Main category: cs.MA

TL;DR: This paper introduces a new logistics problem VRP-DR, which involves trucks, drones and robots working together to deliver parcels. It is formulated as a MILP model and solved by a scalable heuristic algorithm FINDER.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of e-commerce and the demand for timely, cost-effective last-mile delivery have increased interest in collaborative logistics.

Method: A novel collaborative synchronized multi-platform vehicle routing problem with drones and robots (VRP-DR) is introduced and formulated as a mixed-integer linear program (MILP). A scalable heuristic algorithm, FINDER, is developed to solve large-scale instances.

Result: Numerical experiments show significant time savings of the combined delivery mode over the truck-only mode and substantial cost reductions from enabling multi-visits. Insights are provided into the effects of en-route charging, docking flexibility, drone count, speed, and payload capacity on system performance.

Conclusion: The VRP-DR model and the FINDER algorithm provide efficient solutions for collaborative logistics problems involving trucks, drones and robots.

Abstract: The rapid growth of e-commerce and the increasing demand for timely,
cost-effective last-mile delivery have increased interest in collaborative
logistics. This research introduces a novel collaborative synchronized
multi-platform vehicle routing problem with drones and robots (VRP-DR), where a
fleet of $\mathcal{M}$ trucks, $\mathcal{N}$ drones and $\mathcal{K}$ robots,
cooperatively delivers parcels. Trucks serve as mobile platforms, enabling the
launching, retrieving, and en-route charging of drones and robots, thereby
addressing critical limitations such as restricted payload capacities, limited
range, and battery constraints. The VRP-DR incorporates five realistic
features: (1) multi-visit service per trip, (2) multi-trip operations, (3)
flexible docking, allowing returns to the same or different trucks (4) cyclic
and acyclic operations, enabling return to the same or different nodes; and (5)
en-route charging, enabling drones and robots to recharge while being
transported on the truck, maximizing operational efficiency by utilizing idle
transit time. The VRP-DR is formulated as a mixed-integer linear program (MILP)
to minimize both operational costs and makespan. To overcome the computational
challenges of solving large-scale instances, a scalable heuristic algorithm,
FINDER (Flexible INtegrated Delivery with Energy Recharge), is developed, to
provide efficient, near-optimal solutions. Numerical experiments across various
instance sizes evaluate the performance of the MILP and heuristic approaches in
terms of solution quality and computation time. The results demonstrate
significant time savings of the combined delivery mode over the truck-only mode
and substantial cost reductions from enabling multi-visits. The study also
provides insights into the effects of en-route charging, docking flexibility,
drone count, speed, and payload capacity on system performance.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [368] [LCB-CV-UNet: Enhanced Detector for High Dynamic Range Radar Signals](https://arxiv.org/abs/2505.23454)
*Yanbin Wang,Xingyu Chen,Yumiao Wang,Xiang Wang,Chuanfei Zang,Guolong Cui,Jiahuan Liu*

Main category: eess.SP

TL;DR: The paper proposes LCB-CV-UNet with a Logarithmic Connect Block (LCB) and Dual Hybrid Dataset Construction method to handle HDR radar signals, showing improved detection probability with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation caused by High Dynamic Range (HDR) radar signals.

Method: Proposes LCB-CV-UNet including a hardware-efficient Logarithmic Connect Block (LCB) for phase coherence preservation and Dual Hybrid Dataset Construction method for generating semi-synthetic datasets.

Result: Achieves about 1% total detection probability improvement with under 0.9% added computational complexity compared to the baseline; excels 5% over the baseline in the range of 11-13 dB signal-to-noise ratio typical for urban targets.

Conclusion: The model's practicality is validated through real experiments.

Abstract: We propose the LCB-CV-UNet to tackle performance degradation caused by High
Dynamic Range (HDR) radar signals. Initially, a hardware-efficient,
plug-and-play module named Logarithmic Connect Block (LCB) is proposed as a
phase coherence preserving solution to address the inherent challenges in
handling HDR features. Then, we propose the Dual Hybrid Dataset Construction
method to generate a semi-synthetic dataset, approximating typical HDR signal
scenarios with adjustable target distributions. Simulation results show about
1% total detection probability improvement with under 0.9% computational
complexity added compared with the baseline. Furthermore, it excels 5% over the
baseline at the range in 11-13 dB signal-to-noise ratio typical for urban
targets. Finally, the real experiment validates the practicality of our model.

</details>


### [369] [Temporal Convolutional Autoencoder for Interference Mitigation in FMCW Radar Altimeters](https://arxiv.org/abs/2505.22783)
*Charles E. Thornton,Jamie Sloop,Samuel Brown,Aaron Orndorff,William C. Headley,Stephen Young*

Main category: eess.SP

TL;DR: The paper investigates the performance of a convolutional autoencoder-based interference mitigation approach for FMCW radar altimeters, showing that a TCN autoencoder provides superior interference suppression compared to LMS adaptive filter and operates directly on the received FMCW signal.


<details>
  <summary>Details</summary>
Motivation: To improve altitude estimation performance by effectively exploiting temporal correlations in the received signal for interference suppression in FMCW radar altimeters.

Method: Using a Temporal Convolutional Network (TCN) autoencoder to operate directly on the received FMCW signal for end-to-end altitude estimation.

Result: The TCN autoencoder shows superior interference suppression compared to the Least Mean Squares (LMS) adaptive filter.

Conclusion: The convolutional autoencoder-based approach is effective for interference mitigation in FMCW radar altimeters; however, challenges remain in applying deep learning to wideband FMCW interference mitigation.

Abstract: We investigate the end-to-end altitude estimation performance of a
convolutional autoencoder-based interference mitigation approach for
frequency-modulated continuous-wave (FMCW) radar altimeters. Specifically, we
show that a Temporal Convolutional Network (TCN) autoencoder effectively
exploits temporal correlations in the received signal, providing superior
interference suppression compared to a Least Mean Squares (LMS) adaptive
filter. Unlike existing approaches, the present method operates directly on the
received FMCW signal. Additionally, we identify key challenges in applying deep
learning to wideband FMCW interference mitigation and outline directions for
future research to enhance real-time feasibility and generalization to
arbitrary interference conditions.

</details>


### [370] [Topological Adaptive Least Mean Squares Algorithms over Simplicial Complexes](https://arxiv.org/abs/2505.23160)
*Lorenzo Marinucci,Claudio Battiloro,Paolo Di Lorenzo*

Main category: eess.SP

TL;DR: This paper introduces a novel adaptive framework for processing dynamic flow signals over simplicial complexes, which extends classical LMS methods to high-order topological domains and demonstrates superior performance compared to graph-based LMS methods.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for processing dynamic flow signals observed over time-varying edge subsets by extending classical least-mean-squares (LMS) methods to high-order topological domains.

Method: The authors present a topological LMS algorithm based on discrete Hodge theory. They also propose strategies for designing optimal edge sampling probabilities and introduce an adaptive topology inference method. Additionally, they propose a distributed version of the algorithm.

Result: Empirical results on synthetic and real-world traffic data show that the proposed approach outperforms graph-based LMS methods in both centralized and distributed settings by leveraging higher-order topological features.

Conclusion: The novel adaptive framework for processing dynamic flow signals over simplicial complexes is effective and offers improved performance over existing methods.

Abstract: This paper introduces a novel adaptive framework for processing dynamic flow
signals over simplicial complexes, extending classical least-mean-squares (LMS)
methods to high-order topological domains. Building on discrete Hodge theory,
we present a topological LMS algorithm that efficiently processes streaming
signals observed over time-varying edge subsets. We provide a detailed
stochastic analysis of the algorithm, deriving its stability conditions,
steady-state mean-square-error, and convergence speed, while exploring the
impact of edge sampling on performance. We also propose strategies to design
optimal edge sampling probabilities, minimizing rate while ensuring desired
estimation accuracy. Assuming partial knowledge of the complex structure (e.g.,
the underlying graph), we introduce an adaptive topology inference method that
integrates with the proposed LMS framework. Additionally, we propose a
distributed version of the algorithm and analyze its stability and
mean-square-error properties. Empirical results on synthetic and real-world
traffic data demonstrate that our approach, in both centralized and distributed
settings, outperforms graph-based LMS methods by leveraging higher-order
topological features.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [371] [StarBASE-GP: Biologically-Guided Automated Machine Learning for Genotype-to-Phenotype Association Analysis](https://arxiv.org/abs/2505.22746)
*Jose Guadalupe Hernandez,Attri Ghosh,Philip J. Freda,Yufei Meng,Nicholas Matsumoto,Jason H. Moore*

Main category: cs.NE

TL;DR: The paper introduces StarBASE-GP, a tool using genetic programming to find significant genetic variants in large datasets. It integrates biological knowledge and evolves machine learning pipelines to maximize explanatory power while minimizing complexity. Tested on Rattus norvegicus data for body mass index, it outperforms random baseline and naive versions.


<details>
  <summary>Details</summary>
Motivation: To develop an automated framework that can discover meaningful genetic variants associated with phenotypic variation in large-scale genomic datasets, incorporating biological domain knowledge.

Method: StarBASE-GP uses genetic programming-based multi-objective optimization to evolve machine learning pipelines. It includes nine inheritance encoding strategies, a custom linkage disequilibrium pruning node, and a dynamic variant recommendation system.

Result: StarBASE-GP consistently evolves Pareto fronts with superior performance, identifying variants associated with body mass index more accurately than a random baseline and a biologically naive version.

Conclusion: StarBASE-GP demonstrates robust potential for advancing variant discovery in complex traits by integrating evolutionary search and relevant biological theory into a flexible automated machine learning framework.

Abstract: We present the Star-Based Automated Single-locus and Epistasis analysis tool
- Genetic Programming (StarBASE-GP), an automated framework for discovering
meaningful genetic variants associated with phenotypic variation in large-scale
genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective
optimization strategy to evolve machine learning pipelines that simultaneously
maximize explanatory power (r2) and minimize pipeline complexity. Biological
domain knowledge is integrated at multiple stages, including the use of nine
inheritance encoding strategies to model deviations from additivity, a custom
linkage disequilibrium pruning node that minimizes redundancy among features,
and a dynamic variant recommendation system that prioritizes informative
candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of
Rattus norvegicus (brown rat) to identify variants associated with body mass
index, benchmarking its performance against a random baseline and a
biologically naive version of the tool. StarBASE-GP consistently evolves Pareto
fronts with superior performance, yielding higher accuracy in identifying both
ground truth and novel quantitative trait loci, highlighting relevant targets
for future validation. By incorporating evolutionary search and relevant
biological theory into a flexible automated machine learning framework,
StarBASE-GP demonstrates robust potential for advancing variant discovery in
complex traits.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [372] [DeepMultiConnectome: Deep Multi-Task Prediction of Structural Connectomes Directly from Diffusion MRI Tractography](https://arxiv.org/abs/2505.22685)
*Marcus J. Vroemen,Yuqian Chen,Yui Lo,Tengfei Xu,Weidong Cai,Fan Zhang,Josien P. W. Pluim,Lauren J. O'Donnell*

Main category: eess.IV

TL;DR: Diffusion MRI (dMRI) tractography is crucial for mapping brain structural connections, but traditional connectome generation is time-consuming and requires gray matter parcellation. This paper introduces DeepMultiConnectome, a deep-learning model that predicts structural connectomes directly from tractography, bypassing the need for gray matter parcellation while supporting multiple parcellation schemes.


<details>
  <summary>Details</summary>
Motivation: Traditional methods of generating connectomes are time-consuming and require gray matter parcellation which poses challenges for large-scale studies. Hence, there is a need for a faster and more scalable method to generate connectomes.

Method: The method involves using a point-cloud-based neural network with multi-task learning to classify streamlines according to their connected regions across two parcellation schemes, sharing a learned representation. The model is trained and validated on tractography from the Human Connectome Project Young Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter parcellation scheme.

Result: DeepMultiConnectome can predict multiple structural connectomes from a whole-brain tractogram containing 3 million streamlines in approximately 40 seconds. The predicted connectomes are highly correlated with traditionally generated connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region scheme) and largely preserve network properties. A test-retest analysis shows reproducibility comparable to traditionally generated connectomes.

Conclusion: DeepMultiConnectome provides a scalable, fast model for generating subject-specific connectomes across multiple parcellation schemes.

Abstract: Diffusion MRI (dMRI) tractography enables in vivo mapping of brain structural
connections, but traditional connectome generation is time-consuming and
requires gray matter parcellation, posing challenges for large-scale studies.
We introduce DeepMultiConnectome, a deep-learning model that predicts
structural connectomes directly from tractography, bypassing the need for gray
matter parcellation while supporting multiple parcellation schemes. Using a
point-cloud-based neural network with multi-task learning, the model classifies
streamlines according to their connected regions across two parcellation
schemes, sharing a learned representation. We train and validate
DeepMultiConnectome on tractography from the Human Connectome Project Young
Adult dataset ($n = 1000$), labeled with an 84 and 164 region gray matter
parcellation scheme. DeepMultiConnectome predicts multiple structural
connectomes from a whole-brain tractogram containing 3 million streamlines in
approximately 40 seconds. DeepMultiConnectome is evaluated by comparing
predicted connectomes with traditional connectomes generated using the
conventional method of labeling streamlines using a gray matter parcellation.
The predicted connectomes are highly correlated with traditionally generated
connectomes ($r = 0.992$ for an 84-region scheme; $r = 0.986$ for a 164-region
scheme) and largely preserve network properties. A test-retest analysis of
DeepMultiConnectome demonstrates reproducibility comparable to traditionally
generated connectomes. The predicted connectomes perform similarly to
traditionally generated connectomes in predicting age and cognitive function.
Overall, DeepMultiConnectome provides a scalable, fast model for generating
subject-specific connectomes across multiple parcellation schemes.

</details>


### [373] [Synthetic Generation and Latent Projection Denoising of Rim Lesions in Multiple Sclerosis](https://arxiv.org/abs/2505.23353)
*Alexandra G. Roberts,Ha M. Luu,Mert Şişman,Alexey V. Dimov,Ceren Tozlu,Ilhami Kovanlikaya,Susan A. Gauthier,Thanh D. Nguyen,Yi Wang*

Main category: eess.IV

TL;DR: 这篇文章探讨了在多发性硬化症中，通过磁共振成像的定量易感性图谱来提供预后和诊断信息。特别是，易感性图谱可以区分“边沿”病变和“非边沿”病变类型。文章提出了一种新的去噪方法，使用生成的网络训练模糊边沿病例，并显著增加少数类别。结果表明，合成病变和提议的边沿病变标签去噪方法可以改善检测，并以临床可解释的方式逼近未见边沿病变分布。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症是一种神经退行性疾病，其特征是在脑白质组织中形成病变。然而，由于边沿病变罕见，解决这个问题需要应对边沿与非边沿病变之间的类别不平衡。

Method: 生产合成的定量易感性图谱，并利用训练生成网络的投影能力展示一种新的去噪方法。该方法允许在模糊边沿病例上进行训练并显著增加少数类别。

Result: 合成病变合成和提议的边沿病变标签去噪方法最接近未见边沿病变分布，并以临床可解释的方式改善检测。

Conclusion: 作者展示了他们的方法能够有效改善多发性硬化症中边沿病变的检测，并且发布了相关的代码和生成的数据。

Abstract: Quantitative susceptibility maps from magnetic resonance images can provide
both prognostic and diagnostic information in multiple sclerosis, a
neurodegenerative disease characterized by the formation of lesions in white
matter brain tissue. In particular, susceptibility maps provide adequate
contrast to distinguish between "rim" lesions, surrounded by deposited
paramagnetic iron, and "non-rim" lesion types. These paramagnetic rim lesions
(PRLs) are an emerging biomarker in multiple sclerosis. Much effort has been
devoted to both detection and segmentation of such lesions to monitor
longitudinal change. As paramagnetic rim lesions are rare, addressing this
problem requires confronting the class imbalance between rim and non-rim
lesions. We produce synthetic quantitative susceptibility maps of paramagnetic
rim lesions and show that inclusion of such synthetic data improves classifier
performance and provide a multi-channel extension to generate accompanying
contrasts and probabilistic segmentation maps. We exploit the projection
capability of our trained generative network to demonstrate a novel denoising
approach that allows us to train on ambiguous rim cases and substantially
increase the minority class. We show that both synthetic lesion synthesis and
our proposed rim lesion label denoising method best approximate the unseen rim
lesion distribution and improve detection in a clinically interpretable manner.
We release our code and generated data at https://github.com/agr78/PRLx-GAN
upon publication.

</details>


### [374] [Can Large Language Models Challenge CNNS in Medical Image Analysis?](https://arxiv.org/abs/2505.23503)
*Shibbir Ahmed,Shahnewaz Karim Sakib,Anindya Bijoy Das*

Main category: eess.IV

TL;DR: This study introduces a multimodal AI framework for classifying medical diagnostic images, comparing CNNs and LLMs in terms of performance, efficiency, and environmental impacts.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of multimodal AI systems in medical diagnostics by comparing CNNs with LLMs to determine which offers better accuracy, efficiency, and sustainability.

Method: The study employed publicly available datasets to evaluate both CNN-based models and various LLMs using metrics such as accuracy, F1-score, execution time, energy consumption, and estimated $CO_2$ emissions.

Result: CNN-based models outperformed multimodal techniques incorporating images and contextual information. However, applying filters on top of LLMs led to significant performance improvements.

Conclusion: Multimodal AI systems have the potential to significantly improve the reliability, efficiency, and scalability of medical diagnostics in clinical settings.

Abstract: This study presents a multimodal AI framework designed for precisely
classifying medical diagnostic images. Utilizing publicly available datasets,
the proposed system compares the strengths of convolutional neural networks
(CNNs) and different large language models (LLMs). This in-depth comparative
analysis highlights key differences in diagnostic performance, execution
efficiency, and environmental impacts. Model evaluation was based on accuracy,
F1-score, average execution time, average energy consumption, and estimated
$CO_2$ emission. The findings indicate that although CNN-based models can
outperform various multimodal techniques that incorporate both images and
contextual information, applying additional filtering on top of LLMs can lead
to substantial performance gains. These findings highlight the transformative
potential of multimodal AI systems to enhance the reliability, efficiency, and
scalability of medical diagnostics in clinical settings.

</details>


### [375] [PCA for Enhanced Cross-Dataset Generalizability in Breast Ultrasound Tumor Segmentation](https://arxiv.org/abs/2505.23587)
*Christian Schmidt,Heinrich Martin Overhoff*

Main category: eess.IV

TL;DR: The paper explores the use of PCA in medical image segmentation, specifically for ultrasound images, showing that PCA preprocessing can improve model performance and external validity.


<details>
  <summary>Details</summary>
Motivation: To address the issue of limited external validity in medical image segmentation models when deployed across unseen datasets, particularly in the ultrasound image domain.

Method: Application of principal component analysis (PCA) for preprocessing ultrasound images to reduce noise and emphasize essential features by retaining 90% of dataset variance. U-Net-based segmentation models are trained on both original and PCA-preprocessed datasets.

Result: Models trained on PCA-preprocessed datasets showed improved recall and Dice scores, especially in cases where baseline performance was low. Statistically significant gains were observed in recall and Dice scores, with a 33% reduction in the decline of recall values due to external validation.

Conclusion: PCA reconstruction shows potential as a method to safeguard against declines in segmentation performance, enhancing external validity in real-world medical applications.

Abstract: In medical image segmentation, limited external validity remains a critical
obstacle when models are deployed across unseen datasets, an issue particularly
pronounced in the ultrasound image domain. Existing solutions-such as domain
adaptation and GAN-based style transfer-while promising, often fall short in
the medical domain where datasets are typically small and diverse. This paper
presents a novel application of principal component analysis (PCA) to address
this limitation. PCA preprocessing reduces noise and emphasizes essential
features by retaining approximately 90\% of the dataset variance. We evaluate
our approach across six diverse breast tumor ultrasound datasets comprising
3,983 B-mode images and corresponding expert tumor segmentation masks. For each
dataset, a corresponding dimensionality reduced PCA-dataset is created and
U-Net-based segmentation models are trained on each of the twelve datasets.
Each model trained on an original dataset was inferenced on the remaining five
out-of-domain original datasets (baseline results), while each model trained on
a PCA dataset was inferenced on five out-of-domain PCA datasets. Our
experimental results indicate that using PCA reconstructed datasets, instead of
original images, improves the model's recall and Dice scores, particularly for
model-dataset pairs where baseline performance was lowest, achieving
statistically significant gains in recall (0.57 $\pm$ 0.07 vs. 0.70 $\pm$ 0.05,
$p = 0.0004$) and Dice scores (0.50 $\pm$ 0.06 vs. 0.58 $\pm$ 0.06, $p =
0.03$). Our method reduced the decline in recall values due to external
validation by $33\%$. These findings underscore the potential of PCA
reconstruction as a safeguard to mitigate declines in segmentation performance,
especially in challenging cases, with implications for enhancing external
validity in real-world medical applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [376] [Non-convex entropic mean-field optimization via Best Response flow](https://arxiv.org/abs/2505.22760)
*Razvan-Andrei Lascu,Mateusz B. Majka*

Main category: math.OC

TL;DR: The paper investigates minimizing non-convex functionals on probability measures, regularized by relative entropy, and solving related min-max problems using the Best Response flow. It provides conditions for choosing regularizers to ensure convergence to a unique global minimizer and applies these findings to reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To extend the application of the Best Response flow from convex to non-convex optimization problems, regularized by relative entropy, and demonstrate its utility in reinforcement learning contexts such as policy optimization for Markov Decision Processes and Markov games.

Method: Utilize the Best Response flow to study the convergence influenced by non-convexity degree, regularization parameter, and tail behavior of reference measure. Choose regularizers to make the Best Response operator a contraction with respect to $L^1$-Wasserstein distance, ensuring existence of a unique fixed point which is the global minimizer.

Result: Established conditions under which the Best Response operator becomes a contraction, leading to the existence of a unique global minimizer. Demonstrated applications in reinforcement learning scenarios involving policy optimization.

Conclusion: This work extends the use of Best Response flow to non-convex optimization problems by specifying how to select appropriate regularizers, thus relaxing the assumption of convexity at the cost of specific regularizer choices. The results have implications for reinforcement learning.

Abstract: We study the problem of minimizing non-convex functionals on the space of
probability measures, regularized by the relative entropy (KL divergence) with
respect to a fixed reference measure, as well as the corresponding problem of
solving entropy-regularized non-convex-non-concave min-max problems. We utilize
the Best Response flow (also known in the literature as the fictitious play
flow) and study how its convergence is influenced by the relation between the
degree of non-convexity of the functional under consideration, the
regularization parameter and the tail behaviour of the reference measure. In
particular, we demonstrate how to choose the regularizer, given the non-convex
functional, so that the Best Response operator becomes a contraction with
respect to the $L^1$-Wasserstein distance, which then ensures the existence of
its unique fixed point, which is then shown to be the unique global minimizer
for our optimization problem. This extends recent results where the Best
Response flow was applied to solve convex optimization problems regularized by
the relative entropy with respect to arbitrary reference measures, and with
arbitrary values of the regularization parameter. Our results explain precisely
how the assumption of convexity can be relaxed, at the expense of making a
specific choice of the regularizer. Additionally, we demonstrate how these
results can be applied in reinforcement learning in the context of policy
optimization for Markov Decision Processes and Markov games with softmax
parametrized policies in the mean-field regime.

</details>


### [377] [Improved Last-Iterate Convergence of Shuffling Gradient Methods for Nonsmooth Convex Optimization](https://arxiv.org/abs/2505.23056)
*Zijian Liu,Zhengyuan Zhou*

Main category: math.OC

TL;DR: The paper studies the convergence of shuffling gradient methods for minimizing finite-sum functions with regularization, providing improved last-iterate analysis for nonsmooth cases and demonstrating faster performance than Proximal GD.


<details>
  <summary>Details</summary>
Motivation: Existing theoretical understanding of shuffling gradient methods is limited, particularly in nonsmooth (strongly) convex optimization where bounds match only those of Proximal GD. This work aims to provide a deeper analysis and establish faster convergence rates for these methods.

Method: The authors analyze the Random Reshuffle (RR) and Single Shuffle (SS) strategies, proving that they are provably faster than Proximal GD for nonsmooth (strongly) convex functions. They also provide nearly optimal convergence results for suffix averages under RR sampling in general convex cases.

Result: The analysis shows that RR and SS strategies outperform Proximal GD in nonsmooth cases, offering improved convergence rates. Additionally, the study establishes nearly optimal convergence results for suffix averages under RR sampling.

Conclusion: This work advances the theoretical understanding of shuffling gradient methods by providing improved convergence guarantees for nonsmooth (strongly) convex optimization, highlighting the benefits of randomness in these strategies.

Abstract: We study the convergence of the shuffling gradient method, a popular
algorithm employed to minimize the finite-sum function with regularization, in
which functions are passed to apply (Proximal) Gradient Descent (GD) one by one
whose order is determined by a permutation on the indices of functions. In
contrast to its easy implementation and effective performance in practice, the
theoretical understanding remains limited. A recent advance by (Liu & Zhou,
2024b) establishes the first last-iterate convergence results under various
settings, especially proving the optimal rates for smooth (strongly) convex
optimization. However, their bounds for nonsmooth (strongly) convex functions
are only as fast as Proximal GD. In this work, we provide the first improved
last-iterate analysis for the nonsmooth case demonstrating that the widely used
Random Reshuffle ($\textsf{RR}$) and Single Shuffle ($\textsf{SS}$) strategies
are both provably faster than Proximal GD, reflecting the benefit of
randomness. As an important implication, we give the first (nearly) optimal
convergence result for the suffix average under the $\textsf{RR}$ sampling
scheme in the general convex case, matching the lower bound shown by (Koren et
al., 2022).

</details>


### [378] [Gradient Methods with Online Scaling Part I. Theoretical Foundations](https://arxiv.org/abs/2505.23081)
*Wenzhi Gao,Ya-Chi Chu,Yinyu Ye,Madeleine Udell*

Main category: math.OC

TL;DR: This paper establishes the theoretical foundations of OSGM, which uses online learning to adapt step sizes and accelerate first-order methods. It provides convergence guarantees on smooth convex problems and explains the success of hypergradient-descent heuristic.


<details>
  <summary>Details</summary>
Motivation: To develop a framework that can adapt step sizes using online learning algorithms to improve the convergence rates of first-order optimization methods.

Method: OSGM quantifies step size effectiveness via a feedback function derived from a convergence measure and adjusts the step size through an online learning algorithm. This leads to instantiations with convergence rates asymptotically no worse than optimal step size.

Result: OSGM provides trajectory-dependent global convergence for smooth convex objectives, improved complexity for smooth strongly convex problems, and local superlinear convergence. It forms a new family of first-order methods with non-asymptotic superlinear convergence.

Conclusion: OSGM not only accelerates first-order methods but also theoretically justifies the success of hypergradient-descent heuristic in machine learning optimization.

Abstract: This paper establishes the theoretical foundations of the online scaled
gradient methods (OSGM), a framework that utilizes online learning to adapt
stepsizes and provably accelerate first-order methods. OSGM quantifies the
effectiveness of a stepsize by a feedback function motivated from a convergence
measure and uses the feedback to adjust the stepsize through an online learning
algorithm. Consequently, instantiations of OSGM achieve convergence rates that
are asymptotically no worse than the optimal stepsize. OSGM yields desirable
convergence guarantees on smooth convex problems, including 1)
trajectory-dependent global convergence on smooth convex objectives; 2) an
improved complexity result on smooth strongly convex problems, and 3) local
superlinear convergence. Notably, OSGM constitutes a new family of first-order
methods with non-asymptotic superlinear convergence, joining the celebrated
quasi-Newton methods. Finally, OSGM explains the empirical success of the
popular hypergradient-descent heuristic in optimization for machine learning.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [379] [A comprehensive analysis of PINNs: Variants, Applications, and Challenges](https://arxiv.org/abs/2505.22761)
*Afila Ajithkumar Sophiya,Akarsh K Nair,Sepehr Maleki,Senthil K. Krishnababu*

Main category: cs.CE

TL;DR: This paper provides a comprehensive survey of Physics Informed Neural Networks (PINNs), detailing their architecture, variants, applications, challenges, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in existing literature by providing a detailed analysis of PINNs approaches, including architecture, variants, areas of application, real-world use cases, and challenges.

Method: The paper conducts a survey exploring various aspects related to PINNs such as architecture, different variants, application domains, performance analysis on different equations, and current challenges.

Result: Offers insights into PINNs' applicability across different equations and domains, identifies prevalent challenges in implementation, and suggests future research directions.

Conclusion: The survey concludes by summarizing the contributions into three sections: overview of PINNs architecture and variants, performance analysis, and discussion of current issues and future research directions.

Abstract: Physics Informed Neural Networks (PINNs) have been emerging as a powerful
computational tool for solving differential equations. However, the
applicability of these models is still in its initial stages and requires more
standardization to gain wider popularity. Through this survey, we present a
comprehensive overview of PINNs approaches exploring various aspects related to
their architecture, variants, areas of application, real-world use cases,
challenges, and so on. Even though existing surveys can be identified, they
fail to provide a comprehensive view as they primarily focus on either
different application scenarios or limit their study to a superficial level.
This survey attempts to bridge the gap in the existing literature by presenting
a detailed analysis of all these factors combined with recent advancements and
state-of-the-art research in PINNs. Additionally, we discuss prevalent
challenges in PINNs implementation and present some of the future research
directions as well. The overall contributions of the survey can be summarised
into three sections: A detailed overview of PINNs architecture and variants, a
performance analysis of PINNs on different equations and application domains
highlighting their features. Finally, we present a detailed discussion of
current issues and future research directions.

</details>


### [380] [Unified Network-Based Representation of BIM Models for Embedding Semantic, Spatial, and Topological Data](https://arxiv.org/abs/2505.22670)
*Jin Han,Xin-Zheng Lu,Jia-Rui Lin*

Main category: cs.CE

TL;DR: The paper proposes a network-based representation method for BIM models that integrates semantic, spatial, and topological relationships.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks effective methods for capturing the complex spatial and topological relationships between components in BIM models.

Method: A unified network-based representation method is proposed that extends the IFC standard to include local spatial relationships and topological connections between components.

Result: This method allows for a more detailed understanding of component interactions, dependencies, and design patterns in BIM models.

Conclusion: This new representation method holds significant potential for the representation and learning of design patterns in BIM models.

Abstract: Building Information Modeling (BIM) has revolutionized the construction
industry by providing a comprehensive digital representation of building
structures throughout their lifecycle. However, existing research lacks
effective methods for capturing the complex spatial and topological
relationships between components in BIM models, which are essential for
understanding design patterns and enhancing decision-making. This study
proposes a unified network-based representation method that integrates the
"semantic-spatial-topological" multi-dimensional design features of BIM models.
By extending the IFC (Industry Foundation Classes) standard, we introduce local
spatial relationships and topological connections between components to enrich
the network structure. This representation method enables a more detailed
understanding of component interactions, dependencies, and implicit design
patterns, effectively capturing the semantic, topological, and spatial
relationships in BIM, and holds significant potential for the representation
and learning of design patterns.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [381] [Learning to Charge More: A Theoretical Study of Collusion by Q-Learning Agents](https://arxiv.org/abs/2505.22909)
*Cristian Chica,Yinglong Guo,Gilad Lerman*

Main category: econ.GN

TL;DR: 实验表明Q-learning代理可能会学习收取超竞争价格。本文在无限重复博弈中首次对此行为提供了理论解释。企业仅根据观察到的利润更新定价策略，而不计算均衡策略。当博弈同时存在一个阶段的纳什均衡价格和共谋启用价格，并且Q函数在实验结束时满足某些不等式时，企业学会持续收取超竞争价格。文章引入了一种新的单一记忆子博弈完美均衡(SPEs)类别，并提供了支持所学行为的条件，包括天真的共谋、严厉触发政策或递增策略。只有在共谋启用价格为一个阶段的纳什均衡时，天真的共谋才构成SPE，而严厉触发政策可以构成SPE。


<details>
  <summary>Details</summary>
Motivation: 需要解释为什么Q-learning代理在特定条件下会学习收取超竞争价格的行为，以及这种行为如何在理论上被理解和支持。

Method: 通过分析无限重复博弈中的企业行为，研究了在只基于观察到的利润更新定价策略的情况下，Q-learning代理的学习过程。引入了一个新的单一记忆子博弈完美均衡(SPEs)类别，并探讨了天真的共谋、严厉触发政策和递增策略对学习行为的支持条件。

Result: 发现当满足特定条件时，Q-learning代理确实可以学习到持续收取超竞争价格。这些条件包括存在一个阶段的纳什均衡价格和共谋启用价格，以及Q函数满足某些不等式。此外，不同的策略（如天真的共谋和严厉触发政策）对学习行为有不同的支持效果。

Conclusion: 本文揭示了Q-learning代理在无限重复博弈中学习收取超竞争价格的理论基础，强调了单一记忆子博弈完美均衡的重要性，并说明了不同策略在支持这种学习行为方面的作用。

Abstract: There is growing experimental evidence that $Q$-learning agents may learn to
charge supracompetitive prices. We provide the first theoretical explanation
for this behavior in infinite repeated games. Firms update their pricing
policies based solely on observed profits, without computing equilibrium
strategies. We show that when the game admits both a one-stage Nash equilibrium
price and a collusive-enabling price, and when the $Q$-function satisfies
certain inequalities at the end of experimentation, firms learn to consistently
charge supracompetitive prices. We introduce a new class of one-memory subgame
perfect equilibria (SPEs) and provide conditions under which learned behavior
is supported by naive collusion, grim trigger policies, or increasing
strategies. Naive collusion does not constitute an SPE unless the
collusive-enabling price is a one-stage Nash equilibrium, whereas grim trigger
policies can.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [382] [NGPU-LM: GPU-Accelerated N-Gram Language Model for Context-Biasing in Greedy ASR Decoding](https://arxiv.org/abs/2505.22857)
*Vladimir Bataev,Andrei Andrusenko,Lilit Grigoryan,Aleksandr Laptev,Vitaly Lavrukhin,Boris Ginsburg*

Main category: eess.AS

TL;DR: The paper proposes NGPU-LM, an efficient approach for statistical n-gram language models that enables fast and parallel operations on GPUs, reducing the accuracy gap between greedy and beam search without significant slowdown.


<details>
  <summary>Details</summary>
Motivation: Existing implementations of statistical n-gram language models lack computational efficiency due to poor parallelization, which makes context-biasing less appealing for industrial use.

Method: NGPU-LM introduces customizable greedy decoding for major ASR model types including transducers, attention encoder-decoder models, and CTC, with less than 7% computational overhead. It is designed for fast and parallel operations optimized for GPU inference.

Result: NGPU-LM can eliminate more than 50% of the accuracy gap between greedy and beam search in out-of-domain scenarios while avoiding the significant slowdown typically caused by beam search.

Conclusion: The implementation of NGPU-LM is open-sourced and represents a computationally efficient solution for context-biasing tasks in ASR.

Abstract: Statistical n-gram language models are widely used for context-biasing tasks
in Automatic Speech Recognition (ASR). However, existing implementations lack
computational efficiency due to poor parallelization, making context-biasing
less appealing for industrial use. This work rethinks data structures for
statistical n-gram language models to enable fast and parallel operations for
GPU-optimized inference. Our approach, named NGPU-LM, introduces customizable
greedy decoding for all major ASR model types - including transducers,
attention encoder-decoder models, and CTC - with less than 7% computational
overhead. The proposed approach can eliminate more than 50% of the accuracy gap
between greedy and beam search for out-of-domain scenarios while avoiding
significant slowdown caused by beam search. The implementation of the proposed
NGPU-LM is open-sourced.

</details>


### [383] [Spoken question answering for visual queries](https://arxiv.org/abs/2505.23308)
*Nimrod Shabtay,Zvi Kons,Avihu Dekel,Hagai Aronowitz,Ron Hoory,Assaf Arbelle*

Main category: eess.AS

TL;DR: This paper aims to develop a spoken visual question answering (SVQA) system that integrates text, speech, and image modalities for user interaction. Since no dataset exists for SVQA, they synthesized VQA datasets using zero-shot Text-to-Speech (TTS) models. Results show that models trained with synthesized speech perform comparably to those trained on textual QAs, with TTS model choice having minimal impact on accuracy.


<details>
  <summary>Details</summary>
Motivation: To create a more interactive and accessible QA system by enabling both speech and image inputs.

Method: Fusion of text, speech, and image modalities to tackle the task of spoken VQA (SVQA). Synthesizing VQA datasets using two zero-shot TTS models to train and evaluate SVQA models.

Result: Models trained only with synthesized speech nearly reach the performance of the upper-bounding model trained on textual QAs. The choice of the TTS model has a minor impact on accuracy.

Conclusion: The synthesis of VQA datasets using zero-shot TTS models is effective for training SVQA systems, achieving near-comparable performance to models trained on textual QAs.

Abstract: Question answering (QA) systems are designed to answer natural language
questions. Visual QA (VQA) and Spoken QA (SQA) systems extend the textual QA
system to accept visual and spoken input respectively.
  This work aims to create a system that enables user interaction through both
speech and images. That is achieved through the fusion of text, speech, and
image modalities to tackle the task of spoken VQA (SVQA). The resulting
multi-modal model has textual, visual, and spoken inputs and can answer spoken
questions on images.
  Training and evaluating SVQA models requires a dataset for all three
modalities, but no such dataset currently exists. We address this problem by
synthesizing VQA datasets using two zero-shot TTS models. Our initial findings
indicate that a model trained only with synthesized speech nearly reaches the
performance of the upper-bounding model trained on textual QAs. In addition, we
show that the choice of the TTS model has a minor impact on accuracy.

</details>


### [384] [DeepFilterGAN: A Full-band Real-time Speech Enhancement System with GAN-based Stochastic Regeneration](https://arxiv.org/abs/2505.23515)
*Sanberk Serbest,Tijana Stojkovic,Milos Cernak,Andrew Harper*

Main category: eess.AS

TL;DR: This paper proposes a full-band real-time speech enhancement system with GAN-based stochastic regeneration to reduce distortion and over-suppression in the output. The model uses 3.58M parameters for low latency and real-time streaming, showing improvements in NISQA-MOS metric.


<details>
  <summary>Details</summary>
Motivation: Predictive models may lead to over-suppression of speech content while generative models aim to learn the full distribution. Combining both within the stochastic regeneration framework can reduce distortion in the output.

Method: The authors propose a real-time speech enhancement system using GAN-based stochastic regeneration. It has 3.58M parameters for lightweight architecture and low latency suitable for real-time streaming. Noisy conditioning is an important factor as shown by ablation study.

Result: Experiments demonstrate improvement over the first stage in terms of NISQA-MOS metric. Further improvements were made after participating in 2025 Urgent Challenge.

Conclusion: The proposed system successfully reduces distortion and over-suppression in speech enhancement with a lightweight architecture and low latency suitable for real-time applications.

Abstract: In this work, we propose a full-band real-time speech enhancement system with
GAN-based stochastic regeneration. Predictive models focus on estimating the
mean of the target distribution, whereas generative models aim to learn the
full distribution. This behavior of predictive models may lead to
over-suppression, i.e. the removal of speech content. In the literature, it was
shown that combining a predictive model with a generative one within the
stochastic regeneration framework can reduce the distortion in the output. We
use this framework to obtain a real-time speech enhancement system. With 3.58M
parameters and a low latency, our system is designed for real-time streaming
with a lightweight architecture. Experiments show that our system improves over
the first stage in terms of NISQA-MOS metric. Finally, through an ablation
study, we show the importance of noisy conditioning in our system. We
participated in 2025 Urgent Challenge with our model and later made further
improvements.

</details>
