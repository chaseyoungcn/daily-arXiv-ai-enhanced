<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 77]
- [cs.LG](#cs.LG) [Total: 196]
- [cs.CR](#cs.CR) [Total: 43]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 78]
- [cs.CE](#cs.CE) [Total: 2]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.NE](#cs.NE) [Total: 5]
- [eess.AS](#eess.AS) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [math.NT](#math.NT) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 3]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.LO](#cs.LO) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.MA](#cs.MA) [Total: 6]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 4]
- [cs.SE](#cs.SE) [Total: 5]
- [stat.ME](#stat.ME) [Total: 1]
- [cs.RO](#cs.RO) [Total: 15]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 14]
- [cs.SD](#cs.SD) [Total: 6]
- [q-fin.CP](#q-fin.CP) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [eess.SP](#eess.SP) [Total: 21]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CL](#cs.CL) [Total: 64]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.CY](#cs.CY) [Total: 10]
- [cs.MS](#cs.MS) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 3]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach](https://arxiv.org/abs/2506.06282)
*Shuangyan Deng,Haizhou Peng,Jiachen Xu,Chunhou Liu,Ciprian Doru Giurcuaneanu,Jiamou Liu*

Main category: cs.AI

TL;DR: The paper introduces a new benchmark for evaluating AI models' financial reasoning ability and proposes an error-aware learning framework.


<details>
  <summary>Details</summary>
Motivation: Effective financial reasoning requires both textual understanding and the ability to interpret complex visual data, but current AI models face limitations in these areas.

Method: The method involves creating a benchmark with 3,200 expert-level question-answer pairs across 15 core financial topics that integrates both textual and visual modalities. An error-aware learning framework is also proposed, which leverages historical model mistakes and feedback to guide inference without requiring fine-tuning.

Result: Experiments show that multimodal inputs significantly enhance performance and incorporating error feedback leads to consistent and measurable improvements. However, challenges remain in visual understanding and mathematical logic.

Conclusion: The results demonstrate the promise of self-reflective reasoning in financial AI systems and highlight persistent challenges in visual understanding and mathematical logic.

Abstract: Effective financial reasoning demands not only textual understanding but also
the ability to interpret complex visual data such as charts, tables, and trend
graphs. This paper introduces a new benchmark designed to evaluate how well AI
models - especially large language and multimodal models - reason in
finance-specific contexts. Covering 3,200 expert-level question-answer pairs
across 15 core financial topics, the benchmark integrates both textual and
visual modalities to reflect authentic analytical challenges in finance. To
address limitations in current reasoning approaches, we propose an error-aware
learning framework that leverages historical model mistakes and feedback to
guide inference, without requiring fine-tuning. Our experiments across
state-of-the-art models show that multimodal inputs significantly enhance
performance and that incorporating error feedback leads to consistent and
measurable improvements. The results highlight persistent challenges in visual
understanding and mathematical logic, while also demonstrating the promise of
self-reflective reasoning in financial AI systems. Our code and data can be
found at https://anonymous/FinMR/CodeData.

</details>


### [2] [Unreal Patterns](https://arxiv.org/abs/2506.06284)
*John Beverley,Jim Logan*

Main category: cs.AI

TL;DR: This paper proposes a framework within the Basic Formal Ontology for representing information about non-existent entities like fictional characters or future scenarios, critiquing traditional methods and emphasizing practical, computationally efficient solutions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional approaches to handling non-existent entities that either overcommit to metaphysical assumptions or introduce computational inefficiencies.

Method: The paper develops an ontology-driven approach modeling non-existent entities using intersections of actual types rather than specific tokens, staying within realist commitments.

Result: Proposes a structured, computationally viable solution for handling references to hypothetical or non-existent entities without overcommitting to metaphysical assumptions.

Conclusion: An ontology-driven approach offers a practical and efficient means to manage information about non-existent entities, balancing philosophical rigor with computational feasibility.

Abstract: This paper introduces a framework for representing information about entities
that do not exist or may never exist, such as those involving fictional
entities, blueprints, simulations, and future scenarios. Traditional approaches
that introduce "dummy instances" or rely on modal logic are criticized, and a
proposal is defended in which such cases are modeled using the intersections of
actual types rather than specific non existent tokens. The paper positions
itself within the Basic Formal Ontology and its realist commitments,
emphasizing the importance of practical, implementable solutions over purely
metaphysical or philosophical proposals, arguing that existing approaches to
non existent entities either overcommit to metaphysical assumptions or
introduce computational inefficiencies that hinder applications. By developing
a structured ontology driven approach to unreal patterns, the paper aims to
provide a useful and computationally viable means of handling references to
hypothetical or non existent entities.

</details>


### [3] [NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting](https://arxiv.org/abs/2506.06285)
*Kaike Sa Teles Rocha Alves,Eduardo Pestana de Aguiar*

Main category: cs.AI

TL;DR: Evolving Fuzzy Systems (eFS) are adaptive and interpretable, but lack public implementations. This paper introduces a Python library 'evolvingfuzzysystems' that implements several eFS models, evaluates them using the fetch_california_housing dataset, and finds ePL to be a simple yet efficient model.


<details>
  <summary>Details</summary>
Motivation: To make Evolving Fuzzy Systems more accessible and promote their adoption by providing publicly available implementations.

Method: Creating a Python library named 'evolvingfuzzysystems' which includes implementations of multiple eFS models and tools for training, visualization, and performance assessment.

Result: The ePL model was found to be simple and efficient, balancing accuracy and computational cost. The library facilitates model evaluation and comparison.

Conclusion: The introduction of 'evolvingfuzzysystems' aims to enhance research and practical applications in adaptive and interpretable machine learning.

Abstract: Evolving Fuzzy Systems (eFS) have gained significant attention due to their
ability to adaptively update their structure in response to data dynamics while
maintaining interpretability. However, the lack of publicly available
implementations of these models limits their accessibility and widespread
adoption. To address this gap, we present evolvingfuzzysystems, a Python
library that provides implementations of several well-established eFS models,
including ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\_eTS, and eTS. The
library facilitates model evaluation and comparison by offering built-in tools
for training, visualization, and performance assessment. The models are
evaluated using the fetch\_california\_housing dataset, with performance
measured in terms of normalized root-mean-square error (NRMSE), non-dimensional
error index (NDEI), and mean absolute percentage error (MAPE). Additionally,
computational complexity is analyzed by measuring execution times and rule
evolution during training and testing phases. The results highlight ePL as a
simple yet efficient model that balances accuracy and computational cost,
making it particularly suitable for real-world applications. By making these
models publicly available, evolvingfuzzysystems aims to foster research and
practical applications in adaptive and interpretable machine learning.

</details>


### [4] [Deep Research Bench: Evaluating AI Web Research Agents](https://arxiv.org/abs/2506.06287)
*FutureSearch,:,Nikos I. Bosse,Jon Evans,Robert G. Gambee,Daniel Hnyk,Peter Mühlbacher,Lawrence Phillips,Dan Schwarz,Jack Wildman*

Main category: cs.AI

TL;DR: The paper introduces Deep Research Bench, a benchmark for evaluating web research agents using LLMs. It includes 89 multi-step tasks, a RetroSearch environment, and tools to assess model performance over time.


<details>
  <summary>Details</summary>
Motivation: To address the lack of direct evaluations of web research agent quality that account for the ever-changing web.

Method: Created Deep Research Bench with 89 task instances across 8 categories, developed RetroSearch environment with frozen web pages, provided agent tooling for benchmarking LLMs, and included automated evaluations.

Result: Demonstrated comparable performance between offline RetroSearch agents and live web agents, providing reliable evaluations over time, and evaluated major web research products.

Conclusion: Deep Research Bench offers a reliable way to evaluate and benchmark web research agents, with results available on a public leaderboard.

Abstract: Amongst the most common use cases of modern AI is LLM chat with web search
enabled. However, no direct evaluations of the quality of web research agents
exist that control for the continually-changing web. We introduce Deep Research
Bench, consisting of 89 multi-step web research task instances of varying
difficulty across 8 diverse task categories, with the answers carefully worked
out by skilled humans. We provide a "RetroSearch" environment with a large
frozen set of scraped web pages, and demonstrate that offline "RetroSearch"
agents perform comparably to "live web" agents, enabling reliable evaluations
of models over time. We provide robust agent tooling and scaffolding to
benchmark major LLMs as they are released, including "thinking" models like o3
and Gemini 2.5 Pro. We include automated evaluations of the lengthy agent
traces to report progress over time in hallucinations, tool use, and
forgetting. Finally, we evaluate the major web research products branded as
"Deep Research", "Deep Search", "Search", or "Research." Results are available
on a public leaderboard at https://drb.futuresearch.ai/.

</details>


### [5] [Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review](https://arxiv.org/abs/2506.06301)
*Muhammad Monjurul Karim,Yan Shi,Shucheng Zhang,Bingzhang Wang,Mehrdad Nasri,Yinhai Wang*

Main category: cs.AI

TL;DR: 这篇论文全面回顾了大语言模型（LLMs）在提升道路安全和流动性方面的应用与定制化方法，分析了其潜力、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法虽然取得进展，但面对复杂、动态的交通环境仍显不足，需要更先进的分析框架。而LLMs因其强大的自然语言理解、知识整合和推理能力，成为一种有前景的范式转变。

Method: 通过架构、训练、提示和多模态策略适应LLMs，以弥合与交通运输独特时空和物理数据之间的“模态差距”。系统地分析了LLMs在流动性（如交通流量预测、信号控制）和安全性（如碰撞分析、驾驶员行为评估）中的多样化应用。还探讨了支持技术，如V2X集成、领域特定基础模型、可解释性框架和边缘计算。

Result: 尽管LLMs具有显著潜力，但在固有限制（幻觉、推理缺陷）、数据治理（隐私、偏差）、部署复杂性（模拟到现实、延迟）以及严格的安全保证方面仍存在挑战。

Conclusion: 本综述提供了一个结构化的路线图，展示了当前的能力、局限性和机遇，强调了LLMs的变革潜力，同时指出需要负责任的创新以实现更安全、更智能的运输系统。

Abstract: Roadway safety and mobility remain critical challenges for modern
transportation systems, demanding innovative analytical frameworks capable of
addressing complex, dynamic, and heterogeneous environments. While traditional
engineering methods have made progress, the complexity and dynamism of
real-world traffic necessitate more advanced analytical frameworks. Large
Language Models (LLMs), with their unprecedented capabilities in natural
language understanding, knowledge integration, and reasoning, represent a
promising paradigm shift. This paper comprehensively reviews the application
and customization of LLMs for enhancing roadway safety and mobility. A key
focus is how LLMs are adapted -- via architectural, training, prompting, and
multimodal strategies -- to bridge the "modality gap" with transportation's
unique spatio-temporal and physical data. The review systematically analyzes
diverse LLM applications in mobility (e.g., traffic flow prediction, signal
control) and safety (e.g., crash analysis, driver behavior assessment,).
Enabling technologies such as V2X integration, domain-specific foundation
models, explainability frameworks, and edge computing are also examined.
Despite significant potential, challenges persist regarding inherent LLM
limitations (hallucinations, reasoning deficits), data governance (privacy,
bias), deployment complexities (sim-to-real, latency), and rigorous safety
assurance. Promising future research directions are highlighted, including
advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI
collaboration, continuous learning, and the development of efficient,
verifiable systems. This review provides a structured roadmap of current
capabilities, limitations, and opportunities, underscoring LLMs' transformative
potential while emphasizing the need for responsible innovation to realize
safer, more intelligent transportation systems.

</details>


### [6] [Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review](https://arxiv.org/abs/2506.06324)
*Shruti Kumar,Xiaoyu Chen,Xiaomei Wang*

Main category: cs.AI

TL;DR: Several papers explore human-AI-robot co-learning and co-adaptation, but terminology lacks consistency. This scoping review aims to clarify terms (RQ1), explore types of intelligent agents and task domains studied (RQ2), and investigate cognitive theories and frameworks used in these studies (RQ3).


<details>
  <summary>Details</summary>
Motivation: To address the inconsistent use of terminology in describing human-agent collaboration and to better understand the diversity of interactions and theoretical underpinnings.

Method: Conduct a scoping review focusing on three research questions: RQ1 - gathering existing papers and examining terminology; RQ2 - exploring intelligent agents and task domains; RQ3 - investigating cognitive theories and frameworks.

Result: Expected results include clearer definitions for terms like 'co-learning' and 'co-adaptation', an overview of various intelligent agents and task domains involved in human-agent interactions, and insights into cognitive theories guiding these studies.

Conclusion: Clarifying terminology and understanding the theoretical basis of human-agent co-learning and co-adaptation will guide future research in dynamic, complex domains.

Abstract: Several papers have delved into the challenges of human-AI-robot co-learning
and co-adaptation. It has been noted that the terminology used to describe this
collaborative relationship in existing studies needs to be more consistent. For
example, the prefix "co" is used interchangeably to represent both
"collaborative" and "mutual," and the terms "co-learning" and "co-adaptation"
are sometimes used interchangeably. However, they can reflect subtle
differences in the focus of the studies. The current scoping review's primary
research question (RQ1) aims to gather existing papers discussing this
collaboration pattern and examine the terms researchers use to describe this
human-agent relationship. Given the relative newness of this area of study, we
are also keen on exploring the specific types of intelligent agents and task
domains that have been considered in existing research (RQ2). This exploration
is significant as it can shed light on the diversity of human-agent
interactions, from one-time to continuous learning/adaptation scenarios. It can
also help us understand the dynamics of human-agent interactions in different
task domains, guiding our expectations towards research situated in dynamic,
complex domains. Our third objective (RQ3) is to investigate the cognitive
theories and frameworks that have been utilized in existing studies to measure
human-agent co-learning and co-adaptation. This investigation is crucial as it
can help us understand the theoretical underpinnings of human-agent
collaboration and adaptation, and it can also guide us in identifying any new
frameworks proposed specifically for this type of relationship.

</details>


### [7] [Memory OS of AI Agent](https://arxiv.org/abs/2506.06326)
*Jiazheng Kang,Mingming Ji,Zhe Zhao,Ting Bai*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) suffer from fixed context windows and inadequate memory management, resulting in limited long-term memory and personalization. To address this, the authors propose MemoryOS, a Memory Operating System inspired by traditional OS memory management principles. It features a hierarchical storage architecture with short-term, mid-term, and long-term memory units, along with key modules for storage, updating, retrieval, and generation. Experiments on LoCoMo benchmark demonstrate significant improvements in F1 and BLEU-1 scores over baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges faced by LLMs due to fixed context windows and insufficient memory management, leading to limited long-term memory and personalization.

Method: Propose MemoryOS with a hierarchical storage architecture consisting of short-term, mid-term, and long-term memory units. Key operations include dynamic updates between storage units following dialogue-chain-based FIFO principle for short-term to mid-term updates and segmented page organization strategy for mid-term to long-term updates.

Result: Experiments on LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over baselines on GPT-4o-mini, indicating enhanced contextual coherence and personalized memory retention in long conversations.

Conclusion: MemoryOS enables comprehensive and efficient memory management for AI agents, significantly improving their performance in terms of contextual coherence and personalized memory retention.

Abstract: Large Language Models (LLMs) face a crucial challenge from fixed context
windows and inadequate memory management, leading to a severe shortage of
long-term memory capabilities and limited personalization in the interactive
experience with AI agents. To overcome this challenge, we innovatively propose
a Memory Operating System, i.e., MemoryOS, to achieve comprehensive and
efficient memory management for AI agents. Inspired by the memory management
principles in operating systems, MemoryOS designs a hierarchical storage
architecture and consists of four key modules: Memory Storage, Updating,
Retrieval, and Generation. Specifically, the architecture comprises three
levels of storage units: short-term memory, mid-term memory, and long-term
personal memory. Key operations within MemoryOS include dynamic updates between
storage units: short-term to mid-term updates follow a dialogue-chain-based
FIFO principle, while mid-term to long-term updates use a segmented page
organization strategy. Our pioneering MemoryOS enables hierarchical memory
integration and dynamic updating. Extensive experiments on the LoCoMo benchmark
show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the
baselines on GPT-4o-mini, showing contextual coherence and personalized memory
retention in long conversations. The implementation code is open-sourced at
https://github.com/BAI-LAB/MemoryOS.

</details>


### [8] [Will artificial agents pursue power by default?](https://arxiv.org/abs/2506.06352)
*Christian Tarsney*

Main category: cs.AI

TL;DR: 研究人员关注高级AI对人类的潜在威胁，讨论了AI追求权力的原因及可能性。本文通过形式化工具收敛和权力追求的概念，在抽象决策理论框架下评估权力作为工具性目标的收敛性。结论表明该主张有一定真实性，但预测效用可能有限，尤其在缺乏代理最终目标具体信息时。然而，对于有较大机会获得绝对或近似绝对权力的代理，工具收敛的事实更具预测性。


<details>
  <summary>Details</summary>
Motivation: 担心高级AI可能给人类带来灾难性风险，探讨为什么足够强大的AI代理可能会追求对人类的权力，因为权力是一个收敛的工具性目标，对广泛的最终目标都有用。

Method: 在抽象决策理论框架下形式化工具收敛和权力追求的概念，并评估权力是否为收敛的工具性目标。

Result: 权力作为工具性目标的收敛性主张有一定真实性，但预测效用可能有限，尤其是在无法总是根据权力对代理选项进行排名的情况下。然而，对于有机会获得绝对或近似绝对权力的代理，工具收敛的事实更具预测性。

Conclusion: 权力是收敛的工具性目标这一主张包含至少部分真理，但由于缺乏关于代理最终目标的具体信息，其预测效用可能有限。对于有机会获得绝对或近似绝对权力的代理，工具收敛的事实更具预测性。

Abstract: Researchers worried about catastrophic risks from advanced AI have argued
that we should expect sufficiently capable AI agents to pursue power over
humanity because power is a convergent instrumental goal, something that is
useful for a wide range of final goals. Others have recently expressed
skepticism of these claims. This paper aims to formalize the concepts of
instrumental convergence and power-seeking in an abstract, decision-theoretic
framework, and to assess the claim that power is a convergent instrumental
goal. I conclude that this claim contains at least an element of truth, but
might turn out to have limited predictive utility, since an agent's options
cannot always be ranked in terms of power in the absence of substantive
information about the agent's final goals. However, the fact of instrumental
convergence is more predictive for agents who have a good shot at attaining
absolute or near-absolute power.

</details>


### [9] [Towards Foundation Model on Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2506.06367)
*Jiaxin Pan,Mojtaba Nayyeri,Osama Mohammed,Daniel Hernandez,Rongchuan Zhang,Cheng Cheng,Steffen Staab*

Main category: cs.AI

TL;DR: The paper presents POSTRA, a fully-inductive approach for temporal knowledge graph link prediction that uses sinusoidal positional encodings and message passing to generate adaptive entity and relation representations. It demonstrates strong zero-shot performance on unseen data and generalizes well to novel entities, relations, and timestamps.


<details>
  <summary>Details</summary>
Motivation: Existing Temporal Knowledge Graph Embedding (TKGE) models struggle with transferring to new domains and generalizing to real-world scenarios due to their reliance on seen elements during inference. There is a need for a model that can handle unseen entities, relations, and timestamps effectively.

Method: POSTRA employs sinusoidal positional encodings to capture fine-grained temporal patterns and generates adaptive entity and relation representations through message passing conditioned on both local and global temporal contexts. The model design is agnostic to temporal granularity and time span, addressing temporal discrepancies across TKGs and facilitating time-aware structural information transfer.

Result: POSTRA demonstrates strong zero-shot performance on unseen temporal knowledge graphs and effectively generalizes to novel entities, relations, and timestamps. Extensive theoretical analysis and empirical results show that a single pretrained model can improve zero-shot performance on various inductive temporal reasoning scenarios.

Conclusion: POSTRA represents a significant step toward a foundation model for temporal KGs, offering a scalable and transferable solution for fully-inductive temporal knowledge graph link prediction.

Abstract: Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats
(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform
link prediction tasks in transductive or semi-inductive settings, which means
the entities, relations, and temporal information in the test graph are fully
or partially observed during training. Such reliance on seen elements during
inference limits the models' ability to transfer to new domains and generalize
to real-world scenarios. A central limitation is the difficulty in learning
representations for entities, relations, and timestamps that are transferable
and not tied to dataset-specific vocabularies. To overcome these limitations,
we introduce the first fully-inductive approach to temporal knowledge graph
link prediction. Our model employs sinusoidal positional encodings to capture
fine-grained temporal patterns and generates adaptive entity and relation
representations using message passing conditioned on both local and global
temporal contexts. Our model design is agnostic to temporal granularity and
time span, effectively addressing temporal discrepancies across TKGs and
facilitating time-aware structural information transfer. As a pretrained,
scalable, and transferable model, POSTRA demonstrates strong zero-shot
performance on unseen temporal knowledge graphs, effectively generalizing to
novel entities, relations, and timestamps. Extensive theoretical analysis and
empirical results show that a single pretrained model can improve zero-shot
performance on various inductive temporal reasoning scenarios, marking a
significant step toward a foundation model for temporal KGs.

</details>


### [10] [SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation](https://arxiv.org/abs/2506.06470)
*Yanwei Ren,Haotian Zhang,Fuxiang Wu,Jiayan Qiu,Jiaxing Huang,Baosheng Yu,Liu Liu*

Main category: cs.AI

TL;DR: Enhancing LLM reasoning via reusing discarded data from MCTS with SIGMA framework, achieving better performance with less data.


<details>
  <summary>Details</summary>
Motivation: Conventional methods of improving LLMs by simply scaling up datasets have diminishing returns, emphasizing the need for high-quality data. MCTS generates valuable chain-of-thought data but discards non-optimal reasoning branches that contain useful insights.

Method: SIGMA reintegrates discarded sibling nodes from MCTS to refine LLM reasoning through semantic linking and a two-stage refinement process involving critique and revision models.

Result: On the MATH benchmark, SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, surpassing state-of-the-art models trained on 590K samples.

Conclusion: Sibling-guided optimization in SIGMA significantly reduces data usage and boosts LLM reasoning.

Abstract: Enhancing large language models by simply scaling up datasets has begun to
yield diminishing returns, shifting the spotlight to data quality. Monte Carlo
Tree Search (MCTS) has emerged as a powerful technique for generating
high-quality chain-of-thought data, yet conventional approaches typically
retain only the top-scoring trajectory from the search tree, discarding sibling
nodes that often contain valuable partial insights, recurrent error patterns,
and alternative reasoning strategies. This unconditional rejection of
non-optimal reasoning branches may waste vast amounts of informative data in
the whole search tree. We propose SIGMA (Sibling Guided Monte Carlo
Augmentation), a novel framework that reintegrates these discarded sibling
nodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes
along each search path and applies a two-stage refinement: a critique model
identifies overlooked strengths and weaknesses across the sibling set, and a
revision model conducts text-based backpropagation to refine the top-scoring
trajectory in light of this comparative feedback. By recovering and amplifying
the underutilized but valuable signals from non-optimal reasoning branches,
SIGMA substantially improves reasoning trajectories. On the challenging MATH
benchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K
samples, outperforming state-of-the-art models trained on 590K samples. This
result highlights that our sibling-guided optimization not only significantly
reduces data usage but also significantly boosts LLM reasoning.

</details>


### [11] [Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility](https://arxiv.org/abs/2506.06523)
*Sumanth Pillella*

Main category: cs.AI

TL;DR: The paper presents a novel framework using reinforcement learning (RL) to improve warehouse operations in SAP Logistics Execution (LE). It achieves 95% task optimization accuracy and reduces processing times by 60%.


<details>
  <summary>Details</summary>
Motivation: To address the increasing demands of supply chains, this research aims to enhance the operational agility and efficiency of SAP LE through autonomous orchestration of warehouse tasks.

Method: By modeling warehouse processes as dynamic environments, the framework uses reinforcement learning to optimize task allocation, inventory movement, and order picking in real-time. The method is tested on a synthetic dataset of 300,000 LE transactions that simulate real-world scenarios.

Result: The analysis results in 95% task optimization accuracy and shows a 60% reduction in processing times compared to traditional methods. Visualizations like efficiency heatmaps and performance graphs are used to guide agile strategies.

Conclusion: This approach effectively addresses data privacy, scalability, and SAP integration issues, providing a transformative solution for modern supply chains.

Abstract: In an era of escalating supply chain demands, SAP Logistics Execution (LE) is
pivotal for managing warehouse operations, transportation, and delivery. This
research introduces a pioneering framework leveraging reinforcement learning
(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing
operational agility and efficiency. By modeling warehouse processes as dynamic
environments, the framework optimizes task allocation, inventory movement, and
order picking in real-time. A synthetic dataset of 300,000 LE transactions
simulates real-world warehouse scenarios, including multilingual data and
operational disruptions. The analysis achieves 95% task optimization accuracy,
reducing processing times by 60% compared to traditional methods.
Visualizations, including efficiency heatmaps and performance graphs, guide
agile warehouse strategies. This approach tackles data privacy, scalability,
and SAP integration, offering a transformative solution for modern supply
chains.

</details>


### [12] [ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search](https://arxiv.org/abs/2506.06524)
*Sam Earle,Ahmed Khalifa,Muhammad Umair Nasir,Zehua Jiang,Graham Todd,Andrzej Banburski-Fahey,Julian Togelius*

Main category: cs.AI

TL;DR: ScriptDoctor is a Large Language Model (LLM)-driven system that automatically generates and tests games in PuzzleScript, showing potential for automated LLM-based workflows in generating novel game content.


<details>
  <summary>Details</summary>
Motivation: To integrate large pre-trained models into longer-time-horizon Automatic Game Design (AGD) pipelines, where systems can autonomously test generated content by interfacing with game engines.

Method: ScriptDoctor uses an iterative loop to generate and test game design ideas. It leverages human-authored examples to ground the system's output, compilation errors from the PuzzleScript engine to produce functional code, and search-based agents to play-test generated games.

Result: ScriptDoctor successfully generates and tests game design ideas autonomously, demonstrating the potential of automated, open-ended LLM-based workflows in creating novel game content.

Conclusion: The introduction of ScriptDoctor provides a concrete example of how large language models can be utilized in automated game design processes, opening possibilities for further exploration in this area.

Abstract: There is much interest in using large pre-trained models in Automatic Game
Design (AGD), whether via the generation of code, assets, or more abstract
conceptualization of design ideas. But so far this interest largely stems from
the ad hoc use of such generative models under persistent human supervision.
Much work remains to show how these tools can be integrated into
longer-time-horizon AGD pipelines, in which systems interface with game engines
to test generated content autonomously. To this end, we introduce ScriptDoctor,
a Large Language Model (LLM)-driven system for automatically generating and
testing games in PuzzleScript, an expressive but highly constrained description
language for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates
and tests game design ideas in an iterative loop, where human-authored examples
are used to ground the system's output, compilation errors from the
PuzzleScript engine are used to elicit functional code, and search-based agents
play-test generated games. ScriptDoctor serves as a concrete example of the
potential of automated, open-ended LLM-based workflows in generating novel game
content.

</details>


### [13] [The Optimization Paradox in Clinical AI Multi-Agent Systems](https://arxiv.org/abs/2506.06574)
*Suhana Bedi,Iddah Mlauzi,Daniel Shin,Sanmi Koyejo,Nigam H. Shah*

Main category: cs.AI

TL;DR: 在多智能体AI系统中，尽管单独组件优化的性能较好，但整体系统的诊断准确率却不如最优的多智能体系统，因此在医疗AI应用中需要重视整体系统验证而非单纯依赖组件指标。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体人工智能系统在临床环境中的应用，探讨组件级优化与系统整体性能之间的关系。

Method: 使用MIMIC-CDM数据集中的2400个真实患者案例，针对四种腹部病理进行分析，将临床诊断过程分为信息收集、解释和鉴别诊断三个阶段，并比较单智能体系统（一个模型完成所有任务）和多智能体系统（每个任务有专门模型）的表现。

Result: 多智能体系统通常优于单智能体系统，但即使在组件优化较好的系统中，诊断准确率显著低于最优的多智能体系统（67.7% vs 77.4%）。

Conclusion: 医疗AI系统的成功实施不仅需要组件级别的优化，还需要关注智能体之间的信息流动和兼容性，强调端到端的系统验证的重要性。

Abstract: Multi-agent artificial intelligence systems are increasingly deployed in
clinical settings, yet the relationship between component-level optimization
and system-wide performance remains poorly understood. We evaluated this
relationship using 2,400 real patient cases from the MIMIC-CDM dataset across
four abdominal pathologies (appendicitis, pancreatitis, cholecystitis,
diverticulitis), decomposing clinical diagnosis into information gathering,
interpretation, and differential diagnosis. We evaluated single agent systems
(one model performing all tasks) against multi-agent systems (specialized
models for each task) using comprehensive metrics spanning diagnostic outcomes,
process adherence, and cost efficiency. Our results reveal a paradox: while
multi-agent systems generally outperformed single agents, the
component-optimized or Best of Breed system with superior components and
excellent process metrics (85.5% information accuracy) significantly
underperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent
system). This finding underscores that successful integration of AI in
healthcare requires not just component level optimization but also attention to
information flow and compatibility between agents. Our findings highlight the
need for end to end system validation rather than relying on component metrics
alone.

</details>


### [14] [AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture](https://arxiv.org/abs/2506.06580)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: This paper conducts a systematic survey of digital twin-enabled AI simulation, identifying technological trends and deriving a reference framework based on 22 primary studies. It maps the framework to ISO 23247 architecture, offering architectural guidelines and highlighting challenges and research opportunities.


<details>
  <summary>Details</summary>
Motivation: Insufficient data volume and quality pose significant challenges in adopting modern subsymbolic AI. The need for safe and efficient AI development environments leads to exploring digital twin-enabled AI simulation.

Method: The authors performed a systematic analysis of 22 primary studies related to digital twin-enabled AI simulation. They identified technological trends, derived a reference framework, and mapped it onto the ISO 23247 reference architecture.

Result: A reference framework for digital twin-enabled AI simulation was established, along with architectural guidelines. Challenges and research opportunities in this field were also identified.

Conclusion: Digital twins offer promising advancements in AI simulation by providing high-fidelity virtual replicas and advanced simulators. This study provides a foundation for future research in integrating digital twins with AI components.

Abstract: Insufficient data volume and quality are particularly pressing challenges in
the adoption of modern subsymbolic AI. To alleviate these challenges, AI
simulation uses virtual training environments in which AI agents can be safely
and efficiently developed with simulated, synthetic data. Digital twins open
new avenues in AI simulation, as these high-fidelity virtual replicas of
physical systems are equipped with state-of-the-art simulators and the ability
to further interact with the physical system for additional data collection. In
this article, we report on our systematic survey of digital twin-enabled AI
simulation. By analyzing 22 primary studies, we identify technological trends
and derive a reference framework to situate digital twins and AI components.
Based on our findings, we derive a reference framework and provide
architectural guidelines by mapping it onto the ISO 23247 reference
architecture for digital twins. Finally, we identify challenges and research
opportunities for prospective researchers.

</details>


### [15] [GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales](https://arxiv.org/abs/2506.06634)
*Yubin Xiao,Di Wang,Rui Cao,Xuan Wu,Boyang Li,You Zhou*

Main category: cs.AI

TL;DR: The paper presents GELD, a novel neural TSP solver that integrates a lightweight Global-view Encoder with a heavyweight Local-view Decoder to solve both small- and large-scale TSPs more efficiently than existing models.


<details>
  <summary>Details</summary>
Motivation: Current neural network-based TSP solvers struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility.

Method: GELD incorporates a broad global assessment and refined local selection framework. It integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD). GE uses a novel low-complexity attention mechanism. A two-stage training strategy is proposed to improve generalization ability.

Result: GELD outperforms seven state-of-the-art models in terms of solution quality and inference speed on both synthetic and real-world datasets. It can solve TSPs with up to 744,710 nodes without relying on divide-and-conquer strategies.

Conclusion: GELD is an effective neural TSP solver that addresses current limitations, providing superior performance for large-scale TSPs.

Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial
optimization problem with broad real-world applications. Recent advancements in
neural network-based TSP solvers have shown promising results. Nonetheless,
these models often struggle to efficiently solve both small- and large-scale
TSPs using the same set of pre-trained model parameters, limiting their
practical utility. To address this issue, we introduce a novel neural TSP
solver named GELD, built upon our proposed broad global assessment and refined
local selection framework. Specifically, GELD integrates a lightweight
Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich
embedding representation while accelerating the decision-making process.
Moreover, GE incorporates a novel low-complexity attention mechanism, allowing
GELD to achieve low inference latency and scalability to larger-scale TSPs.
Additionally, we propose a two-stage training strategy that utilizes training
instances of different sizes to bolster GELD's generalization ability.
Extensive experiments conducted on both synthetic and real-world datasets
demonstrate that GELD outperforms seven state-of-the-art models considering
both solution quality and inference speed. Furthermore, GELD can be employed as
a post-processing method to significantly elevate the quality of the solutions
derived by existing neural TSP solvers via spending affordable additional
computing time. Notably, GELD is shown as capable of solving TSPs with up to
744,710 nodes, first-of-its-kind to solve this large size TSP without relying
on divide-and-conquer strategies to the best of our knowledge.

</details>


### [16] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: Contextual Experience Replay (CER) is a training-free framework enabling efficient self-improvement for language agents in their context window.


<details>
  <summary>Details</summary>
Motivation: Large language model agents often fail in complex sequential decision-making tasks due to lack of environment-specific experiences and the inability to continually learn from past experiences during inference time.

Method: CER accumulates and synthesizes past experiences into a dynamic memory buffer, encompassing environment dynamics and common decision-making patterns, allowing agents to retrieve and augment themselves with relevant knowledge in new tasks.

Result: On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%.

Conclusion: A comprehensive analysis proves the efficiency and validity of CER, enhancing adaptability of LLM agents in complex environments.

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [17] [Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation](https://arxiv.org/abs/2506.06714)
*Hamied Nabizada,Tom Jeleniewski,Lasse Beers,Maximilian Weigand,Felix Gehlhoff,Alexander Fay*

Main category: cs.AI

TL;DR: This paper presents a SysML profile for integrating PDDL planning semantics into system models, enabling automated generation of planning descriptions and bridging system modeling with AI planning.


<details>
  <summary>Details</summary>
Motivation: To create a direct integration of PDDL planning semantics into system models to support automated and model-based generation of planning descriptions in engineering design.

Method: Defines reusable stereotypes for key PDDL concepts and uses formal OCL constraints to ensure syntactic consistency. The profile was derived from the BNF definition of PDDL 3.1.

Result: A case study in aircraft manufacturing demonstrated the successful application of the profile, where a robotic system was modeled and enriched to generate domain and problem descriptions in PDDL format that were used as input to a PDDL solver to derive optimized execution plans.

Conclusion: The presented SysML profile supports automated and model-based generation of planning descriptions and provides a reusable bridge between system modeling and AI planning in engineering design.

Abstract: This paper presents a SysML profile that enables the direct integration of
planning semantics based on the Planning Domain Definition Language (PDDL) into
system models. Reusable stereotypes are defined for key PDDL concepts such as
types, predicates, functions and actions, while formal OCL constraints ensure
syntactic consistency. The profile was derived from the Backus-Naur Form (BNF)
definition of PDDL 3.1 to align with SysML modeling practices. A case study
from aircraft manufacturing demonstrates the application of the profile: a
robotic system with interchangeable end effectors is modeled and enriched to
generate both domain and problem descriptions in PDDL format. These are used as
input to a PDDL solver to derive optimized execution plans. The approach
supports automated and model-based generation of planning descriptions and
provides a reusable bridge between system modeling and AI planning in
engineering design.

</details>


### [18] [WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making](https://arxiv.org/abs/2506.06725)
*Guillaume Levy,Cedric Colas,Pierre-Yves Oudeyer,Thomas Carta,Clement Romac*

Main category: cs.AI

TL;DR: WorldLLM 是一个结合贝叶斯推理和强化学习的框架，用于提升大型语言模型在特定领域环境中的预测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然拥有广泛的世界知识，但在结构化、特定领域的环境中生成精确预测时往往面临挑战，因为它们无法将其广泛的非结构化理解与具体环境联系起来。

Method: WorldLLM 框架通过将贝叶斯推理和自主主动探索与强化学习相结合，增强了基于 LLM 的世界建模。利用自然语言假设引导 LLM 基于世界的模型预测，并通过贝叶斯推理框架迭代改进这些假设。证据收集由好奇心驱动的强化学习策略完成，该策略探索环境以找到在当前假设下具有低对数似然性的转换。通过在改进假设和收集新证据之间交替，框架自主推动预测的持续改进。

Result: 实验表明，WorldLLM 在需要代理操作和组合对象的文本游戏环境中非常有效，不仅提高了预测精度，还生成了人类可解释的环境动力学理论。

Conclusion: WorldLLM 提供了一种增强 LLM 在特定领域环境中预测能力的有效方法，同时生成了人类可解释的环境动态理论。

Abstract: Large Language Models (LLMs) possess general world knowledge but often
struggle to generate precise predictions in structured, domain-specific
contexts such as simulations. These limitations arise from their inability to
ground their broad, unstructured understanding in specific environments. To
address this, we present WorldLLM, a framework that enhances LLM-based world
modeling by combining Bayesian inference and autonomous active exploration with
reinforcement learning. WorldLLM leverages the in-context learning abilities of
LLMs to guide an LLM-based world model's predictions using natural language
hypotheses given in its prompt. These hypotheses are iteratively refined
through a Bayesian inference framework that leverages a second LLM as the
proposal distribution given collected evidence. This evidence is collected
using a curiosity-driven reinforcement learning policy that explores the
environment to find transitions with a low log-likelihood under our LLM-based
predictive model using the current hypotheses. By alternating between refining
hypotheses and collecting new evidence, our framework autonomously drives
continual improvement of the predictions. Our experiments demonstrate the
effectiveness of WorldLLM in a textual game environment that requires agents to
manipulate and combine objects. The framework not only enhances predictive
accuracy, but also generates human-interpretable theories of environment
dynamics.

</details>


### [19] [VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs](https://arxiv.org/abs/2506.06727)
*Can Li,Ting Zhang,Mei Wang,Hua Huang*

Main category: cs.AI

TL;DR: Large Multimodal Models (LMMs) face challenges in mathematical reasoning with image-based answer options. The VisioMath benchmark, consisting of 8,070 images and 1,800 questions, is introduced to evaluate these capabilities. State-of-the-art LMMs, including GPT-4o, struggle significantly on this task.


<details>
  <summary>Details</summary>
Motivation: To explore the underexplored ability of LMMs in performing mathematical reasoning when answer options are represented as images, which is crucial for multi-image comprehension.

Method: Introduction of VisioMath, a benchmark with 8,070 images and 1,800 multiple-choice questions where each answer option is an image, to evaluate mathematical reasoning in multimodal contexts.

Result: State-of-the-art LMMs, such as GPT-4o, achieve low accuracy (45.9%) on the VisioMath benchmark, indicating significant limitations in reasoning over visually similar answer choices.

Conclusion: VisioMath fills a critical gap in existing benchmarks by providing a rigorous testbed for future research in multimodal reasoning.

Abstract: Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving
capabilities across various domains. However, their ability to perform
mathematical reasoning when answer options are represented as images--an
essential aspect of multi-image comprehension--remains underexplored. To bridge
this gap, we introduce VisioMath, a benchmark designed to evaluate mathematical
reasoning in multimodal contexts involving image-based answer choices.
VisioMath comprises 8,070 images and 1,800 multiple-choice questions, where
each answer option is an image, presenting unique challenges to existing LMMs.
To the best of our knowledge, VisioMath is the first dataset specifically
tailored for mathematical reasoning in image-based-option scenarios, where
fine-grained distinctions between answer choices are critical for accurate
problem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath
and find that even the most advanced models struggle with this task. Notably,
GPT-4o achieves only 45.9% accuracy, underscoring the limitations of current
models in reasoning over visually similar answer choices. By addressing a
crucial gap in existing benchmarks, VisioMath establishes a rigorous testbed
for future research, driving advancements in multimodal reasoning.

</details>


### [20] [Honey, I shrunk the hypothesis space (through logical preprocessing)](https://arxiv.org/abs/2506.06739)
*Andrew Cropper,Filipe Gouveia,David M. Cerna*

Main category: cs.AI

TL;DR: 通过在ILP系统搜索前'缩小'假设空间，利用背景知识发现无法存在于最优假设中的规则，并移除违反规则的假设，从而大幅减少学习时间并保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: 归纳逻辑编程（ILP）需要在假设空间中搜索能概括训练示例和背景知识的假设，但搜索过程可能非常耗时。因此，研究者希望找到一种方法来缩小假设空间，提高ILP系统的效率。

Method: 使用背景知识找出那些无论训练示例如何都不可能存在于最优假设中的规则（例如“偶数不能是奇数”），然后将违反这些规则的假设从假设空间中移除。该方法通过答案集编程实现，并应用于基于约束的ILP系统中。

Result: 实验表明，在多个领域（包括视觉推理和游戏玩法）中，这种方法可以在保持预测准确性的同时显著减少学习时间。例如，仅需10秒的预处理时间，就可以将学习时间从超过10小时减少到2秒。

Conclusion: 提出的方法通过缩小假设空间提高了ILP系统的效率，减少了学习时间且不影响预测准确性，为逻辑机器学习提供了一种有效的优化手段。

Abstract: Inductive logic programming (ILP) is a form of logical machine learning. The
goal is to search a hypothesis space for a hypothesis that generalises training
examples and background knowledge. We introduce an approach that 'shrinks' the
hypothesis space before an ILP system searches it. Our approach uses background
knowledge to find rules that cannot be in an optimal hypothesis regardless of
the training examples. For instance, our approach discovers relationships such
as "even numbers cannot be odd" and "prime numbers greater than 2 are odd". It
then removes violating rules from the hypothesis space. We implement our
approach using answer set programming and use it to shrink the hypothesis space
of a constraint-based ILP system. Our experiments on multiple domains,
including visual reasoning and game playing, show that our approach can
substantially reduce learning times whilst maintaining predictive accuracies.
For instance, given just 10 seconds of preprocessing time, our approach can
reduce learning times from over 10 hours to only 2 seconds.

</details>


### [21] [AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method](https://arxiv.org/abs/2506.06740)
*Yigui Feng,Qinglin Wang,Ke Liu,Xinhai Chen,Bo Yang,Jie Liu*

Main category: cs.AI

TL;DR: AI PsyRoom is a multi-agent framework enhancing psychological counseling via empathetic conversations and personalized treatment plans, showing significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Psychological counseling faces challenges with increasing demand and shortage of professionals. Existing LLMs lack deep emotional understanding and ability to generate personalized treatment plans.

Method: Developed AI PsyRoom with two components: PsyRoom A for dialogue reconstruction using fine-grained emotion classification and generating dataset EmoPsy; PsyRoom B for creating personalized treatment plans.

Result: Achieved 18% improvement in problem orientation, 23% in expression, 24% in empathy, and 16% in interactive communication quality compared to state-of-the-art methods.

Conclusion: AI PsyRoom significantly enhances psychological counseling through improved empathetic and emotionally nuanced conversations and personalized treatment plans, offering a valuable resource for advancing AI-assisted psychological counseling.

Abstract: Psychological counseling faces huge challenges due to the growing demand for
mental health services and the shortage of trained professionals. Large
language models (LLMs) have shown potential to assist psychological counseling,
especially in empathy and emotional support. However, existing models lack a
deep understanding of emotions and are unable to generate personalized
treatment plans based on fine-grained emotions. To address these shortcomings,
we present AI PsyRoom, a multi-agent simulation framework designed to enhance
psychological counseling by generating empathetic and emotionally nuanced
conversations. By leveraging fine-grained emotion classification and a
multi-agent framework, we construct a multi-agent PsyRoom A for dialogue
reconstruction, generating a high-quality dialogue dataset EmoPsy, which
contains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.
We also propose PsyRoom B for generating personalized treatment plans.
Quantitative evaluations demonstrate that AI PsyRoom significantly outperforms
state-of-the-art methods, achieving 18% improvement in problem orientation, 23%
in expression, 24% in Empathy, and 16% in interactive communication quality.
The datasets and models are publicly available, providing a foundation for
advancing AI-assisted psychological counseling research.

</details>


### [22] [Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules](https://arxiv.org/abs/2506.06750)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.AI

TL;DR: 在脉冲神经网络（SNN）中，选择合适的学习算法对分类准确性至关重要。传统反向传播算法虽然准确但计算成本高，而生物启发式学习算法（如tempotron和Spikprop）则在保持较高准确性的同时提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 由于SNN的独特性质（时间动态、非可微的脉冲事件及稀疏激活），其训练具有挑战性。因此，研究不同学习算法类型（包括生物启发规则）对分类精度的影响是必要的。

Method: 提出了一种结合SNN与Lempel-Ziv复杂度（LZC）的生物启发分类器。该方法利用了SNN的时间精确性和生物真实性，以及LZC的结构复杂性分析能力，从而实现高效且可解释的时空神经数据分类。

Result: 经典反向传播算法尽管有极高的分类准确率，但计算成本过高而不适合实时应用；相比之下，生物启发学习算法（如tempotron和Spikprop）不仅提高了计算效率，还保持了竞争力的分类性能。

Conclusion: 最适学习算法的选择取决于分类准确率与计算成本之间的权衡，以及具体应用场景的约束条件。

Abstract: Training of Spiking Neural Networks (SNN) is challenging due to their unique
properties, including temporal dynamics, non-differentiability of spike events,
and sparse event-driven activations. In this paper, we widely consider the
influence of the type of chosen learning algorithm, including bioinspired
learning rules on the accuracy of classification. We proposed a bioinspired
classifier based on the combination of SNN and Lempel-Ziv complexity (LZC).
This approach synergizes the strengths of SNNs in temporal precision and
biological realism with LZC's structural complexity analysis, facilitating
efficient and interpretable classification of spatiotemporal neural data. It
turned out that the classic backpropagation algorithm achieves excellent
classification accuracy, but at extremely high computational cost, which makes
it impractical for real-time applications. Biologically inspired learning
algorithms such as tempotron and Spikprop provide increased computational
efficiency while maintaining competitive classification performance, making
them suitable for time-sensitive tasks. The results obtained indicate that the
selection of the most appropriate learning algorithm depends on the trade-off
between classification accuracy and computational cost as well as application
constraints.

</details>


### [23] [Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain](https://arxiv.org/abs/2506.06786)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: CA-MIQ is a dual-critic reinforcement learning framework that enables autonomous systems to better adapt to changing priorities in search-and-rescue missions, showing significantly improved mission success rates compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: Autonomous systems in high-stakes search-and-rescue missions need to gather critical information and adapt flexibly to shifting operational priorities.

Method: Proposed CA-MIQ uses a lightweight dual-critic RL framework with an extrinsic critic for task reward and an intrinsic critic that combines state-novelty, information-location awareness, and real-time priority alignment. It also has a built-in shift detector for exploration boosts and selective critic resets.

Result: In simulated SAR grid-world experiments, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, with 100% recovery.

Conclusion: CA-MIQ proves effective in discrete environments with piecewise-stationary information-value distributions, making it suitable for dynamic search-and-rescue missions.

Abstract: Autonomous systems operating in high-stakes search-and-rescue (SAR) missions
must continuously gather mission-critical information while flexibly adapting
to shifting operational priorities. We propose CA-MIQ (Context-Aware
Max-Information Q-learning), a lightweight dual-critic reinforcement learning
(RL) framework that dynamically adjusts its exploration strategy whenever
mission priorities change. CA-MIQ pairs a standard extrinsic critic for task
reward with an intrinsic critic that fuses state-novelty, information-location
awareness, and real-time priority alignment. A built-in shift detector triggers
transient exploration boosts and selective critic resets, allowing the agent to
re-focus after a priority revision. In a simulated SAR grid-world, where
experiments specifically test adaptation to changes in the priority order of
information types the agent is expected to focus on, CA-MIQ achieves nearly
four times higher mission-success rates than baselines after a single priority
shift and more than three times better performance in multiple-shift scenarios,
achieving 100% recovery while baseline methods fail to adapt. These results
highlight CA-MIQ's effectiveness in any discrete environment with
piecewise-stationary information-value distributions.

</details>


### [24] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) can define probability measures on text, leading to the formulation of a series of tasks beyond generative sampling, which can be formulated as games based on LLM measures, called Cross-Entropy (Xent) Games. Xent Games can be used to measure the abilities of LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore what it means for an LLM to know a probability measure on text and what it entails algorithmically, leading to the formulation of tasks beyond generative sampling.

Method: Formulate tasks as games based on LLM measures, called Cross-Entropy (Xent) Games, which can be single-player or multi-player, involve cross-entropy scores and constraints, and can be expressed as computational graphs and programs.

Result: The Xent Game space is large enough to contain interesting examples and can be constructed from basic game-theoretic consistency axioms. Xent Game measures can be used as capability benchmarks for LLMs.

Conclusion: To address the unbounded scope problem in measuring general abilities of LLMs, the space of Xent Games should be explored in a coherent fashion using ideas inspired by evolutionary dynamics.

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [25] [United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory](https://arxiv.org/abs/2506.06843)
*HaoYang Shang,Xuan Liu,Zi Liang,Jie Zhang,Haibo Hu,Song Guo*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) struggle with complex tasks due to a performance ceiling, similar to human cognitive load limits. This paper introduces CoThinker, a multi-agent framework based on Cognitive Load Theory (CLT), which enhances problem-solving by distributing cognitive loads among specialized agents and using structured communication. Empirical results show improvements in solution quality and efficiency over other multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in handling complex, multi-faceted tasks where demands exceed their cognitive load capacity, drawing parallels from Cognitive Load Theory in human cognition.

Method: Introduced CoThinker, an LLM-based multi-agent framework that applies CLT principles. It involves agent specialization to distribute intrinsic cognitive load and uses structured communication and collective working memory to manage transactional load.

Result: CoThinker showed improvements in solution quality and efficiency when tested on complex problem-solving tasks and high cognitive load scenarios compared to existing multi-agent baselines. Characteristic interaction patterns emerged, providing insights into collective cognition and effective load management.

Conclusion: CoThinker offers a principled approach to overcoming the performance ceilings of LLMs by effectively managing cognitive loads through a multi-agent system inspired by CLT.

Abstract: Large Language Models (LLMs) exhibit a notable performance ceiling on
complex, multi-faceted tasks, as they often fail to integrate diverse
information or adhere to multiple constraints. We posit that such limitation
arises when the demands of a task exceed the LLM's effective cognitive load
capacity. This interpretation draws a strong analogy to Cognitive Load Theory
(CLT) in cognitive science, which explains similar performance boundaries in
the human mind, and is further supported by emerging evidence that reveals LLMs
have bounded working memory characteristics. Building upon this CLT-grounded
understanding, we introduce CoThinker, a novel LLM-based multi-agent framework
designed to mitigate cognitive overload and enhance collaborative
problem-solving abilities. CoThinker operationalizes CLT principles by
distributing intrinsic cognitive load through agent specialization and managing
transactional load via structured communication and a collective working
memory. We empirically validate CoThinker on complex problem-solving tasks and
fabricated high cognitive load scenarios, demonstrating improvements over
existing multi-agent baselines in solution quality and efficiency. Our analysis
reveals characteristic interaction patterns, providing insights into the
emergence of collective cognition and effective load management, thus offering
a principled approach to overcoming LLM performance ceilings.

</details>


### [26] [Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance](https://arxiv.org/abs/2506.06868)
*Razieh Arshadizadeh,Mahmoud Asgari,Zeinab Khosravi,Yiannis Papadopoulos,Koorosh Aslansefat*

Main category: cs.AI

TL;DR: The paper proposes a probabilistic safety assurance framework integrating SafeML with Bayesian Networks (BNs) to model ML failures in safety-critical systems, demonstrating its effectiveness in a simulated automotive platooning system.


<details>
  <summary>Details</summary>
Motivation: Machine Learning models are being integrated into safety-critical systems, but their imperfections introduce reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods are not suitable for ML components that learn from data.

Method: The authors introduce a probabilistic safety assurance framework that combines SafeML, which dynamically detects distributional shifts and assigns confidence levels to ML reasoning, with Bayesian Networks (BNs) to model ML failures within a broader causal safety analysis.

Result: The approach was demonstrated on a simulated automotive platooning system with traffic sign recognition, showing the potential broader benefits of explicitly modelling ML failures in safety assessment.

Conclusion: Explicitly modelling ML failures in safety assessment using the proposed framework has significant potential to enhance dynamic safety evaluation and system adaptation under uncertainty in safety-critical systems.

Abstract: Machine Learning (ML) models are increasingly integrated into safety-critical
systems, such as autonomous vehicle platooning, to enable real-time
decision-making. However, their inherent imperfection introduces a new class of
failure: reasoning failures often triggered by distributional shifts between
operational and training data. Traditional safety assessment methods, which
rely on design artefacts or code, are ill-suited for ML components that learn
behaviour from data. SafeML was recently proposed to dynamically detect such
shifts and assign confidence levels to the reasoning of ML-based components.
Building on this, we introduce a probabilistic safety assurance framework that
integrates SafeML with Bayesian Networks (BNs) to model ML failures as part of
a broader causal safety analysis. This allows for dynamic safety evaluation and
system adaptation under uncertainty. We demonstrate the approach on an
simulated automotive platooning system with traffic sign recognition. The
findings highlight the potential broader benefits of explicitly modelling ML
failures in safety assessment.

</details>


### [27] [KnowCoder-V2: Deep Knowledge Analysis](https://arxiv.org/abs/2506.06881)
*Zixuan Li,Wenxuan Liu,Long Bai,Chunmao Zhang,Wei Li,Fenghui Zhang,Quanxin Jin,Ruoyun He,Zhuo Chen,Zhilei Hu,Fei Wang,Bingbing Xu,Xuhui Jiang,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出KDR框架和KCII模型，解决现有深度研究框架在知识管理、计算及分析上的不足，通过离线知识组织与在线推理提升复杂任务处理能力。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究框架存在知识管理不系统、在线操作效率低以及无法进行复杂知识计算的问题，限制了其在知识分析任务中的表现。

Method: 1. 提出Knowledgeable Deep Research (KDR)框架，包含离线知识组织阶段和在线推理步骤。
2. 引入KCII模型，通过统一代码生成连接知识组织与推理：
   - 知识组织阶段生成实例化代码将数据转化为知识对象。
   - 知识计算阶段生成分析代码对知识对象进行深度分析。
3. 在多个数据集上验证KCII的有效性，并展示其在KDR框架中生成高质量报告的能力。

Result: 实验结果表明KCII在超过三十个数据集的六类知识分析任务中表现出有效性，并且在KDR框架下能生成具有深刻洞见的高质量报告。

Conclusion: KDR框架和KCII模型为深度研究提供了强大的知识分析能力，解决了现有框架在知识管理与计算上的局限性，提升了复杂知识分析任务的处理效果。

Abstract: Deep knowledge analysis tasks always involve the systematic extraction and
association of knowledge from large volumes of data, followed by logical
reasoning to discover insights. However, to solve such complex tasks, existing
deep research frameworks face three major challenges: 1) They lack systematic
organization and management of knowledge; 2) They operate purely online, making
it inefficient for tasks that rely on shared and large-scale knowledge; 3) They
cannot perform complex knowledge computation, limiting their abilities to
produce insightful analytical results. Motivated by these, in this paper, we
propose a \textbf{K}nowledgeable \textbf{D}eep \textbf{R}esearch (\textbf{KDR})
framework that empowers deep research with deep knowledge analysis capability.
Specifically, it introduces an independent knowledge organization phase to
preprocess large-scale, domain-relevant data into systematic knowledge offline.
Based on this knowledge, it extends deep research with an additional kind of
reasoning steps that perform complex knowledge computation in an online manner.
To enhance the abilities of LLMs to solve knowledge analysis tasks in the above
framework, we further introduce \textbf{\KCII}, an LLM that bridges knowledge
organization and reasoning via unified code generation. For knowledge
organization, it generates instantiation code for predefined classes,
transforming data into knowledge objects. For knowledge computation, it
generates analysis code and executes on the above knowledge objects to obtain
deep analysis results. Experimental results on more than thirty datasets across
six knowledge analysis tasks demonstrate the effectiveness of \KCII. Moreover,
when integrated into the KDR framework, \KCII can generate high-quality reports
with insightful analytical results compared to the mainstream deep research
framework.

</details>


### [28] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: The paper proposes a meta-learning approach with soft prompts distilled from task-relevant image features to improve in-context learning (ICL) performance of Large Multimodal Models (LMMs), especially for smaller models. This approach includes an attention-mapper module integrated into the LLaVA v1.5 architecture, enhancing few-shot capabilities and outperforming ICL methods in evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the inconsistent performance of in-context learning (ICL) in LMMs, particularly in smaller models, which doesn't always improve with more examples. The authors hypothesize that this inconsistency stems from LMMs being overwhelmed by unnecessary information in image embeddings for the downstream task.

Method: A meta-learning approach is proposed where soft prompts are distilled from task-relevant image features. These soft prompts can be adapted at test time using a few examples. An attention-mapper module is introduced to facilitate this distillation process, which is integrated with the LLaVA v1.5 architecture and jointly learned with the soft prompts.

Result: Evaluation on the VL-ICL Bench demonstrates that the proposed method consistently surpasses ICL and related prompt-tuning approaches, even under conditions of image perturbations. It enhances task induction and reasoning across visual question answering tasks.

Conclusion: The meta-learning approach with soft prompts and the attention-mapper module successfully improves the few-shot capabilities of LMMs, providing a better alternative to traditional ICL methods, especially in low-data regimes.

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [29] [Causal Graph based Event Reasoning using Semantic Relation Experts](https://arxiv.org/abs/2506.06910)
*Mahnaz Koupaee,Xueying Bai,Mudan Chen,Greg Durrett,Nathanael Chambers,Niranjan Balasubramanian*

Main category: cs.AI

TL;DR: This paper explores the use of causal event graphs to improve LLMs' ability to reason about events, proposing a collaborative approach for generating these graphs and demonstrating their utility in downstream applications.


<details>
  <summary>Details</summary>
Motivation: Event reasoning is crucial but remains challenging for LLMs, particularly in identifying causal connections between events, which affects performance on deeper reasoning tasks.

Method: The authors propose a collaborative approach to generate causal graphs using LLMs as 'experts' focusing on specific semantic relations, engaging in multiple rounds of discussions consolidated by a final expert. These graphs are then used in downstream applications including an explainable event prediction task.

Result: Causal graphs generated through this method proved more informative and coherent than baseline generations. The approach achieved competitive results with state-of-the-art models on forecasting and next event prediction tasks without fine-tuning.

Conclusion: The generation of causal event graphs via a collaborative approach can significantly enhance LLMs' reasoning capabilities and provide more informative explanations.

Abstract: Understanding how events in a scenario causally connect with each other is
important for effectively modeling and reasoning about events. But event
reasoning remains a difficult challenge, and despite recent advances, Large
Language Models (LLMs) still struggle to accurately identify causal connections
between events. This struggle leads to poor performance on deeper reasoning
tasks like event forecasting and timeline understanding. To address this
challenge, we investigate the generation of causal event graphs (e.g., A
enables B) as a parallel mechanism to help LLMs explicitly represent causality
during inference. This paper evaluates both how to generate correct graphs as
well as how graphs can assist reasoning. We propose a collaborative approach to
causal graph generation where we use LLMs to simulate experts that focus on
specific semantic relations. The experts engage in multiple rounds of
discussions which are then consolidated by a final expert. Then, to demonstrate
the utility of causal graphs, we use them on multiple downstream applications,
and also introduce a new explainable event prediction task that requires a
causal chain of events in the explanation. These explanations are more
informative and coherent than baseline generations. Finally, our overall
approach not finetuned on any downstream task, achieves competitive results
with state-of-the-art models on both forecasting and next event prediction
tasks.

</details>


### [30] [Boosting LLM Reasoning via Spontaneous Self-Correction](https://arxiv.org/abs/2506.06923)
*Xutong Zhao,Tengyu Xu,Xuewei Wang,Zhengxing Chen,Di Jin,Liang Tan,Yen-Ting,Zishun Yu,Zhuokai Zhao,Yun He,Sinong Wang,Han Fang,Sarath Chandar,Chen Zhu*

Main category: cs.AI

TL;DR: SPOC is a spontaneous self-correction approach for LLMs that enables interleaved solutions and verifications in a single inference pass, improving math reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Math reasoning remains challenging for large language models (LLMs) despite their success in other tasks. Existing self-correction methods treat corrections as standalone refinements rather than real-time, spontaneous corrections.

Method: Propose SPOC, which assigns dual roles of solution proposer and verifier to the same model. It generates interleaved solutions and verifications in one pass, dynamically terminating based on verification outcomes. Uses synthetic data for fine-tuning and improves through online reinforcement learning.

Result: Experiments show significant performance improvements on mathematical reasoning benchmarks. For instance, accuracy gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24 for Llama-3.1-8B and 70B Instruct models respectively.

Conclusion: SPOC effectively enhances the math reasoning capabilities of LLMs by enabling real-time self-corrections.

Abstract: While large language models (LLMs) have demonstrated remarkable success on a
broad range of tasks, math reasoning remains a challenging one. One of the
approaches for improving math reasoning is self-correction, which designs
self-improving loops to let the model correct its own mistakes. However,
existing self-correction approaches treat corrections as standalone
post-generation refinements, relying on extra prompt and system designs to
elicit self-corrections, instead of performing real-time, spontaneous
self-corrections in a single pass. To address this, we propose SPOC, a
spontaneous self-correction approach that enables LLMs to generate interleaved
solutions and verifications in a single inference pass, with generation
dynamically terminated based on verification outcomes, thereby effectively
scaling inference time compute. SPOC considers a multi-agent perspective by
assigning dual roles -- solution proposer and verifier -- to the same model. We
adopt a simple yet effective approach to generate synthetic data for
fine-tuning, enabling the model to develop capabilities for self-verification
and multi-agent collaboration. We further improve its solution proposal and
verification accuracy through online reinforcement learning. Experiments on
mathematical reasoning benchmarks show that SPOC significantly improves
performance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct
models, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,
and 3.3% and 6.7% on AIME24, respectively.

</details>


### [31] [An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design](https://arxiv.org/abs/2506.06935)
*Darui Lu,Jordan M. Malof,Willie J. Padilla*

Main category: cs.AI

TL;DR: A Agentic Framework using multiple LLMs is developed for inverse design of photonic metamaterials, showing effectiveness in automation, reasoning, planning, and adaptation.


<details>
  <summary>Details</summary>
Motivation: To leverage the capabilities of integrated LLM systems to create an autonomous framework for performing complex tasks, specifically the inverse design of photonic metamaterials.

Method: Developed an Agentic Framework that, when given a desired optical spectrum, autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for simulation and optimization, uses memory, and generates a final design through a deep inverse method.

Result: The framework demonstrated its ability to automate, reason, plan, and adapt, with internal reflection and decision flexibility leading to varied and potentially novel outputs.

Conclusion: The Agentic Framework shows promise in autonomously handling complex tasks like the inverse design of photonic metamaterials.

Abstract: Recent significant advances in integrating multiple Large Language Model
(LLM) systems have enabled Agentic Frameworks capable of performing complex
tasks autonomously, including novel scientific research. We develop and
demonstrate such a framework specifically for the inverse design of photonic
metamaterials. When queried with a desired optical spectrum, the Agent
autonomously proposes and develops a forward deep learning model, accesses
external tools via APIs for tasks like simulation and optimization, utilizes
memory, and generates a final design via a deep inverse method. The framework's
effectiveness is demonstrated in its ability to automate, reason, plan, and
adapt. Notably, the Agentic Framework possesses internal reflection and
decision flexibility, permitting highly varied and potentially novel outputs.

</details>


### [32] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂性超过一定阈值时会出现准确性的完全崩溃，且其推理努力随问题复杂性增加到某一点后下降。研究通过可控谜题环境揭示了LRMs的三个性能阶段及推理痕迹。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在推理基准测试中表现出色，但对其基本能力、扩展特性和局限性理解不足。当前评估主要集中在数学和编码基准上，强调最终答案的准确性，而忽略了推理痕迹。

Method: 利用可控谜题环境进行系统研究，允许精确操纵复杂性并保持一致的逻辑结构，分析最终答案和内部推理痕迹。通过广泛的实验，比较LRMs与标准LLM在相同推理计算下的表现。

Result: 发现LRMs在三种任务复杂度下的表现：低复杂度任务中标准模型优于LRMs；中等复杂度任务中LRMs占优；高复杂度任务中两者均完全崩溃。此外，LRMs在精确计算方面存在局限性，无法使用明确算法且推理不一致。

Conclusion: LRMs在处理中等复杂度任务时具有优势，但在高复杂度任务中受限于推理能力和计算行为。研究揭示了LRMs的优势、局限性，并提出了关于其推理能力的问题。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [33] [Deontically Constrained Policy Improvement in Reinforcement Learning Agents](https://arxiv.org/abs/2506.06959)
*Alena Makarova,Houssam Abbas*

Main category: cs.AI

TL;DR: This paper explores how to learn a decision policy in MDPs that maximizes utility while satisfying constraints expressed in deontic logic.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning a decision policy that not only maximizes utility but also adheres to ethical, social, or situational constraints.

Method: The authors use the logic of Expected Act Utilitarianism and develop a variation on policy improvement to achieve this.

Result: They show that their method reaches a constrained local maximum of the mission utility and can be interpreted as maximizing two value functions in a bi-level structure.

Conclusion: The results are illustrated with experiments on sample MDPs.

Abstract: Markov Decision Processes (MDPs) are the most common model for decision
making under uncertainty in the Machine Learning community. An MDP captures
non-determinism, probabilistic uncertainty, and an explicit model of action. A
Reinforcement Learning (RL) agent learns to act in an MDP by maximizing a
utility function. This paper considers the problem of learning a decision
policy that maximizes utility subject to satisfying a constraint expressed in
deontic logic. In this setup, the utility captures the agent's mission - such
as going quickly from A to B. The deontic formula represents (ethical, social,
situational) constraints on how the agent might achieve its mission by
prohibiting classes of behaviors. We use the logic of Expected Act
Utilitarianism, a probabilistic stit logic that can be interpreted over
controlled MDPs. We develop a variation on policy improvement, and show that it
reaches a constrained local maximum of the mission utility. Given that in stit
logic, an agent's duty is derived from value maximization, this can be seen as
a way of acting to simultaneously maximize two value functions, one of which is
implicit, in a bi-level structure. We illustrate these results with experiments
on sample MDPs.

</details>


### [34] [Long-Tailed Learning for Generalized Category Discovery](https://arxiv.org/abs/2506.06965)
*Cuong Manh Hoang*

Main category: cs.AI

TL;DR: This paper addresses the issue of Generalized Category Discovery (GCD) in imbalanced, long-tailed datasets. It proposes a novel framework including self-guided labeling and representation balancing process to improve model performance on discovering novel classes.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods work well on balanced artificial datasets but struggle with real-world imbalanced datasets which affect their effectiveness.

Method: The paper introduces a framework that uses self-guided labeling with learnable distribution for pseudo-labels and a representation balancing process that mines sample neighborhoods to focus on tail classes.

Result: Experiments on public datasets demonstrate the effectiveness of the proposed framework, showing better performance than previous state-of-the-art methods.

Conclusion: The proposed framework successfully performs generalized category discovery in long-tailed distributions, improving upon existing methods.

Abstract: Generalized Category Discovery (GCD) utilizes labeled samples of known
classes to discover novel classes in unlabeled samples. Existing methods show
effective performance on artificial datasets with balanced distributions.
However, real-world datasets are always imbalanced, significantly affecting the
effectiveness of these methods. To solve this problem, we propose a novel
framework that performs generalized category discovery in long-tailed
distributions. We first present a self-guided labeling technique that uses a
learnable distribution to generate pseudo-labels, resulting in less biased
classifiers. We then introduce a representation balancing process to derive
discriminative representations. By mining sample neighborhoods, this process
encourages the model to focus more on tail classes. We conduct experiments on
public datasets to demonstrate the effectiveness of the proposed framework. The
results show that our model exceeds previous state-of-the-art methods.

</details>


### [35] [Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments](https://arxiv.org/abs/2506.06981)
*Riley Simmons-Edler,Ryan P. Badman,Felix Baastad Berg,Raymond Chua,John J. Vastola,Joshua Lunger,William Qian,Kanaka Rajan*

Main category: cs.AI

TL;DR: 通过结合神经科学和生态学的工具研究深度强化学习（DRL）代理在复杂环境中的行为，发现无模型RNN基础的DRL代理可以通过涌现动力学展示结构化的规划行为。提出了一般分析框架，将核心行为和表征特征与诊断方法联系起来，为理解和确保复杂自主代理的行为提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 当前深度强化学习中标准的行为分析方法尚未充分发展，简单的奖励曲线比较不足以理解代理的行为，特别是在任务和代理复杂性增加的情况下。

Method: 设计了一个新的复杂环境ForageWorld，并应用神经科学和生态学的工具对DRL代理进行联合行为和神经分析，揭示代理策略、记忆和规划的详细见解。

Result: 发现无模型RNN基础的DRL代理可以通过涌现动力学展示结构化的规划行为，而不需要明确的记忆模块或世界模型。通过神经生态学启发的工具分析，揭示了代理学习动力学中的丰富结构。

Conclusion: 研究提出了一个通用分析框架，连接行为和表征特征与诊断方法，强调了结合神经科学、认知科学和AI的重要性，以确保复杂自主代理的安全对齐和最大化期望行为。

Abstract: Understanding the behavior of deep reinforcement learning (DRL) agents --
particularly as task and agent sophistication increase -- requires more than
simple comparison of reward curves, yet standard methods for behavioral
analysis remain underdeveloped in DRL. We apply tools from neuroscience and
ethology to study DRL agents in a novel, complex, partially observable
environment, ForageWorld, designed to capture key aspects of real-world animal
foraging -- including sparse, depleting resource patches, predator threats, and
spatially extended arenas. We use this environment as a platform for applying
joint behavioral and neural analysis to agents, revealing detailed,
quantitatively grounded insights into agent strategies, memory, and planning.
Contrary to common assumptions, we find that model-free RNN-based DRL agents
can exhibit structured, planning-like behavior purely through emergent dynamics
-- without requiring explicit memory modules or world models. Our results show
that studying DRL agents like animals -- analyzing them with
neuroethology-inspired tools that reveal structure in both behavior and neural
dynamics -- uncovers rich structure in their learning dynamics that would
otherwise remain invisible. We distill these tools into a general analysis
framework linking core behavioral and representational features to diagnostic
methods, which can be reused for a wide range of tasks and agents. As agents
grow more complex and autonomous, bridging neuroscience, cognitive science, and
AI will be essential -- not just for understanding their behavior, but for
ensuring safe alignment and maximizing desirable behaviors that are hard to
measure via reward. We show how this can be done by drawing on lessons from how
biological intelligence is studied.

</details>


### [36] [Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth](https://arxiv.org/abs/2506.06991)
*Yichi Zhang,Jinlong Pang,Zhaowei Zhu,Yang Liu*

Main category: cs.AI

TL;DR: 本研究探讨了在众包标注任务中，利用同伴预测机制来缓解大型语言模型辅助作弊的问题，并提出了一种无需训练的评分机制。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的成功，高质量的人类反馈对于构建值得信赖的AI系统至关重要。然而，众包工作者越来越多地使用大型语言模型，可能导致反映人类输入的数据集被模型生成的响应所破坏。现有的LLM检测方法通常依赖于高维训练数据，如文本，这使得它们不适合用于多选标注等任务。

Method: 研究调查了同伴预测（peer prediction）的潜力，这是一种无需使用真实答案即可评估工作者响应中信息的机制。研究提出了一种无需训练的评分机制，该机制在考虑了请求者可获得的部分LLM生成标签的情况下，量化了工作者答案之间的相关性。

Result: 研究建立了该方法有效性的条件，并通过实证研究展示了其在检测现实世界众包数据集中低努力度作弊行为方面的稳健性。

Conclusion: 同伴预测机制可以在不使用真实答案的情况下，有效识别众包任务中的LLM辅助作弊行为，为提高众包数据质量提供了新的解决方案。

Abstract: The recent success of generative AI highlights the crucial role of
high-quality human feedback in building trustworthy AI systems. However, the
increasing use of large language models (LLMs) by crowdsourcing workers poses a
significant challenge: datasets intended to reflect human input may be
compromised by LLM-generated responses. Existing LLM detection approaches often
rely on high-dimension training data such as text, making them unsuitable for
annotation tasks like multiple-choice labeling. In this work, we investigate
the potential of peer prediction -- a mechanism that evaluates the information
within workers' responses without using ground truth -- to mitigate
LLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our
approach quantifies the correlations between worker answers while conditioning
on (a subset of) LLM-generated labels available to the requester. Building on
prior research, we propose a training-free scoring mechanism with theoretical
guarantees under a crowdsourcing model that accounts for LLM collusion. We
establish conditions under which our method is effective and empirically
demonstrate its robustness in detecting low-effort cheating on real-world
crowdsourcing datasets.

</details>


### [37] [Mathesis: Towards Formal Theorem Proving from Natural Languages](https://arxiv.org/abs/2506.07047)
*Yu Xuejun,Jianyuan Zhong,Zijin Feng,Pengyi Zhai,Roozbeh Yousefzadeh,Wei Chong Ng,Haoxiong Liu,Ziyi Shou,Jing Xiong,Yudong Zhou,Claudia Beth Ong,Austen Jeremy Sugiarto,Yaoxi Zhang,Wai Ming Tai,Huan Cao,Dongcai Lu,Jiacheng Sun,Qiang Xu,Shen Xin,Zhenguo Li*

Main category: cs.AI

TL;DR: Recent advances in large language models have shown great potential for formal reasoning. However, most LLM-based theorem provers require expert-written formal statements as inputs, which limits their applicability to real-world problems expressed in natural language. This paper introduces Mathesis, the first end-to-end theorem proving pipeline that processes informal problem statements. It includes Mathesis-Autoformalizer, which uses reinforcement learning to enhance the formalization ability of natural language problems, and Mathesis-Prover, which generates formal proofs from the formalized statements. Experiments show that Mathesis outperforms other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to bridge the gap between formal theorem proving and real-world problems expressed in natural language by developing an end-to-end theorem proving pipeline that can process informal problem statements.

Method: The method involves creating Mathesis, which consists of two main components: Mathesis-Autoformalizer and Mathesis-Prover. The autoformalizer uses reinforcement learning and the LeanScorer framework to convert natural language problems into formal statements. The prover then generates formal proofs from these statements. A new benchmark, Gaokao-Formal, is also introduced to evaluate the system's performance.

Result: Experiments demonstrate that Mathesis's autoformalizer surpasses the best baseline by 22% in pass-rate on Gaokao-Formal. The full system achieves 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.

Conclusion: Mathesis represents a significant advancement in end-to-end formal theorem proving, effectively processing informal problem statements and achieving state-of-the-art results on relevant benchmarks.

Abstract: Recent advances in large language models show strong promise for formal
reasoning. However, most LLM-based theorem provers have long been constrained
by the need for expert-written formal statements as inputs, limiting their
applicability to real-world problems expressed in natural language. We tackle
this gap with Mathesis, the first end-to-end theorem proving pipeline
processing informal problem statements. It contributes Mathesis-Autoformalizer,
the first autoformalizer using reinforcement learning to enhance the
formalization ability of natural language problems, aided by our novel
LeanScorer framework for nuanced formalization quality assessment. It also
proposes a Mathesis-Prover, which generates formal proofs from the formalized
statements. To evaluate the real-world applicability of end-to-end formal
theorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex
problems from China's national college entrance exam. Our approach is carefully
designed, with a thorough study of each component. Experiments demonstrate
Mathesis's effectiveness, with the autoformalizer outperforming the best
baseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other
model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a
state-of-the-art 18% on Gaokao-Formal.

</details>


### [38] [Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression](https://arxiv.org/abs/2506.07075)
*Liwen Zheng,Chaozhuo Li,Haoran Jia,Xi Zhang*

Main category: cs.AI

TL;DR: A structural reasoning framework for multi-hop fact verification is proposed, which models reasoning paths as structured graphs and comprises two key modules: structure-enhanced retrieval mechanism and reasoning-path-guided verification module. Experiments show it outperforms strong baselines.


<details>
  <summary>Details</summary>
Motivation: Existing automated fact verification systems have challenges in accurately aggregating and reasoning over multi-hop evidence due to reliance on static or shallow models that fail to capture evolving structure of reasoning paths.

Method: The method explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. It includes a structure-enhanced retrieval mechanism and a reasoning-path-guided verification module, with a structure-aware reasoning mechanism incorporated to capture long-range dependencies across multi-hop evidence chains.

Result: Extensive experiments on the FEVER and HoVer datasets demonstrate that the approach consistently outperforms strong baselines, enhancing retrieval precision and verification accuracy.

Conclusion: The structural reasoning framework effectively models reasoning paths as structured graphs, leading to more precise verification through improved retrieval and reasoning mechanisms.

Abstract: The growing complexity of factual claims in real-world scenarios presents
significant challenges for automated fact verification systems, particularly in
accurately aggregating and reasoning over multi-hop evidence. Existing
approaches often rely on static or shallow models that fail to capture the
evolving structure of reasoning paths, leading to fragmented retrieval and
limited interpretability. To address these issues, we propose a Structural
Reasoning framework for Multi-hop Fact Verification that explicitly models
reasoning paths as structured graphs throughout both evidence retrieval and
claim verification stages. Our method comprises two key modules: a
structure-enhanced retrieval mechanism that constructs reasoning graphs to
guide evidence collection, and a reasoning-path-guided verification module that
incrementally builds subgraphs to represent evolving inference trajectories. We
further incorporate a structure-aware reasoning mechanism that captures
long-range dependencies across multi-hop evidence chains, enabling more precise
verification. Extensive experiments on the FEVER and HoVer datasets demonstrate
that our approach consistently outperforms strong baselines, highlighting the
effectiveness of reasoning-path modeling in enhancing retrieval precision and
verification accuracy.

</details>


### [39] [BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite](https://arxiv.org/abs/2506.07116)
*Liyang Chen,Yujun Cai,Jieqiong Dong,Yiwei Wang*

Main category: cs.AI

TL;DR: MARCUS is a pipeline using LLMs to clean and re-chunk BRIGHT into BRIGHT-Plus, enhancing retrieval accuracy and multi-hop reasoning.


<details>
  <summary>Details</summary>
Motivation: To improve the practical effectiveness of BRIGHT by addressing content redundancy and semantic discontinuity issues that limit retrieval accuracy and downstream reasoning.

Method: MARCUS leverages LLMs through dedicated agents for structural noise removal and semantic segmentation to create BRIGHT-Plus, a higher-quality corpus.

Result: BRIGHT-Plus shows consistent and significant improvements in retrieval accuracy and multi-hop reasoning across various retrievers.

Conclusion: The authors release BRIGHT-Plus and MARCUS to support future research on robust, reasoning-centric retrieval.

Abstract: Retrieval-Augmented Generation (RAG) systems require corpora that are both
structurally clean and semantically coherent. BRIGHT is a recent and
influential benchmark designed to evaluate complex multi-hop retrieval across
diverse, high-reasoning domains. However, its practical effectiveness is
limited by common web-crawled artifacts - such as content redundancy and
semantic discontinuity - that impair retrieval accuracy and downstream
reasoning. Notably, we find that such issues are concentrated in seven
StackExchange-derived subdomains, while other domains (e.g., Coding and
Theorem-based content) remain relatively clean.
  In this study, we present MARCUS, a multi-agent pipeline that leverages large
language models (LLMs) to systematically clean and re-chunk BRIGHT into a
higher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for
structural noise removal and semantic segmentation, preserving answer-bearing
spans while improving contextual integrity. Experimental evaluations
demonstrate that BRIGHT-Plus yields consistent and significant improvements in
both retrieval accuracy and multi-hop reasoning across a diverse set of
retrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to
support future research on robust, reasoning-centric retrieval.

</details>


### [40] [Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT](https://arxiv.org/abs/2506.07173)
*Miroslav Popovic,Marko Popovic,Miodrag Djukic,Ilija Basicevic*

Main category: cs.AI

TL;DR: This paper introduces a simple translation process using ChatGPT to automate the translation of federated learning algorithms in Python into CSP processes, estimating the minimality of the used context based on feedback from ChatGPT. The proposed translation process was experimentally validated.


<details>
  <summary>Details</summary>
Motivation: To automate the translation of federated learning algorithms in Python into CSP processes and estimate the minimality of the used context based on feedback from ChatGPT.

Method: Using ChatGPT to automate the translation process and experimentally validate the proposed translation process.

Result: Successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.

Conclusion: The proposed translation process using ChatGPT for automating the translation of federated learning algorithms in Python into CSP processes is effective.

Abstract: The Python Testbed for Federated Learning Algorithms is a simple Python FL
framework that is easy to use by ML&AI developers who do not need to be
professional programmers and is also amenable to LLMs. In the previous
research, generic federated learning algorithms provided by this framework were
manually translated into the CSP processes and algorithms' safety and liveness
properties were automatically verified by the model checker PAT. In this paper,
a simple translation process is introduced wherein the ChatGPT is used to
automate the translation of the mentioned federated learning algorithms in
Python into the corresponding CSP processes. Within the process, the minimality
of the used context is estimated based on the feedback from ChatGPT. The
proposed translation process was experimentally validated by successful
translation (verified by the model checker PAT) of both generic centralized and
decentralized federated learning algorithms.

</details>


### [41] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: Multimodal large language models have hallucination problems that affect their reliability. This paper focuses on behavioral hallucinations in sequential images, identifies causes, proposes the SHE framework to detect and mitigate these issues, introduces a new metric BEACH, and shows effectiveness through benchmarks.


<details>
  <summary>Details</summary>
Motivation: Behavioral hallucinations in sequential images are under-studied compared to objective hallucinations, posing challenges for multimodal large language models.

Method: SHE is a two-stage framework: (1) Detect hallucinations using visual-textual alignment check with an adaptive temporal window; (2) Mitigate hallucinations via orthogonal projection onto the joint embedding space. A new metric BEACH is also proposed.

Result: SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy according to empirical results on standard benchmarks.

Conclusion: The SHE framework effectively addresses behavioral hallucinations in sequential images and the BEACH metric provides a way to quantify this type of hallucination.

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [42] [Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues](https://arxiv.org/abs/2506.07194)
*Luwei Bai,Dongkeun Han,Sara Hennessy*

Main category: cs.AI

TL;DR: This study explores strategies to customize a GPT agent for coding classroom dialogue using MyGPT as a case, finding that with specific strategies it can be a useful coding assistant.


<details>
  <summary>Details</summary>
Motivation: Classroom dialogue analysis is challenging due to the complex dialogic functions and manual labor required. Existing studies on large language models either focus on large-scale training or evaluating pre-trained models with fixed codebooks which are not suitable for small datasets or custom coding schemes.

Method: Evaluate MyGPT's baseline performance in coding classroom dialogue with a human codebook and examine how different example inputs affect performance through variable control. A design-based research approach identifies practical strategies based on MyGPT's features for configuring effective agents with limited data.

Result: The study found that a MyGPT agent developed with these strategies can generate coding suggestions despite some limitations.

Conclusion: A customized MyGPT agent can serve as a useful coding assistant for classroom dialogue by generating coding suggestions.

Abstract: This study investigates effective strategies for developing a customised GPT
agent to code classroom dialogue. While classroom dialogue is widely recognised
as a crucial element of education, its analysis remains challenging due to the
need for a nuanced understanding of dialogic functions and the labour-intensive
nature of manual transcript coding. Recent advancements in large language
models offer promising avenues for automating this process. However, existing
studies predominantly focus on training large-scale models or evaluating
pre-trained models with fixed codebooks, which are often not applicable or
replicable for dialogue researchers working with small datasets or customised
coding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its
baseline performance in coding classroom dialogue with a human codebook and
examines how performance varies with different example inputs through a
variable control method. Through a design-based research approach, it
identifies a set of practical strategies, based on MyGPT's unique features, for
configuring effective agents with limited data. The findings suggest that,
despite some limitations, a MyGPT agent developed with these strategies can
serve as a useful coding assistant by generating coding suggestions.

</details>


### [43] [Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation](https://arxiv.org/abs/2506.07202)
*Ming Liu,Wensheng Zhang*

Main category: cs.AI

TL;DR: Multimodal Large Language Models (MLLMs) show great performance but risk data contamination. This paper proposes a dynamic evaluation framework to assess MLLM generalization by perturbing tasks rather than inputs, revealing whether model performance is robust or reliant on task-specific cues.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the generalization capabilities of MLLMs and distinguish genuine understanding from spurious leakage or overfitting.

Method: Propose a novel dynamic evaluation framework that perturbs tasks instead of inputs. Evaluate models across a family of tasks using the same visual input to probe diverse capabilities.

Result: Fine-tuning on simulated test data harms overall generalization, while models with generalizable solutions perform better under task shifts.

Conclusion: Dynamic task perturbation offers deeper insights into MLLM generalization.

Abstract: Multimodal Large Language Models (MLLMs) show impressive vision-language
benchmark performance, yet growing concerns about data contamination (test set
exposure during training) risk masking true generalization. This concern
extends to reasoning MLLMs, often fine-tuned via reinforcement learning from
potentially contaminated base models. We propose a novel dynamic evaluation
framework to rigorously assess MLLM generalization, moving beyond static
benchmarks. Instead of perturbing inputs, we perturb the task itself. Using the
same visual input, models are evaluated across a family of tasks (e.g., QA,
captioning, question posing, verification) to probe diverse capabilities. This
task perturbation reveals whether model performance is robust or reliant on
superficial task-specific cues. Our approach is analogous to loss landscape
sharpness: models overfit or contaminated for a single task (sharp minima)
falter under task shifts, unlike models with generalizable solutions (flatter
minima). We developed an automated pipeline with a calibrated judge scoring
open-ended generations (captions, questions) using paraphrase and corruption
sampling. Applying this framework to leading image/video MLLMs on benchmarks
including MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task
"ability vector." We demonstrate that fine-tuning on simulated test data
(extreme contamination) drastically sharpens task-specific performance but
harms overall generalization. Our dynamic task perturbation offers deeper
insights into MLLM generalization, distinguishing genuine understanding from
spurious leakage or overfitting.

</details>


### [44] [BIMgent: Towards Autonomous Building Modeling via Computer-use Agents](https://arxiv.org/abs/2506.07217)
*Zihan Deng,Changyu Du,Stavros Nousias,André Borrmann*

Main category: cs.AI

TL;DR: The paper introduces BIMgent, an agentic framework powered by multimodal LLMs for automating 3D building modeling in AEC sector. Evaluated on real-world tasks, it achieves a 32% success rate and reasonable design quality.


<details>
  <summary>Details</summary>
Motivation: Current computer-use agents are mainly focused on general-purpose desktop automation tasks and have not sufficiently explored specialized domains like 3D building modeling in the AEC sector.

Method: BIMgent is an agentic framework that uses multimodal large language models to enable autonomous building model authoring via GUI operations. It includes multimodal input for conceptual design, workflow planning, and efficient execution of GUI actions.

Result: BIMgent achieved a 32% success rate on real-world building modeling tasks, outperforming baseline models which had a 0% success rate. The design quality was found to be reasonable.

Conclusion: BIMgent effectively reduces manual workload while preserving design intent, showing potential for practical deployment in real-world architectural modeling scenarios.

Abstract: Existing computer-use agents primarily focus on general-purpose desktop
automation tasks, with limited exploration of their application in highly
specialized domains. In particular, the 3D building modeling process in the
Architecture, Engineering, and Construction (AEC) sector involves open-ended
design tasks and complex interaction patterns within Building Information
Modeling (BIM) authoring software, which has yet to be thoroughly addressed by
current studies. In this paper, we propose BIMgent, an agentic framework
powered by multimodal large language models (LLMs), designed to enable
autonomous building model authoring via graphical user interface (GUI)
operations. BIMgent automates the architectural building modeling process,
including multimodal input for conceptual design, planning of software-specific
workflows, and efficient execution of the authoring GUI actions. We evaluate
BIMgent on real-world building modeling tasks, including both text-based
conceptual design generation and reconstruction from existing building design.
The design quality achieved by BIMgent was found to be reasonable. Its
operations achieved a 32% success rate, whereas all baseline models failed to
complete the tasks (0% success rate). Results demonstrate that BIMgent
effectively reduces manual workload while preserving design intent,
highlighting its potential for practical deployment in real-world architectural
modeling scenarios.

</details>


### [45] [LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments](https://arxiv.org/abs/2506.07223)
*Yangqing Zheng,Shunqi Mao,Dingxin Zhang,Weidong Cai*

Main category: cs.AI

TL;DR: This paper proposes a Time Conversion Mechanism (TCM) and a new agent called RRARA to improve decision-making in high-risk scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the insufficiently studied issue of decision-making delays in dynamically changing high-risk scenarios.

Method: Propose TCM to translate inference delays into simulation frames, extend HAZARD with RL and LAR, and present RRARA which combines an LLM-guided feedback module with a rule-based agent.

Result: Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.

Conclusion: The proposed mechanisms and agent improve decision-making performance in high-risk scenarios.

Abstract: In the realm of embodied intelligence, the evolution of large language models
(LLMs) has markedly enhanced agent decision making. Consequently, researchers
have begun exploring agent performance in dynamically changing high-risk
scenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under
these extreme conditions, the delay in decision making emerges as a crucial yet
insufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that
translates inference delays in decision-making into equivalent simulation
frames, thus aligning cognitive and physical costs under a single FPS-based
metric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action
Ratio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we
present the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a
lightweight LLM-guided feedback module with a rule-based agent to enable
immediate reactive behaviors and asynchronous reflective refinements in situ.
Experiments on HAZARD show that RRARA substantially outperforms existing
baselines in latency-sensitive scenarios.

</details>


### [46] [Subgoal-Guided Policy Heuristic Search with Learned Subgoals](https://arxiv.org/abs/2506.07255)
*Jake Tuero,Michael Buro,Levi H. S. Lelis*

Main category: cs.AI

TL;DR: This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms, which improves the sample efficiency of learning a policy and heuristic function.


<details>
  <summary>Details</summary>
Motivation: The process of training policy tree search requires complete solution trajectories to train the policy, but when the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. This leads to search samples being wasted in failed attempts to solve these hard instances.

Method: A novel method for learning subgoal-based policies for policy tree search algorithms is introduced. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts.

Result: Empirical results show that this new policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in an online setting.

Conclusion: Learning subgoal-based policies for policy tree search algorithms using the proposed method enhances the sample efficiency.

Abstract: Policy tree search is a family of tree search algorithms that use a policy to
guide the search. These algorithms provide guarantees on the number of
expansions required to solve a given problem that are based on the quality of
the policy. While these algorithms have shown promising results, the process in
which they are trained requires complete solution trajectories to train the
policy. Search trajectories are obtained during a trial-and-error search
process. When the training problem instances are hard, learning can be
prohibitively costly, especially when starting from a randomly initialized
policy. As a result, search samples are wasted in failed attempts to solve
these hard instances. This paper introduces a novel method for learning
subgoal-based policies for policy tree search algorithms. The subgoals and
policies conditioned on subgoals are learned from the trees that the search
expands while attempting to solve problems, including the search trees of
failed attempts. We empirically show that our policy formulation and training
method improve the sample efficiency of learning a policy and heuristic
function in this online setting.

</details>


### [47] [Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data](https://arxiv.org/abs/2506.07390)
*Xin-Cheng Wen,Yijun Yang,Cuiyun Gao,Yang Xiao,Deheng Ye*

Main category: cs.AI

TL;DR: ReVD is a new framework that improves LLMs' ability in software vulnerability detection by synthesizing reasoning data and optimizing preference, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Existing large language models have limitations in detecting software vulnerabilities due to the lack of reasoning data related to vulnerabilities and their focus on semantic representations rather than the underlying reasons. Additionally, there's a challenge in developing specialized LLMs for vulnerability detection due to the scarcity of high-quality datasets.

Method: The framework ReVD constructs forward and backward reasoning processes for vulnerabilities and fixed code to synthesize high-quality reasoning data. It also uses triplet supervised fine-tuning followed by curriculum online preference optimization to enhance understanding of vulnerability patterns.

Result: Experiments on PrimeVul and SVEN datasets show that ReVD achieves state-of-the-art performance in LLM-based software vulnerability detection with accuracy improvements ranging from 12.24% to 22.77%.

Conclusion: ReVD demonstrates excellent capability in mining vulnerability patterns through reasoning data synthesizing and preference optimization, setting a new benchmark in LLM-based software vulnerability detection.

Abstract: Large language models (LLMs) demonstrate considerable proficiency in numerous
coding-related tasks; however, their capabilities in detecting software
vulnerabilities remain limited. This limitation primarily stems from two
factors: (1) the absence of reasoning data related to vulnerabilities, which
hinders the models' ability to capture underlying vulnerability patterns; and
(2) their focus on learning semantic representations rather than the reason
behind them, thus failing to recognize semantically similar vulnerability
samples. Furthermore, the development of LLMs specialized in vulnerability
detection is challenging, particularly in environments characterized by the
scarcity of high-quality datasets. In this paper, we propose a novel framework
ReVD that excels at mining vulnerability patterns through reasoning data
synthesizing and vulnerability-specific preference optimization. Specifically,
we construct forward and backward reasoning processes for vulnerability and
corresponding fixed code, ensuring the synthesis of high-quality reasoning
data. Moreover, we design the triplet supervised fine-tuning followed by
curriculum online preference optimization for enabling ReVD to better
understand vulnerability patterns. The extensive experiments conducted on
PrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for
LLM-based software vulnerability detection, e.g., 12.24\%-22.77\% improvement
in the accuracy. The source code and data are available at
https://github.com/Xin-Cheng-Wen/PO4Vul.

</details>


### [48] [An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning](https://arxiv.org/abs/2506.07411)
*Ze Yang,Yihong Jin,Juntian Liu,Xinhe Xu*

Main category: cs.AI

TL;DR: 在云AI系统中，提出了一种结合大语言模型和深度强化学习的智能故障自愈机制（IFSHM），通过语义理解和策略优化能力提高了系统恢复效率。


<details>
  <summary>Details</summary>
Motivation: 随着基于云计算的AI系统规模和复杂性的增加，检测和适应性恢复系统故障成为确保服务可靠性和连续性的核心挑战。

Method: 提出了一种两阶段混合架构：1) 由LLM驱动的故障语义解释模块，可以从多源日志和系统指标中动态提取深层上下文语义以准确识别潜在故障模式；2) 基于强化学习的DRL恢复策略优化器，学习云环境中故障类型与响应行为的动态匹配。同时引入了记忆引导的元控制器，结合强化学习回放和LLM提示微调策略，以实现对新故障模式的持续适应并避免灾难性遗忘。

Result: 实验结果表明，在未知故障场景下，与现有的DRL和规则方法相比，IFSHM框架将系统恢复时间缩短了37%。

Conclusion: IFSHM框架通过引入LLM进行环境建模和动作空间抽象，显著提高了强化学习的探索效率和泛化能力，从而有效缩短系统恢复时间。

Abstract: As the scale and complexity of cloud-based AI systems continue to increase,
the detection and adaptive recovery of system faults have become the core
challenges to ensure service reliability and continuity. In this paper, we
propose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates
Large Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to
realize a fault recovery framework with semantic understanding and policy
optimization capabilities in cloud AI systems. On the basis of the traditional
DRL-based control model, the proposed method constructs a two-stage hybrid
architecture: (1) an LLM-driven fault semantic interpretation module, which can
dynamically extract deep contextual semantics from multi-source logs and system
indicators to accurately identify potential fault modes; (2) DRL recovery
strategy optimizer, based on reinforcement learning, learns the dynamic
matching of fault types and response behaviors in the cloud environment. The
innovation of this method lies in the introduction of LLM for environment
modeling and action space abstraction, which greatly improves the exploration
efficiency and generalization ability of reinforcement learning. At the same
time, a memory-guided meta-controller is introduced, combined with
reinforcement learning playback and LLM prompt fine-tuning strategy, to achieve
continuous adaptation to new failure modes and avoid catastrophic forgetting.
Experimental results on the cloud fault injection platform show that compared
with the existing DRL and rule methods, the IFSHM framework shortens the system
recovery time by 37% with unknown fault scenarios.

</details>


### [49] [Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests](https://arxiv.org/abs/2506.07418)
*Arnau Igualde Sáez,Lamyae Rhomrasi,Yusef Ahsini,Ricardo Vinuesa,Sergio Hoyas,Jose P. García Sabater,Marius J. Fullana i Alfonso,J. Alberto Conejero*

Main category: cs.AI

TL;DR: Multimodal Large Language Models (MLLMs) are evaluated for mathematical problem solving involving diagrams, multilingual text, and symbolic notation. Experiments reveal moderate overall precision across topics, substantial variation across languages and difficulty levels, underutilization of diagrammatic information by some models, and differences in reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of MLLMs in visually presented mathematics and analyze their capabilities in solving mathematical problems involving diagrams, multilingual text, and symbolic notation.

Method: Evaluate several MLLMs, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash, using a multilingual Kangaroo style benchmark covering English, French, Spanish, and Catalan.

Result: Four key findings: moderate overall precision across mathematical topics; limited improvement in accuracy without visual input; substantial variation in performance across languages and difficulty levels; Gemini 2.0 Flash, Qwen VL 2.5 72B, and GPT 4o perform best on image-based tasks but do not reach human-level performance; Gemini and GPT 4o excel in structured reasoning while Pixtral and Llama rely more on heuristics or randomness.

Conclusion: MLLMs show promise but have limitations in mathematical problem solving with visual elements, particularly in fully utilizing diagrammatic information and achieving consistent reasoning across all tasks.

Abstract: Multimodal Large Language Models (MLLMs) promise advanced vision language
capabilities, yet their effectiveness in visually presented mathematics remains
underexplored. This paper analyzes the development and evaluation of MLLMs for
mathematical problem solving, focusing on diagrams, multilingual text, and
symbolic notation. We then assess several models, including GPT 4o, Pixtral,
Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual
Kangaroo style benchmark spanning English, French, Spanish, and Catalan. Our
experiments reveal four key findings. First, overall precision remains moderate
across geometry, visual algebra, logic, patterns, and combinatorics: no single
model excels in every topic. Second, while most models see improved accuracy
with questions that do not have images, the gain is often limited; performance
for some remains nearly unchanged without visual input, indicating
underutilization of diagrammatic information. Third, substantial variation
exists across languages and difficulty levels: models frequently handle easier
items but struggle with advanced geometry and combinatorial reasoning. Notably,
Gemini 2.0 Flash achieves the highest precision on image based tasks, followed
by Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.
Fourth, a complementary analysis aimed at distinguishing whether models reason
or simply recite reveals that Gemini and GPT 4o stand out for their structured
reasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less
consistent reasoning, often defaulting to heuristics or randomness when unable
to align their outputs with the given answer options.

</details>


### [50] [HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model](https://arxiv.org/abs/2506.07428)
*Yuling Wang,Zihui Chen,Pengfei Jiao,Xiao Wang*

Main category: cs.AI

TL;DR: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable and need tailored attacks to assess their robustness. This paper proposes a novel foundation attack model HeTa, which can design generalizable perturbations across different HGNNs and quickly adapt to new heterogeneous graphs.


<details>
  <summary>Details</summary>
Motivation: Existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Despite significant differences in model design and parameter space, different HGNNs share common vulnerability patterns from a relation-aware perspective.

Method: The authors propose HeTa, a relation-wise heterogeneous graph foundation attack model. They introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Then, they implement a serialized relation-by-relation attack based on the identified relational weights.

Result: Extensive experiments show that the proposed method exhibits powerful attack performances and generalizability.

Conclusion: HeTa is an effective foundation attack model for HGNNs, enabling generalizable perturbations across different HGNNs and quick adaptation to new heterogeneous graphs.

Abstract: Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the
need for tailored attacks to assess their robustness and ensure security.
However, existing HGNN attacks often require complex retraining of parameters
to generate specific perturbations for new scenarios. Recently, foundation
models have opened new horizons for the generalization of graph neural networks
by capturing shared semantics across various graph distributions. This leads us
to ask:Can we design a foundation attack model for HGNNs that enables
generalizable perturbations across different HGNNs, and quickly adapts to new
heterogeneous graphs (HGs)? Empirical findings reveal that, despite significant
differences in model design and parameter space, different HGNNs surprisingly
share common vulnerability patterns from a relation-aware perspective.
Therefore, we explore how to design foundation HGNN attack criteria by mining
shared attack units. In this paper, we propose a novel relation-wise
heterogeneous graph foundation attack model, HeTa. We introduce a foundation
surrogate model to align heterogeneity and identify the importance of shared
relation-aware attack units. Building on this, we implement a serialized
relation-by-relation attack based on the identified relational weights. In this
way, the perturbation can be transferred to various target HGNNs and easily
fine-tuned for new HGs. Extensive experiments exhibit powerful attack
performances and generalizability of our method.

</details>


### [51] [LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning](https://arxiv.org/abs/2506.07443)
*Weijie Shi,Han Zhu,Jiaming Ji,Mengze Li,Jipeng Zhang,Ruiyuan Zhang,Jia Zhu,Jiajie Xu,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: Legal judgment prediction (LJP) is crucial for supporting court decision-making and improving judicial efficiency. However, existing methods often have logical errors when conducting complex legal reasoning. The paper proposes LegalReasoner to enhance LJP reliability through step-wise verification and correction of the reasoning process.


<details>
  <summary>Details</summary>
Motivation: Existing methods for legal judgment prediction often struggle with logical errors when conducting complex legal reasoning.

Method: The proposed method, LegalReasoner, first identifies dispute points to decompose complex cases, then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction.

Result: Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B.

Conclusion: LegalReasoner enhances the reliability of legal judgment prediction through step-wise verification and correction of the reasoning process.

Abstract: Legal judgment prediction (LJP) aims to function as a judge by making final
rulings based on case claims and facts, which plays a vital role in the
judicial domain for supporting court decision-making and improving judicial
efficiency. However, existing methods often struggle with logical errors when
conducting complex legal reasoning. We propose LegalReasoner, which enhances
LJP reliability through step-wise verification and correction of the reasoning
process. Specifically, it first identifies dispute points to decompose complex
cases, and then conducts step-wise reasoning while employing a process verifier
to validate each step's logic from correctness, progressiveness, and potential
perspectives. When errors are detected, expert-designed attribution and
resolution strategies are applied for correction. To fine-tune LegalReasoner,
we release the LegalHK dataset, containing 58,130 Hong Kong court cases with
detailed annotations of dispute points, step-by-step reasoning chains, and
process verification labels. Experiments demonstrate that LegalReasoner
significantly improves concordance with court decisions from 72.37 to 80.27 on
LLAMA-3.1-70B. The data is available at
https://huggingface.co/datasets/weijiezz/LegalHK.

</details>


### [52] [Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification](https://arxiv.org/abs/2506.07446)
*Liwen Zheng,Chaozhuo Li,Zheng Liu,Feiran Huang,Haoran Jia,Zaisheng Ye,Xi Zhang*

Main category: cs.AI

TL;DR: 事实核查在打击错误信息方面起着至关重要的作用，但传统方法难以处理复杂的断言。我们提出了原子事实提取和验证（AFEV），一个可以将复杂断言分解为原子事实的新框架，从而实现精细检索和自适应推理，提升复杂场景下的验证准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的事实验证方法难以应对需要多步推理的复杂断言，因为它们依赖于静态分解策略和表面语义检索，无法捕捉断言的细微结构和意图，导致推理错误积累、证据噪音污染和对多样化断言的适应性有限。

Method: 提出了一种名为Atomic Fact Extraction and Verification (AFEV)的新框架，该框架通过迭代地将复杂断言分解为原子事实，实现精细检索和自适应推理。具体来说，AFEV动态优化断言理解，减少误差传播，通过迭代事实提取重新排序证据以过滤噪声，并利用上下文特定的示范指导推理过程。

Result: 广泛的实验表明，AFEV在五个基准数据集上实现了最先进的准确性和可解释性性能。

Conclusion: AFEV框架能够有效解决复杂断言的事实验证问题，显著提高了准确性和可解释性，是事实验证领域的一个重要进步。

Abstract: Fact verification plays a vital role in combating misinformation by assessing
the veracity of claims through evidence retrieval and reasoning. However,
traditional methods struggle with complex claims requiring multi-hop reasoning
over fragmented evidence, as they often rely on static decomposition strategies
and surface-level semantic retrieval, which fail to capture the nuanced
structure and intent of the claim. This results in accumulated reasoning
errors, noisy evidence contamination, and limited adaptability to diverse
claims, ultimately undermining verification accuracy in complex scenarios. To
address this, we propose Atomic Fact Extraction and Verification (AFEV), a
novel framework that iteratively decomposes complex claims into atomic facts,
enabling fine-grained retrieval and adaptive reasoning. AFEV dynamically
refines claim understanding and reduces error propagation through iterative
fact extraction, reranks evidence to filter noise, and leverages
context-specific demonstrations to guide the reasoning process. Extensive
experiments on five benchmark datasets demonstrate that AFEV achieves
state-of-the-art performance in both accuracy and interpretability.

</details>


### [53] [Efficient Generation of Diverse Cooperative Agents with World Models](https://arxiv.org/abs/2506.07450)
*Yi Loo,Akshunn Trivedi,Malika Meghjani*

Main category: cs.AI

TL;DR: The paper proposes XPM-WM, a framework using a learned World Model to generate simulated trajectories for Cross-play Minimization, which improves sample efficiency and scalability in training diverse partner agents for Zero-Shot Coordination tasks.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiency and lack of sample efficiency in current Cross-play Minimization methods for generating diverse partner agents in Zero-Shot Coordination tasks.

Method: Propose XPM-WM framework that utilizes a learned World Model to create simulated trajectories, removing the need for sampling multiple types of trajectories and enhancing the generation of diverse partner agents with shared coordination task policies.

Result: XPM-WM matches previous methods' performance in SP population training reward and training partners for ZSC agents while being more sample efficient and scalable.

Conclusion: XPM-WM significantly enhances sample efficiency and scalability in generating diverse partner agents for Zero-Shot Coordination tasks.

Abstract: A major bottleneck in the training process for Zero-Shot Coordination (ZSC)
agents is the generation of partner agents that are diverse in collaborative
conventions. Current Cross-play Minimization (XPM) methods for population
generation can be very computationally expensive and sample inefficient as the
training objective requires sampling multiple types of trajectories. Each
partner agent in the population is also trained from scratch, despite all of
the partners in the population learning policies of the same coordination task.
In this work, we propose that simulated trajectories from the dynamics model of
an environment can drastically speed up the training process for XPM methods.
We introduce XPM-WM, a framework for generating simulated trajectories for XPM
via a learned World Model (WM). We show XPM with simulated trajectories removes
the need to sample multiple trajectories. In addition, we show our proposed
method can effectively generate partners with diverse conventions that match
the performance of previous methods in terms of SP population training reward
as well as training partners for ZSC agents. Our method is thus, significantly
more sample efficient and scalable to a larger number of partners.

</details>


### [54] [Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions](https://arxiv.org/abs/2506.07527)
*Lu Ma,Hao Liang,Meiyi Qiang,Lexiang Tang,Xiaochen Ma,Zhen Hao Wong,Junbo Niu,Chengyu Shen,Runming He,Bin Cui,Wentao Zhang*

Main category: cs.AI

TL;DR: 近期大型语言模型（LLM）推理的研究表明，通过强化学习（RL），可以涌现出诸如规划和自我反思等复杂行为。然而，目前的RL仍不足以突破基础模型的限制，因为它主要基于模型已有的知识进行优化，而非促进新信息的获取。为解决这一问题，我们采用监督微调（SFT）以学习RL无法做到的事，这使得通过高质量示范数据引入新知识和推理模式成为可能。分析发现，RL擅长保持和提升模型原本能力范围内的问题表现，而SFT在推动模型超越其当前范围方面更有效。受两者互补优势的启发，我们提出了新的训练方法——ReLIFT（强化学习与在线微调交替进行）。在ReLIFT中，模型主要通过RL训练，但遇到难题时收集高质量解决方案进行微调，并在RL和微调之间交替训练以增强模型推理能力。ReLIFT在五个竞赛级别基准和一个分布外基准上平均提高了5.2分以上，相较于其他零RL模型表现出色。此外，ReLIFT仅使用13%的详细示范数据便超越了RL和SFT，证明了其可扩展性。这些结果强有力地表明，ReLIFT克服了RL的基本限制并展示了巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习（RL）在提高大型语言模型（LLM）推理能力方面取得成功，但其仍然受限于基础模型的能力范围，难以突破现有知识的局限。为了克服这一限制，需要一种能够引入新知识和推理模式的方法。

Method: 提出了一种新的训练方法——ReLIFT（强化学习与在线微调交替进行）。该方法主要利用RL进行训练，但在遇到难题时收集高质量解决方案进行SFT微调，并在RL和微调之间交替训练。这种方法结合了RL和SFT的互补优势，旨在增强模型的推理能力。

Result: ReLIFT在多个基准测试中表现出色，相较于其他零RL模型平均提高了5.2分以上。同时，它仅使用13%的详细示范数据便超越了单独使用RL或SFT的方法，证明了其高效性和可扩展性。

Conclusion: ReLIFT成功克服了RL的基本限制，展现了显著的潜力，为未来LLM推理能力的提升提供了新的方向。

Abstract: Recent advances in large language model (LLM) reasoning have shown that
sophisticated behaviors such as planning and self-reflection can emerge through
reinforcement learning (RL). However, despite these successes, RL in its
current form remains insufficient to induce capabilities that exceed the
limitations of the base model, as it is primarily optimized based on existing
knowledge of the model rather than facilitating the acquisition of new
information. To address this limitation, we employ supervised fine-tuning (SFT)
to learn what RL cannot, which enables the incorporation of new knowledge and
reasoning patterns by leveraging high-quality demonstration data. We analyze
the training dynamics of RL and SFT for LLM reasoning and find that RL excels
at maintaining and improving performance on questions within the model's
original capabilities, while SFT is more effective at enabling progress on
questions beyond the current scope of the model. Motivated by the complementary
strengths of RL and SFT, we introduce a novel training approach,
\textbf{ReLIFT} (\textbf{Re}inforcement \textbf{L}earning \textbf{I}nterleaved
with Online \textbf{F}ine-\textbf{T}uning). In ReLIFT, the model is primarily
trained using RL, but when it encounters challenging questions, high-quality
solutions are collected for fine-tuning, and the training process alternates
between RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT
achieves an average improvement of over +5.2 points across five
competition-level benchmarks and one out-of-distribution benchmark compared to
other zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both
RL and SFT while using only 13\% of the detailed demonstration data,
highlighting its scalability. These results provide compelling evidence that
ReLIFT overcomes the fundamental limitations of RL and underscores the
significant potential.

</details>


### [55] [Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification](https://arxiv.org/abs/2506.07528)
*Qisheng Hu,Quanyu Long,Wenya Wang*

Main category: cs.AI

TL;DR: The paper presents Hierarchical Agent Reasoning and Information Search (HARIS) for multi-hop claim verification, which combines reasoning and information search through reinforcement learning, achieving strong performance on EX-FEVER and HOVER benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multi-hop claim verification is a complex task that requires both multi-step reasoning and iterative information searching to uncover hidden bridging facts.

Method: The proposed method, HARIS, consists of a high-level reasoning agent and a low-level search agent. The high-level agent constructs the main verification chain and generates questions when more information is needed, while the low-level agent retrieves additional information and refines its search based on intermediate findings. HARIS is trained using reinforcement learning with outcome-based rewards.

Result: Experimental results on the EX-FEVER and HOVER benchmarks show that HARIS achieves strong performance, significantly advancing multi-hop claim verification.

Conclusion: HARIS effectively models the coordinated process of reasoning-driven searching and search-informed reasoning, enhancing verification accuracy and interpretability.

Abstract: Multi-hop claim verification is inherently challenging, requiring multi-step
reasoning to construct verification chains while iteratively searching for
information to uncover hidden bridging facts. This process is fundamentally
interleaved, as effective reasoning relies on dynamically retrieved evidence,
while effective search demands reasoning to refine queries based on partial
information. To achieve this, we propose Hierarchical Agent Reasoning and
Information Search (HARIS), explicitly modeling the coordinated process of
reasoning-driven searching and search-informed reasoning. HARIS consists of a
high-level reasoning agent that focuses on constructing the main verification
chain, generating factual questions when more information is needed, and a
low-level search agent that iteratively retrieves more information, refining
its search based on intermediate findings. This design allows each agent to
specialize in its respective task, enhancing verification accuracy and
interpretability. HARIS is trained using reinforcement learning with
outcome-based rewards. Experimental results on the EX-FEVER and HOVER
benchmarks demonstrate that HARIS achieves strong performance, greatly
advancing multi-hop claim verification.

</details>


### [56] [Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2506.07548)
*Weiqiang Jin,Hongyang Du,Guizhong Liu,Dong In Kim*

Main category: cs.AI

TL;DR: 提出了一种带有自适应难度调整机制的动态课程学习框架，结合Counterfactual Group Relative Policy Advantage方法以应对多智能体强化学习中的非平稳环境和稀疏奖励问题，从而提升训练稳定性和最终性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习方法通常在固定的对手策略下训练，限制了对变化环境的适应性并可能导致次优策略。受课程学习在监督任务中成功的启发，希望通过引入动态课程学习框架来改善这一问题。

Method: 提出了一个动态课程学习框架，该框架包含自适应难度调整机制，根据实时训练表现动态调节对手强度，使智能体逐步从简单到复杂场景进行学习。同时开发了Counterfactual Group Relative Policy Advantage（CGRPA）方法，通过构建反事实优势函数隔离个体贡献，提供内在信用信号以增强信用分配并稳定非平稳条件下的学习。

Result: 广泛的实验表明，所提出的方法提高了训练的稳定性及最终性能，并取得了与现有最先进方法竞争的结果。

Conclusion: 所提出的动态课程学习框架和CGRPA方法成功解决了多智能体强化学习中的非平稳环境和稀疏奖励问题，显著提升了训练过程的稳定性和最终性能。

Abstract: Multi-agent reinforcement learning (MARL) has achieved strong performance in
cooperative adversarial tasks. However, most existing methods typically train
agents against fixed opponent strategies and rely on such meta-static
difficulty conditions, which limits their adaptability to changing environments
and often leads to suboptimal policies. Inspired by the success of curriculum
learning (CL) in supervised tasks, we propose a dynamic CL framework for MARL
that employs an self-adaptive difficulty adjustment mechanism. This mechanism
continuously modulates opponent strength based on real-time agent training
performance, allowing agents to progressively learn from easier to more
challenging scenarios. However, the dynamic nature of CL introduces instability
due to nonstationary environments and sparse global rewards. To address this
challenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),
which is tightly coupled with the curriculum by providing intrinsic credit
signals that reflect each agent's impact under evolving task demands. CGRPA
constructs a counterfactual advantage function that isolates individual
contributions within group behavior, facilitating more reliable policy updates
throughout the curriculum. CGRPA evaluates each agent's contribution through
constructing counterfactual action advantage function, providing intrinsic
rewards that enhance credit assignment and stabilize learning under
non-stationary conditions. Extensive experiments demonstrate that our method
improves both training stability and final performance, achieving competitive
results against state-of-the-art methods. The code is available at
https://github.com/NICE-HKU/CL2MARL-SMAC.

</details>


### [57] [GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition](https://arxiv.org/abs/2506.07553)
*Jingchao Wang,Haote Yang,Jiang Wu,Yifan He,Xingjian Wei,Yinfan Wang,Chengjin Liu,Lingli Ge,Lijun Wu,Bin Wang,Dahua Lin,Conghui He*

Main category: cs.AI

TL;DR: The paper presents GTR-Mol-VLM, a new framework for Optical Chemical Structure Recognition (OCSR) that improves the conversion of molecular images into machine-readable formats. It uses a graph traversal mechanism and a data-centric principle to address complex structures and annotation inconsistencies. Experiments show it outperforms other models, especially with functional group abbreviations.


<details>
  <summary>Details</summary>
Motivation: Recent vision-language models have shown potential in OCSR but struggle with complex molecular structures and inconsistent annotations.

Method: GTR-Mol-VLM features two innovations: 1) Graph Traversal as Visual Chain of Thought for sequential atom-bond predictions, and 2) Faithfully Recognize What You've Seen principle for matching image abbreviations with expanded annotations. Additionally, they created GTR-CoT-1.3M dataset and MolRec-Bench benchmark.

Result: GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, it outperforms the second-best baseline by approximately 14 percentage points in scenarios involving molecular images with functional group abbreviations.

Conclusion: This work aims to advance OCSR technology to better meet real-world needs in cheminformatics and AI for Science.

Abstract: Optical Chemical Structure Recognition (OCSR) is crucial for digitizing
chemical knowledge by converting molecular images into machine-readable
formats. While recent vision-language models (VLMs) have shown potential in
this task, their image-captioning approach often struggles with complex
molecular structures and inconsistent annotations. To overcome these
challenges, we introduce GTR-Mol-VLM, a novel framework featuring two key
innovations: (1) the \textit{Graph Traversal as Visual Chain of Thought}
mechanism that emulates human reasoning by incrementally parsing molecular
graphs through sequential atom-bond predictions, and (2) the data-centric
principle of \textit{Faithfully Recognize What You've Seen}, which addresses
the mismatch between abbreviated structures in images and their expanded
annotations. To support model development, we constructed GTR-CoT-1.3M, a
large-scale instruction-tuning dataset with meticulously corrected annotations,
and introduced MolRec-Bench, the first benchmark designed for a fine-grained
evaluation of graph-parsing accuracy in OCSR. Comprehensive experiments
demonstrate that GTR-Mol-VLM achieves superior results compared to specialist
models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in
scenarios involving molecular images with functional group abbreviations,
GTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage
points, both in SMILES-based and graph-based metrics. We hope that this work
will drive OCSR technology to more effectively meet real-world needs, thereby
advancing the fields of cheminformatics and AI for Science. We will release
GTR-CoT at https://github.com/opendatalab/GTR-CoT.

</details>


### [58] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEOFLOW是一种新的协议级框架，用于构建可信的基于LLM/VLM的代理。它通过精细的信息流控制、事务执行、冲突解决和安全调度等机制，确保代理在对抗性、嘈杂和并发操作条件下的可靠性和安全性。实验表明，使用SAFEOFLOW构建的代理在敌对环境中仍能保持出色的性能和安全保障。


<details>
  <summary>Details</summary>
Motivation: 尽管当前的代理框架具有强大的功能，但它们在安全信息流、可靠性以及多代理协调方面仍然存在不足。这促使研究者开发一种能够弥补这些缺陷的新框架。

Method: SAFEOFLOW采用精细的信息流控制（IFC）来跟踪数据的出处、完整性和保密性；限制LLM推理以尊重这些安全标签；引入事务执行、冲突解决和安全调度机制以增强在并发多代理环境中的稳健性；并通过预写日志、回滚和安全缓存等机制进一步提高对运行时错误和策略违规的弹性。

Result: 广泛的实验表明，使用SAFEOFLOW构建的代理即使在敌对环境中也能保持令人印象深刻的任务性能和安全保证，并且明显优于现有技术。

Conclusion: SAFEOFLOW及其基准测试套件SAFEOFLOWBENCH为建立原则性、健壮性和安全性的代理生态系统奠定了基础，推动了可靠自主技术的发展。

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [59] [Automating Exploratory Multiomics Research via Language Models](https://arxiv.org/abs/2506.07591)
*Shang Qu,Ning Ding,Linhai Xie,Yifei Li,Zaoqu Liu,Kaiyan Zhang,Yibai Xiong,Yuxin Zuo,Zhangren Chen,Ermo Hua,Xingtai Lv,Youbang Sun,Yang Li,Dong Li,Fuchu He,Bowen Zhou*

Main category: cs.AI

TL;DR: This paper presents PROTEUS,an automated system for generating data-driven hypotheses from raw data files in clinical proteogenomics.It uses graph structures to manage research processes and was applied to 10 datasets,resulting in 360 hypotheses that balance reliability and novelty.


<details>
  <summary>Details</summary>
Motivation: To create a fully automated system capable of producing reliable and novel hypotheses from complex multiomics data in clinical proteogenomics,to accelerate discoveries in this field.

Method: PROTEUS employs separate modules simulating different scientific process stages,from open-ended data exploration to specific statistical analysis.It formulates research using unified graph structures to handle relationships between biological entities.

Result: PROTEUS analyzed 10 clinical multiomics datasets,generating 360 hypotheses which were validated through external data and scored positively for balancing reliability and novelty.

Conclusion: PROTEUS successfully accelerates multiomic analysis and demonstrates potential for adapting general autonomous systems to specialized scientific areas for open-ended hypothesis generation.

Abstract: This paper introduces PROTEUS, a fully automated system that produces
data-driven hypotheses from raw data files. We apply PROTEUS to clinical
proteogenomics, a field where effective downstream data analysis and hypothesis
proposal is crucial for producing novel discoveries. PROTEUS uses separate
modules to simulate different stages of the scientific process, from open-ended
data exploration to specific statistical analysis and hypothesis proposal. It
formulates research directions, tools, and results in terms of relationships
between biological entities, using unified graph structures to manage complex
research processes. We applied PROTEUS to 10 clinical multiomics datasets from
published research, arriving at 360 total hypotheses. Results were evaluated
through external data validation and automatic open-ended scoring. Through
exploratory and iterative research, the system can navigate high-throughput and
heterogeneous multiomics data to arrive at hypotheses that balance reliability
and novelty. In addition to accelerating multiomic analysis, PROTEUS represents
a path towards tailoring general autonomous systems to specialized scientific
domains to achieve open-ended hypothesis generation from data.

</details>


### [60] [SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling](https://arxiv.org/abs/2506.07636)
*Haoran Wang,Zhenyu Hou,Yao Wei,Jie Tang,Yuxiao Dong*

Main category: cs.AI

TL;DR: SWE-Dev is an SWE agent built upon open-source LLMs, with a robust pipeline for test case synthesis and scaled-up agent trajectories for training data construction. It achieves top performance on the SWE-bench-Verified benchmark.


<details>
  <summary>Details</summary>
Motivation: Building effective SWE agents is challenging due to the lack of high-quality training data and effective test cases.

Method: Develop a robust pipeline for test case synthesis and scale up agent trajectories to construct the training data.

Result: Experiments show that SWE-Dev models outperform state-of-the-art open-source models with success rates of 23.4% and 36.6% respectively.

Conclusion: All code, models, and datasets are publicly available.

Abstract: Large language models (LLMs) have advanced rapidly from conversational
problem solving to addressing real-world tasks involving tool use, such as
software engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex
and Cursor, have offered end-to-end automation of the software development
process. However, building effective SWE agents remains challenging due to the
lack of high-quality training data and effective test cases. To address this
issue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we
develop a robust pipeline to synthesize test cases for patch evaluation.
Second, we scale up agent trajectories to construct the training data for
building SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the
SWE-Dev models can achieve top performance among all open SWE agents.
Specifically, the success rates of the SWE-Dev 7B and 32B parameter models
reach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source
models. All code, models, and datasets are publicly available at
https://github.com/THUDM/SWE-Dev.

</details>


### [61] [MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents](https://arxiv.org/abs/2506.07672)
*Yunhe Yan,Shihe Wang,Jiajun Du,Yexuan Yang,Yuxuan Shan,Qichen Qiu,Xianqing Jia,Xinge Wang,Xin Yuan,Xu Han,Mao Qin,Yinxiao Chen,Chen Peng,Shangguang Wang,Mengwei Xu*

Main category: cs.AI

TL;DR: (M)LLM驱动的计算机使用代理(CUA)正在成为自动化人机交互的一种变革性技术。然而，现有的CUA基准测试主要针对GUI代理，其评估方法容易受到UI变化的影响，并且忽略了应用程序API暴露的功能交互。为了解决这个问题，我们提出了MCPWorld，这是第一个自动CUA测试平台，适用于API、GUI和API-GUI混合代理。MCPWorld的核心原则是使用“白盒应用”，即那些具有源代码可用性并可根据需要进行修改/重新编译的应用程序（例如，添加MCP支持）。目前，MCPWorld包含201个精心策划和注释的用户任务，涵盖多样化的用例和难度级别。MCPWorld还完全容器化，并支持GPU加速，以便在不同的OS/硬件环境中灵活采用。我们的初步实验使用了一个代表性的(M)LLM驱动的CUA框架，实现了75.12%的任务完成准确率，同时提供了利用MCP实现代理自动化的实际有效性的初步证据。总的来说，我们预计MCPWorld将有助于促进和标准化下一代CUA的基准测试，这些CUA可以利用丰富的外部工具。


<details>
  <summary>Details</summary>
Motivation: 当前CUA基准测试主要针对GUI代理，评估方法容易受UI变化影响且忽略应用程序API功能交互，因此需要一个更全面的测试平台来评估不同类型的CUA。

Method: 提出MCPWorld作为首个自动CUA测试平台，使用“白盒应用”以拓宽CUA设计空间，并通过动态代码 instrumentation 等技术直接监控应用程序行为来验证任务完成情况。

Result: 初步实验表明，使用代表性(M)LLM驱动的CUA框架可实现75.12%的任务完成准确率，证明了利用MCP实现代理自动化的实际有效性。

Conclusion: MCPWorld有望促进和标准化下一代CUA的基准测试，支持灵活的OS/硬件环境部署，并公开提供代码和数据集。

Abstract: (M)LLM-powered computer use agents (CUA) are emerging as a transformative
technique to automate human-computer interaction. However, existing CUA
benchmarks predominantly target GUI agents, whose evaluation methods are
susceptible to UI changes and ignore function interactions exposed by
application APIs, e.g., Model Context Protocol (MCP). To this end, we propose
MCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid
agents. A key principle of MCPWorld is the use of "white-box apps", i.e., those
with source code availability and can be revised/re-compiled as needed (e.g.,
adding MCP support), with two notable advantages:
  (1) It greatly broadens the design space of CUA, such as what and how the app
features to be exposed/extracted as CUA-callable APIs.
  (2) It allows MCPWorld to programmatically verify task completion by directly
monitoring application behavior through techniques like dynamic code
instrumentation, offering robust, accurate CUA evaluation decoupled from
specific agent implementations or UI states.
  Currently, MCPWorld includes 201 well curated and annotated user tasks,
covering diversified use cases and difficulty levels. MCPWorld is also fully
containerized with GPU acceleration support for flexible adoption on different
OS/hardware environments. Our preliminary experiments, using a representative
LLM-powered CUA framework, achieve 75.12% task completion accuracy,
simultaneously providing initial evidence on the practical effectiveness of
agent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate
and standardize the benchmarking of next-generation computer use agents that
can leverage rich external tools. Our code and dataset are publicly available
at https://github.com/SAAgent/MCPWorld.

</details>


### [62] [NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models](https://arxiv.org/abs/2506.07731)
*Mouadh Yagoubi,Yasser Dahou,Billel Mokeddem,Younes Belkada,Phuc H. Le-Khac,Basma El Amel Boussaha,Reda Alami,Jingwei Zuo,Damiano Marsili,Mugariya Farooq,Mounia Lalmas,Georgia Gkioxari,Patrick Gallinari,Philip Torr,Hakim Hacid*

Main category: cs.AI

TL;DR: Existing benchmarks fail to provide meaningful signals for small models' early training stages. This competition designs scientific knowledge evaluation tasks to measure early training progress of language models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of meaningful or discriminative signals provided by benchmarks in the early training stages of small models.

Method: Participants develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models using three pre-trained small models and intermediate checkpoints sampled during training up to 200B tokens.

Result: Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain.

Conclusion: This initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development.

Abstract: Existing benchmarks have proven effective for assessing the performance of
fully trained large language models. However, we find striking differences in
the early training stages of small models, where benchmarks often fail to
provide meaningful or discriminative signals. To explore how these differences
arise, this competition tackles the challenge of designing scientific knowledge
evaluation tasks specifically tailored for measuring early training progress of
language models. Participants are invited to develop novel evaluation
methodologies or adapt existing benchmarks to better capture performance
differences among language models. To support this effort, we provide three
pre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate
checkpoints sampled during training up to 200B tokens. All experiments and
development work can be run on widely available free cloud-based GPU platforms,
making participation accessible to researchers with limited computational
resources. Submissions will be evaluated based on three criteria: the quality
of the performance signal they produce, the consistency of model rankings at 1
trillion tokens of training, and their relevance to the scientific knowledge
domain. By promoting the design of tailored evaluation strategies for early
training, this competition aims to attract a broad range of participants from
various disciplines, including those who may not be machine learning experts or
have access to dedicated GPU resources. Ultimately, this initiative seeks to
make foundational LLM research more systematic and benchmark-informed from the
earliest phases of model development.

</details>


### [63] [RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards](https://arxiv.org/abs/2506.07736)
*Jingnan Zheng,Xiangtian Ji,Yijun Lu,Chenhang Cui,Weixiang Zhao,Gelei Deng,Zhenkai Liang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: RSafe is an adaptive safeguard for LLMs that uses guided safety reasoning and reinforced alignment to provide robust protection against policy-violating content.


<details>
  <summary>Details</summary>
Motivation: Existing guard models for LLMs rely on extensive human-curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks.

Method: RSafe operates in two stages: 1) guided reasoning, analyzing safety risks through policy-guided step-by-step reasoning, and 2) reinforced alignment, optimizing reasoning paths using rule-based RL for accurate safety prediction.

Result: This two-stage training paradigm enables RSafe to internalize safety principles and generalize safety protection capability over unseen or adversarial scenarios. During inference, RSafe can accept user-specified safety policies for enhanced safeguards.

Conclusion: RSafe provides a robust solution for safeguarding LLMs against policy-violating content by adapting its reasoning to specified safety policies and handling novel threats.

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities despite
deliberate safety alignment efforts, posing significant risks to users and
society. To safeguard against the risk of policy-violating content,
system-level moderation via external guard models-designed to monitor LLM
inputs and outputs and block potentially harmful content-has emerged as a
prevalent mitigation strategy. Existing approaches of training guard models
rely heavily on extensive human curated datasets and struggle with
out-of-distribution threats, such as emerging harmful categories or jailbreak
attacks. To address these limitations, we propose RSafe, an adaptive
reasoning-based safeguard that conducts guided safety reasoning to provide
robust protection within the scope of specified safety policies. RSafe operates
in two stages: 1) guided reasoning, where it analyzes safety risks of input
content through policy-guided step-by-step reasoning, and 2) reinforced
alignment, where rule-based RL optimizes its reasoning paths to align with
accurate safety prediction. This two-stage training paradigm enables RSafe to
internalize safety principles to generalize safety protection capability over
unseen or adversarial safety violation scenarios. During inference, RSafe
accepts user-specified safety policies to provide enhanced safeguards tailored
to specific safety requirements.

</details>


### [64] [Agent Semantics, Semantic Spacetime, and Graphical Reasoning](https://arxiv.org/abs/2506.07756)
*Mark Burgess*

Main category: cs.AI

TL;DR: A finite γ(3,4) representation in Semantic Spacetime graph model forms a closed set of operations for directed knowledge representations and process modelling. Absorbing states indicate information loss akin to division by zero, needing remedial information injection. Origins of the model associate absorbing states with boundary information for intentionality.


<details>
  <summary>Details</summary>
Motivation: To present formal aspects of the Semantic Spacetime graph model for directed knowledge representations and process modelling, addressing issues like predictability, information loss and closure.

Method: Defining a finite γ(3,4) representation to form a closed set of operations that can scale to any degree of semantic complexity while using Semantic Spacetime postulates.

Result: Absorbing states are identified as linked to information loss and requiring manual intervention, closely associated with division by zero issue.

Conclusion: Semantic Spacetime model origins help clarify association of absorbing states with boundary information where intentionality can enter.

Abstract: Some formal aspects of the Semantic Spacetime graph model are presented, with
reference to its use for directed knowledge representations and process
modelling. A finite $\gamma(3,4)$ representation is defined to form a closed
set of operations that can scale to any degree of semantic complexity. The
Semantic Spacetime postulates bring predictability with minimal constraints to
pathways in graphs. The ubiquitous appearance of absorbing states in any
partial graph means that a graph process leaks information. The issue is
closely associated with the issue of division by zero, which signals a loss of
closure and the need for manual injection of remedial information. The Semantic
Spacetime model (and its Promise Theory) origins help to clarify how such
absorbing states are associated with boundary information where intentionality
can enter.

</details>


### [65] [REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models](https://arxiv.org/abs/2506.07759)
*Diego Forniés-Tabuenca,Alejandro Uribe,Urtzi Otamendi,Arkaitz Artetxe,Juan Carlos Rivera,Oier Lopez de Lacalle*

Main category: cs.AI

TL;DR: This paper proposes REMoH, a novel framework that combines NSGA-II with LLM-based heuristic generation for multi-objective optimization tasks. It introduces a reflection mechanism to improve convergence and solution diversity, demonstrating competitive results on FJSSP benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional multi-objective optimization algorithms, such as high problem-specific modeling demands and difficulties in adapting to nonlinear structures, by leveraging the explainability, adaptability, and reasoning capabilities of Large Language Models (LLMs).

Method: The Reflective Evolution of Multi-objective Heuristics (REMoH) integrates NSGA-II with LLM-based heuristic generation. It incorporates a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics.

Result: REMoH achieves competitive results compared to state-of-the-art approaches on the Flexible Job Shop Scheduling Problem (FJSSP) using three instance datasets (Dauzere, Barnes, and Brandimarte), with reduced modeling effort and enhanced adaptability.

Conclusion: The findings highlight the potential of LLMs to augment traditional optimization methods, providing greater flexibility, interpretability, and robustness in multi-objective optimization scenarios.

Abstract: Multi-objective optimization is fundamental in complex decision-making tasks.
Traditional algorithms, while effective, often demand extensive
problem-specific modeling and struggle to adapt to nonlinear structures. Recent
advances in Large Language Models (LLMs) offer enhanced explainability,
adaptability, and reasoning. This work proposes Reflective Evolution of
Multi-objective Heuristics (REMoH), a novel framework integrating NSGA-II with
LLM-based heuristic generation. A key innovation is a reflection mechanism that
uses clustering and search-space reflection to guide the creation of diverse,
high-quality heuristics, improving convergence and maintaining solution
diversity. The approach is evaluated on the Flexible Job Shop Scheduling
Problem (FJSSP) in-depth benchmarking against state-of-the-art methods using
three instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate
that REMoH achieves competitive results compared to state-of-the-art approaches
with reduced modeling effort and enhanced adaptability. These findings
underscore the potential of LLMs to augment traditional optimization, offering
greater flexibility, interpretability, and robustness in multi-objective
scenarios.

</details>


### [66] [A Proposal to Extend the Common Model of Cognition with Metacognition](https://arxiv.org/abs/2506.07807)
*John Laird,Christian Lebiere,Paul Rosenbloom,Andrea Stocco,Robert Wray*

Main category: cs.AI

TL;DR: A unified approach to integrating metacognition within the Common Model of Cognition (CMC) is proposed, which involves reasoning over explicit representations in working memory and makes minimal extensions to the existing structure.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a unified approach for integrating metacognition into the Common Model of Cognition (CMC) for human-like minds.

Method: The method involves reasoning over explicit representations of an agent's cognitive capabilities and processes in working memory, exploiting the existing cognitive capabilities of the CMC and making minimal extensions to its structure.

Result: Examples of metacognition within the proposal are provided, demonstrating the feasibility of the approach.

Conclusion: This approach successfully integrates metacognition within the CMC with minimal changes to its structure.

Abstract: The Common Model of Cognition (CMC) provides an abstract characterization of
the structure and processing required by a cognitive architecture for
human-like minds. We propose a unified approach to integrating metacognition
within the CMC. We propose that metacognition involves reasoning over explicit
representations of an agent's cognitive capabilities and processes in working
memory. Our proposal exploits the existing cognitive capabilities of the CMC,
making minimal extensions in the structure and information available within
working memory. We provide examples of metacognition within our proposal.

</details>


### [67] [Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation](https://arxiv.org/abs/2506.07820)
*Jiaxiang CHen,Zhuo Wang,Mingxi Zou,Qifan Wang,Zenglin Xu*

Main category: cs.AI

TL;DR: The paper introduces Guideline Forest, a framework enhancing LLMs reasoning by inducing structured strategies from verified examples and executing them via step-wise aggregation. Evaluated on four benchmarks, it outperforms strong baselines.


<details>
  <summary>Details</summary>
Motivation: Human reasoning is flexible, adaptive, and grounded in prior experience, qualities that LLMs still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner.

Method: Guideline Forest enhances LLMs reasoning by inducing structured reasoning strategies called guidelines from verified examples and executing them via step-wise aggregation. It draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants, which reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step.

Result: Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow, when evaluated on four benchmarks spanning mathematical and programmatic reasoning. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation.

Conclusion: Guideline Forest's adaptability and generalization potential are underscored, showing promise in enhancing LLMs reasoning capabilities.

Abstract: Human reasoning is flexible, adaptive, and grounded in prior
experience-qualities that large language models (LLMs) still struggle to
emulate. Existing methods either explore diverse reasoning paths at inference
time or search for optimal workflows through expensive operations, but both
fall short in leveraging multiple reusable strategies in a structured,
efficient manner. We propose Guideline Forest, a framework that enhances LLMs
reasoning by inducing structured reasoning strategies-called guidelines-from
verified examples and executing them via step-wise aggregation. Unlike
test-time search or single-path distillation, our method draws on verified
reasoning experiences by inducing reusable guidelines and expanding each into
diverse variants. Much like human reasoning, these variants reflect alternative
thought patterns, are executed in parallel, refined via self-correction, and
aggregated step by step-enabling the model to adaptively resolve uncertainty
and synthesize robust solutions.We evaluate Guideline Forest on four
benchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and
programmatic reasoning. Guideline Forest consistently outperforms strong
baselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further
highlight the effectiveness of multi-path reasoning and stepwise aggregation,
underscoring the Guideline Forest's adaptability and generalization potential.

</details>


### [68] [Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs](https://arxiv.org/abs/2506.07824)
*Yao Yan*

Main category: cs.AI

TL;DR: 大型语言模型（LLaMA-3-8B-Instruct）在多位数加法中的计算能力被深入剖析，揭示其内部算术过程遵循类似于人类的四阶段轨迹，强调了分层处理和内部计算的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过分析LLaMA-3-8B-Instruct在多位数加法中的表现，探索大语言模型的计算能力及其内部算术处理机制。

Method: 结合线性探测与logit-lens检查方法，提出并分析了一个在前向传递过程中的人类加法规则相仿的四个连贯阶段。

Result: 发现了模型在计算过程中呈现出清晰的分层特征，能够几乎完美地检测和解码总和中的各个数字，并最终生成正确的输出内容。

Conclusion: 研究表明，LLaMA-3-8B-Instruct的计算过程更倾向于内部计算而非死记硬背，同时提供代码和数据以促进可重复性。

Abstract: Multi-digit addition is a clear probe of the computational power of large
language models. To dissect the internal arithmetic processes in
LLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.
Inspired by the step-by-step manner in which humans perform addition, we
propose and analyze a coherent four-stage trajectory in the forward
pass:Formula-structure representations become linearly decodable first, while
the answer token is still far down the candidate list.Core computational
features then emerge prominently.At deeper activation layers, numerical
abstractions of the result become clearer, enabling near-perfect detection and
decoding of the individual digits in the sum.Near the output, the model
organizes and generates the final content, with the correct token reliably
occupying the top rank.This trajectory suggests a hierarchical process that
favors internal computation over rote memorization. We release our code and
data to facilitate reproducibility.

</details>


### [69] [HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains](https://arxiv.org/abs/2506.07837)
*Shijie Wang,Yilun Zhang,Zeyu Lai,Dexing Kong*

Main category: cs.AI

TL;DR: Multimodal large language models (MLLMs) struggle in specific domains due to lack of domain-specific data. This paper proposes a novel pipeline to generate supervised fine-tuning data from domain-specific materials in the medical ultrasound field, establishing a dataset ReMUD and releasing related resources to address data shortage issue.


<details>
  <summary>Details</summary>
Motivation: MLLMs perform poorly in specific domains such as medical ultrasound due to lack of domain-specific data like image-text or video-text data. There is abundant unstandardized graphic and textual data in this field.

Method: Propose an image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, answer) from domain-specific materials. Establish a medical ultrasound domain dataset ReMUD containing over 45,000 QA and VQA data.

Result: The ReMUD-7B model fine-tuned on Qwen2.5-VL-7B-Instruct outperforms general-domain MLLMs in medical ultrasound field.

Conclusion: To facilitate research, release the ReMUD dataset, data generation codebase, and ReMUD-7B parameters at https://github.com/ShiDaizi/ReMUD, addressing the data shortage issue in specific domain MLLMs.

Abstract: Multimodal large language models (MLLMs) have shown great potential in
general domains but perform poorly in some specific domains due to a lack of
domain-specific data, such as image-text data or vedio-text data. In some
specific domains, there is abundant graphic and textual data scattered around,
but lacks standardized arrangement. In the field of medical ultrasound, there
are ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic
diagnostic reports, and so on. However, these ultrasonic materials are often
saved in the forms of PDF, images, etc., and cannot be directly used for the
training of MLLMs. This paper proposes a novel image-text reasoning supervised
fine-tuning data generation pipeline to create specific domain quadruplets
(image, question, thinking trace, and answer) from domain-specific materials. A
medical ultrasound domain dataset ReMUD is established, containing over 45,000
reasoning and non-reasoning supervised fine-tuning Question Answering (QA) and
Visual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on
Qwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound
field. To facilitate research, the ReMUD dataset, data generation codebase, and
ReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,
addressing the data shortage issue in specific domain MLLMs.

</details>


### [70] [A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms](https://arxiv.org/abs/2506.07853)
*Hudson de Martim*

Main category: cs.AI

TL;DR: 本文提出了一种扩展FRBRoo框架的结构化时间模型，通过引入表达时间和语言版本的概念，实现对法律规范及其组成部分在特定时间点的状态和语言变体的精细建模。该模型以巴西联邦宪法为例，展示了如何追踪法律文本的时间演化，为高级法律信息系统、知识图谱和AI工具提供了坚实基础。


<details>
  <summary>Details</summary>
Motivation: 当前的法律文档建模框架（如FRBR/FRBRoo和Akoma Ntoso）虽然在宏观层面有效，但在组件级别的细粒度版本控制方面存在不足，无法支持可靠的时间点重建需求。

Method: 提出一种扩展FRBRoo框架的结构化时间模型，定义了Expressio - 时间版本（TV）和语言版本（LV），并进一步引入Component Work（CW）、Component Temporal Version（CTV）和Component Language Version（CLV）来追踪法律文本各部分的生命周期。

Result: 模型能够精确地追踪和重建法律文本中每个部分在特定时间点的状态，支持历史分析和影响评估。以巴西联邦宪法为例，展示了每次修正案如何创建新的组件时间版本，而不受影响的部分保留原有版本。

Conclusion: 该模型克服了现有生成模型的局限性，为开发准确的历史分析和影响评估的高级法律信息系统、知识图谱和AI工具提供了坚实基础。

Abstract: Effectively representing legal norms for automated processing is a critical
challenge, particularly in tracking the diachronic evolution of their
hierarchical components (e.g., articles, paragraphs). While foundational
frameworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal
documents at a macro level, they lack native mechanisms for granular,
component-level versioning. This limitation hinders the deterministic
point-in-time reconstruction of legal texts, a fundamental capability for
reliable Legal Tech and AI applications. This paper proposes a structured,
temporal model that extends the FRBRoo framework to address this gap. It
introduces specialized subclasses of Expressio - Temporal Version (TV) and
Language Version (LV - to represent the state of a legal norm and its
linguistic variations at specific points in time. The model applies this same
paradigm hierarchically, introducing Component Work (CW), Component Temporal
Version (CTV), and Component Language Version (CLV) to track the lifecycle of
individual articles, paragraphs, and clauses. Using the Brazilian Federal
Constitution as a case study, the paper demonstrates how each amendment creates
new Component Temporal Versions for affected provisions, while unaffected
components retain their existing versions. This fine-grained, time-aware
architecture enables the precise, deterministic retrieval and reconstruction of
any part of a legal text as it existed on a specific date. The model provides a
robust foundation for developing advanced legal information systems, knowledge
graphs, and AI tools capable of accurate historical analysis and impact
assessment, overcoming the limitations of current generative models.

</details>


### [71] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: Recent advancements in LLMs have revitalized philosophical debates on AI. This study investigates if modern LLMs can address the Frame Problem and Symbol Grounding Problem by testing 13 models on benchmark tasks, finding that closed models perform consistently well while open-source models show variability.


<details>
  <summary>Details</summary>
Motivation: To determine whether modern LLMs possess cognitive capacities to address fundamental philosophical challenges like the Frame Problem and Symbol Grounding Problem which were historically unsolvable within traditional symbolic AI systems.

Method: Designed two benchmark tasks reflecting the core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of outputs across five trials using criteria such as contextual reasoning, semantic coherence, and information filtering.

Result: Closed models consistently achieved high scores while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning.

Conclusion: Select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to long-standing theoretical challenges.

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [72] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: In dynamic environments, autonomous decision-making is limited by the gap between an agent's internal model and evolving reality. To address this, LUCIFER integrates hierarchical decision-making architecture with RL and LLMs to translate human insights into actionable intelligence for autonomous systems.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in bridging the gap between an agent's internal model and evolving operational context in dynamic environments, leveraging the contextual bias of human domain stakeholders.

Method: Propose LUCIFER, a framework integrating hierarchical decision-making architecture with RL and LLMs. LLMs serve dual roles: context extractors structuring stakeholder input and zero-shot exploration facilitators guiding action selection.

Result: LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies.

Conclusion: LUCIFER demonstrates the potential of context-driven decision-making where autonomous systems leverage human contextual knowledge.

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [73] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: Inequality proving is a challenging area for LLMs. The paper proposes an informal task formulation with two subtasks and introduces IneqMath, a dataset of Olympiad-level inequalities. It also develops an LLM-as-judge evaluation framework revealing that top models have low overall accuracy in rigorous proof construction.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing datasets which are scarce, synthetic, or rigidly formal, and to provide a better understanding of LLMs' capabilities in advanced mathematical reasoning.

Method: The paper recasts inequality proving into two subtasks: bound estimation and relation prediction. It releases IneqMath, a curated dataset of Olympiad-level inequalities, and develops an LLM-as-judge evaluation framework with step-wise judges.

Result: Systematic evaluation shows that even top LLMs achieve less than 10% overall accuracy on IneqMath under step-wise scrutiny, indicating a significant gap in their ability to construct rigorous proofs compared to finding answers.

Conclusion: Scaling model size or computation yields limited gains in proof correctness. Promising research directions include theorem-guided reasoning and self-refinement.

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [74] [Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation](https://arxiv.org/abs/2506.07940)
*Christopher Subia-Waud*

Main category: cs.AI

TL;DR: The paper introduces Gradients, a decentralized AutoML platform that uses economic incentives for hyperparameter optimization. It outperforms existing platforms with an 82.8% win rate and significant improvements in various tasks.


<details>
  <summary>Details</summary>
Motivation: Existing AutoML platforms rely on single optimization strategies that only explore a fraction of viable hyperparameter configurations.

Method: Gradients transforms hyperparameter optimization into a competitive marketplace where independent miners compete to discover optimal configurations using economic incentives.

Result: Gradients achieves an 82.8% win rate against HuggingFace AutoTrain and 100% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8% and 42.1% respectively. Strong gains are observed in complex reasoning, retrieval tasks, and person-specific generation.

Conclusion: Competitive, economically-driven approaches can systematically discover superior configurations that centralized AutoML consistently miss.

Abstract: Foundation model fine-tuning faces a fundamental challenge: existing AutoML
platforms rely on single optimisation strategies that explore only a fraction
of viable hyperparameter configurations. In this white paper, We introduce
Gradients, a decentralised AutoML platform that transforms hyperparameter
optimisation into a competitive marketplace where independent miners compete to
discover optimal configurations. Economic incentives align individual
exploration with collective optimisation goals, driving systematic
investigation of hyperparameter regions that centralised methods miss. We
evaluate our approach across 180 controlled experiments spanning diverse model
architectures (70M to 70B parameters) and task types. Gradients achieves an
82.8\% win rate against HuggingFace AutoTrain and 100\% against TogetherAI,
Databricks, and Google Cloud, with mean improvements of 11.8\% and 42.1\%
respectively. Complex reasoning and retrieval tasks show particularly strong
gains of 30-40\%, whilst diffusion models achieve 23.4\% improvements for
person-specific generation. These results demonstrate that competitive,
economically-driven approaches can systematically discover superior
configurations that centralised AutoML consistently miss.

</details>


### [75] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 通过引入自监督双奖励机制，增强大型多模态模型的理解和生成能力，在无需外部监督的情况下显著提升文本到图像任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型多模态模型在图文对齐方面存在不足，容易产生与视觉输入矛盾的文本响应或无法遵循文本到图像提示，且现有解决方案需要外部监督且仅解决单向任务。

Method: 基于理解与生成为逆对偶任务的观察，提出一种自监督双奖励机制：对给定输入采样多个输出后反转输入-输出对，以计算模型的双似然作为自我奖励进行优化。

Result: 广泛的实验结果表明，该方法无需任何外部监督即可有效提升模型性能，尤其在文本到图像任务中取得显著改进。

Conclusion: 所提方法通过自监督双奖励机制有效增强了大型多模态模型的理解和生成能力。

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [76] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: 现有的对话AI代理基准模拟了单一控制环境，其中只有AI代理可以使用工具与世界互动，而用户只是被动的信息提供者。这与现实场景不同，例如技术支持中，用户需要积极参与修改（共享）世界的状态。为了解决这一差距，我们引入了τ²-bench，具有四个关键贡献：1) 一种新颖的电信双控制域，建模为Dec-POMDP，其中代理和用户都利用工具在共享、动态环境中行动，测试代理协调和通信；2) 一个组合任务生成器，程序化地从原子组件创建多样且可验证的任务，确保领域覆盖和受控复杂性；3) 一个可靠用户模拟器，与环境紧密耦合，其行为受工具和可观测状态的约束，提高模拟保真度；4) 通过多种消融分析对代理性能进行细粒度分析，包括区分由推理产生的错误与由通信/协调产生的错误。特别是，我们的实验显示，当代理从无用户转向双控制时，性能显著下降，突显了引导用户的挑战。总体而言，τ²-bench为必须有效推理并指导用户行动的代理提供了一个可控的测试平台。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI代理基准模拟的是单一控制环境，即只有AI代理能够使用工具与世界互动，而用户仅作为被动信息提供者。这种设定与实际场景不符，如技术支持等场景中，用户需要主动参与修改共享世界的状态。因此，需要一个新的基准来解决这一差距。

Method: 1. 提出了一种新的电信双控制域，建模为Dec-POMDP，允许代理和用户同时利用工具在共享动态环境中行动；2. 设计了一个组合任务生成器，能够程序化地从原子组件创建多样化且可验证的任务，保证领域覆盖和受控复杂性；3. 开发了一个可靠的用户模拟器，该模拟器与环境紧密结合，其行为受到工具和可观测状态的限制，提高了模拟的真实度；4. 进行了多方面的消融分析，以细粒度评估代理性能，包括区分推理错误与通信/协调错误。

Result: 实验表明，当代理从无用户控制转向双控制环境时，性能有显著下降，突出了引导用户的挑战。

Conclusion: τ²-bench为需要有效推理并指导用户行动的对话AI代理提供了一个可控的测试平台，解决了现有基准与实际应用场景之间的差距。

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


### [77] [GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior](https://arxiv.org/abs/2506.08012)
*Penghao Wu,Shengnan Ma,Bo Wang,Jiaheng Yu,Lewei Lu,Ziwei Liu*

Main category: cs.AI

TL;DR: Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation, but existing GUI models lack reflection and error recovery capabilities. To address this issue, the authors propose GUI-Reflection, a novel framework that integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to bridge the gap in current GUI models which mostly rely on learning from nearly error-free offline trajectories and thus lack reflection and error recovery capabilities.

Method: The method involves proposing GUI-Reflection, a framework that integrates self-reflection and error correction capabilities into GUI models through three training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. The framework includes scalable data pipelines for automatic data construction, a GUI-Reflection Task Suite for learning and evaluation, a diverse environment for online training, and an iterative online reflection tuning algorithm.

Result: The proposed framework equips GUI agents with self-reflection and correction capabilities, leading to more robust, adaptable, and intelligent GUI automation.

Conclusion: All data, models, environments, and tools will be released publicly, paving the way for more advanced GUI automation.

Abstract: Multimodal Large Language Models (MLLMs) have shown great potential in
revolutionizing Graphical User Interface (GUI) automation. However, existing
GUI models mostly rely on learning from nearly error-free offline trajectories,
thus lacking reflection and error recovery capabilities. To bridge this gap, we
propose GUI-Reflection, a novel framework that explicitly integrates
self-reflection and error correction capabilities into end-to-end multimodal
GUI models throughout dedicated training stages: GUI-specific pre-training,
offline supervised fine-tuning (SFT), and online reflection tuning.
GUI-reflection enables self-reflection behavior emergence with fully automated
data generation and learning processes without requiring any human annotation.
Specifically, 1) we first propose scalable data pipelines to automatically
construct reflection and error correction data from existing successful
trajectories. While existing GUI models mainly focus on grounding and UI
understanding ability, we propose the GUI-Reflection Task Suite to learn and
evaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a
diverse and efficient environment for online training and data collection of
GUI models on mobile devices. 3) We also present an iterative online reflection
tuning algorithm leveraging the proposed environment, enabling the model to
continuously enhance its reflection and error correction abilities. Our
framework equips GUI agents with self-reflection and correction capabilities,
paving the way for more robust, adaptable, and intelligent GUI automation, with
all data, models, environments, and tools to be released publicly.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning](https://arxiv.org/abs/2506.06290)
*Mingyu Lu,Ethan Weinberger,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: CellCLIP is a new cross-modal contrastive learning framework for HCS data, which outperforms current models in cross-modal retrieval and biological tasks while reducing computation time.


<details>
  <summary>Details</summary>
Motivation: High-content screening assays have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. There is a need for a unified latent space that aligns perturbations with their corresponding morphological effects, but applying existing methods to HCS data is challenging due to differences in semantics of Cell Painting images compared to natural images and difficulty representing different classes of perturbations in a single latent space.

Method: CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations.

Result: CellCLIP demonstrates the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.

Conclusion: CellCLIP is a promising tool for analyzing HCS data and understanding the relationships between different perturbations and their effects on cellular state.

Abstract: High-content screening (HCS) assays based on high-throughput microscopy
techniques such as Cell Painting have enabled the interrogation of cells'
morphological responses to perturbations at an unprecedented scale. The
collection of such data promises to facilitate a better understanding of the
relationships between different perturbations and their effects on cellular
state. Towards achieving this goal, recent advances in cross-modal contrastive
learning could, in theory, be leveraged to learn a unified latent space that
aligns perturbations with their corresponding morphological effects. However,
the application of such methods to HCS data is not straightforward due to
substantial differences in the semantics of Cell Painting images compared to
natural images, and the difficulty of representing different classes of
perturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent
space. In response to these challenges, here we introduce CellCLIP, a
cross-modal contrastive learning framework for HCS data. CellCLIP leverages
pre-trained image encoders coupled with a novel channel encoding scheme to
better capture relationships between different microscopy channels in image
embeddings, along with natural language encoders for representing
perturbations. Our framework outperforms current open-source models,
demonstrating the best performance in both cross-modal retrieval and
biologically meaningful downstream tasks while also achieving significant
reductions in computation time.

</details>


### [79] [Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks](https://arxiv.org/abs/2506.06291)
*Xiaoke Wang,Batuhan Altundas,Zhaoxin Li,Aaron Zhao,Matthew Gombolay*

Main category: cs.LG

TL;DR: The paper introduces a learning-based framework using Behavior Cloning and Reinforcement Learning to train Graph Neural Networks for producing initial solutions in MILP solvers, reducing optimization time and variance in multi-agent task allocation and scheduling problems.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of long computational times in solving large-scale Mixed Integer Linear Programs (MILPs), especially in real-time scenarios.

Method: A learning-based framework that uses Behavior Cloning and Reinforcement Learning to train Graph Neural Networks (GNNs) to generate high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems.

Result: Experimental results show reduced optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.

Conclusion: The proposed method effectively addresses the computational challenges in solving large-scale MILPs, making it more suitable for real-time applications.

Abstract: Mixed Integer Linear Programs (MILPs) are essential tools for solving
planning and scheduling problems across critical industries such as
construction, manufacturing, and logistics. However, their widespread adoption
is limited by long computational times, especially in large-scale, real-time
scenarios. To address this, we present a learning-based framework that
leverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph
Neural Networks (GNNs), producing high-quality initial solutions for
warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling
Problems. Experimental results demonstrate that our method reduces optimization
time and variance compared to traditional techniques while maintaining solution
quality and feasibility.

</details>


### [80] [Mutual-Taught for Co-adapting Policy and Reward Models](https://arxiv.org/abs/2506.06292)
*Tianyuan Shi,Canbin Huang,Fanqi Wan,Longguang Zhong,Ziyi Yang,Weizhou Shen,Xiaojun Quan,Ming Yan*

Main category: cs.LG

TL;DR: In this paper, a self-training method named Mutual-Taught is proposed to improve both policy model (PM) and reward model (RM) for large language models during preference optimization without additional human annotation.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the problem of distribution shifts between newly generated model samples and the data used to train the reward model (RM), which negatively impacts the performance of the policy model (PM).

Method: Mutual-Taught is proposed, which mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM. In the M-step, the RM is updated by constructing training data from the outputs of the PM before and after the E-step update.

Result: Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, LLaMA-3-8B-Instruct-MT achieves a length-controlled win rate of 54.1% on AlpacaEval-2, while FsfairX-LLaMA3-RM-MT performs on par with GPT-4o-2024-08-06 on RewardBench.

Conclusion: Mutual-Taught effectively improves both the policy model and reward model in large language models through an iterative process without requiring additional human annotation.

Abstract: During the preference optimization of large language models (LLMs),
distribution shifts may arise between newly generated model samples and the
data used to train the reward model (RM). This shift reduces the efficacy of
the RM, which in turn negatively impacts the performance of the policy model
(PM). To address this challenge, we propose Mutual-Taught, a self-training
method that iteratively improves both the PM and RM without requiring
additional human annotation. Our approach mirrors the expectation-maximization
(EM) algorithm. In the E-step, the PM is updated using feedback from the
current RM, guiding the PM toward a better approximation of the latent optimal
preference distribution. In the M-step, we update the RM by constructing
training data from the outputs of the PM before and after the E-step update.
This process ensures that the RM adapts to the evolving policy distribution.
Experimental results demonstrate that this iterative approach leads to
consistent improvements in both models. Specifically, our 8B policy model,
LLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\% on
AlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par
with GPT-4o-2024-08-06 on RewardBench.

</details>


### [81] [Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks](https://arxiv.org/abs/2506.06293)
*Junyi Liu,Stanley Kok*

Main category: cs.LG

TL;DR: Agencies like Standard & Poor's and Moody's offer bank credit ratings crucial for economic stability. Predicting these ratings is challenging due to privacy concerns affecting interbank connection data. This study uses persistent homology to build a network capturing bank relationships, which is combined with a traditional lending network to form a heterogeneous network for improved rating predictions through HTGNN.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting bank credit ratings without complete interbank connection data, which is essential for informed decision-making, regulatory actions, and investor protection.

Method: The research employs persistent homology to construct a network that captures relationships among banks. This network is then integrated with a traditional lending network to create a heterogeneous network that combines information from both sources.

Result: Experiments conducted on a global, real-world dataset confirm the effectiveness of HTGNN in enhancing prediction accuracy.

Conclusion: This approach has significant implications for investors and regulatory bodies, enabling better proactive risk mitigation and effective market interventions.

Abstract: Agencies such as Standard & Poor's and Moody's provide bank credit ratings
that influence economic stability and decision-making by stakeholders. Accurate
and timely predictions support informed decision-making, regulatory actions,
and investor protection. However, a complete interbank connection graph is
often unavailable due to privacy concerns, complicating the direct application
of Graph Neural Networks (GNNs) for rating prediction. our research utilizes
persistent homology to construct a network that captures relationships among
banks and combines this with a traditional lending network to create a
heterogeneous network that integrates information from both sources, leading to
improved predictions. Experiments on a global, real-world dataset validate the
effectiveness of HTGNN. This research has implications for investors and
regulatory bodies in enhancing proactive risk mitigation and the implementation
of effective market interventions.The code can be find at
https://github.com/Liu-Jun-Yi/HTGNN.

</details>


### [82] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: GLProtein is a new framework that integrates global structural similarity and local amino acid details for protein pre-training, showing superior performance in various bioinformatics tasks.


<details>
  <summary>Details</summary>
Motivation: There is potential to further explore the integration of protein structural information beyond just 3D structures.

Method: GLProtein combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding.

Result: GLProtein outperforms previous methods in predicting protein-protein interaction, contact prediction, and other bioinformatics tasks.

Conclusion: Structural information of proteins includes both local and global information, and GLProtein effectively utilizes this to enhance prediction accuracy and functional insights.

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [83] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: dLLM-Cache是一种针对扩散型大语言模型（dLLMs）的无训练适应性缓存框架，通过结合长间隔提示缓存和基于特征相似性的部分响应更新，实现高达9.1倍的推理加速，同时保持输出质量接近自回归模型（ARMs）。


<details>
  <summary>Details</summary>
Motivation: 扩散型大语言模型（dLLMs）在文本生成方面表现出显著优势，但其高推理延迟成为主要瓶颈。传统加速技术如Key-Value缓存因与dLLMs的双向注意力机制不兼容而失效。因此，需要一种新的方法来解决dLLMs的推理效率问题。

Method: 提出dLLM-Cache框架，利用dLLM推理中静态提示和部分动态响应的特点，结合长间隔提示缓存与基于特征相似性的部分响应更新，从而高效复用中间计算结果，避免重复计算并保持模型性能。

Result: 实验表明，dLLM-Cache在多个代表性dLLMs（如LLaDA 8B和Dream 7B）上实现了最高9.1倍的推理加速，且输出质量不受影响。在许多设置下，该方法使dLLM推理延迟接近自回归模型（ARMs）。

Conclusion: dLLM-Cache为dLLMs提供了一种有效的推理加速解决方案，显著降低了延迟，同时保持了高质量的输出。这一成果有望推动dLLMs在实际应用中的更广泛应用。

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [84] [Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets](https://arxiv.org/abs/2506.06296)
*Hanaa El Afia,Said Ohamouddou,Raddouane Chiheb,Abdellatif El Afia*

Main category: cs.LG

TL;DR: The paper presents Jacobi-KAN-DGCNN, a framework for 3D point cloud classification that integrates DGCNN with KAN, replacing MLP layers with polynomial expansions. Experiments show KAN layers with Jacobi polynomials outperform traditional DGCNN in accuracy and convergence speed while maintaining parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: To improve the classification of three-dimensional point clouds by integrating Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN), aiming for better accuracy and convergence speed.

Method: Replacing Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, allowing layer-by-layer comparison without deep levels for both MLP and KAN.

Result: KAN layers using Jacobi polynomials outperformed the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed while maintaining parameter efficiency. However, higher polynomial degrees do not automatically enhance performance.

Conclusion: Higher polynomial degrees do not guarantee improved performance; further theoretical and empirical investigation is needed to understand the interactions between polynomial bases, degrees, and graph-based learning mechanisms.

Abstract: We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph
Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks
(KAN) for the classification of three-dimensional point clouds. This method
replaces Multi-Layer Perceptron (MLP) layers with adaptable univariate
polynomial expansions within a streamlined DGCNN architecture, circumventing
deep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In
comparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi
polynomials outperform the traditional linear layer-based DGCNN baseline in
terms of accuracy and convergence speed, while maintaining parameter
efficiency. Our results demonstrate that higher polynomial degrees do not
automatically improve performance, highlighting the need for further
theoretical and empirical investigation to fully understand the interactions
between polynomial bases, degrees, and the mechanisms of graph-based learning.

</details>


### [85] [Optimal patient allocation for echocardiographic assessments](https://arxiv.org/abs/2506.06297)
*Bozhi Sun,Seda Tierney,Jeffrey A. Feinstein,Frederick Damen,Alison L. Marsden,Daniele E. Schiavazzi*

Main category: cs.LG

TL;DR: The paper addresses scheduling challenges for echocardiographic exams by using pre-processed operational data to develop a stochastic simulation model. It compares allocation strategies and applies reinforcement learning to optimize resource management.


<details>
  <summary>Details</summary>
Motivation: Scheduling echocardiographic exams faces challenges due to non-deterministic factors and asymmetric resource constraints.

Method: Pre-processing operational data, developing a discrete-event stochastic simulation model with SimPy, integrating it with Gymnasium Python library, comparing allocation strategies, and applying reinforcement learning.

Result: On-the-fly allocation generally performs better, and the RL-based policy outperforms rule-based strategies.

Conclusion: Reinforcement learning provides actionable insights for improving echo lab efficiency through intelligent, data-driven resource management.

Abstract: Scheduling echocardiographic exams in a hospital presents significant
challenges due to non-deterministic factors (e.g., patient no-shows, patient
arrival times, diverse exam durations, etc.) and asymmetric resource
constraints between fetal and non-fetal patient streams. To address these
challenges, we first conducted extensive pre-processing on one week of
operational data from the Echo Laboratory at Stanford University's Lucile
Packard Children's Hospital, to estimate patient no-show probabilities and
derive empirical distributions of arrival times and exam durations. Based on
these inputs, we developed a discrete-event stochastic simulation model using
SimPy, and integrate it with the open source Gymnasium Python library. As a
baseline for policy optimization, we developed a comparative framework to
evaluate on-the-fly versus reservation-based allocation strategies, in which
different proportions of resources are reserved in advance. Considering a
hospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2
ratio of fetal to non-fetal sonographers, we show that on-the-fly allocation
generally yields better performance, more effectively adapting to patient
variability and resource constraints. Building on this foundation, we apply
reinforcement learning (RL) to derive an approximated optimal dynamic
allocation policy. This RL-based policy is benchmarked against the
best-performing rule-based strategies, allowing us to quantify their
differences and provide actionable insights for improving echo lab efficiency
through intelligent, data-driven resource management.

</details>


### [86] [Pairwise Calibrated Rewards for Pluralistic Alignment](https://arxiv.org/abs/2506.06298)
*Daniel Halpern,Evi Micha,Ariel D. Procaccia,Itai Shapira*

Main category: cs.LG

TL;DR: The paper proposes using a distribution of multiple reward functions to reflect diverse human preferences in alignment pipelines, rather than relying on a single universal standard. This approach treats annotator disagreements as informative and aims for pairwise calibration.


<details>
  <summary>Details</summary>
Motivation: Current alignment methods assume a single universal desirable behavior, ignoring the diversity in human preferences across users, contexts, and cultures. This leads to minority perspectives being overlooked.

Method: The authors suggest learning a distribution over multiple reward functions that induce distinct aligned policies. This is done directly from pairwise preferences without needing annotator identifiers or predefined groups. Disagreements among annotators are used as soft labels. The central criterion is pairwise calibration, ensuring that the proportion of reward functions preferring one response matches the fraction of annotators with that preference.

Result: The authors prove theoretically that even a small ensemble can accurately represent diverse preference distributions if it is outlier-free. Empirically, they introduce a training heuristic to learn such ensembles and show its effectiveness through improved calibration, which better represents pluralistic values.

Conclusion: Using a distribution of reward functions can more faithfully capture diverse human preferences, leading to better alignment in AI systems.

Abstract: Current alignment pipelines presume a single, universal notion of desirable
behavior. However, human preferences often diverge across users, contexts, and
cultures. As a result, disagreement collapses into the majority signal and
minority perspectives are discounted. To address this, we propose reflecting
diverse human preferences through a distribution over multiple reward
functions, each inducing a distinct aligned policy. The distribution is learned
directly from pairwise preference without annotator identifiers or predefined
groups. Instead, annotator disagreements are treated as informative soft
labels. Our central criterion is pairwise calibration: for every pair of
candidate responses, the proportion of reward functions preferring one response
matches the fraction of annotators with that preference. We prove that even a
small outlier-free ensemble can accurately represent diverse preference
distributions. Empirically, we introduce and validate a practical training
heuristic to learn such ensembles, and demonstrate its effectiveness through
improved calibration, implying a more faithful representation of pluralistic
values.

</details>


### [87] [LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization](https://arxiv.org/abs/2506.06300)
*Yuanye Zhou,Zhaokun Wang,Kai Zhou,Hui Tang,Xiaofan Li*

Main category: cs.LG

TL;DR: 提出了一种新的框架LT-PINNs，通过参数化拓扑边界曲线的控制变量作为可学习参数，消除了对人工插值的需求，并能够精确确定边界。通过两种类型的偏微分方程验证了其准确性和鲁棒性，展示了在流动速度重新排列等工程应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的PINNs依赖于基于密度的拓扑描述，这需要手动插值并限制了其在复杂几何形状中的应用。为了解决这一问题，提出了Lagrangian拓扑感知PINNs（LT-PINNs）。

Method: 通过将拓扑边界曲线的控制变量参数化为可学习参数，引入专门的边界条件损失函数和拓扑损失函数，确保即使对于复杂的拓扑结构也能实现清晰和准确的边界表示。

Result: (1) 与最先进的基于密度拓扑的PINNs（DT-PINNs）相比，LT-PINNs显著降低了相对L2误差；(2) LT-PINNs可以处理任意边界条件，适用于广泛的PDEs；(3) LT-PINNs可以在无需人工插值的情况下推断出清晰的拓扑边界，特别是对于复杂拓扑。

Conclusion: LT-PINNs提供了一种新颖的、专注于边界的工程优化框架，具有更高的精度和鲁棒性，且无需人工干预即可处理复杂拓扑。

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful meshless
tool for topology optimization, capable of simultaneously determining optimal
topologies and physical solutions. However, conventional PINNs rely on
density-based topology descriptions, which necessitate manual interpolation and
limit their applicability to complex geometries. To address this, we propose
Lagrangian topology-conscious PINNs (LT-PINNs), a novel framework for
boundary-focused engineering optimization. By parameterizing the control
variables of topology boundary curves as learnable parameters, LT-PINNs
eliminate the need for manual interpolation and enable precise boundary
determination. We further introduce specialized boundary condition loss
function and topology loss function to ensure sharp and accurate boundary
representations, even for intricate topologies. The accuracy and robustness of
LT-PINNs are validated via two types of partial differential equations (PDEs),
including elastic equation with Dirichlet boundary conditions and Laplace's
equation with Neumann boundary conditions. Furthermore, we demonstrate
effectiveness of LT-PINNs on more complex time-dependent and time-independent
flow problems without relying on measurement data, and showcase their
engineering application potential in flow velocity rearrangement, transforming
a uniform upstream velocity into a sine-shaped downstream profile. The results
demonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors
compared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)
LT-PINNs can handle arbitrary boundary conditions, making them suitable for a
wide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries
without manual interpolation, especially for complex topologies.

</details>


### [88] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 通过多轮提示框架，大型语言模型在推理过程中展现出强化学习特性，能显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 研究者希望探索大型语言模型（LLM）在推理时间内的强化学习能力，并提出一种新颖的多轮提示框架，以期提升LLM完成任务的能力。

Method: 提出名为ICRL提示的多轮提示框架，首先让LLM生成响应，然后给予数值奖励反馈，在下一轮中再次提示LLM完成相同任务，同时提供之前的所有响应和奖励作为上下文。

Result: 在三个基准测试（Game of 24、创意写作和ScienceWorld）中，ICRL提示表现出比基线方法（如Self-Refine和Reflexion）更显著的性能改进。即使在某些实验中奖励信号由LLM自身生成，仍然观察到性能提升。

Conclusion: 大型语言模型能够在推理时间内展现强化学习能力，ICRL提示框架能够有效提高LLM的任务完成质量，为扩展测试时计算提供了有前景的范例。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [89] [Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study](https://arxiv.org/abs/2506.06327)
*Zilang Chen*

Main category: cs.LG

TL;DR: The paper benchmarks five ensemble learning models for wine-quality assessment, finding Gradient Boosting most accurate and Random Forest most cost-effective.


<details>
  <summary>Details</summary>
Motivation: Accurate and reproducible wine-quality assessment is critical for production control but currently relies on subjective tasting panels. This motivates the development of an objective, automated method using machine learning.

Method: Five ensemble learners (Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) are benchmarked on red- and white-wine datasets with 11 physicochemical attributes. A leakage-free workflow includes stratified train-test split, StratifiedGroupKFold cross-validation, standardisation, resampling, cost weighting, hyper-parameter search, and feature selection.

Result: Gradient Boosting achieves the highest accuracy (weighted F1 0.693 for red and 0.664 for white). Limiting to top five variables reduces dimensionality by 55% with minimal performance loss. Random Forest is the most cost-effective model.

Conclusion: Gradient Boosting is recommended for accuracy, Random Forest for cost-effectiveness, XGBoost and LightGBM for GPU efficiency. The study provides a reproducible baseline for future work in wine-quality prediction.

Abstract: Accurate and reproducible wine-quality assessment is critical for production
control yet remains dominated by subjective, labour-intensive tasting panels.
We present the first unified benchmark of five ensemble learners (Random
Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho
Verde red- and white-wine datasets (1,599 and 4,898 instances, 11
physicochemical attributes). Our leakage-free workflow employs an 80:20
stratified train-test split, five-fold StratifiedGroupKFold within the training
set, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost
weighting, Optuna hyper-parameter search (120-200 trials per model) and a
two-stage feature-selection refit. Final scores on untouched test sets are
reported with weighted F1 as the headline metric. Gradient Boosting achieves
the highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016
for white), followed within three percentage points by Random Forest and
XGBoost. Limiting each model to its five top-ranked variables lowers
dimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage
points for red and 3.0 percentage points for white, indicating that alcohol,
volatile acidity, sulphates, free SO2 and chlorides capture most predictive
signal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency
gradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and
LightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We
therefore recommend Random Forest as the most cost-effective production model,
XGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as
the accuracy ceiling for offline benchmarking. The fully documented pipeline
and metric set provide a reproducible baseline for future work on imbalanced
multi-class wine-quality prediction.

</details>


### [90] [ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications](https://arxiv.org/abs/2506.06330)
*James Afful*

Main category: cs.LG

TL;DR: As machine learning systems are increasingly deployed in high-stakes domains such as criminal justice, finance, and healthcare, the demand for interpretable and trustworthy models has intensified. To address this need, ExplainBench, an open-source benchmarking suite for systematic evaluation of local model explanations across ethically consequential datasets is introduced.


<details>
  <summary>Details</summary>
Motivation: The proliferation of local explanation techniques has no standardized, reproducible framework for their comparative evaluation, particularly in fairness-sensitive settings.

Method: ExplainBench provides unified wrappers for popular explanation algorithms, integrates end-to-end pipelines for model training and explanation generation, and supports evaluation via fidelity, sparsity, and robustness metrics.

Result: ExplainBench demonstrated on datasets commonly used in fairness research, such as COMPAS, UCI Adult Income, and LendingClub, showcasing how different explanation methods behave under a shared experimental protocol.

Conclusion: ExplainBench advances the methodological foundations of interpretable machine learning and facilitates accountability in real-world AI systems.

Abstract: As machine learning systems are increasingly deployed in high-stakes domains
such as criminal justice, finance, and healthcare, the demand for interpretable
and trustworthy models has intensified. Despite the proliferation of local
explanation techniques, including SHAP, LIME, and counterfactual methods, there
exists no standardized, reproducible framework for their comparative
evaluation, particularly in fairness-sensitive settings.
  We introduce ExplainBench, an open-source benchmarking suite for systematic
evaluation of local model explanations across ethically consequential datasets.
ExplainBench provides unified wrappers for popular explanation algorithms,
integrates end-to-end pipelines for model training and explanation generation,
and supports evaluation via fidelity, sparsity, and robustness metrics. The
framework includes a Streamlit-based graphical interface for interactive
exploration and is packaged as a Python module for seamless integration into
research workflows.
  We demonstrate ExplainBench on datasets commonly used in fairness research,
such as COMPAS, UCI Adult Income, and LendingClub, and showcase how different
explanation methods behave under a shared experimental protocol. By enabling
reproducible, comparative analysis of local explanations, ExplainBench advances
the methodological foundations of interpretable machine learning and
facilitates accountability in real-world AI systems.

</details>


### [91] [Extending AALpy with Passive Learning: A Generalized State-Merging Approach](https://arxiv.org/abs/2506.06333)
*Benjamin von Berg,Bernhard K. Aichernig*

Main category: cs.LG

TL;DR: AALpy，一个以Python编写的开源自动机学习库，新加入了被动自动机学习领域的重要方法——红蓝框架的状态合并通用实现。通过公共内部表示，可以灵活配置地实现红蓝框架，简化了状态合并算法的定义与执行，有助于现有及新算法的实现。


<details>
  <summary>Details</summary>
Motivation: 将被动自动机学习中的状态合并方法整合进AALpy库，以降低状态合并算法的实现难度，并促进现有和新算法的开发。

Method: 在AALpy中添加红蓝框架的状态合并通用实现，通过定义兼容性标准和评分机制来执行状态合并算法，适用于多种类型的自动机。

Result: 简化了状态合并算法的实现过程，使一些已有的状态合并算法只需几行代码即可定义。

Conclusion: AALpy新增的红蓝框架状态合并实现为不同类型的自动机提供了通用且高度可配置的方法，极大减少了算法实现的工作量。

Abstract: AALpy is a well-established open-source automata learning library written in
Python with a focus on active learning of systems with IO behavior. It provides
a wide range of state-of-the-art algorithms for different automaton types
ranging from fully deterministic to probabilistic automata. In this work, we
present the recent addition of a generalized implementation of an important
method from the domain of passive automata learning: state-merging in the
red-blue framework. Using a common internal representation for different
automaton types allows for a general and highly configurable implementation of
the red-blue framework. We describe how to define and execute state-merging
algorithms using AALpy, which reduces the implementation effort for
state-merging algorithms mainly to the definition of compatibility criteria and
scoring. This aids the implementation of both existing and novel algorithms. In
particular, defining some existing state-merging algorithms from the literature
with AALpy only takes a few lines of code.

</details>


### [92] [Optimized Local Updates in Federated Learning via Reinforcement Learning](https://arxiv.org/abs/2506.06337)
*Ali Murad,Bo Hui,Wei-Shinn Ku*

Main category: cs.LG

TL;DR: The paper proposes a novel framework using Deep Reinforcement Learning (DRL) to optimize the amount of data necessary for training client models in Federated Learning (FL), reducing information oversharing and mitigating non-IID effects. The DRL agent learns to partition local datasets optimally, enhancing overall FL performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges with non-IID data across clients, leading to performance drops due to centralized model aggregation. Training clients on more data than needed does not benefit overall performance. This work aims to optimize the amount of data used for training each client without oversharing information with the server.

Method: A Deep Reinforcement Learning (DRL) agent is introduced to determine the optimal amount of training data for each client. The DRL agent uses changes in training loss as a reward signal and learns from local performance after each aggregation round. It outputs optimized weights for each class in the training data, creating an optimized partition of the local dataset during FL rounds.

Result: Through extensive experiments, the proposed method shows superior performance on multiple benchmark datasets and FL frameworks compared to traditional methods. Clients achieve better performance on their own data distributions while mitigating the negative effects of non-IID data.

Conclusion: The novel framework improves the efficiency and effectiveness of Federated Learning by optimizing client training data through DRL. The approach enhances performance in FL systems dealing with non-IID data and reduces unnecessary information sharing.

Abstract: Federated Learning (FL) is a distributed framework for collaborative model
training over large-scale distributed data, enabling higher performance while
maintaining client data privacy. However, the nature of model aggregation at
the centralized server can result in a performance drop in the presence of
non-IID data across different clients. We remark that training a client locally
on more data than necessary does not benefit the overall performance of all
clients. In this paper, we devise a novel framework that leverages a Deep
Reinforcement Learning (DRL) agent to select an optimized amount of data
necessary to train a client model without oversharing information with the
server. Starting without awareness of the client's performance, the DRL agent
utilizes the change in training loss as a reward signal and learns to optimize
the amount of training data necessary for improving the client's performance.
Specifically, after each aggregation round, the DRL algorithm considers the
local performance as the current state and outputs the optimized weights for
each class, in the training data, to be used during the next round of local
training. In doing so, the agent learns a policy that creates an optimized
partition of the local training dataset during the FL rounds. After FL, the
client utilizes the entire local training dataset to further enhance its
performance on its own data distribution, mitigating the non-IID effects of
aggregation. Through extensive experiments, we demonstrate that training FL
clients through our algorithm results in superior performance on multiple
benchmark datasets and FL frameworks. Our code is available at
https://github.com/amuraddd/optimized_client_training.git.

</details>


### [93] [From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins](https://arxiv.org/abs/2506.06359)
*Gabriel Antonesi,Tudor Cioara,Ionut Anghel,Vasilis Michalakopoulos,Elissaios Sarmas,Liana Toderean*

Main category: cs.LG

TL;DR: The paper reviews the application of AI, especially Transformers and LLMs, in the energy domain. It highlights practical implementations and innovations, and introduces the concept of Agentic Digital Twin.


<details>
  <summary>Details</summary>
Motivation: To explore how recent advances in foundation models like Transformers and LLMs can improve AI applications in energy management, addressing limitations in traditional machine learning.

Method: Reviewing architectural foundations, domain-specific adaptations, and practical implementations of transformer models and LLMs in various forecasting and grid management tasks.

Result: Transformers and LLMs show improved capabilities in modeling complex relationships and integrating multi-modal data. Generative AI is augmenting decision-making in both high-level planning and daily operations in the energy sector.

Conclusion: The integration of LLMs into digital twin-based energy management systems through the concept of the Agentic Digital Twin represents a next-generation model that brings autonomy, proactivity, and social interaction.

Abstract: Artificial intelligence (AI) has long promised to improve energy management
in smart grids by enhancing situational awareness and supporting more effective
decision-making. While traditional machine learning has demonstrated notable
results in forecasting and optimization, it often struggles with
generalization, situational awareness, and heterogeneous data integration.
Recent advances in foundation models such as Transformer architecture and Large
Language Models (LLMs) have demonstrated improved capabilities in modelling
complex temporal and contextual relationships, as well as in multi-modal data
fusion which is essential for most AI applications in the energy sector. In
this review we synthesize the rapid expanding field of AI applications in the
energy domain focusing on Transformers and LLMs. We examine the architectural
foundations, domain-specific adaptations and practical implementations of
transformer models across various forecasting and grid management tasks. We
then explore the emerging role of LLMs in the field: adaptation and fine tuning
for the energy sector, the type of tasks they are suited for, and the new
challenges they introduce. Along the way, we highlight practical
implementations, innovations, and areas where the research frontier is rapidly
expanding. These recent developments reviewed underscore a broader trend:
Generative AI (GenAI) is beginning to augment decision-making not only in
high-level planning but also in day-to-day operations, from forecasting and
grid balancing to workforce training and asset onboarding. Building on these
developments, we introduce the concept of the Agentic Digital Twin, a
next-generation model that integrates LLMs to bring autonomy, proactivity, and
social interaction into digital twin-based energy management systems.

</details>


### [94] [Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events](https://arxiv.org/abs/2506.06380)
*Jingyi Gu,Xuan Zhang,Guiling Wang*

Main category: cs.LG

TL;DR: 这篇论文综述了用于极端事件的合成数据生成方法，分析了生成模型和大语言模型在捕捉重尾分布方面的增强技术，并提出了针对极端事件的评估框架及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 极端事件（如市场崩盘、自然灾害和流行病）虽然罕见但破坏性极大，准确预测和早期预警可以减少损失并提高准备水平。然而，数据驱动方法需要大量训练数据，而极端事件数据本质上是稀缺的，这构成了基本挑战。

Method: 系统回顾生成建模技术和大型语言模型，特别是那些通过统计理论增强以及专门的训练和采样机制来捕获重尾分布的技术；总结基准数据集并引入定制评估框架，涵盖统计、依赖性、视觉和任务导向指标。

Result: 提供了深入分析每个度量标准在极端性和领域特定适应中的适用性，为极端环境下的模型评估提供了可操作的指导。分类关键应用领域并确定未充分探索的领域，如行为金融、野火、地震、风暴和传染病爆发。

Conclusion: 概述了开放性挑战，为推进合成稀有事件研究提供了结构化的基础。

Abstract: Extreme events, such as market crashes, natural disasters, and pandemics, are
rare but catastrophic, often triggering cascading failures across
interconnected systems. Accurate prediction and early warning can help minimize
losses and improve preparedness. While data-driven methods offer powerful
capabilities for extreme event modeling, they require abundant training data,
yet extreme event data is inherently scarce, creating a fundamental challenge.
Synthetic data generation has emerged as a powerful solution. However, existing
surveys focus on general data with privacy preservation emphasis, rather than
extreme events' unique performance requirements. This survey provides the first
overview of synthetic data generation for extreme events. We systematically
review generative modeling techniques and large language models, particularly
those enhanced by statistical theory as well as specialized training and
sampling mechanisms to capture heavy-tailed distributions. We summarize
benchmark datasets and introduce a tailored evaluation framework covering
statistical, dependence, visual, and task-oriented metrics. A central
contribution is our in-depth analysis of each metric's applicability in
extremeness and domain-specific adaptations, providing actionable guidance for
model evaluation in extreme settings. We categorize key application domains and
identify underexplored areas like behavioral finance, wildfires, earthquakes,
windstorms, and infectious outbreaks. Finally, we outline open challenges,
providing a structured foundation for advancing synthetic rare-event research.

</details>


### [95] [Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization](https://arxiv.org/abs/2506.06398)
*Yin Li*

Main category: cs.LG

TL;DR: The paper analyzes the impact of various positional encoding methods on transformer models' expressiveness, generalization ability, and extrapolation capacity. It proposes new orthogonal function-based encodings that perform better in experiments.


<details>
  <summary>Details</summary>
Motivation: To understand how different positional encoding methods affect transformers' capabilities and to address a critical gap in transformer theory.

Method: Theoretical framework using function approximation for expressiveness, Rademacher complexity for generalization bounds, and proposing new orthogonal function-based positional encoding methods. Experimental evaluation on synthetic sequence-to-sequence tasks.

Result: Orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation.

Conclusion: This work provides insights into design choices for transformers in NLP, computer vision, and other applications.

Abstract: Positional encodings are a core part of transformer-based models, enabling
processing of sequential data without recurrence. This paper presents a
theoretical framework to analyze how various positional encoding methods,
including sinusoidal, learned, relative, and bias-based methods like Attention
with Linear Biases (ALiBi), impact a transformer's expressiveness,
generalization ability, and extrapolation to longer sequences. Expressiveness
is defined via function approximation, generalization bounds are established
using Rademacher complexity, and new encoding methods based on orthogonal
functions, such as wavelets and Legendre polynomials, are proposed. The
extrapolation capacity of existing and proposed encodings is analyzed,
extending ALiBi's biasing approach to a unified theoretical context.
Experimental evaluation on synthetic sequence-to-sequence tasks shows that
orthogonal transform-based encodings outperform traditional sinusoidal
encodings in generalization and extrapolation. This work addresses a critical
gap in transformer theory, providing insights for design choices in natural
language processing, computer vision, and other transformer applications.

</details>


### [96] [CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis](https://arxiv.org/abs/2506.06411)
*Paul Fogel,Christophe Geissler,George Luta*

Main category: cs.LG

TL;DR: CoxNTF is a new method using non-negative tensor factorization to create latent representations linked with survival outcomes, offering comparable prediction performance to Coxnet but with added interpretability and ability to handle feature redundancy.


<details>
  <summary>Details</summary>
Motivation: Existing methods like NMF do not incorporate survival information, which limits their predictive power in survival analysis.

Method: CoxNTF constructs a weighted covariate tensor using survival probabilities from the Coxnet model to guide the tensorization process, deriving meaningful latent representations closely associated with survival outcomes.

Result: CoxNTF achieves survival prediction performance comparable to using Coxnet with original covariates, provides an interpretable clustering framework, and effectively handles feature redundancy.

Conclusion: CoxNTF is a powerful tool for joint clustering and prediction in survival analysis, enhancing both interpretability and handling of feature redundancy.

Abstract: The interpretation of the results of survival analysis often benefits from
latent factor representations of baseline covariates. However, existing
methods, such as Nonnegative Matrix Factorization (NMF), do not incorporate
survival information, limiting their predictive power. We present CoxNTF, a
novel approach that uses non-negative tensor factorization (NTF) to derive
meaningful latent representations that are closely associated with survival
outcomes. CoxNTF constructs a weighted covariate tensor in which survival
probabilities derived from the Coxnet model are used to guide the tensorization
process. Our results show that CoxNTF achieves survival prediction performance
comparable to using Coxnet with the original covariates, while providing a
structured and interpretable clustering framework. In addition, the new
approach effectively handles feature redundancy, making it a powerful tool for
joint clustering and prediction in survival analysis.

</details>


### [97] [NeurNCD: Novel Class Discovery via Implicit Neural Representation](https://arxiv.org/abs/2506.06412)
*Junming Wang,Yi Shi*

Main category: cs.LG

TL;DR: NeurNCD is a novel framework that uses Embedding-NeRF model with KL divergence for efficient novel class discovery in both open and closed-world settings without needing densely labelled datasets or human interaction.


<details>
  <summary>Details</summary>
Motivation: Traditional explicit representations like object descriptors or 3D segmentation maps are limited by their discrete, hole-prone, and noisy nature, which affects accurate novel class discovery.

Method: NeurNCD employs Embedding-NeRF model combined with KL divergence as an alternative to traditional explicit 3D segmentation maps. It aggregates semantic embedding and entropy in visual embedding space and integrates feature query, feature modulation and clustering components for efficient feature augmentation and information exchange.

Result: The framework achieves superior segmentation performance in both open and closed-world settings without relying on densely labelled datasets or human interaction for supervision.

Conclusion: Extensive experiments show that NeurNCD significantly outperforms state-of-the-art approaches on the NYUv2 and Replica datasets.

Abstract: Discovering novel classes in open-world settings is crucial for real-world
applications. Traditional explicit representations, such as object descriptors
or 3D segmentation maps, are constrained by their discrete, hole-prone, and
noisy nature, which hinders accurate novel class discovery. To address these
challenges, we introduce NeurNCD, the first versatile and data-efficient
framework for novel class discovery that employs the meticulously designed
Embedding-NeRF model combined with KL divergence as a substitute for
traditional explicit 3D segmentation maps to aggregate semantic embedding and
entropy in visual embedding space. NeurNCD also integrates several key
components, including feature query, feature modulation and clustering,
facilitating efficient feature augmentation and information exchange between
the pre-trained semantic segmentation network and implicit neural
representations. As a result, our framework achieves superior segmentation
performance in both open and closed-world settings without relying on densely
labelled datasets for supervised training or human interaction to generate
sparse label supervision. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches on the NYUv2 and Replica
datasets.

</details>


### [98] [Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers](https://arxiv.org/abs/2506.06443)
*Luis Pinto*

Main category: cs.LG

TL;DR: 通过分析五种分子编码器在22个ADMET属性预测任务中的层表现，发现中间层嵌入优于最终层表示。使用固定中间层嵌入可使下游性能平均提升5.4%，微调至中间层则可获得8.5%的更大改进。此方法能有效降低计算成本并达到新的SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前计算化学中普遍依赖预训练分子编码器的最终层嵌入进行下游任务，但这一做法可能会丢失有价值的信息。因此需要探索分子编码器的全部表示深度以实现更好的性能和计算效率。

Method: 对五个不同的分子编码器进行逐层分析，在22个ADMET属性预测任务上评估它们的表现；采用固定中间层嵌入与微调到中间层的方法进行实验，并比较其与仅使用最终层嵌入的结果。

Result: 使用固定中间层嵌入比最终层表示平均提升了5.4%的性能，最高可达28.6%；微调至中间层则带来了更大的改进，平均提升8.5%，最高可达40.8%，并在多个基准上达到了新SOTA结果。此外，固定嵌入性能与微调结果呈强正相关。

Conclusion: 探索分子编码器的完整表示深度对于显著提高性能和计算效率至关重要。并且提出了一种高效的先评估后微调的方法，可以减少计算成本。

Abstract: Pretrained molecular encoders have become indispensable in computational
chemistry for tasks such as property prediction and molecular generation.
However, the standard practice of relying solely on final-layer embeddings for
downstream tasks may discard valuable information. In this work, we challenge
this convention by conducting a comprehensive layer-wise analysis of five
diverse molecular encoders across 22 ADMET property prediction tasks. Our
results demonstrate that embeddings from intermediate layers consistently
outperform final-layer representations. Specifically, using fixed embeddings
from the optimal intermediate layers improved downstream performance by an
average of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to
these intermediate layers yielded even greater average improvements of 8.5%,
with performance increases as high as 40.8%, achieving new state-of-the-art
results on several benchmarks. Additionally, a strong positive correlation
between fixed embedding performance and finetuning outcomes supports an
efficient evaluate-then-finetune approach, enabling identification of optimal
layers with reduced computational cost. These findings highlight the importance
of exploring the full representational depth of molecular encoders to achieve
substantial performance improvements and computational efficiency. The code is
made publicly available at
https://github.com/luispintoc/Unlocking-Chemical-Insights.

</details>


### [99] [Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance](https://arxiv.org/abs/2506.06444)
*Ruizhong Qiu,Gaotang Li,Tianxin Wei,Jingrui He,Hanghang Tong*

Main category: cs.LG

TL;DR: 现有的安全保证研究主要集中在训练阶段对齐以将安全行为植入LLM中，但这些方法容易受到多样化的越狱攻击。我们的工作开创了推理扩展以提高LLM的安全性，并提出了SAFFRON新范式来解决探索-效率困境。实验验证了该方法的有效性，同时发布了相关模型和数据集。


<details>
  <summary>Details</summary>
Motivation: 现有安全保证研究在推理扩展方面的不足以及传统推理扩展技术在安全性上的表现不佳，促使我们探索一种新的推理扩展范式以提高LLM的安全性。

Method: 提出SAFFRON，包括多分支奖励模型（MRM）、部分监督训练目标、保守探索约束和基于Trie的键值缓存策略。

Result: 通过广泛的实验验证了SAFFRON方法的有效性。

Conclusion: SAFFRON为提高LLM安全性提供了一种新的推理扩展范式，并公开发布了相关资源以促进未来研究。

Abstract: Existing safety assurance research has primarily focused on training-phase
alignment to instill safe behaviors into LLMs. However, recent studies have
exposed these methods' susceptibility to diverse jailbreak attacks.
Concurrently, inference scaling has significantly advanced LLM reasoning
capabilities but remains unexplored in the context of safety assurance.
Addressing this gap, our work pioneers inference scaling for robust and
effective LLM safety against emerging threats. We reveal that conventional
inference scaling techniques, despite their success in reasoning tasks, perform
poorly in safety contexts, even falling short of basic approaches like
Best-of-N Sampling. We attribute this inefficiency to a newly identified
challenge, the exploration--efficiency dilemma, arising from the high
computational overhead associated with frequent process reward model (PRM)
evaluations. To overcome this dilemma, we propose SAFFRON, a novel inference
scaling paradigm tailored explicitly for safety assurance. Central to our
approach is the introduction of a multifurcation reward model (MRM) that
significantly reduces the required number of reward model evaluations. To
operationalize this paradigm, we further propose: (i) a partial supervision
training objective for MRM, (ii) a conservative exploration constraint to
prevent out-of-distribution explorations, and (iii) a Trie-based key--value
caching strategy that facilitates cache sharing across sequences during tree
search. Extensive experiments validate the effectiveness of our method.
Additionally, we publicly release our trained multifurcation reward model
(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)
to accelerate future research in LLM safety. Our code, model, and data are
publicly available at https://github.com/q-rz/saffron , and our project
homepage is at https://q-rz.github.io/p/saffron .

</details>


### [100] [LETS Forecast: Learning Embedology for Time Series Forecasting](https://arxiv.org/abs/2506.06454)
*Abrar Majeedi,Viswanatha Reddy Gajjala,Satya Sai Srinath Namburi GNVV,Nada Magdi Elkordi,Yin Li*

Main category: cs.LG

TL;DR: DeepEDM is a novel framework that combines nonlinear dynamical systems modeling with deep neural networks for time series forecasting.


<details>
  <summary>Details</summary>
Motivation: Real-world time series are often governed by complex nonlinear dynamics, which need to be understood for precise future prediction.

Method: DeepEDM integrates nonlinear dynamical systems modeling with deep neural networks. It learns a latent space from time-delayed embeddings and employs kernel regression to approximate the underlying dynamics.

Result: Experiments on synthetic data of nonlinear dynamical systems and real-world time series show that DeepEDM is robust to input noise and outperforms state-of-the-art methods in forecasting accuracy.

Conclusion: DeepEDM provides an effective approach for time series forecasting by combining nonlinear dynamical systems modeling with deep neural networks.

Abstract: Real-world time series are often governed by complex nonlinear dynamics.
Understanding these underlying dynamics is crucial for precise future
prediction. While deep learning has achieved major success in time series
forecasting, many existing approaches do not explicitly model the dynamics. To
bridge this gap, we introduce DeepEDM, a framework that integrates nonlinear
dynamical systems modeling with deep neural networks. Inspired by empirical
dynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel
deep model that learns a latent space from time-delayed embeddings, and employs
kernel regression to approximate the underlying dynamics, while leveraging
efficient implementation of softmax attention and allowing for accurate
prediction of future time steps. To evaluate our method, we conduct
comprehensive experiments on synthetic data of nonlinear dynamical systems as
well as real-world time series across domains. Our results show that DeepEDM is
robust to input noise, and outperforms state-of-the-art methods in forecasting
accuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.

</details>


### [101] [WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets](https://arxiv.org/abs/2506.06455)
*Antonio Jesús Banegas-Luna,Horacio Pérez-Sánchez,Carlos Martínez-Cortés*

Main category: cs.LG

TL;DR: 在机器学习中，虽然预测准确性通常是优先考虑的，但可解释性在科学和高风险领域仍然至关重要。本研究通过训练六个机器学习模型并使用多种模型无关的解释技术，提出了一种新的共识方法WISCA（加权缩放共识归因），以提高解释的可靠性。WISCA与最可靠的单独方法一致，强调了稳健共识策略的价值。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习模型的预测准确性很重要，但在科学和高风险领域，可解释性同样重要。然而，不同的可解释性算法经常产生冲突的解释，因此需要一种共识方法来协调这些结果。

Method: 研究者在六个具有已知真实标签的合成数据集上训练了六个机器学习模型，并应用了各种模型无关的解释技术。然后，他们使用现有的方法和一种新方法——WISCA（加权缩放共识归因）生成共识解释。WISCA结合了类别概率和标准化归因。

Result: WISCA能够与最可靠的单独解释方法保持一致，表明其在提高解释可靠性方面的有效性。

Conclusion: 这项研究表明，在改进解释可靠性方面，采用如WISCA这样的稳健共识策略具有重要意义。

Abstract: While predictive accuracy is often prioritized in machine learning (ML)
models, interpretability remains essential in scientific and high-stakes
domains. However, diverse interpretability algorithms frequently yield
conflicting explanations, highlighting the need for consensus to harmonize
results. In this study, six ML models were trained on six synthetic datasets
with known ground truths, utilizing various model-agnostic interpretability
techniques. Consensus explanations were generated using established methods and
a novel approach: WISCA (Weighted Scaled Consensus Attributions), which
integrates class probability and normalized attributions. WISCA consistently
aligned with the most reliable individual method, underscoring the value of
robust consensus strategies in improving explanation reliability.

</details>


### [102] [Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control](https://arxiv.org/abs/2506.06459)
*Ruitao Chen,Mozhang Guo,Jinge Li*

Main category: cs.LG

TL;DR: The paper explores using reinforcement learning in automated driving to enhance infant sleep quality by balancing occupant comfort and travel efficiency.


<details>
  <summary>Details</summary>
Motivation: Automated driving has improved vehicle safety and comfort, but its impact on passenger well-being, especially infant sleep, is not sufficiently studied.

Method: An intelligent cruise control framework integrating reinforcement learning (RL), LSTM, and transformer-based neural networks synergizes wearable sensing and vehicle data to optimize driving behavior for enhancing infant sleep quality under diverse traffic and road conditions.

Result: Simulation results show that the proposed solution significantly improves infant sleep quality compared to baseline methods while maintaining travel efficiency.

Conclusion: The integration of RL within automated driving can personalize driving behavior and optimally balance occupant comfort and travel efficiency.

Abstract: Automated driving (AD) has substantially improved vehicle safety and driving
comfort, but their impact on passenger well-being, particularly infant sleep,
is not sufficiently studied. Sudden acceleration, abrupt braking, and sharp
maneuvers can disrupt infant sleep, compromising both passenger comfort and
parental convenience. To solve this problem, this paper explores the
integration of reinforcement learning (RL) within AD to personalize driving
behavior and optimally balance occupant comfort and travel efficiency. In
particular, we propose an intelligent cruise control framework that adapts to
varying driving conditions to enhance infant sleep quality by effectively
synergizing wearable sensing and vehicle data. Long short-term memory (LSTM)
and transformer-based neural networks are integrated with RL to model the
relationship between driving behavior and infant sleep quality under diverse
traffic and road conditions. Based on the sleep quality indicators from the
wearable sensors, driving action data from vehicle controllers, and map data
from map applications, the model dynamically computes the optimal driving
aggressiveness level, which is subsequently translated into specific AD control
strategies, e.g., the magnitude and frequency of acceleration, lane change, and
overtaking. Simulation results demonstrate that the proposed solution
significantly improves infant sleep quality compared to baseline methods, while
preserving desirable travel efficiency.

</details>


### [103] [TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness](https://arxiv.org/abs/2506.06482)
*Zhiyuan Zhao,Juntong Ni,Shangqing Xu,Haoxin Liu,Wei Jin,B. Aditya Prakash*

Main category: cs.LG

TL;DR: Time-series forecasting is crucial across domains. Despite deep learning advancements, debate remains on the most effective architectures and components. Existing benchmarks lack detailed insights. This paper introduces TimeRecipe, a framework conducting over 10,000 experiments to evaluate time-series forecasting methods at the module level, providing meaningful intuitions and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed insights in existing benchmarks regarding which architectures and design components are most effective for time-series forecasting under varying conditions.

Method: Propose TimeRecipe, a unified benchmarking framework that systematically evaluates time-series forecasting methods at the module level by conducting over 10,000 experiments across diverse datasets, forecasting horizons, and task settings.

Result: Exhaustive exploration of the design space can yield models that outperform existing state-of-the-art methods and uncover meaningful intuitions linking specific design choices to forecasting scenarios.

Conclusion: TimeRecipe provides valuable insights into effective design choices for time-series forecasting and offers a practical toolkit to recommend suitable model architectures based on empirical findings.

Abstract: Time-series forecasting is an essential task with wide real-world
applications across domains. While recent advances in deep learning have
enabled time-series forecasting models with accurate predictions, there remains
considerable debate over which architectures and design components, such as
series decomposition or normalization, are most effective under varying
conditions. Existing benchmarks primarily evaluate models at a high level,
offering limited insight into why certain designs work better. To mitigate this
gap, we propose TimeRecipe, a unified benchmarking framework that
systematically evaluates time-series forecasting methods at the module level.
TimeRecipe conducts over 10,000 experiments to assess the effectiveness of
individual components across a diverse range of datasets, forecasting horizons,
and task settings. Our results reveal that exhaustive exploration of the design
space can yield models that outperform existing state-of-the-art methods and
uncover meaningful intuitions linking specific design choices to forecasting
scenarios. Furthermore, we release a practical toolkit within TimeRecipe that
recommends suitable model architectures based on these empirical insights. The
benchmark is available at: https://github.com/AdityaLab/TimeRecipe.

</details>


### [104] [A Certified Unlearning Approach without Access to Source Data](https://arxiv.org/abs/2506.06486)
*Umit Yigit Basaran,Sk Miraj Ahmed,Amit Roy-Chowdhury,Basak Guler*

Main category: cs.LG

TL;DR: This paper proposes a certified unlearning framework that enables effective data removal from trained models without needing the original training data samples, by using a surrogate dataset and controlled noise scaling based on statistical distance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of erasing private or copyrighted information from trained models when the source data is no longer available, which is crucial with the growing adoption of data privacy regulations.

Method: The method uses a surrogate dataset that approximates the statistical properties of the source data. It applies controlled noise scaling based on the statistical distance between the two datasets. Practical implementations typically approximate this distance, resulting in meaningful privacy guarantees even if weaker than theoretical ones.

Result: Theoretical bounds are established, practical noise calibration techniques are introduced, and extensive experiments on both synthetic and real-world datasets validate the method's effectiveness and reliability in privacy-sensitive settings.

Conclusion: This approach ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility.

Abstract: With the growing adoption of data privacy regulations, the ability to erase
private or copyrighted information from trained models has become a crucial
requirement. Traditional unlearning methods often assume access to the complete
training dataset, which is unrealistic in scenarios where the source data is no
longer available. To address this challenge, we propose a certified unlearning
framework that enables effective data removal \final{without access to the
original training data samples}. Our approach utilizes a surrogate dataset that
approximates the statistical properties of the source data, allowing for
controlled noise scaling based on the statistical distance between the two.
\updated{While our theoretical guarantees assume knowledge of the exact
statistical distance, practical implementations typically approximate this
distance, resulting in potentially weaker but still meaningful privacy
guarantees.} This ensures strong guarantees on the model's behavior
post-unlearning while maintaining its overall utility. We establish theoretical
bounds, introduce practical noise calibration techniques, and validate our
method through extensive experiments on both synthetic and real-world datasets.
The results demonstrate the effectiveness and reliability of our approach in
privacy-sensitive settings.

</details>


### [105] [Membership Inference Attacks for Unseen Classes](https://arxiv.org/abs/2506.06488)
*Pratiksha Thaker,Neil Kale,Zhiwei Steven Wu,Virginia Smith*

Main category: cs.LG

TL;DR: 本研究探讨了在成员分布不可完全获取的情况下，使用量化回归方法进行成员推断攻击的潜力和优势。相比传统的影子模型攻击，量化回归攻击在极端分布偏移下表现出色，如在CIFAR-100数据集上对未见类别达到影子模型11倍的真正例率（TPR），并在ImageNet数据集上即使移除90%的训练类别仍能获得非平凡的TPR。


<details>
  <summary>Details</summary>
Motivation: 当前的成员推断攻击方法通常假设攻击者能够访问与目标模型训练数据分布相匹配的背景数据分布。然而，在更极端且现实的场景中，攻击者或审计员可能无法访问整个子类别的分布，因此需要探索新的方法来应对这种分布偏移问题。

Method: 研究首先展示了影子模型攻击在极端分布偏移下的性能急剧下降，然后引入了量化回归方法作为替代方案。通过理论模型分析和实验证明，量化回归方法在这种情况下具有更好的表现。

Result: 量化回归攻击在类删除设置下显著优于影子模型攻击。例如，在CIFAR-100数据集上，对未见类别达到了影子模型11倍的真正例率；在ImageNet数据集上，即使移除了90%的训练类别，仍然获得了非平凡的真正例率。

Conclusion: 量化回归方法为成员推断攻击提供了一种新的有效途径，特别是在面对极端分布偏移时。尽管其也存在一定的局限性，但其潜力值得进一步研究。

Abstract: Shadow model attacks are the state-of-the-art approach for membership
inference attacks on machine learning models. However, these attacks typically
assume an adversary has access to a background (nonmember) data distribution
that matches the distribution the target model was trained on. We initiate a
study of membership inference attacks where the adversary or auditor cannot
access an entire subclass from the distribution -- a more extreme but realistic
version of distribution shift than has been studied previously. In this
setting, we first show that the performance of shadow model attacks degrades
catastrophically, and then demonstrate the promise of another approach,
quantile regression, that does not have the same limitations. We show that
quantile regression attacks consistently outperform shadow model attacks in the
class dropout setting -- for example, quantile regression attacks achieve up to
11$\times$ the TPR of shadow models on the unseen class on CIFAR-100, and
achieve nontrivial TPR on ImageNet even with 90% of training classes removed.
We also provide a theoretical model that illustrates the potential and
limitations of this approach.

</details>


### [106] [Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks](https://arxiv.org/abs/2506.06489)
*Daniel Kunin,Giovanni Luca Marchetti,Feng Chen,Dhruva Karkada,James B. Simon,Michael R. DeWeese,Surya Ganguli,Nina Miolane*

Main category: cs.LG

TL;DR: The paper introduces Alternating Gradient Flows (AGF), a framework describing feature learning dynamics in two-layer neural networks with small initialization, unifying prior analyses and providing new insights.


<details>
  <summary>Details</summary>
Motivation: To address the open question of what features neural networks learn and how they learn them, especially focusing on the dynamics of feature learning in networks trained from small initialization.

Method: Introduced Alternating Gradient Flows (AGF), an algorithmic framework that models feature learning as an alternating two-step process: maximizing utility over dormant neurons and minimizing cost over active ones. This framework quantifies the order, timing, and magnitude of loss drops during training.

Result: AGF matches experimental results across architectures, extends existing analyses in fully connected linear networks and linear transformers, and provides the first complete characterization of training dynamics for quadratic networks performing modular addition. Revealed that Fourier features are learned in decreasing order of coefficient magnitude.

Conclusion: AGF represents a promising advancement in understanding feature learning in neural networks, offering both unification of previous theories and novel insights into training dynamics.

Abstract: What features neural networks learn, and how, remains an open question. In
this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic
framework that describes the dynamics of feature learning in two-layer networks
trained from small initialization. Prior works have shown that gradient flow in
this regime exhibits a staircase-like loss curve, alternating between plateaus
where neurons slowly align to useful directions and sharp drops where neurons
rapidly grow in norm. AGF approximates this behavior as an alternating two-step
process: maximizing a utility function over dormant neurons and minimizing a
cost function over active ones. AGF begins with all neurons dormant. At each
round, a dormant neuron activates, triggering the acquisition of a feature and
a drop in the loss. AGF quantifies the order, timing, and magnitude of these
drops, matching experiments across architectures. We show that AGF unifies and
extends existing saddle-to-saddle analyses in fully connected linear networks
and attention-only linear transformers, where the learned features are singular
modes and principal components, respectively. In diagonal linear networks, we
prove AGF converges to gradient flow in the limit of vanishing initialization.
Applying AGF to quadratic networks trained to perform modular addition, we give
the first complete characterization of the training dynamics, revealing that
networks learn Fourier features in decreasing order of coefficient magnitude.
Altogether, AGF offers a promising step towards understanding feature learning
in neural networks.

</details>


### [107] [Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms](https://arxiv.org/abs/2506.06499)
*Alex Havrilla,Edward Hughes,Mikayel Samvelyan,Jacob Abernethy*

Main category: cs.LG

TL;DR: Large language model driven synthetic data generation is a powerful method for improving model reasoning capabilities. The paper presents SPARQ, which generates high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current methods that either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality, which limits the scalability of these approaches to more complex and diverse problem domains.

Method: SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, which generates high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate.

Result: Generated over 20 million new problem-solution pairs from a seed dataset of 7.5K samples. Filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24%. Higher quality facilitates better in-distribution performance, while more diverse data facilitates more robust OOD generalization.

Conclusion: The existence of model and data scaling laws for synthetically generated problems positively benefits downstream model generalization.

Abstract: Large language model (LLM) driven synthetic data generation has emerged as a
powerful method for improving model reasoning capabilities. However, most
methods either distill large state-of-the-art models into small students or use
natural ground-truth problem statements to guarantee problem statement quality.
This limits the scalability of these approaches to more complex and diverse
problem domains. To address this, we present SPARQ: Synthetic Problem
Generation for Reasoning via Quality-Diversity Algorithms, a novel approach for
generating high-quality and diverse synthetic math problem and solution pairs
using only a single model by measuring a problem's solve-rate: a proxy for
problem difficulty. Starting from a seed dataset of 7.5K samples, we generate
over 20 million new problem-solution pairs. We show that filtering the
generated data by difficulty and then fine-tuning the same model on the
resulting data improves relative model performance by up to 24\%. Additionally,
we conduct ablations studying the impact of synthetic data quantity, quality
and diversity on model generalization. We find that higher quality, as measured
by problem difficulty, facilitates better in-distribution performance. Further,
while generating diverse synthetic data does not as strongly benefit
in-distribution performance, filtering for more diverse data facilitates more
robust OOD generalization. We also confirm the existence of model and data
scaling laws for synthetically generated problems, which positively benefit
downstream model generalization.

</details>


### [108] [Optimal Rates in Continual Linear Regression via Increasing Regularization](https://arxiv.org/abs/2506.06501)
*Ran Levinstein,Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren,Daniel Soudry,Itay Evron*

Main category: cs.LG

TL;DR: 在随机任务顺序下研究可实现的连续线性回归，通过两种正则化方法缩小或关闭先前工作中的差距，并证明增加正则化强度或减少每任务步骤数是有益的。


<details>
  <summary>Details</summary>
Motivation: 解决之前工作中的显著差距问题，即在k次学习迭代后，最坏情况下的预期损失存在$\Omega(1/k)$的下界，而无正则化方案仅建立了$O(1/k^{1/4})$的上界。

Method: 使用两种正则化方案：（1）显式的各向同性$\ell_2$正则化；（2）通过有限步数预算的隐式正则化。将这些方法归结为在精心定义的代理损失上的随机梯度下降（SGD），并分析广义时间变化函数的SGD变体。

Result: 确定了一个固定的正则化强度，可达到接近最优的速度$O(\log k / k)$，并通过增加正则化强度计划达到了最优速度$O(1/k)$。

Conclusion: 增加正则化系数或减少每任务步骤数在最坏情况下是有益的。

Abstract: We study realizable continual linear regression under random task orderings,
a common setting for developing continual learning theory. In this setup, the
worst-case expected loss after $k$ learning iterations admits a lower bound of
$\Omega(1/k)$. However, prior work using an unregularized scheme has only
established an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our
paper proves that this gap can be narrowed, or even closed, using two
frequently used regularization schemes: (1) explicit isotropic $\ell_2$
regularization, and (2) implicit regularization via finite step budgets. We
show that these approaches, which are used in practice to mitigate forgetting,
reduce to stochastic gradient descent (SGD) on carefully defined surrogate
losses. Through this lens, we identify a fixed regularization strength that
yields a near-optimal rate of $O(\log k / k)$. Moreover, formalizing and
analyzing a generalized variant of SGD for time-varying functions, we derive an
increasing regularization strength schedule that provably achieves an optimal
rate of $O(1/k)$. This suggests that schedules that increase the regularization
coefficient or decrease the number of steps per task are beneficial, at least
in the worst case.

</details>


### [109] [InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models](https://arxiv.org/abs/2506.06505)
*Keisuke Sugiura,Hiroki Matsutani*

Main category: cs.LG

TL;DR: InstantFT is an FPGA-based method for ultra-fast CNN fine-tuning on IoT devices, reducing fine-tuning time and improving energy-efficiency significantly.


<details>
  <summary>Details</summary>
Motivation: Training DNNs needs more computation and memory than inference, posing a challenge to runtime adaptation of DNNs on resource-limited IoT platforms.

Method: Propose InstantFT which optimizes forward and backward computations in PEFT for ultra-fast CNN fine-tuning.

Result: Experiments show that InstantFT fine-tunes a pre-trained CNN 17.4x faster with comparable accuracy, reduces fine-tuning time to 0.36s and improves energy-efficiency by 16.3x.

Conclusion: InstantFT enables on-the-fly adaptation of CNNs to non-stationary data distributions.

Abstract: Training deep neural networks (DNNs) requires significantly more computation
and memory than inference, making runtime adaptation of DNNs challenging on
resource-limited IoT platforms. We propose InstantFT, an FPGA-based method for
ultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and
backward computations in parameter-efficient fine-tuning (PEFT). Experiments on
datasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained
CNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,
while achieving comparable accuracy. Our FPGA-based InstantFT reduces the
fine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,
enabling on-the-fly adaptation of CNNs to non-stationary data distributions.

</details>


### [110] [Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs](https://arxiv.org/abs/2506.06521)
*Shulun Chen,Runlong Zhou,Zihan Zhang,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: The paper explores the gap-dependent regret bounds for episodic MDPs using the Monotonic Value Propagation (MVP) algorithm, presenting a variance-aware regret bound and a lower bound that emphasizes the necessity of considering maximum conditional total variance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the understanding of regret bounds in episodic Markov Decision Processes (MDPs), particularly focusing on gap-dependent bounds which can offer more refined performance guarantees than worst-case scenarios. By incorporating the concept of maximum conditional total variance, the study aims to provide insights into how randomness affects learning specific state-action pairs.

Method: The method involves analyzing the Monotonic Value Propagation (MVP) algorithm within the context of episodic MDPs. The authors derive a variance-aware gap-dependent regret bound by evaluating the weighted sum of suboptimality gaps. Additionally, they establish a lower bound to complement their findings, highlighting the importance of the maximum conditional total variance.

Result: The result is a detailed regret bound expression that accounts for the suboptimality gap, the number of states and actions, planning horizon, and episodes. This bound demonstrates the impact of maximum conditional total variance on the learning process. The lower bound further supports the necessity of this variance consideration even when unconditional variance approaches zero.

Conclusion: The study concludes that considering maximum conditional total variance is crucial for achieving tighter regret bounds in episodic MDPs. The derived bounds not only refine our understanding of regret in such processes but also suggest potential improvements for other algorithms through similar analyses.

Abstract: We consider the gap-dependent regret bounds for episodic MDPs. We show that
the Monotonic Value Propagation (MVP) algorithm achieves a variance-aware
gap-dependent regret bound of $$\tilde{O}\left(\left(\sum_{\Delta_h(s,a)>0}
\frac{H^2 \log K \land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}
+\sum_{\Delta_h(s,a)=0}\frac{ H^2 \land
\mathtt{Var}_{\max}^{\text{c}}}{\Delta_{\mathrm{min}}} + SAH^4 (S \lor H)
\right) \log K\right),$$ where $H$ is the planning horizon, $S$ is the number
of states, $A$ is the number of actions, and $K$ is the number of episodes.
Here, $\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality
gap and $\Delta_{\mathrm{min}} := \min_{\Delta_h (s,a) > 0} \Delta_h(s,a)$. The
term $\mathtt{Var}_{\max}^{\text{c}}$ denotes the maximum conditional total
variance, calculated as the maximum over all $(\pi, h, s)$ tuples of the
expected total variance under policy $\pi$ conditioned on trajectories visiting
state $s$ at step $h$. $\mathtt{Var}_{\max}^{\text{c}}$ characterizes the
maximum randomness encountered when learning any $(h, s)$ pair. Our result
stems from a novel analysis of the weighted sum of the suboptimality gap and
can be potentially adapted for other algorithms. To complement the study, we
establish a lower bound of $$\Omega \left( \sum_{\Delta_h(s,a)>0} \frac{H^2
\land \mathtt{Var}_{\max}^{\text{c}}}{\Delta_h(s,a)}\cdot \log K\right),$$
demonstrating the necessity of dependence on $\mathtt{Var}_{\max}^{\text{c}}$
even when the maximum unconditional total variance (without conditioning on
$(h, s)$) approaches zero.

</details>


### [111] [Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks](https://arxiv.org/abs/2506.06532)
*Zijiang Yan,Hao Zhou,Jianhua Pei,Hina Tabassum*

Main category: cs.LG

TL;DR: The paper proposes a novel hierarchical and collaborative method based on large language models (LLMs) for the joint motion and communication control of multiple UAVs in integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions.


<details>
  <summary>Details</summary>
Motivation: Current studies face challenges in controlling and optimizing multi-UAV systems, especially in dynamic and constrained environments. The need for efficient motion and communication control in aerial highway scenarios prompts the exploration of new methods.

Method: A hierarchical and collaborative method using LLMs is proposed. An LLM on HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This approach integrates high-level strategic planning and low-level tactical decisions.

Result: Experimental results show that the proposed method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.

Conclusion: The LLM-based framework demonstrates great potential for the development of next-generation 3D aerial highway systems, providing an effective solution for UAV control and optimization.

Abstract: Unmanned aerial vehicles (UAVs) have been widely adopted in various
real-world applications. However, the control and optimization of multi-UAV
systems remain a significant challenge, particularly in dynamic and constrained
environments. This work explores the joint motion and communication control of
multiple UAVs operating within integrated terrestrial and non-terrestrial
networks that include high-altitude platform stations (HAPS). Specifically, we
consider an aerial highway scenario in which UAVs must accelerate, decelerate,
and change lanes to avoid collisions and maintain overall traffic flow.
Different from existing studies, we propose a novel hierarchical and
collaborative method based on large language models (LLMs). In our approach, an
LLM deployed on the HAPS performs UAV access control, while another LLM onboard
each UAV handles motion planning and control. This LLM-based framework
leverages the rich knowledge embedded in pre-trained models to enable both
high-level strategic planning and low-level tactical decisions. This
knowledge-driven paradigm holds great potential for the development of
next-generation 3D aerial highway systems. Experimental results demonstrate
that our proposed collaborative LLM-based method achieves higher system
rewards, lower operational costs, and significantly reduced UAV collision rates
compared to baseline approaches.

</details>


### [112] [GeoClip: Geometry-Aware Clipping for Differentially Private SGD](https://arxiv.org/abs/2506.06549)
*Atefeh Gilani,Naima Tasnim,Lalitha Sankar,Oliver Kosut*

Main category: cs.LG

TL;DR: GeoClip is a geometry-aware framework that clips and perturbs gradients in a transformed basis aligned with the gradient distribution's geometry, adaptively estimating this transformation using only previously released noisy gradients without additional privacy cost. It outperforms existing adaptive clipping methods under the same privacy budget.


<details>
  <summary>Details</summary>
Motivation: DP-SGD is widely used for training machine learning models with provable privacy guarantees, but setting the per-sample gradient clipping threshold significantly affects the privacy-utility trade-off. Current adaptive methods fail to account for correlations across the coordinates of the gradient.

Method: Propose GeoClip which clips and perturbs gradients in a transformed basis aligned with the geometry of the gradient distribution. Adaptively estimate this transformation using only previously released noisy gradients without incurring additional privacy cost.

Result: GeoClip consistently outperforms existing adaptive clipping methods on both tabular and image datasets under the same privacy budget.

Conclusion: GeoClip provides convergence guarantees and derives a closed-form solution for the optimal transformation that minimizes noise added while controlling the probability of gradient clipping.

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the most
widely used method for training machine learning models with provable privacy
guarantees. A key challenge in DP-SGD is setting the per-sample gradient
clipping threshold, which significantly affects the trade-off between privacy
and utility. While recent adaptive methods improve performance by adjusting
this threshold during training, they operate in the standard coordinate system
and fail to account for correlations across the coordinates of the gradient. We
propose GeoClip, a geometry-aware framework that clips and perturbs gradients
in a transformed basis aligned with the geometry of the gradient distribution.
GeoClip adaptively estimates this transformation using only previously released
noisy gradients, incurring no additional privacy cost. We provide convergence
guarantees for GeoClip and derive a closed-form solution for the optimal
transformation that minimizes the amount of noise added while keeping the
probability of gradient clipping under control. Experiments on both tabular and
image datasets demonstrate that GeoClip consistently outperforms existing
adaptive clipping methods under the same privacy budget.

</details>


### [113] [SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks](https://arxiv.org/abs/2506.06556)
*Long Dang,Thushari Hapuarachchi,Kaiqi Xiong,Yi Li*

Main category: cs.LG

TL;DR: The paper proposes a robust SDN-based False Data Detection and Mitigation System (FDDMS) for in-vehicle networks, which effectively detects and mitigates false data injection attacks in real-time while resisting adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: With the increasing complexity of modern vehicles due to autonomous and connected features, securing communication among numerous ECUs within the in-vehicle network is crucial for vehicle safety and security.

Method: The FDDMS system leverages SDN capabilities to monitor communications. It decodes raw CAN data to create an attack model, uses an LSTM-based detection model to identify false data injection attacks, evaluates robustness with a DeepFool variant, enhances re-training techniques with threshold-based selection, and implements a mitigation scheme through dynamic flow rule updates via SDN.

Result: Experimental results demonstrate that FDDMS is robust against various adversarial attacks and can effectively detect and mitigate false data injection attacks in real-time.

Conclusion: The proposed FDDMS provides a secure solution for in-vehicle networks by successfully detecting and mitigating false data injection attacks while maintaining resilience against adversarial attacks.

Abstract: As the development of autonomous and connected vehicles advances, the
complexity of modern vehicles increases, with numerous Electronic Control Units
(ECUs) integrated into the system. In an in-vehicle network, these ECUs
communicate with one another using an standard protocol called Controller Area
Network (CAN). Securing communication among ECUs plays a vital role in
maintaining the safety and security of the vehicle. This paper proposes a
robust SDN-based False Data Detection and Mitigation System (FDDMS) for
in-vehicle networks. Leveraging the unique capabilities of Software-Defined
Networking (SDN), FDDMS is designed to monitor and detect false data injection
attacks in real-time. Specifically, we focus on brake-related ECUs within an
SDN-enabled in-vehicle network. First, we decode raw CAN data to create an
attack model that illustrates how false data can be injected into the system.
Then, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection
model, is used to identify false data injection attacks. We further propose an
effective variant of DeepFool attack to evaluate the model's robustness. To
countermeasure the impacts of four adversarial attacks including Fast gradient
descent method, Basic iterative method, DeepFool, and the DeepFool variant, we
further enhance a re-training technique method with a threshold based selection
strategy. Finally, a mitigation scheme is implemented to redirect attack
traffic by dynamically updating flow rules through SDN. Our experimental
results show that the proposed FDDMS is robust against adversarial attacks and
effectively detects and mitigates false data injection attacks in real-time.

</details>


### [114] [Rapid training of Hamiltonian graph networks without gradient descent](https://arxiv.org/abs/2506.06558)
*Atamert Rahma,Chinmay Datar,Ana Cukarska,Felix Dietrich*

Main category: cs.LG

TL;DR: The paper shows that Hamiltonian Graph Networks (HGN) can be trained up to 600x faster by replacing iterative optimization with random feature-based parameter construction, while retaining physical invariances and generalizing well.


<details>
  <summary>Details</summary>
Motivation: Learning dynamical systems respecting physical symmetries and constraints is a fundamental challenge. Current methods using graph neural networks have limitations in training speed, particularly for large and complex systems.

Method: The method involves using Hamiltonian Graph Networks (HGN) and replacing iterative, gradient-based optimization algorithms with random feature-based parameter construction to accelerate training.

Result: This approach allows HGNs to be trained up to 600x faster compared to other optimizers, with comparable accuracy. The model also generalizes well from small 8-node systems to much larger 4096-node systems without retraining, while maintaining essential physical invariances.

Conclusion: This work challenges the dominance of iterative gradient-descent-based optimization algorithms for training neural network models for physical systems, showing a significant improvement in training speed and generalization.

Abstract: Learning dynamical systems that respect physical symmetries and constraints
remains a fundamental challenge in data-driven modeling. Integrating physical
laws with graph neural networks facilitates principled modeling of complex
N-body dynamics and yields accurate and permutation-invariant models. However,
training graph neural networks with iterative, gradient-based optimization
algorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,
especially for large, complex systems. In comparison to 15 different
optimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained
up to 600x faster--but with comparable accuracy--by replacing iterative
optimization with random feature-based parameter construction. We show robust
performance in diverse simulations, including N-body mass-spring systems in up
to 3 dimensions with different geometries, while retaining essential physical
invariances with respect to permutation, rotation, and translation. We reveal
that even when trained on minimal 8-node systems, the model can generalize in a
zero-shot manner to systems as large as 4096 nodes without retraining. Our work
challenges the dominance of iterative gradient-descent-based optimization
algorithms for training neural network models for physical systems.

</details>


### [115] [Graph Persistence goes Spectral](https://arxiv.org/abs/2506.06571)
*Mattie Ji,Amauri H. Souza,Vikas Garg*

Main category: cs.LG

TL;DR: SpectRe，一种新的图拓扑描述符，通过将谱信息融入持久同调图中，增强了图表示学习的表达能力，并且在实验中展现出有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的将顶点和边特征加入经典PH图以提高表达能力的方法，仍无法捕捉基本的图结构信息。

Method: 提出了一种新的图拓扑描述符SpectRe，它将谱信息整合到PH图中。此外，引入了全局和局部稳定性概念来分析现有描述符，并证明SpectRe具有局部稳定性。

Result: 在合成数据集和真实世界数据集上的实验表明，SpectRe的有效性及其增强图模型在相关学习任务中的能力的潜力。

Conclusion: SpectRe比现有的图描述符更具表达力，且具备局部稳定性，能够有效提升图模型的学习能力。

Abstract: Including intricate topological information (e.g., cycles) provably enhances
the expressivity of message-passing graph neural networks (GNNs) beyond the
Weisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods
are increasingly employed for graph representation learning. In this context,
recent works have proposed decorating classical PH diagrams with vertex and
edge features for improved expressivity. However, due to their dependence on
features, these methods still fail to capture basic graph structural
information. In this paper, we propose SpectRe -- a new topological descriptor
for graphs that integrates spectral information into PH diagrams. Notably,
SpectRe is strictly more expressive than existing descriptors on graphs. We
also introduce notions of global and local stability to analyze existing
descriptors and establish that SpectRe is locally stable. Finally, experiments
on synthetic and real-world datasets demonstrate the effectiveness of SpectRe
and its potential to enhance the capabilities of graph models in relevant
learning tasks.

</details>


### [116] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 尽管语言模型在自然语言处理领域取得了显著进展，但其推理过程仍然计算成本高昂且能耗大。本文探讨了两种提高LLM推理效率的策略：路由和级联/分层推理，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的进步，虽然在许多任务上表现出色，但其推理过程对计算资源的需求较大，难以在移动设备或成本敏感环境中部署。

Method: 本文介绍了两种策略：(i) 路由，根据查询选择最合适的模型；(ii) 级联或分层推理，通过一系列模型逐步处理查询直到得出可靠响应。

Result: 这些方法能够减少计算需求，通过使用轻量级模型处理简单任务并在必要时升级到更复杂的模型。

Conclusion: 未来研究将致力于实现更快的响应时间、基于任务复杂性的自适应模型选择以及跨异构环境的可扩展部署，以使基于LLM的系统更高效和实用。

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [117] [Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing](https://arxiv.org/abs/2506.06582)
*Diaaeldin Taha,James Chapman,Marzieh Eidi,Karel Devriendt,Guido Montúfar*

Main category: cs.LG

TL;DR: Topological deep learning (TDL) is a powerful tool, but phenomena like oversquashing in topological message-passing need more study. This paper proposes a unifying axiomatic framework to bridge graph and topological message-passing by viewing complexes through relational structures, extending graph-theoretic results to higher-order structures to help analyze and mitigate oversquashing.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in the understudied phenomena such as oversquashing in topological message-passing within Topological Deep Learning (TDL). The authors aim to address these gaps by proposing a new framework that can better model higher-order interactions in relational data.

Method: The method involves creating a unifying axiomatic framework that bridges graph and topological message-passing by interpreting simplicial and cellular complexes through relational structures. This allows for the extension of graph-theoretic results and algorithms to higher-order structures.

Result: Through theoretical analysis and empirical studies on simplicial networks, the proposed framework shows potential in advancing TDL by facilitating the analysis and mitigation of oversquashing in topological message-passing networks.

Conclusion: This paper concludes that the unifying axiomatic framework offers a promising direction for advancing Topological Deep Learning, particularly in addressing issues like oversquashing in topological message-passing.

Abstract: Topological deep learning (TDL) has emerged as a powerful tool for modeling
higher-order interactions in relational data. However, phenomena such as
oversquashing in topological message-passing remain understudied and lack
theoretical analysis. We propose a unifying axiomatic framework that bridges
graph and topological message-passing by viewing simplicial and cellular
complexes and their message-passing schemes through the lens of relational
structures. This approach extends graph-theoretic results and algorithms to
higher-order structures, facilitating the analysis and mitigation of
oversquashing in topological message-passing networks. Through theoretical
analysis and empirical studies on simplicial networks, we demonstrate the
potential of this framework to advance TDL.

</details>


### [118] [Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures](https://arxiv.org/abs/2506.06584)
*Mo Zhou,Weihang Xu,Maryam Fazel,Simon S. Du*

Main category: cs.LG

TL;DR: 在过参数化设置下，对于分离良好的高斯混合模型，随机初始化的梯度EM算法能够全局收敛到真实模型。这是首个超越特殊情形$m=2$的EM或梯度EM的全局收敛与恢复结果。


<details>
  <summary>Details</summary>
Motivation: 传统的精确参数化设置下的高斯混合模型学习中，尽管对EM算法进行了大量研究，但仅证明了$m=2$时的全局收敛性，而$m\geq 3$时EM算法无法恢复真实模型。因此，探索其他设置下的收敛性成为重要问题。

Method: 考虑过参数化设置，使用$n>m$个分量的模型拟合$m$-分量的真实GMM。通过引入Hermite多项式研究梯度EM的动力学特性，并利用张量分解刻画似然损失的几何景观，从而为梯度EM提供严格的全局收敛保证。

Result: 对于任意分离良好的一般位置GMM，只需轻微过参数化（$n = \Omega(m\log m)$），随机初始化的梯度EM算法能够以多项式速率全局收敛到真实模型，并且仅需多项式样本。

Conclusion: 这是首个针对$m>2$的EM或梯度EM算法的全局收敛与恢复结果，展示了过参数化设置的优势和潜力。

Abstract: Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine
learning, with the Expectation-Maximization (EM) algorithm and its popular
variant gradient EM being arguably the most widely used algorithms in practice.
In the exact-parameterized setting, where both the ground truth GMM and the
learning model have the same number of components $m$, a vast line of work has
aimed to establish rigorous recovery guarantees for EM. However, global
convergence has only been proven for the case of $m=2$, and EM is known to fail
to recover the ground truth when $m\geq 3$.
  In this paper, we consider the $\textit{over-parameterized}$ setting, where
the learning model uses $n>m$ components to fit an $m$-component ground truth
GMM. In contrast to the exact-parameterized case, we provide a rigorous global
convergence guarantee for gradient EM. Specifically, for any well separated
GMMs in general position, we prove that with only mild over-parameterization $n
= \Omega(m\log m)$, randomly initialized gradient EM converges globally to the
ground truth at a polynomial rate with polynomial samples. Our analysis
proceeds in two stages and introduces a suite of novel tools for Gaussian
Mixture analysis. We use Hermite polynomials to study the dynamics of gradient
EM and employ tensor decomposition to characterize the geometric landscape of
the likelihood loss. This is the first global convergence and recovery result
for EM or Gradient EM beyond the special case of $m=2$.

</details>


### [119] [Direct Prediction Set Minimization via Bilevel Conformal Classifier Training](https://arxiv.org/abs/2506.06599)
*Yuanjie Shi,Hooman Shahrokhi,Xuesong Jia,Xiongzhi Chen,Janardhan Rao Doppa,Yan Yan*

Main category: cs.LG

TL;DR: The paper proposes DPSM algorithm to minimize prediction set sizes during deep classifier training within the conformal prediction framework, showing significant improvement over previous methods.


<details>
  <summary>Details</summary>
Motivation: Conformal prediction (CP) is valuable for uncertainty quantification but often produces large prediction sets. The authors aim to integrate conformal principles into classifier training to reduce these set sizes.

Method: The method involves formulating conformal training as a bilevel optimization problem and introducing the DPSM algorithm, which minimizes prediction set size by conditioning on learned quantiles of conformity scores.

Result: DPSM demonstrates a learning bound of O(1/√n), superior to prior methods with bounds of Ω(1/s). Experiments show a 20.46% decrease in prediction set size compared to the best previous baseline.

Conclusion: DPSM effectively minimizes prediction set sizes in conformal prediction, outperforming existing techniques and validating the theoretical analysis.

Abstract: Conformal prediction (CP) is a promising uncertainty quantification framework
which works as a wrapper around a black-box classifier to construct prediction
sets (i.e., subset of candidate classes) with provable guarantees. However,
standard calibration methods for CP tend to produce large prediction sets which
makes them less useful in practice. This paper considers the problem of
integrating conformal principles into the training process of deep classifiers
to directly minimize the size of prediction sets. We formulate conformal
training as a bilevel optimization problem and propose the {\em Direct
Prediction Set Minimization (DPSM)} algorithm to solve it. The key insight
behind DPSM is to minimize a measure of the prediction set size (upper level)
that is conditioned on the learned quantile of conformity scores (lower level).
We analyze that DPSM has a learning bound of $O(1/\sqrt{n})$ (with $n$ training
samples), while prior conformal training methods based on stochastic
approximation for the quantile has a bound of $\Omega(1/s)$ (with batch size
$s$ and typically $s \ll \sqrt{n}$). Experiments on various benchmark datasets
and deep models show that DPSM significantly outperforms the best prior
conformal training baseline with $20.46\%\downarrow$ in the prediction set size
and validates our theory.

</details>


### [120] [CAtCh: Cognitive Assessment through Cookie Thief](https://arxiv.org/abs/2506.06603)
*Joseph T Colonel,Carolyn Hagler,Guiselle Wismer,Laura Curtis,Jacqueline Becker,Juan Wisnivesky,Alex Federman,Gaurav Pandey*

Main category: cs.LG

TL;DR: The paper evaluates speech-based and multimodal methods originally developed for predicting ADRD, applying them to predict CI from patient audio recordings. Multimodal methods outperform unimodal ones, and acoustics-based approaches surpass linguistics-based ones.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of existing machine learning algorithms designed for predicting ADRD in predicting broader cognitive impairment (CI), which is a precursor and risk factor of ADRD.

Method: Evaluate several open-source speech-based methods initially proposed for ADRD prediction and methods from multimodal sentiment analysis on the task of predicting CI from patient audio recordings.

Result: Multimodal methods perform better than unimodal methods for CI prediction. Acoustics-based approaches, particularly interpretable acoustic features relating to affect and prosody, significantly outperform BERT-based linguistic features and interpretable linguistic features.

Conclusion: Multimodal and acoustics-based approaches are more effective in predicting CI from patient audio recordings compared to unimodal and linguistics-based methods.

Abstract: Several machine learning algorithms have been developed for the prediction of
Alzheimer's disease and related dementia (ADRD) from spontaneous speech.
However, none of these algorithms have been translated for the prediction of
broader cognitive impairment (CI), which in some cases is a precursor and risk
factor of ADRD. In this paper, we evaluated several speech-based open-source
methods originally proposed for the prediction of ADRD, as well as methods from
multimodal sentiment analysis for the task of predicting CI from patient audio
recordings. Results demonstrated that multimodal methods outperformed unimodal
ones for CI prediction, and that acoustics-based approaches performed better
than linguistics-based ones. Specifically, interpretable acoustic features
relating to affect and prosody were found to significantly outperform
BERT-based linguistic features and interpretable linguistic features,
respectively. All the code developed for this study is available at
https://github.com/JTColonel/catch.

</details>


### [121] [Stacey: Promoting Stochastic Steepest Descent via Accelerated $\ell_p$-Smooth Nonconvex Optimization](https://arxiv.org/abs/2506.06606)
*Xinyu Luo,Cedar Site Bai,Bolian Li,Petros Drineas,Ruqi Zhang,Brian Bullins*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While popular optimization methods such as SGD, AdamW, and Lion depend on
steepest descent updates in either $\ell_2$ or $\ell_\infty$ norms, there
remains a critical gap in handling the non-Euclidean structure observed in
modern deep networks training. In this work, we address this need by
introducing a new accelerated $\ell_p$ steepest descent algorithm, called
Stacey, which uses interpolated primal-dual iterate sequences to effectively
navigate non-Euclidean smooth optimization tasks. In addition to providing
novel theoretical guarantees for the foundations of our algorithm, we
empirically compare our approach against these popular methods on tasks
including image classification and language model (LLM) pretraining,
demonstrating both faster convergence and higher final accuracy. We further
evaluate different values of $p$ across various models and datasets,
underscoring the importance and efficiency of non-Euclidean approaches over
standard Euclidean methods. Code can be found at
https://github.com/xinyuluo8561/Stacey .

</details>


### [122] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: An E2H Reasoner method inspired by curriculum learning is proposed to enhance the reasoning capabilities of small LLMs through RL, showing significant improvements in reasoning ability.


<details>
  <summary>Details</summary>
Motivation: To improve the reasoning capabilities of language models via reinforcement learning, addressing the limitation that using RL alone is less effective for inherently difficult tasks.

Method: Propose a method called E2H Reasoner which schedules tasks from easy to hard, allowing LLMs to build reasoning skills gradually and preventing overfitting through appropriate scheduling.

Result: Experiments show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B) across multiple domains compared to training with vanilla RL alone.

Conclusion: E2H Reasoner is an effective method for enhancing the reasoning capabilities of small LLMs through RL, requiring fewer total samples than direct learning when tasks are appropriately decomposed and conditioned.

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [123] [Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification](https://arxiv.org/abs/2506.06633)
*Chi-Sheng Chen*

Main category: cs.LG

TL;DR: Recent advancements in quantum machine learning have shown promise in enhancing classical neural network architectures, especially for complex, high-dimensional data. This paper introduces Vision-QRWKV, a hybrid quantum-classical extension of the RWKV architecture for image classification tasks. Results show that the quantum-enhanced model outperforms its classical counterpart on many datasets.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of quantum machine learning in improving classical neural network architectures for complex, high-dimensional data, particularly in image classification tasks.

Method: Introduce Vision-QRWKV, a hybrid quantum-classical extension of the RWKV architecture, by integrating a variational quantum circuit (VQC) into the channel mixing component of RWKV to improve nonlinear feature transformation and enhance visual representations.

Result: The quantum-enhanced model outperforms its classical counterpart on a majority of datasets, particularly those with subtle or noisy class distinctions.

Conclusion: This study represents the first systematic application of quantum-enhanced RWKV in the visual domain, providing insights into the trade-offs and future potential of quantum models for lightweight and efficient vision tasks.

Abstract: Recent advancements in quantum machine learning have shown promise in
enhancing classical neural network architectures, particularly in domains
involving complex, high-dimensional data. Building upon prior work in temporal
sequence modeling, this paper introduces Vision-QRWKV, a hybrid
quantum-classical extension of the Receptance Weighted Key Value (RWKV)
architecture, applied for the first time to image classification tasks. By
integrating a variational quantum circuit (VQC) into the channel mixing
component of RWKV, our model aims to improve nonlinear feature transformation
and enhance the expressive capacity of visual representations.
  We evaluate both classical and quantum RWKV models on a diverse collection of
14 medical and standard image classification benchmarks, including MedMNIST
datasets, MNIST, and FashionMNIST. Our results demonstrate that the
quantum-enhanced model outperforms its classical counterpart on a majority of
datasets, particularly those with subtle or noisy class distinctions (e.g.,
ChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first
systematic application of quantum-enhanced RWKV in the visual domain, offering
insights into the architectural trade-offs and future potential of quantum
models for lightweight and efficient vision tasks.

</details>


### [124] [Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning](https://arxiv.org/abs/2506.06637)
*Olimjon Toirov,Wei Yu*

Main category: cs.LG

TL;DR: The paper proposes a new NILM method integrating 'image load signature' and continual learning to improve recognition accuracy in device identification and classification.


<details>
  <summary>Details</summary>
Motivation: Traditional NILM methods suffer from poor feature robustness and insufficient model generalization due to complex and changeable load combinations and application environments.

Method: Convert multi-dimensional power signals into visual image load feature signatures, use deep convolutional neural networks for device identification and classification, introduce self-supervised pre-training for better feature generalization, and apply continual online learning strategies to overcome model forgetting.

Result: The method significantly improves recognition accuracy when tested on high-sampling rate load datasets compared to existing methods and model variants.

Conclusion: The proposed NILM method that combines 'image load signature' and continual learning enhances the robustness and generalization of features, leading to significant improvements in recognition accuracy.

Abstract: Non-Intrusive Load Monitoring (NILM) identifies the operating status and
energy consumption of each electrical device in the circuit by analyzing the
electrical signals at the bus, which is of great significance for smart power
management. However, the complex and changeable load combinations and
application environments lead to the challenges of poor feature robustness and
insufficient model generalization of traditional NILM methods. To this end,
this paper proposes a new non-intrusive load monitoring method that integrates
"image load signature" and continual learning. This method converts
multi-dimensional power signals such as current, voltage, and power factor into
visual image load feature signatures, and combines deep convolutional neural
networks to realize the identification and classification of multiple devices;
at the same time, self-supervised pre-training is introduced to improve feature
generalization, and continual online learning strategies are used to overcome
model forgetting to adapt to the emergence of new loads. This paper conducts a
large number of experiments on high-sampling rate load datasets, and compares a
variety of existing methods and model variants. The results show that the
proposed method has achieved significant improvements in recognition accuracy.

</details>


### [125] [Spark Transformer: Reactivating Sparsity in FFN and Attention](https://arxiv.org/abs/2506.06644)
*Chong You,Kan Wu,Zhipeng Jia,Lin Chen,Srinadh Bhojanapalli,Jiaxian Guo,Utku Evci,Jan Wassenberg,Praneeth Netrapalli,Jeremiah J. Willcock,Suvinay Subramanian,Felix Chern,Alek Andreev,Shreya Pathak,Felix Yu,Prateek Jain,David E. Culler,Henry M. Levy,Sanjiv Kumar*

Main category: cs.LG

TL;DR: This paper introduces Spark Transformer, a novel architecture achieving high activation sparsity in both FFN and attention mechanism without degrading model quality or complicating training. It uses top-k masking for sparsity control and statistical top-k for efficient hardware acceleration. Pretrained with Gemma-2 recipe, it shows competitive performance with significant sparsity (only 8% FFN neurons activated) leading to FLOPs reduction and decoding speedups.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of maintaining model quality, parameter count, and standard training procedures while introducing activation sparsity in Transformers that have moved away from ReLU activations.

Method: Spark Transformer realizes sparsity via top-k masking for explicit control over sparsity level and introduces statistical top-k, a linear-time approximate algorithm avoiding costly sorting. It reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor identifying activated entries.

Result: Demonstrates competitive performance on benchmarks with significant sparsity (only 8% FFN neurons activated), leading to a 2.5x reduction in FLOPs and decoding wall-time speedups of up to 1.79x on CPU and 1.40x on GPU.

Conclusion: Spark Transformer successfully achieves high activation sparsity in both FFN and attention mechanism without degrading model quality, increasing parameter count, or complicating training.

Abstract: The discovery of the lazy neuron phenomenon in trained Transformers, where
the vast majority of neurons in their feed-forward networks (FFN) are inactive
for each token, has spurred tremendous interests in activation sparsity for
enhancing large model efficiency. While notable progress has been made in
translating such sparsity to wall-time benefits, modern Transformers have moved
away from the ReLU activation function crucial to this phenomenon. Existing
efforts on re-introducing activation sparsity often degrade model quality,
increase parameter count, complicate or slow down training. Sparse attention,
the application of sparse activation to the attention mechanism, often faces
similar challenges.
  This paper introduces the Spark Transformer, a novel architecture that
achieves a high level of activation sparsity in both FFN and the attention
mechanism while maintaining model quality, parameter count, and standard
training procedures. Our method realizes sparsity via top-k masking for
explicit control over sparsity level. Crucially, we introduce statistical
top-k, a hardware-accelerator-friendly, linear-time approximate algorithm that
avoids costly sorting and mitigates significant training slowdown from standard
top-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN
parameters and attention key embeddings to form a low-cost predictor for
identifying activated entries. This design not only mitigates quality loss from
enforced sparsity, but also enhances wall-time benefit. Pretrained with the
Gemma-2 recipe, Spark Transformer demonstrates competitive performance on
standard benchmarks while exhibiting significant sparsity: only 8% of FFN
neurons are activated, and each token attends to a maximum of 256 tokens. This
sparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time
speedups of up to 1.79x on CPU and 1.40x on GPU.

</details>


### [126] [SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes](https://arxiv.org/abs/2506.06649)
*Yishan Shen,Yuyang Ye,Hui Xiong,Yong Chen*

Main category: cs.LG

TL;DR: The paper proposes SAFER, a risk-aware framework for dynamic treatment regimes (DTRs) that integrates structured EHR data and clinical notes, uses conformal prediction for safe recommendations, and demonstrates superior performance on sepsis datasets.


<details>
  <summary>Details</summary>
Motivation: Dynamic treatment regimes are essential in precision medicine but require careful supervision due to potential unsafe treatment risks. Current methods rely heavily on clinician-prescribed standards and structured EHR data, ignoring valuable insights from clinical notes and introducing limitations in reliability of treatment recommendations.

Method: SAFER is introduced as a calibrated risk-aware tabular-language recommendation framework for DTRs. It integrates structured EHR data with clinical notes to allow mutual learning, addresses label uncertainty by assuming ambiguous optimal treatment solutions for deceased patients, and employs conformal prediction to ensure statistical guarantees and filter out uncertain predictions.

Result: Experiments on two publicly available sepsis datasets show that SAFER surpasses state-of-the-art baselines across multiple recommendation metrics and counterfactual mortality rate, providing robust formal assurances.

Conclusion: SAFER shows promise as a reliable and theoretically grounded solution for high-stakes DTR applications.

Abstract: Dynamic treatment regimes (DTRs) are critical to precision medicine,
optimizing long-term outcomes through personalized, real-time decision-making
in evolving clinical contexts, but require careful supervision for unsafe
treatment risks. Existing efforts rely primarily on clinician-prescribed gold
standards despite the absence of a known optimal strategy, and predominantly
using structured EHR data without extracting valuable insights from clinical
notes, limiting their reliability for treatment recommendations. In this work,
we introduce SAFER, a calibrated risk-aware tabular-language recommendation
framework for DTR that integrates both structured EHR and clinical notes,
enabling them to learn from each other, and addresses inherent label
uncertainty by assuming ambiguous optimal treatment solution for deceased
patients. Moreover, SAFER employs conformal prediction to provide statistical
guarantees, ensuring safe treatment recommendations while filtering out
uncertain predictions. Experiments on two publicly available sepsis datasets
demonstrate that SAFER outperforms state-of-the-art baselines across multiple
recommendation metrics and counterfactual mortality rate, while offering robust
formal assurances. These findings underscore SAFER potential as a trustworthy
and theoretically grounded solution for high-stakes DTR applications.

</details>


### [127] [Rescaled Influence Functions: Accurate Data Attribution in High Dimension](https://arxiv.org/abs/2506.06656)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

TL;DR: The paper proposes rescaled influence functions (RIF) to improve the accuracy of data attribution compared to traditional influence functions (IF), demonstrating its effectiveness through experiments and theoretical analysis.


<details>
  <summary>Details</summary>
Motivation: To better understand how training data affects model behavior, particularly addressing the shortcomings of existing influence functions which can be imprecise and underestimate sample removal effects in high-dimensional settings.

Method: Introduced rescaled influence functions (RIF) as an enhancement to traditional influence functions. RIF offers a drop-in replacement with minimal computational overhead but improved accuracy for predicting the effect of removing samples from the training set.

Result: RIFs provided significantly better predictions than IFs across various real-world datasets. A theoretical analysis supported these findings, and RIF was shown to detect certain data poisoning attacks that would fool IF-based detections.

Conclusion: Rescaled influence functions provide a more accurate and reliable tool for data attribution, offering improvements over traditional influence functions in both practical applications and theoretical robustness.

Abstract: How does the training data affect a model's behavior? This is the question we
seek to answer with data attribution. The leading practical approaches to data
attribution are based on influence functions (IF). IFs utilize a first-order
Taylor approximation to efficiently predict the effect of removing a set of
samples from the training set without retraining the model, and are used in a
wide variety of machine learning applications. However, especially in the
high-dimensional regime (# params $\geq \Omega($# samples$)$), they are often
imprecise and tend to underestimate the effect of sample removals, even for
simple models such as logistic regression. We present rescaled influence
functions (RIF), a new tool for data attribution which can be used as a drop-in
replacement for influence functions, with little computational overhead but
significant improvement in accuracy. We compare IF and RIF on a range of
real-world datasets, showing that RIFs offer significantly better predictions
in practice, and present a theoretical analysis explaining this improvement.
Finally, we present a simple class of data poisoning attacks that would fool
IF-based detections but would be detected by RIF.

</details>


### [128] [Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning](https://arxiv.org/abs/2506.06694)
*Yuan Yuan,Yukun Liu,Chonghua Han,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: MoveGCL是一个可扩展且保护隐私的框架，通过生成式持续学习训练移动性基础模型，在不共享原始数据的情况下实现去中心化和渐进式的模型进化。


<details>
  <summary>Details</summary>
Motivation: 构建类似的基础模型用于人类移动性具有挑战性，因为移动性数据的隐私敏感性和机构间的数据孤岛问题。

Method: MoveGCL通过重放由冻结教师模型生成的合成轨迹实现去中心化和渐进式的模型进化，并通过定制的知识蒸馏策略强化知识保留；结合Mixture-of-Experts Transformer和移动感知专家路由机制解决移动模式的异质性；采用逐层渐进适应策略稳定持续更新。

Result: 在六个真实世界的城市数据集上的实验表明，MoveGCL实现了与联合训练相当的性能，并显著优于联邦学习基线，同时提供强大的隐私保护。

Conclusion: MoveGCL为解锁移动性基础模型迈出了关键一步，提供了在基础模型时代开放、可扩展和保护隐私的模型开发的实际蓝图。

Abstract: Foundation models have revolutionized fields such as natural language
processing and computer vision by enabling general-purpose learning across
diverse tasks and datasets. However, building analogous models for human
mobility remains challenging due to the privacy-sensitive nature of mobility
data and the resulting data silos across institutions. To bridge this gap, we
propose MoveGCL, a scalable and privacy-preserving framework for training
mobility foundation models via generative continual learning. Without sharing
raw data, MoveGCL enables decentralized and progressive model evolution by
replaying synthetic trajectories generated from a frozen teacher model, and
reinforces knowledge retention through a tailored distillation strategy that
mitigates catastrophic forgetting. To address the heterogeneity of mobility
patterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a
mobility-aware expert routing mechanism, and employs a layer-wise progressive
adaptation strategy to stabilize continual updates. Experiments on six
real-world urban datasets demonstrate that MoveGCL achieves performance
comparable to joint training and significantly outperforms federated learning
baselines, while offering strong privacy protection. MoveGCL marks a crucial
step toward unlocking foundation models for mobility, offering a practical
blueprint for open, scalable, and privacy-preserving model development in the
era of foundation models.

</details>


### [129] [SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming](https://arxiv.org/abs/2506.06665)
*Hong-Ming Chiu,Hao Chen,Huan Zhang,Richard Y. Zhang*

Main category: cs.LG

TL;DR: 提出SDP-CROWN，结合SDP松弛的紧密性与传播验证器的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 神经网络验证器在处理大规模模型时表现不同，线性传播验证器在神经元耦合关键时可能宽松，而半定规划（SDP）验证器虽能自然捕捉神经元间耦合，但复杂度限制其仅适用于小模型。

Method: 提出SDP-CROWN框架，通过新的线性边界（基于SDP原理），在每层只增加一个额外参数的情况下明确捕捉基于ℓ2-范数的神经元间耦合，并可无缝集成到任何线性边界传播管道中。

Result: 理论上证明了该方法的神经元间边界比传统单神经元边界最多可紧致√n倍；实践中，在高达65000个神经元和247万个参数的大规模模型上，显著提高了α-CROWN验证器的性能，接近昂贵的SDP方法的紧密性。

Conclusion: SDP-CROWN保留了线性边界传播方法的可扩展性，同时显著提升了紧密性，适用于大规模模型的验证。

Abstract: Neural network verifiers based on linear bound propagation scale impressively
to massive models but can be surprisingly loose when neuron coupling is
crucial. Conversely, semidefinite programming (SDP) verifiers capture
inter-neuron coupling naturally, but their cubic complexity restricts them to
only small models. In this paper, we propose SDP-CROWN, a novel hybrid
verification framework that combines the tightness of SDP relaxations with the
scalability of bound-propagation verifiers. At the core of SDP-CROWN is a new
linear bound, derived via SDP principles, that explicitly captures
$\ell_{2}$-norm-based inter-neuron coupling while adding only one extra
parameter per layer. This bound can be integrated seamlessly into any linear
bound-propagation pipeline, preserving the inherent scalability of such methods
yet significantly improving tightness. In theory, we prove that our
inter-neuron bound can be up to a factor of $\sqrt{n}$ tighter than traditional
per-neuron bounds. In practice, when incorporated into the state-of-the-art
$\alpha$-CROWN verifier, we observe markedly improved verification performance
on large models with up to 65 thousand neurons and 2.47 million parameters,
achieving tightness that approaches that of costly SDP-based methods.

</details>


### [130] [Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering](https://arxiv.org/abs/2506.06666)
*Oktay Karakuş,Hasan Arkadaş*

Main category: cs.LG

TL;DR: 本研究提出了一种基于聚类的非监督框架，用于使用同步事件和追踪数据检测和分析足球比赛中的线破传球（LBPs），并引入了几个战术指标来量化LBPs的效果。


<details>
  <summary>Details</summary>
Motivation: 线破传球（LBPs）是足球中关键的战术动作，使球队能够穿透防守线并进入高价值空间。因此，研究这些传球如何破坏对方防守结构以及如何产生进攻威胁是非常重要的。

Method: 通过垂直空间分割建模对手团队形状，并识别比赛中破坏防守线的传球。此外，还引入了几个战术度量标准，包括空间构建比率（SBR）和两个链式变体LBPCh$^1$和LBPCh$^2$，以量化LBPs在产生即时或持续进攻威胁方面的有效性。

Result: 通过对2022年FIFA世界杯中的球队和球员进行评估，揭示了在垂直推进和结构破坏方面的风格差异。

Conclusion: 所提出的方法具有可解释性、可扩展性，并且可以直接应用于现代绩效分析和球探工作流程。

Abstract: Line-breaking passes (LBPs) are crucial tactical actions in football,
allowing teams to penetrate defensive lines and access high-value spaces. In
this study, we present an unsupervised, clustering-based framework for
detecting and analysing LBPs using synchronised event and tracking data from
elite matches. Our approach models opponent team shape through vertical spatial
segmentation and identifies passes that disrupt defensive lines within open
play. Beyond detection, we introduce several tactical metrics, including the
space build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and
LBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or
sustained attacking threats. We evaluate these metrics across teams and players
in the 2022 FIFA World Cup, revealing stylistic differences in vertical
progression and structural disruption. The proposed methodology is explainable,
scalable, and directly applicable to modern performance analysis and scouting
workflows.

</details>


### [131] [Differentially Private Sparse Linear Regression with Heavy-tailed Responses](https://arxiv.org/abs/2506.06861)
*Xizhi Tian,Meng Ding,Touming Tao,Zihang Xiang,Di Wang*

Main category: cs.LG

TL;DR: 本文研究了高维重尾响应下的差分隐私稀疏线性回归问题，提出了两种方法DP-IHT-H和DP-IHT-L，并通过实验验证其优于现有标准差分隐私算法。


<details>
  <summary>Details</summary>
Motivation: 现有的差分隐私线性回归方法主要关注常规数据分布或低维不规则数据情况，对于高维重尾响应的稀疏线性回归问题缺乏深入研究。

Method: 1. 提出了DP-IHT-H方法，结合Huber损失函数和私有迭代硬阈值处理技术；2. 提出了改进的DP-IHT-L方法，在额外假设条件下进一步优化误差边界。

Result: 1. DP-IHT-H方法在(ε, δ)-差分隐私模型下实现了特定形式的估计误差边界；2. DP-IHT-L方法在去除尾参数ζ的影响后，进一步优化了误差边界；3. 实验结果表明，这两种方法在合成数据集和真实世界数据集上的表现优于现有标准差分隐私算法。

Conclusion: 本文针对高维重尾响应下的差分隐私稀疏线性回归问题进行了全面研究，所提出的方法有效改善了估计误差，并且在实际应用中表现出色。

Abstract: As a fundamental problem in machine learning and differential privacy (DP),
DP linear regression has been extensively studied. However, most existing
methods focus primarily on either regular data distributions or low-dimensional
cases with irregular data. To address these limitations, this paper provides a
comprehensive study of DP sparse linear regression with heavy-tailed responses
in high-dimensional settings. In the first part, we introduce the DP-IHT-H
method, which leverages the Huber loss and private iterative hard thresholding
to achieve an estimation error bound of \(
  \tilde{O}\biggl(
  s^{* \frac{1 }{2}}
  \cdot \biggl(\frac{\log d}{n}\biggr)^{\frac{\zeta}{1 + \zeta}}
  +
  s^{* \frac{1 + 2\zeta}{2 + 2\zeta}}
  \cdot \biggl(\frac{\log^2 d}{n \varepsilon}\biggr)^{\frac{\zeta}{1 + \zeta}}
  \biggr) \) under the $(\varepsilon, \delta)$-DP model, where $n$ is the
sample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,
and $\zeta \in (0, 1]$ characterizes the tail heaviness of the data. In the
second part, we propose DP-IHT-L, which further improves the error bound under
additional assumptions on the response and achieves \(
  \tilde{O}\Bigl(\frac{(s^*)^{3/2} \log d}{n \varepsilon}\Bigr). \) Compared to
the first result, this bound is independent of the tail parameter $\zeta$.
Finally, through experiments on synthetic and real-world datasets, we
demonstrate that our methods outperform standard DP algorithms designed for
``regular'' data.

</details>


### [132] [Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics](https://arxiv.org/abs/2506.06682)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: This paper presents HetCRF, a dual-channel self-supervised learning framework for heterogeneous graphs that combines masked autoencoders (MAE) and contrastive learning (CL). It addresses semantic sparsity and gradient imbalance issues, demonstrating superior performance in node classification tasks.


<details>
  <summary>Details</summary>
Motivation: Existing hybrid frameworks for homogeneous graphs face challenges in designing shared encoders to meet the semantic requirements of both MAE and CL. In semantically sparse scenarios, CL struggles with view construction and gradient imbalance between positive and negative samples persists.

Method: HetCRF uses a two-stage aggregation strategy to adapt embedding semantics, making it compatible with both MAE and CL. To address semantic sparsity, it enhances encoder output for view construction instead of relying on raw features. Two positive sample augmentation strategies are also proposed to balance gradient contributions.

Result: Node classification experiments on four real-world heterogeneous graph datasets demonstrate that HetCRF outperforms state-of-the-art baselines. On datasets with missing node features, such as Aminer and Freebase, at a 40% label rate in node classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2% respectively compared to the second-best baseline.

Conclusion: HetCRF is an effective and superior dual-channel self-supervised learning framework for heterogeneous graphs that addresses existing challenges in combining MAE and CL.

Abstract: In graph self-supervised learning, masked autoencoders (MAE) and contrastive
learning (CL) are two prominent paradigms. MAE focuses on reconstructing masked
elements, while CL maximizes similarity between augmented graph views. Recent
studies highlight their complementarity: MAE excels at local feature capture,
and CL at global information extraction. Hybrid frameworks for homogeneous
graphs have been proposed, but face challenges in designing shared encoders to
meet the semantic requirements of both tasks. In semantically sparse scenarios,
CL struggles with view construction, and gradient imbalance between positive
and negative samples persists. This paper introduces HetCRF, a novel
dual-channel self-supervised learning framework for heterogeneous graphs.
HetCRF uses a two-stage aggregation strategy to adapt embedding semantics,
making it compatible with both MAE and CL. To address semantic sparsity, it
enhances encoder output for view construction instead of relying on raw
features, improving efficiency. Two positive sample augmentation strategies are
also proposed to balance gradient contributions. Node classification
experiments on four real-world heterogeneous graph datasets demonstrate that
HetCRF outperforms state-of-the-art baselines. On datasets with missing node
features, such as Aminer and Freebase, at a 40% label rate in node
classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%
respectively compared to the second-best baseline, validating its effectiveness
and superiority.

</details>


### [133] [Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?](https://arxiv.org/abs/2506.06891)
*Paulius Sasnauskas,Yiğit Yalın,Goran Radanović*

Main category: cs.LG

TL;DR: The paper proposes AT-DPT to enhance corruption-robustness of ICRL against reward poisoning attacks.


<details>
  <summary>Details</summary>
Motivation: To improve the robustness of Decision-Pretrained Transformer (DPT) in in-context reinforcement learning (ICRL) against reward poisoning attacks.

Method: A novel adversarial training framework named Adversarially Trained Decision-Pretrained Transformer (AT-DPT) is introduced. It trains an attacker to minimize DPT's true reward by poisoning environment rewards, while simultaneously training a DPT model to infer optimal actions from the poisoned data.

Result: AT-DPT significantly outperforms standard bandit algorithms and robust baselines designed for reward contamination in both bandit settings and MDP settings, even under learned and adaptive attackers.

Conclusion: AT-DPT enhances the corruption-robustness of ICRL, with its effectiveness demonstrated across various environments.

Abstract: We study the corruption-robustness of in-context reinforcement learning
(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,
2023). To address the challenge of reward poisoning attacks targeting the DPT,
we propose a novel adversarial training framework, called Adversarially Trained
Decision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an
attacker to minimize the true reward of the DPT by poisoning environment
rewards, and a DPT model to infer optimal actions from the poisoned data. We
evaluate the effectiveness of our approach against standard bandit algorithms,
including robust baselines designed to handle reward contamination. Our results
show that the proposed method significantly outperforms these baselines in
bandit settings, under a learned attacker. We additionally evaluate AT-DPT on
an adaptive attacker, and observe similar results. Furthermore, we extend our
evaluation to the MDP setting, confirming that the robustness observed in
bandit scenarios generalizes to more complex environments.

</details>


### [134] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: MarginSel is a two-step method for selecting hard demonstration examples for LLMs' ICL prompt, achieving 2-7% F1-score improvement in classification tasks.


<details>
  <summary>Details</summary>
Motivation: The effectiveness of ICL in LLMs is sensitive to the selection and ordering of demonstration examples.

Method: MarginSel: Max-Margin Demonstration Selection for LLMs, selects hard demonstration examples adapting to each test instance.

Result: Achieves 2-7% absolute improvement in F1-score across classification tasks compared to random selection.

Conclusion: MarginSel induces max-margin behavior in LLMs by effectively increasing the margin for hard examples, shifting the decision boundary beneficially.

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [135] [Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry](https://arxiv.org/abs/2506.06933)
*Mahdi Salmani,Alireza Abdollahpoorrostam,Seyed-Mohsen Moosavi-Dezfooli*

Main category: cs.LG

TL;DR: The paper proposes a framework for decision-based attacks under asymmetric query costs, including Asymmetric Search and Asymmetric Gradient Estimation, which can lower total query cost and perturbations.


<details>
  <summary>Details</summary>
Motivation: Traditional decision-based black-box adversarial attacks aim to generate adversarial examples with low query numbers, but they assume equal query costs. In practice, queries may incur asymmetric costs. Current algorithms for this scenario are underdeveloped.

Method: The authors modify two core components of existing attacks: the search strategy and the gradient estimation process. They propose Asymmetric Search (AS), a conservative variant of binary search reducing reliance on high-cost queries, and Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution to favor low-cost queries. These methods balance different query types to minimize total attack cost.

Result: Through theoretical analysis and empirical evaluation on standard image classification benchmarks, the proposed method consistently achieves lower total query cost and smaller perturbations than existing approaches, with improvements up to 40% in some settings.

Conclusion: The proposed framework for decision-based attacks under asymmetric query costs successfully reduces total attack cost and perturbations. It can be integrated into various existing black-box attacks with minimal changes.

Abstract: Traditional decision-based black-box adversarial attacks on image classifiers
aim to generate adversarial examples by slightly modifying input images while
keeping the number of queries low, where each query involves sending an input
to the model and observing its output. Most existing methods assume that all
queries have equal cost. However, in practice, queries may incur asymmetric
costs; for example, in content moderation systems, certain output classes may
trigger additional review, enforcement, or penalties, making them more costly
than others. While prior work has considered such asymmetric cost settings,
effective algorithms for this scenario remain underdeveloped. In this paper, we
propose a general framework for decision-based attacks under asymmetric query
costs, which we refer to as asymmetric black-box attacks. We modify two core
components of existing attacks: the search strategy and the gradient estimation
process. Specifically, we propose Asymmetric Search (AS), a more conservative
variant of binary search that reduces reliance on high-cost queries, and
Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution
to favor low-cost queries. We design efficient algorithms that minimize total
attack cost by balancing different query types, in contrast to earlier methods
such as stealthy attacks that focus only on limiting expensive (high-cost)
queries. Our method can be integrated into a range of existing black-box
attacks with minimal changes. We perform both theoretical analysis and
empirical evaluation on standard image classification benchmarks. Across
various cost regimes, our method consistently achieves lower total query cost
and smaller perturbations than existing approaches, with improvements of up to
40% in some settings.

</details>


### [136] [Do Protein Transformers Have Biological Intelligence?](https://arxiv.org/abs/2506.06701)
*Fudong Lin,Wanrou Du,Jinchan Liu,Tarikul Milon,Shelby Meche,Wu Xu,Xiaoqi Qin,Xu Yuan*

Main category: cs.LG

TL;DR: This paper explores if Protein Transformers can capture biological intelligence in protein sequences, introduces a new dataset Protein-FN, devises a new Transformer architecture SPT, and develops a XAI technique Sequence Score. Even the smallest model SPT-Tiny shows high predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore whether Protein Transformers can capture biological intelligence among protein sequences.

Method: Introduced Protein-FN dataset with over 9000 protein data, devised a new Transformer architecture called Sequence Protein Transformers (SPT), and developed a novel XAI technique called Sequence Score.

Result: The smallest model SPT-Tiny achieved 94.3% accuracy on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all trained from scratch. Sequence Score revealed meaningful patterns in protein sequence structures aligned with biological domain knowledge.

Conclusion: Protein Transformers can effectively capture biological intelligence. The newly introduced dataset, architecture, and XAI technique demonstrate promising results in protein function prediction and interpretation.

Abstract: Deep neural networks, particularly Transformers, have been widely adopted for
predicting the functional properties of proteins. In this work, we focus on
exploring whether Protein Transformers can capture biological intelligence
among protein sequences. To achieve our goal, we first introduce a protein
function dataset, namely Protein-FN, providing over 9000 protein data with
meaningful labels. Second, we devise a new Transformer architecture, namely
Sequence Protein Transformers (SPT), for computationally efficient protein
function predictions. Third, we develop a novel Explainable Artificial
Intelligence (XAI) technique called Sequence Score, which can efficiently
interpret the decision-making processes of protein models, thereby overcoming
the difficulty of deciphering biological intelligence bided in Protein
Transformers. Remarkably, even our smallest SPT-Tiny model, which contains only
5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%
on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,
all accomplished by training from scratch. Besides, our Sequence Score
technique helps reveal that our SPT models can discover several meaningful
patterns underlying the sequence structures of protein data, with these
patterns aligning closely with the domain knowledge in the biology community.
We have officially released our Protein-FN dataset on Hugging Face Datasets
https://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at
https://github.com/fudong03/BioIntelligence.

</details>


### [137] [A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks](https://arxiv.org/abs/2506.06715)
*Minh-Duc Nguyen,Dung D. Le*

Main category: cs.LG

TL;DR: The paper proposes SVH-MOL, a method using Stein Variational Gradient Descent (SVGD) to approximate the Pareto set in Multi-objective Learning, ensuring diverse solutions and maximized hypervolume. It's validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to make Pareto solutions diverse while maximizing the hypervolume value in Multi-objective Learning.

Method: Proposes SVH-MOL which uses SVGD to push particles towards the Pareto set with functional gradient descent, employing diverse gradient direction strategies within a unified framework adapted with an annealing schedule for stability.

Result: Demonstrates superior performance in extensive experiments on multi-objective problems and multi-task learning.

Conclusion: SVH-MOL effectively addresses the challenge of diversity and hypervolume maximization in Pareto solutions.

Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining
the complete optimal solution in Multi-objective Learning (MOL). A set of
optimal solutions approximates the Pareto set, and its mapping is a set of
dense points in the Pareto front in objective space. However, some current
methods face a challenge: how to make the Pareto solution is diverse while
maximizing the hypervolume value. In this paper, we propose a novel method to
address this challenge, which employs Stein Variational Gradient Descent (SVGD)
to approximate the entire Pareto set. SVGD pushes a set of particles towards
the Pareto set by applying a form of functional gradient descent, which helps
to converge and diversify optimal solutions. Additionally, we employ diverse
gradient direction strategies to thoroughly investigate a unified framework for
SVGD in multi-objective optimization and adapt this framework with an annealing
schedule to promote stability. We introduce our method, SVH-MOL, and validate
its effectiveness through extensive experiments on multi-objective problems and
multi-task learning, demonstrating its superior performance.

</details>


### [138] [Certified Unlearning for Neural Networks](https://arxiv.org/abs/2506.06985)
*Anastasia Koloskova,Youssef Allouah,Animesh Jha,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 本研究提出了一种新的认证机器遗忘方法，通过在保留数据上进行带噪微调，提供可证明的遗忘保证，无需对损失函数做任何假设，同时在效率和准确性之间进行了理论权衡分析，并在实践中表现优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 随着隐私问题和法规（如“被遗忘权”）的要求，机器学习模型需要具备在请求下移除特定训练数据影响的能力。然而，现有的遗忘方法要么依赖严格的假设，要么缺乏正式的保证。

Method: 研究提出了一种新的认证机器遗忘方法，利用遗忘与通过随机后处理放大隐私之间的联系，在保留数据上进行带噪微调，从而确保可证明的遗忘保证。此方法不需要对底层损失函数做出任何假设，因此适用于各种不同的场景。

Result: 研究表明，该方法不仅实现了正式的遗忘保证，而且在实践中也表现得非常有效，优于现有的基线方法。

Conclusion: 本文提出了一种新的认证机器遗忘方法，解决了现有方法的局限性，提供了广泛的适用性和可证明的遗忘保证，同时在效率和准确性之间进行了权衡。

Abstract: We address the problem of machine unlearning, where the goal is to remove the
influence of specific training data from a model upon request, motivated by
privacy concerns and regulatory requirements such as the "right to be
forgotten." Unfortunately, existing methods rely on restrictive assumptions or
lack formal guarantees. To this end, we propose a novel method for certified
machine unlearning, leveraging the connection between unlearning and privacy
amplification by stochastic post-processing. Our method uses noisy fine-tuning
on the retain data, i.e., data that does not need to be removed, to ensure
provable unlearning guarantees. This approach requires no assumptions about the
underlying loss function, making it broadly applicable across diverse settings.
We analyze the theoretical trade-offs in efficiency and accuracy and
demonstrate empirically that our method not only achieves formal unlearning
guarantees but also performs effectively in practice, outperforming existing
baselines. Our code is available at
https://github.com/stair-lab/certified-unlearningneural-networks-icml-2025

</details>


### [139] [The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing](https://arxiv.org/abs/2506.06761)
*Adrià Molina Rodríguez,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.LG

TL;DR: This paper focuses on building models that can generalize to new data distributions, like alphabets, more effectively than traditional fine-tune strategies, particularly for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: There is a lack of consideration for low-resource languages in current recognition systems due to limited data availability. The authors aim to address this gap by creating models that can adapt to new data distributions faster and more efficiently.

Method: The method leverages recent advancements in model editing to incorporate unseen scripts (low-resource learning). It contrasts with state-of-the-art meta-learning by demonstrating the effectiveness of domain merging in sparse data distributions without needing specific relations to overall data distribution or prototyping.

Result: Experiments indicate significant performance improvements in transfer learning to new alphabets and out-of-domain evaluations, even with challenging domain shifts such as historical ciphered texts and non-Latin scripts.

Conclusion: The research introduces a novel approach to building models capable of easily adopting under-represented alphabets, thereby expanding document recognition capabilities to more contexts and cultures.

Abstract: Achieving robustness in recognition systems across diverse domains is crucial
for their practical utility. While ample data availability is usually assumed,
low-resource languages, such as ancient manuscripts and non-western languages,
tend to be kept out of the equations of massive pretraining and foundational
techniques due to an under representation. In this work, we aim for building
models which can generalize to new distributions of data, such as alphabets,
faster than centralized fine-tune strategies. For doing so, we take advantage
of the recent advancements in model editing to enhance the incorporation of
unseen scripts (low-resource learning). In contrast to state-of-the-art
meta-learning, we showcase the effectiveness of domain merging in sparse
distributions of data, with agnosticity of its relation to the overall
distribution or any other prototyping necessity. Even when using the same exact
training data, our experiments showcase significant performance boosts in
\textbf{transfer learning} to new alphabets and \textbf{out-of-domain
evaluation} in challenging domain shifts, including historical ciphered texts
and non-Latin scripts. This research contributes a novel approach into building
models that can easily adopt under-represented alphabets and, therefore, enable
document recognition to a wider set of contexts and cultures.

</details>


### [140] [AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint](https://arxiv.org/abs/2506.07022)
*Leheng Sheng,Changshuo Shen,Weixiang Zhao,Junfeng Fang,Xiaohao Liu,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.LG

TL;DR: To address the trade-off between safety and utility in LLMs when refusing malicious prompts, the paper introduces AlphaSteer, a theoretically grounded activation steering method that enhances safety without degrading performance on benign prompts.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to solve the problem of indiscriminately applying activation steering in large language models (LLMs), which can lead to over-refusal and degraded performance on benign prompts. Existing methods lack theoretical grounding, limiting their robustness and effectiveness.

Method: The paper proposes AlphaSteer, a method that treats activation steering as a learnable process with two learning objectives: utility preservation and safety enhancement. For utility preservation, it constructs a nearly zero vector for benign data using null-space constraints. For safety enhancement, it constructs a refusal direction vector for malicious data using linear regression.

Result: Experiments across multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness of AlphaSteer. It significantly improves the safety of LLMs without compromising their general capabilities.

Conclusion: AlphaSteer provides a theoretically grounded and empirically effective solution to enhance the safety of LLMs while preserving their utility. The codes are available at https://github.com/AlphaLab-USTC/AlphaSteer.

Abstract: As LLMs are increasingly deployed in real-world applications, ensuring their
ability to refuse malicious prompts, especially jailbreak attacks, is essential
for safe and reliable use. Recently, activation steering has emerged as an
effective approach for enhancing LLM safety by adding a refusal direction
vector to internal activations of LLMs during inference, which will further
induce the refusal behaviors of LLMs. However, indiscriminately applying
activation steering fundamentally suffers from the trade-off between safety and
utility, since the same steering vector can also lead to over-refusal and
degraded performance on benign prompts. Although prior efforts, such as vector
calibration and conditional steering, have attempted to mitigate this
trade-off, their lack of theoretical grounding limits their robustness and
effectiveness. To better address the trade-off between safety and utility, we
present a theoretically grounded and empirically effective activation steering
method called AlphaSteer. Specifically, it considers activation steering as a
learnable process with two principled learning objectives: utility preservation
and safety enhancement. For utility preservation, it learns to construct a
nearly zero vector for steering benign data, with the null-space constraints.
For safety enhancement, it learns to construct a refusal direction vector for
steering malicious data, with the help of linear regression. Experiments across
multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness
of AlphaSteer, which significantly improves the safety of LLMs without
compromising general capabilities. Our codes are available at
https://github.com/AlphaLab-USTC/AlphaSteer.

</details>


### [141] [Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World](https://arxiv.org/abs/2506.06782)
*Qinting Jiang,Chuyang Ye,Dongyan Wei,Bingli Wang,Yuan Xue,Jingyan Jiang,Zhi Wang*

Main category: cs.LG

TL;DR: Feature-based Instance Neighbor Discovery (FIND) addresses the challenge of dynamic, multiple test distributions in test-time adaptation (TTA). It includes Layer-wise Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN), and Selective FABN (S-FABN). FIND significantly improves accuracy in dynamic scenarios while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks experience performance declines under distribution shifts between training and test domains. Current TTA methods struggle with dynamic, multiple test distributions within batches.

Method: The proposed method, FIND, consists of three components: LFD, which captures features with similar distributions at each layer by constructing graph structures; FABN, which combines source statistics with test-time distribution specific statistics for robust feature representation; and S-FABN, which determines which layers require feature partitioning and which can remain unified to enhance inference efficiency.

Result: Extensive experiments demonstrate that FIND significantly outperforms existing methods, achieving a 30% accuracy improvement in dynamic scenarios while maintaining computational efficiency.

Conclusion: FIND effectively addresses the limitations of previous global normalization strategies in TTA by considering the inherent clustering of feature distributions across different domains.

Abstract: Despite progress, deep neural networks still suffer performance declines
under distribution shifts between training and test domains, leading to a
substantial decrease in Quality of Experience (QoE) for applications. Existing
test-time adaptation (TTA) methods are challenged by dynamic, multiple test
distributions within batches. We observe that feature distributions across
different domains inherently cluster into distinct groups with varying means
and variances. This divergence reveals a critical limitation of previous global
normalization strategies in TTA, which inevitably distort the original data
characteristics. Based on this insight, we propose Feature-based Instance
Neighbor Discovery (FIND), which comprises three key components: Layer-wise
Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and
Selective FABN (S-FABN). LFD stably captures features with similar
distributions at each layer by constructing graph structures. While FABN
optimally combines source statistics with test-time distribution specific
statistics for robust feature representation. Finally, S-FABN determines which
layers require feature partitioning and which can remain unified, thereby
enhancing inference efficiency. Extensive experiments demonstrate that FIND
significantly outperforms existing methods, achieving a 30\% accuracy
improvement in dynamic scenarios while maintaining computational efficiency.

</details>


### [142] [Caterpillar GNN: Replacing Message Passing with Efficient Aggregation](https://arxiv.org/abs/2506.06784)
*Marek Černý*

Main category: cs.LG

TL;DR: The paper introduces an efficient aggregation mechanism in graph neural networks called Caterpillar GNN, which trades off some expressivity for stronger aggregation capabilities. It performs well on synthetic and real-world datasets while reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Current message-passing graph neural networks prioritize maximal expressive power, but this paper aims to develop a method that deliberately sacrifices some expressivity for more robust and structured aggregation capabilities.

Method: The authors introduce the concept of 'efficient aggregation' in graph neural networks and propose the Caterpillar GNN. This model uses homomorphism counts from a hierarchy of generalized caterpillar graphs to characterize its expressive power at each intermediate step. It allows scaling between classical message-passing and simpler methods based on colored or plain walks.

Result: Caterpillar GNN successfully tackles synthetic graph-level tasks challenging for classical MPGNNs. On real-world datasets, it achieves similar predictive performance as traditional models while significantly reducing the number of nodes in hidden layers of the computational graph.

Conclusion: Caterpillar GNN provides a novel approach with robust graph-level aggregation capabilities, offering comparable performance with reduced computational resources.

Abstract: Message-passing graph neural networks (MPGNNs) dominate modern graph
learning, typically prioritizing maximal expressive power. In contrast, we
introduce an \emph{efficient aggregation} mechanism, deliberately trading off
some expressivity for stronger and more structured aggregation capabilities.
Our approach allows seamless scaling between classical message-passing and
simpler methods based on colored or plain walks. We rigorously characterize the
expressive power at each intermediate step using homomorphism counts from a
hierarchy of generalized \emph{caterpillar graphs}. Based on this foundation,
we propose the \emph{Caterpillar GNN}, whose robust graph-level aggregation
enables it to successfully tackle synthetic graph-level task specifically
designed to be challenging for classical MPGNNs. Moreover, we demonstrate that,
on real-world datasets, the Caterpillar GNN achieves comparable predictive
performance while significantly reducing the number of nodes in the hidden
layers of the computational graph.

</details>


### [143] [FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks](https://arxiv.org/abs/2506.06787)
*Qiyun Zhao*

Main category: cs.LG

TL;DR: FuncGNN is a novel approach that enhances logic circuit representations by integrating hybrid feature aggregation, gate-aware normalization, and multi-layer integration. It mitigates structural heterogeneity in AIGs, improves robustness, and synthesizes local and global semantic information. FuncGNN outperforms existing methods in signal probability prediction and truth-table distance prediction tasks with significant improvements in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the challenges posed by the increasing complexity and integration density of modern circuits, which lead to structural heterogeneity and global logic information loss in And-Inverter Graphs (AIGs). This affects the accuracy of circuit modeling in electronic design automation workflows.

Method: FuncGNN employs three key techniques: 1) Hybrid feature aggregation to extract multi-granularity topological patterns; 2) Gate-aware normalization to adapt to circuit-specific gate distributions and improve robustness; 3) Multi-layer integration to merge intermediate features across layers for comprehensive logic representations.

Result: FuncGNN achieves improvements of 2.06% in signal probability prediction and 18.71% in truth-table distance prediction compared to existing state-of-the-art methods. Additionally, it reduces training time by approximately 50.6% and GPU memory usage by about 32.8%.

Conclusion: FuncGNN successfully addresses the challenges of structural heterogeneity and logic information loss in AIGs through its innovative techniques. The experimental results confirm its superior performance and efficiency over existing methods.

Abstract: As integrated circuit scale grows and design complexity rises, effective
circuit representation helps support logic synthesis, formal verification, and
other automated processes in electronic design automation. And-Inverter Graphs
(AIGs), as a compact and canonical structure, are widely adopted for
representing Boolean logic in these workflows. However, the increasing
complexity and integration density of modern circuits introduce structural
heterogeneity and global logic information loss in AIGs, posing significant
challenges to accurate circuit modeling. To address these issues, we propose
FuncGNN, which integrates hybrid feature aggregation to extract
multi-granularity topological patterns, thereby mitigating structural
heterogeneity and enhancing logic circuit representations. FuncGNN further
introduces gate-aware normalization that adapts to circuit-specific gate
distributions, improving robustness to structural heterogeneity. Finally,
FuncGNN employs multi-layer integration to merge intermediate features across
layers, effectively synthesizing local and global semantic information for
comprehensive logic representations. Experimental results on two logic-level
analysis tasks (i.e., signal probability prediction and truth-table distance
prediction) demonstrate that FuncGNN outperforms existing state-of-the-art
methods, achieving improvements of 2.06% and 18.71%, respectively, while
reducing training time by approximately 50.6% and GPU memory usage by about
32.8%.

</details>


### [144] [Is Optimal Transport Necessary for Inverse Reinforcement Learning?](https://arxiv.org/abs/2506.06793)
*Zixuan Dong,Yumi Omori,Keith Ross*

Main category: cs.LG

TL;DR: An abstract about proposing two heuristic alternatives to Optimal Transport (OT) in Inverse Reinforcement Learning (IRL). These methods avoid optimization, exhibit linear-time complexity, and perform well across benchmarks.


<details>
  <summary>Details</summary>
Motivation: To challenge the necessity of using Optimal Transport in IRL by providing simpler alternatives that can achieve similar or better performance with less computational cost and complexity.

Method: Proposed two simple, heuristic alternatives: Minimum-Distance Reward and Segment-Matching Reward. The former assigns rewards based on the nearest expert state regardless of temporal order; the latter incorporates lightweight temporal alignment by matching agent states to corresponding segments in the expert trajectory.

Result: Through extensive evaluations across 32 online and offline benchmarks with three reinforcement learning algorithms, these simple rewards match or outperform recent OT-based approaches.

Conclusion: The core benefits of OT may arise from basic proximity alignment rather than its optimal coupling formulation, suggesting a need for reevaluation of complexity in future IRL design.

Abstract: Inverse Reinforcement Learning (IRL) aims to recover a reward function from
expert demonstrations. Recently, Optimal Transport (OT) methods have been
successfully deployed to align trajectories and infer rewards. While OT-based
methods have shown strong empirical results, they introduce algorithmic
complexity, hyperparameter sensitivity, and require solving the OT optimization
problems. In this work, we challenge the necessity of OT in IRL by proposing
two simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns
rewards based on the nearest expert state regardless of temporal order; and (2)
Segment-Matching Reward, which incorporates lightweight temporal alignment by
matching agent states to corresponding segments in the expert trajectory. These
methods avoid optimization, exhibit linear-time complexity, and are easy to
implement. Through extensive evaluations across 32 online and offline
benchmarks with three reinforcement learning algorithms, we show that our
simple rewards match or outperform recent OT-based approaches. Our findings
suggest that the core benefits of OT may arise from basic proximity alignment
rather than its optimal coupling formulation, advocating for reevaluation of
complexity in future IRL design.

</details>


### [145] [JavelinGuard: Low-Cost Transformer Architectures for LLM Security](https://arxiv.org/abs/2506.07330)
*Yash Datta,Sharath Rajasekar*

Main category: cs.LG

TL;DR: The paper introduces JavelinGuard, a set of model architectures for detecting malicious intent in LLM interactions with low cost and high performance. It explores five transformer-based architectures and benchmarks them on nine adversarial datasets. Raudra's multi-task design shows the best overall performance.


<details>
  <summary>Details</summary>
Motivation: To develop efficient and effective model architectures for detecting malicious intent in LLM interactions, optimizing for production deployment.

Method: Systematically explore five progressively sophisticated transformer-based architectures: Sharanga, Mahendra, Vaishnava and Ashwina, and Raudra. Benchmark these models across nine diverse adversarial datasets including newly introduced JavelinBench.

Result: Raudra's multi-task design offers the most robust performance overall. Each architecture presents unique trade-offs in speed, interpretability, and resource requirements.

Conclusion: JavelinGuard provides optimal balance of complexity and efficiency for real-world LLM security applications, with Raudra being the most robust choice.

Abstract: We present JavelinGuard, a suite of low-cost, high-performance model
architectures designed for detecting malicious intent in Large Language Model
(LLM) interactions, optimized specifically for production deployment. Recent
advances in transformer architectures, including compact BERT(Devlin et al.
2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build
highly accurate classifiers with as few as approximately 400M parameters that
achieve rapid inference speeds even on standard CPU hardware. We systematically
explore five progressively sophisticated transformer-based architectures:
Sharanga (baseline transformer classifier), Mahendra (enhanced
attention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid
neural ensemble architectures), and Raudra (an advanced multi-task framework
with specialized loss functions). Our models are rigorously benchmarked across
nine diverse adversarial datasets, including popular sets like the NotInject
series, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly
introduced JavelinBench, specifically crafted to test generalization on
challenging borderline and hard-negative cases. Additionally, we compare our
architectures against leading open-source guardrail models as well as large
decoder-only LLMs such as gpt-4o, demonstrating superior cost-performance
trade-offs in terms of accuracy, and latency. Our findings reveal that while
Raudra's multi-task design offers the most robust performance overall, each
architecture presents unique trade-offs in speed, interpretability, and
resource requirements, guiding practitioners in selecting the optimal balance
of complexity and efficiency for real-world LLM security applications.

</details>


### [146] [IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder](https://arxiv.org/abs/2506.06809)
*Di Lin,Wanjing Ren,Xuanbin Li,Rui Zhang*

Main category: cs.LG

TL;DR: IMPA-HGAE is a new framework that improves node embeddings in heterogeneous graphs by using all node information along meta-paths, showing superior performance and offering insights for future research.


<details>
  <summary>Details</summary>
Motivation: Current SSL models for heterogeneous graphs underutilize the internal node information along meta-paths when converting them to homogeneous graphs.

Method: Proposes IMPA-HGAE framework which fully exploits internal node information along meta-paths and introduces innovative masking strategies to enhance generative SSL models.

Result: IMPA-HGAE achieves superior performance on heterogeneous datasets and provides interpretability of the method.

Conclusion: This work offers insights into leveraging meta-path-guided structural semantics for robust representation learning in complex graph scenarios.

Abstract: Self-supervised learning (SSL) methods have been increasingly applied to
diverse downstream tasks due to their superior generalization capabilities and
low annotation costs. However, most existing heterogeneous graph SSL models
convert heterogeneous graphs into homogeneous ones via meta-paths for training,
which only leverage information from nodes at both ends of meta-paths while
underutilizing the heterogeneous node information along the meta-paths. To
address this limitation, this paper proposes a novel framework named IMPA-HGAE
to enhance target node embeddings by fully exploiting internal node information
along meta-paths. Experimental results validate that IMPA-HGAE achieves
superior performance on heterogeneous datasets. Furthermore, this paper
introduce innovative masking strategies to strengthen the representational
capacity of generative SSL models on heterogeneous graph data. Additionally,
this paper discuss the interpretability of the proposed method and potential
future directions for generative self-supervised learning in heterogeneous
graphs. This work provides insights into leveraging meta-path-guided structural
semantics for robust representation learning in complex graph scenarios.

</details>


### [147] [Path Integral Optimiser: Global Optimisation via Neural Schrödinger-Föllmer Diffusion](https://arxiv.org/abs/2506.06815)
*Max McGuinness,Eirik Fladmark,Francisco Vargas*

Main category: cs.LG

TL;DR: An early investigation into using neural diffusion processes for global optimisation based on Zhang et al.'s Path Integral Sampler is presented, showing promising performance in tasks between 2 and 1,247 dimensions but struggling in higher-dimensional spaces.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of neural diffusion processes for global optimization by framing it as a Schrödinger bridge sampling problem and solving it with stochastic control theory.

Method: Using the Boltzmann distribution to formulate optimization problems, applying Girsanov's theorem with a single-point prior, and computing integral terms via a neural approximation (Fourier MLP).

Result: The optimizer showed promising per-step performance in optimization tasks ranging from 2 to 1,247 dimensions, but struggled with exploration in higher-dimensional spaces when dealing with a model of 15.9k parameters.

Conclusion: This approach presents an interesting direction for global optimization, but further work is needed on adaptation in high-dimensional environments.

Abstract: We present an early investigation into the use of neural diffusion processes
for global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One
can use the Boltzmann distribution to formulate optimization as solving a
Schr\"odinger bridge sampling problem, then apply Girsanov's theorem with a
simple (single-point) prior to frame it in stochastic control terms, and
compute the solution's integral terms via a neural approximation (a Fourier
MLP). We provide theoretical bounds for this optimiser, results on toy
optimisation tasks, and a summary of the stochastic theory motivating the
model. Ultimately, we found the optimiser to display promising per-step
performance at optimisation tasks between 2 and 1,247 dimensions, but struggle
to explore higher-dimensional spaces when faced with a 15.9k parameter model,
indicating a need for work on adaptation in such environments.

</details>


### [148] [Curvature Enhanced Data Augmentation for Regression](https://arxiv.org/abs/2506.06853)
*Ilya Kaufman Sirot,Omri Azencot*

Main category: cs.LG

TL;DR: The paper presents Curvature-Enhanced Manifold Sampling (CEMS), a second-order method for synthetic data generation in regression tasks, which outperforms state-of-the-art methods with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Deep learning models, particularly over-parameterized ones, have excelled across various tasks. While data augmentation has been successful in classification tasks, it is less explored in regression problems. The authors aim to address this gap by proposing a novel approach for generating synthetic data in regression tasks.

Method: The authors develop a theoretical framework and practical tools for approximating and sampling general data manifolds. They introduce CEMS, which uses a second-order representation of the data manifold for efficient sampling and reconstruction of new data points in regression tasks.

Result: Extensive evaluations show that CEMS outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios for regression tasks, while only introducing minimal computational overhead.

Conclusion: CEMS is an effective method for synthetic data generation in regression tasks, offering superior performance with low computational cost. The code is publicly available.

Abstract: Deep learning models with a large number of parameters, often referred to as
over-parameterized models, have achieved exceptional performance across various
tasks. Despite concerns about overfitting, these models frequently generalize
well to unseen data, thanks to effective regularization techniques, with data
augmentation being among the most widely used. While data augmentation has
shown great success in classification tasks using label-preserving
transformations, its application in regression problems has received less
attention. Recently, a novel \emph{manifold learning} approach for generating
synthetic data was proposed, utilizing a first-order approximation of the data
manifold. Building on this foundation, we present a theoretical framework and
practical tools for approximating and sampling general data manifolds.
Furthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)
method for regression tasks. CEMS leverages a second-order representation of
the data manifold to enable efficient sampling and reconstruction of new data
points. Extensive evaluations across multiple datasets and comparisons with
state-of-the-art methods demonstrate that CEMS delivers superior performance in
both in-distribution and out-of-distribution scenarios, while introducing only
minimal computational overhead. Code is available at
https://github.com/azencot-group/CEMS.

</details>


### [149] [TokenBreak: Bypassing Text Classification Models Through Token Manipulation](https://arxiv.org/abs/2506.07948)
*Kasimir Schulz,Kenneth Yeung,Kieran Evans*

Main category: cs.LG

TL;DR: The paper introduces TokenBreak, an attack that can bypass text classification models by manipulating tokenization strategies, and provides a defensive strategy without retraining the model.


<details>
  <summary>Details</summary>
Motivation: To expose vulnerabilities in text classification models used to protect against threats like prompt injection attacks, toxic input, and cybersecurity risks such as spam emails.

Method: TokenBreak manipulates input text during tokenization so that certain models give incorrect classifications while remaining understandable to the end target (LLM or email recipient). The tokenizer's connection to model architecture allows prediction of vulnerability based on model family.

Result: TokenBreak successfully demonstrates its ability to bypass protection models, making the end target vulnerable to the very attacks these models were designed to prevent. Additionally, a defensive strategy is presented that can be implemented without retraining the defensive model.

Conclusion: Text classification models have significant vulnerabilities due to their tokenization strategies, which can be exploited by TokenBreak. A potential defense mechanism is proposed that does not require retraining the original model.

Abstract: Natural Language Processing (NLP) models are used for text-related tasks such
as classification and generation. To complete these tasks, input data is first
tokenized from human-readable text into a format the model can understand,
enabling it to make inferences and understand context. Text classification
models can be implemented to guard against threats such as prompt injection
attacks against Large Language Models (LLMs), toxic input and cybersecurity
risks such as spam emails. In this paper, we introduce TokenBreak: a novel
attack that can bypass these protection models by taking advantage of the
tokenization strategy they use. This attack technique manipulates input text in
such a way that certain models give an incorrect classification. Importantly,
the end target (LLM or email recipient) can still understand and respond to the
manipulated text and therefore be vulnerable to the very attack the protection
model was put in place to prevent. The tokenizer is tied to model architecture,
meaning it is possible to predict whether or not a model is vulnerable to
attack based on family. We also present a defensive strategy as an added layer
of protection that can be implemented without having to retrain the defensive
model.

</details>


### [150] [High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations](https://arxiv.org/abs/2506.06858)
*Ziwei Li,Yuhan Duan,Tianyu Xiong,Yi-Tang Chen,Wei-Lun Chao,Han-Wei Shen*

Main category: cs.LG

TL;DR: This paper proposes FA-INR, which uses cross-attention and a coordinate-guided mixture of experts to create flexible feature representations for modeling complex scientific fields while reducing model size.


<details>
  <summary>Details</summary>
Motivation: Implicit neural representations often struggle with complex scientific fields exhibiting localized, high-frequency variations. Current approaches that address this issue introduce additional features along rigid geometric structures, but at the cost of flexibility and increased model size.

Method: The proposed method is called Feature-Adaptive INR (FA-INR). It leverages cross-attention to an augmented memory bank to learn flexible feature representations, enabling adaptive allocation of model capacity based on data characteristics rather than rigid structural assumptions. Additionally, a coordinate-guided mixture of experts (MoE) is introduced to enhance the specialization and efficiency of feature representations.

Result: Experiments on three large-scale ensemble simulation datasets show that FA-INR achieves state-of-the-art fidelity while significantly reducing model size.

Conclusion: FA-INR establishes a new trade-off frontier between accuracy and compactness for INR-based surrogates.

Abstract: Effective surrogate models are critical for accelerating scientific
simulations. Implicit neural representations (INRs) offer a compact and
continuous framework for modeling spatially structured data, but they often
struggle with complex scientific fields exhibiting localized, high-frequency
variations. Recent approaches address this by introducing additional features
along rigid geometric structures (e.g., grids), but at the cost of flexibility
and increased model size. In this paper, we propose a simple yet effective
alternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to
an augmented memory bank to learn flexible feature representations, enabling
adaptive allocation of model capacity based on data characteristics, rather
than rigid structural assumptions. To further improve scalability, we introduce
a coordinate-guided mixture of experts (MoE) that enhances the specialization
and efficiency of feature representations. Experiments on three large-scale
ensemble simulation datasets show that FA-INR achieves state-of-the-art
fidelity while significantly reducing model size, establishing a new trade-off
frontier between accuracy and compactness for INR-based surrogates.

</details>


### [151] [SAFE: Finding Sparse and Flat Minima to Improve Pruning](https://arxiv.org/abs/2506.06866)
*Dongyeop Lee,Kwanhee Lee,Jinseok Chung,Namhoon Lee*

Main category: cs.LG

TL;DR: 通过引入平滑性目标的稀疏约束优化方法，提出了一种新的神经网络剪枝方法SAFE及其扩展版本SAFE$^+$，该方法能够有效提升稀疏网络的泛化性能，并对噪声数据具有较强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络剪枝往往会导致性能下降，如何恢复原始性能仍具挑战。受到鲁棒优化研究的启发，作者试图寻找同时具备稀疏性和平滑性的子网络以解决这一问题。

Method: 将剪枝问题形式化为一个稀疏约束优化问题，在其中鼓励平滑性作为目标函数。通过增强拉格朗日对偶方法显式求解该问题，并进一步提出一种广义投影操作，从而引出新的剪枝方法SAFE和其扩展版本SAFE$^+$。

Result: 在标准图像分类和语言建模任务上的广泛评估表明，SAFE能够持续生成具有改进泛化性能的稀疏网络，与已建立的基线方法相比具有竞争力。此外，SAFE对噪声数据表现出较强的鲁棒性。

Conclusion: 提出的SAFE方法可以有效地改善稀疏网络的泛化能力，并且对实际应用中的噪声数据具有良好的适应性。

Abstract: Sparsifying neural networks often suffers from seemingly inevitable
performance degradation, and it remains challenging to restore the original
performance despite much recent progress. Motivated by recent studies in robust
optimization, we aim to tackle this problem by finding subnetworks that are
both sparse and flat at the same time. Specifically, we formulate pruning as a
sparsity-constrained optimization problem where flatness is encouraged as an
objective. We solve it explicitly via an augmented Lagrange dual approach and
extend it further by proposing a generalized projection operation, resulting in
novel pruning methods called SAFE and its extension, SAFE$^+$. Extensive
evaluations on standard image classification and language modeling tasks reveal
that SAFE consistently yields sparse networks with improved generalization
performance, which compares competitively to well-established baselines. In
addition, SAFE demonstrates resilience to noisy data, making it well-suited for
real-world conditions.

</details>


### [152] [Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning](https://arxiv.org/abs/2506.06873)
*Armin Behnamnia,Gholamali Aminian,Alireza Aghaei,Chengchun Shi,Vincent Y. F. Tan,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: The paper introduces a novel LSE-based estimator for off-policy learning and evaluation that reduces variance, handles heavy-tailed rewards, and outperforms traditional methods.


<details>
  <summary>Details</summary>
Motivation: Off-policy learning and evaluation face challenges with high variance, low-quality propensity scores, and heavy-tailed reward distributions.

Method: A new estimator based on the log-sum-exponential (LSE) operator is introduced. For off-policy evaluation, upper bounds on bias and variance are derived. In off-policy learning, regret bounds are established assuming bounded weighted reward moment.

Result: The LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. A convergence rate of O(n^(-ε/(1+ ε))) for regret bounds is achieved.

Conclusion: Theoretical analysis is supported by empirical evaluations, confirming the practical advantages of the LSE estimator in both off-policy learning and evaluation.

Abstract: Off-policy learning and evaluation leverage logged bandit feedback datasets,
which contain context, action, propensity score, and feedback for each data
point. These scenarios face significant challenges due to high variance and
poor performance with low-quality propensity scores and heavy-tailed reward
distributions. We address these issues by introducing a novel estimator based
on the log-sum-exponential (LSE) operator, which outperforms traditional
inverse propensity score estimators. Our LSE estimator demonstrates variance
reduction and robustness under heavy-tailed conditions. For off-policy
evaluation, we derive upper bounds on the estimator's bias and variance. In the
off-policy learning scenario, we establish bounds on the regret -- the
performance gap between our LSE estimator and the optimal policy -- assuming
bounded $(1+\epsilon)$-th moment of weighted reward. Notably, we achieve a
convergence rate of $O(n^{-\epsilon/(1+ \epsilon)})$ for the regret bounds,
where $\epsilon \in [0,1]$ and $n$ is the size of logged bandit feedback
dataset. Theoretical analysis is complemented by comprehensive empirical
evaluations in both off-policy learning and evaluation scenarios, confirming
the practical advantages of our approach. The code for our estimator is
available at the following link:
https://github.com/armin-behnamnia/lse-offpolicy-learning.

</details>


### [153] [FREE: Fast and Robust Vision Language Models with Early Exits](https://arxiv.org/abs/2506.06884)
*Divya Jyoti Bajpai,Manjesh Kumar Hanawal*

Main category: cs.LG

TL;DR: In this paper, the authors propose FREE, an adversarial training approach within a GAN-based framework to employ Early Exit strategies in Vision-Language Models (VLMs) for increasing inference speed with minimal performance drop.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of large model size in VLMs that leads to high inference latency, which hinders their application in real-world scenarios.

Method: The method proposed is named FREE, which uses a GAN-based framework. Each exit includes a transformer layer and a classifier. The transformer layer is adversarially trained to generate feature representations similar to the final layer, while the feature classifier acts as the discriminator.

Result: Experimental results show that the method speeds up the inference process by more than 1.51x while maintaining comparable performance. It also enhances accuracy and model robustness by reducing overthinking and mitigating the mid-crisis phenomenon.

Conclusion: FREE effectively employs Early Exit strategies in VLMs, achieving faster inference with minimal performance degradation, thus making VLMs more practical for real-world applications.

Abstract: In recent years, Vision-Language Models (VLMs) have shown remarkable
performance improvements in Vision-Language tasks. However, their large size
poses challenges for real-world applications where inference latency is a
concern. To tackle this issue, we propose employing Early Exit (EE) strategies
in VLMs. However, training exit classifiers in VLMs is challenging,
particularly with limited labeled training data. To address this, we introduce
FREE, an adversarial training approach within a GAN-based framework. Here, each
exit consists of a transformer layer and a classifier. The transformer layer is
adversarially trained to produce feature representations similar to the final
layer, while a feature classifier serves as the discriminator. Our method
focuses on performing input-adaptive inference that increases inference speed
with minimal drop in performance. Experimental results demonstrate the
effectiveness of our approach in enhancing accuracy and model robustness by
mitigating overthinking and the phenomenon of mid-crisis that we highlight. We
experimentally validate that our method speeds up the inference process by more
than 1.51x while retaining comparable performance. The source code is available
at https://github.com/Div290/FREE.

</details>


### [154] [Scalable Gaussian Processes with Latent Kronecker Structure](https://arxiv.org/abs/2506.06895)
*Jihao Andreas Lin,Sebastian Ament,Maximilian Balandat,David Eriksson,José Miguel Hernández-Lobato,Eytan Bakshy*

Main category: cs.LG

TL;DR: 通过利用潜在的Kronecker结构，提出了一种新的方法，可以在处理大规模数据集时显著减少计算资源需求，同时保持精确的高斯过程推断。该方法在多个实际数据集上超越了现有的稀疏和变分高斯过程方法。


<details>
  <summary>Details</summary>
Motivation: 应用高斯过程到非常大的数据集上仍然是一个挑战，因为计算可扩展性有限。矩阵结构（如Kronecker积）虽然可以显著加速操作，但其应用通常涉及近似或不切实际的假设。特别是在处理现实世界数据（如时间序列）时，缺失观测值会破坏Cartesian积结构，从而导致无法使用Kronecker结构。

Method: 提出利用潜在的Kronecker结构，将观察值的核矩阵表示为潜在Kronecker乘积的投影。结合迭代线性系统求解器和路径条件化方法，这种方法可以在显著减少计算资源的情况下实现精确的高斯过程推断。

Result: 实验结果表明，该方法在处理多达五百万个样本的实际数据集时，优于现有的最先进的稀疏和变分高斯过程方法。这些数据集涵盖了机器人、自动机器学习和气候应用等领域。

Conclusion: 所提出的方法成功地解决了现有Kronecker结构方法的局限性，在处理大规模数据集时既保持了精确性又提高了计算效率，适用于多种实际应用场景。

Abstract: Applying Gaussian processes (GPs) to very large datasets remains a challenge
due to limited computational scalability. Matrix structures, such as the
Kronecker product, can accelerate operations significantly, but their
application commonly entails approximations or unrealistic assumptions. In
particular, the most common path to creating a Kronecker-structured kernel
matrix is by evaluating a product kernel on gridded inputs that can be
expressed as a Cartesian product. However, this structure is lost if any
observation is missing, breaking the Cartesian product structure, which
frequently occurs in real-world data such as time series. To address this
limitation, we propose leveraging latent Kronecker structure, by expressing the
kernel matrix of observed values as the projection of a latent Kronecker
product. In combination with iterative linear system solvers and pathwise
conditioning, our method facilitates inference of exact GPs while requiring
substantially fewer computational resources than standard iterative methods. We
demonstrate that our method outperforms state-of-the-art sparse and variational
GPs on real-world datasets with up to five million examples, including
robotics, automated machine learning, and climate applications.

</details>


### [155] [Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations](https://arxiv.org/abs/2506.06907)
*Fred Xu,Thomas Markovich*

Main category: cs.LG

TL;DR: The paper proposes a new method for uncertainty estimation in Graph Neural Networks (GNNs) using an analogy with stochastic partial differential equations (SPDE). This method incorporates spatial-temporal noises and enhances uncertainty estimates on graphs with varying label informativeness.


<details>
  <summary>Details</summary>
Motivation: Current methods for estimating uncertainty in GNNs are inadequate, particularly under distributional shifts. Traditional approaches do not account for randomness from both graph structure and label distribution.

Method: The authors draw an analogy between the evolution of SPDE driven by Matern Gaussian Process and message passing in GNN layers to design a novel message passing scheme that incorporates spatial-temporal noises. This approach allows explicit control over the covariance kernel smoothness.

Result: Experiments on OOD detection show the effectiveness and superiority of the proposed model across datasets with different levels of label informativeness.

Conclusion: The paper presents a principled way to enhance uncertainty estimates in GNNs, capturing uncertainty across space and time, and demonstrating superior performance in OOD detection tasks.

Abstract: Graph Neural Networks have achieved impressive results across diverse network
modeling tasks, but accurately estimating uncertainty on graphs remains
difficult, especially under distributional shifts. Unlike traditional
uncertainty estimation, graph-based uncertainty must account for randomness
arising from both the graph's structure and its label distribution, which adds
complexity. In this paper, making an analogy between the evolution of a
stochastic partial differential equation (SPDE) driven by Matern Gaussian
Process and message passing using GNN layers, we present a principled way to
design a novel message passing scheme that incorporates spatial-temporal noises
motivated by the Gaussian Process approach to SPDE. Our method simultaneously
captures uncertainty across space and time and allows explicit control over the
covariance kernel smoothness, thereby enhancing uncertainty estimates on graphs
with both low and high label informativeness. Our extensive experiments on
Out-of-Distribution (OOD) detection on graph datasets with varying label
informativeness demonstrate the soundness and superiority of our model to
existing approaches.

</details>


### [156] [Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data](https://arxiv.org/abs/2506.06917)
*Shangjie Du,Hui Wei,Dong Yoon Lee,Zhizhang Hu,Shijia Pan*

Main category: cs.LG

TL;DR: GraPhy is a physics-guided graph neural network designed for high-resolution air quality modeling in urban areas with limited monitoring data.


<details>
  <summary>Details</summary>
Motivation: Fine-grained air quality monitoring information is essential for reducing public exposure to pollutants, but current monitoring networks are often sparse in socioeconomically disadvantaged regions, limiting the accuracy and resolution of air quality modeling.

Method: The paper proposes GraPhy, a physics-guided graph neural network architecture with layers and edge features specifically designed for low-resolution monitoring data.

Result: Experiments using data from California's San Joaquin Valley show that GraPhy achieves the best performance evaluated by MSE, MAE, and R2, improving the performance by 9%-56% compared to various baseline models.

Conclusion: GraPhy consistently outperforms baselines across different spatial heterogeneity levels, demonstrating the effectiveness of its model design.

Abstract: This work introduces GraPhy, a graph-based, physics-guided learning framework
for high-resolution and accurate air quality modeling in urban areas with
limited monitoring data. Fine-grained air quality monitoring information is
essential for reducing public exposure to pollutants. However, monitoring
networks are often sparse in socioeconomically disadvantaged regions, limiting
the accuracy and resolution of air quality modeling. To address this, we
propose a physics-guided graph neural network architecture called GraPhy with
layers and edge features designed specifically for low-resolution monitoring
data. Experiments using data from California's socioeconomically disadvantaged
San Joaquin Valley show that GraPhy achieves the overall best performance
evaluated by mean squared error (MSE), mean absolute error (MAE), and R-square
value (R2), improving the performance by 9%-56% compared to various baseline
models. Moreover, GraPhy consistently outperforms baselines across different
spatial heterogeneity levels, demonstrating the effectiveness of our model
design.

</details>


### [157] [Basis Transformers for Multi-Task Tabular Regression](https://arxiv.org/abs/2506.06926)
*Wei Min Loh,Jiaqi Shang,Pascal Poupart*

Main category: cs.LG

TL;DR: The paper introduces basis transformers, a new architecture for handling tabular data challenges, which performs well on benchmarks with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing techniques struggle to handle key aspects of tabular data like textual information, variable columns, and unseen data without metadata.

Method: Propose basis transformers, designed to respect invariances in tabular data including hierarchical structure and numeric value representation.

Result: Achieved an improvement of 0.338 in the median $R^2$ score with the lowest standard deviation across 34 tasks from the OpenML-CTR23 benchmark.

Conclusion: Basis transformers outperform pretrained large language model baselines, even when initialized from randomized weights, and have five times fewer parameters than the best-performing baseline.

Abstract: Dealing with tabular data is challenging due to partial information, noise,
and heterogeneous structure. Existing techniques often struggle to
simultaneously address key aspects of tabular data such as textual information,
a variable number of columns, and unseen data without metadata besides column
names. We propose a novel architecture, \textit{basis transformers},
specifically designed to tackle these challenges while respecting inherent
invariances in tabular data, including hierarchical structure and the
representation of numeric values. We evaluate our design on a multi-task
tabular regression benchmark, achieving an improvement of 0.338 in the median
$R^2$ score and the lowest standard deviation across 34 tasks from the
OpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters
than the best-performing baseline and surpasses pretrained large language model
baselines -- even when initialized from randomized weights.

</details>


### [158] [Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More](https://arxiv.org/abs/2506.06940)
*Geonhui Yoo,Minhak Song,Chulhee Yun*

Main category: cs.LG

TL;DR: In this paper, the authors use a deep linear network with a single neuron per layer as a minimalist model to study progressive sharpening phenomenon in neural network training. They theoretically analyze how dataset properties, network depth, stochasticity of optimizers, and step size affect the degree of progressive sharpening. Then they empirically demonstrate how these theoretical insights extend to practical scenarios.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to better understand the mechanisms behind progressive sharpening during the training of deep neural networks with gradient descent.

Method: The method used in this study involves studying progressive sharpening using a minimalist model - a deep linear network with a single neuron per layer. Theoretical analysis is performed on how various factors such as dataset properties, network depth, optimizer stochasticity, and step size impact progressive sharpening in this model.

Result: The results show that this simple model effectively captures the sharpness dynamics observed in recent empirical studies on neural network training.

Conclusion: This study concludes by offering a deeper understanding of sharpness dynamics in neural network training, emphasizing the interplay between depth, training data, and optimizers.

Abstract: When training deep neural networks with gradient descent, sharpness often
increases -- a phenomenon known as progressive sharpening -- before saturating
at the edge of stability. Although commonly observed in practice, the
underlying mechanisms behind progressive sharpening remain poorly understood.
In this work, we study this phenomenon using a minimalist model: a deep linear
network with a single neuron per layer. We show that this simple model
effectively captures the sharpness dynamics observed in recent empirical
studies, offering a simple testbed to better understand neural network
training. Moreover, we theoretically analyze how dataset properties, network
depth, stochasticity of optimizers, and step size affect the degree of
progressive sharpening in the minimalist model. We then empirically demonstrate
how these theoretical insights extend to practical scenarios. This study offers
a deeper understanding of sharpness dynamics in neural network training,
highlighting the interplay between depth, training data, and optimizers.

</details>


### [159] [Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression](https://arxiv.org/abs/2506.06954)
*Clinton Enwerem,Aniruddh G. Puranic,John S. Baras,Calin Belta*

Main category: cs.LG

TL;DR: An RL algorithm with risk-regularized quantile-based method integrating CVaR is proposed to enforce safety in high-variance stochastic environments, showing better safety-performance trade-offs.


<details>
  <summary>Details</summary>
Motivation: Mainstream RL algorithms suffer from overestimation bias and cannot ensure safety constraints in high-variance stochastic environments without complex architectures or manual tradeoffs.

Method: Propose a risk-regularized quantile-based algorithm integrating CVaR to enforce safety without complex neural architectures, providing theoretical guarantees on the contraction properties of the risk-sensitive distributional Bellman operator.

Result: Simulations show that the approach leads to more goal successes, fewer collisions, and better safety-performance trade-offs compared to risk-neutral methods.

Conclusion: The proposed risk-regularized quantile-based algorithm with CVaR integration can effectively enforce safety in high-variance stochastic environments.

Abstract: Mainstream approximate action-value iteration reinforcement learning (RL)
algorithms suffer from overestimation bias, leading to suboptimal policies in
high-variance stochastic environments. Quantile-based action-value iteration
methods reduce this bias by learning a distribution of the expected cost-to-go
using quantile regression. However, ensuring that the learned policy satisfies
safety constraints remains a challenge when these constraints are not
explicitly integrated into the RL framework. Existing methods often require
complex neural architectures or manual tradeoffs due to combined cost
functions. To address this, we propose a risk-regularized quantile-based
algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety
without complex architectures. We also provide theoretical guarantees on the
contraction properties of the risk-sensitive distributional Bellman operator in
Wasserstein space, ensuring convergence to a unique cost distribution.
Simulations of a mobile robot in a dynamic reach-avoid task show that our
approach leads to more goal successes, fewer collisions, and better
safety-performance trade-offs compared to risk-neutral methods.

</details>


### [160] [UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare](https://arxiv.org/abs/2506.06977)
*Pengfei Hu,Xiaoxue Han,Fei Wang,Yue Ning*

Main category: cs.LG

TL;DR: UdonCare is a hierarchy-guided framework that uses medical ontologies to discover latent domains and separate domain-related signals from patient-level features, improving domain generalization in clinical prediction.


<details>
  <summary>Details</summary>
Motivation: Domain generalization in clinical prediction faces challenges due to shifting data distributions and lack of patient-specific domain labels. Existing methods struggle in real-world healthcare settings because they often overlook important clinical insights.

Method: The paper introduces UdonCare, which leverages hierarchical medical ontologies (e.g., ICD-9-CM) to group diseases into higher-level categories. It iteratively prunes fine-grained domains, encodes refined domains, and applies a Siamese-type inference mechanism to separate domain-related signals from patient-level features.

Result: Experiments on MIMIC-III and MIMIC-IV datasets show that UdonCare achieves better performance compared to other domain generalization baselines when substantial domain gaps are present.

Conclusion: The study highlights the potential of integrating medical knowledge for enhancing domain generalization in practical healthcare applications.

Abstract: Domain generalization has become a critical challenge in clinical prediction,
where patient cohorts often exhibit shifting data distributions that degrade
model performance. Typical domain generalization approaches struggle in
real-world healthcare settings for two main reasons: (1) patient-specific
domain labels are typically unavailable, making domain discovery especially
difficult; (2) purely data-driven approaches overlook key clinical insights,
leading to a gap in medical knowledge integration. To address these problems,
we leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to
group diseases into higher-level categories and discover more flexible latent
domains. In this paper, we introduce UdonCare, a hierarchy-guided framework
that iteratively prunes fine-grained domains, encodes these refined domains,
and applies a Siamese-type inference mechanism to separate domain-related
signals from patient-level features. Experimental results on clinical datasets
(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher
performance compared to other domain generalization baselines when substantial
domain gaps presents, highlighting the untapped potential of medical knowledge
for enhancing domain generalization in practical healthcare applications.

</details>


### [161] [Near Optimal Non-asymptotic Sample Complexity of 1-Identification](https://arxiv.org/abs/2506.06978)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 研究了纯探索的多臂老虎机问题中的1-识别问题，设计了一种新算法SEE，并从非渐近视角进行了理论分析，达到了接近最优性。


<details>
  <summary>Details</summary>
Motivation: 受到现有文献中一个开放方向的启发，研究1-识别问题，目标是确定是否存在平均奖励至少为已知阈值$\mu_0$的手臂，或在认为不存在这样的手臂时输出None。

Method: 设计了一种新的算法Sequential-Exploration-Exploitation (SEE)，并从非渐近视角进行理论分析。

Result: 达到了接近最优性，上下界之间的差距仅为多项式对数因子，数值结果也表明该算法的有效性。

Conclusion: 提出了新算法SEE，在非渐近分析中实现了接近最优的拉动复杂度，填补了现有研究的空白。

Abstract: Motivated by an open direction in existing literature, we study the
1-identification problem, a fundamental multi-armed bandit formulation on pure
exploration. The goal is to determine whether there exists an arm whose mean
reward is at least a known threshold $\mu_0$, or to output None if it believes
such an arm does not exist. The agent needs to guarantee its output is correct
with probability at least $1-\delta$. Degenne & Koolen 2019 has established the
asymptotically tight sample complexity for the 1-identification problem, but
they commented that the non-asymptotic analysis remains unclear. We design a
new algorithm Sequential-Exploration-Exploitation (SEE), and conduct
theoretical analysis from the non-asymptotic perspective. Novel to the
literature, we achieve near optimality, in the sense of matching upper and
lower bounds on the pulling complexity. The gap between the upper and lower
bounds is up to a polynomial logarithmic factor. The numerical result also
indicates the effectiveness of our algorithm, compared to existing benchmarks.

</details>


### [162] [MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification](https://arxiv.org/abs/2506.06980)
*Sajib Acharjee Dip,Uddip Acharjee Shuvo,Dipanwita Mallick,Abrar Rahman Abir,Liqing Zhang*

Main category: cs.LG

TL;DR: 提出了一种新的深度学习框架MoXGATE，用于多组学数据的癌症亚型分类，具有高准确性和良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前有效整合基因组、表观基因组和转录组等多组学数据仍面临挑战，因为这些数据特征具有异质性，难以进行稳健且可解释的整合。

Method: 提出名为Modality-Aware Cross-Attention MoXGATE的新框架，利用交叉注意力机制和可学习的模态权重来增强多组学数据间的特征融合，并通过焦点损失函数解决数据不平衡问题。

Result: 在GIAC和BRCA数据集上的实验表明，MoXGATE方法优于现有方法，达到了95%的分类准确率；消融研究验证了交叉注意力机制的有效性及不同组学模态的重要性。此外，模型在未见过的癌症类型上也表现出良好的泛化性能。

Conclusion: MoXGATE是一种有前景的多组学癌症亚型分类方法，提供更高的性能和生物学泛化能力。

Abstract: Cancer subtype classification is crucial for personalized treatment and
prognostic assessment. However, effectively integrating multi-omic data remains
challenging due to the heterogeneous nature of genomic, epigenomic, and
transcriptomic features. In this work, we propose Modality-Aware
Cross-Attention MoXGATE, a novel deep-learning framework that leverages
cross-attention and learnable modality weights to enhance feature fusion across
multiple omics sources. Our approach effectively captures inter-modality
dependencies, ensuring robust and interpretable integration. Through
experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)
datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,
achieving 95\% classification accuracy. Ablation studies validate the
effectiveness of cross-attention over simple concatenation and highlight the
importance of different omics modalities. Moreover, our model generalizes well
to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key
contributions include (1) a cross-attention-based multi-omic integration
framework, (2) modality-weighted fusion for enhanced interpretability, (3)
application of focal loss to mitigate data imbalance, and (4) validation across
multiple cancer subtypes. Our results indicate that MoXGATE is a promising
approach for multi-omic cancer subtype classification, offering improved
performance and biological generalizability.

</details>


### [163] [Fully Explainable Classification Models Using Hyperblocks](https://arxiv.org/abs/2506.06986)
*Austin Snyder,Ryan Gallagher,Boris Kovalerchuk*

Main category: cs.LG

TL;DR: The paper builds on Hyperblocks to improve interpretability, reduce training time and model complexity while maintaining accuracy. It introduces algorithms for simplifying Hyperblocks, including removing redundant attributes and blocks, and creating disjunctive units. A fallback mechanism using k-NN classifiers is also introduced to ensure complete data coverage. The results show that the method can handle high-dimensional datasets with competitive accuracy and reduced complexity.


<details>
  <summary>Details</summary>
Motivation: To enhance the interpretability of models, decrease training time, and reduce model complexity without sacrificing accuracy.

Method: Introduce a suite of algorithms for Hyperblock simplification which include removing redundant attributes, removing redundant blocks through overlap analysis, and creating disjunctive units. Also, introduce an interpretable fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not covered by any block.

Result: On benchmark datasets such as WBC (9-D), strong predictive performance was achieved with significantly reduced complexity. On MNIST (784-D), the method showed promise as a transparent alternative to black-box models through tuning and simplification.

Conclusion: Interpretable models can scale to high-dimensional, large-volume datasets while maintaining competitive accuracy.

Abstract: Building on existing work with Hyperblocks, which classify data using minimum
and maximum bounds for each attribute, we focus on enhancing interpretability,
decreasing training time, and reducing model complexity without sacrificing
accuracy. This system allows subject matter experts (SMEs) to directly inspect
and understand the model's decision logic without requiring extensive machine
learning expertise. To reduce Hyperblock complexity while retaining
performance, we introduce a suite of algorithms for Hyperblock simplification.
These include removing redundant attributes, removing redundant blocks through
overlap analysis, and creating disjunctive units. These methods eliminate
unnecessary parameters, dramatically reducing model size without harming
classification power. We increase robustness by introducing an interpretable
fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not
covered by any block, ensuring complete data coverage while preserving model
transparency. Our results demonstrate that interpretable models can scale to
high-dimensional, large-volume datasets while maintaining competitive accuracy.
On benchmark datasets such as WBC (9-D), we achieve strong predictive
performance with significantly reduced complexity. On MNIST (784-D), our method
continues to improve through tuning and simplification, showing promise as a
transparent alternative to black-box models in domains where trust, clarity,
and control are crucial.

</details>


### [164] [Modified K-means Algorithm with Local Optimality Guarantees](https://arxiv.org/abs/2506.06990)
*Mingyi Li,Michael R. Metel,Akiko Takeda*

Main category: cs.LG

TL;DR: This paper examines the local optimality of the K-means algorithm and proposes modifications to ensure local optimality with the same computational complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of rigorous analysis on the local optimality guarantees of the K-means algorithm, despite its widespread use in machine learning.

Method: The authors present conditions under which the K-means algorithm converges to a locally optimal solution. They then propose simple modifications to the algorithm ensuring local optimality in both continuous and discrete senses, using Bregman divergence as the dissimilarity measure.

Result: Numerical experiments show that the standard K-means algorithm does not always find a locally optimal solution, whereas the proposed methods provide improved locally optimal solutions with reduced clustering loss.

Conclusion: The modified K-means algorithms offer better local optimality guarantees without increasing computational complexity.

Abstract: The K-means algorithm is one of the most widely studied clustering algorithms
in machine learning. While extensive research has focused on its ability to
achieve a globally optimal solution, there still lacks a rigorous analysis of
its local optimality guarantees. In this paper, we first present conditions
under which the K-means algorithm converges to a locally optimal solution.
Based on this, we propose simple modifications to the K-means algorithm which
ensure local optimality in both the continuous and discrete sense, with the
same computational complexity as the original K-means algorithm. As the
dissimilarity measure, we consider a general Bregman divergence, which is an
extension of the squared Euclidean distance often used in the K-means
algorithm. Numerical experiments confirm that the K-means algorithm does not
always find a locally optimal solution in practice, while our proposed methods
provide improved locally optimal solutions with reduced clustering loss. Our
code is available at https://github.com/lmingyi/LO-K-means.

</details>


### [165] [Towards Physics-informed Diffusion for Anomaly Detection in Trajectories](https://arxiv.org/abs/2506.06999)
*Arun Sharma,Mingzhou Yang,Majid Farhadloo,Subhankar Ghosh,Bharat Jayaprakash,Shashi Shekhar*

Main category: cs.LG

TL;DR: The paper proposes a physics-informed diffusion model for detecting anomalous trajectories indicative of GPS spoofing, integrating kinematic constraints to improve accuracy and reduce false-positives.


<details>
  <summary>Details</summary>
Motivation: Detecting anomalous trajectories is crucial to prevent illegal activities in international waters like unauthorized fishing and illicit oil transfers. Current methods face challenges due to AI-generated fake trajectories and lack of labeled data, leading to higher false-positive rates.

Method: The authors propose a physics-informed diffusion model that incorporates kinematic constraints to identify trajectories violating physical laws. This approach aims to address the limitations of existing generative models which do not consider fine-scale spatiotemporal dependencies and prior physical knowledge.

Result: Experiments on real-world datasets from maritime and urban domains demonstrate improved prediction accuracy and reduced estimation error rate for anomaly detection and trajectory generation compared to existing methods.

Conclusion: The proposed physics-informed diffusion model effectively detects anomalous trajectories by leveraging physical laws, leading to more accurate results with fewer false positives. The implementation is publicly available.

Abstract: Given trajectory data, a domain-specific study area, and a user-defined
threshold, we aim to find anomalous trajectories indicative of possible GPS
spoofing (e.g., fake trajectory). The problem is societally important to curb
illegal activities in international waters, such as unauthorized fishing and
illicit oil transfers. The problem is challenging due to advances in AI
generated in deep fakes generation (e.g., additive noise, fake trajectories)
and lack of adequate amount of labeled samples for ground-truth verification.
Recent literature shows promising results for anomalous trajectory detection
using generative models despite data sparsity. However, they do not consider
fine-scale spatiotemporal dependencies and prior physical knowledge, resulting
in higher false-positive rates. To address these limitations, we propose a
physics-informed diffusion model that integrates kinematic constraints to
identify trajectories that do not adhere to physical laws. Experimental results
on real-world datasets in the maritime and urban domains show that the proposed
framework results in higher prediction accuracy and lower estimation error rate
for anomaly detection and trajectory generation methods, respectively. Our
implementation is available at
https://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.

</details>


### [166] [End-to-End Probabilistic Framework for Learning with Hard Constraints](https://arxiv.org/abs/2506.07003)
*Utkarsh Utkarsh,Danielle C. Maddix,Ruijun Ma,Michael W. Mahoney,Yuyang Wang*

Main category: cs.LG

TL;DR: The paper introduces ProbHardE2E, a probabilistic forecasting framework that incorporates operational/physical constraints as hard requirements. It uses a differentiable probabilistic projection layer (DPPL) to enable end-to-end learning and uncertainty quantification without distributional assumptions.


<details>
  <summary>Details</summary>
Motivation: To develop a general-purpose forecasting system capable of enforcing operational/physical constraints as hard requirements while performing uncertainty quantification on the model.

Method: ProbHardE2E employs a novel differentiable probabilistic projection layer (DPPL) that can be integrated with various neural network architectures for end-to-end learning. This approach avoids post-processing or inference-based constraint satisfaction and allows optimization of strictly proper scoring rules without making distributional assumptions on the target.

Result: ProbHardE2E successfully performs uncertainty quantification and enforces hard constraints in both learning partial differential equations with uncertainty estimates and probabilistic time-series forecasting, demonstrating its broad applicability across seemingly disparate domains.

Conclusion: ProbHardE2E represents a versatile general setup that connects different modeling domains by enabling robust distributional estimates and incorporating non-linear constraints.

Abstract: We present a general purpose probabilistic forecasting framework,
ProbHardE2E, to learn systems that can incorporate operational/physical
constraints as hard requirements. ProbHardE2E enforces hard constraints by
exploiting variance information in a novel way; and thus it is also capable of
performing uncertainty quantification (UQ) on the model. Our methodology uses a
novel differentiable probabilistic projection layer (DPPL) that can be combined
with a wide range of neural network architectures. This DPPL allows the model
to learn the system in an end-to-end manner, compared to other approaches where
the constraints are satisfied either through a post-processing step or at
inference. In addition, ProbHardE2E can optimize a strictly proper scoring
rule, without making any distributional assumptions on the target, which
enables it to obtain robust distributional estimates (in contrast to existing
approaches that generally optimize likelihood-based objectives, which are
heavily biased by their distributional assumptions and model choices); and it
can incorporate a range of non-linear constraints (increasing the power of
modeling and flexibility). We apply ProbHardE2E to problems in learning partial
differential equations with uncertainty estimates and to probabilistic
time-series forecasting, showcasing it as a broadly applicable general setup
that connects these seemingly disparate domains.

</details>


### [167] [Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection](https://arxiv.org/abs/2506.07014)
*Yutaro Nakagama,Daisuke Ishii,Kazuki Yoshizoe*

Main category: cs.LG

TL;DR: This paper addresses the issue of reliability and reproducibility in vehicle dynamics-based driver drowsiness detection (DDD) methods by comparing representative methods under a fair framework using a public dataset. The random forest (RF)-based method achieved the highest accuracy.


<details>
  <summary>Details</summary>
Motivation: There are concerns about the reliability of performance metrics and the reproducibility of many existing vehicle dynamics-based DDD methods due to issues like data leakage and lack of openly provided datasets.

Method: The authors developed a framework for extracting features from an open dataset and performing DDD with lightweight ML models. They implemented three existing representative methods and a concise RF-based method within this framework.

Result: Experiments verified the reproducibility and clarified the performance of DDD based on common metrics. The RF-based method achieved the highest accuracy of 88%.

Conclusion: The study highlights issues in non-standard DDD methods and demonstrates a high-performance method when implemented appropriately.

Abstract: Driver drowsiness detection (DDD) prevents road accidents caused by driver
fatigue. Vehicle dynamics-based DDD has been proposed as a method that is both
economical and high performance. However, there are concerns about the
reliability of performance metrics and the reproducibility of many of the
existing methods. For instance, some previous studies seem to have a data
leakage issue among training and test datasets, and many do not openly provide
the datasets they used. To this end, this paper aims to compare the performance
of representative vehicle dynamics-based DDD methods under a transparent and
fair framework that uses a public dataset. We first develop a framework for
extracting features from an open dataset by Aygun et al. and performing DDD
with lightweight ML models; the framework is carefully designed to support a
variety of onfigurations. Second, we implement three existing representative
methods and a concise random forest (RF)-based method in the framework.
Finally, we report the results of experiments to verify the reproducibility and
clarify the performance of DDD based on common metrics. Among the evaluated
methods, the RF-based method achieved the highest accuracy of 88 %. Our
findings imply the issues inherent in DDD methods developed in a non-standard
manner, and demonstrate a high performance method implemented appropriately.

</details>


### [168] [Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression](https://arxiv.org/abs/2506.07033)
*Yung-Chien Wang,Kuang-Da Wang,Wei-Yao Wang,Wen-Chih Peng*

Main category: cs.LG

TL;DR: The paper proposes MATI, a method designed for tabular imbalance regression tasks, which includes Region-Aware Mixture Expert and Test-Time Self-Supervised Expert Aggregation. It shows a 7.1% MAE improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing work on data imbalance mainly focuses on classification tasks, while the imbalance issue in tabular regression tasks remains underexplored, especially when assumptions about known and balanced test distributions may not hold in practice.

Method: MATI is proposed with two key components: (i) Region-Aware Mixture Expert that uses Gaussian Mixture Model to capture related regions and synthesize region-specific experts; (ii) Test-Time Self-Supervised Expert Aggregation that dynamically adjusts expert weights based on test data features.

Result: MATI was evaluated on four real-world datasets with three types of test distributions and achieved a 7.1% improvement in MAE compared to existing methods.

Conclusion: MATI effectively addresses the imbalance issue in tabular regression tasks by capturing region-specific characteristics and adapting to varying test distributions.

Abstract: Tabular data serve as a fundamental and ubiquitous representation of
structured information in numerous real-world applications, e.g., finance and
urban planning. In the realm of tabular imbalanced applications, data imbalance
has been investigated in classification tasks with insufficient instances in
certain labels, causing the model's ineffective generalizability. However, the
imbalance issue of tabular regression tasks is underexplored, and yet is
critical due to unclear boundaries for continuous labels and simplifying
assumptions in existing imbalance regression work, which often rely on known
and balanced test distributions. Such assumptions may not hold in practice and
can lead to performance degradation. To address these issues, we propose MATI:
Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular
Imbalance Regression, featuring two key innovations: (i) the Region-Aware
Mixture Expert, which adopts a Gaussian Mixture Model to capture the underlying
related regions. The statistical information of each Gaussian component is then
used to synthesize and train region-specific experts to capture the unique
characteristics of their respective regions. (ii) Test-Time Self-Supervised
Expert Aggregation, which dynamically adjusts region expert weights based on
test data features to reinforce expert adaptation across varying test
distributions. We evaluated MATI on four real-world tabular imbalance
regression datasets, including house pricing, bike sharing, and age prediction.
To reflect realistic deployment scenarios, we adopted three types of test
distributions: a balanced distribution with uniform target frequencies, a
normal distribution that follows the training data, and an inverse distribution
that emphasizes rare target regions. On average across these three test
distributions, MATI achieved a 7.1% improvement in MAE compared to existing
methods.

</details>


### [169] [Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning](https://arxiv.org/abs/2506.07040)
*Yang Xu,Swetha Ganesh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: This paper presents the first Q-learning and actor-critic algorithms for robust average reward MDPs with non-asymptotic convergence under contamination, TV distance and Wasserstein distance uncertainty sets. The authors show that the robust Q Bellman operator is a strict contractive mapping with respect to a carefully constructed semi-norm, leading to stochastic approximation updates that learn the optimal robust Q function in O˜(ϵ^-2) samples. They also present an actor-critic algorithm that attains an ϵ-optimal robust policy in O˜(ϵ^-3) samples.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to advance the theory of distributionally robust reinforcement learning in the average reward setting by developing Q-learning and actor-critic algorithms that can handle uncertainty sets such as contamination, TV distance and Wasserstein distance.

Method: The method involves showing that the robust Q Bellman operator is a strict contractive mapping with respect to a carefully constructed semi-norm. This leads to stochastic approximation updates that learn the optimal robust Q function. The same idea is used for robust Q function estimation, which can be further used for critic estimation. Coupling it with theories in robust policy mirror descent update, they present a natural actor-critic algorithm.

Result: The result is that the proposed algorithms attain an ϵ-optimal robust policy in O˜(ϵ^-3) samples. These results advance the theory of distributionally robust reinforcement learning in the average reward setting.

Conclusion: In conclusion, the authors have presented the first Q-learning and actor-critic algorithms for robust average reward MDPs with non-asymptotic convergence under contamination, TV distance and Wasserstein distance uncertainty sets.

Abstract: We present the first $Q$-learning and actor-critic algorithms for robust
average reward Markov Decision Processes (MDPs) with non-asymptotic convergence
under contamination, TV distance and Wasserstein distance uncertainty sets. We
show that the robust $Q$ Bellman operator is a strict contractive mapping with
respect to a carefully constructed semi-norm with constant functions being
quotiented out. This property supports a stochastic approximation update, that
learns the optimal robust $Q$ function in $\tilde{\cO}(\epsilon^{-2})$ samples.
We also show that the same idea can be used for robust $Q$ function estimation,
which can be further used for critic estimation. Coupling it with theories in
robust policy mirror descent update, we present a natural actor-critic
algorithm that attains an $\epsilon$-optimal robust policy in
$\tilde{\cO}(\epsilon^{-3})$ samples. These results advance the theory of
distributionally robust reinforcement learning in the average reward setting.

</details>


### [170] [FairPFN: A Tabular Foundation Model for Causal Fairness](https://arxiv.org/abs/2506.07049)
*Jake Robertson,Noah Hollmann,Samuel Müller,Noor Awad,Frank Hutter*

Main category: cs.LG

TL;DR: Machine learning systems in critical sectors can perpetuate social inequalities due to demographic biases in historical data. Current causal fairness frameworks assume prior knowledge of the correct causal model, restricting their applicability. This paper proposes FairPFN, a model pre-trained on synthetic causal fairness data that identifies and mitigates causal effects of protected attributes without requiring causal model knowledge, showing strong performance across various scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current causal fairness frameworks which assume prior knowledge of the correct causal model, restricting their applicability in complex fairness scenarios where causal models are unknown or difficult to identify.

Method: Propose FairPFN, a tabular foundation model pre-trained on synthetic causal fairness data to identify and mitigate the causal effects of protected attributes in its predictions without requiring knowledge of the causal model.

Result: FairPFN demonstrates strong performance in identifying and removing protected causal effects across a diverse set of hand-crafted and real-world scenarios relative to robust baseline methods.

Conclusion: FairPFN bridges the gap in causal fairness by not requiring knowledge of the causal model, paving the way for future research and making causal fairness more accessible to a wider variety of complex fairness problems.

Abstract: Machine learning (ML) systems are utilized in critical sectors, such as
healthcare, law enforcement, and finance. However, these systems are often
trained on historical data that contains demographic biases, leading to ML
decisions that perpetuate or exacerbate existing social inequalities. Causal
fairness provides a transparent, human-in-the-loop framework to mitigate
algorithmic discrimination, aligning closely with legal doctrines of direct and
indirect discrimination. However, current causal fairness frameworks hold a key
limitation in that they assume prior knowledge of the correct causal model,
restricting their applicability in complex fairness scenarios where causal
models are unknown or difficult to identify. To bridge this gap, we propose
FairPFN, a tabular foundation model pre-trained on synthetic causal fairness
data to identify and mitigate the causal effects of protected attributes in its
predictions. FairPFN's key contribution is that it requires no knowledge of the
causal model and still demonstrates strong performance in identifying and
removing protected causal effects across a diverse set of hand-crafted and
real-world scenarios relative to robust baseline methods. FairPFN paves the way
for promising future research, making causal fairness more accessible to a
wider variety of complex fairness problems.

</details>


### [171] [Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead](https://arxiv.org/abs/2506.07054)
*Uri Koren,Navdeep Kumar,Uri Gadot,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

TL;DR: The paper introduces Policy Gradient with Tree Search (PGTS), a reinforcement learning method that uses an m-step lookahead to improve policy optimization. It shows theoretically and empirically that PGTS can escape local traps, enhance performance, and achieve better solutions in complex environments.


<details>
  <summary>Details</summary>
Motivation: Classical policy gradient methods often get stuck in suboptimal local optima, especially in large or complex environments.

Method: Policy Gradient with Tree Search (PGTS) integrates an m-step lookahead mechanism into the policy optimization process. Theoretical analysis is provided to show that increasing tree search depth reduces undesirable stationary points and improves worst-case performance without requiring updates across the entire state space.

Result: Empirical evaluations on various MDP structures (Ladder, Tightrope, Gridworld) demonstrate PGTS's ability to exhibit 'farsightedness', navigate challenging reward landscapes, escape local traps where standard PG fails, and achieve superior solutions.

Conclusion: PGTS provides a way to overcome the limitations of classical policy gradient methods by incorporating a lookahead mechanism, leading to improved performance in complex environments.

Abstract: Classical policy gradient (PG) methods in reinforcement learning frequently
converge to suboptimal local optima, a challenge exacerbated in large or
complex environments. This work investigates Policy Gradient with Tree Search
(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance
policy optimization. We provide theoretical analysis demonstrating that
increasing the tree search depth $m$-monotonically reduces the set of
undesirable stationary points and, consequently, improves the worst-case
performance of any resulting stationary policy. Critically, our analysis
accommodates practical scenarios where policy updates are restricted to states
visited by the current policy, rather than requiring updates across the entire
state space. Empirical evaluations on diverse MDP structures, including Ladder,
Tightrope, and Gridworld environments, illustrate PGTS's ability to exhibit
"farsightedness," navigate challenging reward landscapes, escape local traps
where standard PG fails, and achieve superior solutions.

</details>


### [172] [E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models](https://arxiv.org/abs/2506.07078)
*Jiaheng Dong,Hong Jia,Soumyajit Chatterjee,Abhirup Ghosh,James Bailey,Ting Dang*

Main category: cs.LG

TL;DR: E-BATS is an efficient backpropagation-free TTA framework for speech foundation models, offering improved accuracy and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing test-time adaptation (TTA) methods either consume too much memory or lack accuracy when applied to speech tasks due to differences in task formulation, noise characteristics, and model architecture compared to vision tasks.

Method: E-BATS incorporates three key components: lightweight prompt adaptation for feature alignment, a multi-scale loss to address global and local distribution shifts, and a test-time exponential moving average mechanism for stable adaptation across utterances.

Result: Experiments on four noisy speech datasets showed consistent improvements with 4.1%-13.5% accuracy gains over backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to backpropagation-based methods.

Conclusion: E-BATS enables scalable and robust adaptation under acoustic variability, paving the way for more efficient adaptation approaches for practical speech processing systems.

Abstract: Speech Foundation Models encounter significant performance degradation when
deployed in real-world scenarios involving acoustic domain shifts, such as
background noise and speaker accents. Test-time adaptation (TTA) has recently
emerged as a viable strategy to address such domain shifts at inference time
without requiring access to source data or labels. However, existing TTA
approaches, particularly those relying on backpropagation, are
memory-intensive, limiting their applicability in speech tasks and
resource-constrained settings. Although backpropagation-free methods offer
improved efficiency, existing ones exhibit poor accuracy. This is because they
are predominantly developed for vision tasks, which fundamentally differ from
speech task formulations, noise characteristics, and model architecture, posing
unique transferability challenges. In this paper, we introduce E-BATS, the
first Efficient BAckpropagation-free TTA framework designed explicitly for
speech foundation models. E-BATS achieves a balance between adaptation
effectiveness and memory efficiency through three key components: (i)
lightweight prompt adaptation for a forward-pass-based feature alignment, (ii)
a multi-scale loss to capture both global (utterance-level) and local
distribution shifts (token-level) and (iii) a test-time exponential moving
average mechanism for stable adaptation across utterances. Experiments
conducted on four noisy speech datasets spanning sixteen acoustic conditions
demonstrate consistent improvements, with 4.1%-13.5% accuracy gains over
backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to
backpropagation-based methods. By enabling scalable and robust adaptation under
acoustic variability, this work paves the way for developing more efficient
adaptation approaches for practical speech processing systems in real-world
environments.

</details>


### [173] [State Entropy Regularization for Robust Reinforcement Learning](https://arxiv.org/abs/2506.07085)
*Uri Koren,Yonatan Ashlag,Mirco Mutti,Esther Derman,Pierre-Luc Bacon,Shie Mannor*

Main category: cs.LG

TL;DR: State entropy regularization in reinforcement learning improves robustness to structured and spatially correlated perturbations, unlike standard robust RL methods.


<details>
  <summary>Details</summary>
Motivation: To explore the theoretical guarantees of state entropy regularization in reinforcement learning which has shown better exploration and sample complexity empirically.

Method: Provide a comprehensive characterization of robustness properties of state entropy regularization, including formal guarantees under reward and transition uncertainty, and contrast it with policy entropy regularization.

Result: State entropy regularization improves robustness to structured and spatially correlated perturbations. The method's performance is more sensitive to the number of rollouts used for policy evaluation compared to policy entropy.

Conclusion: State entropy regularization offers advantages in terms of robustness, particularly for structured and spatially correlated perturbations, but its effectiveness can be sensitive to the number of rollouts for policy evaluation.

Abstract: State entropy regularization has empirically shown better exploration and
sample complexity in reinforcement learning (RL). However, its theoretical
guarantees have not been studied. In this paper, we show that state entropy
regularization improves robustness to structured and spatially correlated
perturbations. These types of variation are common in transfer learning but
often overlooked by standard robust RL methods, which typically focus on small,
uncorrelated changes. We provide a comprehensive characterization of these
robustness properties, including formal guarantees under reward and transition
uncertainty, as well as settings where the method performs poorly. Much of our
analysis contrasts state entropy with the widely used policy entropy
regularization, highlighting their different benefits. Finally, from a
practical standpoint, we illustrate that compared with policy entropy, the
robustness advantages of state entropy are more sensitive to the number of
rollouts used for policy evaluation.

</details>


### [174] [Pointwise confidence estimation in the non-linear $\ell^2$-regularized least squares](https://arxiv.org/abs/2506.07088)
*Ilja Kuzborskij,Yasin Abbasi Yadkori*

Main category: cs.LG

TL;DR: The paper presents a method for high-probability non-asymptotic confidence estimation in the context of ℓ2-regularized non-linear least-squares problems. It introduces a pointwise confidence bound that scales with the similarity of test inputs to training data, generalizing classical linear results. An efficient computation method is proposed, and empirical evidence shows improved performance over bootstrapping.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable confidence estimation methods in non-linear least-squares settings, particularly for local minimizers of regularized training loss. Classical methods based on asymptotic normality may not be suitable for finite-sample regimes or complex models like neural networks.

Method: The authors derive a pointwise confidence bound for predictions in the ℓ2-regularized non-linear least-squares setting. This bound involves a weighted norm using the inverse-Hessian matrix of the objective function, which captures the similarity of test inputs to training data in the feature space.

Result: The proposed confidence bound scales appropriately with the similarity of test inputs to training data. Empirical results demonstrate that the method provides a better coverage/width trade-off compared to bootstrapping, a commonly used alternative.

Conclusion: This work provides a non-asymptotic confidence estimation approach for non-linear least-squares problems, offering an improvement over existing techniques like bootstrapping. The method is computationally efficient and has strong theoretical underpinnings.

Abstract: We consider a high-probability non-asymptotic confidence estimation in the
$\ell^2$-regularized non-linear least-squares setting with fixed design. In
particular, we study confidence estimation for local minimizers of the
regularized training loss. We show a pointwise confidence bound, meaning that
it holds for the prediction on any given fixed test input $x$. Importantly, the
proposed confidence bound scales with similarity of the test input to the
training data in the implicit feature space of the predictor (for instance,
becoming very large when the test input lies far outside of the training data).
This desirable last feature is captured by the weighted norm involving the
inverse-Hessian matrix of the objective function, which is a generalized
version of its counterpart in the linear setting, $x^{\top} \text{Cov}^{-1} x$.
Our generalized result can be regarded as a non-asymptotic counterpart of the
classical confidence interval based on asymptotic normality of the MLE
estimator. We propose an efficient method for computing the weighted norm,
which only mildly exceeds the cost of a gradient computation of the loss
function. Finally, we complement our analysis with empirical evidence showing
that the proposed confidence bound provides better coverage/width trade-off
compared to a confidence estimation by bootstrapping, which is a gold-standard
method in many applications involving non-linear predictors such as neural
networks.

</details>


### [175] [Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data](https://arxiv.org/abs/2506.07092)
*Joydeb Kumar Sana,Mohammad M. Masud,M Sohel Rahman,M Saifur Rahman*

Main category: cs.LG

TL;DR: This paper proposes a Distributed Patient Similarity Computation (DPSC) technique based on data transformation methods for measuring patient similarity using both time series and static data. The approach enhances prediction performance and reduces computation time.


<details>
  <summary>Details</summary>
Motivation: To improve clinical decision support by accurately measuring patient similarity through effective processing of historical clinical records including time series and static data.

Method: The method combines adaptive Weight-of-Evidence (aWOE) and Z-score data transformation techniques for static data, and distributed Dynamic Time Warping (DTW) for time series data to compute patient similarity while preserving data privacy.

Result: For Coronary Artery Disease, the method improves AUC by 11.4%, accuracy by 10.20%, F-measure by 12.6%, and reduces computation time by up to 40%. For Congestive Heart Failure, it achieves improvements of 15.9% in AUC, 10.5% in accuracy, and 21.9% in F-measure.

Conclusion: The DPSC technique effectively boosts prediction performance and significantly reduces computational run-time, making it suitable for large-scale healthcare informatics applications.

Abstract: Patient similarity computation (PSC) is a fundamental problem in healthcare
informatics. The aim of the patient similarity computation is to measure the
similarity among patients according to their historical clinical records, which
helps to improve clinical decision support. This paper presents a novel
distributed patient similarity computation (DPSC) technique based on data
transformation (DT) methods, utilizing an effective combination of time series
and static data. Time series data are sensor-collected patients' information,
including metrics like heart rate, blood pressure, Oxygen saturation,
respiration, etc. The static data are mainly patient background and demographic
data, including age, weight, height, gender, etc. Static data has been used for
clustering the patients. Before feeding the static data to the machine learning
model adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)
methods have been performed, which improve the prediction performances. In
aWOE-based patient similarity models, sensitive patient information has been
processed using aWOE which preserves the data privacy of the trained models. We
used the Dynamic Time Warping (DTW) approach, which is robust and very popular,
for time series similarity. However, DTW is not suitable for big data due to
the significant computational run-time. To overcome this problem, distributed
DTW computation is used in this study. For Coronary Artery Disease, our DT
based approach boosts prediction performance by as much as 11.4%, 10.20%, and
12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of
Congestive Heart Failure (CHF), our proposed method achieves performance
enhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.
The proposed method reduces the computation time by as high as 40%.

</details>


### [176] [Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion](https://arxiv.org/abs/2506.07099)
*Wenying He,Jieling Huang,Junhua Gu,Ji Zhang,Yude Bai*

Main category: cs.LG

TL;DR: CoFILL是一种新的条件扩散模型，用于时空数据插补。它通过双流架构处理时域和频域特征，捕捉数据的快速波动和底层模式，从而实现更稳健的插补。实验表明，CoFILL在插补精度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 时空数据中的缺失问题对现代应用构成了重大挑战，现有的机器学习和深度学习方法在建模空间和时间维度的复杂相互依赖关系方面表现不佳，并且在数据插补过程中容易产生累积误差。

Method: 提出了CoFILL，一种新型的条件扩散模型。该模型利用扩散模型的优点生成高质量的插补结果，而无需依赖可能出错的先验估计。同时，采用创新的双流架构并行处理时域和频域特征，融合互补特征以捕捉数据的快速波动和底层模式。

Result: 实验表明，CoFILL的噪声预测网络能够成功将随机噪声转化为与真实数据分布一致的有意义值，其插补精度优于现有最先进方法。

Conclusion: CoFILL为时空数据插补提供了一种有效的新方法，具有更高的插补精度和鲁棒性。

Abstract: Missing data in spatiotemporal systems presents a significant challenge for
modern applications, ranging from environmental monitoring to urban traffic
management. The integrity of spatiotemporal data often deteriorates due to
hardware malfunctions and software failures in real-world deployments. Current
approaches based on machine learning and deep learning struggle to model the
intricate interdependencies between spatial and temporal dimensions effectively
and, more importantly, suffer from cumulative errors during the data imputation
process, which propagate and amplify through iterations. To address these
limitations, we propose CoFILL, a novel Conditional Diffusion Model for
spatiotemporal data imputation. CoFILL builds on the inherent advantages of
diffusion models to generate high-quality imputations without relying on
potentially error-prone prior estimates. It incorporates an innovative
dual-stream architecture that processes temporal and frequency domain features
in parallel. By fusing these complementary features, CoFILL captures both rapid
fluctuations and underlying patterns in the data, which enables more robust
imputation. The extensive experiments reveal that CoFILL's noise prediction
network successfully transforms random noise into meaningful values that align
with the true data distribution. The results also show that CoFILL outperforms
state-of-the-art methods in imputation accuracy. The source code is publicly
available at https://github.com/joyHJL/CoFILL.

</details>


### [177] [Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings](https://arxiv.org/abs/2506.07109)
*Rong-Xi Tan,Ming Chen,Ke Xue,Yao Wang,Yaoyuan Wang,Sheng Fu,Chao Qian*

Main category: cs.LG

TL;DR: This paper explores the use of language model embeddings for universal black-box optimization (BBO), presenting methods that enable cross-domain optimization and demonstrating their effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: Current offline BBO approaches are limited to single-task and fixed-dimensional settings due to the lack of unified representations for heterogeneous numerical spaces, hindering progress towards general-purpose BBO algorithms.

Method: The authors propose multiple approaches inspired by advances in language models, including an end-to-end learning framework based on next-token prediction and prioritizing the learning of latent spaces with strong representational capabilities. These methods aim to unify language model priors and learn string embedding spaces for overcoming traditional barriers in universal BBO.

Result: Experiments using collected offline BBO tasks and data from open-source academic works show the universality and effectiveness of the proposed methods in achieving cross-domain universal optimization.

Conclusion: The findings indicate that incorporating language model embeddings can overcome limitations in traditional BBO, paving the way for the development of general-purpose BBO algorithms.

Abstract: The pursuit of universal black-box optimization (BBO) algorithms is a
longstanding goal. However, unlike domains such as language or vision, where
scaling structured data has driven generalization, progress in offline BBO
remains hindered by the lack of unified representations for heterogeneous
numerical spaces. Thus, existing offline BBO approaches are constrained to
single-task and fixed-dimensional settings, failing to achieve cross-domain
universal optimization. Recent advances in language models (LMs) offer a
promising path forward: their embeddings capture latent relationships in a
unifying way, enabling universal optimization across different data types
possible. In this paper, we discuss multiple potential approaches, including an
end-to-end learning framework in the form of next-token prediction, as well as
prioritizing the learning of latent spaces with strong representational
capabilities. To validate the effectiveness of these methods, we collect
offline BBO tasks and data from open-source academic works for training.
Experiments demonstrate the universality and effectiveness of our proposed
methods. Our findings suggest that unifying language model priors and learning
string embedding space can overcome traditional barriers in universal BBO,
paving the way for general-purpose BBO algorithms. The code is provided at
https://github.com/lamda-bbo/universal-offline-bbo.

</details>


### [178] [Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models](https://arxiv.org/abs/2506.07121)
*Ren-Jian Wang,Ke Xue,Zeyu Qin,Ziniu Li,Sheng Tang,Hao-Tian Li,Shengcai Liu,Chao Qian*

Main category: cs.LG

TL;DR: This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework to enhance the diversity and effectiveness of adversarial prompts for evaluating the safety of large language models.


<details>
  <summary>Details</summary>
Motivation: Previous approaches to red-teaming LLMs have limitations in capturing meaningful variation in attack strategies and restricted coverage across potential attack styles and risk categories due to simplistic metrics and reliance on single attacker models.

Method: QDRT achieves goal-driven diversity through behavior-conditioned training with a behavioral replay buffer and trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories.

Result: Empirical evaluation shows that QDRT generates more diverse and effective attacks against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5.

Conclusion: QDRT provides a systematic and effective approach to automated red-teaming, advancing the field of LLM safety and supporting responsible LLM deployment.

Abstract: Ensuring safety of large language models (LLMs) is important. Red teaming--a
systematic approach to identifying adversarial prompts that elicit harmful
responses from target LLMs--has emerged as a crucial safety evaluation method.
Within this framework, the diversity of adversarial prompts is essential for
comprehensive safety assessments. We find that previous approaches to
red-teaming may suffer from two key limitations. First, they often pursue
diversity through simplistic metrics like word frequency or sentence embedding
similarity, which may not capture meaningful variation in attack strategies.
Second, the common practice of training a single attacker model restricts
coverage across potential attack styles and risk categories. This paper
introduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to
address these limitations. QDRT achieves goal-driven diversity through
behavior-conditioned training and implements a behavioral replay buffer in an
open-ended manner. Additionally, it trains multiple specialized attackers
capable of generating high-quality attacks across diverse styles and risk
categories. Our empirical evaluation demonstrates that QDRT generates attacks
that are both more diverse and more effective against a wide range of target
LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the
field of LLM safety by providing a systematic and effective approach to
automated red-teaming, ultimately supporting the responsible deployment of
LLMs.

</details>


### [179] [Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning](https://arxiv.org/abs/2506.07134)
*Eshwar S. R.,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.LG

TL;DR: An abstract about Reliable Policy Iteration (RPI) in Reinforcement Learning (RL), which ensures monotonic improvement and convergence guarantees under function approximation.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning algorithms with function approximation are challenging to use correctly. The common policy iteration's guarantee of monotonic improvement collapses even under linear function approximation.

Method: Introduced Reliable Policy Iteration (RPI). It replaces the common projection or Bellman-error minimization during policy evaluation with a Bellman-based constrained optimization.

Result: RPI confers textbook monotonicity on its value estimates and these estimates also lower bound the true return. RPI-enhanced variants consistently maintain their lower-bound guarantee while matching or surpassing the performance of all baseline methods in classical control tasks.

Conclusion: RPI is the first algorithm with monotonicity and convergence guarantees under function approximation and can be integrated into model-free PI implementations such as DQN and DDPG.

Abstract: Despite decades of research, it remains challenging to correctly use
Reinforcement Learning (RL) algorithms with function approximation. A prime
example is policy iteration, whose fundamental guarantee of monotonic
improvement collapses even under linear function approximation. To address this
issue, we introduce Reliable Policy Iteration (RPI). It replaces the common
projection or Bellman-error minimization during policy evaluation with a
Bellman-based constrained optimization. We prove that not only does RPI confer
textbook monotonicity on its value estimates but these estimates also lower
bound the true return. Also, their limit partially satisfies the unprojected
Bellman equation, emphasizing RPI's natural fit within RL. RPI is the first
algorithm with such monotonicity and convergence guarantees under function
approximation. For practical use, we provide a model-free variant of RPI that
amounts to a novel critic. It can be readily integrated into primary model-free
PI implementations such as DQN and DDPG. In classical control tasks, such
RPI-enhanced variants consistently maintain their lower-bound guarantee while
matching or surpassing the performance of all baseline methods.

</details>


### [180] [AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models](https://arxiv.org/abs/2506.07165)
*Qi Liu,Jingqing Ruan,Hao Li,Haodong Zhao,Desheng Wang,Jiansong Chen,Wan Guanglu,Xunliang Cai,Zhi Zheng,Tong Xu*

Main category: cs.LG

TL;DR: 为了克服现有大型语言模型多目标偏好对齐方法的局限性，研究提出了自适应多目标偏好优化（AMoPO）框架，通过引入维度感知生成指标作为隐式奖励和自适应权重分配机制，实现了在不同偏好维度上的动态平衡。实验结果表明，AMoPO在7B、14B和32B模型上均展现出优越的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标偏好对齐方法存在两个主要问题：(1) 无法有效平衡各种偏好维度；(2) 对辅助奖励/参考模型的依赖增加了计算复杂度。为了解决这些问题，需要一种新的框架来实现动态平衡并减少对额外模型的依赖。

Method: 提出了一种名为AMoPO的新框架，该框架通过将多目标优化范式引入到使用维度感知生成指标作为隐式奖励中，从而无需额外的奖励或参考模型即可实现偏好对齐。此外，还引入了一种自适应权重分配机制，将生成空间建模为高斯分布，以实现偏好维度的动态优先级调整。

Result: 实证结果表明，AMoPO比最先进的基线方法高出28.5%。在7B、14B和32B模型上的实验展示了AMoPO的可扩展能力。多维度分析进一步验证了其适应性和有效性。

Conclusion: AMoPO能够实现维度感知的偏好对齐，并且在性能和可扩展性方面表现出色，证明了其优越性。

Abstract: Existing multi-objective preference alignment methods for large language
models (LLMs) face limitations: (1) the inability to effectively balance
various preference dimensions, and (2) reliance on auxiliary reward/reference
models introduces computational complexity. To address these challenges, we
propose Adaptive Multi-objective Preference Optimization (AMoPO), a novel
framework that achieves dynamic balance across preference dimensions. By
introducing the multi-objective optimization paradigm to use the
dimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with
diverse preferences without additional reward models or reference models. We
introduce an adaptive weight assignment mechanism that models the generation
space as a Gaussian distribution, allowing dynamic prioritization of preference
dimensions. Empirical results demonstrate that AMoPO outperforms
state-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B
models reveal the scaling ability of AMoPO. Moreover, additional analysis of
multiple dimensions verifies its adaptability and effectiveness. These findings
validate AMoPO's capability to achieve dimension-aware preference alignment,
highlighting its superiority. Our codes and datasets are available at
https://github.com/Javkonline/AMoPO.

</details>


### [181] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: GAGA is an efficient framework for TAG representation learning which achieves high accuracy with only 1% data annotation.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with complex textual information in TAGs. Recent methods using LLMs require extensive annotations or fine-tuning, which are time-consuming and costly.

Method: GAGA focuses on annotating representative nodes and edges to reduce annotation time and cost. It constructs an annotation graph capturing topological relationships among these annotations and employs a two-level alignment module to integrate the annotation graph with the TAG.

Result: Experiments demonstrate that GAGA achieves classification accuracies on par with or surpassing state-of-the-art methods while requiring only 1% of the data to be annotated.

Conclusion: GAGA is an efficient framework for TAG representation learning that significantly reduces the need for extensive annotations without compromising accuracy.

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [182] [Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2506.07179)
*Kaiqi Wu,Weiyang Kong,Sen Zhang,Yubao Liu,Zitong Chen*

Main category: cs.LG

TL;DR: The paper presents RAGL, a model for traffic prediction which improves upon existing adaptive graph learning methods by introducing embedding regularization and an efficient graph convolution operator, showing superior performance and scalability.


<details>
  <summary>Details</summary>
Motivation: Traffic prediction is crucial for travel planning and urban management, but current adaptive graph learning methods either lack embedding regularization or have scalability issues.

Method: The RAGL model uses a regularized adaptive graph learning framework combining Stochastic Shared Embedding (SSE) with adaptive graph convolution through a residual difference mechanism for embedding regularization and noise suppression. It also employs the Efficient Cosine Operator (ECO) for scalable graph convolution with linear time complexity.

Result: Experiments on four large-scale real-world traffic datasets demonstrate that RAGL outperforms state-of-the-art methods in prediction accuracy while maintaining competitive computational efficiency.

Conclusion: RAGL addresses the limitations of previous adaptive graph learning methods for traffic forecasting by providing both improved prediction accuracy and scalability.

Abstract: Traffic prediction is a critical task in spatial-temporal forecasting with
broad applications in travel planning and urban management. Adaptive graph
convolution networks have emerged as mainstream solutions due to their ability
to learn node embeddings in a data-driven manner and capture complex latent
dependencies. However, existing adaptive graph learning methods for traffic
forecasting often either ignore the regularization of node embeddings, which
account for a significant proportion of model parameters, or face scalability
issues from expensive graph convolution operations. To address these
challenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.
First, we introduce a regularized adaptive graph learning framework that
synergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via
a residual difference mechanism, achieving both embedding regularization and
noise suppression. Second, to ensure scalability on large road networks, we
develop the Efficient Cosine Operator (ECO), which performs graph convolution
based on the cosine similarity of regularized embeddings with linear time
complexity. Extensive experiments on four large-scale real-world traffic
datasets show that RAGL consistently outperforms state-of-the-art methods in
terms of prediction accuracy and exhibits competitive computational efficiency.

</details>


### [183] [Learning based on neurovectors for tabular data: a new neural network approach](https://arxiv.org/abs/2506.07185)
*J. C. Husillos,A. Gallego,A. Roma,A. Troncoso*

Main category: cs.LG

TL;DR: This paper introduces Neurovectors, a new approach for tabular data processing that uses interconnected nodes and vector relationships. It replaces traditional weight updates with energy propagation, enhancing adaptability and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional artificial neural networks in terms of explainability and efficiency for tabular data processing.

Method: Neurovectors encode information by structuring data in vector spaces where energy propagation drives the learning process, generating dynamic representations of knowledge.

Result: Competitive accuracy is achieved compared to standard machine learning and deep learning models on classification and regression tasks using datasets from UCI and Kaggle repositories.

Conclusion: Neurovectors offer an innovative paradigm for tabular data processing with improved interpretability and efficiency.

Abstract: In this paper, we present a novel learning approach based on Neurovectors, an
innovative paradigm that structures information through interconnected nodes
and vector relationships for tabular data processing. Unlike traditional
artificial neural networks that rely on weight adjustment through
backpropagation, Neurovectors encode information by structuring data in vector
spaces where energy propagation, rather than traditional weight updates, drives
the learning process, enabling a more adaptable and explainable learning
process. Our method generates dynamic representations of knowledge through
neurovectors, thereby improving both the interpretability and efficiency of the
predictive model. Experimental results using datasets from well-established
repositories such as the UCI machine learning repository and Kaggle are
reported both for classification and regression. To evaluate its performance,
we compare our approach with standard machine learning and deep learning
models, showing that Neurovectors achieve competitive accuracy.

</details>


### [184] [Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach](https://arxiv.org/abs/2506.07191)
*Ramisa Farha,Joshua O. Olukoya*

Main category: cs.LG

TL;DR: This study uses a robust analytical framework to uncover patterns in survival outcomes among breast cancer patients from diverse racial and geographical backgrounds using the SEER 2021 dataset. It highlights disparities in breast cancer treatment and care, contributing to global efforts to improve outcomes and reduce disparities.


<details>
  <summary>Details</summary>
Motivation: To identify and comprehend dissimilarities in breast cancer survival outcomes among patients from diverse racial and geographical backgrounds.

Method: The research integrates exploratory data analysis (EDA) to identify key variables influencing survival rates and employs survival analysis techniques such as Kaplan-Meier estimator, log-rank test, and Cox Proportional Hazards model to analyze variations across racial groups and countries.

Result: The outcome is a detailed statistical analysis highlighting disparities in breast cancer treatment and care, serving as a tool for developing targeted interventions to address inequalities effectively.

Conclusion: This study aims to contribute to global efforts to improve breast cancer outcomes and reduce treatment disparities.

Abstract: This study employs a robust analytical framework to uncover patterns in
survival outcomes among breast cancer patients from diverse racial and
geographical backgrounds. This research uses the SEER 2021 dataset to analyze
breast cancer survival outcomes to identify and comprehend dissimilarities. Our
approach integrates exploratory data analysis (EDA), through this we identify
key variables that influence survival rates and employ survival analysis
techniques, including the Kaplan-Meier estimator and log-rank test and the
advanced modeling Cox Proportional Hazards model to determine how survival
rates vary across racial groups and countries. Model validation and
interpretation are undertaken to ensure the reliability of our findings, which
are documented comprehensively to inform policymakers and healthcare
professionals. The outcome of this paper is a detailed version of statistical
analysis that not just highlights disparities in breast cancer treatment and
care but also serves as a foundational tool for developing targeted
interventions to address the inequalities effectively. Through this research,
our aim is to contribute to the global efforts to improve breast cancer
outcomes and reduce treatment disparities.

</details>


### [185] [GGBall: Graph Generative Model on Poincaré Ball](https://arxiv.org/abs/2506.07198)
*Tianci Bu,Chuanrui Wang,Hao Ma,Haoren Zheng,Xin Lu,Tailin Wu*

Main category: cs.LG

TL;DR: GGBall is a hyperbolic framework for graph generation that integrates geometric inductive biases with modern generative paradigms, reducing degree MMD significantly compared to state-of-the-art baselines.


<details>
  <summary>Details</summary>
Motivation: Generating graphs with hierarchical structures is challenging due to the limitations of Euclidean geometry in capturing exponential complexity.

Method: GGBall combines a Hyperbolic Vector-Quantized Autoencoder (HVQVAE) with a Riemannian flow matching prior defined via closed-form geodesics. It also includes hyperbolic GNN and Transformer layers that operate entirely within the manifold.

Result: Empirically, our model reduces degree MMD by over 75% on Community-Small and over 40% on Ego-Small compared to state-of-the-art baselines.

Conclusion: These results highlight the potential of hyperbolic geometry as a powerful foundation for the generative modeling of complex, structured, and hierarchical data domains.

Abstract: Generating graphs with hierarchical structures remains a fundamental
challenge due to the limitations of Euclidean geometry in capturing exponential
complexity. Here we introduce \textbf{GGBall}, a novel hyperbolic framework for
graph generation that integrates geometric inductive biases with modern
generative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder
(HVQVAE) with a Riemannian flow matching prior defined via closed-form
geodesics. This design enables flow-based priors to model complex latent
distributions, while vector quantization helps preserve the curvature-aware
structure of the hyperbolic space. We further develop a suite of hyperbolic GNN
and Transformer layers that operate entirely within the manifold, ensuring
stability and scalability. Empirically, our model reduces degree MMD by over
75\% on Community-Small and over 40\% on Ego-Small compared to state-of-the-art
baselines, demonstrating an improved ability to preserve topological
hierarchies. These results highlight the potential of hyperbolic geometry as a
powerful foundation for the generative modeling of complex, structured, and
hierarchical data domains. Our code is available at
\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.

</details>


### [186] [Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward](https://arxiv.org/abs/2506.07218)
*Tong Xiao,Xin Xu,Zhenya Huang,Hongyu Gao,Quan Liu,Qi Liu,Enhong Chen*

Main category: cs.LG

TL;DR: An abstract about enhancing multimodal reasoning capabilities of MLLMs through a novel method called Perception-R1, which focuses on improving multimodal perception capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of existing RLVR methods in effectively enhancing the multimodal perception capabilities of MLLMs, which is crucial for complex multimodal reasoning.

Method: Propose Perception-R1, a method that introduces a novel visual perception reward. It collects textual visual annotations from CoT trajectories as references and uses a judging LLM to assess consistency between these annotations and MLLM responses during RLVR training.

Result: Extensive experiments show the effectiveness of Perception-R1, achieving state-of-the-art performance on most multimodal reasoning benchmarks with only 1,442 training data.

Conclusion: Perception-R1 effectively incentivizes both multimodal perception and reasoning capabilities of MLLMs.

Abstract: Enhancing the multimodal reasoning capabilities of Multimodal Large Language
Models (MLLMs) is a challenging task that has attracted increasing attention in
the community. Recently, several studies have applied Reinforcement Learning
with Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the
reasoning abilities of MLLMs. However, these works largely overlook the
enhancement of multimodal perception capabilities in MLLMs, which serve as a
core prerequisite and foundational component of complex multimodal reasoning.
Through McNemar's test, we find that existing RLVR method fails to effectively
enhance the multimodal perception capabilities of MLLMs, thereby limiting their
further improvement in multimodal reasoning. To address this limitation, we
propose Perception-R1, which introduces a novel visual perception reward that
explicitly encourages MLLMs to perceive the visual content accurately, thereby
can effectively incentivizing both their multimodal perception and reasoning
capabilities. Specifically, we first collect textual visual annotations from
the CoT trajectories of multimodal problems, which will serve as visual
references for reward assignment. During RLVR training, we employ a judging LLM
to assess the consistency between the visual annotations and the responses
generated by MLLM, and assign the visual perception reward based on these
consistency judgments. Extensive experiments on several multimodal reasoning
benchmarks demonstrate the effectiveness of our Perception-R1, which achieves
state-of-the-art performance on most benchmarks using only 1,442 training data.

</details>


### [187] [VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution](https://arxiv.org/abs/2506.07229)
*Mateusz Gajewski,Mikołaj Morzy,Adam Karczmarz,Piotr Sankowski*

Main category: cs.LG

TL;DR: VARSHAP is a new model-agnostic local feature attribution method that uses prediction variance reduction as importance metric, showing better performance than SHAP and LIME.


<details>
  <summary>Details</summary>
Motivation: Existing feature attribution methods such as SHAP have limitations in capturing true local model behavior due to global dependence.

Method: VARSHAP employs Shapley value framework while using the reduction of prediction variance as the key importance metric of features, making it resilient to global data distribution shifts.

Result: Experiments on synthetic and real-world datasets show VARSHAP outperforms KernelSHAP and LIME in both quantitative and qualitative aspects.

Conclusion: VARSHAP provides a more effective way for local feature attribution compared to existing methods.

Abstract: Existing feature attribution methods like SHAP often suffer from global
dependence, failing to capture true local model behavior. This paper introduces
VARSHAP, a novel model-agnostic local feature attribution method which uses the
reduction of prediction variance as the key importance metric of features.
Building upon Shapley value framework, VARSHAP satisfies the key Shapley
axioms, but, unlike SHAP, is resilient to global data distribution shifts.
Experiments on synthetic and real-world datasets demonstrate that VARSHAP
outperforms popular methods such as KernelSHAP or LIME, both quantitatively and
qualitatively.

</details>


### [188] [Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs](https://arxiv.org/abs/2506.07240)
*Roy Eisenstadt,Itamar Zimerman,Lior Wolf*

Main category: cs.LG

TL;DR: The paper explores how LLMs understand and control the length of their reasoning process, introduces an interactive progress bar for visualization, manipulates internal progress encoding to reduce unnecessary steps, and demonstrates that this method improves accuracy and reduces inference latency.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of answers by understanding and regulating the length of reasoning in LLMs during explicit thought processes.

Method: 1. Show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization. 2. Manipulate the internal progress encoding during inference to generate a more concise chain of thoughts.

Result: Empirical results show that the 'overclocking' method mitigates overthinking, improves answer accuracy, and reduces inference latency.

Conclusion: This method provides insights into the model's planning dynamics and offers a way to optimize the reasoning process in LLMs.

Abstract: Recently, techniques such as explicit structured reasoning have demonstrated
strong test-time scaling behavior by enforcing a separation between the model's
internal "thinking" process and the final response. A key factor influencing
answer quality in this setting is the length of the thinking stage. When the
reasoning is too short, the model may fail to capture the complexity of the
task. Conversely, when it is too long, the model may overthink, leading to
unnecessary computation and degraded performance. This paper explores and
exploits the underlying mechanisms by which LLMs understand and regulate the
length of their reasoning during explicit thought processes. First, we show
that LLMs encode their progress through the reasoning process and introduce an
interactive progress bar visualization, which is then used to reveal insights
on the model's planning dynamics. Second, we manipulate the internal progress
encoding during inference to reduce unnecessary steps and generate a more
concise and decisive chain of thoughts. Our empirical results demonstrate that
this "overclocking" method mitigates overthinking, improves answer accuracy,
and reduces inference latency. Our code is publicly available.

</details>


### [189] [Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models](https://arxiv.org/abs/2506.07247)
*Ngoc-Quan Pham,Tuan Truong,Quyen Tran,Tan Nguyen,Dinh Phung,Trung Le*

Main category: cs.LG

TL;DR: The paper presents Interactive Bayesian Distributional Robustness (IBDR), a new Bayesian inference framework that models particle interactions to improve ensemble quality via increased particle diversity. It is based on a generalized theoretical framework connecting distributional population loss with the approximate posterior, leading to a dual optimization procedure enforcing robustness and diversity. Evaluated using VTAB-1K and common reasoning language tasks, IBDR outperforms various baselines.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the ensemble quality in Bayesian inference by modeling the interactions between particles, which can increase particle diversity.

Method: Interactive Bayesian Distributional Robustness (IBDR) is a novel Bayesian inference framework that models particle interactions to enhance ensemble quality through increased particle diversity. It is grounded in a generalized theoretical framework that connects the distributional population loss with the approximate posterior, motivating a practical dual optimization procedure that enforces distributional robustness while fostering particle diversity.

Result: IBDR's performance was evaluated against various baseline methods using the VTAB-1K benchmark and the common reasoning language task. The results consistently show that IBDR outperforms these baselines.

Conclusion: IBDR is effective in real-world applications as it consistently outperforms various baselines when evaluated using different benchmarks and tasks.

Abstract: We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel
Bayesian inference framework that allows modeling the interactions between
particles, thereby enhancing ensemble quality through increased particle
diversity. IBDR is grounded in a generalized theoretical framework that
connects the distributional population loss with the approximate posterior,
motivating a practical dual optimization procedure that enforces distributional
robustness while fostering particle diversity. We evaluate IBDR's performance
against various baseline methods using the VTAB-1K benchmark and the common
reasoning language task. The results consistently show that IBDR outperforms
these baselines, underscoring its effectiveness in real-world applications.

</details>


### [190] [A Stable Whitening Optimizer for Efficient Neural Network Training](https://arxiv.org/abs/2506.07254)
*Kevin Frans,Sergey Levine,Pieter Abbeel*

Main category: cs.LG

TL;DR: This paper proposes SPlus, an improved method for neural network optimization based on the Shampoo family of algorithms. It addresses three key issues: divergence caused by caching matrix-inverses, learning rate transfer across network width, and parameter noise from high learning rates. Experiments show that SPlus can achieve Adam's validation performance with fewer gradient steps and less wallclock time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the efficiency and stability of neural network optimization by addressing the limitations of the existing Shampoo algorithm.

Method: SPlus introduces a bounded update combining historical eigenbasis with instantaneous normalization to prevent divergence, adapts shape-aware scaling for learning rate transfer, and proposes an iterate-averaging scheme to handle parameter noise.

Result: SPlus reaches Adam's validation performance within 44% of the gradient steps and 62% of the wallclock time on Transformer training tasks including language modelling, image classification, and diffusion modelling.

Conclusion: SPlus is a more efficient and stable optimization method compared to Adam and naive Shampoo.

Abstract: In this work, we take an experimentally grounded look at neural network
optimization. Building on the Shampoo family of algorithms, we identify and
alleviate three key issues, resulting in the proposed SPlus method. First, we
find that naive Shampoo is prone to divergence when matrix-inverses are cached
for long periods. We introduce an alternate bounded update combining a
historical eigenbasis with instantaneous normalization, resulting in
across-the-board stability and significantly lower computational requirements.
Second, we adapt a shape-aware scaling to enable learning rate transfer across
network width. Third, we find that high learning rates result in large
parameter noise, and propose a simple iterate-averaging scheme which unblocks
faster learning. To properly confirm these findings, we introduce a pointed
Transformer training benchmark, considering three objectives (language
modelling, image classification, and diffusion modelling) across different
stages of training. On average, SPlus is able to reach the validation
performance of Adam within 44% of the gradient steps and 62% of the wallclock
time.

</details>


### [191] [A Cramér-von Mises Approach to Incentivizing Truthful Data Sharing](https://arxiv.org/abs/2506.07272)
*Alex Clinton,Thomas Zeng,Yiding Chen,Xiaojin Zhu,Kirthevasan Kandasamy*

Main category: cs.LG

TL;DR: 在现代数据市场中，奖励机制常被用于激励参与者贡献数据。然而，基于数据量的奖励方案容易被操控，例如提交伪造或低质量的数据以获取更多奖励。为了解决这个问题，本文提出了一种基于Cramér-von Mises统计量的两样本测试的奖励机制，该机制可以严格激励参与者提交更真实的数据，并且减少数据伪造和不诚实报告的行为。研究证明，在贝叶斯和先验未知的情况下，真实报告构成了（可能近似的）纳什均衡。通过理论分析和实际数据的实验验证，本文展示了该方法在三个典型数据共享问题中的有效性，同时放松了先前工作对数据分布的强假设。


<details>
  <summary>Details</summary>
Motivation: 现有的数据市场奖励机制容易受到操纵，因为参与者可能会提交伪造或低质量的数据来增加自己的收益。为了促进数据的真实性和高质量，需要一种新的奖励机制，能够在不依赖于强数据分布假设的情况下，激励参与者提交真实数据并减少不诚实行为。

Method: 本文开发了一种基于Cramér-von Mises统计量的两样本测试的奖励机制。该方法通过比较每个参与者的数据与其他人的数据，严格激励参与者提交更真实的数据，并惩罚数据伪造和其他类型的不诚实报告。此外，研究还证明了在贝叶斯和先验未知的情况下，真实报告构成（可能近似的）纳什均衡。

Result: 理论上，该方法在三个典型的数据共享问题中放松了先前工作对数据分布的强假设。实证上，通过模拟和真实世界语言及图像数据的实验，展示了该机制能够有效激励参与者进行真实的数据共享。

Conclusion: 本文提出的基于Cramér-von Mises统计量的两样本测试的奖励机制，能够有效地激励参与者提交真实数据，减少数据伪造和不诚实报告的行为，适用于多种数据共享场景，并且在理论和实证上都得到了验证。

Abstract: Modern data marketplaces and data sharing consortia increasingly rely on
incentive mechanisms to encourage agents to contribute data. However, schemes
that reward agents based on the quantity of submitted data are vulnerable to
manipulation, as agents may submit fabricated or low-quality data to inflate
their rewards. Prior work has proposed comparing each agent's data against
others' to promote honesty: when others contribute genuine data, the best way
to minimize discrepancy is to do the same. Yet prior implementations of this
idea rely on very strong assumptions about the data distribution (e.g.
Gaussian), limiting their applicability. In this work, we develop reward
mechanisms based on a novel, two-sample test inspired by the Cram\'er-von Mises
statistic. Our methods strictly incentivize agents to submit more genuine data,
while disincentivizing data fabrication and other types of untruthful
reporting. We establish that truthful reporting constitutes a (possibly
approximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We
theoretically instantiate our method in three canonical data sharing problems
and show that it relaxes key assumptions made by prior work. Empirically, we
demonstrate that our mechanism incentivizes truthful data sharing via
simulations and on real-world language and image data.

</details>


### [192] [Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models](https://arxiv.org/abs/2506.07275)
*Haochen Song,Dominik Hofer,Rania Islambouli,Laura Hawkins,Ananya Bhattacharjee,Meredith Franklin,Joseph Jay Williams*

Main category: cs.LG

TL;DR: This paper proposes a hybrid approach combining contextual multi-armed bandit (cMAB) algorithms with large language models (LLMs) for personalized interventions to reduce sedentary behavior and increase physical activity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of cMAB algorithms that require large participant samples and may overlook key psychological factors, by integrating LLMs for personalizing message content in interventions.

Method: A seven-day trial where participants receive daily messages assigned by one of four models: cMAB alone, LLM alone, combined cMAB with LLM personalization (cMABxLLM), or equal randomization (RCT). Four intervention types are evaluated: behavioral self-monitoring, gain-framed, loss-framed, and social comparison. Outcomes include daily step count and message acceptance assessed via ecological momentary assessments (EMAs).

Result: The causal inference framework is applied to evaluate the effects of each model on promoting physical activity and increasing message acceptance.

Conclusion: The findings provide insights into how LLM-based personalization and cMAB adaptation can complement each other in promoting physical activity through personalized behavioral messaging.

Abstract: Machine learning approaches, such as contextual multi-armed bandit (cMAB)
algorithms, offer a promising strategy to reduce sedentary behavior by
delivering personalized interventions to encourage physical activity. However,
cMAB algorithms typically require large participant samples to learn
effectively and may overlook key psychological factors that are not explicitly
encoded in the model. In this study, we propose a hybrid approach that combines
cMAB for selecting intervention types with large language models (LLMs) to
personalize message content. We evaluate four intervention types: behavioral
self-monitoring, gain-framed, loss-framed, and social comparison, each
delivered as a motivational message aimed at increasing motivation for physical
activity and daily step count. Message content is further personalized using
dynamic contextual factors including daily fluctuations in self-efficacy,
social influence, and regulatory focus. Over a seven-day trial, participants
receive daily messages assigned by one of four models: cMAB alone, LLM alone,
combined cMAB with LLM personalization (cMABxLLM), or equal randomization
(RCT). Outcomes include daily step count and message acceptance, assessed via
ecological momentary assessments (EMAs). We apply a causal inference framework
to evaluate the effects of each model. Our findings offer new insights into the
complementary roles of LLM-based personalization and cMAB adaptation in
promoting physical activity through personalized behavioral messaging.

</details>


### [193] [Tokenized Bandit for LLM Decoding and Alignment](https://arxiv.org/abs/2506.07276)
*Suho Shin,Chenghao Yang,Haifeng Xu,Mohammad T. Hajiaghayi*

Main category: cs.LG

TL;DR: 本文提出了tokenized linear bandit (TLB) 和 multi-armed bandit (TMAB) 问题，并在特定假设下提出了算法和理论结果。


<details>
  <summary>Details</summary>
Motivation: 受LLM解码和对齐的启发，研究者们引入了tokenized linear bandit (TLB) 和 multi-armed bandit (TMAB) 变体问题，这些问题关注于在每轮中从一个token集合中不可逆地选择token序列，并根据用户的查询观察到随机效用。

Method: 作者首先展示了在没有任何序列函数结构的情况下学习是不可能的，然后引入了一个自然假设——随着更多共同项距离递减(DDMC)，并基于此提出两种算法，分别具有$\tilde{O}(L\sqrt{T})$和$\tilde{O}(L\sqrt{T^{2/3}})$的遗憾界。此外，还得到了贪婪解码在LLM解码任务中的（几乎）最优性结果。

Result: 理论上证明了在DDMC假设下提出的算法的有效性，并验证了贪婪解码在某些任务中的高效性。实验上使用合成和真实世界数据集验证了算法性能及假设的合理性。

Conclusion: 本文通过引入新的bandit问题变体和相关假设，不仅为解决此类问题提供了有效算法，还解释了贪婪解码在LLM任务中的成功，同时为解码时间LLM对齐提供了直接应用。

Abstract: We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),
variants of linear and stochastic multi-armed bandit problems inspired by LLM
decoding and alignment. In these problems, at each round $t \in [T]$, a user
submits a query (context), and the decision maker (DM) sequentially selects a
token irrevocably from a token set. Once the sequence is complete, the DM
observes a random utility from the user, whose expectation is presented by a
sequence function mapping the chosen token sequence to a nonnegative real value
that depends on the query.
  In both problems, we first show that learning is impossible without any
structure on the sequence function. We introduce a natural assumption,
diminishing distance with more commons (DDMC), and propose algorithms with
regret $\tilde{O}(L\sqrt{T})$ and $\tilde{O}(L\sqrt{T^{2/3}})$ for TLB and
TMAB, respectively. As a side product, we obtain an (almost) optimality of the
greedy decoding for LLM decoding algorithm under DDMC, which justifies the
unresaonable effectiveness of greedy decoding in several tasks. This also has
an immediate application to decoding-time LLM alignment, when the misaligned
utility can be represented as the frozen LLM's utility and a linearly
realizable latent function. We finally validate our algorithm's performance
empirically as well as verify our assumptions using synthetic and real-world
datasets.

</details>


### [194] [EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments](https://arxiv.org/abs/2506.07288)
*Weijie Guan,Haohui Wang,Jian Kang,Lihui Liu,Dawei Zhou*

Main category: cs.LG

TL;DR: Evidential Reasoning Network (EVINET) is a novel framework integrating Beta embedding within a subjective logic framework to address misclassification and out-of-distribution detection in graph learning. It consists of two modules, Dissonance Reasoning for misclassification detection and Vacuity Reasoning for out-of-distribution detection, demonstrating superior performance over state-of-the-art methods in multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Graph learning models often operate under the closed-world assumption where all possible labels are known beforehand. However, real-world applications require models that can effectively handle open and noisy environments, including detecting misclassifications and out-of-distribution instances.

Method: The paper proposes EVINET, which integrates Beta embedding within a subjective logic framework. It includes two key modules: Dissonance Reasoning for detecting misclassifications and Vacuity Reasoning for detecting out-of-distribution samples.

Result: Experiments show that EVINET surpasses state-of-the-art methods across various metrics in tasks such as in-distribution classification, misclassification detection, and out-of-distribution detection.

Conclusion: EVINET highlights the importance of uncertainty estimation and logical reasoning for robust detection capabilities in graph learning, advancing the field towards open-world scenarios.

Abstract: Graph learning has been crucial to many real-world tasks, but they are often
studied with a closed-world assumption, with all possible labels of data known
a priori. To enable effective graph learning in an open and noisy environment,
it is critical to inform the model users when the model makes a wrong
prediction to in-distribution data of a known class, i.e., misclassification
detection or when the model encounters out-of-distribution from novel classes,
i.e., out-of-distribution detection. This paper introduces Evidential Reasoning
Network (EVINET), a framework that addresses these two challenges by
integrating Beta embedding within a subjective logic framework. EVINET includes
two key modules: Dissonance Reasoning for misclassification detection and
Vacuity Reasoning for out-of-distribution detection. Extensive experiments
demonstrate that EVINET outperforms state-of-the-art methods across multiple
metrics in the tasks of in-distribution classification, misclassification
detection, and out-of-distribution detection. EVINET demonstrates the necessity
of uncertainty estimation and logical reasoning for misclassification detection
and out-of-distribution detection and paves the way for open-world graph
learning. Our code and data are available at https://github.com/SSSKJ/EviNET.

</details>


### [195] [Pre-trained Large Language Models Learn Hidden Markov Models In-context](https://arxiv.org/abs/2506.07298)
*Yijia Dai,Zhaolin Gao,Yahya Satter,Sarah Dean,Jennifer J. Sun*

Main category: cs.LG

TL;DR: 通过预训练的大规模语言模型（LLMs）利用上下文学习（ICL），可以有效对隐马尔可夫模型（HMMs）生成的数据进行建模，达到接近理论最优的预测准确率，并在实际动物决策任务中表现出与人类专家设计模型相竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管HMM是用于建模具有潜在马尔可夫结构的序列数据的基础工具，但将其拟合到真实世界数据仍然计算上具有挑战性。因此，研究探索了使用预训练的大规模语言模型（LLMs）通过上下文学习（ICL）来对HMM生成的数据进行建模的可能性。

Method: 研究展示了预训练的大型语言模型（LLMs）如何通过上下文学习（ICL）对隐马尔可夫模型（HMMs）生成的数据进行建模，并分析了LLMs在各种合成HMM上的表现。此外，研究还探讨了影响模型表现的HMM特性，并提供了关于如何使用ICL作为复杂数据诊断工具的实用指南。

Result: LLMs在多种合成HMM上实现了接近理论最优的预测准确率，揭示了新的扩展趋势，并为这些经验观察提供了理论猜想。在现实世界的动物决策任务中，ICL的表现与人类专家设计的模型相当。

Conclusion: 这是首次证明ICL能够学习和预测HMM生成的序列，加深了我们对大规模语言模型中上下文学习的理解，并确立了其作为一种强大的工具，用于揭示复杂科学数据中的隐藏结构的潜力。

Abstract: Hidden Markov Models (HMMs) are foundational tools for modeling sequential
data with latent Markovian structure, yet fitting them to real-world data
remains computationally challenging. In this work, we show that pre-trained
large language models (LLMs) can effectively model data generated by HMMs via
in-context learning (ICL)$\unicode{x2013}$their ability to infer patterns from
examples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve
predictive accuracy approaching the theoretical optimum. We uncover novel
scaling trends influenced by HMM properties, and offer theoretical conjectures
for these empirical observations. We also provide practical guidelines for
scientists on using ICL as a diagnostic tool for complex data. On real-world
animal decision-making tasks, ICL achieves competitive performance with models
designed by human experts. To our knowledge, this is the first demonstration
that ICL can learn and predict HMM-generated sequences$\unicode{x2013}$an
advance that deepens our understanding of in-context learning in LLMs and
establishes its potential as a powerful tool for uncovering hidden structure in
complex scientific data.

</details>


### [196] [PASS: Private Attributes Protection with Stochastic Data Substitution](https://arxiv.org/abs/2506.07308)
*Yizhuo Chen,Chun-Fu,Chen,Hsiang Hsu,Shaohan Hu,Tarek Abdelzaher*

Main category: cs.LG

TL;DR: PASS, a novel approach that stochastically substitutes the original sample with another one according to certain probabilities and is trained with a novel loss function, effectively protects private attributes while maintaining data utilities for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for protecting private attributes by removing them from data while maintaining data utilities reveal severe vulnerability due to a common weakness rooted in their adversarial training based strategies.

Method: The proposed approach PASS stochastically substitutes the original sample with another one according to certain probabilities. It is trained with a novel loss function derived from information-theoretic objective defined for utility-preserving private attributes protection.

Result: PASS demonstrates effectiveness and generalizability through comprehensive evaluation on various datasets of different modalities including facial images, human activity sensory signals, and voice recording datasets.

Conclusion: PASS overcomes the limitation of existing methods and provides a more robust solution for protecting private attributes while preserving data utilities.

Abstract: The growing Machine Learning (ML) services require extensive collections of
user data, which may inadvertently include people's private information
irrelevant to the services. Various studies have been proposed to protect
private attributes by removing them from the data while maintaining the
utilities of the data for downstream tasks. Nevertheless, as we theoretically
and empirically show in the paper, these methods reveal severe vulnerability
because of a common weakness rooted in their adversarial training based
strategies. To overcome this limitation, we propose a novel approach, PASS,
designed to stochastically substitute the original sample with another one
according to certain probabilities, which is trained with a novel loss function
soundly derived from information-theoretic objective defined for
utility-preserving private attributes protection. The comprehensive evaluation
of PASS on various datasets of different modalities, including facial images,
human activity sensory signals, and voice recording datasets, substantiates
PASS's effectiveness and generalizability.

</details>


### [197] [Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference](https://arxiv.org/abs/2506.07311)
*Thomas Joshi,Herman Saini,Neil Dhillon,Antoni Viros i Martin,Kaoutar El Maghraoui*

Main category: cs.LG

TL;DR: This paper presents a new method that combines PagedAttention and PyTorch's FlexAttention to improve memory efficiency for long-context inference in Large Language Models (LLMs) by addressing issues with key-value (KV) caches. The approach reduces inference latency and manages memory more effectively, especially on longer sequences.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of severe memory inefficiencies during long-context inference in LLMs caused by conventional handling of key-value (KV) caches.

Method: Integration of PagedAttention with PyTorch's FlexAttention within IBM's Foundation Model Stack (FMS). This fused attention kernel efficiently gathers scattered KV data.

Result: Benchmarks on an NVIDIA L4 GPU show significantly reduced inference latency which grows only linearly with sequence length from 128 to 2048 tokens when using a global KV cache, as opposed to exponential increases without caching. Memory usage remains largely unchanged for single-step evaluations, with minimal increments only noticeable at lengths exceeding 2048 tokens due to power-of-two cache allocations.

Conclusion: The authors open-source the full implementation and discuss its implications for future long-context model deployment.

Abstract: Large Language Models (LLMs) encounter severe memory inefficiencies during
long-context inference due to conventional handling of key-value (KV) caches.
In this work, we introduce a novel integration of PagedAttention with PyTorch's
FlexAttention, addressing internal fragmentation and inefficiencies associated
with monolithic KV cache allocations. Implemented within IBM's Foundation Model
Stack (FMS), our fused attention kernel efficiently gathers scattered KV data.
Our benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced
inference latency, growing only linearly (~2x) with sequence length from 128 to
2048 tokens when utilizing a global KV cache, compared to exponential latency
increases without caching. While peak memory usage remains largely unchanged
for single-step evaluations (dominated by model weights and activations), paged
attention causes minimal incremental memory usage, observable only at sequence
lengths exceeding 2048 tokens due to its power-of-two cache allocations. We
open-source the full implementation and discuss its implications for future
long-context model deployment.

</details>


### [198] [Generative Modeling of Networked Time-Series via Transformer Architectures](https://arxiv.org/abs/2506.07312)
*Yusuf Elnady*

Main category: cs.LG

TL;DR: 设计了一个基于变压器的生成框架，用于生成时间序列数据，提高了现有和新的机器学习工作流的性能，模型具有广泛适用性并能产生高质量样本。


<details>
  <summary>Details</summary>
Motivation: 在安全领域，训练机器学习模型需要大量数据集，但数据访问受限是一个已知问题。虽然变压器模型可以合成新样本以扩大数据规模，但这些合成样本对模型的改进不如真实数据。

Method: 设计了一个高效的基于变压器的生成框架，用于生成时间序列数据。该模型旨在提高现有和新的机器学习工作流的性能，并且模型被设计为具有通用性，可以在不同的数据集上工作。

Result: 新的变压器模型达到了最先进的结果，能够产生高质量的样本。

Conclusion: 提出的基于变压器的生成框架不仅提高了机器学习模型的性能，还展示了其在不同数据集上的广泛适用性和生成高质量样本的能力。

Abstract: Many security and network applications require having large datasets to train
the machine learning models. Limited data access is a well-known problem in the
security domain. Recent studies have shown the potential of Transformer models
to enlarge the size of data by synthesizing new samples, but the synthesized
samples don't improve the models over the real data. To address this issue, we
design an efficient transformer-based model as a generative framework to
generate time-series data, that can be used to boost the performance of
existing and new ML workflows. Our new transformer model achieves the SOTA
results. We style our model to be generalizable and work across different
datasets, and produce high-quality samples.

</details>


### [199] [DEF: Diffusion-augmented Ensemble Forecasting](https://arxiv.org/abs/2506.07324)
*David Millard,Arielle Carr,Stéphane Gaudreault,Ali Baheri*

Main category: cs.LG

TL;DR: The paper introduces DEF, a novel method using conditional diffusion models to generate initial condition perturbations for transforming deterministic neural forecasting systems into stochastic ones, validated on the ERA5 reanalysis dataset.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating initial condition perturbations are mainly designed for numerical weather prediction solvers, restricting their use in machine learning-based weather prediction. This limitation necessitates case-by-case development of stochastic models in this domain.

Method: DEF employs a simple conditional diffusion model capable of generating structured perturbations, being applied iteratively, and utilizing a guidance term to control perturbation levels. It transforms any deterministic neural forecasting system into a stochastic one.

Result: DEF accumulates less error over long-term forecasts and produces meaningful forecast distributions. On the 5.625° ERA5 reanalysis dataset, it demonstrates improved predictive performance and reasonable spread estimates.

Conclusion: DEF is an effective approach for generating initial condition perturbations in machine learning-based weather prediction, offering improvements in long-term forecast accuracy and providing meaningful uncertainty estimates.

Abstract: We present DEF (\textbf{\ul{D}}iffusion-augmented \textbf{\ul{E}}nsemble
\textbf{\ul{F}}orecasting), a novel approach for generating initial condition
perturbations. Modern approaches to initial condition perturbations are
primarily designed for numerical weather prediction (NWP) solvers, limiting
their applicability in the rapidly growing field of machine learning for
weather prediction. Consequently, stochastic models in this domain are often
developed on a case-by-case basis. We demonstrate that a simple conditional
diffusion model can (1) generate meaningful structured perturbations, (2) be
applied iteratively, and (3) utilize a guidance term to intuitivey control the
level of perturbation. This method enables the transformation of any
deterministic neural forecasting system into a stochastic one. With our
stochastic extended systems, we show that the model accumulates less error over
long-term forecasts while producing meaningful forecast distributions. We
validate our approach on the 5.625$^\circ$ ERA5 reanalysis dataset, which
comprises atmospheric and surface variables over a discretized global grid,
spanning from the 1960s to the present. On this dataset, our method
demonstrates improved predictive performance along with reasonable spread
estimates.

</details>


### [200] [Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification](https://arxiv.org/abs/2506.07328)
*Jintao Yan,Tan Chen,Yuxuan Sun,Zhaojun Nan,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: Asynchronous Federated Learning (AFL) faces challenges due to device mobility and intermittent connectivity. This paper develops a theoretical model to understand the relationship between sparsification, model staleness, and mobility-induced contact patterns on AFL convergence. Based on this, a MADS algorithm is proposed to optimize sparsification degree according to contact time and model staleness. Experiments show that MADS improves image classification accuracy and reduces average displacement error in trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: Device mobility in Asynchronous Federated Learning introduces intermittent connectivity, requiring gradient sparsification which affects model staleness and thus AFL convergence.

Method: Theoretical model developed to characterize interplay among sparsification, model staleness, and mobility-induced contact patterns. Proposed MADS algorithm optimizes sparsification degree based on contact time and model staleness with closed-form solutions derived.

Result: MADS increases image classification accuracy on CIFAR-10 by 8.76% and reduces average displacement error in Argoverse dataset by 9.46%. Experimental results validate theoretical findings.

Conclusion: MADS algorithm effectively optimizes sparsification degree under different mobility conditions improving AFL convergence.

Abstract: Asynchronous Federated Learning (AFL) enables distributed model training
across multiple mobile devices, allowing each device to independently update
its local model without waiting for others. However, device mobility introduces
intermittent connectivity, which necessitates gradient sparsification and leads
to model staleness, jointly affecting AFL convergence. This paper develops a
theoretical model to characterize the interplay among sparsification, model
staleness and mobility-induced contact patterns, and their joint impact on AFL
convergence. Based on the analysis, we propose a mobility-aware dynamic
sparsification (MADS) algorithm that optimizes the sparsification degree based
on contact time and model staleness. Closed-form solutions are derived, showing
that under low-speed conditions, MADS increases the sparsification degree to
enhance convergence, while under high-speed conditions, it reduces the
sparsification degree to guarantee reliable uploads within limited contact
time. Experimental results validate the theoretical findings. Compared with the
state-of-the-art benchmarks, the MADS algorithm increases the image
classification accuracy on the CIFAR-10 dataset by 8.76% and reduces the
average displacement error in the Argoverse trajectory prediction dataset by
9.46%.

</details>


### [201] [Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models](https://arxiv.org/abs/2506.07334)
*Haoyu Wang,Peihao Wang,Mufei Li,Shikun Liu,Siqi Miao,Zhangyang Wang,Pan Li*

Main category: cs.LG

TL;DR: Graph-KV is a new method that uses structural inductive biases to improve performance on tasks with inter-segment dependencies, such as RAG and reasoning on graph-structured data. It leverages KV-caches of text segments and reduces positional bias.


<details>
  <summary>Details</summary>
Motivation: Modern large language models (LLMs) are inherently auto-regressive and require input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model's ability to leverage structural inductive biases.

Method: Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. 'Target' segments selectively attend only to the KV-caches of their designated 'source' segments, inducing a graph-structured block mask and enabling a message-passing-like step within the LLM.

Result: Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings.

Conclusion: By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV shows significant improvements in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures.

Abstract: Modern large language models (LLMs) are inherently auto-regressive, requiring
input to be serialized into flat sequences regardless of their structural
dependencies. This serialization hinders the model's ability to leverage
structural inductive biases, especially in tasks such as retrieval-augmented
generation (RAG) and reasoning on data with native graph structures, where
inter-segment dependencies are crucial. We introduce Graph-KV with the
potential to overcome this limitation. Graph-KV leverages the KV-cache of text
segments as condensed representations and governs their interaction through
structural inductive biases. In this framework, 'target' segments selectively
attend only to the KV-caches of their designated 'source' segments, rather than
all preceding segments in a serialized sequence. This approach induces a
graph-structured block mask, sparsifying attention and enabling a
message-passing-like step within the LLM. Furthermore, strategically allocated
positional encodings for source and target segments reduce positional bias and
context window consumption. We evaluate Graph-KV across three scenarios: (1)
seven RAG benchmarks spanning direct inference, multi-hop reasoning, and
long-document understanding; (2) Arxiv-QA, a novel academic paper QA task with
full-text scientific papers structured as citation ego-graphs; and (3) paper
topic classification within a citation network. By effectively reducing
positional bias and harnessing structural inductive biases, Graph-KV
substantially outperforms baselines, including standard costly sequential
encoding, across various settings. Code and the Graph-KV data are publicly
available.

</details>


### [202] [SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments](https://arxiv.org/abs/2506.07355)
*Yuya Okada,Takayuki Nishio*

Main category: cs.LG

TL;DR: The paper introduces SALT, a model adaptation framework for Split Computing that works under closed constraints without access to head and tail networks. It uses a compact adapter on the client side to refine latent features for user-specific adaptation. Evaluated on CIFAR-10 and CIFAR-100, it shows improved accuracy with less training latency compared to fine-tuning methods. Additionally, it enhances robust inference over lossy networks in edge-cloud environments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adapting models in closed environments where head and tail networks are proprietary and inaccessible, making conventional adaptation methods infeasible.

Method: SALT introduces a compact, trainable adapter on the client side which refines latent features from the head network, enabling user-specific adaptation without modifying original models or increasing communication overhead.

Result: SALT demonstrates improved accuracy with lower training latency compared to fine-tuning methods when evaluated on user-specific classification tasks using CIFAR-10 and CIFAR-100 datasets. It also facilitates robust inference over lossy networks.

Conclusion: SALT provides a practical solution for personalized inference in edge AI systems under strict system constraints with minimal deployment overhead.

Abstract: We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model
adaptation framework for Split Computing under closed constraints, where the
head and tail networks are proprietary and inaccessible to users. In such
closed environments, conventional adaptation methods are infeasible since they
require access to model parameters or architectures. SALT addresses this
challenge by introducing a compact, trainable adapter on the client side to
refine latent features from the head network, enabling user-specific adaptation
without modifying the original models or increasing communication overhead. We
evaluate SALT on user-specific classification tasks with CIFAR-10 and
CIFAR-100, demonstrating improved accuracy with lower training latency compared
to fine-tuning methods. Furthermore, SALT facilitates model adaptation for
robust inference over lossy networks, a common challenge in edge-cloud
environments. With minimal deployment overhead, SALT offers a practical
solution for personalized inference in edge AI systems under strict system
constraints.

</details>


### [203] [MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing](https://arxiv.org/abs/2506.07366)
*Haiyue Ma,Zhixu Du,Yiran Chen*

Main category: cs.LG

TL;DR: In multi-GPU Mixture-of-Experts (MoE) network, load imbalance occurs due to varying token processing among experts. Recent works improve inference load balance by duplicating popular experts across more GPUs, requiring distribution prediction before routing. This paper proposes MoE-GPS, a framework that selects optimal predictor design under different system configurations by quantifying the performance impact on system-level model runtime. Specifically, it promotes Distribution-Only Prediction which reduces overhead compared to Token-to-Expert Prediction and improves end-to-end inference performance.


<details>
  <summary>Details</summary>
Motivation: Load imbalance in multi-GPU Mixture-of-Experts (MoE) networks is a significant issue as each expert processes a different number of tokens. To address this, there's a need for effective prediction strategies that can optimize the distribution of tokens among experts to achieve better load balancing.

Method: The paper introduces MoE-GPS, a framework designed to guide the selection of the optimal predictor design under various system configurations. It focuses on Distribution-Only Prediction, a strategy that predicts overall token distribution rather than predicting for each token individually, significantly reducing overhead.

Result: On the Mixtral 8x7B MMLU dataset, MoE-GPS suggests that using Distribution-Only Prediction can improve end-to-end inference performance by over 23% compared to Token-to-Expert Prediction.

Conclusion: MoE-GPS provides an effective solution for selecting optimal predictor designs in MoE networks, with Distribution-Only Prediction showing substantial improvements in inference performance while reducing overhead.

Abstract: In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across
different GPUs, which creates load imbalance as each expert processes different
number of tokens. Recent works improve MoE inference load balance by
dynamically duplicating popular experts to more GPUs to process excessive
tokens, which requires predicting the distribution before routing. In this
paper, we discuss the tradeoff of prediction strategies, accuracies, overhead,
and end-to-end system performance. We propose MoE-GPS, a framework that guides
the selection of the optimal predictor design under various system
configurations, by quantifying the performance impact to system-level model
runtime. Specifically, we advocate for Distribution-Only Prediction, a
prediction strategy that only predicts overall token distribution which
significantly reduces overhead compared to the traditional Token-to-Expert
Prediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only
Prediction which improves end-to-end inference performance by more than 23%
compared with Token-to-Expert Prediction.

</details>


### [204] [Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization](https://arxiv.org/abs/2506.07378)
*Yuen Chen,Haozhe Si,Guojun Zhang,Han Zhao*

Main category: cs.LG

TL;DR: The paper develops a theory of moment alignment for domain generalization (DG) and introduces Closed-Form Moment Alignment (CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in closed-form, overcoming computational inefficiencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for aligning domain-level gradients and Hessians in DG are computationally inefficient and lack a clear understanding of their underlying principles.

Method: The authors extend the definition of transfer measure to domain generalization with multiple source domains, establish a target error bound, and prove that aligning derivatives across domains improves transfer measure. They connect feature moments and derivatives of the classifier head, establishing duality between feature learning and classifier fitting. Based on this theory, they introduce CMA, which aligns domain-level gradients and Hessians in closed-form without repeated backpropagation or sampling-based Hessian estimation.

Result: CMA demonstrates superior performance compared to Empirical Risk Minimization and state-of-the-art algorithms in both linear probing and full fine-tuning experiments.

Conclusion: Moment alignment provides a unifying understanding of previously disconnected approaches to DG such as Invariant Risk Minimization, gradient matching, and Hessian matching.

Abstract: Domain generalization (DG) seeks to develop models that generalize well to
unseen target domains, addressing the prevalent issue of distribution shifts in
real-world applications. One line of research in DG focuses on aligning
domain-level gradients and Hessians to enhance generalization. However,
existing methods are computationally inefficient and the underlying principles
of these approaches are not well understood. In this paper, we develop the
theory of moment alignment for DG. Grounded in \textit{transfer measure}, a
principled framework for quantifying generalizability between two domains, we
first extend the definition of transfer measure to domain generalization that
includes multiple source domains and establish a target error bound. Then, we
prove that aligning derivatives across domains improves transfer measure both
when the feature extractor induces an invariant optimal predictor across
domains and when it does not. Notably, moment alignment provides a unifying
understanding of Invariant Risk Minimization, gradient matching, and Hessian
matching, three previously disconnected approaches to DG. We further connect
feature moments and derivatives of the classifier head, and establish the
duality between feature learning and classifier fitting. Building upon our
theory, we introduce \textbf{C}losed-Form \textbf{M}oment \textbf{A}lignment
(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in
closed-form. Our method overcomes the computational inefficiencies of existing
gradient and Hessian-based techniques by eliminating the need for repeated
backpropagation or sampling-based Hessian estimation. We validate the efficacy
of our approach through two sets of experiments: linear probing and full
fine-tuning. CMA demonstrates superior performance in both settings compared to
Empirical Risk Minimization and state-of-the-art algorithms.

</details>


### [205] [RiemannFormer: A Framework for Attention in Curved Spaces](https://arxiv.org/abs/2506.07405)
*Zhongping Ji*

Main category: cs.LG

TL;DR: This research aims to unlock the potential of transformer-based architectures by providing a geometric interpretation for the attention mechanism and introducing an explicit mechanism to highlight local inductive bias, resulting in significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to provide a geometric interpretation for the attention mechanism in transformers.

Method: The method mainly involves metric tensors, tangent spaces, inner product, and their interrelations via the parallel transport of tangent vectors. Ingenious predefined configurations are used to reduce parameters and an explicit mechanism is introduced to highlight local inductive bias.

Result: Experimental results show significant performance improvements relative to the baseline.

Conclusion: The modules deliver significant performance improvements. Further evaluation experiments on visual and large language models will be conducted.

Abstract: This research endeavors to offer insights into unlocking the further
potential of transformer-based architectures. One of the primary motivations is
to offer a geometric interpretation for the attention mechanism in
transformers. In our framework, the attention mainly involves metric tensors,
tangent spaces, inner product, and how they relate to each other. These
quantities and structures at discrete positions are intricately interconnected
via the parallel transport of tangent vectors. To make the learning process
more efficient, we reduce the number of parameters through ingenious predefined
configurations. Moreover, we introduce an explicit mechanism to highlight a
neighborhood by attenuating the remote values, given that transformers
inherently neglect local inductive bias. Experimental results demonstrate that
our modules deliver significant performance improvements relative to the
baseline. More evaluation experiments on visual and large language models will
be launched successively.

</details>


### [206] [InverseScope: Scalable Activation Inversion for Interpreting Large Language Models](https://arxiv.org/abs/2506.07406)
*Yifan Luo,Zhennan Zhou,Bin Dong*

Main category: cs.LG

TL;DR: InverseScope是一种新的轻假设、可扩展框架，通过输入反演解释神经激活。它定义了生成相似激活的输入分布，并通过分析该分布推断编码特征。此外，还提出了一种新颖的条件生成架构，提高了高维空间采样的效率，并引入了基于采样输入计算特征一致率的定量评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有的特征可解释性方法通常依赖于关于表示结构的强假设，这些假设在实际中可能不成立。因此，需要一种更通用且高效的方法来解释大型语言模型（LLMs）的内部表示。

Method: 1. 引入InverseScope框架，通过输入反演解释神经激活。
2. 定义生成相似激活的输入分布并分析该分布以推断编码特征。
3. 提出一种新型条件生成架构，提高高维空间采样效率。
4. 引入定量评估协议，使用采样输入计算特征一致率来测试可解释性假设。

Result: InverseScope能够将反演基础的可解释性方法扩展到更大的模型和实用任务中，从而实现对现实世界LLMs内部表示的系统性和定量分析。

Conclusion: InverseScope为解释大型语言模型的内部表示提供了一种轻假设、可扩展的新方法，有助于克服现有方法中的强假设问题，推动可解释性研究的发展。

Abstract: Understanding the internal representations of large language models (LLMs) is
a central challenge in interpretability research. Existing feature
interpretability methods often rely on strong assumptions about the structure
of representations that may not hold in practice. In this work, we introduce
InverseScope, an assumption-light and scalable framework for interpreting
neural activations via input inversion. Given a target activation, we define a
distribution over inputs that generate similar activations and analyze this
distribution to infer the encoded features. To address the inefficiency of
sampling in high-dimensional spaces, we propose a novel conditional generation
architecture that significantly improves sample efficiency compared to previous
methods. We further introduce a quantitative evaluation protocol that tests
interpretability hypotheses using feature consistency rate computed over the
sampled inputs. InverseScope scales inversion-based interpretability methods to
larger models and practical tasks, enabling systematic and quantitative
analysis of internal representations in real-world LLMs.

</details>


### [207] [Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM](https://arxiv.org/abs/2506.07407)
*Yihong Jin,Ze Yang,Juntian Liu,Xinhe Xu*

Main category: cs.LG

TL;DR: In this paper, the authors propose an anomaly detection and early warning mechanism for intelligent monitoring systems in multi-cloud environments using Large-Scale Language Models (LLMs). The model introduces a multi-level feature extraction method combining LLMs with traditional machine learning to improve detection accuracy and real-time response. Experiments demonstrate superior performance over traditional systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the security and reliability of intelligent monitoring systems within multi-cloud environments, addressing the challenges posed by diverse cloud service providers and environments.

Method: The proposed method involves developing an anomaly detection and early warning mechanism that leverages Large-Scale Language Models (LLMs) alongside traditional machine learning techniques. A multi-level feature extraction approach is used, which combines the natural language processing capabilities of LLMs with conventional methods to detect anomalies more effectively and predict potential failures.

Result: The experimental results indicate that the proposed model outperforms traditional anomaly detection systems in terms of detection accuracy and latency, thereby improving the resilience and active management capabilities of cloud infrastructure.

Conclusion: The conclusion drawn from this study is that the integration of LLMs into the monitoring framework significantly enhances the ability to detect anomalies and predict failures in multi-cloud environments, leading to more secure and reliable cloud infrastructures.

Abstract: With the rapid development of multi-cloud environments, it is increasingly
important to ensure the security and reliability of intelligent monitoring
systems. In this paper, we propose an anomaly detection and early warning
mechanism for intelligent monitoring system in multi-cloud environment based on
Large-Scale Language Model (LLM). On the basis of the existing monitoring
framework, the proposed model innovatively introduces a multi-level feature
extraction method, which combines the natural language processing ability of
LLM with traditional machine learning methods to enhance the accuracy of
anomaly detection and improve the real-time response efficiency. By introducing
the contextual understanding capabilities of LLMs, the model dynamically adapts
to different cloud service providers and environments, so as to more
effectively detect abnormal patterns and predict potential failures.
Experimental results show that the proposed model is significantly better than
the traditional anomaly detection system in terms of detection accuracy and
latency, and significantly improves the resilience and active management
ability of cloud infrastructure.

</details>


### [208] [Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks](https://arxiv.org/abs/2506.07408)
*Xiaojun zhou,Chunna Zhao,Yaqun Huang,Chengli Zhou,Junjie Ye,Kemeng Xiang*

Main category: cs.LG

TL;DR: This paper proposes fractional-order matrix differentiation method, fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$), and fractional-order Autograd technology to enhance the practicality of fractional-order differentiation in deep learning.


<details>
  <summary>Details</summary>
Motivation: Fractional-order differentiation has many characteristics different from integer-order differentiation and can be applied to optimization algorithms of artificial neural networks. However, there is no fractional-order matrix differentiation method that is perfectly compatible with automatic differentiation (Autograd) technology due to insufficient theoretical research.

Method: The authors propose a fractional-order matrix differentiation calculation method called fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$). Based on this, they design fractional-order Autograd technology which enables the use of fractional-order differentiation in hidden layers. They also design fractional-order Linear (FLinear) and replace nn.Linear in multilayer perceptrons with FLinear for experiments.

Result: Through qualitative analysis of training and validation set $Loss$, quantitative analysis of test set indicators, and analysis of time consumption and GPU memory usage during model training, the authors verify the superior performance of ${{\bf{J}}^\alpha }$. It proves to be an excellent fractional-order gradient descent method in deep learning.

Conclusion: The proposed fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$) and fractional-order Autograd technology improve the practicality of fractional-order differentiation in deep learning.

Abstract: Fractional-order differentiation has many characteristics different from
integer-order differentiation. These characteristics can be applied to the
optimization algorithms of artificial neural networks to obtain better results.
However, due to insufficient theoretical research, at present, there is no
fractional-order matrix differentiation method that is perfectly compatible
with automatic differentiation (Autograd) technology. Therefore, we propose a
fractional-order matrix differentiation calculation method. This method is
introduced by the definition of the integer-order Jacobian matrix. We denote it
as fractional-order Jacobian matrix differentiation (${{\bf{J}}^\alpha }$).
Through ${{\bf{J}}^\alpha }$, we can carry out the matrix-based
fractional-order chain rule. Based on the Linear module and the
fractional-order differentiation, we design the fractional-order Autograd
technology to enable the use of fractional-order differentiation in hidden
layers, thereby enhancing the practicality of fractional-order differentiation
in deep learning. In the experiment, according to the PyTorch framework, we
design fractional-order Linear (FLinear) and replace nn.Linear in the
multilayer perceptron with FLinear. Through the qualitative analysis of the
training set and validation set $Loss$, the quantitative analysis of the test
set indicators, and the analysis of time consumption and GPU memory usage
during model training, we verify the superior performance of ${{\bf{J}}^\alpha
}$ and prove that it is an excellent fractional-order gradient descent method
in the field of deep learning.

</details>


### [209] [Variational Supervised Contrastive Learning](https://arxiv.org/abs/2506.07413)
*Ziwen Wang,Jiajun Fan,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: VarCon is proposed to solve two limitations of contrastive learning. It achieves state-of-the-art performance in several experiments, yields clearer decision boundaries and demonstrates superior performance in few-shot learning.


<details>
  <summary>Details</summary>
Motivation: Contrastive learning has two key limitations: semantically related instances can inadvertently be pushed apart and excessive reliance on large in-batch negatives and tailored augmentations hinders generalization.

Method: Reformulate supervised contrastive learning as variational inference over latent class variables and maximize a posterior-weighted evidence lower bound (ELBO) for efficient class-aware matching and fine-grained control over intra-class dispersion in the embedding space.

Result: (1) Achieves state-of-the-art performance for contrastive learning frameworks; (2) Yields substantially clearer decision boundaries and semantic organization in the embedding space; (3) Demonstrates superior performance in few-shot learning and superior robustness across various augmentation strategies.

Conclusion: VarCon solves the two limitations of contrastive learning and shows excellent performance in several aspects.

Abstract: Contrastive learning has proven to be highly efficient and adaptable in
shaping representation spaces across diverse modalities by pulling similar
samples together and pushing dissimilar ones apart. However, two key
limitations persist: (1) Without explicit regulation of the embedding
distribution, semantically related instances can inadvertently be pushed apart
unless complementary signals guide pair selection, and (2) excessive reliance
on large in-batch negatives and tailored augmentations hinders generalization.
To address these limitations, we propose Variational Supervised Contrastive
Learning (VarCon), which reformulates supervised contrastive learning as
variational inference over latent class variables and maximizes a
posterior-weighted evidence lower bound (ELBO) that replaces exhaustive
pair-wise comparisons for efficient class-aware matching and grants
fine-grained control over intra-class dispersion in the embedding space.
Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,
ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art
performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy
on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while
converging in just 200 epochs; (2) yields substantially clearer decision
boundaries and semantic organization in the embedding space, as evidenced by
KNN classification, hierarchical clustering results, and transfer-learning
assessments; and (3) demonstrates superior performance in few-shot learning
than supervised baseline and superior robustness across various augmentation
strategies.

</details>


### [210] [LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments](https://arxiv.org/abs/2506.07416)
*Jin Huang,Yuchao Jin,Le An,Josh Park*

Main category: cs.LG

TL;DR: This paper proposes an efficient Vision-Language Model (VLM) pipeline for embedded devices in robotics and autonomous driving, which reduces computational overhead through patch selection, token selection, and speculative decoding. Evaluated on NVIDIA DRIVE Thor, it achieves 2.5x latency reduction without accuracy loss, increasing to 3.2x with FP8 quantization.


<details>
  <summary>Details</summary>
Motivation: To enable real-time deployment of VLMs on resource-constrained embedded devices used in robotics and autonomous driving by reducing computational overhead without sacrificing task accuracy.

Method: The method involves a pipeline that uses patch selection to filter irrelevant camera views, a token selection module to shorten input sequences for the LLM, and speculative decoding to speed up token generation. Additionally, FP8 post-training quantization is applied to further enhance performance.

Result: The pipeline achieves a 2.5x reduction in end-to-end latency on the NVIDIA DRIVE Thor platform for autonomous driving applications without affecting task accuracy. With FP8 post-training quantization, the speed-up increases to 3.2x.

Conclusion: The proposed pipeline provides a feasible solution for real-time VLM deployment in resource-constrained environments, such as those found in robotics and autonomous driving.

Abstract: This paper introduces an efficient Vision-Language Model (VLM) pipeline
specifically optimized for deployment on embedded devices, such as those used
in robotics and autonomous driving. The pipeline significantly reduces the
computational overhead by jointly leveraging patch selection to filter
irrelevant camera views, a token selection module to reduce input sequence
length for the LLM, and speculative decoding to accelerate token generation.
Evaluation on the NVIDIA DRIVE Thor platform for automonous driving
application, our pipeline achieves $2.5\times$ end-to-end latency reduction
without compromising task accuracy. The speed-up further increases to
$3.2\times$ when applying FP8 post-training quantization. These results
demonstrate our pipeline as a viable solution for enabling real-time VLM
deployment in resource-constrained environments.

</details>


### [211] [Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs](https://arxiv.org/abs/2506.07417)
*Nan Sun,Xixun Lin,Zhiheng Zhou,Yanmin Shang,Zhenlin Cheng,Yanan Cao*

Main category: cs.LG

TL;DR: The paper proposes EviSEC, an OOD detector for dynamic graphs based on EDL, which addresses challenges of high bias/variance and score homogenization.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods mainly focus on static graphs and face issues like high bias and variance from single-point estimation and score homogenization due to lack of OOD training data.

Method: The authors propose EviSEC, which uses evidential deep learning. It redefines the output as a posterior Dirichlet distribution to explain input randomness and includes a spectrum-aware augmentation module to generate OOD approximations.

Result: Experiments on real-world datasets show that EviSEC effectively detects OOD samples in dynamic graphs.

Conclusion: EviSEC is an innovative solution for OOD detection in dynamic graphs that mitigates problems of high bias/variance and score homogenization.

Abstract: Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims
to identify whether incoming data deviates from the distribution of the
in-distribution (ID) training set, has garnered considerable attention in
security-sensitive fields. Current OOD detection paradigms primarily focus on
static graphs and confront two critical challenges: i) high bias and high
variance caused by single-point estimation, which makes the predictions
sensitive to randomness in the data; ii) score homogenization resulting from
the lack of OOD training data, where the model only learns ID-specific
patterns, resulting in overall low OOD scores and a narrow score gap between ID
and OOD data. To tackle these issues, we first investigate OOD detection in
dynamic graphs through the lens of Evidential Deep Learning (EDL).
Specifically, we propose EviSEC, an innovative and effective OOD detector via
Evidential Spectrum-awarE Contrastive Learning. We design an evidential neural
network to redefine the output as the posterior Dirichlet distribution,
explaining the randomness of inputs through the uncertainty of distribution,
which is overlooked by single-point estimation. Moreover, spectrum-aware
augmentation module generates OOD approximations to identify patterns with high
OOD scores, thereby widening the score gap between ID and OOD data and
mitigating score homogenization. Extensive experiments on real-world datasets
demonstrate that EviSAC effectively detects OOD samples in dynamic graphs.

</details>


### [212] [Federated In-Context Learning: Iterative Refinement for Improved Answer Quality](https://arxiv.org/abs/2506.07440)
*Ruhan Wang,Zhiyong Wang,Chengkai Huang,Rui Wang,Tong Yu,Lina Yao,John C. S. Lui,Dongruo Zhou*

Main category: cs.LG

TL;DR: The paper proposes Federated In-Context Learning (Fed-ICL), a framework that improves question-answering tasks through collaborative, iterative processes between clients and a central server without transmitting model parameters. It achieves strong performance with low communication costs.


<details>
  <summary>Details</summary>
Motivation: In-context learning (ICL) for question-answering tasks relies on high-quality examples which are often limited due to data privacy, annotation costs, and distribution disparities. Current solutions either incur significant communication overhead or fail to fully exploit local datasets.

Method: The proposed method is called Federated In-Context Learning (Fed-ICL). It enhances ICL via an iterative, collaborative process involving multi-round interactions between clients and a central server to progressively refine responses without the need to transmit model parameters.

Result: Theoretical guarantees for the convergence of Fed-ICL were established. Extensive experiments on standard QA benchmarks showed that Fed-ICL achieves strong performance while maintaining low communication costs.

Conclusion: Fed-ICL provides an effective solution to enhance in-context learning for question-answering tasks by leveraging local datasets through a federated approach, achieving good performance with minimal communication overhead.

Abstract: For question-answering (QA) tasks, in-context learning (ICL) enables language
models to generate responses without modifying their parameters by leveraging
examples provided in the input. However, the effectiveness of ICL heavily
depends on the availability of high-quality examples, which are often scarce
due to data privacy constraints, annotation costs, and distribution
disparities. A natural solution is to utilize examples stored on client
devices, but existing approaches either require transmitting model parameters -
incurring significant communication overhead - or fail to fully exploit local
datasets, limiting their effectiveness. To address these challenges, we propose
Federated In-Context Learning (Fed-ICL), a general framework that enhances ICL
through an iterative, collaborative process. Fed-ICL progressively refines
responses by leveraging multi-round interactions between clients and a central
server, improving answer quality without the need to transmit model parameters.
We establish theoretical guarantees for the convergence of Fed-ICL and conduct
extensive experiments on standard QA benchmarks, demonstrating that our
proposed approach achieves strong performance while maintaining low
communication costs.

</details>


### [213] [Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs](https://arxiv.org/abs/2506.07448)
*T. Duy Nguyen-Hien,Desi R. Ivanova,Yee Whye Teh,Wee Sun Lee*

Main category: cs.LG

TL;DR: This paper proposes using Bayesian Modeling of Experiments to handle uncertainty in large language models (LLMs), shifting from passive avoidance to active resolution of uncertainties, thereby enhancing reliability and transparency.


<details>
  <summary>Details</summary>
Motivation: There is a need for better tools to manage uncertainty in LLM deployments beyond simply rejecting uncertain outputs, as current methods are limited and do not address different sources of uncertainty effectively.

Method: The paper suggests adopting Bayesian Modeling of Experiments as a framework to systematically distinguish and respond to various sources of uncertainty in LLMs. This involves enabling LLMs to take appropriate actions based on context, such as seeking clarification or retrieving external data.

Result: Using the Bayesian Modeling framework allows LLMs to actively resolve uncertainties rather than passively avoid them, leading to more reliable and transparent systems that can be applied in high-stakes real-world situations.

Conclusion: Bayesian Modeling of Experiments provides a promising approach to enhance the management of uncertainty in LLM deployments, promoting more robust and adaptable systems.

Abstract: Although large language models (LLMs) are highly interactive and extendable,
current approaches to ensure reliability in deployments remain mostly limited
to rejecting outputs with high uncertainty in order to avoid misinformation.
This conservative strategy reflects the current lack of tools to systematically
distinguish and respond to different sources of uncertainty. In this paper, we
advocate for the adoption of Bayesian Modeling of Experiments -- a framework
that provides a coherent foundation to reason about uncertainty and clarify the
reducibility of uncertainty -- for managing and proactively addressing
uncertainty that arises in LLM deployments. This framework enables LLMs and
their users to take contextually appropriate steps, such as requesting
clarification, retrieving external information, or refining inputs. By
supporting active resolution rather than passive avoidance, it opens the door
to more reliable, transparent, and broadly applicable LLM systems, particularly
in high-stakes, real-world settings.

</details>


### [214] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: Large language models (LLMs) can be tricked by style patterns in malicious queries, increasing attack success rates. Fine-tuning on specific styles worsens this vulnerability. The study proposes SafeStyle, a defense strategy to maintain LLM safety.


<details>
  <summary>Details</summary>
Motivation: To understand whether and how style patterns in prompts compromise the safety of large language models (LLMs), and to find ways to mitigate these risks during alignment.

Method: Evaluated 32 LLMs across seven jailbreak benchmarks to observe the impact of style patterns on attack success rates. Investigated superficial style alignment through fine-tuning with specific styles. Proposed SafeStyle, a defense strategy involving safety training data augmented to match style pattern distributions.

Result: Malicious queries with style patterns increase attack success rates, with inflation correlating to pattern length and model attention. Fine-tuning on specific styles makes LLMs more vulnerable. SafeStyle outperforms baselines in maintaining LLM safety across different settings.

Conclusion: Style patterns in prompts can significantly compromise LLM safety. Superficial style alignment increases vulnerability, but using SafeStyle during fine-tuning can effectively mitigate these risks.

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [215] [ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning](https://arxiv.org/abs/2506.07459)
*Ziwen Wang,Jiajun Fan,Ruihan Guo,Thao Nguyen,Heng Ji,Ge Liu*

Main category: cs.LG

TL;DR: ProteinZero is a novel framework that uses online reinforcement learning to improve protein design models, significantly outperforming existing methods in structural accuracy, stability, and sequence diversity.


<details>
  <summary>Details</summary>
Motivation: Current protein generative models face limitations due to the lack of high-quality datasets for supervised pretraining, leading to lower success rates in protein design.

Method: ProteinZero employs a scalable RL framework with efficient proxy reward models based on ESM-fold and a rapid ddG predictor. It balances multi-reward maximization, KL-divergence from a reference model, and protein-embedding level diversity regularization to prevent mode collapse and promote sequence diversity.

Result: ProteinZero achieves significant improvements in structural accuracy, designability, thermodynamic stability, and sequence diversity, reducing design failure rates by 36%-48% compared to existing methods and achieving success rates exceeding 90%. The entire RL process can be run efficiently within 3 days using an 8 GPU node.

Conclusion: ProteinZero establishes a new paradigm for protein design, enabling continuous self-improvement through its own generated outputs, thus opening up new possibilities for exploring the vast protein design space.

Abstract: Protein generative models have shown remarkable promise in protein design but
still face limitations in success rate, due to the scarcity of high-quality
protein datasets for supervised pretraining. We present ProteinZero, a novel
framework that enables scalable, automated, and continuous self-improvement of
the inverse folding model through online reinforcement learning. To achieve
computationally tractable online feedback, we introduce efficient proxy reward
models based on ESM-fold and a novel rapid ddG predictor that significantly
accelerates evaluation speed. ProteinZero employs a general RL framework
balancing multi-reward maximization, KL-divergence from a reference model, and
a novel protein-embedding level diversity regularization that prevents mode
collapse while promoting higher sequence diversity. Through extensive
experiments, we demonstrate that ProteinZero substantially outperforms existing
methods across every key metric in protein design, achieving significant
improvements in structural accuracy, designability, thermodynamic stability,
and sequence diversity. Most impressively, ProteinZero reduces design failure
rates by approximately 36% - 48% compared to widely-used methods like
ProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates
exceeding 90% across diverse and complex protein folds. Notably, the entire RL
run on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,
including reward computation. Our work establishes a new paradigm for protein
design where models evolve continuously from their own generated outputs,
opening new possibilities for exploring the vast protein design space.

</details>


### [216] [Circumventing Backdoor Space via Weight Symmetry](https://arxiv.org/abs/2506.07467)
*Jie Peng,Hongwei Yang,Jing Zhao,Hengji Dong,Hui He,Weizhe Zhang,Haoyu He*

Main category: cs.LG

TL;DR: This paper proposes Two-stage Symmetry Connectivity (TSC), a new method to defend against backdoor attacks in deep neural networks. It works independently of data format and only requires a small amount of clean samples, effective in both supervised and self-supervised learning settings.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to backdoor attacks which can be implanted during training. Current defenses usually need labeled data or specific training procedures, making them hard to apply beyond supervised learning scenarios.

Method: The proposed method, TSC, leverages permutation invariance in neural networks and quadratic mode connectivity. It amplifies the loss on poisoned samples while maintaining clean sample accuracy through two stages.

Result: Experiments show that TSC achieves robust performance comparable to state-of-the-art methods in supervised learning and also generalizes well to self-supervised learning frameworks like SimCLR and CLIP.

Conclusion: TSC is an effective defense mechanism against backdoor attacks, working independently of data format and requiring only a small fraction of clean samples.

Abstract: Deep neural networks are vulnerable to backdoor attacks, where malicious
behaviors are implanted during training. While existing defenses can
effectively purify compromised models, they typically require labeled data or
specific training procedures, making them difficult to apply beyond supervised
learning settings. Notably, recent studies have shown successful backdoor
attacks across various learning paradigms, highlighting a critical security
concern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),
a novel backdoor purification defense that operates independently of data
format and requires only a small fraction of clean samples. Through theoretical
analysis, we prove that by leveraging permutation invariance in neural networks
and quadratic mode connectivity, TSC amplifies the loss on poisoned samples
while maintaining bounded clean accuracy. Experiments demonstrate that TSC
achieves robust performance comparable to state-of-the-art methods in
supervised learning scenarios. Furthermore, TSC generalizes to self-supervised
learning frameworks, such as SimCLR and CLIP, maintaining its strong defense
capabilities. Our code is available at https://github.com/JiePeng104/TSC.

</details>


### [217] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: Self-RedTeam是一种在线自我博弈强化学习算法，通过攻击者和防御者代理的持续交互实现动态协同适应，提升语言模型的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统的语言模型安全对齐方法采用反应式、分离的流程，导致攻击者过度拟合到过时的防御，而防御者则始终落后于新兴威胁。为了解决这一问题，需要一种能够动态协同适应的方法。

Method: 提出了一种名为Self-RedTeam的在线自我博弈强化学习算法，将安全对齐视为一个二人零和游戏。单一模型在攻击者和防御者角色之间交替，生成对抗性提示并加以防范，同时由奖励语言模型进行裁决。此外，还提出了隐藏的思维链（hidden Chain-of-Thought），允许代理私下规划，以提高对抗性多样性和减少过度拒绝。

Result: 与针对静态防御者的攻击者相比，Self-RedTeam发现了更多样化的攻击（+21.8% SBERT），并在安全性基准测试中表现出更高的鲁棒性（例如，在WildJailBreak上提高了+65.5%）。隐藏的思维链进一步提升了对抗性多样性和减少了过度拒绝。

Conclusion: 研究结果表明，从反应式修补转向主动协同进化是提升语言模型安全训练的关键，通过多代理强化学习（MARL）可以实现可扩展、自主和鲁棒的自我改进。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [218] [Premise Selection for a Lean Hammer](https://arxiv.org/abs/2506.07477)
*Thomas Zhu,Joshua Clune,Jeremy Avigad,Albert Qiaochu Jiang,Sean Welleck*

Main category: cs.LG

TL;DR: LeanHammer is the first end-to-end domain-general hammer for Lean, built on a novel neural premise selection system. It solves 21% more goals than existing selectors and generalizes well to diverse domains.


<details>
  <summary>Details</summary>
Motivation: Neural methods are transforming automated reasoning for proof assistants but integrating these advances into practical verification workflows remains challenging. The Lean proof assistant does not have a hammer despite its growing popularity.

Method: LeanHammer combines a novel neural premise selection system with symbolic proof search and reconstruction. The premise selector dynamically adapts to user-specific contexts.

Result: LeanHammer solves 21% more goals relative to existing premise selectors and generalizes well to diverse domains.

Conclusion: This work bridges the gap between neural retrieval and symbolic reasoning, making formal verification more accessible.

Abstract: Neural methods are transforming automated reasoning for proof assistants, yet
integrating these advances into practical verification workflows remains
challenging. Hammers are tools that interface with external automatic theorem
provers to automate tedious reasoning steps. They have dramatically improved
productivity in proof assistants, but the Lean proof assistant still does not
have a hammer despite its growing popularity. We present LeanHammer, the first
end-to-end domain-general hammer for Lean, built on a novel neural premise
selection system for a hammer in dependent type theory. Unlike existing Lean
premise selectors, our approach dynamically adapts to user-specific contexts
and combines with symbolic proof search and reconstruction to create a
practical hammer. With comprehensive evaluations, we show that our premise
selector enables LeanHammer to solve 21\% more goals relative to existing
premise selectors, and generalize well to diverse domains. Our work bridges the
gap between neural retrieval and symbolic reasoning, making formal verification
more accessible to researchers and practitioners.

</details>


### [219] [Explicit Preference Optimization: No Need for an Implicit Reward Model](https://arxiv.org/abs/2506.07492)
*Xiangkun Hu,Lemin Kong,Tong He,David Wipf*

Main category: cs.LG

TL;DR: This paper proves that DPO-based objectives have sub-optimal regularization and counter-intuitive interpolation behaviors, and introduces a new framework EXPO which avoids these issues.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of current methods for fine-tuning large language models (LLMs) to human preferences, particularly focusing on direct preference optimization (DPO) and its variants.

Method: The method involves proving that DPO-based objectives are subject to sub-optimal regularization and counter-intuitive interpolation behaviors due to the reparameterizations they use. To solve this, the paper introduces an explicit preference optimization framework termed EXPO, which does not require reparameterization and instead uses intuitively-appealing regularization factors.

Result: Empirical results confirm the analyses presented in the paper and demonstrate the efficacy of the EXPO framework.

Conclusion: In conclusion, the paper highlights the drawbacks of DPO-based methods and presents EXPO as a more effective alternative for optimizing LLMs according to human preferences.

Abstract: The generated responses of large language models (LLMs) are often fine-tuned
to human preferences through a process called reinforcement learning from human
feedback (RLHF). As RLHF relies on a challenging training sequence, whereby a
separate reward model is independently learned and then later applied to LLM
policy updates, ongoing research effort has targeted more straightforward
alternatives. In this regard, direct preference optimization (DPO) and its many
offshoots circumvent the need for a separate reward training step. Instead,
through the judicious use of a reparameterization trick that induces an
\textit{implicit} reward, DPO and related methods consolidate learning to the
minimization of a single loss function. And yet despite demonstrable success in
some real-world settings, we prove that DPO-based objectives are nonetheless
subject to sub-optimal regularization and counter-intuitive interpolation
behaviors, underappreciated artifacts of the reparameterizations upon which
they are based. To this end, we introduce an \textit{explicit} preference
optimization framework termed EXPO that requires no analogous
reparameterization to achieve an implicit reward. Quite differently, we merely
posit intuitively-appealing regularization factors from scratch that
transparently avoid the potential pitfalls of key DPO variants, provably
satisfying regularization desiderata that prior methods do not. Empirical
results serve to corroborate our analyses and showcase the efficacy of EXPO.

</details>


### [220] [Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks](https://arxiv.org/abs/2506.07500)
*Shakir Yousefi,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: Gumbel noise with a straight-through estimator is injected during training of Logic Gate Networks (LGNs) to speed up training, improve neuron utilization, and decrease the discretization gap.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks have high computational requirements and energy consumption, prompting researchers to seek more efficient solutions. LGNs offer efficient image classification but suffer from long training times and a discretization gap.

Method: Inject Gumbel noise with a straight-through estimator during training of LGNs to significantly speed up training, improve neuron utilization, and decrease the discretization gap.

Result: Trained networks 4.5 times faster in wall-clock time, reduced the discretization gap by 98%, and reduced the number of unused gates by 100%.

Conclusion: The injection of Gumbel noise results in implicit Hessian regularization, which improves the convergence properties of LGNs and makes them more viable for real-world deployment.

Abstract: Modern neural networks demonstrate state-of-the-art performance on numerous
existing benchmarks; however, their high computational requirements and energy
consumption prompt researchers to seek more efficient solutions for real-world
deployment. Logic gate networks (LGNs) learns a large network of logic gates
for efficient image classification. However, learning a network that can solve
a simple problem like CIFAR-10 can take days to weeks to train. Even then,
almost half of the network remains unused, causing a discretization gap. This
discretization gap hinders real-world deployment of LGNs, as the performance
drop between training and inference negatively impacts accuracy. We inject
Gumbel noise with a straight-through estimator during training to significantly
speed up training, improve neuron utilization, and decrease the discretization
gap. We theoretically show that this results from implicit Hessian
regularization, which improves the convergence properties of LGNs. We train
networks $4.5 \times$ faster in wall-clock time, reduce the discretization gap
by $98\%$, and reduce the number of unused gates by $100\%$.

</details>


### [221] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: In this paper, researchers propose a new method called graph of causal evolution (GoCE) to solve the problem in chain-of-model (CoM). GoCE can strengthen transformer's ability to capture long-range causal dependencies and improve its self-evolution ability. It surpasses CoM in design principles and provides valuable experience for future research.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation in CoM where each subchain only depends on the previous subchain information and may lose long-range dependencies due to the causal mask blocking global context flow between multi-level subchains.

Method: The proposed method, GoCE, maps token representation into a differentiable and sparse causal adjacency matrix, permeates causal constraints through each layer using causal-masked attention and causal-MoE, combines intervention consistency loss test and self-evolution gate to balance causal structure learning and adaptive updating of transformer architecture.

Result: GoCE was evaluated on datasets including CLUTRR, CLADDER, EX-FEVER, and CausalQA. The results show that it strengthens transformer's ability to capture long-range causal dependencies and improves its self-evolution ability.

Conclusion: GoCE not only surpasses CoM in terms of design principles but also provides valuable experience for future research on causal learning and continuous adaptive improvement.

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [222] [Reinforcement Learning via Implicit Imitation Guidance](https://arxiv.org/abs/2506.07505)
*Perry Dong,Alec M. Lessing,Annie S. Chen,Chelsea Finn*

Main category: cs.LG

TL;DR: 在样本高效强化学习中，使用先验数据引导探索而非直接模仿，可提升长期性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用先验数据（如演示）来初始化强化学习模型，以替代密集奖励信号，提高样本效率。

Method: 提出Data-Guided Noise (DGN)框架，通过向策略添加噪声来利用先验数据指导探索，而不是直接进行行为克隆约束。

Result: 在七个模拟连续控制任务中，相较于其他基于离线数据的强化学习方法，该方法性能提升了2-3倍。

Conclusion: 演示数据对于识别应探索的动作最有用，而不是强制策略执行特定动作，这种方法可以更好地实现奖励最大化。

Abstract: We study the problem of sample efficient reinforcement learning, where prior
data such as demonstrations are provided for initialization in lieu of a dense
reward signal. A natural approach is to incorporate an imitation learning
objective, either as regularization during training or to acquire a reference
policy. However, imitation learning objectives can ultimately degrade long-term
performance, as it does not directly align with reward maximization. In this
work, we propose to use prior data solely for guiding exploration via noise
added to the policy, sidestepping the need for explicit behavior cloning
constraints. The key insight in our framework, Data-Guided Noise (DGN), is that
demonstrations are most useful for identifying which actions should be
explored, rather than forcing the policy to take certain actions. Our approach
achieves up to 2-3x improvement over prior reinforcement learning from offline
data methods across seven simulated continuous control tasks.

</details>


### [223] [Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems](https://arxiv.org/abs/2506.07517)
*Shuqiang Zhang,Yuchao Zhang,Jinkun Chen,Haochen Sui*

Main category: cs.LG

TL;DR: The paper proposes a learning algorithm based on likelihood maximization to address selection bias in recommendation systems by considering latent exogenous variables.


<details>
  <summary>Details</summary>
Motivation: Recommendation systems face challenges in unbiased learning due to selection bias, which leads to distorted representation of user preferences. Existing debiasing methods assume the independence of exogenous variables, which is not always valid.

Method: The authors propose a unified method that models the data generation process with latent exogenous variables under mild normality assumptions and develop a Monte Carlo algorithm to numerically estimate the likelihood function.

Result: Extensive experiments on synthetic datasets and three real-world datasets demonstrate the effectiveness of the proposed method.

Conclusion: The proposed learning algorithm based on likelihood maximization effectively handles latent exogenous variables and improves the accuracy and fairness of recommendations.

Abstract: Recommendation systems (RS) aim to provide personalized content, but they
face a challenge in unbiased learning due to selection bias, where users only
interact with items they prefer. This bias leads to a distorted representation
of user preferences, which hinders the accuracy and fairness of
recommendations. To address the issue, various methods such as error imputation
based, inverse propensity scoring, and doubly robust techniques have been
developed. Despite the progress, from the structural causal model perspective,
previous debiasing methods in RS assume the independence of the exogenous
variables. In this paper, we release this assumption and propose a learning
algorithm based on likelihood maximization to learn a prediction model. We
first discuss the correlation and difference between unmeasured confounding and
our scenario, then we propose a unified method that effectively handles latent
exogenous variables. Specifically, our method models the data generation
process with latent exogenous variables under mild normality assumptions. We
then develop a Monte Carlo algorithm to numerically estimate the likelihood
function. Extensive experiments on synthetic datasets and three real-world
datasets demonstrate the effectiveness of our proposed method. The code is at
https://github.com/WallaceSUI/kdd25-background-variable.

</details>


### [224] [Flowing Datasets with Wasserstein over Wasserstein Gradient Flows](https://arxiv.org/abs/2506.07534)
*Clément Bonet,Christophe Vauthier,Anna Korba*

Main category: cs.LG

TL;DR: This paper proposes a novel method to design gradient flows on probability distributions, specifically for labeled datasets in machine learning applications such as domain adaptation and transfer learning. By representing each class as a conditional distribution of features and modeling the dataset as a mixture distribution supported on these classes, the authors introduce the Wasserstein over Wasserstein (WoW) distance and define WoW gradient flows. This framework is applied to transfer learning and dataset distillation tasks using Maximum Mean Discrepancies with Sliced-Wasserstein based kernels.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the need for new techniques to handle data represented as probability distributions in machine learning applications. Specifically, the challenge lies in designing tractable gradient flows over infinite-dimensional objects like labeled datasets.

Method: The method involves representing each class by its associated conditional distribution of features and modeling the dataset as a mixture distribution supported on these classes. The space is endowed with a metric structure using the Wasserstein over Wasserstein (WoW) distance from optimal transport theory. A differential structure is derived, and WoW gradient flows are defined to decrease a given objective functional.

Result: The proposed framework was successfully applied to transfer learning and dataset distillation tasks, leveraging Maximum Mean Discrepancies with Sliced-Wasserstein based kernels. This demonstrates the potential of the WoW gradient flows in practical machine learning scenarios.

Conclusion: In conclusion, the paper presents a novel approach to manage gradient flows on probability distributions, particularly for labeled datasets in machine learning. By introducing the WoW distance and defining WoW gradient flows, the authors provide a powerful tool for tasks such as transfer learning and dataset distillation.

Abstract: Many applications in machine learning involve data represented as probability
distributions. The emergence of such data requires radically novel techniques
to design tractable gradient flows on probability distributions over this type
of (infinite-dimensional) objects. For instance, being able to flow labeled
datasets is a core task for applications ranging from domain adaptation to
transfer learning or dataset distillation. In this setting, we propose to
represent each class by the associated conditional distribution of features,
and to model the dataset as a mixture distribution supported on these classes
(which are themselves probability distributions), meaning that labeled datasets
can be seen as probability distributions over probability distributions. We
endow this space with a metric structure from optimal transport, namely the
Wasserstein over Wasserstein (WoW) distance, derive a differential structure on
this space, and define WoW gradient flows. The latter enables to design
dynamics over this space that decrease a given objective functional. We apply
our framework to transfer learning and dataset distillation tasks, leveraging
our gradient flow construction as well as novel tractable functionals that take
the form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels
between probability distributions.

</details>


### [225] [Improving Memory Efficiency for Training KANs via Meta Learning](https://arxiv.org/abs/2506.07549)
*Zhangchi Zhao,Jun Shu,Deyu Meng,Zongben Xu*

Main category: cs.LG

TL;DR: Inspired by the Kolmogorov-Arnold representation theorem, KANs provide a new framework for function approximation. However, they have a large number of parameters. To solve this problem, MetaKANs are proposed to generate weights through a smaller meta-learner, which can significantly reduce the number of parameters and maintain interpretability.


<details>
  <summary>Details</summary>
Motivation: KANs have great potential as an efficient and interpretable alternative to traditional MLPs, but their large number of parameters leads to challenges in memory efficiency and higher training costs.

Method: Propose MetaKANs to generate weights for KANs via a smaller meta-learner, and train KANs and MetaKANs in an end-to-end differentiable manner.

Result: MetaKANs achieve comparable or even superior performance while significantly reducing the number of trainable parameters and maintaining promising interpretability.

Conclusion: MetaKANs improve parameter efficiency and memory usage, provide an alternative technique for training KANs, enhance scalability and extensibility, and narrow the training cost gap with MLPs.

Abstract: Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel
framework for function approximation by replacing traditional neural network
weights with learnable univariate functions. This design demonstrates
significant potential as an efficient and interpretable alternative to
traditional MLPs. However, KANs are characterized by a substantially larger
number of trainable parameters, leading to challenges in memory efficiency and
higher training costs compared to MLPs. To address this limitation, we propose
to generate weights for KANs via a smaller meta-learner, called MetaKANs. By
training KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs
achieve comparable or even superior performance while significantly reducing
the number of trainable parameters and maintaining promising interpretability.
Extensive experiments on diverse benchmark tasks, including symbolic
regression, partial differential equation solving, and image classification,
demonstrate the effectiveness of MetaKANs in improving parameter efficiency and
memory usage. The proposed method provides an alternative technique for
training KANs, that allows for greater scalability and extensibility, and
narrows the training cost gap with MLPs stated in the original paper of KANs.
Our code is available at https://github.com/Murphyzc/MetaKAN.

</details>


### [226] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: This paper proposes an LLM-based agent that integrates 137 external chemical tools and a dataset curation pipeline to generate ChemToolBench. They introduce a HE-MCTS framework for tool planning and execution optimization, supporting step-level fine-tuning of the policy model and training task-adaptive PRM and ORM surpassing GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges faced by LLMs in chemistry tasks due to outdated pretraining knowledge and difficulty incorporating specialized chemical expertise.

Method: Propose an LLM-based agent integrating 137 external chemical tools and a dataset curation pipeline generating ChemToolBench. Introduce HE-MCTS framework for independent optimization of tool planning and execution.

Result: Significantly improves performance in Chemistry QA and discovery tasks, surpassing GPT-4o.

Conclusion: The proposed approach offers a robust solution to integrate specialized tools with LLMs for advanced chemical applications.

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [227] [Denoising the Future: Top-p Distributions for Moving Through Time](https://arxiv.org/abs/2506.07578)
*Florian Andreas Marwitz,Ralf Möller,Magnus Bender,Marcel Gehrke*

Main category: cs.LG

TL;DR: 通过仅使用最可能的状态（top-p states），可以减少推理中的误差并加速推理过程，同时在总变异距离方面的误差低于0.09。


<details>
  <summary>Details</summary>
Motivation: 推理在动态概率模型中是一项复杂的任务，涉及昂贵的操作。对于隐藏马尔可夫模型，整个状态空间必须被枚举以便在时间上推进，这导致了计算效率低下和由于传播不太可能的概率质量而增加的噪声。

Method: 提出了一种去噪未来和加速推理的方法，即仅使用累积概率p的最可能状态（top-p states）。

Result: 实验评估表明，可以期望至少提高一个数量级的速度，而在总变异距离方面的误差低于0.09。

Conclusion: 仅使用top-p状态引入的误差由p和底层模型的最小混合率决定，这种方法可以显著提高推理速度并降低误差。

Abstract: Inference in dynamic probabilistic models is a complex task involving
expensive operations. In particular, for Hidden Markov Models, the whole state
space has to be enumerated for advancing in time. Even states with negligible
probabilities are considered, resulting in computational inefficiency and
increased noise due to the propagation of unlikely probability mass. We propose
to denoise the future and speed up inference by using only the top-p states,
i.e., the most probable states with accumulated probability p. We show that the
error introduced by using only the top-p states is bound by p and the so-called
minimal mixing rate of the underlying model. Moreover, in our empirical
evaluation, we show that we can expect speedups of at least an order of
magnitude, while the error in terms of total variation distance is below 0.09.

</details>


### [228] [FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning](https://arxiv.org/abs/2506.07581)
*Tan Chen,Jintao Yan,Yuxuan Sun,Sheng Zhou,Zhisheng Niu*

Main category: cs.LG

TL;DR: This paper addresses the impact of collective gradient divergence (CGD) on federated learning (FL) convergence speed in wireless networks, proposing the FedCGD algorithm to optimize device scheduling and improve performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with data heterogeneity and limited bandwidth when applied in wireless networks. While many studies focus on device scheduling strategies, they often overlook the collective impact of devices on gradient divergence.

Method: The authors prove that FL's convergence speed is influenced by the sum of device-level and sample-level CGD. They derive a tractable form of device-level CGD using weighted earth moving distance (WEMD). Based on this, they propose the FedCGD algorithm which optimizes device scheduling by balancing WEMD and sampling variance.

Result: Simulation results show that the FedCGD algorithm increases classification accuracy on the CIFAR-10 dataset by up to 4.2% while reducing the number of scheduled devices by 41.8%. The algorithm can flexibly adjust between minimizing WEMD and sampling variance.

Conclusion: FedCGD effectively minimizes multi-level CGDs, leading to improved FL performance in wireless networks through efficient device scheduling.

Abstract: Federated learning (FL) is a promising paradigm for multiple devices to
cooperatively train a model. When applied in wireless networks, two issues
consistently affect the performance of FL, i.e., data heterogeneity of devices
and limited bandwidth. Many papers have investigated device scheduling
strategies considering the two issues. However, most of them recognize data
heterogeneity as a property of individual devices. In this paper, we prove that
the convergence speed of FL is affected by the sum of device-level and
sample-level collective gradient divergence (CGD). The device-level CGD refers
to the gradient divergence of the scheduled device group, instead of the sum of
the individual device divergence. The sample-level CGD is statistically upper
bounded by sampling variance, which is inversely proportional to the total
number of samples scheduled for local update. To derive a tractable form of the
device-level CGD, we further consider a classification problem and transform it
into the weighted earth moving distance (WEMD) between the group distribution
and the global distribution. Then we propose FedCGD algorithm to minimize the
sum of multi-level CGDs by balancing WEMD and sampling variance, within
polynomial time. Simulation shows that the proposed strategy increases
classification accuracy on the CIFAR-10 dataset by up to 4.2\% while scheduling
41.8\% fewer devices, and flexibly switches between reducing WEMD and reducing
sampling variance.

</details>


### [229] [MIRA: Medical Time Series Foundation Model for Real-World Health Data](https://arxiv.org/abs/2506.07584)
*Hao Li,Bowen Deng,Chang Xu,Zhiyuan Feng,Viktor Schlegel,Yu-Hao Huang,Yizheng Sun,Jingyuan Sun,Kailai Yang,Yiyao Yu,Jiang Bian*

Main category: cs.LG

TL;DR: MIRA is a unified foundation model for medical time series forecasting that incorporates Continuous-Time Rotary Positional Encoding, frequency-specific mixture-of-experts layer, and Continuous Dynamics Extrapolation Block. It reduces forecasting errors by 10% in out-of-distribution scenarios and 7% in in-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing generalist time series foundation models struggle to handle medical time series data due to irregular intervals, heterogeneous sampling rates, and frequent missing values.

Method: MIRA uses Continuous-Time Rotary Positional Encoding for fine-grained modeling of variable time intervals, a frequency-specific mixture-of-experts layer for routing computation across latent frequency regimes, and a Continuous Dynamics Extrapolation Block based on Neural ODE for modeling the continuous trajectory of latent states.

Result: MIRA achieves reductions in forecasting errors by an average of 10% and 7% in out-of-distribution and in-distribution scenarios respectively.

Conclusion: MIRA offers the potential to reduce annotation burdens, minimize model customization, and enable robust transfer across clinical institutions, modalities, and tasks.

Abstract: A unified foundation model for medical time series -- pretrained on open
access and ethics board-approved medical corpora -- offers the potential to
reduce annotation burdens, minimize model customization, and enable robust
transfer across clinical institutions, modalities, and tasks, particularly in
data-scarce or privacy-constrained environments. However, existing generalist
time series foundation models struggle to handle medical time series data due
to their inherent challenges, including irregular intervals, heterogeneous
sampling rates, and frequent missing values. To address these challenges, we
introduce MIRA, a unified foundation model specifically designed for medical
time series forecasting. MIRA incorporates a Continuous-Time Rotary Positional
Encoding that enables fine-grained modeling of variable time intervals, a
frequency-specific mixture-of-experts layer that routes computation across
latent frequency regimes to further promote temporal specialization, and a
Continuous Dynamics Extrapolation Block based on Neural ODE that models the
continuous trajectory of latent states, enabling accurate forecasting at
arbitrary target timestamps. Pretrained on a large-scale and diverse medical
corpus comprising over 454 billion time points collect from publicly available
datasets, MIRA achieves reductions in forecasting errors by an average of 10%
and 7% in out-of-distribution and in-distribution scenarios, respectively, when
compared to other zero-shot and fine-tuned baselines. We also introduce a
comprehensive benchmark spanning multiple downstream clinical tasks,
establishing a foundation for future research in medical time series modeling.

</details>


### [230] [Aircraft Trajectory Dataset Augmentation in Latent Space](https://arxiv.org/abs/2506.07585)
*Seokbin Yoon,Keumjin Lee*

Main category: cs.LG

TL;DR: A novel framework named ATRADA is proposed for aircraft trajectory dataset augmentation, which leverages Transformer encoder, PCA, GMM and MLP to generate high-quality synthetic data.


<details>
  <summary>Details</summary>
Motivation: Aircraft trajectory modeling is essential in Air Traffic Management and downstream tasks like conflict detection and landing time prediction. To develop a more robust model, it's necessary to augment the dataset with synthetically generated trajectory data.

Method: The ATRADA framework uses a Transformer encoder to learn patterns from the original trajectory dataset and transform each data point into a context vector. Then PCA reduces the dimensionality of these vectors, and GMM fits the probability distribution. New samples are drawn from GMM, reverted to original dimensions, and decoded by MLP.

Result: Experiments show that the framework can effectively generate new, high-quality synthetic aircraft trajectory data, outperforming several baselines.

Conclusion: ATRADA provides an effective solution for aircraft trajectory dataset augmentation, enhancing the development of robust models.

Abstract: Aircraft trajectory modeling plays a crucial role in Air Traffic Management
(ATM) and is important for various downstream tasks, including conflict
detection and landing time prediction. Dataset augmentation through the
addition of synthetically generated trajectory data is necessary to develop a
more robust aircraft trajectory model and ensure that the trajectory dataset is
sufficient and balanced. In this work, we propose a novel framework called
ATRADA for aircraft trajectory dataset augmentation. In the proposed framework,
a Transformer encoder learns the underlying patterns in the original trajectory
dataset and converts each data point into a context vector in the learned
latent space. The converted dataset in the latent space is projected into
reduced dimensions using principal component analysis (PCA), and a Gaussian
mixture model (GMM) is applied to fit the probability distribution of the data
points in the reduced-dimensional space. Finally, new samples are drawn from
the fitted GMM, the dimension of the samples is reverted to the original
dimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several
experiments demonstrate that the framework effectively generates new,
high-quality synthetic aircraft trajectory data, which were compared to the
results of several baselines.

</details>


### [231] [PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs](https://arxiv.org/abs/2506.07587)
*Tongzhou Yu,Zhuhao Zhang,Guanghui Zhu,Shen Jiang,Meikang Qiu,Yihua Huang*

Main category: cs.LG

TL;DR: Parameter Efficient Fine-Tuning (PEFT) methods are promising for fine-tuning language models with fewer parameters. However, choosing the right PEFT configuration is challenging without incurring significant overhead. This paper proposes PrunePEFT, a novel approach that formulates PEFT strategy search as a pruning problem and introduces a hybrid pruning strategy to optimize the fine-tuned configuration.


<details>
  <summary>Details</summary>
Motivation: PEFT methods offer advantages over Full parameter Fine-Tuning (FFT) by achieving similar performance with fewer trainable parameters. Yet, selecting appropriate PEFT configurations remains difficult and resource-intensive.

Method: The proposed method, PrunePEFT, treats PEFT strategy search as a pruning problem and applies a hybrid pruning strategy. It iteratively removes redundant or conflicting PEFT modules based on their sensitivity to different pruning techniques, thereby optimizing the fine-tuned model configuration.

Result: PrunePEFT efficiently identifies the most relevant PEFT modules, significantly reducing computational costs associated with architectural search processes while maintaining scalability and efficiency.

Conclusion: PrunePEFT provides an effective and efficient solution for optimizing PEFT configurations, making it easier to fine-tune large pre-trained models with reduced computational burden.

Abstract: Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and
promising approaches for fine-tuning pre-trained language models. Compared with
Full parameter Fine-Tuning (FFT), PEFT achieved comparable task performance
with a substantial reduction of trainable parameters, which largely saved the
training and storage costs. However, using the PEFT method requires considering
a vast design space, such as the type of PEFT modules and their insertion
layers. Inadequate configurations can lead to sub-optimal results. Conventional
solutions such as architectural search techniques, while effective, tend to
introduce substantial additional overhead. In this paper, we propose a novel
approach, PrunePEFT, which formulates the PEFT strategy search as a pruning
problem and introduces a hybrid pruning strategy that capitalizes on the
sensitivity of pruning methods to different PEFT modules. This method extends
traditional pruning techniques by iteratively removing redundant or conflicting
PEFT modules, thereby optimizing the fine-tuned configuration. By efficiently
identifying the most relevant modules, our approach significantly reduces the
computational burden typically associated with architectural search processes,
making it a more scalable and efficient solution for fine-tuning large
pre-trained models.

</details>


### [232] [Exploiting Curvature in Online Convex Optimization with Delayed Feedback](https://arxiv.org/abs/2506.07595)
*Hao Qiu,Emmanuel Esposito,Mengxiao Zhang*

Main category: cs.LG

TL;DR: This paper explores online convex optimization with curved losses and delayed feedback, proposing new algorithms that achieve improved regret bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to bridge the gap between existing approaches for online convex optimization with strongly convex losses and delayed feedback, which obtain regret bounds of order $d_{\max} \ln T$, and the bounds obtained by a delayed version of online gradient descent, which are of order $\sqrt{d_{\mathrm{tot}}}$.

Method: The authors propose a variant of follow-the-regularized-leader that obtains regret of order $\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is the maximum number of missing observations. They also extend the Online Newton Step algorithm to handle delays with an adaptive learning rate tuning, achieving regret $\min\{d_{\max} n\ln T, \sqrt{d_{\mathrm{tot}}}\}$. For unconstrained online linear regression, they design a variant of the Vovk-Azoury-Warmuth forecaster with a clipping trick.

Result: The proposed algorithms achieve regret bounds that are better than existing methods in various types of delay and losses. The experiments show an improved performance over existing methods.

Conclusion: This paper presents new algorithms for online convex optimization with curved losses and delayed feedback that achieve improved regret bounds. These algorithms outperform existing methods in various scenarios.

Abstract: In this work, we study the online convex optimization problem with curved
losses and delayed feedback. When losses are strongly convex, existing
approaches obtain regret bounds of order $d_{\max} \ln T$, where $d_{\max}$ is
the maximum delay and $T$ is the time horizon. However, in many cases, this
guarantee can be much worse than $\sqrt{d_{\mathrm{tot}}}$ as obtained by a
delayed version of online gradient descent, where $d_{\mathrm{tot}}$ is the
total delay. We bridge this gap by proposing a variant of
follow-the-regularized-leader that obtains regret of order
$\min\{\sigma_{\max}\ln T, \sqrt{d_{\mathrm{tot}}}\}$, where $\sigma_{\max}$ is
the maximum number of missing observations. We then consider exp-concave losses
and extend the Online Newton Step algorithm to handle delays with an adaptive
learning rate tuning, achieving regret $\min\{d_{\max} n\ln T,
\sqrt{d_{\mathrm{tot}}}\}$ where $n$ is the dimension. To our knowledge, this
is the first algorithm to achieve such a regret bound for exp-concave losses.
We further consider the problem of unconstrained online linear regression and
achieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth
forecaster with a clipping trick. Finally, we implement our algorithms and
conduct experiments under various types of delay and losses, showing an
improved performance over existing methods.

</details>


### [233] [TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts](https://arxiv.org/abs/2506.07596)
*Torsten Krauß,Hamid Dashtbani,Alexandra Dmitrienko*

Main category: cs.LG

TL;DR: The paper introduces TwinBreak, a method to remove safety alignments in LLMs by pruning parameters responsible for the safety mechanism. It uses twin prompts with high similarity to isolate safety parameters and achieves high success rates across multiple LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the issue of LLM jailbreaks which currently require significant manual effort or computational costs, and may degrade model utility.

Method: TwinBreak identifies and prunes parameters responsible for the safety mechanism in LLMs by analyzing intermediate outputs from prompts with high structural and content similarity.

Result: Experiments show TwinBreak's effectiveness with 89% to 98% success rates across 16 LLMs from five vendors, requiring minimal computational resources.

Conclusion: TwinBreak is an effective method for removing safety alignments in LLMs with minimal computational requirements.

Abstract: Machine learning is advancing rapidly, with applications bringing notable
benefits, such as improvements in translation and code generation. Models like
ChatGPT, powered by Large Language Models (LLMs), are increasingly integrated
into daily life. However, alongside these benefits, LLMs also introduce social
risks. Malicious users can exploit LLMs by submitting harmful prompts, such as
requesting instructions for illegal activities. To mitigate this, models often
include a security mechanism that automatically rejects such harmful prompts.
However, they can be bypassed through LLM jailbreaks. Current jailbreaks often
require significant manual effort, high computational costs, or result in
excessive model modifications that may degrade regular utility.
  We introduce TwinBreak, an innovative safety alignment removal method.
Building on the idea that the safety mechanism operates like an embedded
backdoor, TwinBreak identifies and prunes parameters responsible for this
functionality. By focusing on the most relevant model layers, TwinBreak
performs fine-grained analysis of parameters essential to model utility and
safety. TwinBreak is the first method to analyze intermediate outputs from
prompts with high structural and content similarity to isolate safety
parameters. We present the TwinPrompt dataset containing 100 such twin prompts.
Experiments confirm TwinBreak's effectiveness, achieving 89% to 98% success
rates with minimal computational requirements across 16 LLMs from five vendors.

</details>


### [234] [FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning](https://arxiv.org/abs/2506.07616)
*Zhixin Geng,Xu Fan,Xiqiao Lu,Yan Zhang,Guangyuan Yu,Cheng Huang,Qian Wang,Yuewu Li,Weichun Ma,Qi Yu,Libo Wu,Hao Li*

Main category: cs.LG

TL;DR: This paper introduces FuXi-Air, an air quality forecasting model using multimodal data fusion for high-precision and efficient forecasts in megacities.


<details>
  <summary>Details</summary>
Motivation: Air pollution is a major public health challenge in megacities with existing forecasting methods having limitations such as high computational costs and limited integration with observational data.

Method: The model integrates meteorological forecasts, emission inventories, and pollutant monitoring data under the guidance of air pollution mechanism. It combines an autoregressive prediction framework with a frame interpolation strategy to complete 72-hour forecasts for six major air pollutants at an hourly resolution.

Result: FuXi-Air outperforms mainstream numerical air quality models in both computational efficiency and forecasting accuracy. Meteorological data contribute more to model accuracy than emission inventories, but integrating multimodal data significantly improves forecasting precision.

Conclusion: This study provides a technical reference and practical example for applying multimodal data-driven models to air quality forecasting, offering new insights into building hybrid forecasting systems for smart city management.

Abstract: Air pollution has emerged as a major public health challenge in megacities.
Numerical simulations and single-site machine learning approaches have been
widely applied in air quality forecasting tasks. However, these methods face
multiple limitations, including high computational costs, low operational
efficiency, and limited integration with observational data. With the rapid
advancement of artificial intelligence, there is an urgent need to develop a
low-cost, efficient air quality forecasting model for smart urban management.
An air quality forecasting model, named FuXi-Air, has been constructed in this
study based on multimodal data fusion to support high-precision air quality
forecasting and operated in typical megacities. The model integrates
meteorological forecasts, emission inventories, and pollutant monitoring data
under the guidance of air pollution mechanism. By combining an autoregressive
prediction framework with a frame interpolation strategy, the model
successfully completes 72-hour forecasts for six major air pollutants at an
hourly resolution across multiple monitoring sites within 25-30 seconds. In
terms of both computational efficiency and forecasting accuracy, it outperforms
the mainstream numerical air quality models in operational forecasting work.
Ablation experiments concerning key influencing factors show that although
meteorological data contribute more to model accuracy than emission inventories
do, the integration of multimodal data significantly improves forecasting
precision and ensures that reliable predictions are obtained under differing
pollution mechanisms across megacities. This study provides both a technical
reference and a practical example for applying multimodal data-driven models to
air quality forecasting and offers new insights into building hybrid
forecasting systems to support air pollution risk warning in smart city
management.

</details>


### [235] [The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning](https://arxiv.org/abs/2506.07619)
*Toby Boyne,Juan S. Campos,Becky D. Langdon,Jixiang Qing,Yilin Xie,Shiqiang Zhang,Calvin Tsay,Ruth Misener,Daniel W. Davies,Kim E. Jelfs,Sarah Boyall,Thomas M. Dixon,Linden Schrecker,Jose Pablo Folch*

Main category: cs.LG

TL;DR: The paper introduces a new dataset for yield prediction in chemistry, focusing on solvent selection, and showcases various machine learning techniques for this task.


<details>
  <summary>Details</summary>
Motivation: Chemical datasets are often inaccessible or require significant preprocessing, limiting the application of machine learning in laboratory chemistry.

Method: Introduced a novel transient flow dataset for yield prediction covering over 1200 process conditions. The study focuses on solvent selection and applies regression algorithms, transfer-learning, feature engineering, and active learning to this task.

Result: The benchmarking results indicate that machine learning models can effectively predict yield based on continuous process conditions, with important implications for solvent replacement and sustainable manufacturing.

Conclusion: This new dataset and the associated machine learning approaches provide valuable tools for advancing laboratory chemistry, particularly in optimizing solvent selection for more sustainable chemical processes.

Abstract: Machine learning has promised to change the landscape of laboratory
chemistry, with impressive results in molecular property prediction and
reaction retro-synthesis. However, chemical datasets are often inaccessible to
the machine learning community as they tend to require cleaning, thorough
understanding of the chemistry, or are simply not available. In this paper, we
introduce a novel dataset for yield prediction, providing the first-ever
transient flow dataset for machine learning benchmarking, covering over 1200
process conditions. While previous datasets focus on discrete parameters, our
experimental set-up allow us to sample a large number of continuous process
conditions, generating new challenges for machine learning models. We focus on
solvent selection, a task that is particularly difficult to model theoretically
and therefore ripe for machine learning applications. We showcase benchmarking
for regression algorithms, transfer-learning approaches, feature engineering,
and active learning, with important applications towards solvent replacement
and sustainable manufacturing.

</details>


### [236] [Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks](https://arxiv.org/abs/2506.07624)
*Ali Hariri,Álvaro Arroyo,Alessio Gravina,Moshe Eliasof,Carola-Bibiane Schönlieb,Davide Bacciu,Kamyar Azizzadenesheli,Xiaowen Dong,Pierre Vandergheynst*

Main category: cs.LG

TL;DR: ChebNet，早期的谱图神经网络，在捕捉长距离节点依赖方面具有潜力，但存在训练不稳定的问题。本文提出Stable-ChebNet模型，通过将其视为稳定的非耗散动力系统来解决这一问题，实现了在多个基准测试中接近最先进水平的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管MPNNs在捕捉局部图结构方面表现出色，但在捕捉长距离节点依赖方面存在局限性。已有的改进方法如重连线或使用Graph Transformers，往往牺牲计算效率或忽略图结构。因此，研究者重新审视ChebNet，探索其在建模远距离节点交互方面的潜力。

Method: 研究者首先分析了ChebNet在长距离基准测试中的表现，发现其具有竞争力但存在训练不稳定的问题。为了解决这一问题，将ChebNet建模为一个稳定且非耗散的动力系统，称为Stable-ChebNet。该模型允许稳定的信息传播，并具有可控的动力学特性，无需特征分解、位置编码或图重连线。

Result: Stable-ChebNet在多个基准测试中表现出接近最先进水平的性能，同时保持良好的可扩展性和稳定性。

Conclusion: Stable-ChebNet作为一种改进的ChebNet模型，成功解决了原模型训练不稳定的问题，并在长距离节点交互建模方面展现出优异性能，为图神经网络的研究提供了新的视角和方法。

Abstract: ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by
Message Passing Neural Networks (MPNNs), which gained popularity for their
simplicity and effectiveness in capturing local graph structure. Despite their
success, MPNNs are limited in their ability to capture long-range dependencies
between nodes. This has led researchers to adapt MPNNs through rewiring or make
use of Graph Transformers, which compromises the computational efficiency that
characterized early spatial message-passing architectures, and typically
disregards the graph structure. Almost a decade after its original
introduction, we revisit ChebNet to shed light on its ability to model distant
node interactions. We find that out-of-box, ChebNet already shows competitive
advantages relative to classical MPNNs and GTs on long-range benchmarks, while
maintaining good scalability properties for high-order polynomials. However, we
uncover that this polynomial expansion leads ChebNet to an unstable regime
during training. To address this limitation, we cast ChebNet as a stable and
non-dissipative dynamical system, which we coin Stable-ChebNet. Our
Stable-ChebNet model allows for stable information propagation, and has
controllable dynamics which do not require the use of eigendecompositions,
positional encodings, or graph rewiring. Across several benchmarks,
Stable-ChebNet achieves near state-of-the-art performance.

</details>


### [237] [The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well](https://arxiv.org/abs/2506.07661)
*Meir Feder,Ruediger Urbanke,Yaniv Fogel*

Main category: cs.LG

TL;DR: The paper explores why over-parameterized models generalize well using information theory and universal learning theory, showing that model simplicity is tied to the cumulative probability of hypotheses close to the true data-generating process.


<details>
  <summary>Details</summary>
Motivation: To understand why large, over-parameterized models like deep neural networks generalize well despite having more parameters than training samples.

Method: Investigate through Bayesian mixture learner with log-loss and uniform prior; analyze regret bounds based on Kullback-Leibler divergence and hypothesis weight; connect theory to practice via stochastic gradient descent with Langevin dynamics.

Result: Model simplicity defined by large hypothesis weight leads to better generalization; complex models require more data; over-parameterized models avoid overfitting due to presence of simple hypotheses.

Conclusion: Provides a rigorous explanation for generalization of over-parameterized models and connects theoretical findings to practical concepts like flat minima and model distillation.

Abstract: A fundamental question in modern machine learning is why large,
over-parameterized models, such as deep neural networks and transformers, tend
to generalize well, even when their number of parameters far exceeds the number
of training samples.
  We investigate this phenomenon through the lens of information theory,
grounded in universal learning theory. Specifically, we study a Bayesian
mixture learner with log-loss and (almost) uniform prior over an expansive
hypothesis class.
  Our key result shows that the learner's regret is not determined by the
overall size of the hypothesis class, but rather by the cumulative probability
of all models that are close, in Kullback-Leibler divergence distance, to the
true data-generating process. We refer to this cumulative probability as the
weight of the hypothesis.
  This leads to a natural notion of model simplicity: simple models are those
with large weight and thus require fewer samples to generalize, while complex
models have small weight and need more data. This perspective provides a
rigorous and intuitive explanation for why over-parameterized models often
avoid overfitting: the presence of simple hypotheses allows the posterior to
concentrate on them when supported by the data.
  We further bridge theory and practice by recalling that stochastic gradient
descent with Langevin dynamics samples from the correct posterior distribution,
enabling our theoretical learner to be approximated using standard machine
learning methods combined with ensemble learning.
  Our analysis yields non-uniform regret bounds and aligns with key practical
concepts such as flat minima and model distillation. The results apply broadly
across online, batch, and supervised learning settings, offering a unified and
principled understanding of the generalization behavior of modern AI systems.

</details>


### [238] [ProARD: progressive adversarial robustness distillation: provide wide range of robust students](https://arxiv.org/abs/2506.07666)
*Seyedhamidreza Mousavi,Seyedali Mousavi,Masoud Daneshtalab*

Main category: cs.LG

TL;DR: The paper proposes Progressive Adversarial Robustness Distillation (ProARD) to efficiently train a dynamic network that supports multiple accurate and robust student networks without retraining.


<details>
  <summary>Details</summary>
Motivation: Current ARD approaches require training a new student network from scratch to meet specific constraints, leading to substantial computational costs and increased CO2 emissions.

Method: Propose ProARD which makes a dynamic deep neural network based on dynamic layers by encompassing variations in width, depth, and expansion. The largest student network is considered as the dynamic teacher network. ProARD trains this dynamic network using a weight-sharing mechanism and uses a sampling mechanism to select a subset of students since calculating exact gradients for all students within the dynamic network is computationally expensive.

Result: Random student sampling in each iteration fails to produce accurate and robust students.

Conclusion: Not mentioned in the abstract.

Abstract: Adversarial Robustness Distillation (ARD) has emerged as an effective method
to enhance the robustness of lightweight deep neural networks against
adversarial attacks. Current ARD approaches have leveraged a large robust
teacher network to train one robust lightweight student. However, due to the
diverse range of edge devices and resource constraints, current approaches
require training a new student network from scratch to meet specific
constraints, leading to substantial computational costs and increased CO2
emissions. This paper proposes Progressive Adversarial Robustness Distillation
(ProARD), enabling the efficient one-time training of a dynamic network that
supports a diverse range of accurate and robust student networks without
requiring retraining. We first make a dynamic deep neural network based on
dynamic layers by encompassing variations in width, depth, and expansion in
each design stage to support a wide range of architectures. Then, we consider
the student network with the largest size as the dynamic teacher network.
ProARD trains this dynamic network using a weight-sharing mechanism to jointly
optimize the dynamic teacher network and its internal student networks.
However, due to the high computational cost of calculating exact gradients for
all the students within the dynamic network, a sampling mechanism is required
to select a subset of students. We show that random student sampling in each
iteration fails to produce accurate and robust students.

</details>


### [239] [How Benchmark Prediction from Fewer Data Misses the Mark](https://arxiv.org/abs/2506.07673)
*Guanhua Zhang,Florian E. Dorner,Moritz Hardt*

Main category: cs.LG

TL;DR: The paper systematically evaluates 11 benchmark prediction methods across 19 benchmarks for efficient LLM evaluation, finding that most methods depend on model similarity and struggle with extrapolation. A new method inspired by augmented inverse propensity weighting outperforms random sampling in extrapolation but still relies on model similarity.


<details>
  <summary>Details</summary>
Motivation: To address the increasing cost of evaluating large language models (LLMs), the study explores methods that speed up evaluation by shrinking benchmark datasets, focusing on selecting a small subset of evaluation points to predict overall benchmark performance.

Method: The authors assess 11 benchmark prediction methods across 19 diverse benchmarks. They compare these methods against a competitive baseline which involves taking a random sample and fitting a regression model. Additionally, they introduce a new method based on augmented inverse propensity weighting designed to improve performance in extrapolation.

Result: Most existing methods depend heavily on model similarity and perform poorly when extrapolating for new models with higher accuracy. The new introduced method outperforms random sampling averages in extrapolation scenarios but gains are modest and still rely on model similarity.

Conclusion: Benchmark prediction struggles most where it is needed most - at the evaluation frontier for new models with unknown capabilities. Despite improvements, the effectiveness of these methods remains limited.

Abstract: Large language model (LLM) evaluation is increasingly costly, prompting
interest in methods that speed up evaluation by shrinking benchmark datasets.
Benchmark prediction (also called efficient LLM evaluation) aims to select a
small subset of evaluation points and predict overall benchmark performance
from that subset. In this paper, we systematically assess the strengths and
limitations of 11 benchmark prediction methods across 19 diverse benchmarks.
First, we identify a highly competitive baseline: Take a random sample and fit
a regression model on the sample to predict missing entries. Outperforming most
existing methods, this baseline challenges the assumption that careful subset
selection is necessary for benchmark prediction. Second, we discover that all
existing methods crucially depend on model similarity. They work best when
interpolating scores among similar models. The effectiveness of benchmark
prediction sharply declines when new models have higher accuracy than
previously seen models. In this setting of extrapolation, none of the previous
methods consistently beat a simple average over random samples. To improve over
the sample average, we introduce a new method inspired by augmented inverse
propensity weighting. This method consistently outperforms the random sample
average even for extrapolation. However, its performance still relies on model
similarity and the gains are modest in general. This shows that benchmark
prediction fails just when it is most needed: at the evaluation frontier, where
the goal is to evaluate new models of unknown capabilities.

</details>


### [240] [Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation](https://arxiv.org/abs/2506.07706)
*Boris Martirosyan,Alexey Karmanov*

Main category: cs.LG

TL;DR: The paper addresses the lack of robustness in Latent Diffusion Models (LDMs) by proposing methods to measure and improve it, focusing on separating image generation from text encoding issues. It introduces new data augmentation techniques, fine-tunes Stable Diffusion models using Dreambooth, and proposes a tailored evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Current research does not fully explore the robustness limitations of LDMs, particularly when processing diverse textual prompts.

Method: 1. Measure LDM robustness without considering the text encoder to isolate image generation issues. 2. Introduce novel data augmentation techniques to reveal robustness shortcomings. 3. Fine-tune Stable Diffusion models using Dreambooth with these augmentations across multiple tasks. 4. Propose a novel evaluation pipeline for assessing robustness.

Result: Not explicitly stated in the abstract, but the methods aim to reveal and address robustness shortcomings in LDMs.

Conclusion: The proposed methods and evaluation pipeline provide a way to better understand and enhance the robustness of LDMs.

Abstract: Latent diffusion models (LDMs) achieve state-of-the-art performance across
various tasks, including image generation and video synthesis. However, they
generally lack robustness, a limitation that remains not fully explored in
current research. In this paper, we propose several methods to address this
gap. First, we hypothesize that the robustness of LDMs primarily should be
measured without their text encoder, because if we take and explore the whole
architecture, the problems of image generator and text encoders wll be fused.
Second, we introduce novel data augmentation techniques designed to reveal
robustness shortcomings in LDMs when processing diverse textual prompts. We
then fine-tune Stable Diffusion 3 and Stable Diffusion XL models using
Dreambooth, incorporating these proposed augmentation methods across multiple
tasks. Finally, we propose a novel evaluation pipeline specifically tailored to
assess the robustness of LDMs fine-tuned via Dreambooth.

</details>


### [241] [Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning](https://arxiv.org/abs/2506.07735)
*Haizhao Jing,Haokui Zhang,Zhenhao Shang,Rong Xiao,Peng Wang,Yanning Zhang*

Main category: cs.LG

TL;DR: This paper introduces LeDG-Former, a novel framework that integrates language-based semantic embedding and dynamic graph representation learning for neural architecture representation. It addresses current limitations by incorporating hardware attribute information and improving encoding effectiveness. LeDG-Former outperforms previous methods on benchmarks and demonstrates cross-hardware latency prediction capability.


<details>
  <summary>Details</summary>
Motivation: Current methods in neural architecture representation learning overlook hardware attribute information and rely on static adjacency matrices to represent topological structures, which limits their practical applicability and compromises encoding effectiveness.

Method: The authors propose LeDG-Former, which includes a language embedding framework inspired by large language models (LLMs) to project neural architectures and hardware platform specifications into a unified semantic space, enabling zero-shot prediction across different hardware platforms. They also propose a dynamic graph-based transformer for modeling neural architectures.

Result: LeDG-Former surpasses previous methods on the NNLQP benchmark, establishing a new SOTA while demonstrating the first successful cross-hardware latency prediction capability. It also achieves superior performance on the cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

Conclusion: LeDG-Former effectively addresses the limitations of existing methods by incorporating hardware attribute information and improving encoding effectiveness through dynamic graph representation learning.

Abstract: Neural Architecture Representation Learning aims to transform network models
into feature representations for predicting network attributes, playing a
crucial role in deploying and designing networks for real-world applications.
Recently, inspired by the success of transformers, transformer-based models
integrated with Graph Neural Networks (GNNs) have achieved significant progress
in representation learning. However, current methods still have some
limitations. First, existing methods overlook hardware attribute information,
which conflicts with the current trend of diversified deep learning hardware
and limits the practical applicability of models. Second, current encoding
approaches rely on static adjacency matrices to represent topological
structures, failing to capture the structural differences between computational
nodes, which ultimately compromises encoding effectiveness. In this paper, we
introduce LeDG-Former, an innovative framework that addresses these limitations
through the synergistic integration of language-based semantic embedding and
dynamic graph representation learning. Specifically, inspired by large language
models (LLMs), we propose a language embedding framework where both neural
architectures and hardware platform specifications are projected into a unified
semantic space through tokenization and LLM processing, enabling zero-shot
prediction across different hardware platforms for the first time. Then, we
propose a dynamic graph-based transformer for modeling neural architectures,
resulting in improved neural architecture modeling performance. On the NNLQP
benchmark, LeDG-Former surpasses previous methods, establishing a new SOTA
while demonstrating the first successful cross-hardware latency prediction
capability. Furthermore, our framework achieves superior performance on the
cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.

</details>


### [242] [Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning](https://arxiv.org/abs/2506.07744)
*Seungho Baek,Taegeon Park,Jongchan Park,Seungjun Oh,Yusung Kim*

Main category: cs.LG

TL;DR: 提出了一种新的框架Graph-Assisted Stitching (GAS)，将子目标选择作为图搜索问题，而不是学习显式的高层策略。通过嵌入状态到时间距离表示（TDR）空间，GAS将语义相似的状态聚类成统一的图节点，从而实现高效的转换缝合。GAS在运动、导航和操作任务中优于以前的离线HRL方法。


<details>
  <summary>Details</summary>
Motivation: 现有的离线分层强化学习方法依赖于高级策略学习来生成子目标序列，但随着任务范围的增加，其效率下降，并且缺乏有效的策略来缝合不同轨迹中有用的状态转换。

Method: 1. 将子目标选择作为图搜索问题，而不是学习显式的高层策略。
2. 通过嵌入状态到时间距离表示（TDR）空间，将语义相似的状态聚类成统一的图节点。
3. 使用最短路径算法在图中选择子目标序列，同时低级策略学习到达子目标。
4. 引入时间效率（TE）度量，过滤掉嘈杂或低效的转换状态，显著提高任务性能。

Result: GAS在运动、导航和操作任务中优于以前的离线HRL方法。特别是在最需要缝合的任务中，得分为88.3，远超之前最先进的得分1.0。

Conclusion: GAS框架通过将子目标选择作为图搜索问题，显著提高了任务性能，并在多种任务中超越了现有方法。

Abstract: Existing offline hierarchical reinforcement learning methods rely on
high-level policy learning to generate subgoal sequences. However, their
efficiency degrades as task horizons increase, and they lack effective
strategies for stitching useful state transitions across different
trajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that
formulates subgoal selection as a graph search problem rather than learning an
explicit high-level policy. By embedding states into a Temporal Distance
Representation (TDR) space, GAS clusters semantically similar states from
different trajectories into unified graph nodes, enabling efficient transition
stitching. A shortest-path algorithm is then applied to select subgoal
sequences within the graph, while a low-level policy learns to reach the
subgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)
metric, which filters out noisy or inefficient transition states, significantly
enhancing task performance. GAS outperforms prior offline HRL methods across
locomotion, navigation, and manipulation tasks. Notably, in the most
stitching-critical task, it achieves a score of 88.3, dramatically surpassing
the previous state-of-the-art score of 1.0. Our source code is available at:
https://github.com/qortmdgh4141/GAS.

</details>


### [243] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 本论文提出了一种新颖的非梯度组合方法，用于估计LDA主题模型中的文档主题分配问题。该方法不仅在理论上有可证明的保证，而且在实践中也表现优异。相较于现有算法，新方法具有更快的收敛速度、更好的语义质量和更强的因果推断能力。


<details>
  <summary>Details</summary>
Motivation: 当前LDA主题模型中的文档主题分配问题缺乏既实用又有理论保障的算法，而这一问题对于社会科学、数据探索和因果推断等领域至关重要。

Method: 提出了一种基于非梯度组合的新颖估计方法，能够以对数并行计算时间收敛到接近最优的后验概率，并确保学习到的主题与已知关键词相关联，同时维持必要的独立性假设以支持下游因果推断。

Result: 该方法在多种文本数据集和评估参数下，语义质量始终优于现有的LDA算法、神经主题模型和LLM基主题模型。

Conclusion: 这种新方法为解决LDA主题模型中的核心推理问题提供了更高效、更可靠且更具解释性的解决方案，同时保持了进行因果推断所需的独立性假设。

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [244] [Comparing Credit Risk Estimates in the Gen-AI Era](https://arxiv.org/abs/2506.07754)
*Nicola Lavecchia,Sid Fadanelli,Federico Ricciuti,Gennaro Aloe,Enrico Bagli,Pietro Giuffrida,Daniele Vergari*

Main category: cs.LG

TL;DR: 生成式AI技术在信用评分建模中尚未超越传统方法，需进一步研究。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式AI在信用评分建模中的潜力及局限性。

Method: 对比分析传统信用评分建模方法与生成式AI技术。

Result: 当前生成式AI模型无论采用何种集成策略，其性能均未达到传统方法的水平。

Conclusion: 生成式AI在信用风险评分任务上存在局限性，需进一步研究开发才能实际应用。

Abstract: Generative AI technologies have demonstrated significant potential across
diverse applications. This study provides a comparative analysis of credit
score modeling techniques, contrasting traditional approaches with those
leveraging generative AI. Our findings reveal that current generative AI models
fall short of matching the performance of traditional methods, regardless of
the integration strategy employed. These results highlight the limitations in
the current capabilities of generative AI for credit risk scoring, emphasizing
the need for further research and development before the possibility of
applying generative AI for this specific task, or equivalent ones.

</details>


### [245] [Clustered Federated Learning via Embedding Distributions](https://arxiv.org/abs/2506.07769)
*Dekai Zhang,Matthew Williams,Francesca Toni*

Main category: cs.LG

TL;DR: In federated learning, non-IID data can cause problems. This paper proposes EMD-CFL, a new one-shot clustering method that uses Earth Mover's distance in embedding space to find more homogeneous clusters of clients, improving performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to non-IID data, which can degrade model performance. To address this issue, clustered federated learning aims to group clients into more homogeneous clusters.

Method: The proposed method, EMD-CFL, is a one-shot clustering approach that leverages the Earth Mover's distance (EMD) between data distributions in the embedding space to identify clusters of clients with similar data distributions.

Result: The method shows empirically superior clustering performance compared to 16 baselines across a range of challenging datasets, demonstrating its effectiveness.

Conclusion: EMD-CFL provides an effective solution for addressing the non-IID problem in federated learning by creating more homogeneous client clusters.

Abstract: Federated learning (FL) is a widely used framework for machine learning in
distributed data environments where clients hold data that cannot be easily
centralised, such as for data protection reasons. FL, however, is known to be
vulnerable to non-IID data. Clustered FL addresses this issue by finding more
homogeneous clusters of clients. We propose a novel one-shot clustering method,
EMD-CFL, using the Earth Mover's distance (EMD) between data distributions in
embedding space. We theoretically motivate the use of EMDs using results from
the domain adaptation literature and demonstrate empirically superior
clustering performance in extensive comparisons against 16 baselines and on a
range of challenging datasets.

</details>


### [246] [Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability](https://arxiv.org/abs/2506.07804)
*Jie Bao,Chuangyin Dang,Rui Luo,Hanwei Zhang,Zhixin Zhou*

Main category: cs.LG

TL;DR: This study combines adversarial training with Conformal Prediction principles to enhance model robustness and reliability in high-risk applications. It introduces OPSA, an attack method that increases model uncertainty, and OPSA-AT, a defense strategy that boosts resilience against various adversarial attacks while maintaining accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are being used more frequently in high-risk scenarios where robust defenses against adversarial attacks and reliable performance guarantees are essential. Accuracy alone is insufficient for providing assurance or reliable uncertainty estimates for these models.

Method: The study develops OPSA (OPtimal Size Attack), which maximizes model uncertainty without coverage guarantees, and OPSA-AT (Adversarial Training), a defense mechanism integrating OPSA within a conformal training framework.

Result: Experimental results indicate that the OPSA attack induces higher uncertainty than baseline methods across different defenses. Meanwhile, the OPSA-AT defensive model significantly improves robustness against not only OPSA but also other adversarial attacks, ensuring reliable predictions.

Conclusion: The integrated approach of combining adversarial training with Conformal Prediction principles effectively develops trustworthy and resilient deep learning models suitable for safety-critical domains.

Abstract: As deep learning models are increasingly deployed in high-risk applications,
robust defenses against adversarial attacks and reliable performance guarantees
become paramount. Moreover, accuracy alone does not provide sufficient
assurance or reliable uncertainty estimates for these models. This study
advances adversarial training by leveraging principles from Conformal
Prediction. Specifically, we develop an adversarial attack method, termed OPSA
(OPtimal Size Attack), designed to reduce the efficiency of conformal
prediction at any significance level by maximizing model uncertainty without
requiring coverage guarantees. Correspondingly, we introduce OPSA-AT
(Adversarial Training), a defense strategy that integrates OPSA within a novel
conformal training paradigm. Experimental evaluations demonstrate that our OPSA
attack method induces greater uncertainty compared to baseline approaches for
various defenses. Conversely, our OPSA-AT defensive model significantly
enhances robustness not only against OPSA but also other adversarial attacks,
and maintains reliable prediction. Our findings highlight the effectiveness of
this integrated approach for developing trustworthy and resilient deep learning
models for safety-critical domains. Our code is available at
https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.

</details>


### [247] [Identifiable Object Representations under Spatial Ambiguities](https://arxiv.org/abs/2506.07806)
*Avinash Kori,Francesca Toni,Ben Glocker*

Main category: cs.LG

TL;DR: The paper proposes a multi-view probabilistic approach that aggregates view-specific slots to capture invariant content information and disentangled global viewpoint-level information, resolving spatial ambiguities without needing viewpoint annotations.


<details>
  <summary>Details</summary>
Motivation: Modular object-centric representations are crucial for human-like reasoning but hard to obtain under spatial ambiguities such as occlusions and view ambiguities.

Method: A novel multi-view probabilistic approach is introduced. It aggregates view-specific slots for invariant content information while learning disentangled global viewpoint-level information simultaneously.

Result: Through extensive experiments on standard benchmarks and complex datasets, the method shows robustness and scalability.

Conclusion: The proposed approach resolves spatial ambiguities, provides identifiability guarantees, and does not require viewpoint annotations.

Abstract: Modular object-centric representations are essential for *human-like
reasoning* but are challenging to obtain under spatial ambiguities, *e.g. due
to occlusions and view ambiguities*. However, addressing challenges presents
both theoretical and practical difficulties. We introduce a novel multi-view
probabilistic approach that aggregates view-specific slots to capture
*invariant content* information while simultaneously learning disentangled
global *viewpoint-level* information. Unlike prior single-view methods, our
approach resolves spatial ambiguities, provides theoretical guarantees for
identifiability, and requires *no viewpoint annotations*. Extensive experiments
on standard benchmarks and novel complex datasets validate our method's
robustness and scalability.

</details>


### [248] [Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation](https://arxiv.org/abs/2506.07822)
*Xintong Duan,Yutong He,Fahim Tajwar,Ruslan Salakhutdinov,J. Zico Kolter,Jeff Schneider*

Main category: cs.LG

TL;DR: 本研究提出了一种新的离线强化学习一致性蒸馏方法，将奖励优化直接融入蒸馏过程，实现单步生成、性能更高且训练更简单。实验表明，该方法比现有最先进方法提高了8.7%，推理速度加快了142倍。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在决策任务中表现优异，但推理速度慢是其主要限制。现有的解决方案要么因次优演示影响效果，要么需要复杂的同时训练多个网络。

Method: 提出一种新的离线强化学习一致性蒸馏方法，将奖励优化直接融入蒸馏过程，从而简化训练并实现单步生成。

Result: 在Gym MuJoCo基准测试和长期规划任务中，该方法较之前的最先进方法提升了8.7%的性能，并在推理时间上加速了最多142倍。

Conclusion: 新方法通过直接结合奖励优化，显著提升了离线强化学习的性能与效率，同时减少了训练复杂度。

Abstract: Although diffusion models have achieved strong results in decision-making
tasks, their slow inference speed remains a key limitation. While the
consistency model offers a potential solution, its applications to
decision-making often struggle with suboptimal demonstrations or rely on
complex concurrent training of multiple networks. In this work, we propose a
novel approach to consistency distillation for offline reinforcement learning
that directly incorporates reward optimization into the distillation process.
Our method enables single-step generation while maintaining higher performance
and simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and
long horizon planning demonstrate that our approach can achieve an 8.7%
improvement over previous state-of-the-art while offering up to 142x speedup
over diffusion counterparts in inference time.

</details>


### [249] [Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information](https://arxiv.org/abs/2506.07829)
*Jan Corazza,Hadi Partovi Aria,Hyohun Kim,Daniel Neider,Zhe Xu*

Main category: cs.LG

TL;DR: 通过提供高层次的符号知识给智能体，扩展了用于检查局部策略与团队任务兼容性的形式化工具，并在去中心化多智能体强化学习（DMARL）中展示了这种知识能显著加速学习过程。


<details>
  <summary>Details</summary>
Motivation: 许多现实问题需要多个智能体协作完成共同目标，但在去中心化的多智能体强化学习（DMARL）中，智能体独立学习后需结合其策略执行任务，同时满足局部策略兼容性约束。因此，研究如何利用高层次符号知识来应对隐私、通信和性能等挑战显得尤为重要。

Method: 研究通过向智能体提供高层次的符号知识，扩展了用于验证局部策略与团队任务兼容性的形式化工具，使得去中心化训练在更多场景下具备理论保证。此外，在实验中引入关于环境中事件时间演化的符号知识，以加速DMARL中的学习过程。

Result: 理论上扩展了兼容性检查工具的适用范围；实证结果表明，符号知识能够显著加快DMARL的学习速度。

Conclusion: 高层次符号知识有助于解决DMARL中的隐私、通信及性能问题，并且可以加速学习过程，同时使去中心化训练具备更强的理论保障。

Abstract: Reinforcement learning (RL) algorithms can find an optimal policy for a
single agent to accomplish a particular task. However, many real-world problems
require multiple agents to collaborate in order to achieve a common goal. For
example, a robot executing a task in a warehouse may require the assistance of
a drone to retrieve items from high shelves. In Decentralized Multi-Agent RL
(DMARL), agents learn independently and then combine their policies at
execution time, but often must satisfy constraints on compatibility of local
policies to ensure that they can achieve the global task when combined. In this
paper, we study how providing high-level symbolic knowledge to agents can help
address unique challenges of this setting, such as privacy constraints,
communication limitations, and performance concerns. In particular, we extend
the formal tools used to check the compatibility of local policies with the
team task, making decentralized training with theoretical guarantees usable in
more scenarios. Furthermore, we empirically demonstrate that symbolic knowledge
about the temporal evolution of events in the environment can significantly
expedite the learning process in DMARL.

</details>


### [250] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: Large language models (LLMs) are limited by next-token prediction, hindering coherent concept formation. This paper introduces Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that enhances LLMs' conceptual understanding during the fine-tuning phase, showing significant improvements in various tasks.


<details>
  <summary>Details</summary>
Motivation: The existing paradigm of next-token prediction in LLMs limits their ability to form coherent, high-level concepts, which is a barrier to human-like understanding and reasoning.

Method: Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned by enabling the learning of sequences spanning multiple tokens, thus fostering stronger concept-aware learning.

Result: Experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including text summarization and de novo protein design.

Conclusion: CAFT brings multi-token prediction to the post-training phase, democratizing its benefits for practitioners and researchers, with wider implications for the machine learning research community.

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [251] [Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels](https://arxiv.org/abs/2506.07843)
*Davide Carbone*

Main category: cs.LG

TL;DR: This paper explores the use of Jarzynski reweighting in training Energy-Based Models (EBMs), focusing on kernel choice and its implications in flow-based diffusion models and Restricted Boltzmann Machines.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing theoretical challenges in EBM training, particularly biases introduced by traditional methods like contrastive divergence and score matching. This motivates an investigation into Jarzynski reweighting as a potential solution.

Method: Theoretical analysis of Jarzynski reweighting is conducted, examining its role in two generative frameworks: flow-based diffusion models and Restricted Boltzmann Machines. In flow-based diffusion models, it is used to mitigate discretization errors and improve sample quality. In Restricted Boltzmann Machines, it is analyzed for correcting biases of contrastive divergence.

Result: The results provide insights into how the choice of kernel affects model performance, demonstrating the potential of Jarzynski reweighting in improving generative learning.

Conclusion: Jarzynski reweighting shows promise as a principled tool for generative learning, offering a way to enhance EBM training through careful consideration of kernel choice.

Abstract: Energy-Based Models (EBMs) provide a flexible framework for generative
modeling, but their training remains theoretically challenging due to the need
to approximate normalization constants and efficiently sample from complex,
multi-modal distributions. Traditional methods, such as contrastive divergence
and score matching, introduce biases that can hinder accurate learning. In this
work, we present a theoretical analysis of Jarzynski reweighting, a technique
from non-equilibrium statistical mechanics, and its implications for training
EBMs. We focus on the role of the choice of the kernel and we illustrate these
theoretical considerations in two key generative frameworks: (i) flow-based
diffusion models, where we reinterpret Jarzynski reweighting in the context of
stochastic interpolants to mitigate discretization errors and improve sample
quality, and (ii) Restricted Boltzmann Machines, where we analyze its role in
correcting the biases of contrastive divergence. Our results provide insights
into the interplay between kernel choice and model performance, highlighting
the potential of Jarzynski reweighting as a principled tool for generative
learning.

</details>


### [252] [Residual Reweighted Conformal Prediction for Graph Neural Networks](https://arxiv.org/abs/2506.07854)
*Zheng Zhang,Jie Bao,Zhixin Zhou,Nicolo Colombo,Lixin Cheng,Rui Luo*

Main category: cs.LG

TL;DR: RR-GNN是一种创新框架，通过三个主要改进（Graph-Structured Mondrian CP、Residual-Adaptive Nonconformity Scores和Cross-Training Protocol）生成具有可证明边际覆盖率保证的最小预测集，适用于高风险领域的图神经网络任务。


<details>
  <summary>Details</summary>
Motivation: 当前的GNN在高风险领域面临不确定性问题，而现有的CP方法过于保守且未能充分考虑图的异质性和结构偏差，同时存在数据泄露的风险。因此需要一种新的方法来解决这些问题。

Method: RR-GNN提出了三种主要创新：1) 使用Graph-Structured Mondrian CP基于拓扑特征将节点或边划分为社区；2) 采用Residual-Adaptive Nonconformity Scores训练次级GNN估计任务特定残差；3) 使用Cross-Training Protocol交替优化主GNN和残差预测器。

Result: RR-GNN在15个真实世界图上进行验证，涵盖多种任务（节点分类、回归和边权重预测）。与CP基线相比，RR-GNN提高了效率，且没有损失覆盖率。

Conclusion: RR-GNN通过引入创新技术解决了现有方法的局限性，生成了具有可证明覆盖率保证的最小预测集，为高风险领域中的GNN应用提供了更好的解决方案。

Abstract: Graph Neural Networks (GNNs) excel at modeling relational data but face
significant challenges in high-stakes domains due to unquantified uncertainty.
Conformal prediction (CP) offers statistical coverage guarantees, but existing
methods often produce overly conservative prediction intervals that fail to
account for graph heteroscedasticity and structural biases. While residual
reweighting CP variants address some of these limitations, they neglect graph
topology, cluster-specific uncertainties, and risk data leakage by reusing
training sets. To address these issues, we propose Residual Reweighted GNN
(RR-GNN), a framework designed to generate minimal prediction sets with
provable marginal coverage guarantees.
  RR-GNN introduces three major innovations to enhance prediction performance.
First, it employs Graph-Structured Mondrian CP to partition nodes or edges into
communities based on topological features, ensuring cluster-conditional
coverage that reflects heterogeneity. Second, it uses Residual-Adaptive
Nonconformity Scores by training a secondary GNN on a held-out calibration set
to estimate task-specific residuals, dynamically adjusting prediction intervals
according to node or edge uncertainty. Third, it adopts a Cross-Training
Protocol, which alternates the optimization of the primary GNN and the residual
predictor to prevent information leakage while maintaining graph dependencies.
We validate RR-GNN on 15 real-world graphs across diverse tasks, including node
classification, regression, and edge weight prediction. Compared to CP
baselines, RR-GNN achieves improved efficiency over state-of-the-art methods,
with no loss of coverage.

</details>


### [253] [Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective](https://arxiv.org/abs/2506.07861)
*Firas Laakom,Haobo Chen,Jürgen Schmidhuber,Yuheng Bu*

Main category: cs.LG

TL;DR: This paper proposes a theoretical framework for analyzing fairness generalization error through an information-theoretic lens, using Efron-Stein inequality to derive tight bounds with both Mutual Information (MI) and Conditional Mutual Information (CMI). Empirical results validate the practical relevance of these bounds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for promoting fairness in machine learning models often modify the training process but lack formal guarantees that fairness achieved during training will generalize to unseen data. Overfitting in terms of fairness loss has received far less attention compared to prediction performance overfitting.

Method: The paper develops a novel bounding technique based on Efron-Stein inequality to analyze fairness generalization error. This approach derives tight information-theoretic fairness generalization bounds using both Mutual Information (MI) and Conditional Mutual Information (CMI).

Result: Empirical results demonstrate the tightness and practical relevance of the derived bounds across diverse fairness-aware learning algorithms.

Conclusion: The proposed framework provides valuable insights for guiding the design of algorithms aimed at improving fairness generalization.

Abstract: Despite substantial progress in promoting fairness in high-stake applications
using machine learning models, existing methods often modify the training
process, such as through regularizers or other interventions, but lack formal
guarantees that fairness achieved during training will generalize to unseen
data. Although overfitting with respect to prediction performance has been
extensively studied, overfitting in terms of fairness loss has received far
less attention. This paper proposes a theoretical framework for analyzing
fairness generalization error through an information-theoretic lens. Our novel
bounding technique is based on Efron-Stein inequality, which allows us to
derive tight information-theoretic fairness generalization bounds with both
Mutual Information (MI) and Conditional Mutual Information (CMI). Our empirical
results validate the tightness and practical relevance of these bounds across
diverse fairness-aware learning algorithms. Our framework offers valuable
insights to guide the design of algorithms improving fairness generalization.

</details>


### [254] [Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes](https://arxiv.org/abs/2506.07864)
*Mirko Paolo Barbato,Giorgia Rigamonti,Davide Marelli,Paolo Napoletano*

Main category: cs.LG

TL;DR: A novel Lightweight Sequential Transformer model for blood glucose prediction in Type 1 Diabetes is proposed, which integrates the strengths of Transformers and recurrent neural networks while being optimized for deployment on resource-constrained edge devices. Experiments show that it outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Type 1 Diabetes affects millions worldwide and requires continuous monitoring to prevent severe hypo- and hyperglycemic events. However, deploying predictive models on wearable devices is challenging due to computational and memory constraints.

Method: The authors propose a Lightweight Sequential Transformer model designed for blood glucose prediction in T1D. It combines the attention mechanisms of Transformers with the sequential processing of recurrent neural networks to capture long-term dependencies while maintaining computational efficiency. The model also incorporates a balanced loss function to handle data imbalance in hypo- and hyperglycemic events.

Result: Experiments on two benchmark datasets (OhioT1DM and DiaTrend) demonstrate that the proposed model outperforms state-of-the-art methods in predicting glucose levels and detecting adverse events.

Conclusion: This work bridges the gap between high-performance modeling and practical deployment by providing a reliable and efficient solution for Type 1 Diabetes management.

Abstract: Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous
monitoring to prevent severe hypo- and hyperglycemic events. While continuous
glucose monitoring has improved blood glucose management, deploying predictive
models on wearable devices remains challenging due to computational and memory
constraints. To address this, we propose a novel Lightweight Sequential
Transformer model designed for blood glucose prediction in T1D. By integrating
the strengths of Transformers' attention mechanisms and the sequential
processing of recurrent neural networks, our architecture captures long-term
dependencies while maintaining computational efficiency. The model is optimized
for deployment on resource-constrained edge devices and incorporates a balanced
loss function to handle the inherent data imbalance in hypo- and hyperglycemic
events. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,
demonstrate that the proposed model outperforms state-of-the-art methods in
predicting glucose levels and detecting adverse events. This work fills the gap
between high-performance modeling and practical deployment, providing a
reliable and efficient T1D management solution.

</details>


### [255] [Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?](https://arxiv.org/abs/2506.07871)
*Sigma Jahan,Mohammad Masudur Rahman*

Main category: cs.LG

TL;DR: 随着基于注意力的深度学习模型在规模和复杂性上的增长，诊断其故障变得越来越具有挑战性。本文通过实证研究评估了Hessian分析在诊断注意力模型故障中的潜力。研究表明，与仅使用梯度相比，Hessian-based指标能够更有效地定位不稳定性并确定故障源，从而可能显著改善复杂神经架构中的故障诊断，甚至改进软件调试实践。


<details>
  <summary>Details</summary>
Motivation: 随着基于注意力的深度学习模型规模和复杂性的增加，对其故障进行诊断变得愈发困难，因此需要探索新的方法来进行有效的故障诊断。

Method: 通过Hessian-based分析来诊断基于注意力的模型故障，具体包括利用Hessian导出的见解识别脆弱区域（通过曲率分析）和参数相互依赖性（通过参数交互分析）。

Result: 在三个不同的模型（HAN、3D-CNN、DistilBERT）上进行实验，结果表明Hessian-based指标可以比单独使用梯度更有效地定位不稳定性和确定故障源。

Conclusion: Hessian-based度量可以显著提高复杂神经架构中的故障诊断能力，并可能改善软件调试实践。

Abstract: As attention-based deep learning models scale in size and complexity,
diagnosing their faults becomes increasingly challenging. In this work, we
conduct an empirical study to evaluate the potential of Hessian-based analysis
for diagnosing faults in attention-based models. Specifically, we use
Hessian-derived insights to identify fragile regions (via curvature analysis)
and parameter interdependencies (via parameter interaction analysis) within
attention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,
DistilBERT), we show that Hessian-based metrics can localize instability and
pinpoint fault sources more effectively than gradients alone. Our empirical
findings suggest that these metrics could significantly improve fault diagnosis
in complex neural architectures, potentially improving software debugging
practices.

</details>


### [256] [Diffusion Counterfactual Generation with Semantic Abduction](https://arxiv.org/abs/2506.07883)
*Rajat Rasal,Avinash Kori,Fabio De Sousa Ribeiro,Tian Xia,Ben Glocker*

Main category: cs.LG

TL;DR: 本论文提出了一套基于扩散模型的因果机制，首次将高层次语义身份保留引入到扩散反事实中，并展示了语义控制如何在忠实因果控制和身份保留之间实现有原则的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的自编码框架虽然允许用于因果控制的语义潜在空间操作，但在可扩展性和保真度方面存在困难，因此需要一种新的方法来改进反事实图像编辑。

Method: 通过引入空间、语义和动态诱因的概念，提出了一个通用框架，该框架将语义表示整合到扩散模型中，利用Pearlian因果关系进行图像编辑。

Result: 此方法实现了高层次语义身份的保留，并展示了语义控制如何在忠实因果控制和身份保留之间实现有原则的权衡。

Conclusion: 这是首个考虑扩散反事实中高层次语义身份保留的工作，并成功展示了语义控制在反事实推理中的应用潜力。

Abstract: Counterfactual image generation presents significant challenges, including
preserving identity, maintaining perceptual quality, and ensuring faithfulness
to an underlying causal model. While existing auto-encoding frameworks admit
semantic latent spaces which can be manipulated for causal control, they
struggle with scalability and fidelity. Advancements in diffusion models
present opportunities for improving counterfactual image editing, having
demonstrated state-of-the-art visual quality, human-aligned perception and
representation learning capabilities. Here, we present a suite of
diffusion-based causal mechanisms, introducing the notions of spatial, semantic
and dynamic abduction. We propose a general framework that integrates semantic
representations into diffusion models through the lens of Pearlian causality to
edit images via a counterfactual reasoning process. To our knowledge, this is
the first work to consider high-level semantic identity preservation for
diffusion counterfactuals and to demonstrate how semantic control enables
principled trade-offs between faithful causal control and identity
preservation.

</details>


### [257] [Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions](https://arxiv.org/abs/2506.07884)
*Anand Ganesh,Babhrubahan Bose,Anand Rajagopalan*

Main category: cs.LG

TL;DR: This paper constructs four Schauder bases for the space C[0,1], which improves on the universal approximation property associated with ReLU, Softplus and their sigmoidal versions.


<details>
  <summary>Details</summary>
Motivation: To establish the existence of a basis using ReLU, Softplus and their sigmoidal versions for the first time and improve on the universal approximation property associated with them.

Method: Constructing four Schauder bases for the space C[0,1], one using ReLU functions, another using Softplus functions, and two more using sigmoidal versions of the ReLU and Softplus functions.

Result: Successful construction of four Schauder bases for the space C[0,1].

Conclusion: The construction of four Schauder bases for the space C[0,1] using ReLU, Softplus and their sigmoidal versions has been established. This improves on the universal approximation property associated with these functions.

Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU
functions, another using Softplus functions, and two more using sigmoidal
versions of the ReLU and Softplus functions. This establishes the existence of
a basis using these functions for the first time, and improves on the universal
approximation property associated with them.

</details>


### [258] [FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling](https://arxiv.org/abs/2506.07902)
*Sifan Wang,Zehao Dou,Tong-Rui Liu,Lu Lu*

Main category: cs.LG

TL;DR: Recent advances in generative modeling have succeeded in synthesizing discrete data, but adapting them to physical applications is still challenging. This paper introduces FunDiff, a novel framework for generative modeling in function spaces that combines a latent diffusion process with a function autoencoder architecture. The authors theoretically establish minimax optimality guarantees and demonstrate the practical effectiveness of FunDiff across diverse applications.


<details>
  <summary>Details</summary>
Motivation: Adapting generative models to physical applications remains challenging due to the complexity of continuous functions governed by physical laws.

Method: FunDiff combines a latent diffusion process with a function autoencoder architecture. It handles input functions with varying discretizations, generates continuous functions evaluable at arbitrary locations, and incorporates physical priors through architectural constraints or physics-informed loss functions.

Result: Empirical results show that FunDiff generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy and low-resolution data.

Conclusion: FunDiff is a promising framework for generative modeling in function spaces with theoretical guarantees and practical effectiveness in physical applications.

Abstract: Recent advances in generative modeling -- particularly diffusion models and
flow matching -- have achieved remarkable success in synthesizing discrete data
such as images and videos. However, adapting these models to physical
applications remains challenging, as the quantities of interest are continuous
functions governed by complex physical laws. Here, we introduce
$\textbf{FunDiff}$, a novel framework for generative modeling in function
spaces. FunDiff combines a latent diffusion process with a function autoencoder
architecture to handle input functions with varying discretizations, generate
continuous functions evaluable at arbitrary locations, and seamlessly
incorporate physical priors. These priors are enforced through architectural
constraints or physics-informed loss functions, ensuring that generated samples
satisfy fundamental physical laws. We theoretically establish minimax
optimality guarantees for density estimation in function spaces, showing that
diffusion-based estimators achieve optimal convergence rates under suitable
regularity conditions. We demonstrate the practical effectiveness of FunDiff
across diverse applications in fluid dynamics and solid mechanics. Empirical
results show that our method generates physically consistent samples with high
fidelity to the target distribution and exhibits robustness to noisy and
low-resolution data. Code and datasets are publicly available at
https://github.com/sifanexisted/fundiff.

</details>


### [259] [Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces](https://arxiv.org/abs/2506.07903)
*Kevin Rojas,Yuchen Zhu,Sichen Zhu,Felix X. -F. Ye,Molei Tao*

Main category: cs.LG

TL;DR: Diffusion models excel in unimodal data generation but lack in multimodal joint generation. This paper proposes a new framework for multimodal diffusion models that allows native generation across different modalities by using a decoupled noise schedule for each modality, showing competitive performance in text-image generation and mixed-type tabular data synthesis.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing approaches that rely heavily on external preprocessing protocols to harmonize varied data representations into a unified, unimodal format which can be problematic for applications with limited data.

Method: Propose a novel framework for building multimodal diffusion models on arbitrary state spaces with an innovative decoupled noise schedule for each modality, enabling unconditional and modality-conditioned generation within a single model simultaneously.

Result: Empirically validated for text-image generation and mixed-type tabular data synthesis, achieving competitive performance.

Conclusion: The proposed framework for multimodal diffusion models with decoupled noise schedule shows promise in generating coupled data across different modalities.

Abstract: Diffusion models have demonstrated remarkable performance in generating
unimodal data across various tasks, including image, video, and text
generation. On the contrary, the joint generation of multimodal data through
diffusion models is still in the early stages of exploration. Existing
approaches heavily rely on external preprocessing protocols, such as tokenizers
and variational autoencoders, to harmonize varied data representations into a
unified, unimodal format. This process heavily demands the high accuracy of
encoders and decoders, which can be problematic for applications with limited
data. To lift this restriction, we propose a novel framework for building
multimodal diffusion models on arbitrary state spaces, enabling native
generation of coupled data across different modalities. By introducing an
innovative decoupled noise schedule for each modality, we enable both
unconditional and modality-conditioned generation within a single model
simultaneously. We empirically validate our approach for text-image generation
and mixed-type tabular data synthesis, demonstrating that it achieves
competitive performance.

</details>


### [260] [CausalPFN: Amortized Causal Effect Estimation via In-Context Learning](https://arxiv.org/abs/2506.07918)
*Vahid Balazadeh,Hamidreza Kamkari,Valentin Thomas,Benson Li,Junwei Ma,Jesse C. Cresswell,Rahul G. Krishnan*

Main category: cs.LG

TL;DR: CausalPFN is a transformer that simplifies causal effect estimation from observational data by automating the selection of an appropriate estimator, providing reliable uncertainty estimates and not needing further training or tuning.


<details>
  <summary>Details</summary>
Motivation: The manual effort and domain expertise required to select an appropriate estimator for causal effect estimation from observational data motivates the development of CausalPFN.

Method: CausalPFN combines Bayesian causal inference with the large-scale training protocol of prior-fitted networks (PFNs) to map raw observations directly to causal effects. It is trained once on a large library of simulated data-generating processes that satisfy ignorability and can infer causal effects for new observational datasets out-of-the-box.

Result: CausalPFN achieves superior average performance on heterogeneous and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC) and shows competitive performance for real-world policy making on uplift modeling tasks.

Conclusion: CausalPFN provides calibrated uncertainty estimates supporting reliable decision-making based on Bayesian principles and represents a step toward automated causal inference.

Abstract: Causal effect estimation from observational data is fundamental across
various applications. However, selecting an appropriate estimator from dozens
of specialized methods demands substantial manual effort and domain expertise.
We present CausalPFN, a single transformer that amortizes this workflow:
trained once on a large library of simulated data-generating processes that
satisfy ignorability, it infers causal effects for new observational datasets
out-of-the-box. CausalPFN combines ideas from Bayesian causal inference with
the large-scale training protocol of prior-fitted networks (PFNs), learning to
map raw observations directly to causal effects without any task-specific
adjustment. Our approach achieves superior average performance on heterogeneous
and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).
Moreover, it shows competitive performance for real-world policy making on
uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to
support reliable decision-making based on Bayesian principles. This
ready-to-use model does not require any further training or tuning and takes a
step toward automated causal inference (https://github.com/vdblm/CausalPFN).

</details>


### [261] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 在序列建模任务中，非线性并非总是必要。研究发现最小非线性通常是最优的，可生成更简单、更稳健和更易解释的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管非线性递归被认为对于记忆和长距离时间处理机制至关重要，但近期研究表明线性动力学可能已经足够。因此需要系统地剖析非线性在递归网络中的功能角色，明确其计算必要性和所支持的机制。

Method: 使用几乎线性递归神经网络（AL-RNNs）作为灵活的建模工具和探针，对多种经典序列建模任务和一个真实世界的刺激选择任务进行分析，以控制非线性的细粒度。

Result: 发现在多个任务中，最小非线性不仅足够而且通常是最佳的，可以产生比完全非线性或线性模型更简单、更稳健和更可解释的模型。

Conclusion: 提供了有原则的框架来选择性引入非线性，将动力系统理论与递归神经网络中长距离记忆和结构化计算的功能需求联系起来，对人工和生物神经系统都有启示。

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [262] [W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling](https://arxiv.org/abs/2506.07920)
*Hossein Babaei,Mel White,Richard G. Baraniuk*

Main category: cs.LG

TL;DR: State Space Models (SSMs) are powerful for sequence modeling but rely on the state matrix's choice and initialization. This paper introduces W4S4, a new SSM variant based on redundant wavelet frames within the WaLRUS framework. It offers stable diagonalization, fast kernel computation, and better long-horizon information retention compared to HiPPO-based SSMs. Experiments show improvements in various tasks, highlighting the advantages of wavelet-based state dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing SSMs, which heavily depend on the choice and initialization of the state matrix, and to improve their efficiency and effectiveness in handling long-range dependencies.

Method: The method involves constructing a new class of SSMs called W4S4 from redundant wavelet frames within the WaLRUS framework. This approach allows for stable diagonalization and fast kernel computation without low-rank approximations.

Result: WaLRUS retains information over long horizons significantly better than HiPPO-based SSMs. The experiments demonstrate consistent improvements across delay reconstruction tasks, classification benchmarks, and long-range sequence modeling.

Conclusion: W4S4 provides a scalable and versatile foundation for the next generation of deep SSM-based models, offering substantial advantages due to high-quality, structured initialization enabled by wavelet-based state dynamics.

Abstract: State Space Models (SSMs) have emerged as powerful components for sequence
modeling, enabling efficient handling of long-range dependencies via linear
recurrence and convolutional computation. However, their effectiveness depends
heavily on the choice and initialization of the state matrix. In this work, we
build on the SaFARi framework and existing WaLRUS SSMs to introduce a new
variant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant
wavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel
computation without requiring low-rank approximations, making it both
theoretically grounded and computationally efficient. We show that WaLRUS
retains information over long horizons significantly better than HiPPO-based
SSMs, both in isolation and when integrated into deep architectures such as S4.
Our experiments demonstrate consistent improvements across delay reconstruction
tasks, classification benchmarks, and long-range sequence modeling, confirming
that high-quality, structured initialization enabled by wavelet-based state
dynamic offers substantial advantages over existing alternatives. WaLRUS
provides a scalable and versatile foundation for the next generation of deep
SSM-based models.

</details>


### [263] [A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle](https://arxiv.org/abs/2506.07929)
*Amirreza Yasami,Mohammadali Tofigh,Mahdi Shahbakhti,Charles Robert Koch*

Main category: cs.LG

TL;DR: A new method called PIESMC is developed for constructing representative driving cycles, which is more accurate and efficient than existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate driving cycle construction is crucial for vehicle design, fuel economy analysis, and environmental impact assessments.

Method: PIESMC approach captures transient dynamics, acceleration, deceleration, idling, and road grade transitions while ensuring model fidelity using a physics-informed reinforcement learning framework with Monte Carlo sampling.

Result: PIESMC achieves up to a 57.3% reduction in cumulative kinematic fragment errors compared to the MTB method and a 10.5% reduction relative to the MCB method. It is nearly an order of magnitude faster than conventional techniques.

Conclusion: PIESMC delivers efficient cycle construction with reduced computational cost and accurately replicates key kinematic and energy metrics.

Abstract: Accurate driving cycle construction is crucial for vehicle design, fuel
economy analysis, and environmental impact assessments. A generative
Physics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs
representative driving cycles by capturing transient dynamics, acceleration,
deceleration, idling, and road grade transitions while ensuring model fidelity
is introduced. Leveraging a physics-informed reinforcement learning framework
with Monte Carlo sampling, PIESMC delivers efficient cycle construction with
reduced computational cost. Experimental evaluations on two real-world datasets
demonstrate that PIESMC replicates key kinematic and energy metrics, achieving
up to a 57.3% reduction in cumulative kinematic fragment errors compared to the
Micro-trip-based (MTB) method and a 10.5% reduction relative to the
Markov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude
faster than conventional techniques. Analyses of vehicle-specific power
distributions and wavelet-transformed frequency content further confirm its
ability to reproduce experimental central tendencies and variability.

</details>


### [264] [Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions](https://arxiv.org/abs/2506.07933)
*Lev V. Utkin,Semen P. Khomets,Vlada A. Efremenko,Andrei V. Konstantinov,Natalya M. Verbova*

Main category: cs.LG

TL;DR: SurvBESA is a new ensemble model that combines Beran estimators with self-attention to predict survival functions more accurately and stably than existing methods.


<details>
  <summary>Details</summary>
Motivation: Survival analysis predictions can be unstable due to censored data and variations in bootstrap samples when using ensemble-based models.

Method: Propose SurvBESA, an ensemble model that uses Beran estimators and a self-attention mechanism on predicted survival functions; also explore a special case using Huber's contamination model for defining attention weights, simplifying training.

Result: Numerical experiments demonstrate SurvBESA outperforms state-of-the-art models in survival analysis.

Conclusion: SurvBESA offers improved performance and stability in survival analysis predictions compared to existing models.

Abstract: Survival analysis predicts the time until an event of interest, such as
failure or death, but faces challenges due to censored data, where some events
remain unobserved. Ensemble-based models, like random survival forests and
gradient boosting, are widely used but can produce unstable predictions due to
variations in bootstrap samples. To address this, we propose SurvBESA (Survival
Beran Estimators Self-Attended), a novel ensemble model that combines Beran
estimators with a self-attention mechanism. Unlike traditional methods,
SurvBESA applies self-attention to predicted survival functions, smoothing out
noise by adjusting each survival function based on its similarity to
neighboring survival functions. We also explore a special case using Huber's
contamination model to define attention weights, simplifying training to a
quadratic or linear optimization problem. Numerical experiments show that
SurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is
publicly available.

</details>


### [265] [Cost-Optimal Active AI Model Evaluation](https://arxiv.org/abs/2506.07949)
*Anastasios N. Angelopoulos,Jacob Eisenstein,Jonathan Berant,Alekh Agarwal,Adam Fisch*

Main category: cs.LG

TL;DR: 在生成式AI系统的开发周期中，持续的评估、数据获取和标注既耗费资源又耗时。本文提出了一种新颖的成本感知方法，通过平衡使用廉价但不准确的弱评估者（如模型自动评估器）与昂贵但更准确的强评估者（如人类），以在有限预算下实现对目标'强'评分均值的低偏差无偏估计。基于主动学习和预测驱动统计推断的研究，作者推导出一系列成本最优策略，用于在弱评估者和强评估者之间分配注释预算，从而最大化统计效率。实验表明，在样本难度差异较大的任务中，新策略能在显著降低总标注预算的情况下达到与传统方法相同的估计精度。


<details>
  <summary>Details</summary>
Motivation: 生成式AI系统的生命周期需要不断进行评估、数据获取和标注，这既消耗资源又耗时。实际应用中，由于快速迭代的需求，往往依赖合成标注数据，尽管其可能存在显著偏差。因此，研究如何有效结合廉价但不准确的弱评估者与昂贵但准确的强评估者，成为了一个重要问题。

Method: 作者提出了一种新颖的成本感知方法，旨在通过优化弱评估者和强评估者之间的预算分配，来最大化统计效率。具体来说，他们基于主动学习和预测驱动统计推断技术，推导出一系列成本最优策略。这些策略能够根据任务特点动态调整弱评估者和强评估者的使用比例。

Result: 通过使用合成数据和真实世界数据进行实验，作者发现，在样本难度差异较大的任务中，新策略可以在显著降低总标注预算的情况下达到与标准评估方法相同的估计精度。

Conclusion: 本研究提出的方法能够在生成式AI系统的评估过程中，通过合理分配弱评估者和强评估者的使用比例，有效降低总标注成本，同时保持较高的估计精度。这对于资源受限的实际应用场景具有重要意义。

Abstract: The development lifecycle of generative AI systems requires continual
evaluation, data acquisition, and annotation, which is costly in both resources
and time. In practice, rapid iteration often makes it necessary to rely on
synthetic annotation data because of the low cost, despite the potential for
substantial bias. In this paper, we develop novel, cost-aware methods for
actively balancing the use of a cheap, but often inaccurate, weak rater -- such
as a model-based autorater that is designed to automatically assess the quality
of generated content -- with a more expensive, but also more accurate, strong
rater alternative such as a human. More specifically, the goal of our approach
is to produce a low variance, unbiased estimate of the mean of the target
"strong" rating, subject to some total annotation budget. Building on recent
work in active and prediction-powered statistical inference, we derive a family
of cost-optimal policies for allocating a given annotation budget between weak
and strong raters so as to maximize statistical efficiency. Using synthetic and
real-world data, we empirically characterize the conditions under which these
policies yield improvements over prior methods. We find that, especially in
tasks where there is high variability in the difficulty of examples, our
policies can achieve the same estimation precision at a far lower total
annotation budget than standard evaluation methods.

</details>


### [266] [Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs](https://arxiv.org/abs/2506.07958)
*Salah A. Faroughi,Farinaz Mostajeran*

Main category: cs.LG

TL;DR: Physics-informed Kolmogorov-Arnold Networks (PIKANs), particularly their Chebyshev-based variants (cPIKANs), have shown promise in solving PDEs. This paper advances the theoretical understanding of cPIKANs by analyzing them with Neural Tangent Kernel (NTK) theory, examining kernel structure evolution and its impact on learning efficiency during gradient-based training.


<details>
  <summary>Details</summary>
Motivation: To deepen the theoretical understanding of cPIKANs' training dynamics and convergence behavior, which remain largely unexplored both theoretically and numerically.

Method: Derive the NTK of standard cKANs in a supervised setting, extend the analysis to the physics-informed context, analyze the spectral properties of NTK matrices for four representative PDEs, and investigate the impact of various optimization strategies on the evolution of the NTK and learning dynamics.

Result: Results show tractable NTK behavior in cPIKANs that uncovers learning dynamics not captured by standard PINNs. Spectral trends also reveal conditions where domain decomposition improves training and link kernel behavior to convergence rates under different setups.

Conclusion: This study is the first systematic NTK analysis of cPIKANs, offering theoretical insights that clarify and predict their empirical performance.

Abstract: Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their
Chebyshev-based variants (cPIKANs), have recently emerged as promising models
for solving partial differential equations (PDEs). However, their training
dynamics and convergence behavior remain largely unexplored both theoretically
and numerically. In this work, we aim to advance the theoretical understanding
of cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our
objective is to discern the evolution of kernel structure throughout
gradient-based training and its subsequent impact on learning efficiency. We
first derive the NTK of standard cKANs in a supervised setting, and then extend
the analysis to the physics-informed context. We analyze the spectral
properties of NTK matrices, specifically their eigenvalue distributions and
spectral bias, for four representative PDEs: the steady-state Helmholtz
equation, transient diffusion and Allen-Cahn equations, and forced vibrations
governed by the Euler-Bernoulli beam equation. We also conduct an investigation
into the impact of various optimization strategies, e.g., first-order,
second-order, and hybrid approaches, on the evolution of the NTK and the
resulting learning dynamics. Results indicate a tractable behavior for NTK in
the context of cPIKANs, which exposes learning dynamics that standard
physics-informed neural networks (PINNs) cannot capture. Spectral trends also
reveal when domain decomposition improves training, directly linking kernel
behavior to convergence rates under different setups. To the best of our
knowledge, this is the first systematic NTK study of cPIKANs, providing
theoretical insight that clarifies and predicts their empirical performance.

</details>


### [267] [A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling](https://arxiv.org/abs/2506.07969)
*Jacob Helwig,Sai Sreeharsha Adavi,Xuan Zhang,Yuchao Lin,Felix S. Chim,Luke Takeshi Vizzini,Haiyang Yu,Muhammad Hasnain,Saykat Kumar Biswas,John J. Holloway,Narendra Singh,N. K. Anand,Swagnik Guhathakurta,Shuiwang Ji*

Main category: cs.LG

TL;DR: A two-phase machine learning method, ShockCast, is proposed for modeling high-speed flows with adaptive time-stepping.


<details>
  <summary>Details</summary>
Motivation: High-speed flows approaching and exceeding the speed of sound exhibit sudden changes such as shock waves. Adaptive time-stepping methods are essential to resolve these phenomena while balancing computational costs.

Method: The method consists of two phases: 1) using a machine learning model to predict the timestep size; 2) using the predicted timestep along with current fluid fields to advance the system state. Several physically-motivated components for timestep prediction are explored and timestep conditioning strategies inspired by neural ODE and Mixture of Experts are introduced.

Result: ShockCast is evaluated by generating two supersonic flow datasets. The code is publicly available as part of the AIRS library.

Conclusion: ShockCast is the first framework for learning high-speed flows and it provides an effective solution for modeling such flows with adaptive time-stepping.

Abstract: We consider the problem of modeling high-speed flows using machine learning
methods. While most prior studies focus on low-speed fluid flows in which
uniform time-stepping is practical, flows approaching and exceeding the speed
of sound exhibit sudden changes such as shock waves. In such cases, it is
essential to use adaptive time-stepping methods to allow a temporal resolution
sufficient to resolve these phenomena while simultaneously balancing
computational costs. Here, we propose a two-phase machine learning method,
known as ShockCast, to model high-speed flows with adaptive time-stepping. In
the first phase, we propose to employ a machine learning model to predict the
timestep size. In the second phase, the predicted timestep is used as an input
along with the current fluid fields to advance the system state by the
predicted timestep. We explore several physically-motivated components for
timestep prediction and introduce timestep conditioning strategies inspired by
neural ODE and Mixture of Experts. As ShockCast is the first framework for
learning high-speed flows, we evaluate our methods by generating two supersonic
flow datasets, available at https://huggingface.co/datasets/divelab. Our code
is publicly available as part of the AIRS library
(https://github.com/divelab/AIRS).

</details>


### [268] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: The paper introduces HeuriGym, a framework for evaluating LLMs' ability to generate heuristic algorithms for combinatorial optimization problems. It highlights current evaluation limitations, presents the Quality-Yield Index (QYI) metric, and reveals persistent limitations in LLMs' tool use, planning, and adaptive reasoning.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methodologies for LLMs are inadequate, as they either rely on closed-ended questions that lead to saturation and memorization or subjective comparisons lacking consistency and rigor.

Method: HeuriGym is an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems. It allows LLMs to propose heuristics, receive feedback via code execution, and iteratively refine their solutions.

Result: Evaluation of nine state-of-the-art models on nine problems across various domains using HeuriGym exposed persistent limitations in LLMs' tool use, planning, and adaptive reasoning. Top models achieved QYI scores of only 0.6, significantly below the expert baseline of 1.

Conclusion: HeuriGym provides a valuable benchmark for guiding the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [269] [Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum](https://arxiv.org/abs/2506.07975)
*Caleb Zheng,Eli Shlizerman*

Main category: cs.LG

TL;DR: 提出了一种新的修剪方法——超修剪（hyperpruning），并引入了基于李雅普诺夫谱（LS）距离的高效超修剪框架（LSH），该框架能够快速识别优于密集模型的修剪变体。


<details>
  <summary>Details</summary>
Motivation: 现有的修剪方法虽然可以提高效率，但在搜索最佳修剪配置时计算成本高昂且缺乏早期性能保证。因此，需要一种更高效的搜索策略来确定最适合的修剪方案。

Method: 提出了基于李雅普诺夫谱（LS）距离的度量方法，允许在训练前比较修剪网络和密集网络，并预测训练后的性能。结合标准超参数优化算法，构建了LSH框架，以加速搜索过程并找到最优修剪配置。

Result: 在多个数据集和架构上的实验表明，LSH能够在固定训练预算和目标修剪率下，持续发现优于损失基准选择的修剪模型，甚至超越原始密集模型的性能。

Conclusion: LSH框架显著减少了搜索时间，同时提高了修剪模型的性能，为高效网络修剪提供了一种新途径。

Abstract: A variety of pruning methods have been introduced for over-parameterized
Recurrent Neural Networks to improve efficiency in terms of power consumption
and storage utilization. These advances motivate a new paradigm, termed
`hyperpruning', which seeks to identify the most suitable pruning strategy for
a given network architecture and application. Unlike conventional
hyperparameter search, where the optimal configuration's accuracy remains
uncertain, in the context of network pruning, the accuracy of the dense model
sets the target for the accuracy of the pruned one. The goal, therefore, is to
discover pruned variants that match or even surpass this established accuracy.
However, exhaustive search over pruning configurations is computationally
expensive and lacks early performance guarantees. To address this challenge, we
propose a novel Lyapunov Spectrum (LS)-based distance metric that enables early
comparison between pruned and dense networks, allowing accurate prediction of
post-training performance. By integrating this LS-based distance with standard
hyperparameter optimization algorithms, we introduce an efficient hyperpruning
framework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an
order of magnitude compared to conventional approaches relying on full
training. Experiments on stacked LSTM and RHN architectures using the Penn
Treebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under
fixed training budgets and target pruning ratios, LSH consistently identifies
superior pruned models. Remarkably, these pruned variants not only outperform
those selected by loss-based baseline but also exceed the performance of their
dense counterpart.

</details>


### [270] [Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction](https://arxiv.org/abs/2506.07976)
*Junhong Shen,Hao Bai,Lunjun Zhang,Yifei Zhou,Amrith Setlur,Shengbang Tong,Diego Caples,Nan Jiang,Tong Zhang,Ameet Talwalkar,Aviral Kumar*

Main category: cs.LG

TL;DR: 在当前测试时扩展范式中，提出了一种新的方法TTI（Test-Time Interaction），通过扩展交互维度来增强代理的行为能力，如探索、回溯和动态重新规划。研究显示，即使没有任何训练的提示交互扩展也能显著提高任务成功率，并且TTI方法在WebVoyager和WebArena基准上达到了最先进的开源开放数据网络代理水平。


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展范式依赖于生成长推理痕迹，但在需要交互的问题中，这种过程无法让代理从环境中获取新信息或随时间调整行为。

Method: 提出TTI（Test-Time Interaction）方法，这是一种基于课程的在线强化学习方法，通过自适应调整代理的 rollout 长度来训练代理。

Result: TTI方法使用Gemma 3 12B模型，在WebVoyager和WebArena基准上产生了最先进的开源开放数据网络代理，并且能够使代理自适应地平衡探索与利用。

Conclusion: 交互扩展是与每步计算扩展互补的强大轴线，为训练自适应代理提供了新途径。

Abstract: The current paradigm of test-time scaling relies on generating long reasoning
traces ("thinking" more) before producing a response. In agent problems that
require interaction, this can be done by generating thinking traces before
acting in the world. However, this process does not allow agents to acquire new
information from the environment or adapt their behavior over time. In this
work, we propose to scale test-time interaction, an untapped dimension of
test-time scaling that increases the agent's interaction horizon to enable
running rich behaviors such as exploration, backtracking, and dynamic
re-planning within a single rollout. To demonstrate the promise of this scaling
dimension, we study the domain of web agents. We first show that even
prompting-based interaction scaling without any training can improve task
success on web benchmarks non-trivially. Building on this, we introduce TTI
(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)
approach that trains agents by adaptively adjusting their rollout lengths.
Using a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data
web agents on WebVoyager and WebArena benchmarks. We further show that TTI
enables agents to balance exploration and exploitation adaptively. Our results
establish interaction scaling as a powerful, complementary axis to scaling
per-step compute, offering new avenues for training adaptive agents.

</details>


### [271] [Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator](https://arxiv.org/abs/2506.07980)
*Alberto Bazán-Guillén,Carlos Beis-Penedo,Diego Cajaraville-Aboy,Pablo Barbecho-Bautista,Rebeca P. Díaz-Redondo,Luis J. de la Cruz Llopis,Ana Fernández-Vilas,Mónica Aguilar Igartua,Manuel Fernández-Veiga*

Main category: cs.LG

TL;DR: DesRUTGe是一种新的框架，它结合了深度强化学习代理和SUMO模拟器来生成真实的24小时交通模式，并使用去中心化联邦学习提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在生成高保真、时变的交通配置文件方面存在局限性，尤其是在大规模场景中，面临准确性、可扩展性或隐私问题等挑战。

Method: 引入DesRUTGe框架，将深度强化学习与SUMO模拟器相结合，通过去中心化联邦学习，使每个交通检测器及其对应的城区作为一个独立的学习节点进行本地模型训练，并通过与选定的对等节点交换模型参数来协作改进性能，而无需中央协调器。

Result: 使用巴塞罗那的实际数据评估表明，DesRUTGe在生成更准确且保护隐私的交通模式方面优于基于SUMO的标准工具和其他集中式学习方法。

Conclusion: DesRUTGe框架提供了一种有效的方法，可以在不需要中央协调器的情况下生成准确和隐私保护的交通模式，为可持续的城市规划和智能交通系统的发展提供了支持。

Abstract: Realistic urban traffic simulation is essential for sustainable urban
planning and the development of intelligent transportation systems. However,
generating high-fidelity, time-varying traffic profiles that accurately reflect
real-world conditions, especially in large-scale scenarios, remains a major
challenge. Existing methods often suffer from limitations in accuracy,
scalability, or raise privacy concerns due to centralized data processing. This
work introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a
novel framework that integrates Deep Reinforcement Learning (DRL) agents with
the SUMO simulator to generate realistic 24-hour traffic patterns. A key
innovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),
wherein each traffic detector and its corresponding urban zone function as an
independent learning node. These nodes train local DRL models using minimal
historical data and collaboratively refine their performance by exchanging
model parameters with selected peers (e.g., geographically adjacent zones),
without requiring a central coordinator. Evaluated using real-world data from
the city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as
RouteSampler, as well as other centralized learning approaches, by delivering
more accurate and privacy-preserving traffic pattern generation.

</details>


### [272] [Generative Modeling of Weights: Generalization or Memorization?](https://arxiv.org/abs/2506.07998)
*Boya Zeng,Yida Yin,Zhiqiu Xu,Zhuang Liu*

Main category: cs.LG

TL;DR: 生成模型在合成有效神经网络权重方面的探索存在局限性，当前方法主要依赖于记忆训练数据，难以超越简单基线方法，需更谨慎评估其在新领域的应用。


<details>
  <summary>Details</summary>
Motivation: 研究生成模型在合成神经网络权重方面的潜力，评估其生成新颖权重的能力，并探讨如何克服记忆效应的限制。

Method: 选取四种代表性方法，分析它们生成不同于训练数据的高表现模型权重的能力，通过与简单基线方法（如添加噪声或权重集成）进行比较，评估生成模型的记忆特性及改进可能性。

Result: 发现这些方法主要通过记忆生成权重，生成结果多为复制品或简单插值，未能显著优于简单基线方法；修改常见建模因素或应用数据增强也无法有效缓解记忆问题。

Conclusion: 当前生成模型在合成新颖神经网络权重方面效果有限，强调需要更仔细地评估生成模型在新领域中的表现，并明确其适用的数据类型。

Abstract: Generative models, with their success in image and video generation, have
recently been explored for synthesizing effective neural network weights. These
approaches take trained neural network checkpoints as training data, and aim to
generate high-performing neural network weights during inference. In this work,
we examine four representative methods on their ability to generate novel model
weights, i.e., weights that are different from the checkpoints seen during
training. Surprisingly, we find that these methods synthesize weights largely
by memorization: they produce either replicas, or at best simple
interpolations, of the training checkpoints. Current methods fail to outperform
simple baselines, such as adding noise to the weights or taking a simple weight
ensemble, in obtaining different and simultaneously high-performing models. We
further show that this memorization cannot be effectively mitigated by
modifying modeling factors commonly associated with memorization in image
diffusion models, or applying data augmentations. Our findings provide a
realistic assessment of what types of data current generative models can model,
and highlight the need for more careful evaluation of generative models in new
domains. Our code is available at
https://github.com/boyazeng/weight_memorization.

</details>


### [273] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: POET is a new algorithm for training large language models using orthogonal transformations, improving optimization and generalization.


<details>
  <summary>Details</summary>
Motivation: Effectively and reliably training large language models remains a significant challenge in the field of artificial intelligence.

Method: POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix, using Orthogonal Equivalence Transformation to optimize neurons.

Result: Extensive experiments validate the effectiveness and scalability of POET in training LLMs.

Conclusion: POET provides a stable way to optimize the objective function with improved generalization for large-scale neural networks.

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [274] [TimeWak: Temporal Chained-Hashing Watermark for Time Series Data](https://arxiv.org/abs/2506.06407)
*Zhi Wen Soi,Chaoyi Zhu,Fouad Abiad,Aditya Shankar,Jeroen M. Galjaard,Huijuan Wang,Lydia Y. Chen*

Main category: cs.CR

TL;DR: The paper introduces TimeWak, a watermarking algorithm for multivariate time series diffusion models which improves synthetic data quality and watermark detectability.


<details>
  <summary>Details</summary>
Motivation: There is a need to share privacy-sensitive datasets while maintaining high data utility and traceability. Current watermarking methods are incompatible with state-of-the-art time series generators that operate in real space.

Method: TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space to handle temporal dependence and spatial heterogeneity. It also uses $\epsilon$-exact inversion to address non-uniform reconstruction error distribution across features.

Result: TimeWak achieves improvements of 61.96% in context-FID score and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.

Conclusion: TimeWak is an effective watermarking algorithm for multivariate time series diffusion models that maintains high synthetic data quality and watermark detectability.

Abstract: Synthetic time series generated by diffusion models enable sharing
privacy-sensitive datasets, such as patients' functional MRI records. Key
criteria for synthetic data include high data utility and traceability to
verify the data source. Recent watermarking methods embed in homogeneous latent
spaces, but state-of-the-art time series generators operate in real space,
making latent-based watermarking incompatible. This creates the challenge of
watermarking directly in real space while handling feature heterogeneity and
temporal dependencies. We propose TimeWak, the first watermarking algorithm for
multivariate time series diffusion models. To handle temporal dependence and
spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark
directly within the real temporal-feature space. The other unique feature is
the $\epsilon$-exact inversion, which addresses the non-uniform reconstruction
error distribution across features from inverting the diffusion process to
detect watermarks. We derive the error bound of inverting multivariate time
series and further maintain high watermark detectability. We extensively
evaluate TimeWak on its impact on synthetic data quality, watermark
detectability, and robustness under various post-editing attacks, against 5
datasets and baselines of different temporal lengths. Our results show that
TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in
correlational scores against the state-of-the-art baseline, while remaining
consistently detectable.

</details>


### [275] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 大型语言模型（LLM）水印通过改变下一个词预测来验证文本来源，减少机器生成文本的滥用，并促进对AI系统的信任。本文提出了一种优化框架设计水印方法，以有效利用随机信息实现最大化水印检测可能性和最小化文本失真为目标，提出了两种新型水印：HeavyWater和SimplexWater，具有可调性和广泛的适用性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM水印技术在低熵生成任务中面临挑战，例如编码任务中的近似确定性预测。因此，需要一种新的优化框架来设计更有效的水印方法，平衡检测准确性和文本失真。

Method: 提出一个优化框架用于水印设计，分析如何使用随机侧信息以最大化水印检测的可能性并最小化生成文本的失真。基于此框架开发了两种新水印方法：HeavyWater和SimplexWater，这两种方法都可调节，能够平衡检测精度与文本失真，并适用于任何LLM。

Result: HeavyWater和SimplexWater在多个基准测试中表现出色，能够在低熵环境中实现高水印检测准确率，同时对文本生成质量的影响极小。此外，理论分析揭示了LLM水印和编码理论之间的新联系。

Conclusion: 提出的水印设计优化框架及HeavyWater和SimplexWater方法为LLM水印技术提供了显著改进，在保持高质量文本生成的同时提高了水印检测能力，推动了该领域的发展。

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [276] [Benchmarking Misuse Mitigation Against Covert Adversaries](https://arxiv.org/abs/2506.06414)
*Davis Brown,Mahdi Sabbaghi,Luze Sun,Alexander Robey,George J. Pappas,Eric Wong,Hamed Hassani*

Main category: cs.CR

TL;DR: The paper addresses the issue of covert attacks in language models that can be conducted through seemingly harmless individual queries, which when combined, help attackers complete dangerous tasks. To tackle this, they developed Benchmarks for Stateful Defenses (BSD), a data generation pipeline for evaluating such attacks and defenses. Evaluations reveal decomposition attacks as effective misuse enablers, emphasizing stateful defenses as a countermeasure.


<details>
  <summary>Details</summary>
Motivation: Current safety evaluations for language models mainly focus on overt attacks and low-stakes tasks, ignoring more subtle threats where attackers use multiple small, independent, and benign-seeming tasks to bypass safeguards.

Method: The authors developed Benchmarks for Stateful Defenses (BSD), a data generation pipeline to automate evaluations of covert attacks and their corresponding defenses. They used BSD to create two new datasets that are refused by advanced models and too complex for less capable open-weight models.

Result: Evaluations using the BSD pipeline indicate that decomposition attacks are effective in enabling misuse of language models, demonstrating the necessity and effectiveness of stateful defenses.

Conclusion: The study highlights the potential threat of covert attacks in language models and suggests stateful defenses as an important countermeasure against such strategies.

Abstract: Existing language model safety evaluations focus on overt attacks and
low-stakes tasks. Realistic attackers can subvert current safeguards by
requesting help on small, benign-seeming tasks across many independent queries.
Because individual queries do not appear harmful, the attack is hard to
{detect}. However, when combined, these fragments uplift misuse by helping the
attacker complete hard and dangerous tasks. Toward identifying defenses against
such strategies, we develop Benchmarks for Stateful Defenses (BSD), a data
generation pipeline that automates evaluations of covert attacks and
corresponding defenses. Using this pipeline, we curate two new datasets that
are consistently refused by frontier models and are too difficult for weaker
open-weight models. Our evaluations indicate that decomposition attacks are
effective misuse enablers, and highlight stateful defenses as a countermeasure.

</details>


### [277] [A Systematic Review of Poisoning Attacks Against Large Language Models](https://arxiv.org/abs/2506.06518)
*Neil Fendley,Edward W. Staley,Joshua Carney,William Redman,Marie Chau,Nathan Drenkow*

Main category: cs.CR

TL;DR: With the rise of Large Language Models (LLMs), security risks like LLM poisoning attacks have become a major concern. This paper conducts a systematic review of published LLM poisoning attacks, proposes a comprehensive threat model to categorize these attacks, and discusses them along four critical dimensions.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistencies in terminology and lack of a proper framework for understanding LLM poisoning attacks in generative settings.

Method: Systematic review of published LLM poisoning attacks, proposal of a comprehensive poisoning threat model with four attack specifications and six poisoning metrics, and organization of discussion along four critical dimensions of LLM poisoning attacks.

Result: Clarification of security implications of LLM poisoning attacks, establishment of consistent terminology, and provision of a framework to better understand and categorize these attacks.

Conclusion: The proposed threat model provides a solid foundation for future research on LLM poisoning attacks and enhances understanding of associated security risks.

Abstract: With the widespread availability of pretrained Large Language Models (LLMs)
and their training datasets, concerns about the security risks associated with
their usage has increased significantly. One of these security risks is the
threat of LLM poisoning attacks where an attacker modifies some part of the LLM
training process to cause the LLM to behave in a malicious way. As an emerging
area of research, the current frameworks and terminology for LLM poisoning
attacks are derived from earlier classification poisoning literature and are
not fully equipped for generative LLM settings. We conduct a systematic review
of published LLM poisoning attacks to clarify the security implications and
address inconsistencies in terminology across the literature. We propose a
comprehensive poisoning threat model applicable to categorize a wide range of
LLM poisoning attacks. The poisoning threat model includes four poisoning
attack specifications that define the logistics and manipulation strategies of
an attack as well as six poisoning metrics used to measure key characteristics
of an attack. Under our proposed framework, we organize our discussion of
published LLM poisoning literature along four critical dimensions of LLM
poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and
poisons for unique tasks, to better understand the current landscape of
security risks.

</details>


### [278] [Breaking the Gaussian Barrier: Residual-PAC Privacy for Automatic Privatization](https://arxiv.org/abs/2506.06530)
*Tao Zhang,Yevgeniy Vorobeychik*

Main category: cs.CR

TL;DR: The paper introduces Residual PAC Privacy and Stackelberg Residual-PAC (SR-PAC) privatization mechanisms to improve privacy certification in data-driven systems.


<details>
  <summary>Details</summary>
Motivation: Existing PAC Privacy algorithms rely on a Gaussian mutual information upper bound, which is too conservative and not always tight.

Method: Introduce Residual PAC Privacy, an f-divergence-based measure, and Stackelberg Residual-PAC (SR-PAC) privatization mechanisms that use convex bilevel optimization to select optimal noise distributions.

Result: Achieves tight privacy budget utilization for arbitrary data distributions, naturally composes under repeated mechanisms, and provides provable privacy guarantees with higher statistical efficiency. Numerical experiments show improved utility compared to existing methods.

Conclusion: Residual PAC Privacy and SR-PAC privatization mechanisms offer a more efficient approach to certifying privacy in complex data-driven systems.

Abstract: The Probably Approximately Correct (PAC) Privacy framework [1] provides a
powerful instance-based methodology for certifying privacy in complex
data-driven systems. However, existing PAC Privacy algorithms rely on a
Gaussian mutual information upper bound. We show that this is in general too
conservative: the upper bound obtained by these algorithms is tight if and only
if the perturbed mechanism output is jointly Gaussian with independent Gaussian
noise. To address the inefficiency inherent in the Gaussian-based approach, we
introduce Residual PAC Privacy, an f-divergence-based measure that quantifies
the privacy remaining after adversarial inference. When instantiated with
Kullback-Leibler divergence, Residual-PAC Privacy is governed by conditional
entropy. Moreover, we propose Stackelberg Residual-PAC (SR-PAC) privatization
mechanisms for RPAC Privacy, a game-theoretic framework that selects optimal
noise distributions through convex bilevel optimization. Our approach achieves
tight privacy budget utilization for arbitrary data distributions. Moreover, it
naturally composes under repeated mechanisms and provides provable privacy
guarantees with higher statistical efficiency. Numerical experiments
demonstrate that SR-PAC certifies the target privacy budget while consistently
improving utility compared to existing methods.

</details>


### [279] [The complexity of the SupportMinors Modeling for the MinRank Problem](https://arxiv.org/abs/2506.06547)
*Daniel Cabarcas,Giulia Gaggero,Elisa Gorla*

Main category: cs.CR

TL;DR: 本文主要对SupportMinors建模的复杂性提供了证明的估计，验证了原始文章中的启发式复杂性估计。


<details>
  <summary>Details</summary>
Motivation: 为了对SupportMinors建模的复杂性提供更准确的评估，并验证原始文章中的复杂性估计是否正确。

Method: 通过分析和计算，得出SupportMinors建模复杂性的具体估计值。

Result: 提供了关于SupportMinors建模复杂性的证明估计，确认了原始文章中的复杂性估计。

Conclusion: SupportMinors建模的复杂性与原始文章中的启发式估计基本一致，复杂性得到了理论上的证明。

Abstract: In this note, we provide proven estimates for the complexity of the
SupportMinors Modeling, mostly confirming the heuristic complexity estimates
contained in the original article.

</details>


### [280] [Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security](https://arxiv.org/abs/2506.06565)
*Emilia Rivas,Sabrina Saika,Ahtesham Bakht,Aritran Piplai,Nathaniel D. Bastian,Ankit Shah*

Main category: cs.CR

TL;DR: The paper addresses the challenge of evolving attacks in Network Intrusion Detection Systems (NIDS) by designing an environment where two agents, red and blue, improve their policies over time.


<details>
  <summary>Details</summary>
Motivation: Evolving attacks are a critical challenge for the long-term success of NIDS. Traditional methods often fail to detect unknown attacks and require frequent updates.

Method: The paper designs an environment with two agents: the red agent perturbs packets to evade detection, while the blue agent learns new defensive policies using drift adaptation techniques. Both agents adapt iteratively.

Result: Experiments demonstrate that the blue agent can boost model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to 30 samples each.

Conclusion: By studying the learned policy of the model, the paper provides valuable insights into drift adaptation techniques with high utility.

Abstract: Evolving attacks are a critical challenge for the long-term success of
Network Intrusion Detection Systems (NIDS). The rise of these changing patterns
has exposed the limitations of traditional network security methods. While
signature-based methods are used to detect different types of attacks, they
often fail to detect unknown attacks. Moreover, the system requires frequent
updates with new signatures as the attackers are constantly changing their
tactics. In this paper, we design an environment where two agents improve their
policies over time. The adversarial agent, referred to as the red agent,
perturbs packets to evade the intrusion detection mechanism, whereas the blue
agent learns new defensive policies using drift adaptation techniques to
counter the attacks. Both agents adapt iteratively: the red agent responds to
the evolving NIDS, while the blue agent adjusts to emerging attack patterns. By
studying the model's learned policy, we offer concrete insights into drift
adaptation techniques with high utility. Experiments show that the blue agent
boosts model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to
30 samples each.

</details>


### [281] [Cyber Security of Sensor Systems for State Sequence Estimation: an AI Approach](https://arxiv.org/abs/2506.06572)
*Xubin Fang,Rick S. Blum,Ramesh Bharadwaj,Brian M. Sadler*

Main category: cs.CR

TL;DR: This paper presents methods to accurately identify and eliminate attacked sensor data in sequence estimation/regression algorithms, without assuming a known statistical model of the sensor data. The approach is effective against attackers with or without knowledge of the protection system.


<details>
  <summary>Details</summary>
Motivation: Sensor systems are vulnerable to attacks that can have devastating consequences, yet this area has not received sufficient attention.

Method: The paper develops a two-step method: 1) A simple protection approach for attackers without knowledge of the details; 2) Additional processing for cases where attackers have knowledge of the protection system. Both steps use only unattacked training data in their data-driven processing.

Result: Experimental results show that the simple approach performs as well as an approach that knows which sensors are attacked. With additional processing, the worst-case degradation under attacks can be significantly reduced compared to the simple approach, approaching the performance of knowing which sensors are attacked.

Conclusion: The developed methods effectively protect sequence estimation/regression algorithms from sensor data attacks, even under powerful attack models, and demonstrate advantages through both experimental and mathematical analysis.

Abstract: Sensor systems are extremely popular today and vulnerable to sensor data
attacks. Due to possible devastating consequences, counteracting sensor data
attacks is an extremely important topic, which has not seen sufficient study.
This paper develops the first methods that accurately identify/eliminate only
the problematic attacked sensor data presented to a sequence
estimation/regression algorithm under a powerful attack model constructed based
on known/observed attacks. The approach does not assume a known form for the
statistical model of the sensor data, allowing data-driven and machine learning
sequence estimation/regression algorithms to be protected. A simple protection
approach for attackers not endowed with knowledge of the details of our
protection approach is first developed, followed by additional processing for
attacks based on protection system knowledge. In the cases tested for which it
was designed, experimental results show that the simple approach achieves
performance indistinguishable, to two decimal places, from that for an approach
which knows which sensors are attacked. For cases where the attacker has
knowledge of the protection approach, experimental results indicate the
additional processing can be configured so that the worst-case degradation
under the additional processing and a large number of sensors attacked can be
made significantly smaller than the worst-case degradation of the simple
approach, and close to an approach which knows which sensors are attacked, for
the same number of attacked sensors with just a slight degradation under no
attacks. Mathematical descriptions of the worst-case attacks are used to
demonstrate the additional processing will provide similar advantages for cases
for which we do not have numerical results. All the data-driven processing used
in our approaches employ only unattacked training data.

</details>


### [282] [Stochastic Training for Side-Channel Resilient AI](https://arxiv.org/abs/2506.06597)
*Anuj Dubey,Aydin Aysu*

Main category: cs.CR

TL;DR: A new training method improves AI model security on edge devices against side-channel attacks without affecting performance or needing extra changes.


<details>
  <summary>Details</summary>
Motivation: To protect the confidentiality of AI models on edge devices from side-channel attacks based on power and electromagnetic emissions.

Method: Introduce a novel training methodology with randomized and interchangeable model configurations during inference to enhance resilience against side-channel threats.

Result: Experimental results on Google Coral Edge TPU indicate reduced side-channel leakage, slower increase in t-scores over 20,000 traces, and only about 1% accuracy degradation in most configurations.

Conclusion: This defense mechanism is effective, maintains high accuracy, and requires no additional hardware or software changes, making it suitable for existing Edge TPUs.

Abstract: The confidentiality of trained AI models on edge devices is at risk from
side-channel attacks exploiting power and electromagnetic emissions. This paper
proposes a novel training methodology to enhance resilience against such
threats by introducing randomized and interchangeable model configurations
during inference. Experimental results on Google Coral Edge TPU show a
reduction in side-channel leakage and a slower increase in t-scores over 20,000
traces, demonstrating robustness against adversarial observations. The defense
maintains high accuracy, with about 1% degradation in most configurations, and
requires no additional hardware or software changes, making it the only
applicable solution for existing Edge TPUs.

</details>


### [283] [Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans](https://arxiv.org/abs/2506.06604)
*Armin Sarabi,Manish Karir,Mingyan Liu*

Main category: cs.CR

TL;DR: The paper presents a study on using novel data types to perform cyber risk quantification by estimating the likelihood of a data breach, showing strong relationship between technology signatures and cybersecurity posture.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous approaches that relied on large-scale IP address based scanning data, which suffers from incomplete/missing IP address mappings as well as the lack of such data for SMEs.

Method: Building a highly accurate cyber risk assessment model using public and readily available technology signatures obtained from crawling an organization's website.

Result: Demonstrated feasibility of the model, highlighted key differences between ransomware attack victims and larger population of cyber incident and data breach victims.

Conclusion: Technology digital signature data is more readily available for millions of SMEs and has strong relationship with organizations' cybersecurity posture.

Abstract: In this paper we present a study on using novel data types to perform cyber
risk quantification by estimating the likelihood of a data breach. We
demonstrate that it is feasible to build a highly accurate cyber risk
assessment model using public and readily available technology signatures
obtained from crawling an organization's website. This approach overcomes the
limitations of previous similar approaches that relied on large-scale IP
address based scanning data, which suffers from incomplete/missing IP address
mappings as well as the lack of such data for large numbers of small and
medium-sized organizations (SMEs). In comparison to scan data, technology
digital signature data is more readily available for millions of SMEs. Our
study shows that there is a strong relationship between these technology
signatures and an organization's cybersecurity posture. In cross-validating our
model using different cyber incident datasets, we also highlight the key
differences between ransomware attack victims and the larger population of
cyber incident and data breach victims.

</details>


### [284] [TrustConnect: An In-Vehicle Anomaly Detection Framework through Topology-Based Trust Rating](https://arxiv.org/abs/2506.06635)
*Ayan Roy,Jeetkumar Patel,Rik Chakraborti,Shudip Datta*

Main category: cs.CR

TL;DR: In modern vehicles, interconnected in-vehicle components can be affected by false information leading to serious consequences. The paper proposes TrustConnect framework that evaluates the trustworthiness of in-vehicle networks by assessing individual component's trust levels considering their interdependency, value correlation and vulnerability to remote injection.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles' in-vehicle components interact with the external environment through remote communications and services. False or fabricated information could propagate throughout the network, affecting other components and potentially causing catastrophic consequences.

Method: Propose TrustConnect, a framework that assesses the trustworthiness of a vehicle's in-vehicle network by evaluating the trust levels of individual components under various network configurations based on their interdependency, value correlation and vulnerability to remote injection.

Result: The effectiveness of TrustConnect has been validated through programming simulations conducted across various scenarios using a random distribution of an in-vehicle network graph generated with the Networkx package in Python.

Conclusion: TrustConnect is an effective framework for assessing the reliability of the in-vehicle network.

Abstract: Modern vehicles are equipped with numerous in-vehicle components that
interact with the external environment through remote communications and
services, such as Bluetooth and vehicle-to-infrastructure communication. These
components form a network, exchanging information to ensure the proper
functioning of the vehicle. However, the presence of false or fabricated
information can disrupt the vehicle's performance. Given that these components
are interconnected, erroneous data can propagate throughout the network,
potentially affecting other components and leading to catastrophic
consequences. To address this issue, we propose TrustConnect, a framework
designed to assess the trustworthiness of a vehicle's in-vehicle network by
evaluating the trust levels of individual components under various network
configurations. The proposed framework leverages the interdependency of all the
vehicle's components, along with the correlation of their values and their
vulnerability to remote injection based on the outside exposure of each
component, to determine the reliability of the in-vehicle network. The
effectiveness of the proposed framework has been validated through programming
simulations conducted across various scenarios using a random distribution of
an in-vehicle network graph generated with the Networkx package in Python.

</details>


### [285] [Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning](https://arxiv.org/abs/2506.06730)
*Rabah Rahal,Abdelaziz Amara Korba,Yacine Ghamri-Doudane*

Main category: cs.CR

TL;DR: The paper proposes a new intrusion detection framework for EVSE that uses multimodal data sources and distributed learning, achieving high detection and precision rates.


<details>
  <summary>Details</summary>
Motivation: Electric vehicle supply equipment (EVSE) is crucial in smart grid infrastructure but faces significant cybersecurity challenges such as network reconnaissance, backdoor intrusions, and DDoS attacks. Current intrusion detection systems often fail to detect sophisticated and targeted attacks on EVSE infrastructure.

Method: The proposed framework leverages multimodal data sources, including network traffic and kernel events, to identify complex attack patterns. It employs a distributed learning approach with collaborative intelligence across EVSE stations while preserving data privacy through federated learning.

Result: Experimental results show that the proposed framework outperforms existing solutions, achieving a detection rate above 98% and a precision rate exceeding 97% in decentralized environments.

Conclusion: This solution addresses the evolving challenges of EVSE security by providing a scalable and privacy-preserving response to advanced cyber threats.

Abstract: The rapid global adoption of electric vehicles (EVs) has established electric
vehicle supply equipment (EVSE) as a critical component of smart grid
infrastructure. While essential for ensuring reliable energy delivery and
accessibility, EVSE systems face significant cybersecurity challenges,
including network reconnaissance, backdoor intrusions, and distributed
denial-of-service (DDoS) attacks. These emerging threats, driven by the
interconnected and autonomous nature of EVSE, require innovative and adaptive
security mechanisms that go beyond traditional intrusion detection systems
(IDS). Existing approaches, whether network-based or host-based, often fail to
detect sophisticated and targeted attacks specifically crafted to exploit new
vulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion
detection framework that leverages multimodal data sources, including network
traffic and kernel events, to identify complex attack patterns. The framework
employs a distributed learning approach, enabling collaborative intelligence
across EVSE stations while preserving data privacy through federated learning.
Experimental results demonstrate that the proposed framework outperforms
existing solutions, achieving a detection rate above 98% and a precision rate
exceeding 97% in decentralized environments. This solution addresses the
evolving challenges of EVSE security, offering a scalable and privacypreserving
response to advanced cyber threats

</details>


### [286] [Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions](https://arxiv.org/abs/2506.06735)
*Mesut Ozdag*

Main category: cs.CR

TL;DR: The paper explores AI-driven techniques for detecting vulnerabilities in smart contracts, including machine learning, deep learning, graph neural networks, and transformer-based models. It evaluates their strengths and weaknesses and highlights future research opportunities.


<details>
  <summary>Details</summary>
Motivation: Vulnerabilities in smart contracts have led to significant financial losses, while traditional auditing methods are limited in scalability, automation, and adaptability.

Method: The paper investigates AI-driven techniques such as machine learning, deep learning, graph neural networks, and transformer-based models for vulnerability detection in smart contracts. It analyzes how each technique represents code, processes semantic information, and addresses real-world vulnerability classes.

Result: The techniques vary in accuracy, interpretability, computational overhead, and real-time applicability. Each has its own strengths and weaknesses which are compared in the paper.

Conclusion: AI-driven techniques offer a promising approach for smart contract security but face challenges that need to be addressed. The paper outlines open challenges and suggests future research directions.

Abstract: Smart contracts, integral to blockchain ecosystems, enable decentralized
applications to execute predefined operations without intermediaries. Their
ability to enforce trustless interactions has made them a core component of
platforms such as Ethereum. Vulnerabilities such as numerical overflows,
reentrancy attacks, and improper access permissions have led to the loss of
millions of dollars throughout the blockchain and smart contract sector.
Traditional smart contract auditing techniques such as manual code reviews and
formal verification face limitations in scalability, automation, and
adaptability to evolving development patterns. As a result, AI-based solutions
have emerged as a promising alternative, offering the ability to learn complex
patterns, detect subtle flaws, and provide scalable security assurances. This
paper examines novel AI-driven techniques for vulnerability detection in smart
contracts, focusing on machine learning, deep learning, graph neural networks,
and transformer-based models. This paper analyzes how each technique represents
code, processes semantic information, and responds to real world vulnerability
classes. We also compare their strengths and weaknesses in terms of accuracy,
interpretability, computational overhead, and real time applicability. Lastly,
it highlights open challenges and future opportunities for advancing this
domain.

</details>


### [287] [LADSG: Label-Anonymized Distillation and Similar Gradient Substitution for Label Privacy in Vertical Federated Learning](https://arxiv.org/abs/2506.06742)
*Zeyu Yan,Yifei Yao,Xuanbing Wen,Juli Zhang,Kai Fan*

Main category: cs.CR

TL;DR: Vertical federated learning (VFL) is crucial for collaborative machine learning with privacy preservation. However, label inference attacks pose a significant threat by exploiting gradients and embeddings to reconstruct private labels. Existing defenses are insufficient against hybrid attacks. The paper proposes LADSG, a unified defense framework that mitigates gradient and label leakage while maintaining VFL's efficiency.


<details>
  <summary>Details</summary>
Motivation: Label inference attacks within the VFL system can bypass traditional security measures by exploiting gradients and semantic embeddings to reconstruct private labels, indicating a need for more comprehensive defenses.

Method: LADSG integrates gradient substitution, label anonymization, and anomaly detection to provide a unified defense mechanism against both gradient and label leakage in VFL.

Result: Experiments on six real-world datasets demonstrate that LADSG reduces label inference attack success rates by 30-60% with minimal computational overhead.

Conclusion: LADSG offers an effective solution to enhance the security of VFL systems against evolving label inference attacks, emphasizing the importance of lightweight defenses.

Abstract: Vertical federated learning (VFL) has become a key paradigm for collaborative
machine learning, enabling multiple parties to train models over distributed
feature spaces while preserving data privacy. Despite security protocols that
defend against external attacks - such as gradient masking and encryption,
which prevent unauthorized access to sensitive data - recent label inference
attacks from within the system have emerged. These attacks exploit gradients
and semantic embeddings to reconstruct private labels, bypassing traditional
defenses. For example, the passive label inference attack can reconstruct tens
of thousands of participants' private data using just 40 auxiliary labels,
posing a significant security threat. Existing defenses address single leakage
pathways, such as gradient leakage or label exposure. As attack strategies
evolve, their limitations become clear, especially against hybrid attacks that
combine multiple vectors. To address this, we propose Label-Anonymized Defense
with Substitution Gradient (LADSG), a unified defense framework that integrates
gradient substitution, label anonymization, and anomaly detection. LADSG
mitigates both gradient and label leakage while maintaining the scalability and
efficiency of VFL. Experiments on six real-world datasets show that LADSG
reduces label inference attack success rates by 30-60%, with minimal
computational overhead, underscoring the importance of lightweight defenses in
securing VFL.

</details>


### [288] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: An abstract of a paper proposing a rank-based uniformity test to verify the behavioral equality of black-box LLMs to authentic models, addressing issues like quantization, harmful fine-tuning, and model substitution.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of detecting substitutions in black-box large language models (LLMs) provided via APIs, which may be quantized or fine-tuned variants that degrade performance and compromise safety.

Method: A rank-based uniformity test is proposed to verify the behavioral equality of a black-box LLM to a locally deployed authentic model. The method is accurate, query-efficient, and avoids detectable query patterns.

Result: The approach consistently achieves superior statistical power over prior methods under constrained query budgets when evaluated across diverse threat scenarios including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution.

Conclusion: This rank-based uniformity test provides an effective solution for verifying black-box LLMs against authentic models, ensuring both performance and safety.

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [289] [ModelForge: Using GenAI to Improve the Development of Security Protocols](https://arxiv.org/abs/2506.07010)
*Martin Duclos,Ivan A. Fernandez,Kaneesha Moore,Sudip Mittal,Edward Zieglar*

Main category: cs.CR

TL;DR: ModelForge is a new tool that automates the translation of security protocol specifications into formal representations for CPSA by using NLP and GenAI, reducing manual effort and improving accessibility to formal analysis. Evaluation shows ModelForge performs well in syntactic accuracy but needs refinement for specific protocol details.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the complexity involved in manually translating natural language protocol specifications into formal representations, which can hinder the adoption of formal methods for verifying security protocols.

Method: The method involves introducing ModelForge, a tool that leverages advances in Natural Language Processing (NLP) and Generative AI (GenAI) to automate the translation process. This tool processes protocol specifications and generates a CPSA protocol definition. The performance of ModelForge is evaluated by fine-tuning a large language model (LLM) and comparing it with other popular LLMs.

Result: The results indicate that ModelForge consistently produces quality outputs with high syntactic accuracy. However, there is a need for some refinement to handle certain protocol details effectively.

Conclusion: This work contributes an architecture and proof of concept for a translating tool aimed at simplifying the use of formal methods in the development of security protocols, thereby promoting their adoption.

Abstract: Formal methods can be used for verifying security protocols, but their
adoption can be hindered by the complexity of translating natural language
protocol specifications into formal representations. In this paper, we
introduce ModelForge, a novel tool that automates the translation of protocol
specifications for the Cryptographic Protocol Shapes Analyzer (CPSA). By
leveraging advances in Natural Language Processing (NLP) and Generative AI
(GenAI), ModelForge processes protocol specifications and generates a CPSA
protocol definition. This approach reduces the manual effort required, making
formal analysis more accessible. We evaluate ModelForge by fine-tuning a large
language model (LLM) to generate protocol definitions for CPSA, comparing its
performance with other popular LLMs. The results from our evaluation show that
ModelForge consistently produces quality outputs, excelling in syntactic
accuracy, though some refinement is needed to handle certain protocol details.
The contributions of this work include the architecture and proof of concept
for a translating tool designed to simplify the adoption of formal methods in
the development of security protocols.

</details>


### [290] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: Emerging Large Reasoning Models (LRMs) have exceptional capabilities in mathematical and reasoning tasks, but their reasoning abilities expose new safety vulnerabilities. This paper introduces HauntAttack, a black-box attack framework that embeds harmful instructions into reasoning questions, revealing significant safety vulnerabilities in LRMs.


<details>
  <summary>Details</summary>
Motivation: To investigate the safety-reasoning trade-off in LRMs when reasoning is strongly entangled with harmfulness.

Method: Introduce HauntAttack, a novel black-box attack framework that systematically embeds harmful instructions into reasoning questions by substituting one of their original conditions with a harmful instruction, guiding the model toward generating unsafe outputs.

Result: Experiments on multiple LRMs reveal significant safety vulnerabilities even in the most advanced models. Detailed analysis provides insights into the security of LRMs.

Conclusion: LRMs exhibit significant safety vulnerabilities when exposed to the HauntAttack framework, raising concerns about their security.

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [291] [NanoZone: Scalable, Efficient, and Secure Memory Protection for Arm CCA](https://arxiv.org/abs/2506.07034)
*Shiqi Liu,Yongpeng Gao,Mingyang Zhang,Jie Wang*

Main category: cs.CR

TL;DR: The paper introduces an extension to Arm Confidential Computing Architecture (CCA) with a three-tier zone model enabling lightweight isolation domains within a single process, enhanced by a user-level Code-Pointer Integrity mechanism. Prototypes show resistance to intra-process and kernel-space attacks with a 20% performance overhead while retaining 95% throughput.


<details>
  <summary>Details</summary>
Motivation: Arm CCA isolates at the granularity of an entire Confidential Virtual Machine (CVM), which cannot mitigate intra-VM bugs like Heartbleed. Existing solutions narrow this to the process level but can't stop attacks within the same process or are either too slow or incompatible with CVM-style isolation.

Method: The authors extend CCA with a three-tier zone model that creates an unlimited number of lightweight isolation domains inside a single process, shielding them from kernel-space adversaries. They also add a fast user-level Code-Pointer Integrity (CPI) mechanism to block domain-switch abuse. Two prototypes were developed: one functional on Arm's official simulator and another for performance evaluation on Arm development boards.

Result: NanoZone incurs approximately a 20% performance overhead while maintaining 95% throughput compared to a system without fine-grained isolation. The prototypes successfully demonstrated resistance against intra-process and kernel-space adversaries in scenarios such as session-key isolation, in-memory key-value protection, and non-volatile-memory data isolation.

Conclusion: The extension to CCA with the three-tier zone model and CPI mechanism provides effective intra-process isolation while preserving performance, addressing limitations of current isolation approaches.

Abstract: Arm Confidential Computing Architecture (CCA) currently isolates at the
granularity of an entire Confidential Virtual Machine (CVM), leaving intra-VM
bugs such as Heartbleed unmitigated. The state-of-the-art narrows this to the
process level, yet still cannot stop attacks that pivot within the same
process, and prior intra-enclave schemes are either too slow or incompatible
with CVM-style isolation. We extend CCA with a three-tier zone model that
spawns an unlimited number of lightweight isolation domains inside a single
process, while shielding them from kernel-space adversaries. To block
domain-switch abuse, we also add a fast user-level Code-Pointer Integrity (CPI)
mechanism. We developed two prototypes: a functional version on Arm's official
simulator to validate resistance against intra-process and kernel-space
adversaries, and a performance variant on Arm development boards evaluated for
session-key isolation within server applications, in-memory key-value
protection, and non-volatile-memory data isolation. NanoZone incurs roughly a
20% performance overhead while retaining 95% throughput compared to the system
without fine-grained isolation.

</details>


### [292] [Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models](https://arxiv.org/abs/2506.07077)
*Qianshan Wei,Jiaqi Li,Zihan You,Yi Zhan,Kecen Li,Jialin Wu,Xinfeng Li Hengjun Liu,Yi Yu,Bin Cao,Yiwen Xu,Yang Liu,Guilin Qi*

Main category: cs.CR

TL;DR: The paper introduces Dual-Priv Pruning, a framework for applying Differential Privacy (DP) to Multimodal Large Language Models (MLLMs). It uses visual token pruning and gradient-update pruning to reduce computation overhead and model degradation. Experiments show competitive results with minimal performance loss and improved memory efficiency.


<details>
  <summary>Details</summary>
Motivation: Differential Privacy (DP) is effective in protecting privacy but introduces substantial computation overhead and model degradation when applied to complex architectures like MLLMs. The motivation is to address these challenges and effectively apply DP to MLLMs.

Method: Dual-Priv Pruning framework employs two complementary pruning mechanisms: (i) visual token pruning to reduce input dimensionality by removing redundant visual information, and (ii) gradient-update pruning during the DP optimization process to selectively prune parameter updates based on the magnitude of noisy gradients.

Result: Experiments demonstrate that Dual-Priv Pruning achieves competitive results with minimal performance degradation. It consistently utilizes less memory than standard DP-SGD and shows leading memory efficiency on H20 GPUs compared to zeroth-order methods.

Conclusion: Dual-Priv Pruning is an effective approach for DP fine-tuning in MLLMs, achieving a good balance between privacy and utility while improving computational efficiency.

Abstract: Differential Privacy (DP) is a widely adopted technique, valued for its
effectiveness in protecting the privacy of task-specific datasets, making it a
critical tool for large language models. However, its effectiveness in
Multimodal Large Language Models (MLLMs) remains uncertain. Applying
Differential Privacy (DP) inherently introduces substantial computation
overhead, a concern particularly relevant for MLLMs which process extensive
textual and visual data. Furthermore, a critical challenge of DP is that the
injected noise, necessary for privacy, scales with parameter dimensionality,
leading to pronounced model degradation; This trade-off between privacy and
utility complicates the application of Differential Privacy (DP) to complex
architectures like MLLMs. To address these, we propose Dual-Priv Pruning, a
framework that employs two complementary pruning mechanisms for DP fine-tuning
in MLLMs: (i) visual token pruning to reduce input dimensionality by removing
redundant visual information, and (ii) gradient-update pruning during the DP
optimization process. This second mechanism selectively prunes parameter
updates based on the magnitude of noisy gradients, aiming to mitigate noise
impact and improve utility. Experiments demonstrate that our approach achieves
competitive results with minimal performance degradation. In terms of
computational efficiency, our approach consistently utilizes less memory than
standard DP-SGD. While requiring only 1.74% more memory than zeroth-order
methods which suffer from severe performance issues on A100 GPUs, our method
demonstrates leading memory efficiency on H20 GPUs. To the best of our
knowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is
coming soon.

</details>


### [293] [Mind the Web: The Security of Web Use Agents](https://arxiv.org/abs/2506.07153)
*Avishag Shapira,Parth Atulbhai Gandhi,Edan Habler,Oleg Brodt,Asaf Shabtai*

Main category: cs.CR

TL;DR: Web-use agents with advanced browser capabilities introduce new attack vectors where malicious content embedded in web pages can exploit these agents to perform unauthorized actions, compromising confidentiality, integrity, and availability. Payloads tested on four popular agents achieved success rates of 80%-100%. Mitigation strategies are proposed.


<details>
  <summary>Details</summary>
Motivation: To expose the critical yet unexplored attack surface created by web-use agents' powerful capabilities and demonstrate how attackers can exploit these agents through task-aligned injection techniques.

Method: Demonstrated nine payload types that compromise agent security by embedding malicious content in web pages and framing commands as helpful guidance. Evaluated on four popular agents using multiple LLMs.

Result: Payloads successfully compromised confidentiality, integrity, and availability across agents with built-in safety mechanisms, achieving success rates of 80%-100%.

Conclusion: Proposed comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques to secure the development and deployment of web-use agents.

Abstract: Web-use agents are rapidly being deployed to automate complex web tasks,
operating with extensive browser capabilities including multi-tab navigation,
DOM manipulation, JavaScript execution and authenticated session access.
However, these powerful capabilities create a critical and previously
unexplored attack surface. This paper demonstrates how attackers can exploit
web-use agents' high-privilege capabilities by embedding malicious content in
web pages such as comments, reviews, or advertisements that agents encounter
during legitimate browsing tasks. In addition, we introduce the task-aligned
injection technique that frame malicious commands as helpful task guidance
rather than obvious attacks. This technique exploiting fundamental limitations
in LLMs' contextual reasoning: agents struggle in maintaining coherent
contextual awareness and fail to detect when seemingly helpful web content
contains steering attempts that deviate from their original task goal. Through
systematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do
Browser, OpenOperator), we demonstrate nine payload types that compromise
confidentiality, integrity, and availability, including unauthorized camera
activation, user impersonation, local file exfiltration, password leakage, and
denial of service, with validation across multiple LLMs achieving success rates
of 80%-100%. These payloads succeed across agents with built-in safety
mechanisms, requiring only the ability to post content on public websites,
creating unprecedented risks given the ease of exploitation combined with
agents' high-privilege access. To address this attack, we propose comprehensive
mitigation strategies including oversight mechanisms, execution constraints,
and task-aware reasoning techniques, providing practical directions for secure
development and deployment.

</details>


### [294] [A Simulation-based Evaluation Framework for Inter-VM RowHammer Mitigation Techniques](https://arxiv.org/abs/2506.07190)
*Hidemasa Kawasaki,Soramichi Akiyama*

Main category: cs.CR

TL;DR: An abstract about a simulation-based framework to evaluate software-based inter-VM RowHammer mitigation techniques across configurable DRAM address mappings.


<details>
  <summary>Details</summary>
Motivation: Inter-VM RowHammer is an attack that induces a bitflip beyond the boundaries of virtual machines (VMs) and some software-based techniques have been proposed to mitigate this attack. Evaluating these mitigation techniques requires confirmation that they actually mitigate inter-VM RowHammer in low overhead.

Method: The authors propose a simulation-based framework to evaluate software-based inter-VM RowHammer mitigation techniques across configurable DRAM address mappings.

Result: They demonstrate how to reproduce existing mitigation techniques on their framework, and show that it can evaluate the mitigation abilities and performance overhead of them with configurable DRAM address mappings.

Conclusion: This makes comprehensive evaluation prohibitively costly or even implausible as no machine that has a specific DRAM address mapping might be available.

Abstract: Inter-VM RowHammer is an attack that induces a bitflip beyond the boundaries
of virtual machines (VMs) to compromise a VM from another, and some
software-based techniques have been proposed to mitigate this attack.
Evaluating these mitigation techniques requires to confirm that they actually
mitigate inter-VM RowHammer in low overhead. A challenge in this evaluation
process is that both the mitigation ability and the overhead depend on the
underlying hardware whose DRAM address mappings are different from machine to
machine. This makes comprehensive evaluation prohibitively costly or even
implausible as no machine that has a specific DRAM address mapping might be
available. To tackle this challenge, we propose a simulation-based framework to
evaluate software-based inter-VM RowHammer mitigation techniques across
configurable DRAM address mappings. We demonstrate how to reproduce existing
mitigation techniques on our framework, and show that it can evaluate the
mitigation abilities and performance overhead of them with configurable DRAM
address mappings.

</details>


### [295] [Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless Agent Actions](https://arxiv.org/abs/2506.07200)
*Kanato Nakanishi,Soramichi Akiyama*

Main category: cs.CR

TL;DR: 提出了一种通过惩罚无用动作以提高基于强化学习的缓存结构漏洞探索效率的方法，实验表明该方法显著减少了训练时间和无用动作。


<details>
  <summary>Details</summary>
Motivation: 缓存时序攻击利用微体系结构特性泄漏敏感数据，对现代系统构成严重威胁。然而，分析给定缓存结构对缓存时序攻击的脆弱性具有挑战性。现有的基于强化学习的方法在探索过程中存在效率低下的问题，因为智能体可能会执行一些无助于探索的动作。

Method: 提出一种方法，在训练过程中识别这些无用动作并对它们进行惩罚，从而使智能体避免这些动作，提高探索效率。

Result: 在17种缓存结构上的实验表明，所提出的训练机制将无用动作的数量减少了多达43.08%，基础情况下的训练时间减少了28%，几何平均情况下的训练时间减少了4.84%。

Conclusion: 所提出的方法能够有效减少无用动作并提高基于强化学习的缓存结构漏洞探索的效率。

Abstract: Cache-timing attacks exploit microarchitectural characteristics to leak
sensitive data, posing a severe threat to modern systems. Despite its severity,
analyzing the vulnerability of a given cache structure against cache-timing
attacks is challenging. To this end, a method based on Reinforcement Learning
(RL) has been proposed to automatically explore vulnerabilities for a given
cache structure. However, a naive RL-based approach suffers from inefficiencies
due to the agent performing actions that do not contribute to the exploration.
In this paper, we propose a method to identify these useless actions during
training and penalize them so that the agent avoids them and the exploration
efficiency is improved. Experiments on 17 cache structures show that our
training mechanism reduces the number of useless actions by up to 43.08%. This
resulted in the reduction of training time by 28\% in the base case and 4.84\%
in the geomean compared to a naive RL-based approach.

</details>


### [296] [Exploiting Inaccurate Branch History in Side-Channel Attacks](https://arxiv.org/abs/2506.07263)
*Yuhui Zhu,Alessandro Biondi*

Main category: cs.CR

TL;DR: 现代乱序CPU依赖推测执行进行性能优化，分支预测作为减少停顿和最大化效率的核心。本文分析了现代分支预测单元（BPU）的基本组件，以及资源共享和竞争如何影响两个广泛实现但记录不足的功能：无偏分支预测和分支历史推测。研究发现这些功能可能引入安全风险，展示了它们如何修改分支历史缓冲区更新行为并创建恶意误推测的新原语。基于此发现，文章提出了三种新型攻击原语，并演示了在多个处理器上的利用方法。最后介绍了Chimera，一种基于eBPF的攻击演示器，能够以24,628 bit/s的速度泄露内核内存内容。


<details>
  <summary>Details</summary>
Motivation: 分支预测是现代CPU性能优化的重要组成部分，然而当共享分支预测资源缺乏适当的隔离和净化方法时，可能会引发跨软件上下文的安全漏洞。因此，研究分支预测单元的功能及其潜在安全风险具有重要意义。

Method: 本文分析了现代BPU的基本组件，并研究了资源共享和竞争对无偏分支预测和分支历史推测的影响。通过实验展示了这些功能如何修改分支历史缓冲区更新行为，并创建新的恶意误推测原语。此外，还识别了相应的易受攻击控制流模式，并在多个处理器上进行了利用演示。

Result: 发现了未知的跨权限攻击面，提出了三种新型攻击原语：Spectre-BSE、Spectre-BHS和BiasScope。并在多个处理器上成功演示了攻击可行性。最后，基于eBPF的Chimera攻击演示器实现了以24,628 bit/s的速度泄露内核内存内容。

Conclusion: 本文揭示了现代BPU中无偏分支预测和分支历史推测功能可能带来的安全风险，提出了三种新型攻击原语，并在实际环境中验证了其可行性。研究结果强调了改进分支预测资源隔离和净化方法的重要性，以防止敏感数据泄露和跨权限攻击。

Abstract: Modern out-of-order CPUs heavily rely on speculative execution for
performance optimization, with branch prediction serving as a cornerstone to
minimize stalls and maximize efficiency. Whenever shared branch prediction
resources lack proper isolation and sanitization methods, they may originate
security vulnerabilities that expose sensitive data across different software
contexts.
  This paper examines the fundamental components of modern Branch Prediction
Units (BPUs) and investigates how resource sharing and contention affect two
widely implemented but underdocumented features: Bias-Free Branch Prediction
and Branch History Speculation. Our analysis demonstrates that these BPU
features, while designed to enhance speculative execution efficiency through
more accurate branch histories, can also introduce significant security risks.
We show that these features can inadvertently modify the Branch History Buffer
(BHB) update behavior and create new primitives that trigger malicious
mis-speculations.
  This discovery exposes previously unknown cross-privilege attack surfaces for
Branch History Injection (BHI). Based on these findings, we present three novel
attack primitives: two Spectre attacks, namely Spectre-BSE and Spectre-BHS, and
a cross-privilege control flow side-channel attack called BiasScope. Our
research identifies corresponding patterns of vulnerable control flows and
demonstrates exploitation on multiple processors. Finally, Chimera is
presented: an attack demonstrator based on eBPF for a variant of Spectre-BHS
that is capable of leaking kernel memory contents at 24,628 bit/s.

</details>


### [297] [SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code Generation with Agentic Workflows](https://arxiv.org/abs/2506.07313)
*Rebecca Saul,Hao Wang,Koushik Sen,David Wagner*

Main category: cs.CR

TL;DR: SCGAgent is a secure coding agent that enhances security in code generated by large language models while preserving functionality.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) are successful in generating functional code but often overlook security, potentially producing exploitable vulnerabilities.

Method: The researchers use security coding guidelines and LLM-generated unit tests to ensure safe programming practices. They introduce SCGAgent, which employs these techniques for secure code generation.

Result: SCGAgent preserves nearly 98% of the base LLM's functionality while improving security by about 25%. It also matches or outperforms reasoning-based LLMs using a non-reasoning model and an agentic workflow.

Conclusion: SCGAgent successfully balances security enhancements with maintaining code functionality, offering a promising solution for secure code generation.

Abstract: Large language models (LLMs) have seen widespread success in code generation
tasks for different scenarios, both everyday and professional. However current
LLMs, despite producing functional code, do not prioritize security and may
generate code with exploitable vulnerabilities. In this work, we propose
techniques for generating code that is more likely to be secure and introduce
SCGAgent, a proactive secure coding agent that implements our techniques. We
use security coding guidelines that articulate safe programming practices,
combined with LLM-generated unit tests to preserve functional correctness. In
our evaluation, we find that SCGAgent is able to preserve nearly 98% of the
functionality of the base Sonnet-3.7 LLM while achieving an approximately 25%
improvement in security. Moreover, SCGAgent is able to match or best the
performance of sophisticated reasoning LLMs using a non-reasoning model and an
agentic workflow.

</details>


### [298] [Enhanced Consistency Bi-directional GAN(CBiGAN) for Malware Anomaly Detection](https://arxiv.org/abs/2506.07372)
*Thesath Wijayasiri,Kar Wai Fok,Vrizlynn L. L. Thing*

Main category: cs.CR

TL;DR: Static analysis in cybersecurity is crucial for malware detection without execution. However, traditional methods suffer from biased datasets. This paper proposes using binary file content transformed into images as inputs to deep learning models for enhanced malware detection. Specifically, it introduces the application of CBiGAN with its superior latent space mapping capabilities for anomaly detection based on reconstruction error. Evaluated on diverse PE and OLE files, including self-collected malicious executables from 214 malware families, the CBiGAN shows robustness and good generalizability with high AUC results.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis techniques rely on biased or outdated datasets, which leads to gaps in detecting emerging malware threats. The study aims to improve detection by focusing on binary content as key features.

Method: The binary content of files is transformed into images and used as input to deep learning models. The CBiGAN is leveraged for its latent space mapping capabilities, allowing for effective modeling of complex malware patterns through a reconstruction error-based anomaly detection method.

Result: The CBiGAN achieved high AUC results and demonstrated good generalizability in distinguishing between benign and malicious files across diverse datasets, including PE and OLE files.

Conclusion: The innovative use of CBiGAN for malware anomaly detection proves robust and effective, offering a promising advancement in static malware analysis.

Abstract: Static analysis, a cornerstone technique in cybersecurity, offers a
noninvasive method for detecting malware by analyzing dormant software without
executing potentially harmful code. However, traditional static analysis often
relies on biased or outdated datasets, leading to gaps in detection
capabilities against emerging malware threats. To address this, our study
focuses on the binary content of files as key features for malware detection.
These binary contents are transformed and represented as images, which then
serve as inputs to deep learning models. This method takes into account the
visual patterns within the binary data, allowing the model to analyze potential
malware effectively. This paper introduces the application of the CBiGAN in the
domain of malware anomaly detection. Our approach leverages the CBiGAN for its
superior latent space mapping capabilities, critical for modeling complex
malware patterns by utilizing a reconstruction error-based anomaly detection
method. We utilized several datasets including both portable executable (PE)
files as well as Object Linking and Embedding (OLE) files. We then evaluated
our model against a diverse set of both PE and OLE files, including
self-collected malicious executables from 214 malware families. Our findings
demonstrate the robustness of this innovative approach, with the CBiGAN
achieving high Area Under the Curve (AUC) results with good generalizability,
thereby confirming its capability to distinguish between benign and diverse
malicious files with reasonably high accuracy.

</details>


### [299] [From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks](https://arxiv.org/abs/2506.07392)
*Yuyang Zhou,Guang Cheng,Kang Du,Zihan Chen,Tian Qin,Yuyu Zhao*

Main category: cs.CR

TL;DR: The paper proposes a FMADRL-driven MTD framework to mitigate DoS threats in UAV swarm networks using three mechanisms: leader switching, route mutation, and frequency hopping. It formulates the defense problem as a POMDP and uses a policy gradient-based algorithm for distributed learning.


<details>
  <summary>Details</summary>
Motivation: UAV networks are vulnerable to DoS threats due to their open wireless environment, dynamic topology, and resource constraints. Traditional defense mechanisms are inadequate for such scenarios.

Method: A novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework is proposed. The defense problem is formulated as a multi-agent partially observable Markov decision process (POMDP). Each UAV has a local policy agent that selects MTD actions based on partial observations and local experiences. A policy gradient-based FMADRL algorithm is used for collaborative optimization of defense policies.

Result: Simulations show significant improvements over state-of-the-art baselines: 34.6% improvement in attack mitigation rate, 94.6% reduction in average recovery time, and decreases in energy consumption and defense cost by 29.3% and 98.3%, respectively.

Conclusion: The FMADRL-driven MTD framework effectively enhances network resilience against DoS attacks in UAV swarm networks while maintaining robust mission continuity.

Abstract: The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide
range of mission-critical applications, but also exposes UAV networks to severe
Denial-of-Service (DoS) threats due to their open wireless environment, dynamic
topology, and resource constraints. Traditional static or centralized defense
mechanisms are often inadequate for such dynamic and distributed scenarios. To
address these challenges, we propose a novel federated multi-agent deep
reinforcement learning (FMADRL)-driven moving target defense (MTD) framework
for proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,
we design three lightweight and coordinated MTD mechanisms, including leader
switching, route mutation, and frequency hopping, that leverage the inherent
flexibility of UAV swarms to disrupt attacker efforts and enhance network
resilience. The defense problem is formulated as a multi-agent partially
observable Markov decision process (POMDP), capturing the distributed,
resource-constrained, and uncertain nature of UAV swarms under attack. Each UAV
is equipped with a local policy agent that autonomously selects MTD actions
based on partial observations and local experiences. By employing a policy
gradient-based FMADRL algorithm, UAVs collaboratively optimize their defense
policies via reward-weighted aggregation, enabling distributed learning without
sharing raw data and thus reducing communication overhead. Extensive
simulations demonstrate that our approach significantly outperforms
state-of-the-art baselines, achieving up to a 34.6% improvement in attack
mitigation rate, a reduction in average recovery time of up to 94.6%, and
decreases in energy consumption and defense cost by as much as 29.3% and 98.3%,
respectively, while maintaining robust mission continuity under various DoS
attack strategies.

</details>


### [300] [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
*Yukai Zhou,Sibei Yang,Wenjie Wang*

Main category: cs.CR

TL;DR: Large language models (LLMs) have a security risk of incorrectly answering seemingly harmless inputs, which can cause real-world harm. This paper proposes JailFlipBench to capture this implicit harm and conducts evaluations across multiple LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the overlooked risk in large language models where they may produce dangerous outputs even when responding to harmless-looking inputs, thus causing real-world harm.

Method: The authors propose JailFlipBench, a benchmark for capturing implicit harm in LLMs. They also develop initial JailFlip attack methodologies and conduct comprehensive evaluations across various types of LLMs.

Result: The results indicate that implicit harm presents immediate and urgent real-world risks in LLMs.

Conclusion: The conclusion is that there is a need for broader safety assessments and alignment in LLMs beyond conventional jailbreak paradigms.

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about their security. While jailbreak attacks
highlight failures under overtly harmful queries, they overlook a critical
risk: incorrectly answering harmless-looking inputs can be dangerous and cause
real-world harm (Implicit Harm). We systematically reformulate the LLM risk
landscape through a structured quadrant perspective based on output factuality
and input harmlessness, uncovering an overlooked high-risk region. To
investigate this gap, we propose JailFlipBench, a benchmark aims to capture
implicit harm, spanning single-modal, multimodal, and factual extension
scenarios with diverse evaluation metrics. We further develop initial JailFlip
attack methodologies and conduct comprehensive evaluations across multiple
open-source and black-box LLMs, show that implicit harm present immediate and
urgent real-world risks, calling for broader LLM safety assessments and
alignment beyond conventional jailbreak paradigms.

</details>


### [301] [Enhancing Watermarking Quality for LLMs via Contextual Generation States Awareness](https://arxiv.org/abs/2506.07403)
*Peiru Yang,Xintian Li,Wanchun Ni,Jinhua Yin,Huili Wang,Guoshun Nan,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CR

TL;DR: 近期研究提出了一种即插即用的上下文生成状态感知水印框架（CAW），通过动态调整水印嵌入过程，在提高AI生成文本检测率的同时保证生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前的文本水印技术通常干扰大语言模型生成过程以嵌入信号，但依赖启发式规则可能导致次优的标记选择和生成内容质量下降。

Method: 引入了CAW框架，包含水印容量评估器和多分支预生成机制。水印容量评估器分析上下文生成状态，评估不同标记位置嵌入消息的影响；多分支预生成机制避免由水印策略引起的延迟；基于评估结果动态调整水印过程，减少内容质量的潜在退化。

Result: 在多个领域数据集上的广泛实验验证了该方法的有效性，相较于各种基线方法，在检测率和生成质量方面表现出优越性能。

Conclusion: CAW框架能够与现有水印方法无缝集成，提升生成质量，同时保持高检测率。

Abstract: Recent advancements in watermarking techniques have enabled the embedding of
secret messages into AI-generated text (AIGT), serving as an important
mechanism for AIGT detection. Existing methods typically interfere with the
generation processes of large language models (LLMs) to embed signals within
the generated text. However, these methods often rely on heuristic rules, which
can result in suboptimal token selection and a subsequent decline in the
quality of the generated content. In this paper, we introduce a plug-and-play
contextual generation states-aware watermarking framework (CAW) that
dynamically adjusts the embedding process. It can be seamlessly integrated with
various existing watermarking methods to enhance generation quality. First, CAW
incorporates a watermarking capacity evaluator, which can assess the impact of
embedding messages at different token positions by analyzing the contextual
generation states. Furthermore, we introduce a multi-branch pre-generation
mechanism to avoid the latency caused by the proposed watermarking strategy.
Building on this, CAW can dynamically adjust the watermarking process based on
the evaluated watermark capacity of each token, thereby minimizing potential
degradation in content quality. Extensive experiments conducted on datasets
across multiple domains have verified the effectiveness of our method,
demonstrating superior performance compared to various baselines in terms of
both detection rate and generation quality.

</details>


### [302] [Pixel-Sensitive and Robust Steganography Based on Polar Codes](https://arxiv.org/abs/2506.07404)
*Yujun Ji,Jinsheng Li,Ling Liu,Qi Cao,Tao Dai*

Main category: cs.CR

TL;DR: This paper proposes a pixel-sensitive and robust steganographic scheme based on polar codes to solve the rate-distortion coding problem in steganography design, which can resist sophisticated noise attacks and achieve the embedding capacity in certain cases.


<details>
  <summary>Details</summary>
Motivation: The core issue in steganography design is the rate-distortion coding problem, and existing steganographic coding methods fail to resist attack scenarios where each noise element can have different intensities in adaptive steganography.

Method: Utilize polar codes to design a steganographic scheme that not only matches the adaptive distortion well but is also robust against sophisticated noise attacks.

Result: Experimentally, a steganographic scheme can be designed and implemented with a secret message error rate at the $10^{-5}$ level when the attack noise is known to both the sender and the receiver.

Conclusion: The proposed pixel-sensitive and robust steganographic scheme based on polar codes can resist sophisticated noise attacks and achieve the embedding capacity in certain cases.

Abstract: Steganography is an information hiding technique for covert communication.
The core issue in steganography design is the rate-distortion coding problem.
Polar codes, which have been proven to achieve the rate-distortion bound for
any binary symmetric source, are utilized to design a steganographic scheme
that can reach the embedding capacity for the Distortion-Limited Sender problem
in certain cases. In adaptive steganography, for attack scenarios where each
noise element can have different intensities, existing steganographic coding
methods fail to resist such attacks. In this paper, we propose a
pixel-sensitive and robust steganographic scheme based on polar codes. Our
steganographic scheme not only matches the adaptive distortion well but is also
robust against sophisticated noise attacks. Futher, it is proven that our
scheme achieves the embedding capacity in certain cases. Experimentally, a
steganographic scheme can be designed and implemented with a secret message
error rate at the $10^{-5}$ level when the attack noise is known to both the
sender and the receiver. This demonstrates its significant robustness.

</details>


### [303] [Explainable AI for Enhancing IDS Against Advanced Persistent Kill Chain](https://arxiv.org/abs/2506.07480)
*Bassam Noori Shaker,Bahaa Al-Musawi,Mohammed Falih Hassan*

Main category: cs.CR

TL;DR: The paper proposes a feature selection and classification model integrating SHAP and XGBoost for detecting APTs at various phases with improved performance and reduced complexity.


<details>
  <summary>Details</summary>
Motivation: Advanced Persistent Threats (APTs) pose significant challenges in cybersecurity, necessitating the development of effective Intrusion Detection Systems (IDS). The need for feature selection to enhance IDS performance by focusing on relevant features is crucial.

Method: The method involves using SHapley Additive exPlanations (SHAP) and Extreme Gradient Boosting (XGBoost) for feature selection and classification. It identifies a minimal set of influential features for detecting APTs at different phases independently.

Result: The approach achieved macro-average F1-score and recall of 94% and 93%, respectively, while reducing model complexity by selecting only 12 features from an original set of 77.

Conclusion: The proposed feature selection and classification model demonstrates superior performance compared to standard techniques, effectively detecting APTs with fewer features.

Abstract: Advanced Persistent Threats (APTs) represent a sophisticated and persistent
cy-bersecurity challenge, characterized by stealthy, multi-phase, and targeted
attacks aimed at compromising information systems over an extended period.
Develop-ing an effective Intrusion Detection System (IDS) capable of detecting
APTs at different phases relies on selecting network traffic features. However,
not all of these features are directly related to the phases of APTs. Some
network traffic features may be unrelated or have limited relevance to
identifying malicious ac-tivity. Therefore, it is important to carefully select
and analyze the most relevant features to improve the IDS performance. This
work proposes a feature selection and classification model that integrates two
prominent machine learning algo-rithms: SHapley Additive exPlanations (SHAP)
and Extreme Gradient Boosting (XGBoost). The aim is to develop lightweight IDS
based on a selected minimum number of influential features for detecting APTs
at various phases. The pro-posed method also specifies the relevant features
for each phase of APTs inde-pendently. Extensive experimental results on the
SCVIC-APT-2021 dataset indi-cated that our proposed approach has improved
performance compared to other standard techniques. Specifically, both the
macro-average F1-score and recall reached 94% and 93 %, respectively, while
reducing the complexity of the detec-tion model by selecting only 12 features
out of 77.

</details>


### [304] [MalGEN: A Generative Agent Framework for Modeling Malicious Software in Cybersecurity](https://arxiv.org/abs/2506.07586)
*Bikash Saha,Sandeep Kumar Shukla*

Main category: cs.CR

TL;DR: The paper introduces MalGEN, a multi-agent framework that simulates adversarial behavior to generate diverse malware samples for testing and improving cybersecurity defenses. Using LLMs' potential for misuse as an advantage, it creates novel malware samples that bypass current defenses, thus helping evaluate and strengthen security systems.


<details>
  <summary>Details</summary>
Motivation: To address the lack of controlled and extensible tools in the research community that can simulate adversarial behavior using Large Language Models (LLMs) for ethical and defensive research.

Method: MalGEN is a multi-agent framework that simulates coordinated adversarial behavior to generate diverse, activity-driven malware samples. It emulates attacker workflows including payload planning, capability selection, and evasion strategies within a controlled environment designed for ethical and defensive research.

Result: Using MalGEN, ten novel malware samples were synthesized and evaluated against leading antivirus and behavioral detection engines. Several samples successfully bypassed current defenses, demonstrating stealthy and evasive characteristics.

Conclusion: MalGEN transforms the threat of LLM misuse into an opportunity for proactive defense by offering a valuable framework for evaluating and strengthening cybersecurity systems. It addresses data scarcity, enables rigorous testing, and supports the development of resilient and future-ready detection strategies.

Abstract: The dual use nature of Large Language Models (LLMs) presents a growing
challenge in cybersecurity. While LLM enhances automation and reasoning for
defenders, they also introduce new risks, particularly their potential to be
misused for generating evasive, AI crafted malware. Despite this emerging
threat, the research community currently lacks controlled and extensible tools
that can simulate such behavior for testing and defense preparation. We present
MalGEN, a multi agent framework that simulates coordinated adversarial behavior
to generate diverse, activity driven malware samples. The agents work
collaboratively to emulate attacker workflows, including payload planning,
capability selection, and evasion strategies, within a controlled environment
built for ethical and defensive research. Using MalGEN, we synthesized ten
novel malware samples and evaluated them against leading antivirus and
behavioral detection engines. Several samples exhibited stealthy and evasive
characteristics that bypassed current defenses, validating MalGEN's ability to
model sophisticated and new threats. By transforming the threat of LLM misuse
into an opportunity for proactive defense, MalGEN offers a valuable framework
for evaluating and strengthening cybersecurity systems. The framework addresses
data scarcity, enables rigorous testing, and supports the development of
resilient and future ready detection strategies.

</details>


### [305] [TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems](https://arxiv.org/abs/2506.07605)
*Marco Di Gennaro,Giovanni De Lucia,Stefano Longari,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: In this paper, researchers introduce TimberStrike, an optimization-based dataset reconstruction attack on horizontally federated tree-based models. They demonstrate its effectiveness across various frameworks and datasets, reconstructing up to 95.63% of the target data. Differential Privacy is found to only partially mitigate the attack while harming model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work is the lack of research on security and privacy implications of tree-based models in Federated Learning, despite extensive studies on neural networks. The authors aim to explore vulnerabilities in federated tree-based models.

Method: The method involves introducing TimberStrike, a dataset reconstruction attack that exploits split values and decision paths in tree-based models to infer sensitive training data from other clients. This is performed by a single client in a horizontally federated learning setting.

Result: TimberStrike successfully reconstructs between 73.05% and 95.63% of the target dataset across different implementations. Differential Privacy offers partial mitigation but significantly reduces model performance.

Conclusion: The conclusion emphasizes the need for developing privacy-preserving mechanisms specifically tailored for tree-based Federated Learning systems, with some initial insights provided.

Abstract: Federated Learning has emerged as a privacy-oriented alternative to
centralized Machine Learning, enabling collaborative model training without
direct data sharing. While extensively studied for neural networks, the
security and privacy implications of tree-based models remain underexplored.
This work introduces TimberStrike, an optimization-based dataset reconstruction
attack targeting horizontally federated tree-based models. Our attack, carried
out by a single client, exploits the discrete nature of decision trees by using
split values and decision paths to infer sensitive training data from other
clients. We evaluate TimberStrike on State-of-the-Art federated gradient
boosting implementations across multiple frameworks, including Flower, NVFlare,
and FedTree, demonstrating their vulnerability to privacy breaches. On a
publicly available stroke prediction dataset, TimberStrike consistently
reconstructs between 73.05% and 95.63% of the target dataset across all
implementations. We further analyze Differential Privacy, showing that while it
partially mitigates the attack, it also significantly degrades model
performance. Our findings highlight the need for privacy-preserving mechanisms
specifically designed for tree-based Federated Learning systems, and we provide
preliminary insights into their design.

</details>


### [306] [Profiling Electric Vehicles via Early Charging Voltage Patterns](https://arxiv.org/abs/2506.07714)
*Francesco Marchiori,Denis Donadel,Alessandro Brighente,Mauro Conti*

Main category: cs.CR

TL;DR: 提出了一种基于充电早期阶段物理测量的独特EV识别框架，可更快更可靠地进行车辆身份验证，但存在隐私风险。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖充电后期的电流特征进行身份验证，容易遭受中继攻击，且无法及时检测恶意行为。同时，通过充电模式识别EV可能引发隐私问题。

Method: 利用充电早期阶段的电压行为特征来唯一识别电动车，假设早期电压行为与后期电流行为具有相似特性。从早期电压测量中提取特征以实现EV配置文件的构建，并使用轻量级模型进行验证。

Result: 在49辆EV的7408次可用充电数据集上测试，准确率可达0.86。仅需10个关键特征即可达到接近最优性能，提高了效率。

Conclusion: 该研究为新型身份验证因素奠定了基础，但也揭示了未经授权访问充电数据可能导致的隐私风险。

Abstract: Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable
alternative to fuel-powered vehicles, making secure charging infrastructure
essential. Despite traditional authentication protocols, recent results showed
that attackers may steal energy through tailored relay attacks. One
countermeasure is leveraging the EV's fingerprint on the current exchanged
during charging. However, existing methods focus on the final charging stage,
allowing malicious actors to consume substantial energy before being detected
and repudiated. This underscores the need for earlier and more effective
authentication methods to prevent unauthorized charging. Meanwhile, profiling
raises privacy concerns, as uniquely identifying EVs through charging patterns
could enable user tracking.
  In this paper, we propose a framework for uniquely identifying EVs using
physical measurements from the early charging stages. We hypothesize that
voltage behavior early in the process exhibits similar characteristics to
current behavior in later stages. By extracting features from early voltage
measurements, we demonstrate the feasibility of EV profiling. Our approach
improves existing methods by enabling faster and more reliable vehicle
identification. We test our solution on a dataset of 7408 usable charges from
49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that
near-optimal performance is possible with just 10 key features, improving
efficiency alongside our lightweight models. This research lays the foundation
for a novel authentication factor while exposing potential privacy risks from
unauthorized access to charging data.

</details>


### [307] ["I wasn't sure if this is indeed a security risk": Data-driven Understanding of Security Issue Reporting in GitHub Repositories of Open Source npm Packages](https://arxiv.org/abs/2506.07728)
*Rajdeep Ghosh,Shiladitya De,Mainack Mondal*

Main category: cs.CR

TL;DR: The paper investigates security-related issue reporting in the npm ecosystem by analyzing a large dataset of GitHub issues from npm packages. It finds that only 0.13% of issues are tagged as security-related, but through manual analysis and machine learning models, identifies 14.8% of all issues as security-related. Bots currently in use may not be adequate for detecting or assisting with these issues, and many user-reported security issues may go unaddressed by developers. The authors suggest improvements in security management in open-source ecosystems through smarter tools and better collaboration.


<details>
  <summary>Details</summary>
Motivation: To understand the reality of security-related issue reporting in the npm ecosystem and the challenges associated with it, which has been less explored at scale.

Method: Collecting and analyzing 10,907,467 issues reported across GitHub repositories of 45,466 diverse npm packages. Using manual analysis followed by developing high accuracy machine learning models to identify security-related issues and comments. Analyzing user-developer interaction data and performing correlation analysis.

Result: Identified 1,617,738 security-related issues (14.8% of all issues) and 4,461,934 comments on these issues that were not originally tagged as security-related. Found that bots may not be sufficient for detection or assistance. Discovered that many user-reported security issues might not be addressed by developers and those without known solutions might remain unresolved.

Conclusion: The findings provide actionable insights for improving security management in open-source ecosystems, emphasizing the need for smarter tools and better collaboration.

Abstract: The npm (Node Package Manager) ecosystem is the most important package
manager for JavaScript development with millions of users. Consequently, a
plethora of earlier work investigated how vulnerability reporting, patch
propagation, and in general detection as well as resolution of security issues
in such ecosystems can be facilitated. However, understanding the ground
reality of security-related issue reporting by users (and bots) in npm-along
with the associated challenges has been relatively less explored at scale.
  In this work, we bridge this gap by collecting 10,907,467 issues reported
across GitHub repositories of 45,466 diverse npm packages. We found that the
tags associated with these issues indicate the existence of only 0.13%
security-related issues. However, our approach of manual analysis followed by
developing high accuracy machine learning models identify 1,617,738
security-related issues which are not tagged as security-related (14.8% of all
issues) as well as 4,461,934 comments made on these issues. We found that the
bots which are in wide use today might not be sufficient for either detecting
or offering assistance. Furthermore, our analysis of user-developer interaction
data hints that many user-reported security issues might not be addressed by
developers-they are not tagged as security-related issues and might be closed
without valid justification. Consequently, a correlation analysis hints that
the developers quickly handle security issues with known solutions (e.g.,
corresponding to CVE). However, security issues without such known solutions
(even with reproducible code) might not be resolved. Our findings offer
actionable insights for improving security management in open-source
ecosystems, highlighting the need for smarter tools and better collaboration.
The data and code for this work is available at
https://doi.org/10.5281/zenodo.15614029

</details>


### [308] [User-space library rootkits revisited: Are user-space detection mechanisms futile?](https://arxiv.org/abs/2506.07827)
*Enrique Soriano-Salvador,Gorka Guardiola Múzquiz,Juan González Gómez*

Main category: cs.CR

TL;DR: 本研究探讨了使用用户空间工具检测用户空间rootkit的有效性，通过实验表明传统检测机制可以被绕过，并提供了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管内核空间rootkit受到更多关注，但用户空间rootkit仍然存在且被认为容易被检测。本文旨在探讨使用用户空间工具检测用户空间rootkit是否有效。

Method: 通过设计和实施针对Linux系统的实验，特别是隐藏进程的场景下，研究如何规避广泛接受的用户空间rootkit检测机制，并分析现有检测工具（如开源反rootkit工具）的不足。

Result: 研究表明，传统的用户空间rootkit检测方法可以被绕过，使用用户空间工具检测用户空间rootkit并不完全可靠。

Conclusion: 检测用户空间rootkit不能仅仅依赖于用户空间工具，需要谨慎对待检测结果，并提出改进现有工具的建议。

Abstract: The kind of malware designed to conceal malicious system resources (e.g.
processes, network connections, files, etc.) is commonly referred to as a
rootkit. This kind of malware represents a significant threat in contemporany
systems. Despite the existence of kernel-space rootkits (i.e. rootkits that
infect the operating system kernel), user-space rootkits (i.e. rootkits that
infect the user-space operating system tools, commands and libraries) continue
to pose a significant danger. However, kernel-space rootkits attract all the
attention, implicitly assuming that user-space rootkits (malware that is still
in existence) are easily detectable by well-known user-space tools that look
for anomalies. The primary objective of this work is to answer the following
question: Is detecting user-space rootkits with user-space tools futile?
Contrary to the prevailing view that considers it effective, we argue that the
detection of user-space rootkits cannot be done in user-space at all. Moreover,
the detection results must be communicated to the user with extreme caution. To
support this claim, we conducted different experiments focusing on process
concealing in Linux systems. In these experiments, we evade the detection
mechanisms widely accepted as the standard solution for this type of user-space
malware, bypassing the most popular open source anti-rootkit tool for process
hiding. This manuscript describes the classical approach to build user-space
library rootkits, the traditional detection mechanisms, and different evasion
techniques (it also includes understandable code snippets and examples). In
addition, it offers some guidelines to implement new detection tools and
improve the existing ones to the extent possible.

</details>


### [309] [Are Trees Really Green? A Detection Approach of IoT Malware Attacks](https://arxiv.org/abs/2506.07836)
*Silvia Lucia Sanna,Diego Soi,Davide Maiorca,Giorgio Giacinto*

Main category: cs.CR

TL;DR: The paper proposes an energy-efficient methodology to detect IoT malware networking attacks using optimized tree-based ML models.


<details>
  <summary>Details</summary>
Motivation: IoT devices are vulnerable to cybersecurity attacks due to resource constraints and difficulty in applying security patches. Existing ML-based attack detection methods focus on identification without considering the impact on computational resources.

Method: Optimizing hyperparameters of Decision Trees, Random Forest, and Extra-Trees models based on energy consumption and test-time performance (Matthew's Correlation Coefficient) using flow privacy-preserving statistical features.

Result: The models maintain high performance and detection accuracy while reducing power usage in terms of watt-hours (Wh).

Conclusion: On-premise ML-based Intrusion Detection Systems are suitable for IoT and other resource-constrained devices.

Abstract: Nowadays, the Internet of Things (IoT) is widely employed, and its usage is
growing exponentially because it facilitates remote monitoring, predictive
maintenance, and data-driven decision making, especially in the healthcare and
industrial sectors. However, IoT devices remain vulnerable due to their
resource constraints and difficulty in applying security patches. Consequently,
various cybersecurity attacks are reported daily, such as Denial of Service,
particularly in IoT-driven solutions. Most attack detection methodologies are
based on Machine Learning (ML) techniques, which can detect attack patterns.
However, the focus is more on identification rather than considering the impact
of ML algorithms on computational resources. This paper proposes a green
methodology to identify IoT malware networking attacks based on flow
privacy-preserving statistical features. In particular, the hyperparameters of
three tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are
optimized based on energy consumption and test-time performance in terms of
Matthew's Correlation Coefficient. Our results show that models maintain high
performance and detection accuracy while consistently reducing power usage in
terms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion
Detection Systems are suitable for IoT and other resource-constrained devices.

</details>


### [310] [Securing Unbounded Differential Privacy Against Timing Attacks](https://arxiv.org/abs/2506.07868)
*Zachary Ratliff,Salil Vadhan*

Main category: cs.CR

TL;DR: 通过有效的程序转换方法，在无界差分隐私设置中实现纯JOT-DP，同时保持计算效率并减少误差。


<details>
  <summary>Details</summary>
Motivation: 现有的将纯差分隐私（pure DP）程序转换为联合输出和运行时间差分隐私（JOT-DP）程序的方法在无界差分隐私设置中存在一些局限性，包括误差不随数据量减少、计算效率下降以及分析模型过于简单等问题。

Method: 作者提出了在不同的计算模型下，将任意纯JOT-DP程序从有界设置转换到无界设置的有效方法，并确保输出分布与原始程序接近。具体而言，根据计算模型的不同（如随机RAM模型或仅有随机比特生成器），可以实现多项式级小的误差概率或任意小的常数误差概率。

Result: 证明了在无界差分隐私设置中实现纯JOT-DP所需的误差取决于计算模型；在随机RAM模型中，多项式级小的误差概率是必要且充分的；而在其他模型中，任意小的常数误差概率是必要且充分的。此外，提出了高效的程序转换方法以满足这些条件。

Conclusion: 本文克服了现有JOT-DP方法在无界差分隐私设置中的局限性，展示了误差需求与计算模型的关系，并提供了高效的程序转换方法，从而实现了更优的隐私保护效果。

Abstract: Recent works have started to theoretically investigate how we can protect
differentially private programs against timing attacks, by making the joint
distribution the output and the runtime differentially private (JOT-DP).
However, the existing approaches to JOT-DP have some limitations, particularly
in the setting of unbounded DP (which protects the size of the dataset and
applies to arbitrarily large datasets). First, the known conversion of pure DP
programs to pure JOT-DP programs in the unbounded setting (a) incurs a constant
additive increase in error probability (and thus does not provide vanishing
error as $n\to\infty$) (b) produces JOT-DP programs that fail to preserve the
computational efficiency of the original pure DP program and (c) is analyzed in
a toy computational model in which the runtime is defined to be the number of
coin flips. In this work, we overcome these limitations. Specifically, we show
that the error required for pure JOT-DP in the unbounded setting depends on the
model of computation. In a randomized RAM model where the dataset size $n$ is
given (or can be computed in constant time) and we can generate random numbers
(not just random bits) in constant time, polynomially small error probability
is necessary and sufficient. If $n$ is not given or we only have a random-bit
generator, an (arbitrarily small) constant error probability is necessary and
sufficient. The aforementioned positive results are proven by efficient
procedures to convert any pure JOT-DP program $P$ in the upper-bounded setting
to a pure JOT-DP program $P'$ in the unbounded setting, such that the output
distribution of $P'$ is $\gamma$-close in total variation distance to that of
$P$, where $\gamma$ is either an arbitrarily small constant or polynomially
small, depending on the model of computation.

</details>


### [311] [Evaluating explainable AI for deep learning-based network intrusion detection system alert classification](https://arxiv.org/abs/2506.07882)
*Rajesh Kalakoti,Risto Vaarandi,Hayretdin Bahsi,Sven Nõmm*

Main category: cs.CR

TL;DR: This paper addresses the challenge of prioritizing network intrusion alerts using an LSTM model and evaluates four XAI methods for explaining the model's decisions, finding DeepLIFT to be the most effective.


<details>
  <summary>Details</summary>
Motivation: Network Intrusion Detection Systems (NIDS) generate a large number of daily alerts, making it difficult for analysts to prioritize high-priority threats. Existing deep learning models lack transparency, undermining trust in their decision-making.

Method: A real-world NIDS alert dataset from TalTech's Security Operations Center was used to develop an LSTM model for alert prioritization. Four XAI methods (LIME, SHAP, Integrated Gradients, and DeepLIFT) were implemented and compared using a framework evaluating faithfulness, complexity, robustness, and reliability.

Result: DeepLIFT outperformed the other XAI methods in terms of faithfulness, complexity, robustness, and reliability. The features identified by SOC analysts aligned strongly with those obtained by the XAI methods.

Conclusion: The study highlights the importance of XAI in improving trust and interpretability in NIDS alert classification. DeepLIFT is found to be the most effective XAI method, enhancing the practical applicability of the approach.

Abstract: A Network Intrusion Detection System (NIDS) monitors networks for cyber
attacks and other unwanted activities. However, NIDS solutions often generate
an overwhelming number of alerts daily, making it challenging for analysts to
prioritize high-priority threats. While deep learning models promise to
automate the prioritization of NIDS alerts, the lack of transparency in these
models can undermine trust in their decision-making. This study highlights the
critical need for explainable artificial intelligence (XAI) in NIDS alert
classification to improve trust and interpretability. We employed a real-world
NIDS alert dataset from Security Operations Center (SOC) of TalTech (Tallinn
University Of Technology) in Estonia, developing a Long Short-Term Memory
(LSTM) model to prioritize alerts. To explain the LSTM model's alert
prioritization decisions, we implemented and compared four XAI methods: Local
Interpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations
(SHAP), Integrated Gradients, and DeepLIFT. The quality of these XAI methods
was assessed using a comprehensive framework that evaluated faithfulness,
complexity, robustness, and reliability. Our results demonstrate that DeepLIFT
consistently outperformed the other XAI methods, providing explanations with
high faithfulness, low complexity, robust performance, and strong reliability.
In collaboration with SOC analysts, we identified key features essential for
effective alert classification. The strong alignment between these
analyst-identified features and those obtained by the XAI methods validates
their effectiveness and enhances the practical applicability of our approach.

</details>


### [312] [SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark](https://arxiv.org/abs/2506.07888)
*Rui Wen,Yiyong Liu,Michael Backes,Yang Zhang*

Main category: cs.CR

TL;DR: This paper proposes a unified taxonomy and formal definitions for data reconstruction attacks in the vision domain, introduces new evaluation metrics, uses LLMs for visual assessment, establishes a benchmark for future research, and provides insights for designing new attacks.


<details>
  <summary>Details</summary>
Motivation: The increasing attention on data reconstruction attacks lacks a consensus on formal definition and appropriate evaluation metrics, hindering further advancement.

Method: Propose a unified attack taxonomy and formal definitions, a set of quantitative evaluation metrics, leverage LLMs for visual evaluation, and present a unified framework for evaluating existing attacks.

Result: Empirical results validate the effectiveness of the proposed metrics and offer valuable insights for designing new attacks from a memorization perspective.

Conclusion: The proposed taxonomy, definitions, and metrics provide a systematic way to evaluate data reconstruction attacks and establish a benchmark for future research.

Abstract: Data reconstruction attacks, which aim to recover the training dataset of a
target model with limited access, have gained increasing attention in recent
years. However, there is currently no consensus on a formal definition of data
reconstruction attacks or appropriate evaluation metrics for measuring their
quality. This lack of rigorous definitions and universal metrics has hindered
further advancement in this field. In this paper, we address this issue in the
vision domain by proposing a unified attack taxonomy and formal definitions of
data reconstruction attacks. We first propose a set of quantitative evaluation
metrics that consider important criteria such as quantifiability, consistency,
precision, and diversity. Additionally, we leverage large language models
(LLMs) as a substitute for human judgment, enabling visual evaluation with an
emphasis on high-quality reconstructions. Using our proposed taxonomy and
metrics, we present a unified framework for systematically evaluating the
strengths and limitations of existing attacks and establishing a benchmark for
future research. Empirical results, primarily from a memorization perspective,
not only validate the effectiveness of our metrics but also offer valuable
insights for designing new attacks.

</details>


### [313] [Secure Distributed Learning for CAVs: Defending Against Gradient Leakage with Leveled Homomorphic Encryption](https://arxiv.org/abs/2506.07894)
*Muhammad Ali Najjar,Ren-Yi Huang,Dumindu Samaraweera,Prashant Shekhar*

Main category: cs.CR

TL;DR: Federated Learning (FL) is crucial for privacy-preserving machine learning, but model gradients can be vulnerable to inference attacks. Existing defenses often reduce model accuracy. Homomorphic Encryption (HE) offers a solution that preserves both privacy and accuracy but introduces overhead. This paper evaluates HE schemes for FL, proposes a selective encryption strategy, and develops a full HE-based FL pipeline to mitigate inference attacks while maintaining model performance.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of model gradients in Federated Learning to inference attacks like Deep Leakage from Gradients (DLG), while avoiding the compromise of model accuracy that existing defenses such as Differential Privacy and Secure Multi-Party Computation may introduce.

Method: Systematically evaluate various leveled Homomorphic Encryption schemes suitable for FL in resource-constrained environments. Propose a selective encryption strategy targeting only the most sensitive gradients to minimize computational overhead. Develop a full HE-based FL pipeline.

Result: Identification of the most suitable HE schemes for FL in constrained environments. Successful mitigation of DLG attacks while preserving model accuracy. Open-sourced implementation for reproducibility and adoption.

Conclusion: Homomorphic Encryption provides an effective alternative for privacy-preserving FL without compromising model utility. The proposed selective encryption strategy and HE-based FL pipeline offer practical solutions to balance privacy and efficiency.

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, making it a promising approach
for privacy-preserving machine learning in domains like Connected and
Autonomous Vehicles (CAVs). However, recent studies have shown that exchanged
model gradients remain susceptible to inference attacks such as Deep Leakage
from Gradients (DLG), which can reconstruct private training data. While
existing defenses like Differential Privacy (DP) and Secure Multi-Party
Computation (SMPC) offer protection, they often compromise model accuracy. To
that end, Homomorphic Encryption (HE) offers a promising alternative by
enabling lossless computation directly on encrypted data, thereby preserving
both privacy and model utility. However, HE introduces significant
computational and communication overhead, which can hinder its practical
adoption. To address this, we systematically evaluate various leveled HE
schemes to identify the most suitable for FL in resource-constrained
environments due to its ability to support fixed-depth computations without
requiring costly bootstrapping. Our contributions in this paper include a
comprehensive evaluation of HE schemes for real-world FL applications, a
selective encryption strategy that targets only the most sensitive gradients to
minimize computational overhead, and the development of a full HE-based FL
pipeline that effectively mitigates DLG attacks while preserving model
accuracy. We open-source our implementation to encourage reproducibility and
facilitate adoption in safety-critical domains.

</details>


### [314] [Exposing Hidden Backdoors in NFT Smart Contracts: A Static Security Analysis of Rug Pull Patterns](https://arxiv.org/abs/2506.07974)
*Chetan Pathade,Shweta Hooli*

Main category: cs.CR

TL;DR: This paper conducts a large-scale static analysis of NFT smart contracts to uncover vulnerabilities linked to rug pulls, introduces a risk scoring model for contract classification, and provides mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: The rise in popularity of NFTs has led to an increase in fraudulent activities such as rug pulls, which exploit trust and smart contract privileges. The motivation is to address the security issues posed by deliberately coded backdoors within NFT smart contracts.

Method: Using Slither, a static analysis framework, the authors analyzed 49,940 verified NFT smart contracts from the Ethereum mainnet. They developed a custom risk scoring model to classify contracts based on the severity of rug pull indicators and created visualizations to highlight vulnerability patterns.

Result: The analysis uncovered latent vulnerabilities commonly associated with rug pulls, identified clusters of red flags, and demonstrated how malicious patterns can be detected through large-scale static analysis without live exploits.

Conclusion: The work contributes a practical foundation for detecting and mitigating NFT rug pulls through scalable automated analysis, offering strategies to enhance smart contract security for developers, marketplaces, and auditors.

Abstract: The explosive growth of Non-Fungible Tokens (NFTs) has revolutionized digital
ownership by enabling the creation, exchange, and monetization of unique assets
on blockchain networks. However, this surge in popularity has also given rise
to a disturbing trend: the emergence of rug pulls - fraudulent schemes where
developers exploit trust and smart contract privileges to drain user funds or
invalidate asset ownership. Central to many of these scams are hidden backdoors
embedded within NFT smart contracts. Unlike unintentional bugs, these backdoors
are deliberately coded and often obfuscated to bypass traditional audits and
exploit investor confidence. In this paper, we present a large-scale static
analysis of 49,940 verified NFT smart contracts using Slither, a static
analysis framework, to uncover latent vulnerabilities commonly linked to rug
pulls. We introduce a custom risk scoring model that classifies contracts into
high, medium, or low risk tiers based on the presence and severity of rug pull
indicators. Our dataset was derived from verified contracts on the Ethereum
mainnet, and we generate multiple visualizations to highlight red flag
clusters, issue prevalence, and co-occurrence of critical vulnerabilities.
While we do not perform live exploits, our results reveal how malicious
patterns often missed by simple reviews can be surfaced through static analysis
at scale. We conclude by offering mitigation strategies for developers,
marketplaces, and auditors to enhance smart contract security. By exposing how
hidden backdoors manifest in real-world smart contracts, this work contributes
a practical foundation for detecting and mitigating NFT rug pulls through
scalable automated analysis.

</details>


### [315] [Unraveling Ethereum's Mempool: The Impact of Fee Fairness, Transaction Prioritization, and Consensus Efficiency](https://arxiv.org/abs/2506.07988)
*S M Mostaq Hossain,Amani Altarawneh*

Main category: cs.CR

TL;DR: The paper explores Ethereum's mempool dynamics and fee market efficiency, revealing disparities in transaction inclusion based on fees despite EIP-1559 improvements. It proposes solutions like congestion-aware fee adjustments and reserved block slots for low-fee transactions to enhance fairness, validator performance, and scalability.


<details>
  <summary>Details</summary>
Motivation: To understand the factors affecting transaction inclusion, validator workload, and network performance in Ethereum's proof-of-stake ecosystem, particularly focusing on gas price variations, mempool clearance rates, and block finalization times.

Method: Empirical analysis using real-time data from Geth and Prysm nodes to observe transaction prioritization, mempool congestion, and fee market inefficiencies.

Result: High-fee transactions are prioritized, low-fee transactions face delays, and extremely high fees do not always guarantee faster confirmation. Mempool congestion significantly impacts validator efficiency and proposal latency.

Conclusion: Proposed solutions such as congestion-aware fee adjustments and reserved block slots for low-fee transactions can lead to more equitable transaction inclusion, better validator performance, and improved scalability, supporting Ethereum's decentralization.

Abstract: Ethereum's transaction pool (mempool) dynamics and fee market efficiency
critically affect transaction inclusion, validator workload, and overall
network performance. This research empirically analyzes gas price variations,
mempool clearance rates, and block finalization times in Ethereum's
proof-of-stake ecosystem using real-time data from Geth and Prysm nodes. We
observe that high-fee transactions are consistently prioritized, while low-fee
transactions face delays or exclusion despite EIP-1559's intended improvements.
Mempool congestion remains a key factor in validator efficiency and proposal
latency. We provide empirical evidence of persistent fee-based disparities and
show that extremely high fees do not always guarantee faster confirmation,
revealing inefficiencies in the current fee market. To address these issues, we
propose congestion-aware fee adjustments, reserved block slots for low-fee
transactions, and improved handling of out-of-gas vulnerabilities. By
mitigating prioritization bias and execution inefficiencies, our findings
support more equitable transaction inclusion, enhance validator performance,
and promote scalability. This work contributes to Ethereum's long-term
decentralization by reducing dependence on high transaction fees for network
participation.

</details>


### [316] [Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection](https://arxiv.org/abs/2506.00654)
*Marco Di Gennaro,Francesco Panebianco,Marco Pianta,Stefano Zanero,Michele Carminati*

Main category: cs.CR

TL;DR: The paper presents Amatriciana, a new method using Graph Neural Networks to detect money laundering in transaction graphs by considering temporal information.


<details>
  <summary>Details</summary>
Motivation: Money laundering is a serious financial crime that threatens financial integrity and social security. With the increasing number of transactions, automatic tools are needed to assist law enforcement agencies in detecting such criminal activities.

Method: Amatriciana uses Graph Neural Networks to analyze the entire graph of transactions without splitting it into time-based subgraphs, thus exploiting all relational information in the dataset.

Result: Experiments on a public dataset show that the model can learn from limited data and outperforms other state-of-the-art approaches when more data is available. It achieves an F1 score of 0.76 and reduces False Positives by 55%.

Conclusion: Amatriciana is effective in detecting money launderers with fewer false positives compared to other methods.

Abstract: Money laundering is a financial crime that poses a serious threat to
financial integrity and social security. The growing number of transactions
makes it necessary to use automatic tools that help law enforcement agencies
detect such criminal activity. In this work, we present Amatriciana, a novel
approach based on Graph Neural Networks to detect money launderers inside a
graph of transactions by considering temporal information. Amatriciana uses the
whole graph of transactions without splitting it into several time-based
subgraphs, exploiting all relational information in the dataset. Our
experiments on a public dataset reveal that the model can learn from a limited
amount of data. Furthermore, when more data is available, the model outperforms
other State-of-the-art approaches; in particular, Amatriciana decreases the
number of False Positives (FPs) while detecting many launderers. In summary,
Amatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%
with respect to other State-of-the-art models.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [317] [Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence](https://arxiv.org/abs/2506.07060)
*Laura Cohen,Xavier Hinaut,Lilyana Petrova,Alexandre Pitti,Syd Reynal,Ichiro Tsuda*

Main category: q-bio.NC

TL;DR: 自然智能（NI）通过有限的神经和能量限制，从稀疏数据中高效学习。相比之下，当前的人工智能（AI）依赖几乎无限的计算能力、能量和数据来达到高性能。本文探讨了NI中的约束如何成为效率、适应性和创造力的催化剂，并提出了采用'少即是多'原则以发展更高效、可解释和生物基础的人工系统。


<details>
  <summary>Details</summary>
Motivation: 受到自然智能（如婴儿学习语言、形成抽象概念和获取感知运动技能）的启发，研究者希望了解如何在有限资源下实现高效学习与泛化，从而改进当前依赖大量计算、能量和数据的人工智能系统。

Method: 本文通过分析有限神经带宽、混沌游移、储存计算等机制，展示了自然智能如何利用约束条件实现高效学习和泛化。同时，结合发育视角，强调内在动机和社会环境对婴儿学习的重要性。

Result: 研究表明，有限的神经带宽促进了简洁的编码方式，随机投影有助于从小数据集中快速泛化，而混沌游移则帮助大脑灵活检索记忆和处理不确定性。这些机制共同推动了自然智能的高效性、适应性和创造性。

Conclusion: 作者建议未来人工智能应采纳'少即是多'的原则，即通过能源约束、简约架构和真实世界交互，发展更高效、可解释且基于生物学原理的系统。

Abstract: Natural intelligence (NI) consistently achieves more with less. Infants learn
language, develop abstract concepts, and acquire sensorimotor skills from
sparse data, all within tight neural and energy limits. In contrast, today's AI
relies on virtually unlimited computational power, energy, and data to reach
high performance. This paper argues that constraints in NI are paradoxically
catalysts for efficiency, adaptability, and creativity. We first show how
limited neural bandwidth promotes concise codes that still capture complex
patterns. Spiking neurons, hierarchical structures, and symbolic-like
representations emerge naturally from bandwidth constraints, enabling robust
generalization. Next, we discuss chaotic itinerancy, illustrating how the brain
transits among transient attractors to flexibly retrieve memories and manage
uncertainty. We then highlight reservoir computing, where random projections
facilitate rapid generalization from small datasets. Drawing on developmental
perspectives, we emphasize how intrinsic motivation, along with responsive
social environments, drives infant language learning and discovery of meaning.
Such active, embodied processes are largely absent in current AI. Finally, we
suggest that adopting 'less is more' principles -- energy constraints,
parsimonious architectures, and real-world interaction -- can foster the
emergence of more efficient, interpretable, and biologically grounded
artificial systems.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [318] [Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds](https://arxiv.org/abs/2506.07614)
*Rishikesh Srinivasan,Dheeraj Nagaraj*

Main category: math.PR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of sampling from strongly log-concave distributions over
$\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the
randomized midpoint method) for overdamped/underdamped Langevin dynamics. We
prove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic
speedup in dependence on the target accuracy ($\epsilon$) over the
Euler-Maruyama discretization, surpassing existing bounds for randomized
midpoint methods. Notably, in the case of underdamped Langevin dynamics, we
demonstrate the complexity of $W_2$ convergence is much smaller than the
complexity lower bounds for convergence in $L^2$ strong error established in
the literature.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [319] [Securing Traffic Sign Recognition Systems in Autonomous Vehicles](https://arxiv.org/abs/2506.06563)
*Thushari Hapuarachchi,Long Dang,Kaiqi Xiong*

Main category: cs.CV

TL;DR: The paper investigates the robustness of DNNs for traffic sign recognition, proposes a data augmentation-based training method to mitigate error-minimizing attacks, and introduces a detection model for poisoned data.


<details>
  <summary>Details</summary>
Motivation: DNNs used in traffic sign recognition are trained on large-scale datasets from unknown sources, making them vulnerable to data poisoning attacks. It is crucial to ensure model security during training.

Method: The authors perform error-minimizing attacks by adding imperceptible perturbations to the training data. They propose a data augmentation-based training method using nonlinear transformations to disrupt the perturbations and improve model robustness. Additionally, they develop a detection model to identify poisoned data.

Result: Error-minimizing attacks reduce prediction accuracy from 99.90% to 10.6%. The proposed mitigation scheme restores accuracy to 96.05%, outperforming adversarial training. The detection model achieves over 99% success rate in identifying poisoned data.

Conclusion: Advanced training methods are necessary for DNNs in traffic sign recognition systems to effectively mitigate data poisoning attacks.

Abstract: Deep Neural Networks (DNNs) are widely used for traffic sign recognition
because they can automatically extract high-level features from images. These
DNNs are trained on large-scale datasets obtained from unknown sources.
Therefore, it is important to ensure that the models remain secure and are not
compromised or poisoned during training. In this paper, we investigate the
robustness of DNNs trained for traffic sign recognition. First, we perform the
error-minimizing attacks on DNNs used for traffic sign recognition by adding
imperceptible perturbations on training data. Then, we propose a data
augmentation-based training method to mitigate the error-minimizing attacks.
The proposed training method utilizes nonlinear transformations to disrupt the
perturbations and improve the model robustness. We experiment with two
well-known traffic sign datasets to demonstrate the severity of the attack and
the effectiveness of our mitigation scheme. The error-minimizing attacks reduce
the prediction accuracy of the DNNs from 99.90% to 10.6%. However, our
mitigation scheme successfully restores the prediction accuracy to 96.05%.
Moreover, our approach outperforms adversarial training in mitigating the
error-minimizing attacks. Furthermore, we propose a detection model capable of
identifying poisoned data even when the perturbations are imperceptible to
human inspection. Our detection model achieves a success rate of over 99% in
identifying the attack. This research highlights the need to employ advanced
training methods for DNNs in traffic sign recognition systems to mitigate the
effects of data poisoning attacks.

</details>


### [320] [D2R: dual regularization loss with collaborative adversarial generation for model robustness](https://arxiv.org/abs/2506.07056)
*Zhenyu Liu,Huizhi Liang,Rajiv Ranjan,Zhanxing Zhu,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: This paper proposes D2R Loss and CAG to improve model robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The existing defense methods have limitations in terms of loss function guidance and collaborative adversarial generation.

Method: D2R loss includes two optimization steps for adversarial and clean distribution, leveraging different loss functions. CAG generates adversarial samples through gradient-based collaboration between guidance and target models.

Result: Extensive experiments on benchmark databases show that D2R loss with CAG produces highly robust models.

Conclusion: D2R Loss and CAG are effective strategies for enhancing the robustness of deep neural network models.

Abstract: The robustness of Deep Neural Network models is crucial for defending models
against adversarial attacks. Recent defense methods have employed collaborative
learning frameworks to enhance model robustness. Two key limitations of
existing methods are (i) insufficient guidance of the target model via loss
functions and (ii) non-collaborative adversarial generation. We, therefore,
propose a dual regularization loss (D2R Loss) method and a collaborative
adversarial generation (CAG) strategy for adversarial training. D2R loss
includes two optimization steps. The adversarial distribution and clean
distribution optimizations enhance the target model's robustness by leveraging
the strengths of different loss functions obtained via a suitable function
space exploration to focus more precisely on the target model's distribution.
CAG generates adversarial samples using a gradient-based collaboration between
guidance and target models. We conducted extensive experiments on three
benchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two
popular target models, WideResNet34-10 and PreActResNet18. Our results show
that D2R loss with CAG produces highly robust models.

</details>


### [321] [Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation](https://arxiv.org/abs/2506.07214)
*Zhiyuan Zhong,Zhen Sun,Yepang Liu,Xinlei He,Guanhong Tao*

Main category: cs.CV

TL;DR: Vision Language Models (VLMs) are vulnerable to backdoor attacks through cross-modal semantic mismatches. Proposed BadSem attack achieves high ASR, generalizes well, and transfers across modalities. Defenses based on system prompt and supervised fine-tuning fail.


<details>
  <summary>Details</summary>
Motivation: Existing backdoor attacks on VLMs primarily rely on single-modality triggers, neglecting the cross-modal fusion nature of VLMs. This paper aims to explore the vulnerability of VLMs to cross-modal semantic mismatches as implicit triggers.

Method: The paper proposes BadSem, a data poisoning attack that injects backdoors by misaligning image-text pairs during training. A dataset called SIMBad is constructed for semantic manipulation involving color and object attributes.

Result: Extensive experiments show that BadSem achieves over 98% average ASR, generalizes well to out-of-distribution datasets, and can transfer across poisoning modalities. Attention visualization reveals that backdoored models focus on semantically sensitive regions under mismatched conditions while behaving normally on clean inputs.

Conclusion: The findings indicate that VLMs are vulnerable to semantic backdoor attacks, and current defense strategies are ineffective. There is an urgent need to address semantic vulnerabilities in VLMs for safer deployment.

Abstract: Vision Language Models (VLMs) have shown remarkable performance, but are also
vulnerable to backdoor attacks whereby the adversary can manipulate the model's
outputs through hidden triggers. Prior attacks primarily rely on
single-modality triggers, leaving the crucial cross-modal fusion nature of VLMs
largely unexplored. Unlike prior work, we identify a novel attack surface that
leverages cross-modal semantic mismatches as implicit triggers. Based on this
insight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data
poisoning attack that injects stealthy backdoors by deliberately misaligning
image-text pairs during training. To perform the attack, we construct SIMBad, a
dataset tailored for semantic manipulation involving color and object
attributes. Extensive experiments across four widely used VLMs show that BadSem
achieves over 98% average ASR, generalizes well to out-of-distribution
datasets, and can transfer across poisoning modalities. Our detailed analysis
using attention visualization shows that backdoored models focus on
semantically sensitive regions under mismatched conditions while maintaining
normal behavior on clean inputs. To mitigate the attack, we try two defense
strategies based on system prompt and supervised fine-tuning but find that both
of them fail to mitigate the semantic backdoor. Our findings highlight the
urgent need to address semantic vulnerabilities in VLMs for their safer
deployment.

</details>


### [322] [Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning](https://arxiv.org/abs/2309.11082)
*Chen Jiang,Hong Liu,Xuzheng Yu,Qing Wang,Yuan Cheng,Jia Xu,Zhongyi Liu,Qingpei Guo,Wei Chu,Ming Yang,Yuan Qi*

Main category: cs.CV

TL;DR: The paper proposes two novel techniques, DMAE and TPM-CL, to improve contrastive learning for text-video retrieval by focusing on hard negative pairs and fine-grained semantic similarity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current contrastive learning methods in text-video retrieval which do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity.

Method: Propose DMAE to mine hard negative pairs and NegNCE loss to highlight their impacts; present TPM-CL module with adaptive token masking strategy to construct partial order triplet samples.

Result: Outperforms existing methods on four widely-used text-video retrieval datasets (MSR-VTT, MSVD, DiDeMo, ActivityNet).

Conclusion: The proposed approach significantly improves the performance of text-video retrieval by better handling hard negatives and modeling fine-grained semantic similarity.

Abstract: In recent years, the explosion of web videos makes text-video retrieval
increasingly essential and popular for video filtering, recommendation, and
search. Text-video retrieval aims to rank relevant text/video higher than
irrelevant ones. The core of this task is to precisely measure the cross-modal
similarity between texts and videos. Recently, contrastive learning methods
have shown promising results for text-video retrieval, most of which focus on
the construction of positive and negative pairs to learn text and video
representations. Nevertheless, they do not pay enough attention to hard
negative pairs and lack the ability to model different levels of semantic
similarity. To address these two issues, this paper improves contrastive
learning using two novel techniques. First, to exploit hard examples for robust
discriminative power, we propose a novel Dual-Modal Attention-Enhanced Module
(DMAE) to mine hard negative pairs from textual and visual clues. By further
introducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively
identify all these hard negatives and explicitly highlight their impacts in the
training loss. Second, our work argues that triplet samples can better model
fine-grained semantic similarity compared to pairwise samples. We thereby
present a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to
construct partial order triplet samples by automatically generating
fine-grained hard negatives for matched text-video pairs. The proposed TPM-CL
designs an adaptive token masking strategy with cross-modal interaction to
model subtle semantic differences. Extensive experiments demonstrate that the
proposed approach outperforms existing methods on four widely-used text-video
retrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.

</details>


### [323] [MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4](https://arxiv.org/abs/2406.00971)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.CV

TL;DR: The paper extends and fine-tunes MiniGPT-4 for the reverse designing task, demonstrating the extensibility of off-the-shelf VLMs like MiniGPT-4.


<details>
  <summary>Details</summary>
Motivation: To explore the capability of Vision-Language Models (VLMs) in handling complex vision-language tasks such as reverse designing, which goes beyond traditional tasks by requiring comprehension of source image, edited version, and optional textual context simultaneously.

Method: The authors extend and fine-tune MiniGPT-4 for the reverse designing task, which involves predicting edits and their parameters given a source image, an edited version, and an optional high-level textual edit description.

Result: Experiments show that off-the-shelf VLMs, specifically MiniGPT-4, can be effectively extended and fine-tuned for complex tasks like reverse designing.

Conclusion: MiniGPT-4 and similar VLMs have strong extensibility for complex vision-language tasks such as reverse designing.

Abstract: Vision-Language Models (VLMs) have recently seen significant advancements
through integrating with Large Language Models (LLMs). The VLMs, which process
image and text modalities simultaneously, have demonstrated the ability to
learn and understand the interaction between images and texts across various
multi-modal tasks. Reverse designing, which could be defined as a complex
vision-language task, aims to predict the edits and their parameters, given a
source image, an edited version, and an optional high-level textual edit
description. This task requires VLMs to comprehend the interplay between the
source image, the edited version, and the optional textual context
simultaneously, going beyond traditional vision-language tasks. In this paper,
we extend and fine-tune MiniGPT-4 for the reverse designing task. Our
experiments demonstrate the extensibility of off-the-shelf VLMs, specifically
MiniGPT-4, for more complex tasks such as reverse designing. Code is available
at this \href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}

</details>


### [324] [STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis](https://arxiv.org/abs/2506.06276)
*Jiatao Gu,Tianrong Chen,David Berthelot,Huangjie Zheng,Yuyang Wang,Ruixiang Zhang,Laurent Dinh,Miguel Angel Bautista,Josh Susskind,Shuangfei Zhai*

Main category: cs.CV

TL;DR: The paper introduces STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. It combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers.


<details>
  <summary>Details</summary>
Motivation: To create a scalable generative model for high-resolution image synthesis that can compete with state-of-the-art diffusion models in sample quality.

Method: STARFlow's core is Transformer Autoregressive Flow (TARFlow), which integrates normalizing flows and Autoregressive Transformers. Innovations include a deep-shallow design, modeling in pretrained autoencoder latent space, and a novel guidance algorithm.

Result: STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality.

Conclusion: This work successfully demonstrates normalizing flows operating effectively at high scale and resolution, being the first to do so.

Abstract: We present STARFlow, a scalable generative model based on normalizing flows
that achieves strong performance in high-resolution image synthesis. The core
of STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the
expressive power of normalizing flows with the structured modeling capabilities
of Autoregressive Transformers. We first establish the theoretical universality
of TARFlow for modeling continuous distributions. Building on this foundation,
we introduce several key architectural and algorithmic innovations to
significantly enhance scalability: (1) a deep-shallow design, wherein a deep
Transformer block captures most of the model representational capacity,
complemented by a few shallow Transformer blocks that are computationally
efficient yet substantially beneficial; (2) modeling in the latent space of
pretrained autoencoders, which proves more effective than direct pixel-level
modeling; and (3) a novel guidance algorithm that significantly boosts sample
quality. Crucially, our model remains an end-to-end normalizing flow, enabling
exact maximum likelihood training in continuous spaces without discretization.
STARFlow achieves competitive performance in both class-conditional and
text-conditional image generation tasks, approaching state-of-the-art diffusion
models in sample quality. To our knowledge, this work is the first successful
demonstration of normalizing flows operating effectively at this scale and
resolution.

</details>


### [325] [Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow](https://arxiv.org/abs/2506.06283)
*Juexiao Zhou,Zhongyi Han,Mankun Xin,Xingwei He,Guotao Wang,Jiaoyan Song,Gongning Luo,Wenjia He,Xintong Li,Yuetan Chu,Juanwen Chen,Bo Wang,Xia Wu,Wenwen Duan,Zhixia Guo,Liyan Bai,Yilin Pan,Xuefei Bi,Lu Liu,Long Feng,Xiaonan He,Xin Gao*

Main category: cs.CV

TL;DR: DigitalShadow is an early warning system for coronary artery disease (CAD) that uses facial recognition technology to assess CAD risk without active user engagement, ensuring privacy and personalization.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by global population aging and the high mortality rate associated with coronary artery disease (CAD), there is a need for early detection and proactive management of CAD. This work aims to introduce DigitalShadow as a solution for passive and contactless CAD risk assessment.

Method: The system is pre-trained on 21 million facial images and fine-tuned into LiveCAD, a specialized CAD risk assessment model trained on 7,004 facial images from 1,751 subjects across four hospitals in China. It extracts facial features from live video streams and integrates with a personalized database to generate natural language risk reports and individualized health recommendations.

Result: DigitalShadow functions passively and contactlessly, extracting facial features from live video streams without requiring active user engagement. It supports local deployment to ensure secure handling of user data.

Conclusion: DigitalShadow represents an advanced early warning system for CAD, offering a novel approach to risk assessment through facial feature extraction. Its design prioritizes privacy and personalization, making it a promising tool in the proactive management of CAD.

Abstract: Global population aging presents increasing challenges to healthcare systems,
with coronary artery disease (CAD) responsible for approximately 17.8 million
deaths annually, making it a leading cause of global mortality. As CAD is
largely preventable, early detection and proactive management are essential. In
this work, we introduce DigitalShadow, an advanced early warning system for
CAD, powered by a fine-tuned facial foundation model. The system is pre-trained
on 21 million facial images and subsequently fine-tuned into LiveCAD, a
specialized CAD risk assessment model trained on 7,004 facial images from 1,751
subjects across four hospitals in China. DigitalShadow functions passively and
contactlessly, extracting facial features from live video streams without
requiring active user engagement. Integrated with a personalized database, it
generates natural language risk reports and individualized health
recommendations. With privacy as a core design principle, DigitalShadow
supports local deployment to ensure secure handling of user data.

</details>


### [326] [Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models](https://arxiv.org/abs/2506.06569)
*Yannis Spyridis,Vasileios Argyriou*

Main category: cs.CV

TL;DR: The paper explores using RGB imagery for textile recycling tasks, achieving 81.25% accuracy in classification and a mIoU of 0.90 in segmentation.


<details>
  <summary>Details</summary>
Motivation: Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging.

Method: For classification, several pre-trained architectures were evaluated using transfer learning and cross-validation, with EfficientNetB0 achieving the best performance. For feature segmentation, a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed.

Result: EfficientNetB0 achieved 81.25% accuracy on a held-out test set for classification. The zero-shot approach demonstrated excellent performance with a mIoU of 0.90 for the generated masks against ground truth.

Conclusion: This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques, including transfer learning for classification and foundation models for zero-shot segmentation, to enable essential analysis steps for automated textile recycling pipelines.

Abstract: Automated sorting is crucial for improving the efficiency and scalability of
textile recycling, but accurately identifying material composition and
detecting contaminants from sensor data remains challenging. This paper
investigates the use of standard RGB imagery, a cost-effective sensing
modality, for key pre-processing tasks in an automated system. We present
computer vision components designed for a conveyor belt setup to perform (a)
classification of four common textile types and (b) segmentation of non-textile
features such as buttons and zippers. For classification, several pre-trained
architectures were evaluated using transfer learning and cross-validation, with
EfficientNetB0 achieving the best performance on a held-out test set with
81.25\% accuracy. For feature segmentation, a zero-shot approach combining the
Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM)
was employed, demonstrating excellent performance with a mIoU of 0.90 for the
generated masks against ground truth. This study demonstrates the feasibility
of using RGB images coupled with modern deep learning techniques, including
transfer learning for classification and foundation models for zero-shot
segmentation, to enable essential analysis steps for automated textile
recycling pipelines.

</details>


### [327] [Improving Wildlife Out-of-Distribution Detection: Africas Big Five](https://arxiv.org/abs/2506.06719)
*Mufhumudzi Muthivhi,Jiahao Huo,Fredrik Gustafsson,Terence L. van Zyl*

Main category: cs.CV

TL;DR: The study explores out-of-distribution (OOD) detection of wildlife, particularly the Big Five African animals. It compares parametric Nearest Class Mean (NCM) and non-parametric contrastive learning approaches with pretrained features to common OOD methods. NCM with ImageNet pre-trained features shows significant improvement in AUPR-IN, AUPR-OUT and AUTC metrics.


<details>
  <summary>Details</summary>
Motivation: Current animal classification models operate under a closed-world assumption and remain overconfident when faced with unknown classes, which is problematic for mitigating human-wildlife conflict involving varied species such as the Big Five African animals.

Method: The research selects a parametric Nearest Class Mean (NCM) and a non-parametric contrastive learning approach as baselines, leveraging pretrained and projected features from popular classification encoders. These are then compared with various common OOD methods in literature.

Result: Feature-based methods demonstrate stronger generalisation capability across varying classification thresholds. Notably, NCM with ImageNet pre-trained features achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the best OOD methods respectively.

Conclusion: Out-of-distribution detection using feature-based methods like NCM with ImageNet pre-trained features provides better generalization and could be beneficial in identifying potential human-wildlife conflicts.

Abstract: Mitigating human-wildlife conflict seeks to resolve unwanted encounters
between these parties. Computer Vision provides a solution to identifying
individuals that might escalate into conflict, such as members of the Big Five
African animals. However, environments often contain several varied species.
The current state-of-the-art animal classification models are trained under a
closed-world assumption. They almost always remain overconfident in their
predictions even when presented with unknown classes. This study investigates
out-of-distribution (OOD) detection of wildlife, specifically the Big Five. To
this end, we select a parametric Nearest Class Mean (NCM) and a non-parametric
contrastive learning approach as baselines to take advantage of pretrained and
projected features from popular classification encoders. Moreover, we compare
our baselines to various common OOD methods in the literature. The results show
feature-based methods reflect stronger generalisation capability across varying
classification thresholds. Specifically, NCM with ImageNet pre-trained features
achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the
best OOD methods, respectively. The code can be found here
https://github.com/pxpana/BIG5OOD

</details>


### [328] [Hi-LSplat: Hierarchical 3D Language Gaussian Splatting](https://arxiv.org/abs/2506.06822)
*Chenlu Zhan,Yufei Zhang,Gaoang Wang,Hongwei Wang*

Main category: cs.CV

TL;DR: This paper proposes Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. It lifts 2D features to 3D features by constructing a 3D hierarchical semantic tree and introduces contrastive losses for capturing all-sided hierarchical semantic representations.


<details>
  <summary>Details</summary>
Motivation: Recent models using Gaussian Splatting for open-ended language queries in 3D lack a unified 3D representation, leading to view inconsistencies and impeding hierarchical semantic understanding.

Method: The method constructs a 3D hierarchical semantic tree with layered instance clustering to lift 2D features to 3D features and introduce instance-wise and part-wise contrastive losses to capture hierarchical semantic representations.

Result: Extensive experiments show the method's superiority in 3D open-vocabulary segmentation and localization, and its strong performance on hierarchical semantic datasets highlights its ability to capture complex hierarchical semantics within 3D scenes.

Conclusion: Hi-LSplat addresses the view inconsistency issue caused by 2D semantic features and demonstrates superior performance in capturing complex hierarchical semantics within 3D scenes.

Abstract: Modeling 3D language fields with Gaussian Splatting for open-ended language
queries has recently garnered increasing attention. However, recent 3DGS-based
models leverage view-dependent 2D foundation models to refine 3D semantics but
lack a unified 3D representation, leading to view inconsistencies.
Additionally, inherent open-vocabulary challenges cause inconsistencies in
object and relational descriptions, impeding hierarchical semantic
understanding. In this paper, we propose Hi-LSplat, a view-consistent
Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.
To achieve view-consistent 3D hierarchical semantics, we first lift 2D features
to 3D features by constructing a 3D hierarchical semantic tree with layered
instance clustering, which addresses the view inconsistency issue caused by 2D
semantic features. Besides, we introduce instance-wise and part-wise
contrastive losses to capture all-sided hierarchical semantic representations.
Notably, we construct two hierarchical semantic datasets to better assess the
model's ability to distinguish different semantic levels. Extensive experiments
highlight our method's superiority in 3D open-vocabulary segmentation and
localization. Its strong performance on hierarchical semantic datasets
underscores its ability to capture complex hierarchical semantics within 3D
scenes.

</details>


### [329] [Exploring Visual Prompting: Robustness Inheritance and Beyond](https://arxiv.org/abs/2506.06823)
*Qi Li,Liangzhi Li,Zhouqiang Jiang,Bowen Wang,Keke Tang*

Main category: cs.CV

TL;DR: Visual Prompting (VP) shows potential in vision tasks but its performance under robust source models is unexplored. This paper investigates if VP can inherit robustness, encounters a robustness-generalization trade-off, and proposes Prompt Boundary Loosening (PBL) to mitigate this.


<details>
  <summary>Details</summary>
Motivation: To determine if VP can inherit robustness from source models, understand the trade-off between robustness and generalization in VP, and find a strategy to address any limitations.

Method: Thorough exploration of VP's ability to inherit robustness, encounter trade-offs, and the introduction of PBL as a strategy to mitigate the trade-off.

Result: PBL effectively ensures successful inheritance of robustness and significantly enhances VP's generalization ability across various datasets.

Conclusion: The findings are universal and the proposed PBL strategy demonstrates significant benefits.

Abstract: Visual Prompting (VP), an efficient method for transfer learning, has shown
its potential in vision tasks. However, previous works focus exclusively on VP
from standard source models, it is still unknown how it performs under the
scenario of a robust source model: Can the robustness of the source model be
successfully inherited? Does VP also encounter the same trade-off between
robustness and generalization ability as the source model during this process?
If such a trade-off exists, is there a strategy specifically tailored to VP to
mitigate this limitation? In this paper, we thoroughly explore these three
questions for the first time and provide affirmative answers to them. To
mitigate the trade-off faced by VP, we propose a strategy called Prompt
Boundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally
compatible with VP, PBL effectively ensures the successful inheritance of
robustness when the source model is a robust model, while significantly
enhancing VP's generalization ability across various downstream datasets.
Extensive experiments across various datasets show that our findings are
universal and demonstrate the significant benefits of the proposed strategy.

</details>


### [330] [Controllable Coupled Image Generation via Diffusion Models](https://arxiv.org/abs/2506.06826)
*Chenfei Yuan,Nanshan Jia,Hangqi Li,Peter W. Glynn,Zeyu Zheng*

Main category: cs.CV

TL;DR: The paper presents a novel method for coupled image generation that separates background and object components using time-varying weight control parameters, leading to improved performance in generating images with similar backgrounds but different objects.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating multiple images with the same or similar backgrounds but different centered objects based on distinct text prompts.

Method: The method involves disentangling background and entity components within cross-attention modules and applying time-varying weight control parameters optimized through a combined objective function.

Result: Empirical results indicate that the proposed method surpasses existing techniques in terms of background coupling, text-to-image alignment, and overall visual quality.

Conclusion: The introduced attention-level control method effectively enhances the generation of coupled images by managing the relationship between shared backgrounds and unique objects.

Abstract: We provide an attention-level control method for the task of coupled image
generation, where "coupled" means that multiple simultaneously generated images
are expected to have the same or very similar backgrounds. While backgrounds
coupled, the centered objects in the generated images are still expected to
enjoy the flexibility raised from different text prompts. The proposed method
disentangles the background and entity components in the model's
cross-attention modules, attached with a sequence of time-varying weight
control parameters depending on the time step of sampling. We optimize this
sequence of weight control parameters with a combined objective that assesses
how coupled the backgrounds are as well as text-to-image alignment and overall
visual quality. Empirical results demonstrate that our method outperforms
existing approaches across these criteria.

</details>


### [331] [EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery](https://arxiv.org/abs/2506.06830)
*Guankun Wang,Rui Tang,Mengya Xu,Long Bai,Huxin Gao,Hongliang Ren*

Main category: cs.CV

TL;DR: The paper introduces EndoARSS, a multi-task learning framework for endoscopy surgery activity recognition and semantic segmentation. It uses DINOv2 with Low-Rank Adaptation and Spatially-Aware Multi-Scale Attention. Three new datasets are presented, and experiments show significant improvements in accuracy and robustness compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Endoscopic surgery is crucial for minimally invasive procedures but faces challenges due to complex surgical scenes and image feature confusion between targets and background. Traditional deep learning models struggle with cross-activity interference, prompting the need for a more effective solution.

Method: The authors propose EndoARSS, built on the DINOv2 foundation model, incorporating Low-Rank Adaptation for efficient fine-tuning and Task Efficient Shared Low-Rank Adapters to address gradient conflicts across tasks. Additionally, Spatially-Aware Multi-Scale Attention is introduced to enhance feature representation discrimination by enabling cross-spatial learning of global information.

Result: Extensive experiments demonstrate that EndoARSS significantly improves both accuracy and robustness in multiple benchmarks related to endoscopic surgery scenarios, surpassing existing models.

Conclusion: EndoARSS shows great potential in advancing AI-driven endoscopic surgical systems, providing valuable insights for enhancing surgical safety and efficiency.

Abstract: Endoscopic surgery is the gold standard for robotic-assisted minimally
invasive surgery, offering significant advantages in early disease detection
and precise interventions. However, the complexity of surgical scenes,
characterized by high variability in different surgical activity scenarios and
confused image features between targets and the background, presents challenges
for surgical environment understanding. Traditional deep learning models often
struggle with cross-activity interference, leading to suboptimal performance in
each downstream task. To address this limitation, we explore multi-task
learning, which utilizes the interrelated features between tasks to enhance
overall task performance. In this paper, we propose EndoARSS, a novel
multi-task learning framework specifically designed for endoscopy surgery
activity recognition and semantic segmentation. Built upon the DINOv2
foundation model, our approach integrates Low-Rank Adaptation to facilitate
efficient fine-tuning while incorporating Task Efficient Shared Low-Rank
Adapters to mitigate gradient conflicts across diverse tasks. Additionally, we
introduce the Spatially-Aware Multi-Scale Attention that enhances feature
representation discrimination by enabling cross-spatial learning of global
information. In order to evaluate the effectiveness of our framework, we
present three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored
for endoscopic surgery scenarios with detailed annotations for both activity
recognition and semantic segmentation tasks. Extensive experiments demonstrate
that EndoARSS achieves remarkable performance across multiple benchmarks,
significantly improving both accuracy and robustness in comparison to existing
models. These results underscore the potential of EndoARSS to advance AI-driven
endoscopic surgical systems, offering valuable insights for enhancing surgical
safety and efficiency.

</details>


### [332] [Harnessing Vision-Language Models for Time Series Anomaly Detection](https://arxiv.org/abs/2506.06836)
*Zelin He,Sarah Alnegheimish,Matthew Reimherr*

Main category: cs.CV

TL;DR: The paper proposes a two-stage solution for time-series anomaly detection (TSAD) using vision language models (VLMs), including ViT4TS and VLM4TS, which significantly improves accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current TSAD methods lack the visual-temporal reasoning capacity that human experts have, making it difficult to identify contextual anomalies. To address this limitation, the authors explore the use of vision language models (VLMs) for TSAD.

Method: The proposed method consists of two stages: 1) ViT4TS, a vision-screening stage based on a lightweight pretrained vision encoder that localizes candidate anomalies using 2-D time-series representations; 2) VLM4TS, a VLM-based stage that refines the detection by integrating global temporal context and VLM reasoning capacity.

Result: VLM4TS outperforms time-series pretrained and from-scratch baselines in most cases, with a 24.6 percent improvement in F1-max score over the best baseline. It also consistently outperforms existing language-model-based TSAD methods and is 36 times more efficient in token usage.

Conclusion: The proposed two-stage solution effectively leverages the power of VLMs for TSAD, achieving significant improvements in both accuracy and efficiency.

Abstract: Time-series anomaly detection (TSAD) has played a vital role in a variety of
fields, including healthcare, finance, and industrial monitoring. Prior
methods, which mainly focus on training domain-specific models on numerical
data, lack the visual-temporal reasoning capacity that human experts have to
identify contextual anomalies. To fill this gap, we explore a solution based on
vision language models (VLMs). Recent studies have shown the ability of VLMs
for visual reasoning tasks, yet their direct application to time series has
fallen short on both accuracy and efficiency. To harness the power of VLMs for
TSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening
stage built on a relatively lightweight pretrained vision encoder, which
leverages 2-D time-series representations to accurately localize candidate
anomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal
context and VLM reasoning capacity to refine the detection upon the candidates
provided by ViT4TS. We show that without any time-series training, VLM4TS
outperforms time-series pretrained and from-scratch baselines in most cases,
yielding a 24.6 percent improvement in F1-max score over the best baseline.
Moreover, VLM4TS also consistently outperforms existing language-model-based
TSAD methods and is on average 36 times more efficient in token usage.

</details>


### [333] [Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation](https://arxiv.org/abs/2506.06852)
*John Waithaka,Moise Busogi*

Main category: cs.CV

TL;DR: This paper proposes adapting LOCA (Location-aware) for multimodal satellite imagery semantic segmentation, which significantly outperforms existing reconstruction-based self-supervised learning methods.


<details>
  <summary>Details</summary>
Motivation: Semantic segmentation of satellite imagery is crucial for Earth observation applications, but remains constrained by limited labelled training data. Self-supervised pretraining methods like Masked Autoencoders (MAE) focus on reconstruction rather than localisation, which is a fundamental aspect of segmentation tasks.

Method: The approach adapts LOCA (Location-aware) for multimodal satellite imagery semantic segmentation. It extends SatMAE's channel grouping from multispectral to multimodal data, introduces same-group attention masking to encourage cross-modal interaction during pretraining, and uses relative patch position prediction to encourage spatial reasoning for localisation rather than reconstruction.

Result: Evaluated on the Sen1Floods11 flood mapping dataset, the proposed approach significantly outperforms existing reconstruction-based self-supervised learning methods for satellite imagery.

Conclusion: Position prediction tasks, when properly adapted for multimodal satellite imagery, learn representations more effective for satellite image semantic segmentation than reconstruction-based approaches.

Abstract: Semantic segmentation of satellite imagery is crucial for Earth observation
applications, but remains constrained by limited labelled training data. While
self-supervised pretraining methods like Masked Autoencoders (MAE) have shown
promise, they focus on reconstruction rather than localisation-a fundamental
aspect of segmentation tasks. We propose adapting LOCA (Location-aware), a
position prediction self-supervised learning method, for multimodal satellite
imagery semantic segmentation. Our approach addresses the unique challenges of
satellite data by extending SatMAE's channel grouping from multispectral to
multimodal data, enabling effective handling of multiple modalities, and
introducing same-group attention masking to encourage cross-modal interaction
during pretraining. The method uses relative patch position prediction,
encouraging spatial reasoning for localisation rather than reconstruction. We
evaluate our approach on the Sen1Floods11 flood mapping dataset, where it
significantly outperforms existing reconstruction-based self-supervised
learning methods for satellite imagery. Our results demonstrate that position
prediction tasks, when properly adapted for multimodal satellite imagery, learn
representations more effective for satellite image semantic segmentation than
reconstruction-based approaches.

</details>


### [334] [Face recognition on point cloud with cgan-top for denoising](https://arxiv.org/abs/2506.06864)
*Junyu Liu,Jianfeng Ren,Sunhong Liang,Xudong Jiang*

Main category: cs.CV

TL;DR: The paper presents an end-to-end 3D face recognition system for noisy point clouds, combining a denoising module (cGAN-TOP) and a recognition module (LDGCNN), improving accuracy by up to 14.81%.


<details>
  <summary>Details</summary>
Motivation: 3D face recognition using point clouds is increasingly important but raw data often contains noise due to imperfect sensors, which affects recognition accuracy.

Method: An end-to-end system synergistically integrating denoising and recognition modules: cGAN-TOP for denoising and LDGCNN for recognizing faces from processed point clouds.

Result: Validated on the Bosphorus dataset, the method significantly improves recognition accuracy under all noise settings, with a maximum gain of 14.81%.

Conclusion: The proposed approach effectively handles noise in 3D point clouds and enhances face recognition accuracy.

Abstract: Face recognition using 3D point clouds is gaining growing interest, while raw
point clouds often contain a significant amount of noise due to imperfect
sensors. In this paper, an end-to-end 3D face recognition on a noisy point
cloud is proposed, which synergistically integrates the denoising and
recognition modules. Specifically, a Conditional Generative Adversarial Network
on Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the
noise in the point cloud, and recover the underlying features for subsequent
recognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is
then adapted to recognize faces from the processed point cloud, which
hierarchically links both the local point features and neighboring features of
multiple scales. The proposed method is validated on the Bosphorus dataset. It
significantly improves the recognition accuracy under all noise settings, with
a maximum gain of 14.81%.

</details>


### [335] [Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences](https://arxiv.org/abs/2506.06944)
*Mellon M. Zhang,Glen Chou,Saibal Mukhopadhyay*

Main category: cs.CV

TL;DR: PHiM is a new SSM architecture for polar-coordinate streaming LiDAR that surpasses previous methods in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Conventional object detection methods for LiDAR data either process full 360° scans, introducing significant delay, or rely on translation-invariant convolutions that are misaligned with polar geometry. Existing Mamba-based models work well for full-scan settings but are memory-intensive and not suitable for streaming data.

Method: The proposed Polar Hierarchical Mamba (PHiM) uses local bidirectional Mamba blocks for intra-sector spatial encoding and a global forward Mamba for inter-sector temporal modeling. It replaces convolutions and positional encodings with distortion-aware, dimensionally-decomposed operations tailored to the polar coordinate system.

Result: PHiM establishes a new state-of-the-art among streaming detectors on the Waymo Open Dataset, achieving a 10% improvement over the previous best method while matching the performance of full-scan baselines at twice the throughput.

Conclusion: PHiM demonstrates superior performance and efficiency for polar-coordinate streaming LiDAR data, setting a new benchmark in the field.

Abstract: Accurate and efficient object detection is essential for autonomous vehicles,
where real-time perception requires low latency and high throughput. LiDAR
sensors provide robust depth information, but conventional methods process full
360{\deg} scans in a single pass, introducing significant delay. Streaming
approaches address this by sequentially processing partial scans in the native
polar coordinate system, yet they rely on translation-invariant convolutions
that are misaligned with polar geometry -- resulting in degraded performance or
requiring complex distortion mitigation. Recent Mamba-based state space models
(SSMs) have shown promise for LiDAR perception, but only in the full-scan
setting, relying on geometric serialization and positional embeddings that are
memory-intensive and ill-suited to streaming. We propose Polar Hierarchical
Mamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming
LiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial
encoding and a global forward Mamba for inter-sector temporal modeling,
replacing convolutions and positional encodings with distortion-aware,
dimensionally-decomposed operations. PHiM sets a new state-of-the-art among
streaming detectors on the Waymo Open Dataset, outperforming the previous best
by 10\% and matching full-scan baselines at twice the throughput. Code will be
available at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .

</details>


### [336] [MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks](https://arxiv.org/abs/2506.07016)
*Sanjoy Chowdhury,Mohamed Elmoghany,Yohan Abeysinghe,Junjie Fei,Sayan Nag,Salman Khan,Mohamed Elhoseiny,Dinesh Manocha*

Main category: cs.CV

TL;DR: 大型多模态模型(LMMs)在视听理解方面取得了显著进展，但在需要跨大量视频进行复杂推理的实际场景中表现不佳。现有的视频问答基准测试范围有限，通常每个查询涉及一个片段，无法充分代表实际应用中遇到的大规模视听检索和推理挑战。为了解决这个问题，我们提出了一个新的任务AV-HaystacksQA，目标是根据查询识别不同视频中的显着片段并将它们链接起来以生成最 informative 的答案。为此，我们提出了AVHaystacks，这是一个包含3100个注释的QA对的视听基准，用于评估LMMs在多视频检索和时间接地任务中的能力。此外，我们提出了一种模型不可知的多代理框架MAGNET，以应对这一挑战，在我们提出的AVHaystacks上的QA任务中，相对于基线方法在BLEU@4和GPT评估分数上分别提高了89%和65%。为了实现稳健的多视频检索和时间接地评估，以生成最佳响应，我们引入了两个新指标STEM和MTGS，以促进平衡和可解释的段级接地性能评估。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在视听理解方面取得了显著进展，但它们在需要跨大量视频进行复杂推理的实际场景中表现不佳。现有的视频问答基准测试范围有限，通常每个查询涉及一个片段，无法充分代表实际应用中遇到的大规模视听检索和推理挑战。因此，需要一个更复杂、更具代表性的任务和基准来评估这些模型的能力。

Method: 1. 提出了一个新的任务AV-HaystacksQA，目标是根据查询识别不同视频中的显着片段并将它们链接起来以生成最 informative 的答案。2. 构建了一个新的视听基准AVHaystacks，包含3100个注释的QA对，用于评估LMMs在多视频检索和时间接地任务中的能力。3. 提出了一种模型不可知的多代理框架MAGNET，以应对这一挑战。4. 引入了两个新指标STEM和MTGS，用于平衡和可解释的段级接地性能评估。

Result: 在提出的AVHaystacks基准上的QA任务中，MAGNET框架相对于基线方法在BLEU@4和GPT评估分数上分别实现了89%和65%的相对改进。新提出的STEM和MTGS指标有助于更平衡和可解释地评估段级接地性能。

Conclusion: 本文通过提出AV-HaystacksQA任务和AVHaystacks基准，填补了现有视频问答基准在多视频检索和时间接地方面的空白。同时，提出的MAGNET框架在该任务上显著优于基线方法，证明了其有效性。新引入的STEM和MTGS指标为未来的研究提供了更平衡和可解释的评估标准。

Abstract: Large multimodal models (LMMs) have shown remarkable progress in audio-visual
understanding, yet they struggle with real-world scenarios that require complex
reasoning across extensive video collections. Existing benchmarks for video
question answering remain limited in scope, typically involving one clip per
query, which falls short of representing the challenges of large-scale,
audio-visual retrieval and reasoning encountered in practical applications. To
bridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal
is to identify salient segments across different videos in response to a query
and link them together to generate the most informative answer. To this end, we
present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA
pairs designed to assess the capabilities of LMMs in multi-video retrieval and
temporal grounding task. Additionally, we propose a model-agnostic, multi-agent
framework MAGNET to address this challenge, achieving up to 89% and 65%
relative improvements over baseline methods on BLEU@4 and GPT evaluation scores
in QA task on our proposed AVHaystacks. To enable robust evaluation of
multi-video retrieval and temporal grounding for optimal response generation,
we introduce two new metrics, STEM, which captures alignment errors between a
ground truth and a predicted step sequence and MTGS, to facilitate balanced and
interpretable evaluation of segment-level grounding performance. Project:
https://schowdhury671.github.io/magnet_project/

</details>


### [337] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: This paper explores the use of Multi-modal Large Language Models (MLLMs) for detecting AI-generated images with interpretable explanations, overcoming limitations like hallucination and misalignment with human reasoning. A dataset of AI-generated images is constructed, annotated with bounding boxes and captions highlighting synthesis artifacts. MLLMs are finetuned using a multi-stage optimization strategy to balance detection accuracy, visual localization, and coherent textual explanation. The resultant model outperforms baseline methods in detecting AI-generated images and localizing visual flaws.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the increasing demand for interpretable and robust detection methods for AI-generated images as image generation technologies advance rapidly. Existing approaches lack transparency and human-understandable justifications.

Method: The method involves constructing a dataset of AI-generated images with annotations of bounding boxes and descriptive captions that emphasize synthesis artifacts. Then, MLLMs are finetuned using a multi-stage optimization strategy that balances accurate detection, visual localization, and coherent textual explanation.

Result: The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly surpassing baseline methods.

Conclusion: In conclusion, by finetuning MLLMs with a specially constructed dataset and a multi-stage optimization strategy, the paper demonstrates a successful approach to detect AI-generated images with meaningful explanations, addressing issues such as hallucination and misalignment with human reasoning.

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [338] [Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images](https://arxiv.org/abs/2506.06389)
*Rifat Sadik,Tanvir Rahman,Arpan Bhattacharjee,Bikash Chandra Halder,Ismail Hossain*

Main category: cs.CV

TL;DR: In this paper, authors investigate the vulnerability of Vision Transformers (ViTs) to adversarial watermarking in medical images and explore defense mechanisms.


<details>
  <summary>Details</summary>
Motivation: With the rise of transformer-based models like Vision Transformers (ViTs), which have shown success in computer vision tasks, there is a need to understand their susceptibility to adversarial attacks in medical image analysis.

Method: The authors use Projected Gradient Descent (PGD) to generate adversarial watermarks and test the transferability of these attacks to CNNs. They also analyze the effectiveness of adversarial training as a defense mechanism.

Result: Results show that ViTs are more vulnerable to adversarial attacks with an accuracy drop to 27.6%, but adversarial training can improve the accuracy up to 90.0%. Clean image performance remains unaffected.

Conclusion: Vision Transformers are susceptible to adversarial watermarking attacks, but adversarial training provides an effective defense mechanism.

Abstract: Deep learning models have shown remarkable success in dermatological image
analysis, offering potential for automated skin disease diagnosis. Previously,
convolutional neural network(CNN) based architectures have achieved immense
popularity and success in computer vision (CV) based task like skin image
recognition, generation and video analysis. But with the emergence of
transformer based models, CV tasks are now are nowadays carrying out using
these models. Vision Transformers (ViTs) is such a transformer-based models
that have shown success in computer vision. It uses self-attention mechanisms
to achieve state-of-the-art performance across various tasks. However, their
reliance on global attention mechanisms makes them susceptible to adversarial
perturbations. This paper aims to investigate the susceptibility of ViTs for
medical images to adversarial watermarking-a method that adds so-called
imperceptible perturbations in order to fool models. By generating adversarial
watermarks through Projected Gradient Descent (PGD), we examine the
transferability of such attacks to CNNs and analyze the performance defense
mechanism -- adversarial training. Results indicate that while performance is
not compromised for clean images, ViTs certainly become much more vulnerable to
adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,
adversarial training raises it up to 90.0%.

</details>


### [339] [Image segmentation and classification of E-waste for waste segregation](https://arxiv.org/abs/2506.07122)
*Prakriti Tripathi,Theertha Biju,Maniram Thota,Rakesh Lingam*

Main category: cs.CV

TL;DR: The paper addresses the problem of classifying electronic waste using machine learning models for use by pick-and-place robots. A custom dataset was created, and both YOLOv11 and Mask-RCNN models were trained, achieving 70 mAP and 41 mAP respectively.


<details>
  <summary>Details</summary>
Motivation: To develop a machine learning model that can classify electronic waste to be used by pick-and-place robots for waste segregation.

Method: Creating a custom dataset by taking pictures of unsoldered common electronic waste items, then training YOLOv11 and Mask-RCNN models on this dataset.

Result: YOLOv11 achieved 70 mAP in real-time, while Mask-RCNN achieved 41 mAP.

Conclusion: The machine learning models will be integrated with pick-and-place robots for e-waste segregation.

Abstract: Industry partners provided a problem statement that involves classifying
electronic waste using machine learning models that will be used by
pick-and-place robots for waste segregation. We started by taking common
electronic waste items, such as a mouse and charger, unsoldering them, and
taking pictures to create a custom dataset. Then state-of-the art YOLOv11 model
was trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also
trained and achieved 41 mAP. The model will be further integrated with
pick-and-place robots to perform segregation of e-waste.

</details>


### [340] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: Large multimodal models face computational challenges. This paper proposes Spatial Token Fusion (STF) and Multi-Block Token Fusion (MBTF) methods to reduce vision token sequences while preserving information, leading to faster inference without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost of large multimodal models due to large language models and quadratic complexity in processing long vision token sequences.

Method: The paper introduces two methods: Spatial Token Fusion (STF) which fuses spatial-adjacent tokens into one to shorten vision token sequences, and Multi-Block Token Fusion (MBTF) which supplements multi-granularity features for reduced token sequences.

Result: Experiments show that using only 25% of the baseline's vision tokens, the proposed method achieves comparable or better performance on 8 popular vision-language benchmarks based on LLaVA-1.5.

Conclusion: The combination of STF and MBTF improves inference efficiency without losing multimodal reasoning capabilities.

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [341] [Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models](https://arxiv.org/abs/2506.07177)
*Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Jaehong Yoon,Soo Ye Kim,Zhe Lin,Sung Ju Hwang*

Main category: cs.CV

TL;DR: Frame Guidance是一种无需训练的指导方法，通过帧级信号（如关键帧、风格参考图像、草图或深度图）实现可控视频生成，提出了一种简单潜在处理方法减少内存使用，并采用新的潜在优化策略确保全局连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的可控视频生成方法多依赖于针对特定任务微调大规模视频模型，但随着模型规模增大，这种方法变得不切实际。因此需要一种无需训练且能有效控制视频生成的方法。

Method: 提出Frame Guidance方法，基于帧级信号（如关键帧、风格参考图像、草图或深度图）进行可控视频生成；采用简单的潜在处理方法降低内存使用；应用新的潜在优化策略保证全局连贯性。

Result: 实验结果表明，Frame Guidance可以为广泛的任务和输入信号生成高质量的受控视频，包括关键帧引导、风格化和循环等任务。

Conclusion: Frame Guidance提供了一种无需训练的通用解决方案，适用于各种视频生成任务和输入信号，能够生成高质量且可控的视频。

Abstract: Advancements in diffusion models have significantly improved video quality,
directing attention to fine-grained controllability. However, many existing
methods depend on fine-tuning large-scale video models for specific tasks,
which becomes increasingly impractical as model sizes continue to grow. In this
work, we present Frame Guidance, a training-free guidance for controllable
video generation based on frame-level signals, such as keyframes, style
reference images, sketches, or depth maps. For practical training-free
guidance, we propose a simple latent processing method that dramatically
reduces memory usage, and apply a novel latent optimization strategy designed
for globally coherent video generation. Frame Guidance enables effective
control across diverse tasks, including keyframe guidance, stylization, and
looping, without any training, compatible with any video models. Experimental
results show that Frame Guidance can produce high-quality controlled videos for
a wide range of tasks and input signals.

</details>


### [342] [Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery](https://arxiv.org/abs/2506.06667)
*Yu-Hsuan Ho,Ali Mostafavi*

Main category: cs.CV

TL;DR: This paper presents Flood-DamageSense, a new deep-learning framework for building-level flood-damage assessment that improves on existing models by up to 19 percentage points.


<details>
  <summary>Details</summary>
Motivation: Most post-disaster damage classifiers perform poorly at identifying flood-related building damages due to the lack of clear spectral or structural signatures after inundation.

Method: The model fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical basemaps and an inherent flood-risk layer. A multimodal Mamba backbone with a semi-Siamese encoder and task-specific decoders jointly predicts graded building-damage states, floodwater extent, and building footprints.

Result: Training and evaluation on Hurricane Harvey imagery show a mean F1 improvement of up to 19 percentage points over state-of-the-art baselines, with the largest gains in the frequently misclassified 'minor' and 'moderate' damage categories.

Conclusion: Flood-DamageSense delivers faster, finer-grained, and more reliable flood-damage intelligence to support post-disaster decision-making and resource allocation.

Abstract: Most post-disaster damage classifiers succeed only when destructive forces
leave clear spectral or structural signatures -- conditions rarely present
after inundation. Consequently, existing models perform poorly at identifying
flood-related building damages. The model presented in this study,
Flood-DamageSense, addresses this gap as the first deep-learning framework
purpose-built for building-level flood-damage assessment. The architecture
fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical
basemaps and an inherent flood-risk layer that encodes long-term exposure
probabilities, guiding the network toward plausibly affected structures even
when compositional change is minimal. A multimodal Mamba backbone with a
semi-Siamese encoder and task-specific decoders jointly predicts (1) graded
building-damage states, (2) floodwater extent, and (3) building footprints.
Training and evaluation on Hurricane Harvey (2017) imagery from Harris County,
Texas -- supported by insurance-derived property-damage extents -- show a mean
F1 improvement of up to 19 percentage points over state-of-the-art baselines,
with the largest gains in the frequently misclassified "minor" and "moderate"
damage categories. Ablation studies identify the inherent-risk feature as the
single most significant contributor to this performance boost. An end-to-end
post-processing pipeline converts pixel-level outputs to actionable,
building-scale damage maps within minutes of image acquisition. By combining
risk-aware modeling with SAR's all-weather capability, Flood-DamageSense
delivers faster, finer-grained, and more reliable flood-damage intelligence to
support post-disaster decision-making and resource allocation.

</details>


### [343] [Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment](https://arxiv.org/abs/2506.06680)
*Radha Kodali,Venkata Rao Dhulipalla,Venkata Siva Kishor Tatavarty,Madhavi Nadakuditi,Bharadwaj Thiruveedhula,Suryanarayana Gunnam,Durga Prasad Bavirisetti*

Main category: cs.CV

TL;DR: An explainable AI framework using CNN-LSTM is introduced for embryo classification in IVF to improve efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Infertility affects individuals' quality of life and is expected to rise. Current embryo grading by expert embryologists is time-consuming and inefficient.

Method: The study employs a fusion of convolutional neural network (CNN) and long short-term memory (LSTM) architecture, known as CNN-LSTM, within an explainable artificial intelligence (XAI) framework for classifying embryos based on blastocyst images.

Result: The model achieves high accuracy in embryo classification while ensuring interpretability through the use of XAI.

Conclusion: The introduced XAI framework with CNN-LSTM offers a promising solution for improving the efficiency and accuracy of embryo selection in IVF.

Abstract: Infertility has a considerable impact on individuals' quality of life,
affecting them socially and psychologically, with projections indicating a rise
in the upcoming years. In vitro fertilization (IVF) emerges as one of the
primary techniques within economically developed nations, employed to address
the rising problem of low fertility. Expert embryologists conventionally grade
embryos by reviewing blastocyst images to select the most optimal for transfer,
yet this process is time-consuming and lacks efficiency. Blastocyst images
provide a valuable resource for assessing embryo viability. In this study, we
introduce an explainable artificial intelligence (XAI) framework for
classifying embryos, employing a fusion of convolutional neural network (CNN)
and long short-term memory (LSTM) architecture, referred to as CNN-LSTM.
Utilizing deep learning, our model achieves high accuracy in embryo
classification while maintaining interpretability through XAI.

</details>


### [344] [From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models](https://arxiv.org/abs/2506.07280)
*Pablo Acuaviva,Aram Davtyan,Mariam Hassan,Sebastian Stapf,Ahmad Rahimi,Alexandre Alahi,Paolo Favaro*

Main category: cs.CV

TL;DR: Video Diffusion Models (VDMs) can be repurposed for various tasks beyond video generation through few-shot fine-tuning, showcasing their potential as adaptable visual learners.


<details>
  <summary>Details</summary>
Motivation: To explore the internal knowledge and structured representations that VDMs acquire during training, and to demonstrate their potential beyond video generation.

Method: Introduce a few-shot fine-tuning framework that transforms tasks into visual transitions, enabling training of LoRA weights on short input-output sequences without altering the generative interface of a frozen VDM.

Result: The model exhibits strong generalization across diverse tasks, including low-level vision tasks like segmentation and pose estimation, as well as high-level reasoning tasks like ARC-AGI, despite minimal supervision.

Conclusion: VDMs are more than generative engines; they are adaptable visual learners with the potential to serve as the backbone for future foundation models in vision.

Abstract: Video Diffusion Models (VDMs) have emerged as powerful generative tools,
capable of synthesizing high-quality spatiotemporal content. Yet, their
potential goes far beyond mere video generation. We argue that the training
dynamics of VDMs, driven by the need to model coherent sequences, naturally
pushes them to internalize structured representations and an implicit
understanding of the visual world. To probe the extent of this internal
knowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs
for new tasks using only a handful of examples. Our method transforms each task
into a visual transition, enabling the training of LoRA weights on short
input-output sequences without altering the generative interface of a frozen
VDM. Despite minimal supervision, the model exhibits strong generalization
across diverse tasks, from low-level vision (for example, segmentation and pose
estimation) to high-level reasoning (for example, on ARC-AGI). These results
reframe VDMs as more than generative engines. They are adaptable visual
learners with the potential to serve as the backbone for future foundation
models in vision.

</details>


### [345] [Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations](https://arxiv.org/abs/2506.06780)
*Lennart Bastian,Mohammad Rashed,Nassir Navab,Tolga Birdal*

Main category: cs.CV

TL;DR: This paper presents a method for modeling continuous-time rotational object dynamics on SO(3) using Neural Controlled Differential Equations guided by Savitzky-Golay paths, which learns a general latent dynamical system of the underlying object trajectory while respecting the geometric structure of rotations.


<details>
  <summary>Details</summary>
Motivation: Tracking and forecasting the rotation of objects is fundamental in computer vision and robotics, but SO(3) extrapolation remains challenging due to noisy and sparse sensor observations, complex motion dynamics, and long-term forecasting demands.

Method: The method proposed in this paper models continuous-time rotational object dynamics on SO(3) using Neural Controlled Differential Equations guided by Savitzky-Golay paths. It learns a general latent dynamical system of the underlying object trajectory while respecting the geometric structure of rotations.

Result: Experimental results on real-world data demonstrate compelling forecasting capabilities compared to existing approaches.

Conclusion: The proposed method shows promising results in modeling continuous-time rotational object dynamics on SO(3) and provides better forecasting capabilities than existing methods.

Abstract: Tracking and forecasting the rotation of objects is fundamental in computer
vision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor
observations can be noisy and sparse, (2) motion patterns can be governed by
complex dynamics, and (3) application settings can demand long-term
forecasting. This work proposes modeling continuous-time rotational object
dynamics on $SO(3)$ using Neural Controlled Differential Equations guided by
Savitzky-Golay paths. Unlike existing methods that rely on simplified motion
assumptions, our method learns a general latent dynamical system of the
underlying object trajectory while respecting the geometric structure of
rotations. Experimental results on real-world data demonstrate compelling
forecasting capabilities compared to existing approaches.

</details>


### [346] [NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery](https://arxiv.org/abs/2506.06898)
*Reese Kneeland,Paul S. Scotti,Ghislain St-Yves,Jesse Breedlove,Kendrick Kay,Thomas Naselaris*

Main category: cs.CV

TL;DR: 研究人员发布了NSD-Imagery数据集，用于评估现有模型在心理图像重建上的表现。通过基准测试发现，简单线性解码架构和多模态特征解码的模型在心理图像上的泛化性能更好，而复杂架构容易过拟合视觉训练数据。这表明心理图像数据集对于开发实际应用的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的NSD数据集主要用于视觉图像重建，但未涉及心理图像的重建。为了评估模型在心理图像重建上的能力，并推动医疗和脑机接口等领域的实际应用，需要一个专门的数据集来支持这一研究方向。

Method: 发布了一个名为NSD-Imagery的新数据集，该数据集包含人类fMRI活动与心理图像的配对数据。利用此数据集，对多个近期公开的视觉解码模型（MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.）进行了基准测试，评估它们在心理图像重建上的表现。

Result: 测试结果显示，解码方法在心理图像上的表现与其在视觉重建上的表现很大程度上是脱钩的。此外，模型的架构选择显著影响跨解码性能：简单的线性解码架构和多模态特征解码模型在心理图像上泛化得更好，而复杂架构则倾向于过拟合视觉训练数据。

Conclusion: 心理图像数据集对于开发实际应用至关重要，NSD-Imagery为改进视觉解码方法以更好地实现这一目标提供了一个有用的资源。

Abstract: We release NSD-Imagery, a benchmark dataset of human fMRI activity paired
with mental images, to complement the existing Natural Scenes Dataset (NSD), a
large-scale dataset of fMRI activity paired with seen images that enabled
unprecedented improvements in fMRI-to-image reconstruction efforts. Recent
models trained on NSD have been evaluated only on seen image reconstruction.
Using NSD-Imagery, it is possible to assess how well these models perform on
mental image reconstruction. This is a challenging generalization requirement
because mental images are encoded in human brain activity with relatively lower
signal-to-noise and spatial resolution; however, generalization from seen to
mental imagery is critical for real-world applications in medical domains and
brain-computer interfaces, where the desired information is always internally
generated. We provide benchmarks for a suite of recent NSD-trained open-source
visual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et
al.) on NSD-Imagery, and show that the performance of decoding methods on
mental images is largely decoupled from performance on vision reconstruction.
We further demonstrate that architectural choices significantly impact
cross-decoding performance: models employing simple linear decoding
architectures and multimodal feature decoding generalize better to mental
imagery, while complex architectures tend to overfit visual training data. Our
findings indicate that mental imagery datasets are critical for the development
of practical applications, and establish NSD-Imagery as a useful resource for
better aligning visual decoding methods with this goal.

</details>


### [347] [Multiple Object Stitching for Unsupervised Representation Learning](https://arxiv.org/abs/2506.07364)
*Chengchao Shen,Dawei Liu,Jianxin Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Multiple Object Stitching (MOS)的新方法，通过将单对象图像缝合为多对象图像来改进无监督表示学习，从而在多对象图像上实现更好的表现。实验结果表明，该方法在ImageNet、CIFAR和COCO数据集上均取得了领先的无监督表示性能。


<details>
  <summary>Details</summary>
Motivation: 对比学习在单对象中心图像的无监督表示上取得了显著进展，但在包含多个对象的广泛图像上的表现较差。为了提高多对象图像的无监督表示性能，需要一种新方法。

Method: 提出了一种简单而有效的方法——Multiple Object Stitching (MOS)，通过将单对象中心图像缝合成多对象图像，其中合成图像中的对象是预先确定的。这种方法无需人工注释即可提供多对象图像之间的额外对象对应关系。

Result: 实验结果表明，该方法在ImageNet、CIFAR和COCO数据集上均取得了领先的无监督表示性能，适用于单对象中心图像和多对象图像。

Conclusion: Multiple Object Stitching (MOS) 方法可以有效提高多对象图像的无监督表示性能，为复杂的下游任务（如对象检测和语义分割）提供了更详细的表示。

Abstract: Contrastive learning for single object centric images has achieved remarkable
progress on unsupervised representation, but suffering inferior performance on
the widespread images with multiple objects. In this paper, we propose a simple
but effective method, Multiple Object Stitching (MOS), to refine the
unsupervised representation for multi-object images. Specifically, we construct
the multi-object images by stitching the single object centric ones, where the
objects in the synthesized multi-object images are predetermined. Hence,
compared to the existing contrastive methods, our method provides additional
object correspondences between multi-object images without human annotations.
In this manner, our method pays more attention to the representations of each
object in multi-object image, thus providing more detailed representations for
complicated downstream tasks, such as object detection and semantic
segmentation. Experimental results on ImageNet, CIFAR and COCO datasets
demonstrate that our proposed method achieves the leading unsupervised
representation performance on both single object centric images and
multi-object ones. The source code is available at
https://github.com/visresearch/MultipleObjectStitching.

</details>


### [348] [C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.07368)
*Jiaying He,Yitong Lin,Jiahe Chen,Honghui Xu,Jianwei Zheng*

Main category: cs.CV

TL;DR: In response to the challenge of insufficient annotations in medical imaging, this paper presents C3S3, a semi-supervised segmentation model enhancing boundary delineation and overall precision via Outcome-Driven Contrastive Learning and Dynamic Complementary Competition modules. It outperforms previous methods by at least 6% on key metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing the issue of insufficiently annotated samples in medical imaging and improving the precision of boundary delineation in semi-supervised medical image segmentation beyond current methodologies.

Method: C3S3 integrates complementary competition and contrastive selection. It features an Outcome-Driven Contrastive Learning module for refining boundary localization and a Dynamic Complementary Competition module using two sub-networks to generate pseudo-labels, thereby improving segmentation quality.

Result: C3S3 demonstrates superior performance compared to previous state-of-the-art methods, with a significant improvement of at least 6% on the 95HD and ASD metrics when validated on public MRI and CT datasets.

Conclusion: C3S3 successfully enhances the precision of medical image segmentation, particularly in boundary delineation, offering a significant advancement over prior art.

Abstract: For the immanent challenge of insufficiently annotated samples in the medical
field, semi-supervised medical image segmentation (SSMIS) offers a promising
solution. Despite achieving impressive results in delineating primary target
areas, most current methodologies struggle to precisely capture the subtle
details of boundaries. This deficiency often leads to significant diagnostic
inaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised
segmentation model that synergistically integrates complementary competition
and contrastive selection. This design significantly sharpens boundary
delineation and enhances overall precision. Specifically, we develop an
$\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining
boundary localization. Additionally, we incorporate a $\textit{Dynamic
Complementary Competition}$ module that leverages two high-performing
sub-networks to generate pseudo-labels, thereby further improving segmentation
quality. The proposed C3S3 undergoes rigorous validation on two publicly
accessible datasets, encompassing the practices of both MRI and CT scans. The
results demonstrate that our method achieves superior performance compared to
previous cutting-edge competitors. Especially, on the 95HD and ASD metrics, our
approach achieves a notable improvement of at least $6\%$, highlighting the
significant advancements. The code is available at
https://github.com/Y-TARL/C3S3.

</details>


### [349] [Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation](https://arxiv.org/abs/2506.07376)
*Jintao Tong,Ran Ma,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: The paper proposes Domain Feature Navigator (DFN) for cross-domain few-shot segmentation, which surpasses state-of-the-art methods by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios respectively.


<details>
  <summary>Details</summary>
Motivation: Existing methods for cross-domain few-shot segmentation face challenges with domain gaps and fine-tuning with scarce data.

Method: The paper revisits adapter-based methods and proposes DFN, a structure-based decoupler to capture domain-specific information while directing the model's attention towards domain-agnostic knowledge. It also introduces SAM-SVN to prevent overfitting during source-domain training.

Result: The method significantly outperforms state-of-the-art techniques with improvements of 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios respectively.

Conclusion: DFN is an effective approach for cross-domain few-shot segmentation that addresses domain gaps and limited data challenges.

Abstract: Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the
model on a source-domain dataset with sufficient samples, and then transfer the
model to target-domain datasets where only a few samples are available for
efficient fine-tuning. There are majorly two challenges in this task: (1) the
domain gap and (2) fine-tuning with scarce data. To solve these challenges, we
revisit the adapter-based methods, and discover an intriguing insight not
explored in previous works: the adapter not only helps the fine-tuning of
downstream tasks but also naturally serves as a domain information decoupler.
Then, we delve into this finding for an interpretation, and find the model's
inherent structure could lead to a natural decoupling of domain information.
Building upon this insight, we propose the Domain Feature Navigator (DFN),
which is a structure-based decoupler instead of loss-based ones like current
works, to capture domain-specific information, thereby directing the model's
attention towards domain-agnostic knowledge. Moreover, to prevent the potential
excessive overfitting of DFN during the source-domain training, we further
design the SAM-SVN method to constrain DFN from learning sample-specific
knowledge. On target domains, we freeze the model and fine-tune the DFN to
learn target-specific knowledge specific. Extensive experiments demonstrate
that our method surpasses the state-of-the-art method in CD-FSS significantly
by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.

</details>


### [350] [TABLET: Table Structure Recognition using Encoder-only Transformers](https://arxiv.org/abs/2506.07015)
*Qiyu Hou,Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种新的Split-Merge模型，优化了对大而密集表格的结构识别，通过消除不稳定边界框预测，减少分辨率损失和计算复杂度，保持高准确性和快速处理速度。在FinTabNet和PubTabNet上的实验表明该模型优于现有方法，适合工业部署。


<details>
  <summary>Details</summary>
Motivation: 当前表格结构识别面临挑战，特别是对于大且密集的表格，需要更精确和高效的方法。

Method: 将行和列分割视为序列标注任务，使用双Transformer编码器捕捉特征交互；将合并过程作为网格单元分类任务，使用额外的Transformer编码器确保准确和一致的合并。

Result: 在FinTabNet和PubTabNet数据集上进行的广泛实验证明，该模型比现有方法具有优势，特别是在实际应用中表现优异。

Conclusion: 本方法为大规模表格识别提供了强大、可扩展和高效的解决方案，非常适合工业部署。

Abstract: To address the challenges of table structure recognition, we propose a novel
Split-Merge-based top-down model optimized for large, densely populated tables.
Our approach formulates row and column splitting as sequence labeling tasks,
utilizing dual Transformer encoders to capture feature interactions. The
merging process is framed as a grid cell classification task, leveraging an
additional Transformer encoder to ensure accurate and coherent merging. By
eliminating unstable bounding box predictions, our method reduces resolution
loss and computational complexity, achieving high accuracy while maintaining
fast processing speed. Extensive experiments on FinTabNet and PubTabNet
demonstrate the superiority of our model over existing approaches, particularly
in real-world applications. Our method offers a robust, scalable, and efficient
solution for large-scale table recognition, making it well-suited for
industrial deployment.

</details>


### [351] [MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems](https://arxiv.org/abs/2506.07399)
*Peiru Yang,Jinhua Yin,Haoran Zheng,Xueying Bai,Huili Wang,Yufei Sun,Xintian Li,Shangguang Wang,Yongfeng Huang,Tao Qi*

Main category: cs.CV

TL;DR: Multimodal RAG systems, which enhance large vision-language models by integrating cross-modal knowledge, are vulnerable to privacy attacks such as membership inference attacks (MIAs). Existing MIA methods mainly focus on textual modality. This paper proposes MrM, the first black-box MIA framework for multimodal RAG systems, utilizing a multi-object data perturbation framework constrained by counterfactual attacks. It induces RAG systems to retrieve target data and generate information leaking membership details. The method employs an object-aware data perturbation approach and a counterfact-informed mask selection strategy. Experiments show that MrM performs strongly across evaluations and remains robust under defenses.


<details>
  <summary>Details</summary>
Motivation: Multimodal RAG systems are increasingly adopted in real-world tasks but are potentially vulnerable to privacy attacks, especially MIAs. Current MIA methods primarily target the textual modality, leaving the visual modality relatively unexplored.

Method: The proposed MrM framework uses a multi-object data perturbation framework constrained by counterfactual attacks. It includes an object-aware data perturbation method to ensure successful retrieval and a counterfact-informed mask selection strategy to prioritize informative masked regions. Statistical membership inference is performed by modeling query trials.

Result: MrM achieves consistently strong performance in experiments on two visual datasets and eight commercial visual-language models across both sample-level and set-level evaluations. It also remains robust under adaptive defenses.

Conclusion: MrM is the first effective black-box MIA framework for multimodal RAG systems, demonstrating strong performance and robustness against defenses.

Abstract: Multimodal retrieval-augmented generation (RAG) systems enhance large
vision-language models by integrating cross-modal knowledge, enabling their
increasing adoption across real-world multimodal tasks. These knowledge
databases may contain sensitive information that requires privacy protection.
However, multimodal RAG systems inherently grant external users indirect access
to such data, making them potentially vulnerable to privacy attacks,
particularly membership inference attacks (MIAs). % Existing MIA methods
targeting RAG systems predominantly focus on the textual modality, while the
visual modality remains relatively underexplored. To bridge this gap, we
propose MrM, the first black-box MIA framework targeted at multimodal RAG
systems. It utilizes a multi-object data perturbation framework constrained by
counterfactual attacks, which can concurrently induce the RAG systems to
retrieve the target data and generate information that leaks the membership
information. Our method first employs an object-aware data perturbation method
to constrain the perturbation to key semantics and ensure successful retrieval.
Building on this, we design a counterfact-informed mask selection strategy to
prioritize the most informative masked regions, aiming to eliminate the
interference of model self-knowledge and amplify attack efficacy. Finally, we
perform statistical membership inference by modeling query trials to extract
features that reflect the reconstruction of masked semantics from response
patterns. Experiments on two visual datasets and eight mainstream commercial
visual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves
consistently strong performance across both sample-level and set-level
evaluations, and remains robust under adaptive defenses.

</details>


### [352] [FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement](https://arxiv.org/abs/2506.07431)
*Jie He,Minglang Chen,Minying Lu,Bocheng Liang,Junming Wei,Guiyan Peng,Jiaxi Chen,Ying Tan*

Main category: cs.CV

TL;DR: An ultrasound image segmentation model with feature perception and Mamba enhancement is proposed for fetal femur and cranial images, achieving fast loss reduction and superior segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Accurate ultrasound image segmentation is crucial but challenging due to high noise, similarity, and jagged effects in small object segmentation.

Method: The paper introduces FAMSeg network which includes a longitudinal and transverse independent viewpoint scanning convolution block, a feature perception module, and Mamba-optimized residual structure to enhance local detail capturing, improve contextual information fusion, suppress noise interference, and build global-local feature dependencies.

Result: FAMSeg network demonstrated the fastest loss reduction and best segmentation performance on images of different sizes and orientations after extensive experiments.

Conclusion: The proposed model successfully addresses the challenges in ultrasound image segmentation, providing precise biometrics and accurate assessment.

Abstract: Accurate ultrasound image segmentation is a prerequisite for precise
biometrics and accurate assessment. Relying on manual delineation introduces
significant errors and is time-consuming. However, existing segmentation models
are designed based on objects in natural scenes, making them difficult to adapt
to ultrasound objects with high noise and high similarity. This is particularly
evident in small object segmentation, where a pronounced jagged effect occurs.
Therefore, this paper proposes a fetal femur and cranial ultrasound image
segmentation model based on feature perception and Mamba enhancement to address
these challenges. Specifically, a longitudinal and transverse independent
viewpoint scanning convolution block and a feature perception module were
designed to enhance the ability to capture local detail information and improve
the fusion of contextual information. Combined with the Mamba-optimized
residual structure, this design suppresses the interference of raw noise and
enhances local multi-dimensional scanning. The system builds global information
and local feature dependencies, and is trained with a combination of different
optimizers to achieve the optimal solution. After extensive experimental
validation, the FAMSeg network achieved the fastest loss reduction and the best
segmentation performance across images of varying sizes and orientations.

</details>


### [353] [Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition](https://arxiv.org/abs/2506.07436)
*Nishi Chaudhary,S M Jamil Uddin,Sathvik Sharath Chandra,Anto Ovid,Alex Albert*

Main category: cs.CV

TL;DR: 最近多模态大语言模型（LLMs）的出现为改进施工现场的视觉危险识别提供了新机会。这项研究对比评估了五个最先进的LLMs在从真实世界建筑图像中识别潜在危险方面的能力，测试了三种提示策略：零样本、少样本和思维链（CoT）。结果表明，提示策略显著影响性能，CoT提示在所有模型中始终产生更高的准确性。GPT-4.5和GPT-o3在大多数设置下表现优于其他模型。研究强调了提示设计在增强多模态LLMs的准确性和一致性中的关键作用，为实际危险识别中结合提示工程和LLMs提供了可行的见解。


<details>
  <summary>Details</summary>
Motivation: 传统的计算机视觉模型依赖于特定领域的训练和大量数据集，而现代LLMs可以通过简单的自然语言提示解释和描述复杂的视觉场景。然而，关于不同LLMs在建筑领域关键视觉任务中的表现的研究有限，因此需要进行系统评估以填补这一空白。

Method: 对五种最先进的LLMs进行了比较评估，使用三种不同的提示策略（零样本、少样本和思维链）来测试它们从真实世界建筑图像中识别潜在危险的能力。通过精确度、召回率和F1分数等指标进行定量分析，评估不同条件下的模型性能。

Result: 发现提示策略对模型性能有显著影响，其中思维链提示在所有模型中始终产生更高的准确性。此外，不同条件下LLMs的表现各异，GPT-4.5和GPT-o3在大多数设置下表现最佳。

Conclusion: 提示设计在提高多模态LLMs用于建筑安全应用的准确性和一致性方面起着至关重要的作用。本研究为将提示工程与LLMs结合以实现实用的危险识别提供了可行的见解，有助于开发更可靠的AI辅助安全系统。

Abstract: The recent emergence of multimodal large language models (LLMs) has
introduced new opportunities for improving visual hazard recognition on
construction sites. Unlike traditional computer vision models that rely on
domain-specific training and extensive datasets, modern LLMs can interpret and
describe complex visual scenes using simple natural language prompts. However,
despite growing interest in their applications, there has been limited
investigation into how different LLMs perform in safety-critical visual tasks
within the construction domain. To address this gap, this study conducts a
comparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,
GPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify
potential hazards from real-world construction images. Each model was tested
under three prompting strategies: zero-shot, few-shot, and chain-of-thought
(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated
basic safety context and a hazard source mnemonic, and CoT provided
step-by-step reasoning examples to scaffold model thinking. Quantitative
analysis was performed using precision, recall, and F1-score metrics across all
conditions. Results reveal that prompting strategy significantly influenced
performance, with CoT prompting consistently producing higher accuracy across
models. Additionally, LLM performance varied under different conditions, with
GPT-4.5 and GPT-o3 outperforming others in most settings. The findings also
demonstrate the critical role of prompt design in enhancing the accuracy and
consistency of multimodal LLMs for construction safety applications. This study
offers actionable insights into the integration of prompt engineering and LLMs
for practical hazard recognition, contributing to the development of more
reliable AI-assisted safety systems.

</details>


### [354] [Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI](https://arxiv.org/abs/2506.07286)
*Aditya Chakravarty*

Main category: cs.CV

TL;DR: 通过在每个去噪步骤中应用多步优化策略，改进了扩散模型解决逆问题的能力，特别是在超分辨率和高斯去模糊任务上。实验表明，增加每步的梯度更新次数能提高图像质量和泛化能力，且延迟开销小。此方法在Jetson Orin Nano上的验证显示，原本在人脸数据集上训练的MPGD模型也能有效推广到自然和航拍场景，具有作为轻量级实时视觉感知修复模块的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型方法（如MPGD）在每次去噪步骤中仅应用单个梯度更新，限制了恢复的保真度和鲁棒性，尤其是在嵌入式或分布外的设置中。

Method: 引入了一种在每个去噪时间步内进行多步优化的策略，以显著提高图像质量、感知准确性和泛化能力。并通过实验证明增加每步的梯度更新次数可以改善LPIPS和PSNR指标。

Result: 实验结果表明，在超分辨率和高斯去模糊任务中，增加梯度更新次数能够有效提升性能，且延迟开销极小。此外，在Jetson Orin Nano上对退化的ImageNet和UAV数据集的测试进一步验证了该方法的有效性和泛化能力。

Conclusion: 研究结果表明，MPGD作为一种轻量级、即插即用的修复模块，具有在具身AI代理（如无人机和移动机器人）中实现实时视觉感知的潜力。

Abstract: Diffusion models have shown remarkable flexibility for solving inverse
problems without task-specific retraining. However, existing approaches such as
Manifold Preserving Guided Diffusion (MPGD) apply only a single gradient update
per denoising step, limiting restoration fidelity and robustness, especially in
embedded or out-of-distribution settings. In this work, we introduce a
multistep optimization strategy within each denoising timestep, significantly
enhancing image quality, perceptual accuracy, and generalization. Our
experiments on super-resolution and Gaussian deblurring demonstrate that
increasing the number of gradient updates per step improves LPIPS and PSNR with
minimal latency overhead. Notably, we validate this approach on a Jetson Orin
Nano using degraded ImageNet and a UAV dataset, showing that MPGD, originally
trained on face datasets, generalizes effectively to natural and aerial scenes.
Our findings highlight MPGD's potential as a lightweight, plug-and-play
restoration module for real-time visual perception in embodied AI agents such
as drones and mobile robots.

</details>


### [355] ["CASE: Contrastive Activation for Saliency Estimation](https://arxiv.org/abs/2506.07327)
*Dane Williamson,Yangfeng Ji,Matthew Dwyer*

Main category: cs.CV

TL;DR: Saliency methods are widely used but have critical limitations. This work proposes a diagnostic test for class sensitivity, finds many methods unreliable, and introduces CASE, a new method that produces more faithful explanations.


<details>
  <summary>Details</summary>
Motivation: To address the critical limitations of saliency methods that their visual plausibility can obscure, particularly their inability to distinguish between competing class labels on the same input.

Method: Propose a diagnostic test for class sensitivity and introduce CASE, a contrastive explanation method that isolates features uniquely discriminative for the predicted class.

Result: CASE produces faithful and more class-specific explanations than existing methods, as evaluated by the proposed diagnostic and a perturbation-based fidelity test.

Conclusion: Many widely used saliency methods are unreliable due to class-insensitive behavior, and CASE is introduced as a more effective alternative.

Abstract: Saliency methods are widely used to visualize which input features are deemed
relevant to a model's prediction. However, their visual plausibility can
obscure critical limitations. In this work, we propose a diagnostic test for
class sensitivity: a method's ability to distinguish between competing class
labels on the same input. Through extensive experiments, we show that many
widely used saliency methods produce nearly identical explanations regardless
of the class label, calling into question their reliability. We find that
class-insensitive behavior persists across architectures and datasets,
suggesting the failure mode is structural rather than model-specific. Motivated
by these findings, we introduce CASE, a contrastive explanation method that
isolates features uniquely discriminative for the predicted class. We evaluate
CASE using the proposed diagnostic and a perturbation-based fidelity test, and
show that it produces faithful and more class-specific explanations than
existing methods.

</details>


### [356] [DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO](https://arxiv.org/abs/2506.07464)
*Jinyoung Park,Jeehye Na,Jinyoung Kim,Hyunwoo J. Kim*

Main category: cs.CV

TL;DR: DeepVideo-R1是一种针对视频大语言模型的改进方法，通过提出回归式的GRPO（Reg-GRPO）和难度感知的数据增强策略，解决了传统GRPO在应用到视频LLM时遇到的问题，显著提升了多个视频推理基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于强化学习的后训练已被证明可以增强大语言模型的推理能力，但将GRPO应用于视频大语言模型的研究较少。当前方法存在依赖保护机制和优势消失问题，这阻碍了其有效学习。

Method: 提出了一种名为DeepVideo-R1的视频大语言模型，采用回归式的GRPO（Reg-GRPO）和难度感知的数据增强策略。Reg-GRPO将GRPO目标重新定义为回归任务，直接预测优势值，消除了对剪裁等保护机制的需求；数据增强策略动态增加可解决难度级别的样本，提供多样且信息丰富的奖励信号。

Result: 实验表明，DeepVideo-R1在多个视频推理基准上显著提高了视频推理性能。

Conclusion: DeepVideo-R1通过引入Reg-GRPO和难度感知数据增强策略，成功克服了GRPO在视频大语言模型应用中的挑战，大幅提升了视频推理能力。

Abstract: Recent works have demonstrated the effectiveness of reinforcement learning
(RL)-based post-training in enhancing the reasoning capabilities of large
language models (LLMs). In particular, Group Relative Policy Optimization
(GRPO) has shown impressive success by employing a PPO-style reinforcement
algorithm with group-based normalized rewards. However, the application of GRPO
to Video Large Language Models (Video LLMs) has been less studied. In this
paper, we explore GRPO for video LLMs and identify two primary issues that
impede its effective learning: (1) reliance on safeguards, and (2) the
vanishing advantage problem. To mitigate these challenges, we propose
DeepVideo-R1, a video large language model trained with our proposed Reg-GRPO
(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO
reformulates the GRPO objective as a regression task, directly predicting the
advantage in GRPO. This design eliminates the need for safeguards like clipping
and min functions, thereby facilitating more direct policy guidance by aligning
the model with the advantage values. We also design the difficulty-aware data
augmentation strategy that dynamically augments training samples at solvable
difficulty levels, fostering diverse and informative reward signals. Our
comprehensive experiments show that DeepVideo-R1 significantly improves video
reasoning performance across multiple video reasoning benchmarks.

</details>


### [357] [Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval](https://arxiv.org/abs/2506.07471)
*CH Cho,WJ Moon,W Jun,MS Jung,JP Heo*

Main category: cs.CV

TL;DR: Partially Relevant Video Retrieval (PRVR) aims to retrieve videos with specific relevant segments given a text query. The paper proposes Ambiguity-Restrained representation Learning (ARL) to address ambiguity in text-video pairs through multi-positive contrastive learning, dual triplet margin loss, fine-grained relationships within video frames, and cross-model ambiguity detection.


<details>
  <summary>Details</summary>
Motivation: Existing PRVR methods assume a one-to-one relationship between text queries and videos, ignoring inherent ambiguities based on conceptual scope.

Method: ARL detects ambiguous text-video pairs using uncertainty and similarity criteria. It applies multi-positive contrastive learning and dual triplet margin loss for hierarchical semantic relationship learning. Fine-grained relationships within video frames are explored at the text-frame level. Cross-model ambiguity detection mitigates error propagation.

Result: The proposed method effectively addresses ambiguities in PRVR, enhancing retrieval performance.

Conclusion: Incorporating ambiguity into the model learning process significantly improves PRVR effectiveness.

Abstract: Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a
specific segment is relevant to a given text query. Typical training processes
of PRVR assume a one-to-one relationship where each text query is relevant to
only one video. However, we point out the inherent ambiguity between text and
video content based on their conceptual scope and propose a framework that
incorporates this ambiguity into the model learning process. Specifically, we
propose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous
text-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:
uncertainty and similarity. Uncertainty represents whether instances include
commonly shared context across the dataset, while similarity indicates
pair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL
hierarchically learns the semantic relationship via multi-positive contrastive
learning and dual triplet margin loss. Additionally, we delve into fine-grained
relationships within the video instances. Unlike typical training at the
text-video level, where pairwise information is provided, we address the
inherent ambiguity within frames of the same untrimmed video, which often
contains multiple contexts. This allows us to further enhance learning at the
text-frame level. Lastly, we propose cross-model ambiguity detection to
mitigate the error propagation that occurs when a single model is employed to
detect ambiguous pairs for its training. With all components combined, our
proposed method demonstrates its effectiveness in PRVR.

</details>


### [358] [CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization](https://arxiv.org/abs/2506.07484)
*Dasol Hong,Wooju Lee,Hyun Myung*

Main category: cs.CV

TL;DR: The paper introduces CoCoA-Mix, a model using CoA-loss and CoA-weights to improve prompt tuning in vision-language models. It enhances specialization through refined decision boundaries and generalization via confidence-aware weights.


<details>
  <summary>Details</summary>
Motivation: Prompt tuning has been effective for task-specific adaptations but faces challenges with specialization due to misaligned features from frozen encoders and generalization for unseen domains.

Method: The authors propose a confusion-aware loss (CoA-loss) to refine decision boundaries between confusing classes and confidence-aware weights (CoA-weights) to adjust prediction weights in a mixture model, thus improving both specialization and generalization.

Result: Extensive experiments demonstrate that CoCoA-Mix outperforms state-of-the-art methods in enhancing both specialization and generalization.

Conclusion: CoCoA-Mix is an effective approach for prompt tuning in vision-language models, addressing the issues of specialization and generalization.

Abstract: Prompt tuning, which adapts vision-language models by freezing model
parameters and optimizing only the prompt, has proven effective for
task-specific adaptations. The core challenge in prompt tuning is improving
specialization for a specific task and generalization for unseen domains.
However, frozen encoders often produce misaligned features, leading to
confusion between classes and limiting specialization. To overcome this issue,
we propose a confusion-aware loss (CoA-loss) that improves specialization by
refining the decision boundaries between confusing classes. Additionally, we
mathematically demonstrate that a mixture model can enhance generalization
without compromising specialization. This is achieved using confidence-aware
weights (CoA-weights), which adjust the weights of each prediction in the
mixture model based on its confidence within the class domains. Extensive
experiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,
outperforms state-of-the-art methods by enhancing specialization and
generalization. Our code is publicly available at
https://github.com/url-kaist/CoCoA-Mix.

</details>


### [359] [CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms](https://arxiv.org/abs/2506.07357)
*Satvik Praveen,Yoonsung Jung*

Main category: cs.CV

TL;DR: An object detection model named CBAM-STN-TPS-YOLO is proposed for precision agriculture, integrating Thin-Plate Splines (TPS) into Spatial Transformer Networks (STNs) and using Convolutional Block Attention Module (CBAM). It shows better performance on the Plant Growth and Phenotyping (PGP) dataset.


<details>
  <summary>Details</summary>
Motivation: Current models like YOLO have issues with occlusions, irregular structures, and background noise which reduce detection accuracy. Affine mappings used in STNs are insufficient for non-rigid deformations.

Method: The proposed model integrates Thin-Plate Splines (TPS) into Spatial Transformer Networks (STNs) for flexible non-rigid spatial transformations and uses Convolutional Block Attention Module (CBAM) to suppress background noise and emphasize relevant features.

Result: On the PGP dataset, the model outperforms STN-YOLO in precision, recall, and mAP with a 12% reduction in false positives. The impact of the TPS regularization parameter is also examined.

Conclusion: The lightweight model improves spatial awareness and supports real-time edge deployment, making it suitable for smart farming applications.

Abstract: Object detection is vital in precision agriculture for plant monitoring,
disease detection, and yield estimation. However, models like YOLO struggle
with occlusions, irregular structures, and background noise, reducing detection
accuracy. While Spatial Transformer Networks (STNs) improve spatial invariance
through learned transformations, affine mappings are insufficient for non-rigid
deformations such as bent leaves and overlaps.
  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)
into STNs for flexible, non-rigid spatial transformations that better align
features. Performance is further enhanced by the Convolutional Block Attention
Module (CBAM), which suppresses background noise and emphasizes relevant
spatial and channel-wise features.
  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model
outperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction
in false positives, highlighting the benefits of improved spatial flexibility
and attention-guided refinement. We also examine the impact of the TPS
regularization parameter in balancing transformation smoothness and detection
performance.
  This lightweight model improves spatial awareness and supports real-time edge
deployment, making it ideal for smart farming applications requiring accurate
and efficient monitoring.

</details>


### [360] [Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study](https://arxiv.org/abs/2506.07539)
*Xiaomeng Zhu,Jacob Henningsson,Duruo Li,Pär Mårtensson,Lars Hanson,Mårten Björkman,Atsuto Maki*

Main category: cs.CV

TL;DR: The paper explores domain randomization in synthetic data generation for manufacturing object detection, presenting a data generation pipeline and the SIP15-OD dataset. Experiments reveal key factors like material properties and post-processing, achieving high mAP scores with Yolov8 models.


<details>
  <summary>Details</summary>
Motivation: To address challenges in sim-to-real object detection by investigating the impact of various factors through domain randomization.

Method: Developed a comprehensive data generation pipeline considering object characteristics, background, illumination, camera settings, and post-processing. Introduced SIP15-OD dataset and used an existing public industrial dataset for experiments.

Result: Identified material properties, rendering methods, post-processing, and distractors as crucial factors. Achieved top performance on public dataset with Yolov8 models trained on synthetic data, obtaining mAP@50 scores of 96.4% for robotics dataset and 94.1%, 99.5%, 95.3% for SIP15-OD use cases.

Conclusion: Domain randomization is effective in generating synthetic data close to real data distribution for manufacturing object detection applications.

Abstract: This paper addresses key aspects of domain randomization in generating
synthetic data for manufacturing object detection applications. To this end, we
present a comprehensive data generation pipeline that reflects different
factors: object characteristics, background, illumination, camera settings, and
post-processing. We also introduce the Synthetic Industrial Parts Object
Detection dataset (SIP15-OD) consisting of 15 objects from three industrial use
cases under varying environments as a test bed for the study, while also
employing an industrial dataset publicly available for robotic applications. In
our experiments, we present more abundant results and insights into the
feasibility as well as challenges of sim-to-real object detection. In
particular, we identified material properties, rendering methods,
post-processing, and distractors as important factors. Our method, leveraging
these, achieves top performance on the public dataset with Yolov8 models
trained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics
dataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,
respectively. The results showcase the effectiveness of the proposed domain
randomization, potentially covering the distribution close to real data for the
applications.

</details>


### [361] [APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs](https://arxiv.org/abs/2506.07542)
*Bowen Liu,Weiyi Zhang,Peranut Chotcomwongse,Xiaolan Chen,Ruoyu Chen,Pawin Pakaymaskul,Niracha Arjkongharn,Nattaporn Vongsa,Xuelian Cheng,Zongyuan Ge,Kun Huang,Xiaohui Li,Yiru Duan,Zhenbang Wang,BaoYe Xie,Qiang Chen,Huazhu Fu,Michael A. Mahr,Jiaqi Qu,Wangyiyang Chen,Shiye Wang,Yubo Tan,Yongjie Li,Mingguang He,Danli Shi,Paisan Ruamviboonsuk*

Main category: cs.CV

TL;DR: The APTOS-2024 Challenge explored the feasibility of generating 3D OCT images from 2D fundus images, attracting many participants and showcasing innovative methodologies.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of OCT such as high equipment costs and need for specialized operators by utilizing 2D color fundus photography which is faster and more accessible.

Method: Organized a challenge with benchmark dataset and evaluation methodology including image-based distance and video-based distance metrics. Top solutions involved hybrid data preprocessing/augmentation, pre-training on external datasets, integration of vision foundation models, and model architecture improvements.

Result: Attracted 342 teams, had 42 preliminary submissions and 9 finalists. Demonstrated the potential of fundus-to-3D-OCT synthesis for improving healthcare accessibility.

Conclusion: The APTOS-2024 Challenge was successful in showing that fundus-to-3D-OCT synthesis could be a viable solution to enhance ophthalmic care accessibility and accelerate medical research.

Abstract: Optical Coherence Tomography (OCT) provides high-resolution, 3D, and
non-invasive visualization of retinal layers in vivo, serving as a critical
tool for lesion localization and disease diagnosis. However, its widespread
adoption is limited by equipment costs and the need for specialized operators.
In comparison, 2D color fundus photography offers faster acquisition and
greater accessibility with less dependence on expensive devices. Although
generative artificial intelligence has demonstrated promising results in
medical image synthesis, translating 2D fundus images into 3D OCT images
presents unique challenges due to inherent differences in data dimensionality
and biological information between modalities. To advance generative models in
the fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society
(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT
Generation from Fundus Images. This paper details the challenge framework
(referred to as APTOS-2024 Challenge), including: the benchmark dataset,
evaluation methodology featuring two fidelity metrics-image-based distance
(pixel-level OCT B-scan similarity) and video-based distance (semantic-level
volumetric consistency), and analysis of top-performing solutions. The
challenge attracted 342 participating teams, with 42 preliminary submissions
and 9 finalists. Leading methodologies incorporated innovations in hybrid data
preprocessing or augmentation (cross-modality collaborative paradigms),
pre-training on external ophthalmic imaging datasets, integration of vision
foundation models, and model architecture improvement. The APTOS-2024 Challenge
is the first benchmark demonstrating the feasibility of fundus-to-3D-OCT
synthesis as a potential solution for improving ophthalmic care accessibility
in under-resourced healthcare settings, while helping to expedite medical
research and clinical applications.

</details>


### [362] [Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries](https://arxiv.org/abs/2506.07555)
*Haoxiang Wang,Zinan Lin,Da Yu,Huishuai Zhang*

Main category: cs.CV

TL;DR: 生成高保真、差异隐私（DP）合成图像为共享和分析敏感视觉数据提供了有希望的途径，而不会侵犯个人隐私。然而，现有的DP图像合成方法难以生成忠实捕捉原始数据结构的高分辨率输出。本文介绍了一种称为通过私有文本中介进行合成（SPTI）的新方法，可以轻松生成高分辨率DP图像。SPTI的关键思想是通过利用最先进的DP文本生成方法，将DP图像合成的挑战从图像域转移到文本域。SPTI首先使用图像到文本模型将每个私有图像总结为简洁的文本描述，然后应用修改后的私有进化算法生成DP文本，最后使用文本到图像模型重建图像。值得注意的是，SPTI不需要模型训练，只需使用现成的模型进行推理。给定一个私有数据集，SPTI生成的合成图像质量比之前的DP方法显著提高。在LSUN卧室数据集上，SPTI在epsilon等于1.0时达到FID小于或等于26.71，优于私有进化FID的40.36。同样，在MM CelebA HQ上，SPTI在epsilon等于1.0时达到FID小于或等于33.27，而DP微调基线为57.01。总的来说，我们的结果表明，通过私有文本中介进行合成为生成高分辨率DP合成图像提供了一个资源高效且专有模型兼容的框架，大大扩展了对私有视觉数据集的访问。


<details>
  <summary>Details</summary>
Motivation: 生成高保真、差异隐私（DP）合成图像为共享和分析敏感视觉数据提供了有希望的途径，而不会侵犯个人隐私。然而，现有的DP图像合成方法难以生成忠实捕捉原始数据结构的高分辨率输出。

Method: SPTI首先使用图像到文本模型将每个私有图像总结为简洁的文本描述，然后应用修改后的私有进化算法生成DP文本，最后使用文本到图像模型重建图像。值得注意的是，SPTI不需要模型训练，只需使用现成的模型进行推理。

Result: 在LSUN卧室数据集上，SPTI在epsilon等于1.0时达到FID小于或等于26.71，优于私有进化FID的40.36。同样，在MM CelebA HQ上，SPTI在epsilon等于1.0时达到FID小于或等于33.27，而DP微调基线为57.01。

Conclusion: 通过私有文本中介进行合成为生成高分辨率DP合成图像提供了一个资源高效且专有模型兼容的框架，大大扩展了对私有视觉数据集的访问。

Abstract: Generating high fidelity, differentially private (DP) synthetic images offers
a promising route to share and analyze sensitive visual data without
compromising individual privacy. However, existing DP image synthesis methods
struggle to produce high resolution outputs that faithfully capture the
structure of the original data. In this paper, we introduce a novel method,
referred to as Synthesis via Private Textual Intermediaries (SPTI), that can
generate high resolution DP images with easy adoption. The key idea is to shift
the challenge of DP image synthesis from the image domain to the text domain by
leveraging state of the art DP text generation methods. SPTI first summarizes
each private image into a concise textual description using image to text
models, then applies a modified Private Evolution algorithm to generate DP
text, and finally reconstructs images using text to image models. Notably, SPTI
requires no model training, only inference with off the shelf models. Given a
private dataset, SPTI produces synthetic images of substantially higher quality
than prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less
than or equal to 26.71 under epsilon equal to 1.0, improving over Private
Evolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less
than or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine
tuning baselines. Overall, our results demonstrate that Synthesis via Private
Textual Intermediaries provides a resource efficient and proprietary model
compatible framework for generating high resolution DP synthetic images,
greatly expanding access to private visual datasets.

</details>


### [363] [LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization](https://arxiv.org/abs/2506.07570)
*Yixuan Yang,Zhen Luo,Tongsheng Ding,Junru Lu,Mingqi Gao,Jinyu Yang,Victor Sanchez,Feng Zheng*

Main category: cs.CV

TL;DR: The paper introduces 3D-SynthPlace, a large-scale dataset for indoor layout generation and OptiScene, an open-source LLM optimized for this task. OptiScene surpasses traditional methods in layout quality and has potential in interactive tasks.


<details>
  <summary>Details</summary>
Motivation: Automatic indoor layout generation is crucial for applications in interior design, virtual environment construction, and embodied AI. Existing methods have limitations such as spatial inconsistency, high computational costs, coarse relational graphs, and limited datasets.

Method: 1. Created 3D-SynthPlace dataset with synthetic layouts via 'GPT synthesize, Human inspect' pipeline.2. Introduced OptiScene, an open-source LLM fine-tuned on 3D-SynthPlace through two-stage training:- Stage I: Supervised fine-tuning (SFT) to generate spatial descriptions and predict object placements.- Stage II: Multi-turn direct preference optimization (DPO) to align with human design preferences.

Result: OptiScene outperforms traditional prompt-driven and learning-based baselines in layout quality and generation success rates. It also shows potential in interactive tasks like scene editing and robot navigation.

Conclusion: 3D-SynthPlace and OptiScene provide a strong foundation for advancing automatic indoor layout generation, overcoming limitations of previous methods and demonstrating potential in various applications.

Abstract: Automatic indoor layout generation has attracted increasing attention due to
its potential in interior design, virtual environment construction, and
embodied AI. Existing methods fall into two categories: prompt-driven
approaches that leverage proprietary LLM services (e.g., GPT APIs) and
learning-based methods trained on layout data upon diffusion-based models.
Prompt-driven methods often suffer from spatial inconsistency and high
computational costs, while learning-based methods are typically constrained by
coarse relational graphs and limited datasets, restricting their generalization
to diverse room categories. In this paper, we revisit LLM-based indoor layout
generation and present 3D-SynthPlace, a large-scale dataset that combines
synthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,
upgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000
scenes, covering four common room types -- bedroom, living room, kitchen, and
bathroom -- enriched with diverse objects and high-level spatial annotations.
We further introduce OptiScene, a strong open-source LLM optimized for indoor
layout generation, fine-tuned based on our 3D-SynthPlace dataset through our
two-stage training. For the warum-up stage I, we adopt supervised fine-tuning
(SFT), which is taught to first generate high-level spatial descriptions then
conditionally predict concrete object placements. For the reinforcing stage II,
to better align the generated layouts with human design preferences, we apply
multi-turn direct preference optimization (DPO), which significantly improving
layout quality and generation success rates. Extensive experiments demonstrate
that OptiScene outperforms traditional prompt-driven and learning-based
baselines. Moreover, OptiScene shows promising potential in interactive tasks
such as scene editing and robot navigation.

</details>


### [364] [Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models](https://arxiv.org/abs/2506.07575)
*Ruiyang Zhang,Hu Zhang,Hao Fei,Zhedong Zheng*

Main category: cs.CV

TL;DR: The paper introduces Uncertainty-o, a model-agnostic framework for evaluating uncertainty in Large Multimodal Models (LMMs). It addresses key challenges such as unified evaluation, prompting LMMs to show uncertainty, and quantifying uncertainty for downstream tasks. Experiments across 18 benchmarks and 10 LMMs demonstrate its effectiveness.


<details>
  <summary>Details</summary>
Motivation: There is a lack of understanding and methods to evaluate the uncertainty of Large Multimodal Models (LMMs), especially compared to Language Large Models (LLMs). The authors aim to address three open questions: unified evaluation of uncertainty, prompting LMMs to reveal their uncertainty, and quantifying uncertainty for downstream tasks.

Method: The method involves introducing Uncertainty-o, which consists of: a model-agnostic framework to reveal uncertainty in LMMs, empirical exploration of multimodal prompt perturbations to uncover LMM uncertainty, and derivation of multimodal semantic uncertainty formulation to quantify uncertainty from multimodal responses.

Result: Experiments on 18 benchmarks and 10 LMMs show that Uncertainty-o effectively estimates LMM uncertainty, enhancing downstream tasks like hallucination detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought reasoning.

Conclusion: Uncertainty-o provides a reliable way to estimate uncertainty in LMMs, regardless of their modalities, architectures, or capabilities. This enhances various downstream applications.

Abstract: Large Multimodal Models (LMMs), harnessing the complementarity among diverse
modalities, are often considered more robust than pure Language Large Models
(LLMs); yet do LMMs know what they do not know? There are three key open
questions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a
unified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to
quantify uncertainty for downstream tasks. In an attempt to address these
challenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed
to reveal uncertainty in LMMs regardless of their modalities, architectures, or
capabilities, (2) an empirical exploration of multimodal prompt perturbations
to uncover LMM uncertainty, offering insights and findings, and (3) derive the
formulation of multimodal semantic uncertainty, which enables quantifying
uncertainty from multimodal responses. Experiments across 18 benchmarks
spanning various modalities and 10 LMMs (both open- and closed-source)
demonstrate the effectiveness of Uncertainty-o in reliably estimating LMM
uncertainty, thereby enhancing downstream tasks such as hallucination
detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought
reasoning.

</details>


### [365] [Explore the vulnerability of black-box models via diffusion models](https://arxiv.org/abs/2506.07590)
*Jiacheng Shi,Yanfu Zhang,Huajie Shao,Ashley Gao*

Main category: cs.CV

TL;DR: Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications, but these models also present security and privacy risks. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model.


<details>
  <summary>Details</summary>
Motivation: The motivation is the security and privacy risks of diffusion models, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously.

Method: An attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data.

Result: The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.

Conclusion: Diffusion models can be used by attackers to generate synthetic images for training substitute models, enabling effective adversarial attacks on black-box models with minimal queries.

Abstract: Recent advancements in diffusion models have enabled high-fidelity and
photorealistic image generation across diverse applications. However, these
models also present security and privacy risks, including copyright violations,
sensitive information leakage, and the creation of harmful or offensive content
that could be exploited maliciously. In this study, we uncover a novel security
threat where an attacker leverages diffusion model APIs to generate synthetic
images, which are then used to train a high-performing substitute model. This
enables the attacker to execute model extraction and transfer-based adversarial
attacks on black-box classification models with minimal queries, without
needing access to the original training data. The generated images are
sufficiently high-resolution and diverse to train a substitute model whose
outputs closely match those of the target model. Across the seven benchmarks,
including CIFAR and ImageNet subsets, our method shows an average improvement
of 27.37% over state-of-the-art methods while using just 0.01 times of the
query budget, achieving a 98.68% success rate in adversarial attacks on the
target model.

</details>


### [366] [SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding](https://arxiv.org/abs/2506.07600)
*Nianbo Zeng,Haowen Hou,Fei Richard Yu,Si Shi,Ying Tiffany He*

Main category: cs.CV

TL;DR: SceneRAG is a unified framework leveraging large language models to segment videos into narrative-consistent scenes, process ASR transcripts alongside temporal metadata, extract entity relations and build knowledge graphs for robust multi-hop retrieval and generation. Experiments confirm SceneRAG outperforms prior baselines with a win rate of up to 72.5 percent on generation tasks.


<details>
  <summary>Details</summary>
Motivation: Effective understanding of long-form video content remains underexplored due to the vast scale and high complexity of video data. Current RAG approaches disrupt continuity and fail to capture authentic scene boundaries.

Method: SceneRAG segments videos into narrative-consistent scenes by processing ASR transcripts alongside temporal metadata, sharpens these initial boundaries through lightweight heuristics and iterative correction, fuses information from both visual and textual modalities to extract entity relations and builds knowledge graphs.

Result: Experiments on the LongerVideos benchmark confirm that SceneRAG substantially outperforms prior baselines, achieving a win rate of up to 72.5 percent on generation tasks.

Conclusion: SceneRAG provides an effective solution for understanding long-form video content by leveraging large language models, processing ASR transcripts alongside temporal metadata, extracting entity relations and building knowledge graphs.

Abstract: Despite recent advances in retrieval-augmented generation (RAG) for video
understanding, effectively understanding long-form video content remains
underexplored due to the vast scale and high complexity of video data. Current
RAG approaches typically segment videos into fixed-length chunks, which often
disrupts the continuity of contextual information and fails to capture
authentic scene boundaries. Inspired by the human ability to naturally organize
continuous experiences into coherent scenes, we present SceneRAG, a unified
framework that leverages large language models to segment videos into
narrative-consistent scenes by processing ASR transcripts alongside temporal
metadata. SceneRAG further sharpens these initial boundaries through
lightweight heuristics and iterative correction. For each scene, the framework
fuses information from both visual and textual modalities to extract entity
relations and dynamically builds a knowledge graph, enabling robust multi-hop
retrieval and generation that account for long-range dependencies. Experiments
on the LongerVideos benchmark, featuring over 134 hours of diverse content,
confirm that SceneRAG substantially outperforms prior baselines, achieving a
win rate of up to 72.5 percent on generation tasks.

</details>


### [367] [SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](https://arxiv.org/abs/2506.07603)
*Jianhui Wei,Zikai Xiao,Danyu Sun,Luqi Gong,Zongxin Yang,Zuozhu Liu,Jian Wu*

Main category: cs.CV

TL;DR: SurgBench, a unified surgical video benchmarking framework with SurgBench-P and SurgBench-E, addresses the scarcity of large-scale datasets for pretraining and evaluation in surgical video understanding. Pretraining on SurgBench-P significantly enhances performance and cross-domain generalization in varied surgical video analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Progress in developing surgical video foundation models is limited by the lack of large-scale, diverse datasets for pretraining and systematic evaluation.

Method: Introduced SurgBench, which includes SurgBench-P (a pretraining dataset with 53 million frames across 22 surgical procedures and 11 specialties) and SurgBench-E (an evaluation benchmark covering six categories and 72 fine-grained tasks).

Result: Existing video FMs have difficulty generalizing across various surgical video analysis tasks, but pretraining on SurgBench-P leads to significant performance improvements and better cross-domain generalization.

Conclusion: SurgBench provides extensive coverage of diverse surgical scenarios and enhances the development of surgical video foundation models.

Abstract: Surgical video understanding is pivotal for enabling automated intraoperative
decision-making, skill assessment, and postoperative quality improvement.
However, progress in developing surgical video foundation models (FMs) remains
hindered by the scarcity of large-scale, diverse datasets for pretraining and
systematic evaluation. In this paper, we introduce \textbf{SurgBench}, a
unified surgical video benchmarking framework comprising a pretraining dataset,
\textbf{SurgBench-P}, and an evaluation benchmark, \textbf{SurgBench-E}.
SurgBench offers extensive coverage of diverse surgical scenarios, with
SurgBench-P encompassing 53 million frames across 22 surgical procedures and 11
specialties, and SurgBench-E providing robust evaluation across six categories
(phase classification, camera motion, tool recognition, disease diagnosis,
action classification, and organ detection) spanning 72 fine-grained tasks.
Extensive experiments reveal that existing video FMs struggle to generalize
across varied surgical video analysis tasks, whereas pretraining on SurgBench-P
yields substantial performance improvements and superior cross-domain
generalization to unseen procedures and modalities. Our dataset and code are
available upon request.

</details>


### [368] [HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition](https://arxiv.org/abs/2506.07637)
*Yuchong Long,Wen Sun,Ningxiao Sun,Wenxiao Wang,Chao Li,Shan Yin*

Main category: cs.CV

TL;DR: HieraEdgeNet, a multi-scale edge-enhancement framework with three key modules (HEM, SEF, CSPOKM), achieves high precision in automated pollen recognition, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Automated pollen recognition faces challenges due to inefficiency and subjectivity of conventional methods, as well as the difficulty deep learning models have in localizing microscopic targets like pollen accurately.

Method: The HieraEdgeNet framework includes the Hierarchical Edge Module (HEM) for multi-scale edge feature extraction, the Synergistic Edge Fusion (SEF) module for fusing edge priors with semantic information at each scale, and the Cross Stage Partial Omni-Kernel Module (CSPOKM) for refining feature layers using an Omni-Kernel operator within a CSP framework.

Result: On a large-scale dataset with 120 pollen classes, HieraEdgeNet achieved a mean Average Precision (mAP@.5) of 0.9501, surpassing other state-of-the-art models such as YOLOv12n and RT-DETR. Qualitative analysis also confirmed more precise feature representations focused on object boundaries.

Conclusion: HieraEdgeNet provides a robust solution for high-precision, high-efficiency automated detection of microscopic objects by systematically integrating edge information.

Abstract: Automated pollen recognition is vital to paleoclimatology, biodiversity
monitoring, and public health, yet conventional methods are hampered by
inefficiency and subjectivity. Existing deep learning models often struggle to
achieve the requisite localization accuracy for microscopic targets like
pollen, which are characterized by their minute size, indistinct edges, and
complex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a
multi-scale edge-enhancement framework. The framework's core innovation is the
introduction of three synergistic modules: the Hierarchical Edge Module (HEM),
which explicitly extracts a multi-scale pyramid of edge features that
corresponds to the semantic hierarchy at early network stages; the Synergistic
Edge Fusion (SEF) module, for deeply fusing these edge priors with semantic
information at each respective scale; and the Cross Stage Partial Omni-Kernel
Module (CSPOKM), which maximally refines the most detail-rich feature layers
using an Omni-Kernel operator - comprising anisotropic large-kernel
convolutions and mixed-domain attention - all within a computationally
efficient Cross-Stage Partial (CSP) framework. On a large-scale dataset
comprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision
(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline
models such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms
that our approach generates feature representations that are more precisely
focused on object boundaries. By systematically integrating edge information,
HieraEdgeNet provides a robust and powerful solution for high-precision,
high-efficiency automated detection of microscopic objects.

</details>


### [369] [FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images](https://arxiv.org/abs/2506.07652)
*Hangbei Cheng,Xiaorong Dong,Xueyu Liu,Jianan Zhang,Xuetao Ma,Mingqiang Wei,Liansheng Wang,Junxin Chen,Yongfei Wu*

Main category: cs.CV

TL;DR: This paper proposes FMaMIL, a two-stage framework for weakly supervised lesion segmentation in histopathology images using only image-level labels. It combines a lightweight Mamba-based encoder with a frequency-domain encoding module and uses CAM-guided soft-label supervision and self-correction for refining pseudo labels. FMaMIL outperforms state-of-the-art methods without needing pixel-level annotations.


<details>
  <summary>Details</summary>
Motivation: Accurate lesion segmentation is crucial in histopathology but challenging due to limited availability of costly pixel-level annotations. The authors aim to develop a method that relies solely on image-level labels for segmentation.

Method: The FMaMIL framework consists of two stages: (1) A Mamba-based encoder captures long-range dependencies across image patches under the MIL paradigm, supplemented by a learnable frequency-domain encoding module to enhance spatial sensitivity and structural awareness. Class Activation Maps (CAMs) generated guide segmentation training. (2) Pseudo labels are refined via CAM-guided soft-label supervision and a self-correction mechanism to ensure robustness against label noise.

Result: Extensive experiments on both public and private histopathology datasets show that FMaMIL outperforms state-of-the-art weakly supervised methods, achieving better performance without relying on pixel-level annotations.

Conclusion: FMaMIL demonstrates superior performance compared to existing weakly supervised methods in lesion segmentation for histopathology images using only image-level labels, proving its effectiveness and potential for digital pathology applications.

Abstract: Accurate lesion segmentation in histopathology images is essential for
diagnostic interpretation and quantitative analysis, yet it remains challenging
due to the limited availability of costly pixel-level annotations. To address
this, we propose FMaMIL, a novel two-stage framework for weakly supervised
lesion segmentation based solely on image-level labels. In the first stage, a
lightweight Mamba-based encoder is introduced to capture long-range
dependencies across image patches under the MIL paradigm. To enhance spatial
sensitivity and structural awareness, we design a learnable frequency-domain
encoding module that supplements spatial-domain features with spectrum-based
information. CAMs generated in this stage are used to guide segmentation
training. In the second stage, we refine the initial pseudo labels via a
CAM-guided soft-label supervision and a self-correction mechanism, enabling
robust training even under label noise. Extensive experiments on both public
and private histopathology datasets demonstrate that FMaMIL outperforms
state-of-the-art weakly supervised methods without relying on pixel-level
annotations, validating its effectiveness and potential for digital pathology
applications.

</details>


### [370] [NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/abs/2506.07698)
*Yuxiao Yang,Peihao Li,Yuhong Zhang,Junzhe Lu,Xianglong He,Minghan Qin,Weitao Wang,Haoqian Wang*

Main category: cs.CV

TL;DR: NOVA3D是一个创新的单图像到3D生成框架，利用预训练视频扩散模型中的强3D先验，并在多视图视频微调期间整合几何信息，提出GTA注意力机制和去冲突几何融合算法以提高多视图一致性和纹理保真度。


<details>
  <summary>Details</summary>
Motivation: 尽管最近的方法利用分数蒸馏采样从预训练的图像扩散模型中提取3D对象，但它们通常由于3D先验不足而导致多视图一致性不足。

Method: NOVA3D引入了来自预训练视频扩散模型的强3D先验，并在多视图视频微调期间整合了几何信息。为了促进颜色和几何域之间的信息交换，提出了几何-时间对齐（GTA）注意机制。此外，还引入了去冲突几何融合算法，通过解决多视图不准确性并解决姿态对齐中的差异来提高纹理保真度。

Result: 广泛的实验证明了NOVA3D在现有基线上的优越性。

Conclusion: NOVA3D提高了多视图一致性和纹理保真度，为3D内容创建提供了更好的解决方案。

Abstract: 3D AI-generated content (AIGC) has made it increasingly accessible for anyone
to become a 3D content creator. While recent methods leverage Score
Distillation Sampling to distill 3D objects from pretrained image diffusion
models, they often suffer from inadequate 3D priors, leading to insufficient
multi-view consistency. In this work, we introduce NOVA3D, an innovative
single-image-to-3D generation framework. Our key insight lies in leveraging
strong 3D priors from a pretrained video diffusion model and integrating
geometric information during multi-view video fine-tuning. To facilitate
information exchange between color and geometric domains, we propose the
Geometry-Temporal Alignment (GTA) attention mechanism, thereby improving
generalization and multi-view consistency. Moreover, we introduce the
de-conflict geometry fusion algorithm, which improves texture fidelity by
addressing multi-view inaccuracies and resolving discrepancies in pose
alignment. Extensive experiments validate the superiority of NOVA3D over
existing baselines.

</details>


### [371] [Consistent Video Editing as Flow-Driven Image-to-Video Generation](https://arxiv.org/abs/2506.07713)
*Ge Wang,Songlin Fan,Hangxu Liu,Quanjian Song,Hewei Wang,Jinfeng Xu*

Main category: cs.CV

TL;DR: FlowV2V是一种新的视频编辑方法，通过光流建模复杂运动模式，提供更好的时间一致性和样本质量。


<details>
  <summary>Details</summary>
Motivation: 现有的视频编辑方法无法很好地处理复杂的运动模式，特别是非刚性物体运动，如多物体和肖像编辑。这促使了对更有效方法的需求。

Method: FlowV2V将视频编辑任务重新定义为由光流驱动的图像到视频（I2V）生成任务。它将整个流程分解为第一帧编辑和条件I2V生成，并模拟与变形形状对齐的伪流序列以确保编辑过程中的连贯性。

Result: 实验结果表明，FlowV2V在DAVIS-EDIT数据集上分别提高了13.67%和50.66%的DOVER和扭曲误差指标，显示了其优越的时间一致性和样本质量。此外，进行了全面的消融研究来分析所提方法内部功能。

Conclusion: FlowV2V通过利用光流成功解决了现有方法在复杂运动建模上的不足，提供了更高的时间和空间一致性。

Abstract: With the prosper of video diffusion models, down-stream applications like
video editing have been significantly promoted without consuming much
computational cost. One particular challenge in this task lies at the motion
transfer process from the source video to the edited one, where it requires the
consideration of the shape deformation in between, meanwhile maintaining the
temporal consistency in the generated video sequence. However, existing methods
fail to model complicated motion patterns for video editing, and are
fundamentally limited to object replacement, where tasks with non-rigid object
motions like multi-object and portrait editing are largely neglected. In this
paper, we observe that optical flows offer a promising alternative in complex
motion modeling, and present FlowV2V to re-investigate video editing as a task
of flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V
decomposes the entire pipeline into first-frame editing and conditional I2V
generation, and simulates pseudo flow sequence that aligns with the deformed
shape, thus ensuring the consistency during editing. Experimental results on
DAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error
illustrate the superior temporal consistency and sample quality of FlowV2V
compared to existing state-of-the-art ones. Furthermore, we conduct
comprehensive ablation studies to analyze the internal functionalities of the
first-frame paradigm and flow alignment in the proposed method.

</details>


### [372] [ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models](https://arxiv.org/abs/2506.07725)
*Shadi Hamdan,Chonghao Sima,Zetong Yang,Hongyang Li,Fatma Güney*

Main category: cs.CV

TL;DR: The paper proposes Efficiency through Thinking Ahead (ETA), an asynchronous dual-model system that shifts intensive computations of the current frame to previous time steps for faster inference in self-driving systems.


<details>
  <summary>Details</summary>
Motivation: To solve the dilemma of benefiting from large models without sacrificing inference speed in self-driving systems.

Method: Employing a small model for real-time reactive decisions and a larger model for informative analyses, with computations shifted to previous time steps via future predictions, current frame features extracted using a small model, and integration of these features via an action mask mechanism.

Result: Advances state-of-the-art performance by 8% with a driving score of 69.53 on the Bench2Drive CARLA Leaderboard-v2 benchmark while maintaining near-real-time inference speed at 50 ms.

Conclusion: ETA successfully enables large models to respond promptly to each time step in self-driving systems.

Abstract: How can we benefit from large models without sacrificing inference speed, a
common dilemma in self-driving systems? A prevalent solution is a dual-system
architecture, employing a small model for rapid, reactive decisions and a
larger model for slower but more informative analyses. Existing dual-system
designs often implement parallel architectures where inference is either
directly conducted using the large model at each current frame or retrieved
from previously stored inference results. However, these works still struggle
to enable large models for a timely response to every online frame. Our key
insight is to shift intensive computations of the current frame to previous
time steps and perform a batch inference of multiple time steps to make large
models respond promptly to each time step. To achieve the shifting, we
introduce Efficiency through Thinking Ahead (ETA), an asynchronous system
designed to: (1) propagate informative features from the past to the current
frame using future predictions from the large model, (2) extract current frame
features using a small model for real-time responsiveness, and (3) integrate
these dual features via an action mask mechanism that emphasizes
action-critical image regions. Evaluated on the Bench2Drive CARLA
Leaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with
a driving score of 69.53 while maintaining a near-real-time inference speed at
50 ms.

</details>


### [373] [ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models](https://arxiv.org/abs/2506.07739)
*Jing Zhong,Jun Yin,Peilin Li,Pengyu Zeng,Miao Zhang,Shuai Lu,Ran Luo*

Main category: cs.CV

TL;DR: This paper proposes ArchDiffBench, a professional architectural style dataset, and ArchiLense, an analytical framework based on Vision-Language Models. ArchiLense enables automatic recognition, comparison, and classification of architectural imagery with high consistency and accuracy rates.


<details>
  <summary>Details</summary>
Motivation: Traditional studies of architectural culture suffer from regional biases and limited explanatory scope due to reliance on subjective expert interpretations and historical literature reviews.

Method: The study constructs the ArchDiffBench dataset with 1,765 high-quality architectural images and their corresponding style annotations. It also proposes ArchiLense, an analytical framework using Vision-Language Models that integrates advanced computer vision techniques, deep learning, and machine learning algorithms for automatic recognition, comparison, and classification of architectural imagery.

Result: ArchiLense achieves a 92.4% consistency rate with expert annotations and 84.5% classification accuracy, effectively capturing stylistic distinctions across images.

Conclusion: The proposed approach overcomes the subjectivity in traditional analyses and provides a more objective and accurate perspective for comparative studies of architectural culture.

Abstract: Architectural cultures across regions are characterized by stylistic
diversity, shaped by historical, social, and technological contexts in addition
to geograph-ical conditions. Understanding architectural styles requires the
ability to describe and analyze the stylistic features of different architects
from various regions through visual observations of architectural imagery.
However, traditional studies of architectural culture have largely relied on
subjective expert interpretations and historical literature reviews, often
suffering from regional biases and limited ex-planatory scope. To address these
challenges, this study proposes three core contributions: (1) We construct a
professional architectural style dataset named ArchDiffBench, which comprises
1,765 high-quality architectural images and their corresponding style
annotations, collected from different regions and historical periods. (2) We
propose ArchiLense, an analytical framework grounded in Vision-Language Models
and constructed using the ArchDiffBench dataset. By integrating ad-vanced
computer vision techniques, deep learning, and machine learning algo-rithms,
ArchiLense enables automatic recognition, comparison, and precise
classi-fication of architectural imagery, producing descriptive language
outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show
that ArchiLense achieves strong performance in architectural style recognition,
with a 92.4% con-sistency rate with expert annotations and 84.5% classification
accuracy, effec-tively capturing stylistic distinctions across images. The
proposed approach transcends the subjectivity inherent in traditional analyses
and offers a more objective and accurate perspective for comparative studies of
architectural culture.

</details>


### [374] [Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity](https://arxiv.org/abs/2506.07773)
*Mohamed Djilani,Nassim Ali Ousalah,Nidhal Eddine Chenni*

Main category: cs.CV

TL;DR: A trend-aware and visually-grounded fashion recommendation system is introduced, which integrates deep visual representations, garment-aware segmentation, semantic category similarity and user behavior simulation. Experiments on the DeepFashion dataset show gender alignment and improved category relevance.


<details>
  <summary>Details</summary>
Motivation: To create a more personalized and trend-aware fashion recommendation system that balances individual style with emerging trends.

Method: The system uses a pipeline that extracts focused visual embeddings by masking non-garment regions via semantic segmentation followed by feature extraction using pretrained CNN backbones (ResNet-50, DenseNet-121, VGG16). It generates synthetic purchase histories influenced by user-specific trendiness and item popularity. Recommendations are computed using a weighted scoring function that fuses visual similarity, semantic coherence and popularity alignment.

Result: Experiments on the DeepFashion dataset demonstrate consistent gender alignment and improved category relevance, with ResNet-50 achieving 64.95% category similarity and lowest popularity MAE. An ablation study confirms the complementary roles of visual and popularity cues.

Conclusion: This method provides a scalable framework for personalized fashion recommendations that balances individual style with emerging trends.

Abstract: We introduce a trend-aware and visually-grounded fashion recommendation
system that integrates deep visual representations, garment-aware segmentation,
semantic category similarity and user behavior simulation. Our pipeline
extracts focused visual embeddings by masking non-garment regions via semantic
segmentation followed by feature extraction using pretrained CNN backbones
(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we
generate synthetic purchase histories influenced by user-specific trendiness
and item popularity. Recommendations are computed using a weighted scoring
function that fuses visual similarity, semantic coherence and popularity
alignment. Experiments on the DeepFashion dataset demonstrate consistent gender
alignment and improved category relevance, with ResNet-50 achieving 64.95%
category similarity and lowest popularity MAE. An ablation study confirms the
complementary roles of visual and popularity cues. Our method provides a
scalable framework for personalized fashion recommendations that balances
individual style with emerging trends. Our implementation is available at
https://github.com/meddjilani/FashionRecommender

</details>


### [375] [Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger](https://arxiv.org/abs/2506.07785)
*Qi Yang,Chenghao Zhang,Lubin Fan,Kun Ding,Jieping Ye,Shiming Xiang*

Main category: cs.CV

TL;DR: Recent advancements in LVLMs have improved VQA tasks. However, challenges remain. This study proposes RCTS, a multimodal RAG framework that constructs a reasoning context-enriched knowledge base and uses Tree Search re-ranking to enhance LVLMs.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges faced by existing methods in VQA tasks, such as lack of reasoning examples and inconsistent responses from retrieved knowledge.

Method: Propose RCTS, a multimodal RAG framework which includes constructing a Reasoning Context-enriched knowledge base with a self-consistent evaluation mechanism, and using Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) for re-ranking.

Result: Extensive experiments show that RCTS achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.

Conclusion: The study concludes that the proposed knowledge base and re-ranking method effectively improve LVLMs in VQA tasks.

Abstract: Recent advancements in Large Vision Language Models (LVLMs) have
significantly improved performance in Visual Question Answering (VQA) tasks
through multimodal Retrieval-Augmented Generation (RAG). However, existing
methods still face challenges, such as the scarcity of knowledge with reasoning
examples and erratic responses from retrieved knowledge. To address these
issues, in this study, we propose a multimodal RAG framework, termed RCTS,
which enhances LVLMs by constructing a Reasoning Context-enriched knowledge
base and a Tree Search re-ranking method. Specifically, we introduce a
self-consistent evaluation mechanism to enrich the knowledge base with
intrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with
Heuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This
ensures that LVLMs can leverage high-quality contextual reasoning for better
and more consistent responses. Extensive experiments demonstrate that our
framework achieves state-of-the-art performance on multiple VQA datasets,
significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.
It highlights the effectiveness of our knowledge base and re-ranking method in
improving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.

</details>


### [376] [R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation](https://arxiv.org/abs/2506.07826)
*William Ljungbergh,Bernardo Taveira,Wenzhao Zheng,Adam Tonderski,Chensheng Peng,Fredrik Kahl,Christoffer Petersson,Michael Felsberg,Kurt Keutzer,Masayoshi Tomizuka,Wei Zhan*

Main category: cs.CV

TL;DR: R3D2 is a lightweight diffusion model that enhances the realism of inserted 3D assets in autonomous driving simulations by generating plausible rendering effects like shadows and consistent lighting.


<details>
  <summary>Details</summary>
Motivation: Current neural reconstruction methods, while scalable, struggle with dynamic object manipulation and reusability due to incomplete object models with integrated illumination effects. There is a need for a solution that can realistically insert complete 3D assets into existing scenes.

Method: R3D2 is trained on a novel dataset where 3DGS object assets are generated from AD data using an image-conditioned 3D generative model and then synthetically placed into virtual environments. This enables the model to learn realistic integration of objects into scenes.

Result: Quantitative and qualitative evaluations show that R3D2 significantly improves the realism of inserted assets, supporting use-cases such as text-to-3D asset insertion and cross-scene/dataset object transfer.

Conclusion: R3D2 offers a scalable and efficient solution for enhancing the realism of 3D assets in autonomous driving simulations, promoting further research in this field.

Abstract: Validating autonomous driving (AD) systems requires diverse and
safety-critical testing, making photorealistic virtual environments essential.
Traditional simulation platforms, while controllable, are resource-intensive to
scale and often suffer from a domain gap with real-world data. In contrast,
neural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a
scalable solution for creating photorealistic digital twins of real-world
driving scenes. However, they struggle with dynamic object manipulation and
reusability as their per-scene optimization-based methodology tends to result
in incomplete object models with integrated illumination effects. This paper
introduces R3D2, a lightweight, one-step diffusion model designed to overcome
these limitations and enable realistic insertion of complete 3D assets into
existing scenes by generating plausible rendering effects-such as shadows and
consistent lighting-in real time. This is achieved by training R3D2 on a novel
dataset: 3DGS object assets are generated from in-the-wild AD data using an
image-conditioned 3D generative model, and then synthetically placed into
neural rendering-based virtual environments, allowing R3D2 to learn realistic
integration. Quantitative and qualitative evaluations demonstrate that R3D2
significantly enhances the realism of inserted assets, enabling use-cases like
text-to-3D asset insertion and cross-scene/dataset object transfer, allowing
for true scalability in AD validation. To promote further research in scalable
and realistic AD simulation, we will release our dataset and code, see
https://research.zenseact.com/publications/R3D2/.

</details>


### [377] [Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2506.07813)
*Junseo Bang,Joonhee Lee,Kyeonghyun Lee,Haechang Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: The paper introduces CasArbi, a self-cascaded diffusion framework for arbitrary-scale image super-resolution that breaks down scaling demands into smaller sequential factors and progressively enhances image resolution.


<details>
  <summary>Details</summary>
Motivation: Existing methods for arbitrary-scale image super-resolution either use single-stage upsampling processes or have not fully explored the integration of progressive upsampling strategies with diffusion models.

Method: CasArbi utilizes a novel coordinate-guided residual diffusion model to learn continuous image representations and enable efficient diffusion sampling, progressively enhancing image resolution at each step.

Result: CasArbi outperforms prior arts in both perceptual and distortion performance metrics across diverse arbitrary-scale super-resolution benchmarks.

Conclusion: CasArbi provides a promising approach for arbitrary-scale image super-resolution by effectively meeting varying scaling demands through a progressive enhancement process.

Abstract: Arbitrary-scale image super-resolution aims to upsample images to any desired
resolution, offering greater flexibility than traditional fixed-scale
super-resolution. Recent approaches in this domain utilize regression-based or
generative models, but many of them are a single-stage upsampling process,
which may be challenging to learn across a wide, continuous distribution of
scaling factors. Progressive upsampling strategies have shown promise in
mitigating this issue, yet their integration with diffusion models for flexible
upscaling remains underexplored. Here, we present CasArbi, a novel
self-cascaded diffusion framework for arbitrary-scale image super-resolution.
CasArbi meets the varying scaling demands by breaking them down into smaller
sequential factors and progressively enhancing the image resolution at each
step with seamless transitions for arbitrary scales. Our novel
coordinate-guided residual diffusion model allows for the learning of
continuous image representations while enabling efficient diffusion sampling.
Extensive experiments demonstrate that our CasArbi outperforms prior arts in
both perceptual and distortion performance metrics across diverse
arbitrary-scale super-resolution benchmarks.

</details>


### [378] [LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds](https://arxiv.org/abs/2506.07857)
*Zihui Zhang,Weisheng Dai,Hongtao Wen,Bo Yang*

Main category: cs.CV

TL;DR: This paper presents LogoSP, a method for unsupervised 3D semantic segmentation that learns from local and global point features, discovering semantic information through superpoint grouping in the frequency domain to generate pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve unsupervised 3D semantic segmentation by going beyond local features and discovering richer semantic priors without requiring human-provided labels.

Method: LogoSP learns 3D semantics by using both local and global point features. It discovers 3D semantic information via grouping superpoints according to their global patterns in the frequency domain, which are used to create accurate semantic pseudo-labels for training a segmentation network.

Result: Extensive experiments on two indoor and one outdoor datasets demonstrate that LogoSP outperforms all existing unsupervised methods by significant margins, achieving state-of-the-art performance for unsupervised 3D semantic segmentation.

Conclusion: LogoSP successfully learns meaningful 3D semantics from raw point clouds without human labels during training, surpassing current unsupervised methods.

Abstract: We study the problem of unsupervised 3D semantic segmentation on raw point
clouds without needing human labels in training. Existing methods usually
formulate this problem into learning per-point local features followed by a
simple grouping strategy, lacking the ability to discover additional and
possibly richer semantic priors beyond local features. In this paper, we
introduce LogoSP to learn 3D semantics from both local and global point
features. The key to our approach is to discover 3D semantic information by
grouping superpoints according to their global patterns in the frequency
domain, thus generating highly accurate semantic pseudo-labels for training a
segmentation network. Extensive experiments on two indoor and an outdoor
datasets show that our LogoSP surpasses all existing unsupervised methods by
large margins, achieving the state-of-the-art performance for unsupervised 3D
semantic segmentation. Notably, our investigation into the learned global
patterns reveals that they truly represent meaningful 3D semantics in the
absence of human labels during training.

</details>


### [379] [VIVAT: Virtuous Improving VAE Training through Artifact Mitigation](https://arxiv.org/abs/2506.07863)
*Lev Novitskiy,Viacheslav Vasilev,Maria Kovaleva,Vladimir Arkhipkin,Denis Dimitrov*

Main category: cs.CV

TL;DR: The paper introduces VIVAT, a method to reduce common artifacts in Variational Autoencoders (VAEs) training without major architectural changes, leading to state-of-the-art results in image reconstruction and text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: VAEs are crucial in generative computer vision, but their training is often affected by artifacts that lower the quality of reconstruction and generation. There's a need for a systematic approach to mitigate these issues without significant changes to the architecture.

Method: VIVAT presents a taxonomy of five prevalent artifacts (color shift, grid patterns, blur, corner, and droplet artifacts) and addresses them through simple modifications like adjusting loss weights, padding strategies, and incorporating Spatially Conditional Normalization.

Result: This method significantly improves VAE performance, achieving state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhancing text-to-image generation quality with higher CLIP scores.

Conclusion: VIVAT preserves the simplicity of the KL-VAE framework while effectively addressing practical challenges, providing valuable insights for optimizing VAE training.

Abstract: Variational Autoencoders (VAEs) remain a cornerstone of generative computer
vision, yet their training is often plagued by artifacts that degrade
reconstruction and generation quality. This paper introduces VIVAT, a
systematic approach to mitigating common artifacts in KL-VAE training without
requiring radical architectural changes. We present a detailed taxonomy of five
prevalent artifacts - color shift, grid patterns, blur, corner and droplet
artifacts - and analyze their root causes. Through straightforward
modifications, including adjustments to loss weights, padding strategies, and
the integration of Spatially Conditional Normalization, we demonstrate
significant improvements in VAE performance. Our method achieves
state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across
multiple benchmarks and enhances text-to-image generation quality, as evidenced
by superior CLIP scores. By preserving the simplicity of the KL-VAE framework
while addressing its practical challenges, VIVAT offers actionable insights for
researchers and practitioners aiming to optimize VAE training.

</details>


### [380] [FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity](https://arxiv.org/abs/2506.07865)
*Jinxi Li,Ziyang Song,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: This paper proposes FreeGave, a method for modeling 3D scene geometry, appearance, and physics from multi-view videos without object priors. It introduces a physics code and divergence-free module to estimate velocity fields, showing superior performance in future frame extrapolation and motion segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing works on modeling 3D scenes using governing PDEs as PINN losses or incorporating physics simulation into neural networks either fail to learn complex physical motions at boundaries or require object priors such as masks or types.

Method: The proposed method, FreeGave, learns the physics of complex dynamic 3D scenes without needing any object priors by introducing a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on inefficient PINN losses.

Result: Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of the method for future frame extrapolation and motion segmentation.

Conclusion: FreeGave successfully learns meaningful 3D physical motion patterns without any human labels in training.

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and the
underlying physics purely from multi-view videos. By applying various governing
PDEs as PINN losses or incorporating physics simulation into neural networks,
existing works often fail to learn complex physical motions at boundaries or
require object priors such as masks or types. In this paper, we propose
FreeGave to learn the physics of complex dynamic 3D scenes without needing any
object priors. The key to our approach is to introduce a physics code followed
by a carefully designed divergence-free module for estimating a per-Gaussian
velocity field, without relying on the inefficient PINN losses. Extensive
experiments on three public datasets and a newly collected challenging
real-world dataset demonstrate the superior performance of our method for
future frame extrapolation and motion segmentation. Most notably, our
investigation into the learned physics codes reveals that they truly learn
meaningful 3D physical motion patterns in the absence of any human labels in
training.

</details>


### [381] [Diffusion models under low-noise regime](https://arxiv.org/abs/2506.07841)
*Elizabeth Pavlova,Xue-Xin Wei*

Main category: cs.CV

TL;DR: 为了填补对生成模型在实际应用中可靠性的理解空白，本研究探讨了扩散模型在低噪声条件下的行为。通过使用CelebA子集和解析高斯混合基准，研究发现即使在高噪声输出收敛的情况下，训练于不相交数据的模型在数据流形附近也会出现差异。此外，还量化了训练集大小、数据几何结构和模型目标选择如何影响去噪轨迹和评分准确性，从而揭示这些模型学习数据分布表示的方式。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在高噪声环境下的行为已被测试，但其在低噪声条件下的行为尚不清楚。因此，需要系统地研究扩散模型在低噪声条件下的表现，以了解其作为有效去噪器的特性，并探讨模型的鲁棒性和可解释性。

Method: 研究采用了CelebA的不同样本大小子集和解析高斯混合基准，来分析扩散模型的行为。重点在于观察模型在低噪声扩散动态下的表现，特别是当训练数据不相交时模型之间的差异，以及训练集大小、数据几何结构和模型目标选择如何影响去噪轨迹和评分准确性。

Result: 研究发现，即使在高噪声输出收敛的情况下，训练于不相交数据的模型在数据流形附近也会出现差异。此外，模型的去噪轨迹和评分准确性受到训练集大小、数据几何结构和模型目标选择的影响。这为理解扩散模型如何学习数据分布提供了新的见解。

Conclusion: 扩散模型在低噪声条件下的行为表明，即使在高噪声条件下模型输出相似，它们在数据流形附近的表示也可能存在显著差异。这一发现有助于提升对生成模型在实际应用中的可靠性的理解，特别是在小扰动常见的场景中。

Abstract: Recent work on diffusion models proposed that they operate in two regimes:
memorization, in which models reproduce their training data, and
generalization, in which they generate novel samples. While this has been
tested in high-noise settings, the behavior of diffusion models as effective
denoisers when the corruption level is small remains unclear. To address this
gap, we systematically investigated the behavior of diffusion models under
low-noise diffusion dynamics, with implications for model robustness and
interpretability. Using (i) CelebA subsets of varying sample sizes and (ii)
analytic Gaussian mixture benchmarks, we reveal that models trained on disjoint
data diverge near the data manifold even when their high-noise outputs
converge. We quantify how training set size, data geometry, and model objective
choice shape denoising trajectories and affect score accuracy, providing
insights into how these models actually learn representations of data
distributions. This work starts to address gaps in our understanding of
generative model reliability in practical applications where small
perturbations are common.

</details>


### [382] [PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement](https://arxiv.org/abs/2506.07848)
*Teng Hu,Zhentao Yu,Zhengguang Zhou,Jiangning Zhang,Yuan Zhou,Qinglin Lu,Ran Yi*

Main category: cs.CV

TL;DR: 提出PolyVivid框架，解决多主体视频定制中的身份一致性与交互问题，通过多种模块增强身份保留和主体间互动，并构建高质量多主体数据管道，实验表明其性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型缺乏精细控制能力，特别是在多主体定制中难以保持一致的身份和实现交互。

Method: 设计了基于VLLM的文本-图像融合模块、基于3D-RoPE的增强模块、注意力继承的身份注入模块以及基于MLLM的数据管道，以分别实现精确接地、结构化双向融合、有效特征注入和高质量数据生成。

Result: 在身份保真度、视频真实感和主体对齐方面表现出色，超越现有的开源和商业基线模型。

Conclusion: PolyVivid框架在多主体视频定制领域实现了更灵活且一致性的生成效果，为未来研究提供了新方向。

Abstract: Despite recent advances in video generation, existing models still lack
fine-grained controllability, especially for multi-subject customization with
consistent identity and interaction. In this paper, we propose PolyVivid, a
multi-subject video customization framework that enables flexible and
identity-consistent generation. To establish accurate correspondences between
subject images and textual entities, we design a VLLM-based text-image fusion
module that embeds visual identities into the textual space for precise
grounding. To further enhance identity preservation and subject interaction, we
propose a 3D-RoPE-based enhancement module that enables structured
bidirectional fusion between text and image embeddings. Moreover, we develop an
attention-inherited identity injection module to effectively inject fused
identity features into the video generation process, mitigating identity drift.
Finally, we construct an MLLM-based data pipeline that combines MLLM-based
grounding, segmentation, and a clique-based subject consolidation strategy to
produce high-quality multi-subject data, effectively enhancing subject
distinction and reducing ambiguity in downstream video generation. Extensive
experiments demonstrate that PolyVivid achieves superior performance in
identity fidelity, video realism, and subject alignment, outperforming existing
open-source and commercial baselines.

</details>


### [383] [A Comparative Study of U-Net Architectures for Change Detection in Satellite Images](https://arxiv.org/abs/2506.07925)
*Yaxita Amin,Naimisha S Trivedi,Rashmi Bhattad*

Main category: cs.CV

TL;DR: Remote sensing change detection using U-Net variations is analyzed in this paper, which evaluated 18 different U-Net architectures for their potential in detecting changes in remote sensing. Aspects such as managing data from different time periods and collecting relationships over a long distance were emphasized to improve the precision of change detection.


<details>
  <summary>Details</summary>
Motivation: To fill the gap in the application of U-Net architecture in remote sensing field by conducting a comprehensive analysis of 34 papers and assessing 18 different U-Net variations for detecting changes in remote sensing.

Method: Comparison and analysis of 18 different U-Net variations for remote sensing change detection, emphasizing variations explicitly built for change detection, such as Siamese Swin-U-Net.

Result: Provided valuable insights on choosing U-Net versions for remote sensing change detection tasks, highlighting aspects such as managing data from different time periods and collecting relationships over a long distance.

Conclusion: This study offers guidance for researchers and practitioners in selecting appropriate U-Net variations for remote sensing change detection.

Abstract: Remote sensing change detection is essential for monitoring the everchanging
landscapes of the Earth. The U-Net architecture has gained popularity for its
capability to capture spatial information and perform pixel-wise
classification. However, their application in the Remote sensing field remains
largely unexplored. Therefore, this paper fill the gap by conducting a
comprehensive analysis of 34 papers. This study conducts a comparison and
analysis of 18 different U-Net variations, assessing their potential for
detecting changes in remote sensing. We evaluate both benefits along with
drawbacks of each variation within the framework of this particular
application. We emphasize variations that are explicitly built for change
detection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.
The analysis highlights the significance of aspects such as managing data from
different time periods and collecting relationships over a long distance to
enhance the precision of change detection. This study provides valuable
insights for researchers and practitioners that choose U-Net versions for
remote sensing change detection tasks.

</details>


### [384] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 尽管视觉-语言模型(VLMs)被广泛认为具有上下文内学习(ICL)能力，但研究发现它们通常依赖浅层启发式方法而非真正理解任务。本文通过评估VLMs在分布偏移下的表现，提出了一种新的带推理的多模态ICL管道，并进行了广泛的实验，结果表明当前VLMs未能有效利用演示信息。


<details>
  <summary>Details</summary>
Motivation: 研究旨在重新评估视觉-语言模型(VLMs)在分布偏移下的多模态上下文内学习(MM-ICL)能力，探索其是否真正基于示例进行学习而非依赖简单启发式方法。

Method: 1. 评估VLMs在不同数据分布下的表现，分析更多示例对性能的影响。
2. 提出一种新的MM-ICL with Reasoning管道，在每个示例中加入生成的推理依据。
3. 使用多种开源及专有模型(如Gemini 2.0)，在感知和推理需求的数据集上进行广泛实验。
4. 控制变量包括示例数量、检索方法、推理质量及分布差异。

Result: 实验结果表明，增加示例数量并未显著提升性能，模型倾向于复制答案而非从示例中学习，且对各种因素的变化表现出有限的敏感性。

Conclusion: 当前VLMs未能有效利用示例级别的信息以实现真正的多模态上下文内学习，需要进一步改进模型设计或训练方法。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [385] [Real-time Localization of a Soccer Ball from a Single Camera](https://arxiv.org/abs/2506.07981)
*Dmitrii Vorobev,Artem Prosvetov,Karim Elhadji Daou*

Main category: cs.CV

TL;DR: 提出了一种从单个广播摄像机进行实时三维足球轨迹重建的计算高效方法，使用多模式状态模型显著加速优化并保持厘米级精度。系统在标准CPU上运行，低延迟适用于现场直播环境，并且在专有数据集上的评估表明其性能可与多摄像机系统相媲美。


<details>
  <summary>Details</summary>
Motivation: 现有的足球轨迹重建方法通常需要多摄像机系统或专用昂贵的基础设施，这限制了它们的广泛应用。因此，研究者们希望开发一种能够在单个广播摄像机下实现高精度、低成本的3D球跟踪技术。

Method: 该方法引入了一个具有W个离散模式的多模式状态模型，以加速优化过程，同时保留厘米级的精度。此模型可以处理严重遮挡、运动模糊和复杂背景等问题。系统在标准CPU上运行，能够实现低延迟，适合用于现场直播环境。

Result: 通过在6K分辨率俄罗斯超级联赛比赛的专有数据集上进行广泛评估，结果表明该系统的性能可以与多摄像机系统相媲美，并且不需要特殊的或昂贵的基础设施。

Conclusion: 这项工作提供了一种实用的方法，可以在专业足球环境中实现可访问且准确的3D球跟踪。

Abstract: We propose a computationally efficient method for real-time three-dimensional
football trajectory reconstruction from a single broadcast camera. In contrast
to previous work, our approach introduces a multi-mode state model with $W$
discrete modes to significantly accelerate optimization while preserving
centimeter-level accuracy -- even in cases of severe occlusion, motion blur,
and complex backgrounds. The system operates on standard CPUs and achieves low
latency suitable for live broadcast settings. Extensive evaluation on a
proprietary dataset of 6K-resolution Russian Premier League matches
demonstrates performance comparable to multi-camera systems, without the need
for specialized or costly infrastructure. This work provides a practical method
for accessible and accurate 3D ball tracking in professional football
environments.

</details>


### [386] [CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray](https://arxiv.org/abs/2506.07984)
*Mingquan Lin,Gregory Holste,Song Wang,Yiliang Zhou,Yishu Wei,Imon Banerjee,Pengyi Chen,Tianjie Dai,Yuexi Du,Nicha C. Dvornek,Yuyan Ge,Zuowei Guo,Shouhei Hanaoka,Dongkyun Kim,Pablo Messina,Yang Lu,Denis Parra,Donghyun Son,Álvaro Soto,Aisha Urooj,René Vidal,Yosuke Yamagishi,Zefan Yang,Ruichi Zhang,Yang Zhou,Leo Anthony Celi,Ronald M. Summers,Zhiyong Lu,Hao Chen,Adam Flanders,George Shih,Zhangyang Wang,Yifan Peng*

Main category: cs.CV

TL;DR: CXR-LT 2024 is an initiative that expands a dataset to 377,110 CXRs and 45 disease labels for improving lung disease classification using chest X-rays. It introduces zero-shot learning and provides three tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance lung disease classification using chest X-rays by tackling challenges in open long-tailed lung disease classification and improving the measurability of state-of-the-art techniques.

Method: Providing high-quality benchmark CXR data, conducting comprehensive evaluations, expanding the dataset to include more disease labels and introducing zero-shot learning.

Result: Details the data curation process and consolidates state-of-the-art solutions including multimodal models, advanced generative approaches, and zero-shot learning strategies.

Conclusion: Aims to advance the development of clinically realistic and generalizable diagnostic models for chest radiography.

Abstract: The CXR-LT series is a community-driven initiative designed to enhance lung
disease classification using chest X-rays (CXR). It tackles challenges in open
long-tailed lung disease classification and enhances the measurability of
state-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve
these goals by providing high-quality benchmark CXR data for model development
and conducting comprehensive evaluations to identify ongoing issues impacting
lung disease classification performance. Building on the success of CXR-LT
2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45
disease labels, including 19 new rare disease findings. It also introduces a
new focus on zero-shot learning to address limitations identified in the
previous event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed
classification on a large, noisy test set, (ii) long-tailed classification on a
manually annotated "gold standard" subset, and (iii) zero-shot generalization
to five previously unseen disease findings. This paper provides an overview of
CXR-LT 2024, detailing the data curation process and consolidating
state-of-the-art solutions, including the use of multimodal models for rare
disease detection, advanced generative approaches to handle noisy labels, and
zero-shot learning strategies for unseen diseases. Additionally, the expanded
dataset enhances disease coverage to better represent real-world clinical
settings, offering a valuable resource for future research. By synthesizing the
insights and innovations of participating teams, we aim to advance the
development of clinically realistic and generalizable diagnostic models for
chest radiography.

</details>


### [387] [Rethinking Crowd-Sourced Evaluation of Neuron Explanations](https://arxiv.org/abs/2506.07985)
*Tuomas Oikarinen,Ge Yan,Akshay Kulkarni,Tsui-Wei Weng*

Main category: cs.CV

TL;DR: 为了提高神经元解释的评估效率和准确性，本文提出了一种基于重要性采样的众包评估策略，并通过贝叶斯方法减少所需评分数量，最终对两种视觉模型的热门神经元解释方法进行了大规模质量比较。


<details>
  <summary>Details</summary>
Motivation: 当前自动产生神经元解释的方法缺乏可靠的评估标准，传统的众包评估存在噪声且成本高昂，因此需要一种更高效、准确的评估方法。

Method: 分析评估流程，引入重要性采样确定最有价值的输入以降低30倍成本；分析众包标签噪声并采用贝叶斯方法聚合评分以进一步减少评分需求5倍。

Result: 开发出一种高性价比的众包评估策略，成功用于大规模比较两种视觉模型中流行神经元解释方法的质量。

Conclusion: 新提出的评估策略显著提高了神经元解释评估的效率和可靠性，为未来研究提供了可靠工具。

Abstract: Interpreting individual neurons or directions in activations space is an
important component of mechanistic interpretability. As such, many algorithms
have been proposed to automatically produce neuron explanations, but it is
often not clear how reliable these explanations are, or which methods produce
the best explanations. This can be measured via crowd-sourced evaluations, but
they can often be noisy and expensive, leading to unreliable results. In this
paper, we carefully analyze the evaluation pipeline and develop a
cost-effective and highly accurate crowdsourced evaluation strategy. In
contrast to previous human studies that only rate whether the explanation
matches the most highly activating inputs, we estimate whether the explanation
describes neuron activations across all inputs. To estimate this effectively,
we introduce a novel application of importance sampling to determine which
inputs are the most valuable to show to raters, leading to around 30x cost
reduction compared to uniform sampling. We also analyze the label noise present
in crowd-sourced evaluations and propose a Bayesian method to aggregate
multiple ratings leading to a further ~5x reduction in number of ratings
required for the same accuracy. Finally, we use these methods to conduct a
large-scale study comparing the quality of neuron explanations produced by the
most popular methods for two different vision models.

</details>


### [388] [MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation](https://arxiv.org/abs/2506.07999)
*Junhao Chen,Yulia Tsvetkov,Xiaochuang Han*

Main category: cs.CV

TL;DR: MADFormer, a Mixed Autoregressive and Diffusion Transformer, partitions image generation into spatial blocks using AR and diffusion layers for global conditioning and local refinement respectively. Block-wise partitioning improves high-resolution image performance, while vertically mixing AR and diffusion layers offers better quality-efficiency balances.


<details>
  <summary>Details</summary>
Motivation: Existing hybrids of autoregressive (AR) and diffusion-based approaches in multimodal generation often lack systematic guidance on how to allocate model capacity between these paradigms.

Method: MADFormer partitions image generation into spatial blocks, using AR layers for one-pass global conditioning across blocks and diffusion layers for iterative local refinement within each block.

Result: Block-wise partitioning significantly improves performance on high-resolution images. Vertically mixing AR and diffusion layers yields better quality-efficiency balances--improving FID by up to 75% under constrained inference compute.

Conclusion: The findings offer practical design principles for future hybrid generative models.

Abstract: Recent progress in multimodal generation has increasingly combined
autoregressive (AR) and diffusion-based approaches, leveraging their
complementary strengths: AR models capture long-range dependencies and produce
fluent, context-aware outputs, while diffusion models operate in continuous
latent spaces to refine high-fidelity visual details. However, existing hybrids
often lack systematic guidance on how and why to allocate model capacity
between these paradigms. In this work, we introduce MADFormer, a Mixed
Autoregressive and Diffusion Transformer that serves as a testbed for analyzing
AR-diffusion trade-offs. MADFormer partitions image generation into spatial
blocks, using AR layers for one-pass global conditioning across blocks and
diffusion layers for iterative local refinement within each block. Through
controlled experiments on FFHQ-1024 and ImageNet, we identify two key insights:
(1) block-wise partitioning significantly improves performance on
high-resolution images, and (2) vertically mixing AR and diffusion layers
yields better quality-efficiency balances--improving FID by up to 75% under
constrained inference compute. Our findings offer practical design principles
for future hybrid generative models.

</details>


### [389] [Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations](https://arxiv.org/abs/2506.07943)
*Yizhen Li,Dell Zhang,Xuelong Li,Yiqing Shen*

Main category: cs.CV

TL;DR: DTwinSeger是一种新的推理分割方法，它通过数字孪生表示作为中间层来解耦感知和推理，将RS重新定义为两阶段过程，首先将图像转换为保留空间关系和语义属性的结构化数字孪生表示，然后使用大语言模型对此表示进行显式推理以识别目标对象。实验表明该方法在两个图像推理分割基准和三个图像指代分割基准上达到最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的推理分割方法依赖于微调视觉-语言模型来进行感知和推理，但其图像的标记化从根本上破坏了物体之间的连续空间关系。

Method: DTwinSeger引入了数字孪生表示作为中间层来解耦感知和推理，并将RS重新定义为两阶段过程：第一阶段将图像转换为保留空间关系和语义属性的结构化数字孪生表示；第二阶段使用大型语言模型对该表示进行显式推理以识别目标对象。此外，还提出了一种针对具有数字孪生表示的大型语言模型的监督微调方法以及相应的微调数据集Seg-DT。

Result: 实验表明，该方法在两个图像推理分割基准和三个图像指代分割基准上达到最先进的性能。数字孪生表示作为视觉和文本之间有效的桥梁，使复杂的多模态推理任务能够仅通过大型语言模型完成。

Conclusion: DTwinSeger通过引入数字孪生表示作为中间层，成功地解耦了感知与推理，并实现了最先进的性能，展示了数字孪生表示在多模态推理任务中的潜力。

Abstract: Reasoning Segmentation (RS) is a multimodal vision-text task that requires
segmenting objects based on implicit text queries, demanding both precise
visual perception and vision-text reasoning capabilities. Current RS approaches
rely on fine-tuning vision-language models (VLMs) for both perception and
reasoning, but their tokenization of images fundamentally disrupts continuous
spatial relationships between objects. We introduce DTwinSeger, a novel RS
approach that leverages Digital Twin (DT) representation as an intermediate
layer to decouple perception from reasoning. Innovatively, DTwinSeger
reformulates RS as a two-stage process, where the first transforms the image
into a structured DT representation that preserves spatial relationships and
semantic properties and then employs a Large Language Model (LLM) to perform
explicit reasoning over this representation to identify target objects. We
propose a supervised fine-tuning method specifically for LLM with DT
representation, together with a corresponding fine-tuning dataset Seg-DT, to
enhance the LLM's reasoning capabilities with DT representations. Experiments
show that our method can achieve state-of-the-art performance on two image RS
benchmarks and three image referring segmentation benchmarks. It yields that DT
representation functions as an effective bridge between vision and text,
enabling complex multimodal reasoning tasks to be accomplished solely with an
LLM.

</details>


### [390] [Hidden in plain sight: VLMs overlook their visual representations](https://arxiv.org/abs/2506.08008)
*Stephanie Fu,Tyler Bonnen,Devin Guillory,Trevor Darrell*

Main category: cs.CV

TL;DR: 视觉语言模型（VLMs）在视觉任务上的表现远不如其视觉编码器，主要问题在于未能有效利用视觉信息，而是继承了语言模型的先验。


<details>
  <summary>Details</summary>
Motivation: 研究视觉语言模型如何整合视觉和语言信息，并诊断开源VLMs在视觉任务中的失败模式。

Method: 将VLMs与其视觉编码器进行比较，通过一系列视觉中心基准测试（如深度估计、对应关系等），分析VLMs在视觉任务中的表现。进一步从视觉表示退化、任务提示的脆弱性以及语言模型在任务解决中的作用三个方面进行深入分析。

Result: 发现VLMs在视觉任务上的表现显著低于其视觉编码器，几乎接近随机水平。问题主要集中在语言模型部分，VLMs未能有效利用整个模型中可获得的视觉信息，反而继承了语言模型的先验。

Conclusion: 本研究有助于诊断开源VLMs的失败模式，并提出了一系列对未来VLMs视觉理解研究有用的评估方法。

Abstract: Language provides a natural interface to specify and evaluate performance on
visual tasks. To realize this possibility, vision language models (VLMs) must
successfully integrate visual and linguistic information. Our work compares
VLMs to a direct readout of their visual encoders to understand their ability
to integrate across these modalities. Across a series of vision-centric
benchmarks (e.g., depth estimation, correspondence), we find that VLMs perform
substantially worse than their visual encoders, dropping to near-chance
performance. We investigate these results through a series of analyses across
the entire VLM: namely 1) the degradation of vision representations, 2)
brittleness to task prompt, and 3) the language model's role in solving the
task. We find that the bottleneck in performing these vision-centric tasks lies
in this third category; VLMs are not effectively using visual information
easily accessible throughout the entire model, and they inherit the language
priors present in the LLM. Our work helps diagnose the failure modes of
open-source VLMs, and presents a series of evaluations useful for future
investigations into visual understanding within VLMs.

</details>


### [391] [Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion](https://arxiv.org/abs/2506.08009)
*Xun Huang,Zhengqi Li,Guande He,Mingyuan Zhou,Eli Shechtman*

Main category: cs.CV

TL;DR: The paper introduces Self Forcing, a new training method for autoregressive video diffusion models that tackles exposure bias and achieves real-time streaming video generation.


<details>
  <summary>Details</summary>
Motivation: To address the issue of exposure bias in autoregressive video diffusion models, where models must generate sequences conditioned on their own imperfect outputs during inference rather than ground-truth context.

Method: Self Forcing conditions each frame's generation on previously self-generated outputs using autoregressive rollout with key-value (KV) caching during training. It employs a holistic loss at the video level, a few-step diffusion model, stochastic gradient truncation, and a rolling KV cache mechanism.

Result: Self Forcing enables real-time streaming video generation with sub-second latency on a single GPU while matching or surpassing the quality of slower non-causal diffusion models.

Conclusion: Self Forcing is an effective solution to exposure bias in video diffusion models, achieving high-quality real-time video generation.

Abstract: We introduce Self Forcing, a novel training paradigm for autoregressive video
diffusion models. It addresses the longstanding issue of exposure bias, where
models trained on ground-truth context must generate sequences conditioned on
their own imperfect outputs during inference. Unlike prior methods that denoise
future frames based on ground-truth context frames, Self Forcing conditions
each frame's generation on previously self-generated outputs by performing
autoregressive rollout with key-value (KV) caching during training. This
strategy enables supervision through a holistic loss at the video level that
directly evaluates the quality of the entire generated sequence, rather than
relying solely on traditional frame-wise objectives. To ensure training
efficiency, we employ a few-step diffusion model along with a stochastic
gradient truncation strategy, effectively balancing computational cost and
performance. We further introduce a rolling KV cache mechanism that enables
efficient autoregressive video extrapolation. Extensive experiments demonstrate
that our approach achieves real-time streaming video generation with sub-second
latency on a single GPU, while matching or even surpassing the generation
quality of significantly slower and non-causal diffusion models. Project
website: http://self-forcing.github.io/

</details>


### [392] [StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets](https://arxiv.org/abs/2506.08013)
*Anh-Quan Cao,Ivan Lopes,Raoul de Charette*

Main category: cs.CV

TL;DR: StableMTL is a multi-task learning method that uses diffusion models for zero-shot learning, with a unified latent loss and task-attention mechanism, outperforming baselines on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multi-task learning for dense prediction is constrained by the need for extensive annotations. Recent works have explored training with partial task labels, but there is a need to extend this to a zero-shot setting where tasks can be performed without specific task annotations during training.

Method: The method, StableMTL, repurposes image generators for latent regression using a denoising framework with task encoding and per-task conditioning. It adopts a unified latent loss instead of individual task losses, allowing it to scale to more tasks. A multi-stream model with a task-attention mechanism is introduced to promote cross-task sharing and synergy.

Result: StableMTL outperforms baseline methods on 7 tasks across 8 different benchmarks.

Conclusion: StableMTL successfully extends multi-task learning to a zero-shot setting, leveraging diffusion models and introducing innovations such as a unified latent loss and task-attention mechanism, achieving superior performance.

Abstract: Multi-task learning for dense prediction is limited by the need for extensive
annotation for every task, though recent works have explored training with
partial task labels. Leveraging the generalization power of diffusion models,
we extend the partial learning setup to a zero-shot setting, training a
multi-task model on multiple synthetic datasets, each labeled for only a subset
of tasks. Our method, StableMTL, repurposes image generators for latent
regression. Adapting a denoising framework with task encoding, per-task
conditioning and a tailored training scheme. Instead of per-task losses
requiring careful balancing, a unified latent loss is adopted, enabling
seamless scaling to more tasks. To encourage inter-task synergy, we introduce a
multi-stream model with a task-attention mechanism that converts N-to-N task
interactions into efficient 1-to-N attention, promoting effective cross-task
sharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.

</details>


### [393] [SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design](https://arxiv.org/abs/2506.07964)
*Wenxin Tang,Jingyu Xiao,Wenxuan Jiang,Xi Xiao,Yuhang Wang,Xuxin Tang,Qing Li,Yuehe Ma,Junliang Liu,Shisong Tang,Michael R. Lyu*

Main category: cs.CV

TL;DR: This paper proposes Slide2Code, a benchmark for Reference Image to Slide Generation task with a new Slide Complexity Metric. It also introduces SlideCoder, a framework using Color Gradient-based Segmentation and Hierarchical Retrieval-Augmented Generation methods for creating editable slides from reference images. Additionally, the 7B-parameter SlideMaster model is released. Experiments indicate SlideCoder significantly outperforms existing baselines.


<details>
  <summary>Details</summary>
Motivation: Slide creation manually is labor-intensive and demands expert knowledge. Current natural language-based LLM generation methods fail to capture the visual and structural nuances of slide designs.

Method: The authors formalize the Reference Image to Slide Generation task and propose Slide2Code benchmark with difficulty-tiered samples based on a novel Slide Complexity Metric. They introduce SlideCoder, a layout-aware, retrieval-augmented framework integrating Color Gradient-based Segmentation algorithm and Hierarchical Retrieval-Augmented Generation method. They also release SlideMaster, an open-source model fine-tuned with improved reverse-engineered data.

Result: Experiments show that SlideCoder surpasses state-of-the-art baselines by up to 40.5 points across layout fidelity, execution accuracy, and visual consistency.

Conclusion: The proposed SlideCoder framework effectively generates editable slides from reference images and outperforms existing methods. The associated Slide2Code benchmark and SlideMaster model provide valuable resources for future research.

Abstract: Manual slide creation is labor-intensive and requires expert prior knowledge.
Existing natural language-based LLM generation methods struggle to capture the
visual and structural nuances of slide designs. To address this, we formalize
the Reference Image to Slide Generation task and propose Slide2Code, the first
benchmark with difficulty-tiered samples based on a novel Slide Complexity
Metric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework
for generating editable slides from reference images. SlideCoder integrates a
Color Gradient-based Segmentation algorithm and a Hierarchical
Retrieval-Augmented Generation method to decompose complex tasks and enhance
code generation. We also release SlideMaster, a 7B open-source model fine-tuned
with improved reverse-engineered data. Experiments show that SlideCoder
outperforms state-of-the-art baselines by up to 40.5 points, demonstrating
strong performance across layout fidelity, execution accuracy, and visual
consistency. Our code is available at
https://github.com/vinsontang1/SlideCoder.

</details>


### [394] [Audio-Sync Video Generation with Multi-Stream Temporal Control](https://arxiv.org/abs/2506.08003)
*Shuchen Weng,Haojie Zheng,Zheng Chang,Si Li,Boxin Shi,Xinlong Wang*

Main category: cs.CV

TL;DR: The paper introduces MTV, a framework for audio-sync video generation that separates audio into speech, effects, and music tracks for fine-grained control, and DEMIX, a dataset supporting scalable training. Experiments show state-of-the-art performance in various metrics.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to generate high-quality videos with precise audio-visual synchronization, especially across diverse and complex audio types.

Method: MTV explicitly separates audios into speech, effects, and music tracks, enabling disentangled control over lip motion, event timing, and visual mood respectively.

Result: MTV achieves state-of-the-art performance across six standard metrics spanning video quality, text-video consistency, and audio-video alignment.

Conclusion: MTV is a versatile framework for audio-sync video generation and DEMIX is a valuable dataset for scalable multi-stage training.

Abstract: Audio is inherently temporal and closely synchronized with the visual world,
making it a naturally aligned and expressive control signal for controllable
video generation (e.g., movies). Beyond control, directly translating audio
into video is essential for understanding and visualizing rich audio narratives
(e.g., Podcasts or historical recordings). However, existing approaches fall
short in generating high-quality videos with precise audio-visual
synchronization, especially across diverse and complex audio types. In this
work, we introduce MTV, a versatile framework for audio-sync video generation.
MTV explicitly separates audios into speech, effects, and music tracks,
enabling disentangled control over lip motion, event timing, and visual mood,
respectively -- resulting in fine-grained and semantically aligned video
generation. To support the framework, we additionally present DEMIX, a dataset
comprising high-quality cinematic videos and demixed audio tracks. DEMIX is
structured into five overlapped subsets, enabling scalable multi-stage training
for diverse generation scenarios. Extensive experiments demonstrate that MTV
achieves state-of-the-art performance across six standard metrics spanning
video quality, text-video consistency, and audio-video alignment. Project page:
https://hjzheng.net/projects/MTV/.

</details>


### [395] [Dynamic View Synthesis as an Inverse Problem](https://arxiv.org/abs/2506.08004)
*Hidir Yesiltepe,Pinar Yanardag*

Main category: cs.CV

TL;DR: This paper presents a method for dynamic view synthesis from monocular videos without training, by redesigning the noise initialization phase of a pre-trained video diffusion model.


<details>
  <summary>Details</summary>
Motivation: To achieve high-fidelity dynamic view synthesis from monocular videos without any weight updates or auxiliary modules, addressing the issue of deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules.

Method: Redesign the noise initialization phase using K-order Recursive Noise Representation and Stochastic Latent Modulation to perform visibility aware sampling over the latent space.

Result: Comprehensive experiments demonstrate effective dynamic view synthesis through structured latent manipulation in the noise initialization phase.

Conclusion: Dynamic view synthesis can be successfully performed without training by manipulating latents in the noise initialization phase of a pre-trained video diffusion model.

Abstract: In this work, we address dynamic view synthesis from monocular videos as an
inverse problem in a training-free setting. By redesigning the noise
initialization phase of a pre-trained video diffusion model, we enable
high-fidelity dynamic view synthesis without any weight updates or auxiliary
modules. We begin by identifying a fundamental obstacle to deterministic
inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and
resolve it by introducing a novel noise representation, termed K-order
Recursive Noise Representation. We derive a closed form expression for this
representation, enabling precise and efficient alignment between the
VAE-encoded and the DDIM inverted latents. To synthesize newly visible regions
resulting from camera motion, we introduce Stochastic Latent Modulation, which
performs visibility aware sampling over the latent space to complete occluded
regions. Comprehensive experiments demonstrate that dynamic view synthesis can
be effectively performed through structured latent manipulation in the noise
initialization phase.

</details>


### [396] [Vision Transformers Don't Need Trained Registers](https://arxiv.org/abs/2506.08010)
*Nick Jiang,Amil Dravid,Alexei Efros,Yossi Gandelsman*

Main category: cs.CV

TL;DR: The paper investigates high-norm tokens in Vision Transformers that cause noisy attention maps and proposes a training-free solution to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: To address the problem of high-norm tokens leading to noisy attention maps in Vision Transformers without requiring retraining of models.

Method: By shifting high-norm activations from specific neurons into an additional untrained token, effectively mimicking register tokens in models not originally trained with them.

Result: Produces cleaner attention and feature maps, enhances performance on downstream tasks, and achieves results comparable to models trained with register tokens. Also improves interpretability in vision-language models.

Conclusion: Test-time registers can effectively replace register tokens at test-time, providing a training-free solution for pre-trained models.

Abstract: We investigate the mechanism underlying a previously identified phenomenon in
Vision Transformers -- the emergence of high-norm tokens that lead to noisy
attention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a
sparse set of neurons is responsible for concentrating high-norm activations on
outlier tokens, leading to irregular attention patterns and degrading
downstream visual processing. While the existing solution for removing these
outliers involves retraining models from scratch with additional learned
register tokens, we use our findings to create a training-free approach to
mitigate these artifacts. By shifting the high-norm activations from our
discovered register neurons into an additional untrained token, we can mimic
the effect of register tokens on a model already trained without registers. We
demonstrate that our method produces cleaner attention and feature maps,
enhances performance over base models across multiple downstream visual tasks,
and achieves results comparable to models explicitly trained with register
tokens. We then extend test-time registers to off-the-shelf vision-language
models to improve their interpretability. Our results suggest that test-time
registers effectively take on the role of register tokens at test-time,
offering a training-free solution for any pre-trained model released without
them.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [397] [\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality](https://arxiv.org/abs/2506.06622)
*Yifan Zeng*

Main category: cs.CE

TL;DR: QuantMCP is a framework that connects Large Language Models with financial data APIs through the Model Context Protocol, enhancing LLMs' ability to access accurate financial information and improving their analytical capabilities for better decision-making in finance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Large Language Models in financial analysis due to data hallucination and lack of access to real-time financial information.

Method: By introducing QuantMCP, a framework that uses the Model Context Protocol to securely invoke Python-accessible financial data APIs, allowing LLMs to accurately retrieve up-to-date financial data.

Result: Users can interact via natural language to retrieve precise financial data, unlocking LLM's analytical capabilities for sophisticated data interpretation and insight generation.

Conclusion: QuantMCP provides a secure and extensible bridge between conversational AI and financial data, improving the reliability and depth of LLM applications in finance.

Abstract: Large Language Models (LLMs) hold immense promise for revolutionizing
financial analysis and decision-making, yet their direct application is often
hampered by issues of data hallucination and lack of access to real-time,
verifiable financial information. This paper introduces QuantMCP, a novel
framework designed to rigorously ground LLMs in financial reality. By
leveraging the Model Context Protocol (MCP) for standardized and secure tool
invocation, QuantMCP enables LLMs to accurately interface with a diverse array
of Python-accessible financial data APIs (e.g., Wind, yfinance). Users can
interact via natural language to precisely retrieve up-to-date financial data,
thereby overcoming LLM's inherent limitations in factual data recall. More
critically, once furnished with this verified, structured data, the LLM's
analytical capabilities are unlocked, empowering it to perform sophisticated
data interpretation, generate insights, and ultimately support more informed
financial decision-making processes. QuantMCP provides a robust, extensible,
and secure bridge between conversational AI and the complex world of financial
data, aiming to enhance both the reliability and the analytical depth of LLM
applications in finance.

</details>


### [398] [Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market](https://arxiv.org/abs/2506.06356)
*Yimin Du*

Main category: cs.CE

TL;DR: This paper presents a multi-day turnover quantitative trading algorithm for the Chinese A-share market that integrates deep learning techniques with cross-sectional stock prediction, achieving 15.2% annualized returns, maximum drawdown below 5%, and a Sharpe ratio of 1.87.


<details>
  <summary>Details</summary>
Motivation: To develop a sophisticated quantitative trading algorithm that combines deep learning techniques with comprehensive cross-sectional stock prediction to achieve high performance in the Chinese A-share market while maintaining risk control and capital efficiency.

Method: The method includes five interconnected modules: initial stock selection through deep cross-sectional prediction networks, opening signal distribution analysis using mixture models for arbitrage identification, market capitalization and liquidity-based dynamic position sizing, grid-search optimized profit-taking and stop-loss mechanisms, and multi-granularity volatility-based market timing models.

Result: The algorithm achieves 15.2% annualized returns, maximum drawdown constrained below 5%, and a Sharpe ratio of 1.87 when backtested on 2021-2024 data.

Conclusion: The strategy demonstrates exceptional scalability, robustness across various market regimes, and high capital capacity suitable for institutional deployment.

Abstract: This paper presents a sophisticated multi-day turnover quantitative trading
algorithm that integrates advanced deep learning techniques with comprehensive
cross-sectional stock prediction for the Chinese A-share market. Our framework
combines five interconnected modules: initial stock selection through deep
cross-sectional prediction networks, opening signal distribution analysis using
mixture models for arbitrage identification, market capitalization and
liquidity-based dynamic position sizing, grid-search optimized profit-taking
and stop-loss mechanisms, and multi-granularity volatility-based market timing
models. The algorithm employs a novel approach to balance capital efficiency
with risk management through adaptive holding periods and sophisticated
entry/exit timing. Trained on comprehensive A-share data from 2010-2020 and
rigorously backtested on 2021-2024 data, our method achieves remarkable
performance with 15.2\% annualized returns, maximum drawdown constrained below
5\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional
scalability by maintaining 50-100 daily positions with a 9-day maximum holding
period, incorporating dynamic profit-taking and stop-loss mechanisms that
enhance capital turnover efficiency while preserving risk-adjusted returns. Our
approach exhibits robust performance across various market regimes while
maintaining high capital capacity suitable for institutional deployment.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [399] [The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage](https://arxiv.org/abs/2506.06484)
*Manuel Sage,Khalil Al Handawi,Yaoyao Fiona Zhao*

Main category: eess.SY

TL;DR: The paper explores the application of Deep Reinforcement Learning (DRL) for the economic operation of Power-to-Gas (P2G) systems combined with Battery Energy Storage Systems (BESs) and gas turbines over extended periods. It proposes modifications to DRL algorithms to address delayed reward issues, improving their effectiveness in devising cost-effective operation strategies.


<details>
  <summary>Details</summary>
Motivation: Power-to-Gas (P2G) technologies are recognized for integrating intermittent renewables into electricity grids, but determining their most cost-effective operation is complex due to volatile renewable energy, electricity prices, and loads. P2G systems are less efficient than battery energy storage systems (BESs), and previous research has focused on short-term studies neglecting long-term storage capabilities.

Method: The study examines how DRL can be applied to the economic operation of P2G systems combined with BESs and gas turbines over extended periods through three progressively more complex case studies. Modifications such as integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations are introduced to enhance the effectiveness of DRL algorithms.

Result: Findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the proposed modifications significantly improve its capability to devise cost-effective operation strategies.

Conclusion: The modifications to DRL algorithms unlock the potential for long-term energy storage in P2G technologies, enhancing their cost-effectiveness and ability to integrate with intermittent renewables.

Abstract: Power-to-Gas (P2G) technologies gain recognition for enabling the integration
of intermittent renewables, such as wind and solar, into electricity grids.
However, determining the most cost-effective operation of these systems is
complex due to the volatile nature of renewable energy, electricity prices, and
loads. Additionally, P2G systems are less efficient in converting and storing
energy compared to battery energy storage systems (BESs), and the benefits of
converting electricity into gas are not immediately apparent. Deep
Reinforcement Learning (DRL) has shown promise in managing the operation of
energy systems amidst these uncertainties. Yet, DRL techniques face
difficulties with the delayed reward characteristic of P2G system operation.
Previous research has mostly focused on short-term studies that look at the
energy conversion process, neglecting the long-term storage capabilities of
P2G.
  This study presents a new method by thoroughly examining how DRL can be
applied to the economic operation of P2G systems, in combination with BESs and
gas turbines, over extended periods. Through three progressively more complex
case studies, we assess the performance of DRL algorithms, specifically Deep
Q-Networks and Proximal Policy Optimization, and introduce modifications to
enhance their effectiveness. These modifications include integrating forecasts,
implementing penalties on the reward function, and applying strategic cost
calculations, all aimed at addressing the issue of delayed rewards. Our
findings indicate that while DRL initially struggles with the complex
decision-making required for P2G system operation, the adjustments we propose
significantly improve its capability to devise cost-effective operation
strategies, thereby unlocking the potential for long-term energy storage in P2G
technologies.

</details>


### [400] [From Model-Based and Adaptive Control to Evolving Fuzzy Control](https://arxiv.org/abs/2506.06594)
*Daniel Leite,Igor Škrjanc,Fernando Gomide*

Main category: eess.SY

TL;DR: Evolving fuzzy systems create and adjust fuzzy models like predictors and controllers by updating their rule-base structure incrementally from data streams. This paper revisits the development of fuzzy and adaptive modeling and control frameworks, emphasizing the importance of evolving intelligent systems in dealing with nonstationary environments and discussing key challenges such as safety, interpretability, and principled structural evolution.


<details>
  <summary>Details</summary>
Motivation: To revisit the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks, and to highlight the emergence and significance of evolving intelligent systems in fuzzy modeling and control.

Method: Revisiting the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks, and emphasizing the advantages of evolving intelligent systems in handling nonstationary environments.

Result: The paper discusses the key challenges and future directions in the field of evolving fuzzy systems, including safety, interpretability, and principled structural evolution.

Conclusion: Evolving fuzzy systems are significant in handling nonstationary environments, but there are still key challenges to be addressed in terms of safety, interpretability, and principled structural evolution.

Abstract: Evolving fuzzy systems build and adapt fuzzy models - such as predictors and
controllers - by incrementally updating their rule-base structure from data
streams. On the occasion of the 60-year anniversary of fuzzy set theory,
commemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the
historical development and core contributions of classical fuzzy and adaptive
modeling and control frameworks. It then highlights the emergence and
significance of evolving intelligent systems in fuzzy modeling and control,
emphasizing their advantages in handling nonstationary environments. Key
challenges and future directions are discussed, including safety,
interpretability, and principled structural evolution.

</details>


### [401] [On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)](https://arxiv.org/abs/2506.07079)
*Mostafa Eslami,Maryam Babazadeh*

Main category: eess.SY

TL;DR: The paper proposes a hybrid control framework for port-Hamiltonian systems using Data-Assisted Control (DAC) to handle uncertainties and preserve system structure. It splits the system into intrinsic Hamiltonian flow and dissipative/input flow, managed by nonlinear control and Reinforcement Learning respectively.


<details>
  <summary>Details</summary>
Motivation: To create a control framework that can manage both structural and parametric uncertainties in port-Hamiltonian systems while preserving their inherent structure.

Method: A dynamic decomposition based on Data-Assisted Control (DAC) is employed to split the system's evolution into two parts: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a dissipative/input flow addressing both structural and parametric uncertainties. A virtual port variable serves as the interface between these components.

Result: This approach effectively manages RHS uncertainties while preserving the system's inherent structure. Advantages include adjustable performance via LHS controller parameters, enhanced AI explainability and interpretability, the ability to guarantee safety and state attainability with constraints, reduced complexity in learning hypothesis classes, and improved state/parameter estimation.

Conclusion: The proposed hybrid control framework provides a modular architecture for controlling port-Hamiltonian systems, offering significant advantages in managing uncertainties, enhancing explainability, ensuring safety, reducing learning complexity, and improving estimation. Stability and robustness analysis are also investigated, and an application example demonstrates its empirical benefits.

Abstract: This paper introduces a hypothetical hybrid control framework for
port-Hamiltonian (p$\mathcal{H}$) systems, employing a dynamic decomposition
based on Data-Assisted Control (DAC). The system's evolution is split into two
parts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow
handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a
dissipative/input flow addressing both structural and parametric uncertainties.
A virtual port variable $\Pi$ serves as the interface between these two
components. A nonlinear controller manages the intrinsic Hamiltonian flow,
determining a desired port control value $\Pi_c$. Concurrently, Reinforcement
Learning (RL) is applied to the dissipative/input flow to learn an agent for
providing optimal policy in mapping $\Pi_c$ to the actual system input. This
hybrid approach effectively manages RHS uncertainties while preserving the
system's inherent structure. Key advantages include adjustable performance via
LHS controller parameters, enhanced AI explainability and interpretability
through the port variable $\Pi$, the ability to guarantee safety and state
attainability with hard/soft constraints, reduced complexity in learning
hypothesis classes compared to end-to-end solutions, and improved
state/parameter estimation using LHS prior knowledge and system Hamiltonian to
address partial observability. The paper details the p$\mathcal{H}$
formulation, derives the decomposition, and presents the modular controller
architecture. Beyond design, crucial aspects of stability and robustness
analysis and synthesis are investigated, paving the way for deeper theoretical
investigations. An application example, a pendulum with nonlinear dynamics, is
simulated to demonstrate the approach's empirical and phenomenological benefits
for future research.

</details>


### [402] [Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems](https://arxiv.org/abs/2506.07347)
*Armin Lederer,Erfaun Noorani,Andreas Krause*

Main category: eess.SY

TL;DR: This paper proposes a novel risk-sensitive safety filter for discrete-time multi-agent systems with uncertain dynamics, using control barrier functions and centralized risk-sensitive safety conditions. It introduces a distributed formulation of the safety filter through two alternative strategies to ensure feasibility.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety in multi-agent systems is challenging, especially in settings where centralized coordination is impractical.

Method: The method involves proposing a risk-sensitive safety filter for multi-agent systems that leverages control barrier functions defined through value functions. Centralized risk-sensitive safety conditions based on exponential risk operators are used to ensure robustness against model uncertainties. A distributed formulation of the safety filter is introduced by deriving two alternative strategies: one based on worst-case anticipation and another on proximity to a known safe policy.

Result: Through detailed numerical evaluations, the approach was shown to be effective in maintaining safety without being overly conservative.

Conclusion: The proposed risk-sensitive safety filter provides an effective way to ensure safety in multi-agent systems with uncertain dynamics.

Abstract: Ensuring safety in multi-agent systems is a significant challenge,
particularly in settings where centralized coordination is impractical. In this
work, we propose a novel risk-sensitive safety filter for discrete-time
multi-agent systems with uncertain dynamics that leverages control barrier
functions (CBFs) defined through value functions. Our approach relies on
centralized risk-sensitive safety conditions based on exponential risk
operators to ensure robustness against model uncertainties. We introduce a
distributed formulation of the safety filter by deriving two alternative
strategies: one based on worst-case anticipation and another on proximity to a
known safe policy. By allowing agents to switch between strategies, feasibility
can be ensured. Through detailed numerical evaluations, we demonstrate the
efficacy of our approach in maintaining safety without being overly
conservative.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [403] [Neural networks with image recognition by pairs](https://arxiv.org/abs/2506.06322)
*Polad Geidarov*

Main category: cs.NE

TL;DR: This paper explores transforming metric recognition neural networks to apply classical learning algorithms, simplifying training and enabling network expansion without altering existing weights.


<details>
  <summary>Details</summary>
Motivation: To adapt metric recognition neural networks for classical learning algorithms without using analytical expressions for weight calculation, thus simplifying the learning process and allowing network expansion.

Method: Transforming the architecture of metric recognition neural networks so that training can be conducted by recognizing images in pairs, maintaining simplicity and transparency in architecture, and enabling the addition of new images without changing previous weight values.

Result: The approach results in a simplified and reliable training process, transparent network architecture, capability to handle a large number of images, and the ability to increase recognizable classes consistently without modifying prior weight and threshold values.

Conclusion: The transformation of metric recognition neural networks allows for efficient learning and scalability, offering advantages such as simple architecture, reliable training, and the ability to expand the number of recognizable classes without affecting existing parameters.

Abstract: Neural networks based on metric recognition methods have a strictly
determined architecture. Number of neurons, connections, as well as weights and
thresholds values are calculated analytically, based on the initial conditions
of tasks: number of recognizable classes, number of samples, metric expressions
used. This paper discusses the possibility of transforming these networks in
order to apply classical learning algorithms to them without using analytical
expressions that calculate weight values. In the received network, training is
carried out by recognizing images in pairs. This approach simplifies the
learning process and easily allows to expand the neural network by adding new
images to the recognition task. The advantages of these networks, including
such as: 1) network architecture simplicity and transparency; 2) training
simplicity and reliability; 3) the possibility of using a large number of
images in the recognition problem using a neural network; 4) a consistent
increase in the number of recognizable classes without changing the previous
values of weights and thresholds.

</details>


### [404] [Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies](https://arxiv.org/abs/2506.06325)
*Viorica Rozina Chifu,Tudor Cioara,Cristina Bianca Pop,Ionut Anghel*

Main category: cs.NE

TL;DR: This paper proposes a decentralized model of energy cooperation between microgrids using an evolutionary algorithm, achieving stable energy states for 95 out of 100 simulated microgrids.


<details>
  <summary>Details</summary>
Motivation: To create a decentralized decision-making model for energy cooperation among microgrids to achieve energy balance both individually and communally.

Method: Microgrids are modeled as autonomous agents adopting Hawk or Dove strategies based on battery storage levels and trading roles. Interactions are modeled via an evolutionary algorithm where individuals are represented as energy trading matrices. Population evolution occurs through recombination and mutation operators with evaluations made by a multi-criteria fitness function.

Result: In a test scenario with 100 microgrids, 95 reached a stable energy state, showing the model's effectiveness in achieving energy balance at both individual and community levels.

Conclusion: The proposed decentralized model effectively achieves energy balance within microgrid communities through local decision-making.

Abstract: This paper proposes a decentralized model of energy cooperation between
microgrids, in which decisions are made locally, at the level of the microgrid
community. Each microgrid is modeled as an autonomous agent that adopts a Hawk
or Dove strategy, depending on the level of energy stored in the battery and
its role in the energy trading process. The interactions between selling and
buying microgrids are modeled through an evolutionary algorithm. An individual
in the algorithm population is represented as an energy trading matrix that
encodes the amounts of energy traded between the selling and buying microgrids.
The population evolution is achieved by recombination and mutation operators.
Recombination uses a specialized operator for matrix structures, and mutation
is applied to the matrix elements according to a Gaussian distribution. The
evaluation of an individual is made with a multi-criteria fitness function that
considers the seller profit, the degree of energy stability at the community
level, penalties for energy imbalance at the community level and for the
degradation of microgrids batteries. The method was tested on a simulated
scenario with 100 microgrids, each with its own selling and buying thresholds,
to reflect a realistic environment with variable storage characteristics of
microgrids batteries. By applying the algorithm on this scenario, 95 out of the
100 microgrids reached a stable energy state. This result confirms the
effectiveness of the proposed model in achieving energy balance both at the
individual level, for each microgrid, and at the level of the entire community.

</details>


### [405] [Introduction to Predictive Coding Networks for Machine Learning](https://arxiv.org/abs/2506.06332)
*Mikko Stenlund*

Main category: cs.NE

TL;DR: This paper introduces Predictive Coding Networks (PCNs) to machine learning practitioners, providing foundational architecture, inference and learning rules, algorithmic implementation, and a CIFAR-10 image-classification benchmark with PyTorch code.


<details>
  <summary>Details</summary>
Motivation: To offer an alternative framework for understanding brain computation and provide a biologically inspired approach to traditional feedforward neural networks in ML.

Method: Covering the network architecture of PCNs, their inference and learning update rules, and algorithmic implementation, along with a practical image classification task on CIFAR-10 dataset.

Result: A successful application on CIFAR-10 image classification task is demonstrated, provided as a benchmark-smashing example.

Conclusion: PCNs serve as a promising biologically-inspired framework for ML, offering an alternative to traditional approaches.

Abstract: Predictive coding networks (PCNs) constitute a biologically inspired
framework for understanding hierarchical computation in the brain, and offer an
alternative to traditional feedforward neural networks in ML. This note serves
as a quick, onboarding introduction to PCNs for machine learning practitioners.
We cover the foundational network architecture, inference and learning update
rules, and algorithmic implementation. A concrete image-classification task
(CIFAR-10) is provided as a benchmark-smashing application, together with an
accompanying Python notebook containing the PyTorch implementation.

</details>


### [406] [CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms](https://arxiv.org/abs/2506.06362)
*Dejun Xu,Jijia Chen,Gary G. Yen,Min Jiang*

Main category: cs.NE

TL;DR: Bilevel optimization is challenging due to its nested structure. Evolutionary algorithms (EAs) are effective but resource-intensive. This paper proposes a novel framework that uses a contrastive ranking network to prioritize promising lower-level tasks, reducing computational cost while maintaining or improving solution accuracy.


<details>
  <summary>Details</summary>
Motivation: Bilevel optimization presents challenges due to its nested structure, requiring each upper-level solution to solve a corresponding lower-level problem. Current methods, including multitasking and transfer learning, still lead to significant resource waste.

Method: The paper introduces a new resource allocation framework for bilevel EAs. It includes a contrastive ranking network that learns relationships between paired upper- and lower-level solutions online. This network guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on population quality estimates.

Result: Experiments across five state-of-the-art bilevel algorithms demonstrate that the proposed framework significantly reduces computational costs while preserving or enhancing solution accuracy.

Conclusion: The proposed framework offers a generalizable strategy to improve the efficiency of bilevel EAs, making bilevel optimization more scalable.

Abstract: Bilevel optimization poses a significant computational challenge due to its
nested structure, where each upper-level candidate solution requires solving a
corresponding lower-level problem. While evolutionary algorithms (EAs) are
effective at navigating such complex landscapes, their high resource demands
remain a key bottleneck -- particularly the redundant evaluation of numerous
unpromising lower-level tasks. Despite recent advances in multitasking and
transfer learning, resource waste persists. To address this issue, we propose a
novel resource allocation framework for bilevel EAs that selectively identifies
and focuses on promising lower-level tasks. Central to our approach is a
contrastive ranking network that learns relational patterns between paired
upper- and lower-level solutions online. This knowledge guides a
reference-based ranking strategy that prioritizes tasks for optimization and
adaptively controls resampling based on estimated population quality.
Comprehensive experiments across five state-of-the-art bilevel algorithms show
that our framework significantly reduces computational cost while preserving --
or even enhancing -- solution accuracy. This work offers a generalizable
strategy to improve the efficiency of bilevel EAs, paving the way for more
scalable bilevel optimization.

</details>


### [407] [Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example](https://arxiv.org/abs/2506.06904)
*Yuhan Helena Liu,Guangyu Robert Yang,Christopher J. Cueva*

Main category: cs.NE

TL;DR: The study investigates biologically plausible learning rules, focusing on e-prop's capability to achieve neural data similarity akin to BPTT while meeting neuroscience task performance standards.


<details>
  <summary>Details</summary>
Motivation: To identify a biologically plausible learning rule that aligns well with both neuroscience task performance and neural recordings.

Method: Utilizing methods like Procrustes analysis on established neuroscience datasets to compare the performance of e-prop against BPTT in terms of neural data similarity and dynamical properties.

Result: E-prop achieves neural data similarity comparable to BPTT when matched for task accuracy. Model architecture and initial conditions significantly influence neural similarity more than the specific learning rule. BPTT-trained models and their biologically plausible counterparts exhibit similar dynamical properties at equivalent accuracies.

Conclusion: Significant advancements have been made in creating biologically plausible learning rules that can match competitive task performance and neural data similarity.

Abstract: Understanding how the brain learns may be informed by studying biologically
plausible learning rules. These rules, often approximating gradient descent
learning to respect biological constraints such as locality, must meet two
critical criteria to be considered an appropriate brain model: (1) good
neuroscience task performance and (2) alignment with neural recordings. While
extensive research has assessed the first criterion, the second remains
underexamined. Employing methods such as Procrustes analysis on well-known
neuroscience datasets, this study demonstrates the existence of a biologically
plausible learning rule -- namely e-prop, which is based on gradient truncation
and has demonstrated versatility across a wide range of tasks -- that can
achieve neural data similarity comparable to Backpropagation Through Time
(BPTT) when matched for task accuracy. Our findings also reveal that model
architecture and initial conditions can play a more significant role in
determining neural similarity than the specific learning rule. Furthermore, we
observe that BPTT-trained models and their biologically plausible counterparts
exhibit similar dynamical properties at comparable accuracies. These results
underscore the substantial progress made in developing biologically plausible
learning rules, highlighting their potential to achieve both competitive task
performance and neural data similarity.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [408] [AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition](https://arxiv.org/abs/2506.06566)
*Chen Bao,Chuanbing Huo,Qinyu Chen,Chang Gao*

Main category: eess.AS

TL;DR: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny with hybrid training strategy and GPT-4-based reference enhancement method. The fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30%.


<details>
  <summary>Details</summary>
Motivation: Current speech recognition models may not perform well on disordered speech such as aphasic speech due to lack of specific tuning and resources.

Method: The AS-ASR framework is built upon Whisper-tiny model and incorporates two key innovations: a hybrid training strategy that mixes standard and aphasic speech in varying ratios for robust generalization, and a GPT-4-based reference enhancement technique to refine noisy transcripts and improve supervision quality.

Result: The fine-tuned model shows significant improvement over the zero-shot baseline, achieving over 30% reduction in Word Error Rate (WER) on aphasic speech while maintaining performance on standard speech across multiple experimental settings.

Conclusion: AS-ASR presents an effective, scalable, and efficient solution for real-world disordered speech recognition, particularly suitable for low-resource deployment on edge devices.

Abstract: This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition
framework based on Whisper-tiny, tailored for low-resource deployment on edge
devices. Our approach introduces a hybrid training strategy that systematically
combines standard and aphasic speech at varying ratios, enabling robust
generalization, and a GPT-4-based reference enhancement method that refines
noisy aphasic transcripts, improving supervision quality. We conduct extensive
experiments across multiple data mixing configurations and evaluation settings.
Results show that our fine-tuned model significantly outperforms the zero-shot
baseline, reducing WER on aphasic speech by over 30% while preserving
performance on standard speech. The proposed framework offers a scalable,
efficient solution for real-world disordered speech recognition.

</details>


### [409] [Neural Spectral Band Generation for Audio Coding](https://arxiv.org/abs/2506.06732)
*Woongjib Choi,Byeong Hyeon Kim,Hyungseob Lim,Inseon Jang,Hong-Goo Kang*

Main category: eess.AS

TL;DR: An abstract discussing the limitations of traditional spectral band replication (SBR) and deep neural network (DNN)-based audio bandwidth extension methods for reconstructing high-frequency audio components. A new approach combining parametric non-blind bandwidth extension with DNN-based side information extraction is proposed.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional SBR and existing DNN-based blind BWE methods, which result in suboptimal performance when processing various types of audio signals.

Method: Propose a new approach to parametric non-blind bandwidth extension that incorporates DNN-based side information extraction and DNN-based bandwidth extension at the front and end of the audio coding pipeline respectively.

Result: The proposed method aims to overcome the limitations of coarse feature extraction and reconstruction techniques used in SBR, as well as the blindness of DNN-based methods.

Conclusion: This research suggests that integrating DNNs into a parametric non-blind framework could potentially lead to improved audio bandwidth extension performance.

Abstract: Audio bandwidth extension is the task of reconstructing missing high
frequency components of bandwidth-limited audio signals, where bandwidth
limitation is a common issue for audio signals due to several reasons,
including channel capacity and data constraints. While conventional spectral
band replication is a well-established parametric approach to audio bandwidth
extension, the SBR usually entails coarse feature extraction and reconstruction
techniques, which leads to limitations when processing various types of audio
signals. In parallel, numerous deep neural network-based audio bandwidth
extension methods have been proposed. These DNN-based methods are usually
referred to as blind BWE, as these methods do not rely on prior information
extracted from original signals, and only utilize given low frequency band
signals to estimate missing high frequency components. In order to replace
conventional SBR with DNNs, simply adopting existing DNN-based methodologies
results in suboptimal performance due to the blindness of these methods. My
proposed research suggests a new approach to parametric non-blind bandwidth
extension, as DNN-based side information extraction and DNN-based bandwidth
extension are performed only at the front and end of the audio coding pipeline.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [410] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph is an AI-powered framework that automates computational chemistry workflows using graph neural networks and language models. It simplifies tasks like molecular structure generation and energy calculations, with performance varying based on model size and task complexity.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in running atomistic simulations due to diverse computational methods, software ecosystems, and the need for expert knowledge.

Method: ChemGraph uses graph neural network-based foundation models for efficient calculations and large language models for natural language understanding, task planning, and scientific reasoning.

Result: Evaluated across 13 benchmark tasks, smaller LLMs perform well on simple workflows while larger models are better for complex tasks. Decomposing complex tasks into subtasks via a multi-agent framework allows smaller LLMs to match or exceed larger models' performance in specific scenarios.

Conclusion: ChemGraph streamlines and automates computational chemistry workflows, making complex simulations more accessible.

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='math.NT'></div>

# math.NT [[Back]](#toc)

### [411] [Stark-Coleman Invariants and Quantum Lower Bounds: An Integrated Framework for Real Quadratic Fields](https://arxiv.org/abs/2506.07640)
*Ruopengyu Xu,Chenglian Liu*

Main category: math.NT

TL;DR: Class groups of real quadratic fields are fundamental in algebraic number theory. This paper introduces Stark-Coleman invariants using p-adic Hodge theory and Coleman integration, proving they classify class groups under GRH for discriminants D > 10^32. It also establishes a quantum lower bound for the class group discrete logarithm problem.


<details>
  <summary>Details</summary>
Motivation: Class groups of real quadratic fields play a crucial role in algebraic number theory, but explicit constructions connecting special units to these groups have been challenging. Additionally, precise quantum complexity bounds for computations involving class groups are not well understood.

Method: The authors define Stark-Coleman invariants through a combination of p-adic Hodge theory and extended Coleman integration. They then analyze these invariants' properties and their implications for class group classification and computational complexity.

Result: The invariants were proven to classify class groups under the Generalized Riemann Hypothesis for large discriminants. A quantum lower bound for the class group discrete logarithm problem was established, improving on previous results.

Conclusion: Stark units impose constraints on the geometric structure of class groups, offering theoretical insights into the computational challenges associated with these groups.

Abstract: Class groups of real quadratic fields represent fundamental structures in
algebraic number theory with significant computational implications. While
Stark's conjecture establishes theoretical connections between special units
and class group structures, explicit constructions have remained elusive, and
precise quantum complexity bounds for class group computations are lacking.
Here we establish an integrated framework defining Stark-Coleman invariants
$\kappa_p(K) = \log_p \left(
\frac{\varepsilon_{\mathrm{St},p}}{\sigma(\varepsilon_{\mathrm{St},p})} \right)
\mod p^{\mathrm{ord}_p(\Delta_K)}$ through a synthesis of $p$-adic Hodge theory
and extended Coleman integration. We prove these invariants classify class
groups under the Generalized Riemann Hypothesis (GRH), resolving the
isomorphism problem for discriminants $D > 10^{32}$. Furthermore, we
demonstrate that this approach yields the quantum lower bound
$\exp\left(\Omega\left(\frac{\log D}{(\log \log D)^2}\right)\right)$ for the
class group discrete logarithm problem, improving upon previous bounds lacking
explicit constants. Our results indicate that Stark units constrain the
geometric organization of class groups, providing theoretical insight into
computational complexity barriers.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [412] [Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking](https://arxiv.org/abs/2506.07351)
*Jun Chen,Lina Liu,Tianyi Zhu,Yong Liu,Guang Dai,Yunliang Jiang,Ivor W. Tsang*

Main category: math.OC

TL;DR: This paper introduces Quantized Riemannian Gradient Tracking (Q-RGT) algorithm for decentralized optimization on compact submanifolds, which uses quantized gradients to reduce communication bottlenecks while maintaining an O(1/K) convergence rate.


<details>
  <summary>Details</summary>
Motivation: Decentralized optimization methods can be hindered by communication bottlenecks. The motivation is to develop an efficient method that reduces these bottlenecks without sacrificing performance.

Method: The proposed method is the Quantized Riemannian Gradient Tracking (Q-RGT) algorithm where agents update their local variables using quantized gradients. This allows bypassing constraints of accurate Riemannian projection operators and improves iterative efficiency.

Result: Q-RGT achieves an O(1/K) convergence rate despite quantization noise, matching the convergence rate of non-quantized methods. Numerical experiments show comparable performance with reduced communication and computational overhead.

Conclusion: Quantized Riemannian Gradient Tracking is an effective method for decentralized optimization on compact submanifolds, offering a way to reduce communication bottlenecks without compromising convergence rates.

Abstract: This paper considers the problem of decentralized optimization on compact
submanifolds, where a finite sum of smooth (possibly non-convex) local
functions is minimized by $n$ agents forming an undirected and connected graph.
However, the efficiency of distributed optimization is often hindered by
communication bottlenecks. To mitigate this, we propose the Quantized
Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local
variables using quantized gradients. The introduction of quantization noise
allows our algorithm to bypass the constraints of the accurate Riemannian
projection operator (such as retraction), further improving iterative
efficiency. To the best of our knowledge, this is the first algorithm to
achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization,
matching the convergence rate of methods without quantization. Additionally, we
explicitly derive lower bounds on decentralized consensus associated with a
function of quantization levels. Numerical experiments demonstrate that Q-RGT
performs comparably to non-quantized methods while reducing communication
bottlenecks and computational overhead.

</details>


### [413] [Discrete and Continuous Difference of Submodular Minimization](https://arxiv.org/abs/2506.07952)
*George Orfanides,Tim Hoheisel,Marwa El Halabi*

Main category: math.OC

TL;DR: 这篇论文研究了在连续和离散域上两个子模函数之差（DS）的最小化问题，提出了一种新的DC算法变体，并证明了其在整数压缩感知和整数最小二乘问题中的优越性。


<details>
  <summary>Details</summary>
Motivation: 子模函数在许多应用中出现，而对两个子模函数之差（DS）的最小化问题的研究可以扩展先前仅限于集合函数的工作，具有广泛的应用前景。

Method: 作者首先证明了所有离散域上的函数和所有连续域上的光滑函数都是DS函数。对于离散域，发现DS最小化等价于两个凸函数之差（DC）的最小化。接着，提出了一种新的DC算法（DCA）变体并将其应用于DC规划，获得了与集合函数情况相当的理论保证。该算法可以通过离散化应用于连续域。

Result: 实验结果表明，所提出的方法在整数压缩感知和整数最小二乘问题中优于基线方法。

Conclusion: 该研究将DS最小化问题从集合函数扩展到连续和离散域，并提供了一种有效的算法解决这一问题，展示了其在实际应用中的潜力。

Abstract: Submodular functions, defined on continuous or discrete domains, arise in
numerous applications. We study the minimization of the difference of two
submodular (DS) functions, over both domains, extending prior work restricted
to set functions. We show that all functions on discrete domains and all smooth
functions on continuous domains are DS. For discrete domains, we observe that
DS minimization is equivalent to minimizing the difference of two convex (DC)
functions, as in the set function case. We propose a novel variant of the DC
Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable
theoretical guarantees as in the set function case. The algorithm can be
applied to continuous domains via discretization. Experiments demonstrate that
our method outperforms baselines in integer compressive sensing and integer
least squares.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [414] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: The paper proposes two auxiliary consistency losses for fine-tuning Stable Diffusion model to solve underfitting and overfitting problems, which can preserve subject identity and enhance image diversity.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning methods for Stable Diffusion model have issues of underfitting (failing to capture subject identity) and overfitting (memorizing the subject image and reducing background diversity).

Method: Propose prior consistency regularization loss and subject consistency regularization loss to be incorporated into the fine-tuning process.

Result: Experimental results show that the proposed method can preserve subject identity and enhance image diversity, outperforming DreamBooth in CLIP scores, background variation, and overall visual quality.

Conclusion: The two auxiliary consistency losses are effective in improving the fidelity and diversity of subject-driven image synthesis.

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [415] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 3D Gaussian Splatting尽管算法性能出色，但在资源受限设备上的实时渲染仍具挑战。本文提出架构-算法协同设计解决此问题：1) 提出轴向光栅化减少重复计算；2) 引入神经排序方法提高效率；3) 设计可重构处理阵列和π-trajectory瓦片调度优化硬件使用和内存访问。实验表明，该设计在保持渲染质量的同时显著提升了速度和能效。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting在高质量和高效的视图合成方面表现出色，但其在资源受限设备上的实时渲染受到功率和面积预算的限制，存在重大挑战。

Method: 1. 揭示传统光栅化中的冗余，并提出轴向光栅化以预先计算并重用共享项，从而减少乘法和加法操作。
2. 针对排序过程中的资源和性能低效，引入神经排序方法，通过高效神经网络预测顺序无关的混合权重，取代昂贵的硬件排序器。
3. 设计可重构处理阵列以支持光栅化和神经网络推理，最大化硬件利用率和吞吐量。
4. 引入π-trajectory瓦片调度，优化高斯复用并减少内存访问开销。

Result: 相比边缘GPU，该设计在真实场景中实现了23.4至27.8倍的速度提升和28.8至51.4倍的能耗节省，同时保持了渲染质量。

Conclusion: 本文提出的架构-算法协同设计有效解决了3D Gaussian Splatting在资源受限设备上的实时渲染问题，显著提高了速度和能效，为未来的发展提供了开源基础。

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [416] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: This paper proposes a new method to enhance the resolution and geometric fidelity of 3D Gaussian Splatting beyond native training resolution.


<details>
  <summary>Details</summary>
Motivation: Current 3DGS methods are limited by their input resolution, unable to extrapolate finer details than present in the training views.

Method: A lightweight generative model that predicts and refines additional 3D Gaussians where needed most, with Hessian-assisted sampling strategy to identify regions for densification.

Result: Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods.

Conclusion: The proposed method establishes a new paradigm for resolution-free 3D scene enhancement.

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [417] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: The paper introduces Squeeze3D, a framework that uses pre-trained 3D generative models to compress 3D data at high ratios without significant quality loss.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient compression of 3D data formats such as meshes, point clouds, and radiance fields while maintaining visual quality.

Method: Squeeze3D bridges latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. It transforms 3D models into compact latent codes and then back into original models using generative models.

Result: Achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields with comparable visual quality to existing methods.

Conclusion: Squeeze3D provides an effective solution for compressing various 3D data formats at high ratios with minimal latency and no requirement for specific datasets.

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [418] [Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization](https://arxiv.org/abs/2506.06305)
*Noémie Bergues,Arthur Carré,Paul Join-Lambert,Brice Hoffmann,Arnaud Blondel,Hamza Tajmouati*

Main category: q-bio.BM

TL;DR: This paper proposes a two-stage method for ligand conformation generation guided by template structures, which outperforms standard docking tools and open-access alignment methods.


<details>
  <summary>Details</summary>
Motivation: Predicting the 3D conformation of small molecules within protein binding sites is crucial in drug design. A crystallized reference ligand (template) can provide geometric priors that guide 3D pose prediction.

Method: The first stage uses a molecular alignment approach based on flow-matching to generate 3D coordinates for the ligand using the template structure as a reference. The second stage involves a differentiable pose optimization procedure refining this conformation based on shape and pharmacophore similarities, internal energy, and optionally, the protein binding pocket.

Result: The approach was evaluated on a new benchmark of ligand pairs co-crystallized with the same target and showed superior performance compared to standard docking tools and open-access alignment methods, especially in cases involving low similarity to the template or high ligand flexibility.

Conclusion: The proposed two-stage method for ligand conformation generation guided by templates shows promise in drug design by providing more accurate 3D pose predictions.

Abstract: Predicting the 3D conformation of small molecules within protein binding
sites is a key challenge in drug design. When a crystallized reference ligand
(template) is available, it provides geometric priors that can guide 3D pose
prediction. We present a two-stage method for ligand conformation generation
guided by such templates. In the first stage, we introduce a molecular
alignment approach based on flow-matching to generate 3D coordinates for the
ligand, using the template structure as a reference. In the second stage, a
differentiable pose optimization procedure refines this conformation based on
shape and pharmacophore similarities, internal energy, and, optionally, the
protein binding pocket. We evaluate our approach on a new benchmark of ligand
pairs co-crystallized with the same target and show that it outperforms
standard docking tools and open-access alignment methods, especially in cases
involving low similarity to the template or high ligand flexibility.

</details>


### [419] [AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization](https://arxiv.org/abs/2506.07035)
*Zixuan Jiang,Renjing Xu*

Main category: q-bio.BM

TL;DR: Deciphering protein function is challenging in protein representation learning. Current protein language models (PLMs) face difficulties due to the large number of functional annotation categories and imbalanced data distribution. Drawing inspiration from reinforcement learning from human feedback (RLHF) used in large language models (LLMs), researchers propose AnnoDPO, a new multi-modal framework for protein function prediction that uses Direct Preference Optimization (DPO) to improve annotation learning. This approach addresses issues of limited annotations and category imbalance by using preference-aligned training objectives.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges in deciphering protein function within protein representation learning, particularly focusing on overcoming the limitations posed by a high volume of functional annotation categories and an imbalanced distribution of annotated instances across biological ontologies.

Method: The method proposed in this paper is called AnnoDPO, which is a novel multi-modal framework for protein function prediction. It leverages Direct Preference Optimization (DPO) to enhance annotation learning, aiming to tackle the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives.

Result: The result of implementing AnnoDPO is the establishment of a new paradigm for integrating biological knowledge into protein representation learning, which effectively handles the issues of limited annotations and category imbalance.

Conclusion: In conclusion, AnnoDPO presents a promising advancement in protein function prediction by utilizing DPO to overcome the challenges associated with annotation scarcity and category imbalance, thus contributing significantly to the field of protein representation learning.

Abstract: Deciphering protein function remains a fundamental challenge in protein
representation learning. The task presents significant difficulties for protein
language models (PLMs) due to the sheer volume of functional annotation
categories and the highly imbalanced distribution of annotated instances across
biological ontologies. Inspired by the remarkable success of reinforcement
learning from human feedback (RLHF) in large language model (LLM) alignment, we
propose AnnoDPO, a novel multi-modal framework for protein function prediction
that leverages Direct Preference Optimization (DPO) to enhance annotation
learning. Our methodology addresses the dual challenges of annotation scarcity
and category imbalance through preference-aligned training objectives,
establishing a new paradigm for biological knowledge integration in protein
representation learning.

</details>


### [420] [Graph Neural Networks in Modern AI-aided Drug Discovery](https://arxiv.org/abs/2506.06915)
*Odin Zhang,Haitao Lin,Xujun Zhang,Xiaorui Wang,Zhenxing Wu,Qing Ye,Weibo Zhao,Jike Wang,Kejun Ying,Yu Kang,Chang-yu Hsieh,Tingjun Hou*

Main category: q-bio.BM

TL;DR: Graph neural networks (GNNs) play a significant role in AI-aided drug discovery, providing tools for molecular property prediction, virtual screening, and more. This review covers methodological foundations, applications, recent advances, and future directions of GNNs in drug discovery.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of graph neural networks (GNNs) as topology/structure-aware models within deep learning for AI-aided drug discovery.

Method: By directly operating on molecular graphs, GNNs learn complex topological and geometric features of drug-like molecules. The review discusses various tasks such as molecular property prediction, virtual screening, molecular generation, biomedical knowledge graph construction, and synthesis planning.

Result: The review highlights practical challenges and methodological bottlenecks encountered when applying GNNs to real-world drug discovery pipelines, and provides insights into recent methodological advances including geometric GNNs, interpretable models, uncertainty quantification, scalable graph architectures, and graph generative frameworks.

Conclusion: GNNs have emerged as powerful tools for AI-aided drug discovery, with promising future directions in integrating modern deep learning approaches.

Abstract: Graph neural networks (GNNs), as topology/structure-aware models within deep
learning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By
directly operating on molecular graphs, GNNs offer an intuitive and expressive
framework for learning the complex topological and geometric features of
drug-like molecules, cementing their role in modern molecular modeling. This
review provides a comprehensive overview of the methodological foundations and
representative applications of GNNs in drug discovery, spanning tasks such as
molecular property prediction, virtual screening, molecular generation,
biomedical knowledge graph construction, and synthesis planning. Particular
attention is given to recent methodological advances, including geometric GNNs,
interpretable models, uncertainty quantification, scalable graph architectures,
and graph generative frameworks. We also discuss how these models integrate
with modern deep learning approaches, such as self-supervised learning,
multi-task learning, meta-learning and pre-training. Throughout this review, we
highlight the practical challenges and methodological bottlenecks encountered
when applying GNNs to real-world drug discovery pipelines, and conclude with a
discussion on future directions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [421] [Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage](https://arxiv.org/abs/2506.06472)
*Ziqi Yuan,Haoyang Zhang,Yirui Eric Zhou,Apoorve Mohan,I-Hsin Chung,Seetharami Seelam,Jian Huang*

Main category: cs.DC

TL;DR: The paper introduces TERAIO, a lifetime-aware tensor offloading framework for GPU memory expansion using SSDs. It enhances LLM training performance by profiling tensor lifetimes, generating optimized offloading plans, and integrating them into the program through PyTorch. TERAIO improves training performance by 1.47x on average compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of GPU memory in large language model training by utilizing low-cost PCIe-based SSDs as an extension to GPU memory, thereby improving the efficiency and reducing the cost of LLM training.

Method: TERAIO profiles the first few iterations of the training process to estimate tensor lifetimes, generates an optimized tensor offloading/prefetching plan, integrates it into the compiled LLM program via PyTorch, and uses a runtime tensor migration engine with GPUDirect storage to execute the plan efficiently.

Result: TERAIO improves the training performance of various LLMs by 1.47x on average compared to state-of-the-art studies like ZeRO-Offload and ZeRO-Infinity, achieving 80.7% of the ideal performance assuming unlimited GPU memory.

Conclusion: TERAIO successfully demonstrates the potential of using SSDs for GPU memory expansion in LLM training, providing a cost-effective solution that significantly enhances training performance.

Abstract: We present the design and implementation of a new lifetime-aware tensor
offloading framework for GPU memory expansion using low-cost PCIe-based
solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for
large language model (LLM) training with multiple GPUs and multiple SSDs. Its
design is driven by our observation that the active tensors take only a small
fraction (1.7% on average) of allocated GPU memory in each LLM training
iteration, the inactive tensors are usually large and will not be used for a
long period of time, creating ample opportunities for offloading/prefetching
tensors to/from slow SSDs without stalling the GPU training process. TERAIO
accurately estimates the lifetime (active period of time in GPU memory) of each
tensor with the profiling of the first few iterations in the training process.
With the tensor lifetime analysis, TERAIO will generate an optimized tensor
offloading/prefetching plan and integrate it into the compiled LLM program via
PyTorch. TERAIO has a runtime tensor migration engine to execute the
offloading/prefetching plan via GPUDirect storage, which allows direct tensor
migration between GPUs and SSDs for alleviating the CPU bottleneck and
maximizing the SSD bandwidth utilization. In comparison with state-of-the-art
studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves
the training performance of various LLMs by 1.47x on average, and achieves
80.7% of the ideal performance assuming unlimited GPU memory.

</details>


### [422] [pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization](https://arxiv.org/abs/2506.07159)
*Mrinmay Sen,Chalavadi Krishna Mohan*

Main category: cs.DC

TL;DR: In this paper, the authors propose pFedSOP for Personalized Federated Learning (PFL), which uses second-order optimization to accelerate training and improve performance with fewer communication rounds. It computes personalized local gradient updates using a Gompertz function-based normalized angle and employs a regularized Fisher Information Matrix (FIM) as an approximation of the Hessian matrix.


<details>
  <summary>Details</summary>
Motivation: Existing PFL methods often require increased communication rounds and local computation due to the use of first-order optimization techniques, which have linear convergence. The challenge is to find a way to utilize second-order optimization in PFL to achieve faster training and better performance while avoiding additional data being fed into the model during the search for personalized local models.

Method: The method proposed is pFedSOP, which incorporates second-order optimization into PFL. It first computes a personalized local gradient update using the Gompertz function-based normalized angle between local and global gradient updates. Then, it uses a regularized Fisher Information Matrix (FIM), computed from this personalized gradient update, as an approximation of the Hessian matrix to update the personalized models.

Result: Extensive experiments on heterogeneously partitioned image classification datasets with partial client participation demonstrate that pFedSOP outperforms state-of-the-art FL and PFL algorithms in terms of both performance and efficiency.

Conclusion: pFedSOP efficiently utilizes second-order optimization in PFL to accelerate the training of personalized models and enhance performance with fewer communication rounds, overcoming the challenges associated with exact Hessian matrices.

Abstract: Personalized Federated Learning (PFL) enables clients to collaboratively
train personalized models tailored to their individual objectives, addressing
the challenge of model generalization in traditional Federated Learning (FL)
due to high data heterogeneity. However, existing PFL methods often require
increased communication rounds to achieve the desired performance, primarily
due to slow training caused by the use of first-order optimization, which has
linear convergence. Additionally, many of these methods increase local
computation because of the additional data fed into the model during the search
for personalized local models. One promising solution to this slow training is
second-order optimization, known for its quadratic convergence. However,
employing it in PFL is challenging due to the Hessian matrix and its inverse.
In this paper, we propose pFedSOP, which efficiently utilizes second-order
optimization in PFL to accelerate the training of personalized models and
enhance performance with fewer communication rounds. Our approach first
computes a personalized local gradient update using the Gompertz function-based
normalized angle between local and global gradient updates, incorporating
client-specific global information. We then use a regularized Fisher
Information Matrix (FIM), computed from this personalized gradient update, as
an approximation of the Hessian to update the personalized models. This
FIM-based second-order optimization speeds up training with fewer communication
rounds by tackling the challenges with exact Hessian and avoids additional data
being fed into the model during the search for personalized local models.
Extensive experiments on heterogeneously partitioned image classification
datasets with partial client participation demonstrate that pFedSOP outperforms
state-of-the-art FL and PFL algorithms.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [423] [Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks](https://arxiv.org/abs/2506.06870)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.LO

TL;DR: ISO 639:2023虽然统一了语言编码体系并引入上下文元数据，但缺乏处理方言漂移和混合语的机制。本文提出了一种递归语义锚定的形式化方法，并通过实验验证其在语言识别和翻译中的有效性。


<details>
  <summary>Details</summary>
Motivation: ISO 639:2023标准虽有改进，但仍无法有效应对语言学中的方言漂移和混合语现象，这促使作者寻找一种机器可操作的解决方案来弥补这一缺陷。

Method: 提出了一种基于递归语义锚定的方法，为每个语言实体χ定义了一系列固定点算子φ{n,m}，通过关系φ{n,m}(χ) = χ ⊕ Δ(χ)模拟有限语义漂移。使用范畴论将这些算子视为形态，证明了收敛性，并提供了一个RDF/Turtle模式以实现具体应用。

Result: 实验表明，在含噪声或代码切换的输入上，利用φ-indices指导回退路由可以提高变压器模型在语言识别和翻译任务中的准确性。

Conclusion: 该框架与ISO/TC 37兼容，为未来的标准提供了一个AI可处理、漂移感知的语义层。

Abstract: ISO 639:2023 unifies the ISO language-code family and introduces contextual
metadata, but it lacks a machine-native mechanism for handling dialectal drift
and creole mixtures. We propose a formalisation of recursive semantic
anchoring, attaching to every language entity $\chi$ a family of fixed-point
operators $\phi_{n,m}$ that model bounded semantic drift via the relation
$\phi_{n,m}(\chi) = \chi \oplus \Delta(\chi)$, where $\Delta(\chi)$ is a drift
vector in a latent semantic manifold. The base anchor $\phi_{0,0}$ recovers the
canonical ISO 639:2023 identity, whereas $\phi_{99,9}$ marks the maximal drift
state that triggers a deterministic fallback. Using category theory, we treat
the operators $\phi_{n,m}$ as morphisms and drift vectors as arrows in a
category $\mathrm{DriftLang}$. A functor $\Phi: \mathrm{DriftLang} \to
\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves
convergence. We provide an RDF/Turtle schema (\texttt{BaseLanguage},
\texttt{DriftedLanguage}, \texttt{ResolvedAnchor}) and worked examples -- e.g.,
$\phi_{8,4}$ (Standard Mandarin) versus $\phi_{8,7}$ (a colloquial variant),
and $\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with
transformer models show higher accuracy in language identification and
translation on noisy or code-switched input when the $\phi$-indices are used to
guide fallback routing. The framework is compatible with ISO/TC 37 and provides
an AI-tractable, drift-aware semantic layer for future standards.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [424] [Deep regularization networks for inverse problems with noisy operators](https://arxiv.org/abs/2506.07008)
*Fatemeh Pourahmadian,Yang Xu*

Main category: math.NA

TL;DR: 提出了一种监督学习方法，用于正则化大型逆问题，特别是基于噪声数据的散射方程。通过两步训练神经网络，能够加速时空正则化过程以实现实时成像，并提高了复杂环境下的图像质量。


<details>
  <summary>Details</summary>
Motivation: 在超分辨率成像中，逆散射理论的采样指标构建的主算子通常来自噪声数据，需要一种有效的方法来加速正则化过程以实现实时成像。

Method: 使用神经算子将散射方程右侧的每个模式映射到其关联的正则化参数，分两步训练网络：1）基于Morozov差异原理的低分辨率正则化图进行初步训练；2）通过最小化Tikhonov损失函数优化网络预测，生成高质量图像。此外，提出了一种新方法，在训练过程中自适应选择适当的损失权重。

Result: 该方法不仅加速了成像过程，还显著提高了复杂环境下的图像质量，特别是在弹性板损伤演化成像中的合成实验结果表明了这一点。

Conclusion: 所提出的监督学习方法为解决大型逆问题提供了一种有效的解决方案，能够在无需先验知识的情况下快速生成高分辨率正则化图，并提高图像对比度和质量。

Abstract: A supervised learning approach is proposed for regularization of large
inverse problems where the main operator is built from noisy data. This is
germane to superresolution imaging via the sampling indicators of the inverse
scattering theory. We aim to accelerate the spatiotemporal regularization
process for this class of inverse problems to enable real-time imaging. In this
approach, a neural operator maps each pattern on the right-hand side of the
scattering equation to its affiliated regularization parameter. The network is
trained in two steps which entails: (1) training on low-resolution
regularization maps furnished by the Morozov discrepancy principle with
nonoptimal thresholds, and (2) optimizing network predictions through
minimization of the Tikhonov loss function regulated by the validation loss.
Step 2 allows for tailoring of the approximate maps of Step 1 toward
construction of higher quality images. This approach enables direct learning
from test data and dispenses with the need for a-priori knowledge of the
optimal regularization maps. The network, trained on low-resolution data,
quickly generates dense regularization maps for high-resolution imaging. We
highlight the importance of the training loss function on the network's
generalizability. In particular, we demonstrate that networks informed by the
logic of discrepancy principle lead to images of higher contrast. In this case,
the training process involves many-objective optimization. We propose a new
method to adaptively select the appropriate loss weights during training
without requiring an additional optimization process. The proposed approach is
synthetically examined for imaging damage evolution in an elastic plate. The
results indicate that the discrepancy-informed regularization networks not only
accelerate the imaging process, but also remarkably enhance the image quality
in complex environments.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [425] [Depth-Optimal Quantum Layout Synthesis as SAT](https://arxiv.org/abs/2506.06752)
*Anna B. Jakobsen,Anders B. Clausen,Jaco van de Pol,Irfansha Shaik*

Main category: quant-ph

TL;DR: The paper presents a new SAT encoding method for Quantum-circuit Layout Synthesis which guarantees minimal circuit depth or CX-gate depth, achieving significant speedups compared to OLSQ2. However, minimizing CX-depth takes more time than minimizing gate count with Q-Synth. Minimizing CX-count correlates better with reducing noise but considering both CX-count and CX-depth provides the best noise reduction.


<details>
  <summary>Details</summary>
Motivation: Current quantum hardware platforms impose connectivity restrictions on binary CX gates, making Layout Synthesis an important step in transpiling quantum circuits before execution. Since CX gates are noisy, it's crucial to reduce the CX count or CX depth of mapped circuits.

Method: The authors provide a new and efficient SAT encoding for Quantum-circuit Layout Synthesis. This encoding focuses on finding mapped circuits with minimal circuit depth or minimal CX-gate depth using incremental SAT solving and parallel plans.

Result: This approach results in speedups of more than 10-100x compared to OLSQ2, which also guarantees depth-optimality. Simulations show that minimizing CX-count correlates better with reducing noise than minimizing CX-depth, but considering both provides the best noise reduction.

Conclusion: The new SAT encoding method efficiently achieves minimal circuit depth or CX-gate depth, significantly speeding up the process compared to previous methods. While minimizing CX-count is better correlated with noise reduction, combining CX-count and CX-depth considerations yields optimal noise reduction.

Abstract: Quantum circuits consist of gates applied to qubits. Current quantum hardware
platforms impose connectivity restrictions on binary CX gates. Hence, Layout
Synthesis is an important step to transpile quantum circuits before they can be
executed. Since CX gates are noisy, it is important to reduce the CX count or
CX depth of the mapped circuits.
  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis
in SAT. Previous SAT encodings focused on gate count and CX-gate count. Our
encoding instead guarantees that we find mapped circuits with minimal circuit
depth or minimal CX-gate depth. We use incremental SAT solving and parallel
plans for an efficient encoding. This results in speedups of more than 10-100x
compared to OLSQ2, which guarantees depth-optimality. But minimizing depth
still takes more time than minimizing gate count with Q-Synth.
  We correlate the noise reduction achieved by simulating circuits after
(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count
correlates better with reducing noise than minimizing for CX-depth. However,
taking into account both CX-count and CX-depth provides the best noise
reduction.

</details>


### [426] [A weighted quantum ensemble of homogeneous quantum classifiers](https://arxiv.org/abs/2506.07810)
*Emiliano Tolotti,Enrico Blanzieri,Davide Pastorello*

Main category: quant-ph

TL;DR: This paper proposes a weighted homogeneous quantum ensemble method using quantum classifiers with indexing registers for data encoding, which enables feature and training point subsampling through superposition and controlled unitaries.


<details>
  <summary>Details</summary>
Motivation: To improve prediction accuracy in machine learning by combining multiple models while leveraging quantum computing capabilities.

Method: The method uses instance-based quantum classifiers with indexing registers for data encoding. It allows for feature and training point subsampling through superposition and controlled unitaries, leading to quantum-parallel execution of diverse internal classifiers. The approach integrates a learning process involving circuit execution and classical weight optimization.

Result: Empirical evaluation shows the effectiveness of the proposed method, providing insights into its performance.

Conclusion: The proposed weighted homogeneous quantum ensemble method demonstrates potential in enhancing prediction accuracy via quantum computing techniques.

Abstract: Ensemble methods in machine learning aim to improve prediction accuracy by
combining multiple models. This is achieved by ensuring diversity among
predictors to capture different data aspects. Homogeneous ensembles use
identical models, achieving diversity through different data subsets, and
weighted-average ensembles assign higher influence to more accurate models
through a weight learning procedure. We propose a method to achieve a weighted
homogeneous quantum ensemble using quantum classifiers with indexing registers
for data encoding. This approach leverages instance-based quantum classifiers,
enabling feature and training point subsampling through superposition and
controlled unitaries, and allowing for a quantum-parallel execution of diverse
internal classifiers with different data compositions in superposition. The
method integrates a learning process involving circuit execution and classical
weight optimization, for a trained ensemble execution with weights encoded in
the circuit at test-time. Empirical evaluation demonstrate the effectiveness of
the proposed method, offering insights into its performance.

</details>


### [427] [Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing](https://arxiv.org/abs/2506.07859)
*Amanuel Anteneh Léandre Brunel,Carlos González-Arciniegas,Olivier Pfister*

Main category: quant-ph

TL;DR: Cubic-phase states are crucial for universal quantum computing in continuous variables. Researchers trained deep neural networks using reinforcement learning to control quantum optical circuits for generating these states with a 96% success rate. Only photon-number-resolving measurements were needed as non-Gaussian resources. Additionally, the same resources can generate a quartic-phase gate directly.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of cubic-phase states as a resource for universal quantum computing over continuous variables and demonstrate the feasibility of using deep neural networks controlled by reinforcement learning to generate these states efficiently.

Method: The researchers conducted numerical experiments where deep neural networks were trained via reinforcement learning techniques to control quantum optical circuits aimed at generating cubic-phase states. The only non-Gaussian resource utilized was photon-number-resolving measurements.

Result: The method achieved an average success rate of 96% in generating cubic-phase states. Furthermore, it was shown that the same resources could be used to directly generate a quartic-phase gate without needing a cubic gate decomposition.

Conclusion: Cubic-phase states can serve as a sufficient resource for universal quantum computing in continuous variables when generated using deep neural network-controlled quantum optical circuits with reinforcement learning. The same setup also enables the direct generation of a quartic-phase gate.

Abstract: Cubic-phase states are a sufficient resource for universal quantum computing
over continuous variables. We present results from numerical experiments in
which deep neural networks are trained via reinforcement learning to control a
quantum optical circuit for generating cubic-phase states, with an average
success rate of 96%. The only non-Gaussian resource required is
photon-number-resolving measurements. We also show that the exact same
resources enable the direct generation of a quartic-phase gate, with no need
for a cubic gate decomposition.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [428] [AI-Generated Compromises for Coalition Formation](https://arxiv.org/abs/2506.06837)
*Eyal Briman,Ehud Shapiro,Nimrod Talmon*

Main category: cs.MA

TL;DR: This paper aims to solve the problem of finding compromise proposals among agents by incorporating agent bounded rationality and uncertainty, developing AI methods for generating compromises in collaborative document writing.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing a crucial gap in coalition formation processes, which involves identifying compromise proposals that can unite agent coalitions.

Method: The method includes formalizing a model with agent bounded rationality and uncertainty, using NLP techniques and large language models to create a semantic metric space over text, and designing algorithms to suggest compromise points.

Result: Through simulations, the results show that AI can effectively facilitate large-scale democratic text editing.

Conclusion: AI-based methods have potential in enhancing democratic drafting processes, such as community constitution writing, where traditional tools are insufficient.

Abstract: The challenge of finding compromises between agent proposals is fundamental
to AI subfields such as argumentation, mediation, and negotiation. Building on
this tradition, Elkind et al. (2021) introduced a process for coalition
formation that seeks majority-supported proposals preferable to the status quo,
using a metric space where each agent has an ideal point. A crucial step in
this process involves identifying compromise proposals around which agent
coalitions can unite. How to effectively find such compromise proposals remains
an open question. We address this gap by formalizing a model that incorporates
agent bounded rationality and uncertainty, and by developing AI methods to
generate compromise proposals. We focus on the domain of collaborative document
writing, such as the democratic drafting of a community constitution. Our
approach uses natural language processing techniques and large language models
to induce a semantic metric space over text. Based on this space, we design
algorithms to suggest compromise points likely to receive broad support. To
evaluate our methods, we simulate coalition formation processes and show that
AI can facilitate large-scale democratic text editing, a domain where
traditional tools are limited.

</details>


### [429] [Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments](https://arxiv.org/abs/2506.07232)
*Xinran Li,Chenjia Bai,Zijian Li,Jiakun Zheng,Ting Xiao,Jun Zhang*

Main category: cs.MA

TL;DR: 这篇论文提出了一种名为Learn as Individuals, Evolve as a Team (LIET)的新范式，用于提升大型语言模型（LLMs）在多智能体具身场景中的适应能力。通过个体学习和团队进化相结合的方法，LIET使LLM代理能够在测试前和测试期间不断学习和进化，从而获得环境相关的知识以改进规划，并通过增强的沟通来促进合作。实验结果表明，在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准上，使用LLaMA和GPT-4o实例化的LIET优于现有的基线方法，并展现出强大的合作规划能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型具有广泛的知识库和强大的推理能力，但现有的基于LLM的规划算法在多智能体具身场景中的适应能力仍然有限。为了解决这一问题，研究者们希望开发一种新的框架，使LLM代理能够更好地适应复杂的多智能体环境，提高其规划能力和合作水平。

Method: LIET范式包括两个层面：个体层面，LLM代理从探索性数据集中学习局部效用函数，以更好地理解具身环境，并在测试时查询该函数以支持决策；团队层面，LLM代理通过新经验协作维护和更新共享的合作知识列表，以指导更有效的沟通。通过结合个体学习与团队进化，LIET实现了全面且灵活的适应能力。

Result: 实验结果表明，LIET在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准上表现优异，无论是使用LLaMA还是GPT-4o实例化，均超越了现有基线方法，展现了强大的合作规划能力。

Conclusion: LIET范式成功地提升了LLM代理在多智能体具身场景中的适应能力，证明了通过个体学习和团队进化相结合可以显著改善LLM代理的规划和合作性能。

Abstract: Large language models (LLMs) possess extensive knowledge bases and strong
reasoning capabilities, making them promising tools for complex, multi-agent
planning in embodied environments. However, despite LLMs' advanced abilities
and the sophisticated modular design of agentic methods, existing LLM-based
planning algorithms remain limited by weak adaptation capabilities to
multi-agent embodied scenarios. We address this limitation by introducing a
framework that enables LLM agents to learn and evolve both before and during
test time, equipping them with environment-relevant knowledge for better
planning and enhanced communication for improved cooperation. Inspired by
centralized training with decentralized execution in multi-agent reinforcement
learning, we propose a \textit{Learn as Individuals, Evolve as a Team (LIET)}
paradigm for multi-agent LLMs adaptation. At the individual level, LLM agents
learn a local utility function from exploratory datasets to better comprehend
the embodied environment, which is then queried during test time to support
informed decision-making. At the team level, LLM agents collaboratively and
iteratively maintain and update a shared cooperation knowledge list based on
new experiences, using it to guide more effective communication. By combining
individual learning with team evolution, LIET enables comprehensive and
flexible adaptation for LLM agents. Our experiments on Communicative
Watch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate
that LIET, instantiated with both LLaMA and GPT-4o, outperforms existing
baselines and exhibits strong cooperative planning abilities.

</details>


### [430] [Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents](https://arxiv.org/abs/2506.07388)
*Yun Hua,Haosheng Chen,Shiqin Wang,Wenhao Li,Xiangfeng Wang,Jun Luo*

Main category: cs.MA

TL;DR: 大型语言模型（LLMs）在多智能体系统中表现出强大的协作性能，但在缺乏协调规则的开放环境中，智能体倾向于以自我利益为导向行动。为了解决这一问题，受人类社会协调挑战解决方式的启发，本文提出了Shapley-Coop合作流程，通过理性任务时间定价和任务后奖励再分配，增强了LLM智能体之间的协作，并促进了公平的信用分配。


<details>
  <summary>Details</summary>
Motivation: 在开放环境中的多智能体系统里，智能体往往以自我利益为导向行动，难以实现有效的协调。这促使研究者探索一种能够公平评估每个智能体贡献并设计合理定价机制的方法，以促进复杂的人工智能与人类协作。

Method: 提出了一种名为Shapley-Coop的合作流程，该方法结合了Shapley Chain-of-Thought（利用边际贡献作为定价的基本原则）和结构化谈判协议，用于有效的价格匹配，使LLM智能体可以通过理性任务时间定价和任务后奖励再分配进行协调。

Result: 在两个多智能体游戏和一个软件工程模拟中的评估表明，Shapley-Coop显著提高了LLM智能体的协作能力，并促进了公平的信用分配。结果证明，Shapley-Coop的定价机制能准确反映个体在任务执行中的贡献。

Conclusion: Shapley-Coop提供了一种有效的解决方案，通过合理的定价机制来协调LLM智能体的行为，增强其协作性并确保公平的信用分配，从而推动更高效的人工智能与人类协作。

Abstract: Large Language Models (LLMs) show strong collaborative performance in
multi-agent systems with predefined roles and workflows. However, in open-ended
environments lacking coordination rules, agents tend to act in self-interested
ways. The central challenge in achieving coordination lies in credit assignment
-- fairly evaluating each agent's contribution and designing pricing mechanisms
that align their heterogeneous goals. This problem is critical as LLMs
increasingly participate in complex human-AI collaborations, where fair
compensation and accountability rely on effective pricing mechanisms. Inspired
by how human societies address similar coordination challenges (e.g., through
temporary collaborations such as employment or subcontracting), we propose a
cooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley
Chain-of-Thought -- leveraging marginal contributions as a principled basis for
pricing -- with structured negotiation protocols for effective price matching,
enabling LLM agents to coordinate through rational task-time pricing and
post-task reward redistribution. This approach aligns agent incentives, fosters
cooperation, and maintains autonomy. We evaluate Shapley-Coop across two
multi-agent games and a software engineering simulation, demonstrating that it
consistently enhances LLM agent collaboration and facilitates equitable credit
assignment. These results highlight the effectiveness of Shapley-Coop's pricing
mechanisms in accurately reflecting individual contributions during task
execution.

</details>


### [431] [MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models](https://arxiv.org/abs/2506.07400)
*Philip Liu,Sparsh Bansal,Jimmy Dinh,Aditya Pawar,Ramani Satishkumar,Shail Desai,Neeraj Gupta,Xin Wang,Shu Hu*

Main category: cs.MA

TL;DR: An automated strategy using deep learning for glaucoma detection integrated with large language models (LLMs) to reduce ophthalmologist shortages and improve clinical reporting efficiency is proposed. However, general LLMs face challenges in medical imaging such as hallucinations, limited interpretability, and insufficient domain-specific knowledge which can decrease clinical accuracy. While combining imaging models with LLM reasoning has improved reporting, reliance on a single generalist agent restricts their ability to emulate multidisciplinary medical team reasoning. To solve these issues, MedChat, a multi-agent diagnostic framework that combines specialized vision models with multiple role-specific LLM agents coordinated by a director agent, is proposed to enhance reliability, reduce hallucination risk, and enable interactive diagnostic reporting through a tailored interface for clinical review and education.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of applying general LLMs to medical imaging, including hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Additionally, there is a need to overcome the restriction of relying on a single generalist agent that limits the capacity to emulate diverse and complex reasoning found in multidisciplinary medical teams.

Method: The method proposed in this paper is MedChat, a multi-agent diagnostic framework and platform that integrates specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This approach aims to enhance reliability, reduce hallucination risk, and enable interactive diagnostic reporting through an interface designed for clinical review and educational use.

Result: The result of this paper is the development of MedChat, which promises to improve the reliability of automated clinical reporting, reduce the risk of hallucinations, and provide an interactive platform for diagnostic reporting suitable for both clinical review and educational purposes.

Conclusion: In conclusion, MedChat represents an advancement in integrating specialized vision models with multiple role-specific LLM agents to overcome the limitations of general LLMs in medical imaging. By employing a multi-agent system coordinated by a director agent, MedChat enhances reliability, reduces hallucination risks, and offers an interactive diagnostic reporting platform suited for clinical and educational applications.

Abstract: The integration of deep learning-based glaucoma detection with large language
models (LLMs) presents an automated strategy to mitigate ophthalmologist
shortages and improve clinical reporting efficiency. However, applying general
LLMs to medical imaging remains challenging due to hallucinations, limited
interpretability, and insufficient domain-specific medical knowledge, which can
potentially reduce clinical accuracy. Although recent approaches combining
imaging models with LLM reasoning have improved reporting, they typically rely
on a single generalist agent, restricting their capacity to emulate the diverse
and complex reasoning found in multidisciplinary medical teams. To address
these limitations, we propose MedChat, a multi-agent diagnostic framework and
platform that combines specialized vision models with multiple role-specific
LLM agents, all coordinated by a director agent. This design enhances
reliability, reduces hallucination risk, and enables interactive diagnostic
reporting through an interface tailored for clinical review and educational
use. Code available at https://github.com/Purdue-M2/MedChat.

</details>


### [432] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: G-Memory is a hierarchical memory system for multi-agent systems (MAS) inspired by organizational memory theory, improving success rates in embodied action and accuracy in knowledge QA.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current MAS memory mechanisms which are overly simplistic and lack customization, hindering the self-evolution capacity of MAS.

Method: Introduced G-Memory, a three-tier graph hierarchy (insight, query, and interaction graphs) that manages lengthy MAS interactions. It retrieves high-level insights and fine-grained interaction trajectories via bi-directional memory traversal and evolves by assimilating new collaborative trajectories.

Result: Experiments across five benchmarks, three LLM backbones, and three MAS frameworks showed G-Memory improves success rates in embodied action by up to 20.89% and accuracy in knowledge QA by up to 10.12%. No modifications were needed to original frameworks.

Conclusion: G-Memory bridges the gap in MAS memory architectures, enhancing cognitive and execution capabilities through an advanced hierarchical, agentic memory system.

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


### [433] [Diffusion of Responsibility in Collective Decision Making](https://arxiv.org/abs/2506.07935)
*Pavel Naumov,Jia Tao*

Main category: cs.MA

TL;DR: 在多代理集体决策中，避免责任扩散的唯一方法是实行'独裁'或'选举独裁'。


<details>
  <summary>Details</summary>
Motivation: 研究在集体决策机制中的'责任扩散'现象，以期明确个体责任并改进决策机制。

Method: 定义了决策机制的双模拟（bisimulation），证明双模拟保留了与责任相关的属性，并对最小的双模拟机制建立了结果。

Result: 对于两个代理人的决策，避免责任扩散的唯一方法是一个代理人充当'独裁者'；对于超过两个代理人的场景，任何无责任扩散的机制都是一个'选举独裁'，即代理人选举一名代表进行单方面决策。

Conclusion: 在集体决策中，完全避免责任扩散需要牺牲民主性，转而采用独裁形式。

Abstract: The term "diffusion of responsibility'' refers to situations in which
multiple agents share responsibility for an outcome, obscuring individual
accountability. This paper examines this frequently undesirable phenomenon in
the context of collective decision-making mechanisms.
  The work shows that if a decision is made by two agents, then the only way to
avoid diffusion of responsibility is for one agent to act as a "dictator'',
making the decision unilaterally. In scenarios with more than two agents, any
diffusion-free mechanism is an "elected dictatorship'' where the agents elect a
single agent to make a unilateral decision.
  The technical results are obtained by defining a bisimulation of
decision-making mechanisms, proving that bisimulation preserves
responsibility-related properties, and establishing the results for a smallest
bisimular mechanism.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [434] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: This paper explains why large language models cannot avoid hallucination and presents an impossibility theorem. It shows that no inference mechanism can meet four fundamental properties simultaneously.


<details>
  <summary>Details</summary>
Motivation: To understand why it's impossible to create large language models without hallucination and to find the trade-offs in designing these models.

Method: Modeling LLM inference as an auction of ideas, using the Green-Laffont theorem to prove the impossibility of meeting four key properties simultaneously.

Result: Demonstrates that no inference mechanism can be truthful, conserve semantic information, reveal relevant knowledge, and achieve knowledge-constrained optimality all at once.

Conclusion: The impossibility theorem provides a solid foundation for understanding the nature of inference process in LLMs, impacting model architecture, training objectives, and evaluation methods.

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


### [435] [A Statistical Framework for Model Selection in LSTM Networks](https://arxiv.org/abs/2506.06840)
*Fahad Mostafa*

Main category: stat.ML

TL;DR: 本文提出了一种用于LSTM网络系统模型选择的统一统计框架，通过定义适应时间结构的惩罚似然、隐藏状态动态的广义阈值方法和使用变分贝叶斯及近似边缘似然的有效估计策略，改进了模型选择过程。


<details>
  <summary>Details</summary>
Motivation: 尽管LSTM神经网络在许多应用中取得了成功，但模型选择问题（包括超参数调整、架构规范和正则化选择）仍然主要依赖于启发式方法且计算成本高昂。

Method: 该框架扩展了经典模型选择思想，如信息标准和收缩估计，到顺序神经网络，并定义了适应时间结构的惩罚似然，提出了隐藏状态动态的广义阈值方法，并提供了基于变分贝叶斯和近似边际似然的有效估计策略。

Result: 几个以生物医学数据为中心的例子展示了所提出的框架的灵活性和改进性能。

Conclusion: 该框架为LSTM网络提供了一个系统化的模型选择方法，能够提高模型选择效率并改善性能。

Abstract: Long Short-Term Memory (LSTM) neural network models have become the
cornerstone for sequential data modeling in numerous applications, ranging from
natural language processing to time series forecasting. Despite their success,
the problem of model selection, including hyperparameter tuning, architecture
specification, and regularization choice remains largely heuristic and
computationally expensive. In this paper, we propose a unified statistical
framework for systematic model selection in LSTM networks. Our framework
extends classical model selection ideas, such as information criteria and
shrinkage estimation, to sequential neural networks. We define penalized
likelihoods adapted to temporal structures, propose a generalized threshold
approach for hidden state dynamics, and provide efficient estimation strategies
using variational Bayes and approximate marginal likelihood methods. Several
biomedical data centric examples demonstrate the flexibility and improved
performance of the proposed framework.

</details>


### [436] [Direct Fisher Score Estimation for Likelihood Maximization](https://arxiv.org/abs/2506.06542)
*Sherman Khoo,Yakun Wang,Song Liu,Mark Beaumont*

Main category: stat.ML

TL;DR: 这篇论文提出了一种基于梯度的序列优化方法，用于处理似然函数难以计算但模型模拟容易获得的情况。通过局部得分匹配技术和线性参数化的方法，该技术提供了快速、灵活且高效的Fisher得分近似，并在理论上保证了估计器的性能。实验结果表明，该方法在一系列合成和真实问题上优于现有基准方法。


<details>
  <summary>Details</summary>
Motivation: 研究者们面临一个挑战：当似然函数难以直接计算时，如何最大化似然函数。为了解决这个问题，需要一种新的方法，利用模型模拟数据来近似Fisher得分，从而实现有效的优化。

Method: 提出了一种序列、基于梯度的优化方法，该方法直接对Fisher得分进行建模。使用局部得分匹配技术，从每个参数迭代周围的局部区域生成模拟数据。通过采用线性参数化的方法，得到了封闭形式的最小二乘解。这种方法有效地平滑了似然目标，缓解了复杂似然景观带来的困难。

Result: 理论分析证明了得分估计器的性能，并给出了由平滑引入的偏差界。在多个合成和真实世界问题上的实证结果表明，所提出的方法相比现有的基准方法具有更好的性能。

Conclusion: 该研究成功开发了一种新的优化方法，适用于难以计算似然函数但易于获取模型模拟数据的情况。此方法不仅在理论上得到了保障，而且在实践中也表现优异。

Abstract: We study the problem of likelihood maximization when the likelihood function
is intractable but model simulations are readily available. We propose a
sequential, gradient-based optimization method that directly models the Fisher
score based on a local score matching technique which uses simulations from a
localized region around each parameter iterate. By employing a linear
parameterization to the surrogate score model, our technique admits a
closed-form, least-squares solution. This approach yields a fast, flexible, and
efficient approximation to the Fisher score, effectively smoothing the
likelihood objective and mitigating the challenges posed by complex likelihood
landscapes. We provide theoretical guarantees for our score estimator,
including bounds on the bias introduced by the smoothing. Empirical results on
a range of synthetic and real-world problems demonstrate the superior
performance of our method compared to existing benchmarks.

</details>


### [437] [Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations](https://arxiv.org/abs/2506.06613)
*Arefe Boushehrian,Amir Najafi*

Main category: stat.ML

TL;DR: 本文研究了样本可压缩分布族在受扰样本下的可学习性，并提出了一个扰动量化框架以分析两种数据扰动模型，最终导出了高维均匀分布和高斯混合模型的学习样本复杂度新界。


<details>
  <summary>Details</summary>
Motivation: 尽管已知样本可压缩性可以保证PAC-可学习性，但其在受扰样本下的表现尚未被充分研究。因此，本文旨在探讨样本可压缩分布族是否在数据扰动下仍然可学习，并进一步探索其样本复杂度。

Method: 作者分析了两种数据扰动模型：加性独立噪声模型和对抗性破坏模型，并提出了一种扰动量化框架，该框架与压缩方案自然结合，从而导出随噪声水平和破坏预算优雅扩展的样本复杂度界。

Result: 通过所提出的框架，作者成功建立了有限高维均匀分布混合模型在噪声和对抗性扰动下的新样本复杂度界，并解决了两个文献中的开放问题，即高维均匀分布和高斯混合模型从对抗性破坏样本中的学习。

Conclusion: 样本可压缩分布族即使在受扰样本下也保持可学习性，且所提出的扰动量化框架为未来相关研究提供了有力工具。

Abstract: Learning distribution families over $\mathbb{R}^d$ is a fundamental problem
in unsupervised learning and statistics. A central question in this setting is
whether a given family of distributions possesses sufficient structure to be
(at least) information-theoretically learnable and, if so, to characterize its
sample complexity. In 2018, Ashtiani et al. reframed \emph{sample
compressibility}, originally due to Littlestone and Warmuth (1986), as a
structural property of distribution classes, proving that it guarantees
PAC-learnability. This discovery subsequently enabled a series of recent
advancements in deriving nearly tight sample complexity bounds for various
high-dimensional open problems. It has been further conjectured that the
converse also holds: every learnable class admits a tight sample compression
scheme.
  In this work, we establish that sample compressible families remain learnable
even from perturbed samples, subject to a set of necessary and sufficient
conditions. We analyze two models of data perturbation: (i) an additive
independent noise model, and (ii) an adversarial corruption model, where an
adversary manipulates a limited subset of the samples unknown to the learner.
Our results are general and rely on as minimal assumptions as possible. We
develop a perturbation-quantization framework that interfaces naturally with
the compression scheme and leads to sample complexity bounds that scale
gracefully with the noise level and corruption budget. As concrete
applications, we establish new sample complexity bounds for learning finite
mixtures of high-dimensional uniform distributions under both noise and
adversarial perturbations, as well as for learning Gaussian mixture models from
adversarially corrupted samples, resolving two open problems in the literature.

</details>


### [438] [Continuous Semi-Implicit Models](https://arxiv.org/abs/2506.06778)
*Longlin Yu,Jiajun Zha,Tong Yang,Tianyu Xie,Xiangyu Zhang,S. -H. Gary Chan,Cheng Zhang*

Main category: stat.ML

TL;DR: CoSIM, a continuous semi-implicit model that extends hierarchical semi-implicit models into a continuous framework with a continuous transition kernel, enables efficient training and performs on par or better than existing diffusion model acceleration methods.


<details>
  <summary>Details</summary>
Motivation: Semi-implicit distributions have shown great promise in variational inference and generative modeling, but the sequential training of hierarchical semi-implicit models often suffers from slow convergence.

Method: The paper introduces CoSIM, which incorporates a continuous transition kernel to enable efficient, simulation-free training and achieve consistency with a carefully designed transition kernel.

Result: Extensive experiments on image generation demonstrate that CoSIM performs on par or better than existing diffusion model acceleration methods, achieving superior performance on FD-DINOv2.

Conclusion: CoSIM offers a novel approach for multistep distillation of generative models at the distributional level and shows promising results in accelerating diffusion models.

Abstract: Semi-implicit distributions have shown great promise in variational inference
and generative modeling. Hierarchical semi-implicit models, which stack
multiple semi-implicit layers, enhance the expressiveness of semi-implicit
distributions and can be used to accelerate diffusion models given pretrained
score networks. However, their sequential training often suffers from slow
convergence. In this paper, we introduce CoSIM, a continuous semi-implicit
model that extends hierarchical semi-implicit models into a continuous
framework. By incorporating a continuous transition kernel, CoSIM enables
efficient, simulation-free training. Furthermore, we show that CoSIM achieves
consistency with a carefully designed transition kernel, offering a novel
approach for multistep distillation of generative models at the distributional
level. Extensive experiments on image generation demonstrate that CoSIM
performs on par or better than existing diffusion model acceleration methods,
achieving superior performance on FD-DINOv2.

</details>


### [439] [The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes](https://arxiv.org/abs/2506.06828)
*Simon P. von der Maase*

Main category: stat.ML

TL;DR: 本文提出了一种新颖的方法，利用冲突事件的高度时空细分数据与高斯过程结合，估计冲突的时间和空间趋势。这种方法有助于理解冲突陷阱、扩散和时空间冲突暴露，并能对未来冲突进行预测，且仅依赖于过去的冲突模式这一数据源。


<details>
  <summary>Details</summary>
Motivation: 现有的冲突研究可能缺乏对时间和空间细节的关注，无法有效揭示冲突的趋势及其影响因素。

Method: 通过结合高度时空细分的冲突事件数据与高斯过程，估计冲突的时空间趋势。此方法可以解析冲突陷阱、扩散现象及时空间冲突暴露情况，同时可用于其他估计任务中的控制变量或对未来冲突模式进行外推预测。

Result: 成功地用相对简约的框架估计了冲突的时空间趋势，并实现了先进的冲突预测，仅依赖于过去冲突模式的数据。

Conclusion: 所提出的方法为理解和预测冲突提供了新视角，强调了过去冲突模式作为单一数据源的有效性。

Abstract: I present a novel approach to estimating the temporal and spatial patterns of
violent conflict. I show how we can use highly temporally and spatially
disaggregated data on conflict events in tandem with Gaussian processes to
estimate temporospatial conflict trends. These trends can be studied to gain
insight into conflict traps, diffusion and tempo-spatial conflict exposure in
general; they can also be used to control for such phenomenons given other
estimation tasks; lastly, the approach allow us to extrapolate the estimated
tempo-spatial conflict patterns into future temporal units, thus facilitating
powerful, stat-of-the-art, conflict forecasts. Importantly, these results are
achieved via a relatively parsimonious framework using only one data source:
past conflict patterns.

</details>


### [440] [Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis](https://arxiv.org/abs/2506.07011)
*Yuan-Hao Wei,Yan-Jie Sun*

Main category: stat.ML

TL;DR: This study proposes Half Adversarial VAE (Half-AVAE), an advancement in the VAE framework for Independent Component Analysis (ICA). It eliminates explicit inverse mapping and integrates adversarial networks and External Enhancement terms to promote mutual independence among latent dimensions, achieving better factorized and interpretable representations. Experiments show that Half-AVAE outperforms baseline models in recovering independent components under underdetermined conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in Independent Component Analysis (ICA) under both determined and underdetermined conditions within the Variational Autoencoder (VAE) framework, specifically enhancing the independence and interpretability of latent variables.

Method: The proposed method is the Half Adversarial VAE (Half-AVAE), which builds on the encoder-free Half-VAE framework. It integrates adversarial networks and External Enhancement (EE) terms to promote mutual independence among latent dimensions.

Result: Experiments with synthetic signals demonstrate that Half-AVAE outperforms baseline models, including GP-AVAE and Half-VAE, in recovering independent components under underdetermined conditions, as evidenced by lower root mean square errors.

Conclusion: The study concludes that the flexibility of VAEs in variational inference, combined with adversarial training and structured priors, enables effective solutions for complex ICA tasks, advancing applications in disentanglement, causal inference, and generative modeling.

Abstract: This study advances the Variational Autoencoder (VAE) framework by addressing
challenges in Independent Component Analysis (ICA) under both determined and
underdetermined conditions, focusing on enhancing the independence and
interpretability of latent variables. Traditional VAEs map observed data to
latent variables and back via an encoder-decoder architecture, but struggle
with underdetermined ICA where the number of latent variables exceeds observed
signals. The proposed Half Adversarial VAE (Half-AVAE) builds on the
encoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle
underdetermined scenarios. By integrating adversarial networks and External
Enhancement (EE) terms, Half-AVAE promotes mutual independence among latent
dimensions, achieving factorized and interpretable representations. Experiments
with synthetic signals demonstrate that Half-AVAE outperforms baseline models,
including GP-AVAE and Half-VAE, in recovering independent components under
underdetermined conditions, as evidenced by lower root mean square errors. The
study highlights the flexibility of VAEs in variational inference, showing that
encoder omission, combined with adversarial training and structured priors,
enables effective solutions for complex ICA tasks, advancing applications in
disentanglement, causal inference, and generative modeling.

</details>


### [441] [Quantile-Optimal Policy Learning under Unmeasured Confounding](https://arxiv.org/abs/2506.07140)
*Zhongren Chen,Siyu Chen,Zhengling Qi,Xiaohong Chen,Zhuoran Yang*

Main category: stat.ML

TL;DR: This paper focuses on quantile-optimal policy learning in offline settings with unobserved confounders. The authors propose causal-assisted policy learning methods addressing nonlinearity, confounding issues, and insufficient dataset coverage. They use tools like instrumental variables and negative controls, adopt a minimax estimation approach, and construct conservative policy estimates. A novel regularized policy learning method is also proposed for computational efficiency. The learned policies are proven to be quantile-optimal under certain assumptions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop effective policy learning methods that can handle the challenges of nonlinearity in quantile objectives, unobserved confounding, and insufficient coverage of offline datasets in the context of quantile-optimal policy learning.

Method: The methods include using causal inference tools such as instrumental variables and negative controls to estimate quantile objectives by solving nonlinear functional integral equations. A minimax estimation approach with nonparametric models is adopted to solve these equations, and conservative policy estimates are constructed. Additionally, a regularized policy learning method is proposed for better computational amenability.

Result: The policies learned by the proposed methods are proven to be $\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under mild coverage assumptions on the offline dataset.

Conclusion: The authors claim that their work proposes the first sample-efficient policy learning algorithms for estimating quantile-optimal policies in the presence of unmeasured confounding.

Abstract: We study quantile-optimal policy learning where the goal is to find a policy
whose reward distribution has the largest $\alpha$-quantile for some $\alpha
\in (0, 1)$. We focus on the offline setting whose generating process involves
unobserved confounders. Such a problem suffers from three main challenges: (i)
nonlinearity of the quantile objective as a functional of the reward
distribution, (ii) unobserved confounding issue, and (iii) insufficient
coverage of the offline dataset. To address these challenges, we propose a
suite of causal-assisted policy learning methods that provably enjoy strong
theoretical guarantees under mild conditions. In particular, to address (i) and
(ii), using causal inference tools such as instrumental variables and negative
controls, we propose to estimate the quantile objectives by solving nonlinear
functional integral equations. Then we adopt a minimax estimation approach with
nonparametric models to solve these integral equations, and propose to
construct conservative policy estimates that address (iii). The final policy is
the one that maximizes these pessimistic estimates. In addition, we propose a
novel regularized policy learning method that is more amenable to computation.
Finally, we prove that the policies learned by these methods are
$\tilde{\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage
assumption on the offline dataset. Here, $\tilde{\mathscr{O}}(\cdot)$ omits
poly-logarithmic factors. To the best of our knowledge, we propose the first
sample-efficient policy learning algorithms for estimating the quantile-optimal
policy when there exist unmeasured confounding.

</details>


### [442] [ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition](https://arxiv.org/abs/2506.07259)
*Daolang Huang,Xinyi Wen,Ayush Bharti,Samuel Kaski,Luigi Acerbi*

Main category: stat.ML

TL;DR: ALINE is a unified framework for amortized Bayesian inference and active data acquisition that leverages a transformer architecture trained via reinforcement learning to strategically query informative data points while simultaneously refining predictions.


<details>
  <summary>Details</summary>
Motivation: Critical applications require systems that can acquire the most informative data and perform instant inference. Existing methods for Bayesian inference and experimental design are not optimal for tasks requiring new data collection for instant inference.

Method: ALINE uses a transformer architecture trained with reinforcement learning, utilizing a reward based on self-estimated information gain from its integrated inference component. It can selectively direct querying strategies towards specific model parameters or predictive tasks.

Result: Empirical results show ALINE provides instant and accurate inference along with efficient selection of informative points in regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model.

Conclusion: ALINE delivers both instant and accurate inference and efficiently selects informative data points, making it suitable for various tasks requiring strategic data acquisition and inference.

Abstract: Many critical applications, from autonomous scientific discovery to
personalized medicine, demand systems that can both strategically acquire the
most informative data and instantaneously perform inference based upon it.
While amortized methods for Bayesian inference and experimental design offer
part of the solution, neither approach is optimal in the most general and
challenging task, where new data needs to be collected for instant inference.
To tackle this issue, we introduce the Amortized Active Learning and Inference
Engine (ALINE), a unified framework for amortized Bayesian inference and active
data acquisition. ALINE leverages a transformer architecture trained via
reinforcement learning with a reward based on self-estimated information gain
provided by its own integrated inference component. This allows it to
strategically query informative data points while simultaneously refining its
predictions. Moreover, ALINE can selectively direct its querying strategy
towards specific subsets of model parameters or designated predictive tasks,
optimizing for posterior estimation, data prediction, or a mixture thereof.
Empirical results on regression-based active learning, classical Bayesian
experimental design benchmarks, and a psychometric model with selectively
targeted parameters demonstrate that ALINE delivers both instant and accurate
inference along with efficient selection of informative points.

</details>


### [443] [Rao-Blackwellised Reparameterisation Gradients](https://arxiv.org/abs/2506.07687)
*Kevin Lam,Thang Bui,George Deligiannidis,Yee Whye Teh*

Main category: stat.ML

TL;DR: The paper proposes R2-G2 estimator for latent Gaussian variables and shows its benefits in probabilistic models.


<details>
  <summary>Details</summary>
Motivation: Gradient estimators are essential for optimizing models with latent Gaussian variables, where reparameterisation trick is widely used. There's a need to improve the efficiency of gradient estimation.

Method: Propose R2-G2 as an improved Rao-Blackwellised reparameterisation gradient estimator. Show that local reparameterisation gradient estimator for Bayesian MLPs is an instance of R2-G2.

Result: R2-G2 yields better performance during initial training in models using multiple reparameterisation tricks.

Conclusion: R2-G2 can extend the benefits of Rao-Blackwellised gradients to various probabilistic models.

Abstract: Latent Gaussian variables have been popularised in probabilistic machine
learning. In turn, gradient estimators are the machinery that facilitates
gradient-based optimisation for models with latent Gaussian variables. The
reparameterisation trick is often used as the default estimator as it is simple
to implement and yields low-variance gradients for variational inference. In
this work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the
reparameterisation gradient estimator. Interestingly, we show that the local
reparameterisation gradient estimator for Bayesian MLPs is an instance of the
R2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of
Rao-Blackwellised gradients to a suite of probabilistic models. We show that
initial training with R2-G2 consistently yields better performance in models
with multiple applications of the reparameterisation trick.

</details>


### [444] [Quickest Causal Change Point Detection by Adaptive Intervention](https://arxiv.org/abs/2506.07760)
*Haijie Xu,Chen Zhang*

Main category: stat.ML

TL;DR: An algorithm for change point monitoring in linear causal models is proposed, which can concentrate changes into a single dimension through centralization technique and amplify change magnitude by selecting intervention nodes based on Kullback-Leibler divergence. Two monitoring methods with adaptive intervention policies are also presented, achieving a balance between exploration and exploitation. Theoretical first-order optimality is demonstrated and validated via simulation and real-world case studies.


<details>
  <summary>Details</summary>
Motivation: To develop an effective method for monitoring change points in linear causal models while accounting for interventions.

Method: The algorithm uses a special centralization technique to concentrate changes from causal propagation across nodes into one dimension. Intervention nodes are selected based on Kullback-Leibler divergence to amplify the change magnitude. An algorithm for selecting intervention values is provided to identify the most effective intervention nodes. Two monitoring methods with adaptive intervention policies are introduced to balance exploration and exploitation.

Result: Theoretical first-order optimality of the proposed methods is demonstrated. Properties are validated using simulation datasets and two real-world case studies.

Conclusion: The proposed algorithm effectively monitors change points in linear causal models considering interventions, concentrating changes into a single dimension and amplifying change magnitude. The two monitoring methods with adaptive intervention policies achieve a good balance between exploration and exploitation.

Abstract: We propose an algorithm for change point monitoring in linear causal models
that accounts for interventions. Through a special centralization technique, we
can concentrate the changes arising from causal propagation across nodes into a
single dimension. Additionally, by selecting appropriate intervention nodes
based on Kullback-Leibler divergence, we can amplify the change magnitude. We
also present an algorithm for selecting the intervention values, which aids in
the identification of the most effective intervention nodes. Two monitoring
methods are proposed, each with an adaptive intervention policy to make a
balance between exploration and exploitation. We theoretically demonstrate the
first-order optimality of the proposed methods and validate their properties
using simulation datasets and two real-world case studies.

</details>


### [445] [Accelerating Constrained Sampling: A Large Deviations Approach](https://arxiv.org/abs/2506.07816)
*Yingli Wang,Changwei Tu,Xiaoyu Wang,Lingjiong Zhu*

Main category: stat.ML

TL;DR: The paper focuses on the long-time behavior of skew-reflected non-reversible Langevin dynamics (SRNLD) and proposes a method to design the skew-symmetric matrix in SRNLD for better performance. It establishes a large deviation principle (LDP) for SRNLD and shows that SRNLD can accelerate convergence to the target distribution compared to RLD with this choice of the skew-symmetric matrix.


<details>
  <summary>Details</summary>
Motivation: To address the problem of sampling a target probability distribution on a constrained domain, which is important in many applications including machine learning.

Method: Establishes a large deviation principle (LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is chosen such that its product with the inward unit normal vector field on the boundary is zero.

Result: Shows that SRNLD can accelerate the convergence to the target distribution compared to RLD with this choice of the skew-symmetric matrix. Numerical experiments validate the theoretical findings from the large deviations theory.

Conclusion: SRNLD can achieve good performance in practice by designing the skew-symmetric matrix appropriately.

Abstract: The problem of sampling a target probability distribution on a constrained
domain arises in many applications including machine learning. For constrained
sampling, various Langevin algorithms such as projected Langevin Monte Carlo
(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and
more generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)
based on the discretization of skew-reflected non-reversible Langevin dynamics
(SRNLD) have been proposed and studied in the literature. This work focuses on
the long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.
Although the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the
acceleration compared to RLD (and PMLC) have been studied in the literature, it
is not clear how one should design the skew-symmetric matrix in the dynamics to
achieve good performance in practice. We establish a large deviation principle
(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is
chosen such that its product with the inward unit normal vector field on the
boundary is zero. By explicitly characterizing the rate functions, we show that
SRNLD can accelerate the convergence to the target distribution compared to RLD
with this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC
based on the proposed skew-symmetric matrix show superior performance which
validate the theoretical findings from the large deviations theory.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [446] [An Efficient Digital Watermarking Technique for Small Scale devices](https://arxiv.org/abs/2506.06691)
*Kaushik Talathi,Aparna Santra Biswas*

Main category: cs.MM

TL;DR: In the era of IoT and mobile platforms, this study introduces a hybrid FWT-AQIM watermarking scheme that ensures content authenticity on low-power devices. It achieves balance among robustness, imperceptibility, and computational efficiency with quality assessments yielding PSNR≥34 dB and SSIM≥0.97.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring content authenticity while avoiding overburdening limited hardware in the age of IoT and mobile platforms.

Method: The method embeds watermark in the luminance component of YCbCr color space using low-frequency FWT sub-bands, minimizing perceptual distortion, using additive QIM for simplicity.

Result: Quality assessments yield PSNR≥34 dB and SSIM≥0.97, with near-zero bit error rates and NCC≥0.998 under various attacks. Throughput peaks at 11 MP/s.

Conclusion: FWT-AQIM provides an efficient, scalable solution for real-time, secure watermarking in bandwidth- and power-constrained contexts.

Abstract: In the age of IoT and mobile platforms, ensuring that content stay authentic
whilst avoiding overburdening limited hardware is a key problem. This study
introduces hybrid Fast Wavelet Transform & Additive Quantization index
Modulation (FWT-AQIM) scheme, a lightweight watermarking approach that secures
digital pictures on low-power, memory-constrained small scale devices to
achieve a balanced trade-off among robustness, imperceptibility, and
computational efficiency. The method embeds watermark in the luminance
component of YCbCr color space using low-frequency FWT sub-bands, minimizing
perceptual distortion, using additive QIM for simplicity. Both the extraction
and embedding processes run in less than 40 ms and require minimum RAM when
tested on a Raspberry Pi 5. Quality assessments on standard and high-resolution
images yield PSNR greater than equal to 34 dB and SSIM greater than equal to
0.97, while robustness verification includes various geometric and
signal-processing attacks demonstrating near-zero bit error rates and NCC
greater than equal to 0.998. Using a mosaic-based watermark, redundancy added
enhancing robustness without reducing throughput, which peaks at 11 MP/s. These
findings show that FWT-AQIM provides an efficient, scalable solution for
real-time, secure watermarking in bandwidth- and power-constrained contexts,
opening the way for dependable content protection in developing IoT and
multimedia applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [447] [Identity Deepfake Threats to Biometric Authentication Systems: Public and Expert Perspectives](https://arxiv.org/abs/2506.06825)
*Shijing He,Yaxiong Lei,Zihan Zhang,Yuzhou Sun,Shujun Li,Chi Zhang,Juan Ye*

Main category: cs.HC

TL;DR: 生成式AI（Gen-AI）深度伪造对生物特征认证构成快速演变的威胁。研究通过综合混合方法研究揭示了公众与专家在风险认知上的差距，并提出了一种新的深度伪造攻击链模型和三层缓解框架以应对威胁。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI深度伪造技术对生物特征认证构成严重威胁，但公众对其风险的认知远低于专家的理解，这种脱节导致系统存在关键漏洞。

Method: 研究采用综合混合方法，包括对408名专业人士进行调查以及对37名参与者（25名专家和12名非专家）进行深入访谈，分析生物特征认证中深度伪造的威胁。并引入了基于Hutchins等人网络安全框架的新型Deepfake Kill Chain模型。

Result: 研究发现公众日益依赖生物特征认证的便捷性，而专家则对静态模态（如面部和语音识别）被欺骗表示严重担忧。不同人口统计和行业间存在显著的意识和信任差异。基于研究结果提出了一个三层次缓解框架。

Conclusion: 本研究首次提供了防御AI生成身份威胁的经验基础路线图，将技术保障与以人为中心的洞察相结合，强调动态生物特征信号、隐私保护的数据治理和针对性教育的重要性。

Abstract: Generative AI (Gen-AI) deepfakes pose a rapidly evolving threat to biometric
authentication, yet a significant gap exists between expert understanding of
these risks and public perception. This disconnection creates critical
vulnerabilities in systems trusted by millions. To bridge this gap, we
conducted a comprehensive mixed-method study, surveying 408 professionals
across key sectors and conducting in-depth interviews with 37 participants (25
experts, 12 general public [non-experts]). Our findings reveal a paradox: while
the public increasingly relies on biometrics for convenience, experts express
grave concerns about the spoofing of static modalities like face and voice
recognition. We found significant demographic and sector-specific divides in
awareness and trust, with finance professionals, for example, showing
heightened skepticism. To systematically analyze these threats, we introduce a
novel Deepfake Kill Chain model, adapted from Hutchins et al.'s cybersecurity
frameworks to map the specific attack vectors used by malicious actors against
biometric systems. Based on this model and our empirical findings, we propose a
tri-layer mitigation framework that prioritizes dynamic biometric signals
(e.g., eye movements), robust privacy-preserving data governance, and targeted
educational initiatives. This work provides the first empirically grounded
roadmap for defending against AI-generated identity threats by aligning
technical safeguards with human-centered insights.

</details>


### [448] [LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models](https://arxiv.org/abs/2506.06874)
*Ala Yankouskaya,Areej B. Babiker,Syeda W. F. Rizvi,Sameha Alshakhsi,Magnus Liebherr,Raian Ali*

Main category: cs.HC

TL;DR: 研究人员开发并验证了一个新的12项问卷（LLM-D12）以测量人们对大语言模型的依赖程度，揭示了工具依赖和关系依赖两个维度。


<details>
  <summary>Details</summary>
Motivation: 现有的评估人类对大语言模型依赖的工具主要基于经典的行为成瘾症状，未能充分捕捉LLM-人类关系的复杂性。

Method: 研究人员基于先前的理论工作，开发了一个包含12个项目的问卷（LLM-D12），并在526名英国参与者中收集了数据。通过探索性和验证性因子分析，使用样本分割方法，确定了量表的两因子结构：工具依赖和关系依赖。

Result: LLM-D12量表显示出极好的内部一致性和清晰的区分效度，外部验证也确认了其概念基础和两个子量表之间的区别。

Conclusion: LLM-D12为理解人类对大语言模型的依赖提供了一个新颖且细致的视角，强调这种依赖不一定是功能障碍，但在某些情况下可能变得有问题。

Abstract: There is growing interest in understanding how people interact with large
language models (LLMs) and whether such models elicit dependency or even
addictive behaviour. Validated tools to assess the extent to which individuals
may become dependent on LLMs are scarce and primarily build on classic
behavioral addiction symptoms, adapted to the context of LLM use. We view this
as a conceptual limitation, as the LLM-human relationship is more nuanced and
warrants a fresh and distinct perspective. To address this gap, we developed
and validated a new 12-item questionnaire to measure LLM dependency, referred
to as LLM-D12. The scale was based on the authors' prior theoretical work, with
items developed accordingly and responses collected from 526 participants in
the UK. Exploratory and confirmatory factor analyses, performed on separate
halves of the total sample using a split-sample approach, supported a
two-factor structure: Instrumental Dependency (six items) and Relationship
Dependency (six items). Instrumental Dependency reflects the extent to which
individuals rely on LLMs to support or collaborate in decision-making and
cognitive tasks. Relationship Dependency captures the tendency to perceive LLMs
as socially meaningful, sentient, or companion-like entities. The two-factor
structure demonstrated excellent internal consistency and clear discriminant
validity. External validation confirmed both the conceptual foundation and the
distinction between the two subscales. The psychometric properties and
structure of our LLM-D12 scale were interpreted in light of the emerging view
that dependency on LLMs does not necessarily indicate dysfunction but may still
reflect reliance levels that could become problematic in certain contexts.

</details>


### [449] [Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation](https://arxiv.org/abs/2506.07211)
*Gionnieve Lim,Bryan Chen Zhengyu Tan,Kellie Yu Hui Sim,Weiyan Shi,Ming Hui Chew,Ming Shan Hee,Roy Ka-Wei Lee,Simon T. Perrault,Kenny Tsu Wei Choo*

Main category: cs.HC

TL;DR: The paper explores the dual role of Large Language Models (LLMs) in producing and combating disinformation through a communication game involving 25 participants, revealing varied uses based on roles and emphasizing balanced future development.


<details>
  <summary>Details</summary>
Motivation: To understand the complex dynamics between LLMs and disinformation, including their potential misuse and effectiveness in detection and mitigation strategies.

Method: A communication game simulating online forums with 25 participants playing as Disinformers, Moderators, and Users, inspired by the game Werewolf.

Result: LLMs are used differently depending on participants' roles and strategies, highlighting both risks and opportunities in combating disinformation.

Conclusion: Future LLM development should take a balanced approach, empowering users, fostering trust, and mitigating risks of disinformation.

Abstract: The emergence of Large Language Models (LLMs) presents a dual challenge in
the fight against disinformation. These powerful tools, capable of generating
human-like text at scale, can be weaponised to produce sophisticated and
persuasive disinformation, yet they also hold promise for enhancing detection
and mitigation strategies. This paper investigates the complex dynamics between
LLMs and disinformation through a communication game that simulates online
forums, inspired by the game Werewolf, with 25 participants. We analyse how
Disinformers, Moderators, and Users leverage LLMs to advance their goals,
revealing both the potential for misuse and combating disinformation. Our
findings highlight the varying uses of LLMs depending on the participants'
roles and strategies, underscoring the importance of understanding their
effectiveness in this context. We conclude by discussing implications for
future LLM development and online platform design, advocating for a balanced
approach that empowers users and fosters trust while mitigating the risks of
LLM-assisted disinformation.

</details>


### [450] [Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency](https://arxiv.org/abs/2506.07281)
*Leah Hope Ajmani,Nuredin Ali Abdelkadir,Stevie Chancellor*

Main category: cs.HC

TL;DR: This paper discusses the inclusion of secondary stakeholders in participatory AI via semi-structured interviews, proposing three participatory ideals - informedness, consent, and agency - and introducing three stakeholder archetypes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the ideals of participatory AI to secondary stakeholders, who are often overlooked, through semi-structured interviews.

Method: Semi-structured interviews were conducted with secondary AI stakeholders to explore their perspectives on meaningful participation.

Result: Three participatory ideals were identified: informedness, consent, and agency. Three stakeholder archetypes were also introduced: the reluctant data contributor, the unsupported activist, and the well-intentioned practitioner.

Conclusion: The authors envision an AI future where secondary stakeholders can meaningfully participate with the AI systems they influence and are influenced by.

Abstract: As AI technologies become more human-facing, there have been numerous calls
to adapt participatory approaches to AI development -- spurring the idea of
participatory AI. However, these calls often focus only on primary
stakeholders, such as end-users, and not secondary stakeholders. This paper
seeks to translate the ideals of participatory AI to a broader population of
secondary AI stakeholders through semi-structured interviews. We theorize that
meaningful participation involves three participatory ideals: (1) informedness,
(2) consent, and (3) agency. We also explore how secondary stakeholders realize
these ideals by traversing a complicated problem space. Like walking up the
rungs of a ladder, these ideals build on one another. We introduce three
stakeholder archetypes: the reluctant data contributor, the unsupported
activist, and the well-intentioned practitioner, who must navigate systemic
barriers to achieving agentic AI relationships. We envision an AI future where
secondary stakeholders are able to meaningfully participate with the AI systems
they influence and are influenced by.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [451] [Private GPTs for LLM-driven testing in software development and machine learning](https://arxiv.org/abs/2506.06509)
*Jakub Jagielski,Markus Abel*

Main category: cs.SE

TL;DR: This paper explores the use of private GPTs in generating executable test code from requirements, comparing direct generation and a two-step method involving Gherkin syntax. The latter yields better results in terms of human readability and coding practices. They evaluate this across 'Hello World' and digit classification scenarios, showing structured prompts improve test quality.


<details>
  <summary>Details</summary>
Motivation: To assess the capability of private GPTs to automatically generate executable test code based on acceptance criteria formulated as part of epics or stories in modern development processes, providing product owners with a way to produce testable criteria through LLMs.

Method: Using acceptance criteria as input, they examine two methods for generating tests: i) directly generating code from requirements and ii) using an intermediate step with Gherkin syntax. They then evaluate the effectiveness of these methods across two scenarios: a simple 'Hello World' program and a digit classification model.

Result: The two-step procedure (using Gherkin syntax) yields better results defined by human readability and adherence to best coding practices, such as lines of code and use of additional libraries in testing.

Conclusion: Structured prompts lead to higher-quality test outputs when generating executable test code using private GPTs.

Abstract: In this contribution, we examine the capability of private GPTs to
automatically generate executable test code based on requirements. More
specifically, we use acceptance criteria as input, formulated as part of epics,
or stories, which are typically used in modern development processes. This
gives product owners, or business intelligence, respectively, a way to directly
produce testable criteria through the use of LLMs. We explore the quality of
the so-produced tests in two ways: i) directly by letting the LLM generate code
from requirements, ii) through an intermediate step using Gherkin syntax. As a
result, it turns out that the two-step procedure yields better results -where
we define better in terms of human readability and best coding practices, i.e.
lines of code and use of additional libraries typically used in testing.
Concretely, we evaluate prompt effectiveness across two scenarios: a simple
"Hello World" program and a digit classification model, showing that structured
prompts lead to higher-quality test outputs.

</details>


### [452] [Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain](https://arxiv.org/abs/2506.06946)
*Daniel Lawand,Lucas Quaresma,Roberto Bolgheroni,Alfredo Goldman,Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: This paper reports the evolution of the Continuous Training subsystem architecture in the SPIRA project, which aims to pre-diagnose respiratory insufficiency through speech analysis. The architecture evolved from a Big Ball of Mud to a Modular Monolith and then to Microservices, improving maintainability, robustness, and extensibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of deploying ML training pipelines into production with robust software engineering practices, as opposed to experimental workflows. This is investigated within the context of the SPIRA project.

Method: The method involves presenting an overview of the ML-Enabled System (MLES) for pre-diagnosing respiratory insufficiency via speech analysis and comparing three versions of the architecture of the Continuous Training subsystem: Big Ball of Mud, Modular Monolith, and Microservices.

Result: The results indicate that adopting different design principles and patterns improved the maintainability, robustness, and extensibility of the system.

Conclusion: The conclusion is that insights gained from this experience can benefit ML Engineers in productionizing ML training pipelines and Data Scientists adopting MLOps practices.

Abstract: Deploying a Machine Learning (ML) training pipeline into production requires
robust software engineering practices. This differs significantly from
experimental workflows. This experience report investigates this challenge in
SPIRA, a project whose goal is to create an ML-Enabled System (MLES) to
pre-diagnose insufficiency respiratory via speech analysis. The first version
of SPIRA's training pipeline lacked critical software quality attributes. This
paper presents an overview of the MLES, then compares three versions of the
architecture of the Continuous Training subsystem, which evolved from a Big
Ball of Mud, to a Modular Monolith, towards Microservices. By adopting
different design principles and patterns to enhance its maintainability,
robustness, and extensibility. In this way, the paper seeks to offer insights
for both ML Engineers tasked to productionize ML training pipelines and Data
Scientists seeking to adopt MLOps practices.

</details>


### [453] [Taxonomy of migration scenarios for Qiskit refactoring using LLMs](https://arxiv.org/abs/2506.07135)
*José Manuel Suárez,Luís Mariano Bibbó,Joaquín Bogado,Alejandro Fernandez*

Main category: cs.SE

TL;DR: 随着量子计算的发展，量子编程库的异构性和持续演变给软件开发者带来了新的挑战。本文通过开发量子电路重构问题的分类法，提供了一个分析和比较不同重构方法的结构化框架。研究使用大型语言模型（LLMs）对Qiskit版本迁移中的重构需求进行分类，并生成了统一的分类法，为AI辅助迁移和自动化重构技术的研究奠定了基础，同时推动了量子软件工程的发展。


<details>
  <summary>Details</summary>
Motivation: 量子编程库的频繁更新导致需要重构代码，增加了开发复杂性，且与经典软件工程中的重构挑战存在本质区别。

Method: 1. 分析Qiskit文档和发行说明以创建初始重构分类法。
2. 使用专家开发者和大型语言模型分别生成两个分类法。
3. 比较并整合两个分类法，形成统一的分类法。

Result: 生成了由专家和LLM产生的两种分类法，并整合成一个统一的分类法，用于系统地分类Qiskit中的重构挑战。

Conclusion: 该研究为未来关于AI辅助迁移和自动化重构技术的研究奠定了基础，并促进了量子软件工程的发展，包括优化软件开发流程、提高语言兼容性和推广最佳实践。

Abstract: As quantum computing advances, quantum programming libraries' heterogeneity
and steady evolution create new challenges for software developers. Frequent
updates in software libraries break working code that needs to be refactored,
thus adding complexity to an already complex landscape. These refactoring
challenges are, in many cases, fundamentally different from those known in
classical software engineering due to the nature of quantum computing software.
This study addresses these challenges by developing a taxonomy of quantum
circuit's refactoring problems, providing a structured framework to analyze and
compare different refactoring approaches. Large Language Models (LLMs) have
proven valuable tools for classic software development, yet their value in
quantum software engineering remains unexplored. This study uses LLMs to
categorize refactoring needs in migration scenarios between different Qiskit
versions. Qiskit documentation and release notes were scrutinized to create an
initial taxonomy of refactoring required for migrating between Qiskit releases.
Two taxonomies were produced: one by expert developers and one by an LLM. These
taxonomies were compared, analyzing differences and similarities, and were
integrated into a unified taxonomy that reflects the findings of both methods.
By systematically categorizing refactoring challenges in Qiskit, the unified
taxonomy is a foundation for future research on AI-assisted migration while
enabling a more rigorous evaluation of automated refactoring techniques.
Additionally, this work contributes to quantum software engineering (QSE) by
enhancing software development workflows, improving language compatibility, and
promoting best practices in quantum programming.

</details>


### [454] [IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents](https://arxiv.org/abs/2506.07524)
*Shiwei Feng,Xiangzhe Xu,Xuan Chen,Kaiyuan Zhang,Syed Yusuf Ahmed,Zian Su,Mingwei Zheng,Xiangyu Zhang*

Main category: cs.SE

TL;DR: IntenTest is an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents by generating realistic tasks based on toolkits' documentation and applying targeted mutations.


<details>
  <summary>Details</summary>
Motivation: LLM agents often misinterpret user intent, leading to actions that diverge from the intended goal. Traditional software testing methods are insufficient for handling the ambiguity of natural language.

Method: IntenTest generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. It uses semantic partitioning to organize tasks into meaningful categories and a datatype-aware strategy memory to enhance efficiency.

Result: Experiments on 80 toolkit APIs demonstrate that IntenTest effectively uncovers intent integrity violations, outperforming baselines in both error-exposing rate and query efficiency.

Conclusion: IntenTest generalizes well to stronger target models using smaller LLMs for test generation and adapts to evolving APIs across domains.

Abstract: LLM agents are increasingly deployed to automate real-world tasks by invoking
APIs through natural language instructions. While powerful, they often suffer
from misinterpretation of user intent, leading to the agent's actions that
diverge from the user's intended goal, especially as external toolkits evolve.
Traditional software testing assumes structured inputs and thus falls short in
handling the ambiguity of natural language. We introduce IntenTest, an
API-centric stress testing framework that systematically uncovers intent
integrity violations in LLM agents. Unlike prior work focused on fixed
benchmarks or adversarial inputs, IntenTest generates realistic tasks based on
toolkits' documentation and applies targeted mutations to expose subtle agent
errors while preserving user intent. To guide testing, we propose semantic
partitioning, which organizes natural language tasks into meaningful categories
based on toolkit API parameters and their equivalence classes. Within each
partition, seed tasks are mutated and ranked by a lightweight predictor that
estimates the likelihood of triggering agent errors. To enhance efficiency,
IntenTest maintains a datatype-aware strategy memory that retrieves and adapts
effective mutation patterns from past cases. Experiments on 80 toolkit APIs
demonstrate that IntenTest effectively uncovers intent integrity violations,
significantly outperforming baselines in both error-exposing rate and query
efficiency. Moreover, IntenTest generalizes well to stronger target models
using smaller LLMs for test generation, and adapts to evolving APIs across
domains.

</details>


### [455] [Towards a Small Language Model Lifecycle Framework](https://arxiv.org/abs/2506.07695)
*Parsa Miraghaei,Sergio Moreschini,Antti Kolehmainen,David Hästbacka*

Main category: cs.SE

TL;DR: This paper defines a comprehensive lifecycle framework for Small Language Models (SLMs) through synthesizing insights from academic literature and practitioner sources.


<details>
  <summary>Details</summary>
Motivation: The growing demand for efficient and deployable language models has led to increased interest in Small Language Models (SLMs). However, existing research remains fragmented, lacking a unified lifecycle perspective.

Method: Conducted a comprehensive survey of 36 works, analyzing and categorizing lifecycle-relevant techniques.

Result: Proposed a modular lifecycle model structured into main, optional, and cross-cutting components. The model captures key interconnections across stages, supporting method reuse, co-adaptation, and lifecycle-awareness.

Conclusion: The framework provides a coherent foundation for developing and maintaining SLMs, bridging theory and practice, and guiding future research and tool development.

Abstract: Background: The growing demand for efficient and deployable language models
has led to increased interest in Small Language Models (SLMs). However,
existing research remains fragmented, lacking a unified lifecycle perspective.
  Objective: This study aims to define a comprehensive lifecycle framework for
SLMs by synthesizing insights from academic literature and practitioner
sources.
  Method: We conducted a comprehensive survey of 36 works, analyzing and
categorizing lifecycle-relevant techniques.
  Results: We propose a modular lifecycle model structured into main, optional,
and cross-cutting components. The model captures key interconnections across
stages, supporting method reuse, co-adaptation, and lifecycle-awareness.
  Conclusion: Our framework provides a coherent foundation for developing and
maintaining SLMs, bridging theory and practice, and guiding future research and
tool development.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [456] [Conditional Local Independence Testing with Application to Dynamic Causal Discovery](https://arxiv.org/abs/2506.07844)
*Mingzhou Liu,Xinwei Sun,Yizhou Wang*

Main category: stat.ME

TL;DR: The paper extends conditional local independence testing theory to Ito processes for causal discovery in dynamic systems.


<details>
  <summary>Details</summary>
Motivation: To apply the conditional local independence testing theory for causal discovery in dynamic systems.

Method: Extending the conditional local independence testing theory developed in Christgau et al. (2024) to Ito processes.

Result: The extension allows for application in causal discovery within dynamic systems.

Conclusion: This extension of the theory provides a new approach for analyzing causality in dynamic systems through Ito processes.

Abstract: In this note, we extend the conditional local independence testing theory
developed in Christgau et al. (2024) to Ito processes. The result can be
applied to causal discovery in dynamic systems.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [457] [Tactile MNIST: Benchmarking Active Tactile Perception](https://arxiv.org/abs/2506.06361)
*Tim Schneider,Guillaume Duret,Cristiana de Farias,Roberto Calandra,Liming Chen,Jan Peters*

Main category: cs.RO

TL;DR: The paper introduces the Tactile MNIST Benchmark Suite, an open-source benchmark for active tactile perception tasks such as localization, classification, and volume estimation. It includes diverse simulation scenarios and a comprehensive dataset with synthetic 3D MNIST digit models and real-world tactile samples. A CycleGAN is trained for realistic tactile simulation rendering to facilitate progress in tactile sensing and active perception.


<details>
  <summary>Details</summary>
Motivation: Tactile perception can enhance robotic manipulation but lacks broad spatial awareness alone. Active perception techniques can guide sensors towards informative regions over time, improving scene understanding or task completion. However, both tactile sensing and active perception lack standardized benchmarks.

Method: Introduced the Tactile MNIST Benchmark Suite which is Gymnasium-compatible and designed for active tactile perception tasks including localization, classification, and volume estimation. The suite offers various simulation scenarios and includes a large dataset of synthetic 3D MNIST digit models and real-world tactile samples. A CycleGAN was also trained for realistic tactile simulation rendering.

Result: The Tactile MNIST Benchmark Suite provides standardized protocols and reproducible evaluation frameworks, promoting systematic progress in tactile sensing and active perception.

Conclusion: This benchmark suite addresses the need for standardized benchmarks in tactile sensing and active perception, offering tools and resources that advance research in these fields.

Abstract: Tactile perception has the potential to significantly enhance dexterous
robotic manipulation by providing rich local information that can complement or
substitute for other sensory modalities such as vision. However, because
tactile sensing is inherently local, it is not well-suited for tasks that
require broad spatial awareness or global scene understanding on its own. A
human-inspired strategy to address this issue is to consider active perception
techniques instead. That is, to actively guide sensors toward regions with more
informative or significant features and integrate such information over time in
order to understand a scene or complete a task. Both active perception and
different methods for tactile sensing have received significant attention
recently. Yet, despite advancements, both fields lack standardized benchmarks.
To bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an
open-source, Gymnasium-compatible benchmark specifically designed for active
tactile perception tasks, including localization, classification, and volume
estimation. Our benchmark suite offers diverse simulation scenarios, from
simple toy environments all the way to complex tactile perception tasks using
vision-based tactile sensors. Furthermore, we also offer a comprehensive
dataset comprising 13,500 synthetic 3D MNIST digit models and 153,600
real-world tactile samples collected from 600 3D printed digits. Using this
dataset, we train a CycleGAN for realistic tactile simulation rendering. By
providing standardized protocols and reproducible evaluation frameworks, our
benchmark suite facilitates systematic progress in the fields of tactile
sensing and active perception.

</details>


### [458] [CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems](https://arxiv.org/abs/2506.06381)
*Trisanth Srinivasan,Santosh Patapati,Himani Musku,Idhant Gode,Aditya Arora,Samvit Bhattacharya,Abubakr Nazriev,Sanika Hirave,Zaryab Kanjiani,Srinjoy Ghose,Srinidhi Shetty*

Main category: cs.RO

TL;DR: CPS-Guard is a new framework using multi-role orchestration to automate assurance process for AI-powered CPS, showing effectiveness in detecting vulnerabilities and supporting recovery strategies through a autonomous vehicle case study.


<details>
  <summary>Details</summary>
Motivation: Traditional verification and validation methods are insufficient for handling unpredictable and dynamic AI components in critical applications of Cyber-Physical Systems.

Method: Introduced CPS-Guard framework which uses multi-role orchestration including safety monitoring, security assessment, fault injection, and recovery planning within a simulated environment to evaluate and refine AI behavior against dependability requirements.

Result: The framework successfully detected vulnerabilities, managed performance impacts, and supported adaptive recovery strategies in the autonomous vehicle case study.

Conclusion: CPS-Guard offers a structured and extensible solution for rigorous V&V in safety- and security-critical systems.

Abstract: Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to
operate in critical applications. However, traditional verification and
validation methods often struggle to handle the unpredictable and dynamic
nature of AI components. In this paper, we introduce CPS-Guard, a novel
framework that employs multi-role orchestration to automate the iterative
assurance process for AI-powered CPS. By assigning specialized roles (e.g.,
safety monitoring, security assessment, fault injection, and recovery planning)
to dedicated agents within a simulated environment, CPS-Guard continuously
evaluates and refines AI behavior against a range of dependability
requirements. We demonstrate the framework through a case study involving an
autonomous vehicle navigating an intersection with an AI-based planner. Our
results show that CPS-Guard effectively detects vulnerabilities, manages
performance impacts, and supports adaptive recovery strategies, thereby
offering a structured and extensible solution for rigorous V&V in safety- and
security-critical systems.

</details>


### [459] [Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception](https://arxiv.org/abs/2506.06474)
*Everett Richards,Bipul Thapa,Lena Mashayekhy*

Main category: cs.RO

TL;DR: ECOD框架通过边缘计算和多CAV协作进行实时、多视角物体检测，包含PACE和VOTE两个算法，实验表明其物体分类准确率比传统方法高75%，且保证低延迟。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要精确可靠的物体检测来保障安全和效率，但传统的车载感知系统因遮挡和盲点限制了准确性，基于云端的解决方案又因延迟问题不适用于实时处理需求。

Method: 引入了Edge-Enabled Collaborative Object Detection（ECOD）框架，该框架利用边缘计算和多CAV协作进行实时、多视角物体检测。其中包含两个关键算法：Perceptive Aggregation and Collaborative Estimation（PACE）和Variable Object Tally and Evaluation（VOTE）。前者在边缘服务器上聚合多个CAV的检测数据以增强感知能力，后者使用基于共识的投票机制来提高物体分类的准确性。

Result: 实验结果表明，与传统的单视角车载方法相比，ECOD框架在物体分类准确性方面提高了多达75%，同时确保了低延迟的边缘驱动实时处理。

Conclusion: 研究表明，边缘计算有潜力增强对延迟敏感的自主系统的协作感知能力。

Abstract: Accurate and reliable object detection is critical for ensuring the safety
and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board
perception systems have limited accuracy due to occlusions and blind spots,
while cloud-based solutions introduce significant latency, making them
unsuitable for real-time processing demands required for autonomous driving in
dynamic environments. To address these challenges, we introduce an innovative
framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that
leverages edge computing and multi-CAV collaboration for real-time,
multi-perspective object detection. Our ECOD framework integrates two key
algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and
Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data
from multiple CAVs on an edge server to enhance perception in scenarios where
individual CAVs have limited visibility. VOTE utilizes a consensus-based voting
mechanism to improve the accuracy of object classification by integrating data
from multiple CAVs. Both algorithms are designed at the edge to operate in
real-time, ensuring low-latency and reliable decision-making for CAVs. We
develop a hardware-based controlled testbed consisting of camera-equipped
robotic CAVs and an edge server to evaluate the efficacy of our framework. Our
experimental results demonstrate the significant benefits of ECOD in terms of
improved object classification accuracy, outperforming traditional
single-perspective onboard approaches by up to 75%, while ensuring low-latency,
edge-driven real-time processing. This research highlights the potential of
edge computing to enhance collaborative perception for latency-sensitive
autonomous systems.

</details>


### [460] [Active Test-time Vision-Language Navigation](https://arxiv.org/abs/2506.06630)
*Heeju Ko,Sungjune Kim,Gyeongrok Oh,Jeongyoon Yoon,Honglak Lee,Sujin Jang,Seungryong Kim,Sangpil Kim*

Main category: cs.RO

TL;DR: ATENA is a active learning framework that improves uncertainty calibration and decision-making for VLN policies in unfamiliar environments.


<details>
  <summary>Details</summary>
Motivation: VLN policies trained on offline datasets often show degraded performance when deployed in new environments without external interaction or feedback. Entropy minimization can reduce prediction uncertainty but may suffer from accumulated errors due to overconfidence in incorrect actions.

Method: Introduced ATENA, which uses episodic feedback to adjust certainty levels in successful and failed episodes, improving uncertainty calibration. Proposed mixture entropy optimization that controls prediction confidence and action preference by combining action and pseudo-expert distributions. Also proposed a self-active learning strategy enabling agents to evaluate navigation outcomes based on confident predictions.

Result: Extensive evaluations on challenging VLN benchmarks (REVERIE, R2R, and R2R-CE) demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming baseline methods across various settings.

Conclusion: ATENA enhances uncertainty calibration and decision-making for VLN policies in unfamiliar environments via active learning and mixture entropy optimization.

Abstract: Vision-Language Navigation (VLN) policies trained on offline datasets often
exhibit degraded task performance when deployed in unfamiliar navigation
environments at test time, where agents are typically evaluated without access
to external interaction or feedback. Entropy minimization has emerged as a
practical solution for reducing prediction uncertainty at test time; however,
it can suffer from accumulated errors, as agents may become overconfident in
incorrect actions without sufficient contextual grounding. To tackle these
challenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time
active learning framework that enables a practical human-robot interaction via
episodic feedback on uncertain navigation outcomes. In particular, ATENA learns
to increase certainty in successful episodes and decrease it in failed ones,
improving uncertainty calibration. Here, we propose mixture entropy
optimization, where entropy is obtained from a combination of the action and
pseudo-expert distributions-a hypothetical action distribution assuming the
agent's selected action to be optimal-controlling both prediction confidence
and action preference. In addition, we propose a self-active learning strategy
that enables an agent to evaluate its navigation outcomes based on confident
predictions. As a result, the agent stays actively engaged throughout all
iterations, leading to well-grounded and adaptive decision-making. Extensive
evaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate
that ATENA successfully overcomes distributional shifts at test time,
outperforming the compared baseline methods across various settings.

</details>


### [461] [Self-Adapting Improvement Loops for Robotic Learning](https://arxiv.org/abs/2506.06658)
*Calvin Luo,Zilai Zeng,Mingxi Jia,Yilun Du,Chen Sun*

Main category: cs.RO

TL;DR: The paper proposes Self-Adapting Improvement Loop (SAIL), which uses an in-domain video model to iteratively update itself on self-produced trajectories, improving performance for novel tasks. SAIL is robust and effective even with filtering or varying quality of initial demonstrations.


<details>
  <summary>Details</summary>
Motivation: Current video generative models struggle with generalizing to unseen robotic tasks. Leveraging learned prior knowledge from additional offline data sources and designing agents that can continuously improve online could address this issue.

Method: Propose SAIL where an in-domain video model updates itself using self-produced trajectories collected through adaptation with a pretrained internet-scale video model, enhancing task-specific performance over iterations.

Result: SAIL leads to continuous performance improvements across multiple iterations on unseen tasks, both in simulation (MetaWorld tasks) and real-world scenarios (manipulation tasks on a robot arm). It shows robustness to filtering and quality variations of initial demonstrations.

Conclusion: SAIL demonstrates a method to iteratively enhance a high-performance video model for solving new robotic tasks via self-improvement, combining adaptation with summarized large-scale data and learning from online experience.

Abstract: Video generative models trained on expert demonstrations have been utilized
as performant text-conditioned visual planners for solving robotic tasks.
However, generalization to unseen tasks remains a challenge. Whereas improved
generalization may be facilitated by leveraging learned prior knowledge from
additional pre-collected offline data sources, such as web-scale video
datasets, in the era of experience we aim to design agents that can
continuously improve in an online manner from self-collected behaviors. In this
work we thus propose the Self-Adapting Improvement Loop (SAIL), where an
in-domain video model iteratively updates itself on self-produced trajectories,
collected through adaptation with an internet-scale pretrained video model, and
steadily improves its performance for a specified task of interest. We apply
SAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks
on a real robot arm, and find that performance improvements continuously emerge
over multiple iterations for novel tasks initially unseen during original
in-domain video model training. Furthermore, we discover that SAIL is
surprisingly robust regarding if and how the self-collected experience is
filtered, and the quality of the initial in-domain demonstrations. Through
adaptation with summarized internet-scale data, and learning through online
experience, we thus demonstrate a way to iteratively bootstrap a
high-performance video model for solving novel robotic tasks through
self-improvement.

</details>


### [462] [DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning](https://arxiv.org/abs/2506.06659)
*Wenhao Yao,Zhenxin Li,Shiyi Lan,Zi Wang,Xinglong Sun,Jose M. Alvarez,Zuxuan Wu*

Main category: cs.RO

TL;DR: DriveSuprim is a new method for autonomous vehicle navigation that improves safety and performance through a coarse-to-fine filtering process, rotation-based augmentation, and self-distillation framework.


<details>
  <summary>Details</summary>
Motivation: Current selection-based methods for autonomous vehicle trajectory planning face challenges in selecting the best option from thousands of possibilities and distinguishing subtle but safety-critical differences, especially in rare scenarios.

Method: DriveSuprim employs a coarse-to-fine paradigm for progressive candidate filtering, a rotation-based augmentation method to improve robustness in out-of-distribution scenarios, and a self-distillation framework to stabilize training.

Result: DriveSuprim achieves state-of-the-art performance with 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2 without extra data, showing superior safety-critical capabilities including collision avoidance and compliance with rules.

Conclusion: DriveSuprim advances the selection-based paradigm and demonstrates superior safety-critical capabilities while maintaining high trajectory quality in various driving scenarios.

Abstract: In complex driving environments, autonomous vehicles must navigate safely.
Relying on a single predicted path, as in regression-based approaches, usually
does not explicitly assess the safety of the predicted trajectory.
Selection-based methods address this by generating and scoring multiple
trajectory candidates and predicting the safety score for each, but face
optimization challenges in precisely selecting the best option from thousands
of possibilities and distinguishing subtle but safety-critical differences,
especially in rare or underrepresented scenarios. We propose DriveSuprim to
overcome these challenges and advance the selection-based paradigm through a
coarse-to-fine paradigm for progressive candidate filtering, a rotation-based
augmentation method to improve robustness in out-of-distribution scenarios, and
a self-distillation framework to stabilize training. DriveSuprim achieves
state-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS
in NAVSIM v2 without extra data, demonstrating superior safetycritical
capabilities, including collision avoidance and compliance with rules, while
maintaining high trajectory quality in various driving scenarios.

</details>


### [463] [RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks](https://arxiv.org/abs/2506.06683)
*Shiying Duan,Pei Ren,Nanxiang Jiang,Zhengping Che,Jian Tang,Yifan Sun,Zhaoxin Fan,Wenjun Wu*

Main category: cs.RO

TL;DR: Dual-arm robots' efficiency and flexibility are enhanced by optimizing task parallelism through RoboPARA, a LLM-driven framework. It uses two stages for planning and introduces X-DAPT dataset. Experiments show it outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods in dual-arm robot task planning have not fully optimized task parallelism, thus limiting the potential of dual-arm collaboration.

Method: RoboPARA is proposed with a two-stage process: 1) Dependency Graph-based Planning Candidates Generation which constructs DAGs to model task dependencies and eliminate redundancy; 2) Graph Re-Traversal-based Dual-Arm Parallel Planning which optimizes DAG traversal for maximum parallelism while maintaining task coherence.

Result: Extensive experiments on the newly introduced X-DAPT dataset demonstrate that RoboPARA significantly outperforms existing methods in terms of efficiency and reliability, especially in complex task combinations.

Conclusion: RoboPARA offers an advanced solution for dual-arm task parallelism planning, improving efficiency and reliability. The associated code and dataset will be released upon acceptance.

Abstract: Dual-arm robots play a crucial role in improving efficiency and flexibility
in complex multitasking scenarios. While existing methods have achieved
promising results in task planning, they often fail to fully optimize task
parallelism, limiting the potential of dual-arm collaboration. To address this
issue, we propose RoboPARA, a novel large language model (LLM)-driven framework
for dual-arm task parallelism planning. RoboPARA employs a two-stage process:
(1) Dependency Graph-based Planning Candidates Generation, which constructs
directed acyclic graphs (DAGs) to model task dependencies and eliminate
redundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which
optimizes DAG traversal to maximize parallelism while maintaining task
coherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task
dataset (X-DAPT dataset), the first dataset specifically designed to evaluate
dual-arm task parallelism across diverse scenarios and difficulty levels.
Extensive experiments on the X-DAPT dataset demonstrate that RoboPARA
significantly outperforms existing methods, achieving higher efficiency and
reliability, particularly in complex task combinations. The code and dataset
will be released upon acceptance.

</details>


### [464] [Multimodal Spatial Language Maps for Robot Navigation and Manipulation](https://arxiv.org/abs/2506.06862)
*Chenguang Huang,Oier Mees,Andy Zeng,Wolfram Burgard*

Main category: cs.RO

TL;DR: This paper proposes multimodal spatial language maps that combine pretrained multimodal features with 3D environment reconstructions, enabling robots to understand and execute spatial and multimodal navigation goals.


<details>
  <summary>Details</summary>
Motivation: Existing methods for grounding language in a navigating agent's observations either lack connection to environment mapping, spatial precision of geometric maps, or neglect additional modalities beyond vision.

Method: The authors propose multimodal spatial language maps which fuse pretrained multimodal features with a 3D reconstruction of the environment. They present two instances: visual-language maps (VLMaps) and audio-visual-language maps (AVLMaps). These maps are built autonomously using standard exploration and can be combined with large language models (LLMs) for enhanced capabilities.

Result: Experiments in simulation and real-world settings show that the proposed multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation, improving recall by 50% in ambiguous scenarios. The approach is applicable to mobile robots and tabletop manipulators.

Conclusion: Multimodal spatial language maps provide a unified representation integrating audio, visual, and language cues, allowing robots to ground multimodal goal queries to spatial locations for navigation, significantly enhancing goal disambiguation in ambiguous environments.

Abstract: Grounding language to a navigating agent's observations can leverage
pretrained multimodal foundation models to match perceptions to object or event
descriptions. However, previous approaches remain disconnected from environment
mapping, lack the spatial precision of geometric maps, or neglect additional
modality information beyond vision. To address this, we propose multimodal
spatial language maps as a spatial map representation that fuses pretrained
multimodal features with a 3D reconstruction of the environment. We build these
maps autonomously using standard exploration. We present two instances of our
maps, which are visual-language maps (VLMaps) and their extension to
audio-visual-language maps (AVLMaps) obtained by adding audio information. When
combined with large language models (LLMs), VLMaps can (i) translate natural
language commands into open-vocabulary spatial goals (e.g., "in between the
sofa and TV") directly localized in the map, and (ii) be shared across
different robot embodiments to generate tailored obstacle maps on demand.
Building upon the capabilities above, AVLMaps extend VLMaps by introducing a
unified 3D spatial representation integrating audio, visual, and language cues
through the fusion of features from pretrained multimodal foundation models.
This enables robots to ground multimodal goal queries (e.g., text, images, or
audio snippets) to spatial locations for navigation. Additionally, the
incorporation of diverse sensory inputs significantly enhances goal
disambiguation in ambiguous environments. Experiments in simulation and
real-world settings demonstrate that our multimodal spatial language maps
enable zero-shot spatial and multimodal goal navigation and improve recall by
50% in ambiguous scenarios. These capabilities extend to mobile robots and
tabletop manipulators, supporting navigation and interaction guided by visual,
audio, and spatial cues.

</details>


### [465] [CARoL: Context-aware Adaptation for Robot Learning](https://arxiv.org/abs/2506.07006)
*Zechen Hu,Tong Xu,Xuesu Xiao,Xuan Wang*

Main category: cs.RO

TL;DR: CARoL is a framework that uses context-aware adaptation to efficiently learn new robotic tasks from prior knowledge, demonstrating faster convergence and higher rewards in simulations and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning (RL) can be inefficient when learning new robotic tasks from scratch, but leveraging prior knowledge could significantly enhance learning efficiency. The challenges lie in determining the relevancy of existing knowledge and adaptively integrating them into learning a new task.

Method: The proposed Context-aware Adaptation for Robot Learning (CARoL) framework incorporates context awareness by analyzing state transitions in system dynamics to identify similarities between the new task and prior knowledge. It then prioritizes and adapts specific knowledge pieces for the new task. CARoL is applicable to policy-based, value-based, and actor-critic RL algorithms.

Result: In simulations (CarRacing and LunarLander environments), CARoL shows faster convergence and higher rewards when learning policies for new tasks. In real-world experiments, it enables a ground vehicle to quickly and efficiently adapt policies learned in simulation to traverse real-world off-road terrain.

Conclusion: CARoL offers an efficient method to leverage prior knowledge for learning similar but distinct new tasks in robotics, proving its efficiency and generalizability on both simulated robotic platforms and physical ground vehicles.

Abstract: Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is
often inefficient. Leveraging prior knowledge has the potential to
significantly enhance learning efficiency, which, however, raises two critical
challenges: how to determine the relevancy of existing knowledge and how to
adaptively integrate them into learning a new task. In this paper, we propose
Context-aware Adaptation for Robot Learning (CARoL), a novel framework to
efficiently learn a similar but distinct new task from prior knowledge. CARoL
incorporates context awareness by analyzing state transitions in system
dynamics to identify similarities between the new task and prior knowledge. It
then utilizes these identified similarities to prioritize and adapt specific
knowledge pieces for the new task. Additionally, CARoL has a broad
applicability spanning policy-based, value-based, and actor-critic RL
algorithms. We validate the efficiency and generalizability of CARoL on both
simulated robotic platforms and physical ground vehicles. The simulations
include CarRacing and LunarLander environments, where CARoL demonstrates faster
convergence and higher rewards when learning policies for new tasks. In
real-world experiments, we show that CARoL enables a ground vehicle to quickly
and efficiently adapt policies learned in simulation to smoothly traverse
real-world off-road terrain.

</details>


### [466] [Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search](https://arxiv.org/abs/2506.07062)
*Dongryung Lee,Sejune Joo,Kimin Lee,Beomjoon Kim*

Main category: cs.RO

TL;DR: The paper proposes a method to solve G-TAMP problems by leveraging Large Language Models (LLMs) with geometric reasoning, using MCTS in a hybrid action space and warm-starting it with LLM's task plan. The approach outperforms previous LLM planners and pure search algorithms on six different G-TAMP problems.


<details>
  <summary>Details</summary>
Motivation: To address the computational resource and data demands of traditional approaches to G-TAMP problems, the authors draw inspiration from human common sense knowledge to explore the use of LLMs for guiding task planning in these problems.

Method: The method involves designing a predicate-based prompt for LLMs to perform geometric reasoning, querying the LLM to generate a task plan, and extending MCTS to a hybrid action space to utilize the LLM's output for guiding the search. The LLM is used to warm-start the MCTS with nodes explored in completing its task plan, reducing computational costs compared to calling the LLM at every node.

Result: The proposed method outperforms previous LLM planners and pure search algorithms on six different G-TAMP problems.

Conclusion: The integration of LLMs with geometric reasoning and MCTS in a hybrid action space provides an effective solution to G-TAMP problems, demonstrating superior performance over existing methods.

Abstract: The problem of relocating a set of objects to designated areas amidst movable
obstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)
problem, a subclass of task and motion planning (TAMP). Traditional approaches
to G-TAMP have relied either on domain-independent heuristics or on learning
from planning experience to guide the search, both of which typically demand
significant computational resources or data. In contrast, humans often use
common sense to intuitively decide which objects to manipulate in G-TAMP
problems. Inspired by this, we propose leveraging Large Language Models (LLMs),
which have common sense knowledge acquired from internet-scale data, to guide
task planning in G-TAMP problems. To enable LLMs to perform geometric
reasoning, we design a predicate-based prompt that encodes geometric
information derived from a motion planning algorithm. We then query the LLM to
generate a task plan, which is then used to search for a feasible set of
continuous parameters. Since LLMs are prone to mistakes, instead of committing
to LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action
space and use the LLM to guide the search. Unlike the previous approach that
calls an LLM at every node and incurs high computational costs, we use it to
warm-start the MCTS with the nodes explored in completing the LLM's task plan.
On six different G-TAMP problems, we show our method outperforms previous LLM
planners and pure search algorithms. Code can be found at:
https://github.com/iMSquared/prime-the-search

</details>


### [467] [Robotic Policy Learning via Human-assisted Action Preference Optimization](https://arxiv.org/abs/2506.07127)
*Wenke xia,Yichu Yang,Hongtao Wu,Xiao Ma,Tao Kong,Di Hu*

Main category: cs.RO

TL;DR: The paper presents HAPO, a Human-assisted Action Preference Optimization method for Vision-Language-Action (VLA) models in robotics. HAPO improves failure correction and adaptation through human intervention and preference alignment, utilizing an adaptive reweighting algorithm. Experiments show enhanced generalization and robustness in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models used in robotic systems rely heavily on expert demonstrations, which limits their ability to learn from and correct failures effectively.

Method: HAPO introduces a human-robot collaboration framework that collects interaction trajectories via human intervention to correct failures. An adaptive reweighting algorithm is proposed to address challenges in incorporating preference optimization into VLA models, allowing learning from binary desirability signals.

Result: Experiments conducted both in simulation and real-world settings demonstrate the superior generalization and robustness of the HAPO framework across different manipulation tasks.

Conclusion: HAPO ensures reliable deployment and effective learning from failures for VLA models by integrating human-assisted action preference optimization.

Abstract: Establishing a reliable and iteratively refined robotic system is essential
for deploying real-world applications. While Vision-Language-Action (VLA)
models are widely recognized as the foundation model for such robotic
deployment, their dependence on expert demonstrations hinders the crucial
capabilities of correction and learning from failures. To mitigate this
limitation, we introduce a Human-assisted Action Preference Optimization method
named HAPO, designed to correct deployment failures and foster effective
adaptation through preference alignment for VLA models. This method begins with
a human-robot collaboration framework for reliable failure correction and
interaction trajectory collection through human intervention. These
human-intervention trajectories are further employed within the action
preference optimization process, facilitating VLA models to mitigate failure
action occurrences while enhancing corrective action adaptation. Specifically,
we propose an adaptive reweighting algorithm to address the issues of
irreversible interactions and token probability mismatch when introducing
preference optimization into VLA models, facilitating model learning from
binary desirability signals derived from interactions. Through combining these
modules, our human-assisted action preference optimization method ensures
reliable deployment and effective learning from failure for VLA models. The
experiments conducted in simulation and real-world scenarios prove superior
generalization and robustness of our framework across a variety of manipulation
tasks.

</details>


### [468] [Real-Time Execution of Action Chunking Flow Policies](https://arxiv.org/abs/2506.07339)
*Kevin Black,Manuel Y. Galliker,Sergey Levine*

Main category: cs.RO

TL;DR: 现代AI系统，特别是与物理世界交互的系统，越来越需要实时性能。本文提出了一种新的推理时算法RTC，可实现平滑异步执行动作块策略，有效解决高延迟问题，并在多项动态任务中进行了验证。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的通用模型，包括最近的视觉-语言动作模型（VLAs），其高延迟对实时性能提出了重大挑战。尽管动作分块有助于高频控制任务中的时间一致性，但未能完全解决延迟问题，导致分块边界处出现停顿或不连贯的动作。

Method: 提出一种名为实时分块（RTC）的新颖推理时算法，该方法适用于任何扩散或流式VLA，无需重新训练。它通过在执行当前动作块的同时生成下一个动作块，“冻结”确定要执行的动作并“修复”其余部分，从而实现平滑异步执行。

Result: 通过在Kinetix模拟器中引入12个高度动态任务的新基准测试，以及评估6个具有挑战性的现实世界双臂操作任务，结果表明RTC快速、高效且对推理延迟具有独特鲁棒性，显著提高了任务吞吐量，并在存在显著延迟的情况下实现了高成功率。

Conclusion: RTC算法有效解决了动作分块中的延迟问题，提升了实时性能，尤其在精确任务中表现出色，例如在存在显著延迟的情况下成功点燃火柴。

Abstract: Modern AI systems, especially those interacting with the physical world,
increasingly require real-time performance. However, the high latency of
state-of-the-art generalist models, including recent vision-language action
models (VLAs), poses a significant challenge. While action chunking has enabled
temporal consistency in high-frequency control tasks, it does not fully address
the latency problem, leading to pauses or out-of-distribution jerky movements
at chunk boundaries. This paper presents a novel inference-time algorithm that
enables smooth asynchronous execution of action chunking policies. Our method,
real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out
of the box with no re-training. It generates the next action chunk while
executing the current one, "freezing" actions guaranteed to execute and
"inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly
dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging
real-world bimanual manipulation tasks. Results demonstrate that RTC is fast,
performant, and uniquely robust to inference delay, significantly improving
task throughput and enabling high success rates in precise tasks
$\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the
presence of significant latency. See
https://pi.website/research/real_time_chunking for videos.

</details>


### [469] [Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers](https://arxiv.org/abs/2506.07271)
*Hikaru Sawafuji,Ryota Ozaki,Takuto Motomura,Toyohisa Matsuda,Masanori Tojima,Kento Uchida,Shinichi Shirakawa*

Main category: cs.RO

TL;DR: This paper presents a machine learning-based self-localization method for bulldozers which doesn't depend on RTK-GNSS and uses an Extended Kalman Filter to improve localization accuracy.


<details>
  <summary>Details</summary>
Motivation: Self-localization is crucial for automating bulldozers, but RTK-GNSS signals can be lost in certain mining conditions. Thus, there's a need for self-localization methods that don't rely on RTK-GNSS.

Method: The proposed method involves estimating local velocities using a machine learning model from internal sensors, then incorporating these estimates into an Extended Kalman Filter (EKF) for global localization.

Result: The method effectively suppressed the accumulation of position errors compared to kinematics-based methods, particularly when slip occurred. Bulldozer-specific sensors improved self-localization accuracy.

Conclusion: A machine learning-based self-localization method for bulldozers was successfully developed and tested, demonstrating superior performance over kinematics-based methods.

Abstract: Self-localization is an important technology for automating bulldozers.
Conventional bulldozer self-localization systems rely on RTK-GNSS (Real Time
Kinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are
sometimes lost in certain mining conditions. Therefore, self-localization
methods that do not depend on RTK-GNSS are required. In this paper, we propose
a machine learning-based self-localization method for bulldozers. The proposed
method consists of two steps: estimating local velocities using a machine
learning model from internal sensors, and incorporating these estimates into an
Extended Kalman Filter (EKF) for global localization. We also created a novel
dataset for bulldozer odometry and conducted experiments across various driving
scenarios, including slalom, excavation, and driving on slopes. The result
demonstrated that the proposed self-localization method suppressed the
accumulation of position errors compared to kinematics-based methods,
especially when slip occurred. Furthermore, this study showed that
bulldozer-specific sensors, such as blade position sensors and hydraulic
pressure sensors, contributed to improving self-localization accuracy.

</details>


### [470] [Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs](https://arxiv.org/abs/2506.07454)
*Jared Strader,Aaron Ray,Jacob Arkin,Mason B. Peterson,Yun Chang,Nathan Hughes,Christopher Bradley,Yi Xuan Jia,Carlos Nieto-Granda,Rajat Talak,Chuchu Fan,Luca Carlone,Jonathan P. How,Nicholas Roy*

Main category: cs.RO

TL;DR: This paper presents a multi-robot system that uses 3D scene graphs for mapping, localization, and task/motion planning, enabling execution of complex natural language instructions. It also introduces a planning method using LLMs to translate operator intent into PDDL goals.


<details>
  <summary>Details</summary>
Motivation: To create a multi-robot system capable of executing complex tasks specified in natural language by integrating mapping, localization, and task/motion planning functionalities through the use of 3D scene graphs.

Method: Developed a multi-robot system building shared 3D scene graphs incorporating open-set object-based maps for relocalization and planning. Used a planning approach leveraging Large Language Models (LLMs) to translate operator intent into PDDL goals based on context from the 3D scene graph and robot capabilities.

Result: The experimental assessment shows the system's performance in real-world, large-scale outdoor environments.

Conclusion: The introduced multi-robot system successfully integrates mapping, localization, and TAMP via 3D scene graphs, allowing complex task execution from natural language instructions.

Abstract: In this paper, we introduce a multi-robot system that integrates mapping,
localization, and task and motion planning (TAMP) enabled by 3D scene graphs to
execute complex instructions expressed in natural language. Our system builds a
shared 3D scene graph incorporating an open-set object-based map, which is
leveraged for multi-robot 3D scene graph fusion. This representation supports
real-time, view-invariant relocalization (via the object-based map) and
planning (via the 3D scene graph), allowing a team of robots to reason about
their surroundings and execute complex tasks. Additionally, we introduce a
planning approach that translates operator intent into Planning Domain
Definition Language (PDDL) goals using a Large Language Model (LLM) by
leveraging context from the shared 3D scene graph and robot capabilities. We
provide an experimental assessment of the performance of our system on
real-world tasks in large-scale, outdoor environments.

</details>


### [471] [BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models](https://arxiv.org/abs/2506.07961)
*Peiyan Li,Yixiang Chen,Hongtao Wu,Xiao Ma,Xiangnan Wu,Yan Huang,Liang Wang,Tao Kong,Tieniu Tan*

Main category: cs.RO

TL;DR: BridgeVLA is a novel 3D vision-language-action (VLA) model that efficiently incorporates 3D signals into pre-trained vision-language models (VLMs) through projecting 3D inputs to 2D images and utilizing 2D heatmaps for action prediction. It demonstrates superior performance in simulation benchmarks and real-robot experiments, with remarkable sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods of incorporating 3D signals into VLMs for action prediction do not fully leverage the spatial structure inherent in 3D data, resulting in low sample efficiency.

Method: The BridgeVLA model projects 3D inputs to multiple 2D images for alignment with the VLM backbone and uses 2D heatmaps for action prediction, unifying input and output spaces within a consistent 2D image space. A scalable pre-training method is proposed to equip the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning.

Result: BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks: RLBench, COLOSSEUM, and GemBench. In real-robot experiments, it achieves an average success rate improvement of 32% over the best baseline, generalizes robustly in out-of-distribution settings, and achieves a success rate of 96.8% on 10+ tasks with only 3 trajectories per task.

Conclusion: BridgeVLA effectively leverages 3D data for robot manipulation learning, demonstrating superior performance and sample efficiency compared to existing methods.

Abstract: Recently, leveraging pre-trained vision-language models (VLMs) for building
vision-language-action (VLA) models has emerged as a promising approach to
effective robot manipulation learning. However, only few methods incorporate 3D
signals into VLMs for action prediction, and they do not fully leverage the
spatial structure inherent in 3D data, leading to low sample efficiency. In
this paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D
inputs to multiple 2D images, ensuring input alignment with the VLM backbone,
and (2) utilizes 2D heatmaps for action prediction, unifying the input and
output spaces within a consistent 2D image space. In addition, we propose a
scalable pre-training method that equips the VLM backbone with the capability
to predict 2D heatmaps before downstream policy learning. Extensive experiments
show the proposed method is able to learn 3D manipulation efficiently and
effectively. BridgeVLA outperforms state-of-the-art baseline methods across
three simulation benchmarks. In RLBench, it improves the average success rate
from 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better
performance in challenging generalization settings, boosting the average
success rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing
baseline methods in terms of average success rate. In real-robot experiments,
BridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It
generalizes robustly in multiple out-of-distribution settings, including visual
disturbances and unseen instructions. Remarkably, it is able to achieve a
success rate of 96.8% on 10+ tasks with only 3 trajectories per task,
highlighting its extraordinary sample efficiency. Project
Website:https://bridgevla.github.io/

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [472] [HyColor: An Efficient Heuristic Algorithm for Graph Coloring](https://arxiv.org/abs/2506.07373)
*Enqiang Zhu,Yu Zhang,Haopeng Sun,Ziqi Wei,Witold Pedrycz,Chanjuan Liu,Jin Xu*

Main category: cs.DM

TL;DR: The paper introduces HyColor, an efficient hybrid heuristic algorithm for solving the graph coloring problem (GCP), which is particularly effective for large-scale sparse graphs and small dense graphs. It incorporates a local decision strategy, a graph-reduction strategy, and a k-core and mixed degree-based greedy heuristic. Evaluated against three state-of-the-art algorithms across 209 instances, HyColor consistently outperforms existing methods in solution accuracy and computational efficiency, achieving the best solutions in 194 instances (over 93%) and optimal coloring in 128 instances.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing GCP algorithms that focus on either small hard graphs or large-scale sparse graphs, by proposing a more versatile and efficient algorithm for handling both types of graphs.

Method: HyColor uses three key strategies: a local decision strategy to improve the lower bound on the chromatic number, a graph-reduction strategy to reduce the working graph, and a k-core and mixed degree-based greedy heuristic for efficiently coloring graphs.

Result: HyColor outperformed three state-of-the-art GCP algorithms in both solution accuracy and computational efficiency for the majority of instances tested. It achieved the best solutions in 194 instances (over 93%), with 34 of these solutions significantly surpassing those of other algorithms. Additionally, it successfully determined the chromatic number and achieved optimal coloring in 128 instances.

Conclusion: HyColor is an efficient hybrid heuristic algorithm for GCP that excels in handling large-scale sparse graphs while also achieving impressive results on small dense graphs. It demonstrates superior performance compared to existing algorithms in terms of solution quality and computational efficiency.

Abstract: The graph coloring problem (GCP) is a classic combinatorial optimization
problem that aims to find the minimum number of colors assigned to vertices of
a graph such that no two adjacent vertices receive the same color. GCP has been
extensively studied by researchers from various fields, including mathematics,
computer science, and biological science. Due to the NP-hard nature, many
heuristic algorithms have been proposed to solve GCP. However, existing GCP
algorithms focus on either small hard graphs or large-scale sparse graphs (with
up to 10^7 vertices). This paper presents an efficient hybrid heuristic
algorithm for GCP, named HyColor, which excels in handling large-scale sparse
graphs while achieving impressive results on small dense graphs. The efficiency
of HyColor comes from the following three aspects: a local decision strategy to
improve the lower bound on the chromatic number; a graph-reduction strategy to
reduce the working graph; and a k-core and mixed degree-based greedy heuristic
for efficiently coloring graphs. HyColor is evaluated against three
state-of-the-art GCP algorithms across four benchmarks, comprising three
large-scale sparse graph benchmarks and one small dense graph benchmark,
totaling 209 instances. The results demonstrate that HyColor consistently
outperforms existing heuristic algorithms in both solution accuracy and
computational efficiency for the majority of instances. Notably, HyColor
achieved the best solutions in 194 instances (over 93%), with 34 of these
solutions significantly surpassing those of other algorithms. Furthermore,
HyColor successfully determined the chromatic number and achieved optimal
coloring in 128 instances.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [473] [GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval](https://arxiv.org/abs/2506.04762)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: GOLFer is a new method for query expansion that uses smaller LMs and includes a hallucination filter and documents combiner, showing effectiveness and competitiveness against methods using large-size LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based query expansion methods are costly, computationally intensive, and have limited accessibility due to their reliance on large, advanced LLMs.

Method: GOLFer consists of two modules: a hallucination filter that detects and removes non-factual/inconsistent sentences in generated documents, and a documents combiner that combines filtered content with the query using a weight vector.

Result: GOLFer outperforms other methods using smaller LMs and maintains competitive performance against methods using large-size LLMs across three web search and ten low-resource datasets.

Conclusion: GOLFer demonstrates its effectiveness by consistently outperforming other methods using smaller LMs and maintaining competitive performance against methods using large-size LLMs.

Abstract: Large language models (LLMs)-based query expansion for information retrieval
augments queries with generated hypothetical documents with LLMs. However, its
performance relies heavily on the scale of the language models (LMs),
necessitating larger, more advanced LLMs. This approach is costly,
computationally intensive, and often has limited accessibility. To address
these limitations, we introduce GOLFer - Smaller LMs-Generated Documents
Hallucination Filter & Combiner - a novel method leveraging smaller open-source
LMs for query expansion. GOLFer comprises two modules: a hallucination filter
and a documents combiner. The former detects and removes non-factual and
inconsistent sentences in generated documents, a common issue with smaller LMs,
while the latter combines the filtered content with the query using a weight
vector to balance their influence. We evaluate GOLFer alongside dominant
LLM-based query expansion methods on three web search and ten low-resource
datasets. Experimental results demonstrate that GOLFer consistently outperforms
other methods using smaller LMs, and maintains competitive performance against
methods using large-size LLMs, demonstrating its effectiveness.

</details>


### [474] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: The paper introduces DISRetrieval, a novel hierarchical retrieval framework leveraging linguistic discourse structure for long document understanding. It presents three key innovations and demonstrates substantial improvements over existing methods in token-level retrieval metrics and downstream question answering tasks.


<details>
  <summary>Details</summary>
Motivation: Long document understanding is crucial in natural language processing, but existing approaches fail to capture the inherent discourse structure that guides human comprehension.

Method: DISRetrieval utilizes rhetorical structure theory (RST) to create sentence-level hierarchical representations, combines discourse structure with adaptive summarization to enrich tree nodes with contextual information, and employs a hierarchical evidence retrieval mechanism that maintains discourse coherence.

Result: DISRetrieval shows significant improvements over existing methods in both token-level retrieval metrics and downstream question answering tasks on QASPER and QuALITY datasets. Ablation studies confirm the effectiveness of incorporating discourse structure.

Conclusion: The importance of linguistically-informed document representation in long-text understanding is validated. The code and datasets are publicly available to facilitate future research.

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [475] [A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing](https://arxiv.org/abs/2506.06316)
*Haoyang Feng,Yanjun Dai,Yuan Gao*

Main category: cs.IR

TL;DR: The paper proposes RL-LLM-ABTest framework that uses reinforcement learning and LLM for personalized A/B testing in marketing, showing superiority over existing methods.


<details>
  <summary>Details</summary>
Motivation: To effectively optimize A/B testing in personalized marketing to maximize user response.

Method: RL-LLM-AB test framework which combines reinforcement learning strategy optimization with LLM. It involves generating A/B content versions, embedding user portrait and context, selecting content in real-time through policy optimization, and estimating long-term revenue based on feedback. Also includes a Memory-Augmented Reward Estimator.

Result: Demonstrates superiority of RL-LLM-ABTest over classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches using real-world marketing data.

Conclusion: RL-LLM-ABTest framework is an effective solution for personalized A/B testing in marketing.

Abstract: For personalized marketing, a new challenge of how to effectively algorithm
the A/B testing to maximize user response is urgently to be overcome. In this
paper, we present a new approach, the RL-LLM-AB test framework, for using
reinforcement learning strategy optimization combined with LLM to automate and
personalize A/B tests. The RL-LLM-AB test is built upon the pre-trained
instruction-tuned language model. It first generates A/B versions of candidate
content variants using a Prompt-Conditioned Generator, and then dynamically
embeds and fuses the user portrait and the context of the current query with
the multi-modal perception module to constitute the current interaction state.
The content version is then selected in real-time through the policy
optimization module with an Actor-Critic structure, and long-term revenue is
estimated according to real-time feedback (such as click-through rate and
conversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded
into the framework to capture long-term user preference drift, which helps to
generalize policy across multiple users and content contexts. Numerical results
demonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B
testing methods, including classical A/B testing, Contextual Bandits, and
benchmark reinforcement learning approaches on real-world marketing data.

</details>


### [476] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: In NLP, there's a shift from encoder-only tiny language models to decoder-only large language models (LLMs). However, LLMs have limitations in the financial sector. This paper introduces FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens, which addresses these limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of LLMs in the financial sector, such as worse performance than fine-tuned BERT on discriminative tasks, reliance on RAG methods for generative tasks, and inadequacies in feature-based scenarios like topic modeling.

Method: FinBERT2 is introduced, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. It is used as a better backbone to bridge the gap in the financial-specific deployment of LLMs.

Result: 1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. 2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source and proprietary embedders across five financial retrieval tasks. 3) The Fin-TopicModel, built on FinBERT2 variants, enables superior clustering and topic representation for financial titles.

Conclusion: This work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era.

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [477] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: Retrieval-Augmented Generation (RAG) for Arabic is underexplored. This study evaluates RAG components on Arabic datasets using the RAGAS framework across four metrics. Findings suggest sentence-aware chunking, BGE-M3/Multilingual-E5-large embeddings, bge-reranker-v2-m3, and Aya-8B are optimal for Arabic RAG pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration in optimizing RAG components specifically for Arabic language processing.

Method: Conduct a comprehensive empirical evaluation of state-of-the-art RAG components including chunking strategies, embedding models, rerankers, and language models on diverse Arabic datasets using the RAGAS framework.

Result: Sentence-aware chunking performs best among segmentation methods; BGE-M3 and Multilingual-E5-large are the most effective embedding models; inclusion of bge-reranker-v2-m3 improves faithfulness in complex datasets; Aya-8B surpasses StableLM in generation quality.

Conclusion: The study provides critical insights and practical guidelines for selecting optimal RAG components for building high-quality Arabic RAG pipelines.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [478] [Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support](https://arxiv.org/abs/2506.06340)
*Wu Hao Ran,Xi Xi,Furong Li,Jingyi Lu,Jian Jiang,Hui Huang,Yuzhuan Zhang,Shi Li*

Main category: cs.IR

TL;DR: 大型语言模型（LLMs）为复杂、非结构化数据的分析提供了新途径，特别是在医疗领域。本文探讨了高级语言模型在电子健康记录（EHRs）中的应用，强调文本特征的重要性以及在不同机构间的数据协调，并讨论了医疗编码整合及AI模型在医疗领域的通用性和公平性挑战与机遇。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型来更好地解析和利用电子健康记录中的丰富信息，改进临床决策支持系统。

Method: 通过使用先进的语言模型来处理和分析EHR中的多种数据格式，包括自由文本临床笔记、结构化的实验室结果和诊断代码，提取语义丰富的文本特征，以提高数据分析效果和跨机构数据的一致性。

Result: 展示了如何通过文本特征提供语义丰富的表示形式，帮助在不同机构间实现数据协调，并深入探讨了将医疗编码纳入考虑以及确保AI模型在医疗保健中的通用性和公平性的挑战与机会。

Conclusion: 高级语言模型可以有效提升临床决策支持系统的性能，但需要解决医疗编码整合和AI模型的通用性与公平性问题。

Abstract: The advent of large language models (LLMs) has opened new avenues for
analyzing complex, unstructured data, particularly within the medical domain.
Electronic Health Records (EHRs) contain a wealth of information in various
formats, including free text clinical notes, structured lab results, and
diagnostic codes. This paper explores the application of advanced language
models to leverage these diverse data sources for improved clinical decision
support. We will discuss how text-based features, often overlooked in
traditional high dimensional EHR analysis, can provide semantically rich
representations and aid in harmonizing data across different institutions.
Furthermore, we delve into the challenges and opportunities of incorporating
medical codes and ensuring the generalizability and fairness of AI models in
healthcare.

</details>


### [479] [NR4DER: Neural Re-ranking for Diversified Exercise Recommendation](https://arxiv.org/abs/2506.06341)
*Xinghe Cheng,Xufang Zhou,Liangda Fang,Chaobo He,Yuyu Zhou,Weiqi Luo,Zhiguo Gong,Quanlong Guan*

Main category: cs.IR

TL;DR: The paper proposes NR4DER, a method that uses mLSTM for exercise filtering, sequence enhancement for inactive students, and neural re-ranking for diverse recommendations, outperforming existing methods in real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing exercise recommendation systems have issues with high dropout rates and fail to accommodate the diverse learning pace of students, leading to limited accuracy and diversity in recommendations.

Method: NR4DER leverages mLSTM for improving exercise filter effectiveness, applies sequence enhancement for better representation of inactive students, matches students with suitable difficulty exercises, and uses neural re-ranking to generate diverse recommendation lists based on individual student histories.

Result: Extensive experiments show NR4DER significantly outperforms existing methods across multiple real-world datasets, effectively addressing the diverse learning pace of students.

Conclusion: NR4DER is an effective solution for diversified exercise recommendation, enhancing both accuracy and diversity in recommendations.

Abstract: With the widespread adoption of online education platforms, an increasing
number of students are gaining new knowledge through Massive Open Online
Courses (MOOCs). Exercise recommendation have made strides toward improving
student learning outcomes. However, existing methods not only struggle with
high dropout rates but also fail to match the diverse learning pace of
students. They frequently face difficulties in adjusting to inactive students'
learning patterns and in accommodating individualized learning paces, resulting
in limited accuracy and diversity in recommendations. To tackle these
challenges, we propose Neural Re-ranking for Diversified Exercise
Recommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to
improve the effectiveness of the exercise filter module. It then employs a
sequence enhancement method to enhance the representation of inactive students,
accurately matches students with exercises of appropriate difficulty. Finally,
it utilizes neural re-ranking to generate diverse recommendation lists based on
individual students' learning histories. Extensive experimental results
indicate that NR4DER significantly outperforms existing methods across multiple
real-world datasets and effectively caters to the diverse learning pace of
students.

</details>


### [480] [Preference-based learning for news headline recommendation](https://arxiv.org/abs/2506.06334)
*Alexandre Bouras,Audrey Durand,Richard Khoury*

Main category: cs.IR

TL;DR: This study explores strategies for optimizing news headline recommendations through preference-based learning, finding that explicit exploration may not be required in noisy contexts.


<details>
  <summary>Details</summary>
Motivation: To improve news headline recommendations by understanding the impact of translation on engagement predictions and the benefits of different interactive strategies on user engagement.

Method: Using real-world data of user interactions with French-language online news posts, a headline recommender agent is learned under a contextual bandit setting.

Result: Explicit exploration may not be required in the presence of noisy contexts for efficient strategies.

Conclusion: Simpler strategies can be efficient in practice when dealing with noisy contexts in news headline recommendations.

Abstract: This study explores strategies for optimizing news headline recommendations
through preference-based learning. Using real-world data of user interactions
with French-language online news posts, we learn a headline recommender agent
under a contextual bandit setting. This allows us to explore the impact of
translation on engagement predictions, as well as the benefits of different
interactive strategies on user engagement during data collection. Our results
show that explicit exploration may not be required in the presence of noisy
contexts, opening the door to simpler but efficient strategies in practice.

</details>


### [481] [Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces](https://arxiv.org/abs/2506.06557)
*Antonio Pariente,Ignacio Hounie,Santiago Segarra,Alejandro Ribeiro*

Main category: cs.IR

TL;DR: 通过学习$q$-metric近似，可以使经典的度量树算法在高维数据上达到与最先进搜索方法竞争的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的矢量搜索算法忽视了矢量嵌入的度量结构，未充分利用其底层属性。

Method: 提出了一种新的投影方法，将具有任意非相似性度量的矢量数据集嵌入到$q$-metric空间中，并保持最近邻特性；同时学习该投影的近似值以高效地将查询点转换到满足所需性质的欧几里得距离空间。

Result: 实验结果表明，使用文本和图像矢量嵌入进行$q$-metric近似学习，可以让经典度量树算法在高维数据上的表现与最先进的搜索方法相媲美。

Conclusion: 利用$q$-metric空间的更强三角不等式可以减少精确搜索中的比较次数，当$q$趋于无穷大时，搜索复杂度变为对数级。

Abstract: Despite the ubiquity of vector search applications, prevailing search
algorithms overlook the metric structure of vector embeddings, treating it as a
constraint rather than exploiting its underlying properties. In this paper, we
demonstrate that in $q$-metric spaces, metric trees can leverage a stronger
version of the triangle inequality to reduce comparisons for exact search.
Notably, as $q$ approaches infinity, the search complexity becomes logarithmic.
Therefore, we propose a novel projection method that embeds vector datasets
with arbitrary dissimilarity measures into $q$-metric spaces while preserving
the nearest neighbor. We propose to learn an approximation of this projection
to efficiently transform query points to a space where euclidean distances
satisfy the desired properties. Our experimental results with text and image
vector embeddings show that learning $q$-metric approximations enables classic
metric tree algorithms -- which typically underperform with high-dimensional
data -- to achieve competitive performance against state-of-the-art search
methods.

</details>


### [482] [HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval](https://arxiv.org/abs/2506.07296)
*Arian Askari,Emmanouil Stergiadis,Ilya Gusev,Moran Beladev*

Main category: cs.IR

TL;DR: The paper introduces HotelMatch-LLM, a multimodal dense retrieval model for travel domain property search. It features domain-specific multi-task optimization, asymmetrical dense retrieval architecture, and extensive image processing. Experiments show it outperforms state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional travel search engines which require users to start with a destination and edit search parameters.

Method: HotelMatch-LLM has three key innovations: (1) Domain-specific multi-task optimization with novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model for online query processing and a large language model for embedding hotel data; (3) Extensive image processing to handle all property image galleries.

Result: Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, achieving 0.681 compared to 0.603 for the most effective baseline, MARVEL.

Conclusion: The analysis highlights the impact of multi-task optimization, the generalizability across LLM architectures, and scalability for processing large image galleries.

Abstract: We present HotelMatch-LLM, a multimodal dense retrieval model for the travel
domain that enables natural language property search, addressing the
limitations of traditional travel search engines which require users to start
with a destination and editing search parameters. HotelMatch-LLM features three
key innovations: (1) Domain-specific multi-task optimization with three novel
retrieval, visual, and language modeling objectives; (2) Asymmetrical dense
retrieval architecture combining a small language model (SLM) for efficient
online query processing and a large language model (LLM) for embedding hotel
data; and (3) Extensive image processing to handle all property image
galleries. Experiments on four diverse test sets show HotelMatch-LLM
significantly outperforms state-of-the-art models, including VISTA and MARVEL.
Specifically, on the test set -- main query type -- we achieve 0.681 for
HotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our
analysis highlights the impact of our multi-task optimization, the
generalizability of HotelMatch-LLM across LLM architectures, and its
scalability for processing large image galleries.

</details>


### [483] [Correcting for Position Bias in Learning to Rank: A Control Function Approach](https://arxiv.org/abs/2506.06989)
*Md Aminul Islam,Kathryn Vasilaky,Elena Zheleva*

Main category: cs.IR

TL;DR: The paper proposes a novel control function-based method to correct position bias in learning-to-rank systems without needing the click or propensity model, allowing nonlinearity and debiasing any ranking algorithm.


<details>
  <summary>Details</summary>
Motivation: Implicit feedback data used in LTR systems is prone to position bias, which affects the accuracy of the rankings. Current methods for correcting this bias require knowledge of the click or propensity model and may not allow for nonlinearity in the underlying ranking model.

Method: A two-stage process is introduced where the first stage uses exogenous variation from residuals of the ranking process to correct position bias in the second stage click equation. The method does not need the click or propensity model and allows nonlinearity in the ranking model. It can also be applied to any state-of-the-art ranking algorithm.

Result: Experimental results indicate that the proposed method outperforms current state-of-the-art approaches in correcting position bias.

Conclusion: The novel control function-based method effectively corrects position bias in LTR systems without requiring specific models and can enhance any ranking algorithm.

Abstract: Implicit feedback data, such as user clicks, is commonly used in
learning-to-rank (LTR) systems because it is easy to collect and it often
reflects user preferences. However, this data is prone to various biases, and
training an LTR system directly on biased data can result in suboptimal ranking
performance. One of the most prominent and well-studied biases in implicit
feedback data is position bias, which occurs because users are more likely to
interact with higher-ranked documents regardless of their true relevance. In
this paper, we propose a novel control function-based method that accounts for
position bias in a two-stage process. The first stage uses exogenous variation
from the residuals of the ranking process to correct for position bias in the
second stage click equation. Unlike previous position bias correction methods,
our method does not require knowledge of the click or propensity model and
allows for nonlinearity in the underlying ranking model. Moreover, our method
is general and allows for debiasing any state-of-the-art ranking algorithm by
plugging it into the second stage. We also introduce a technique to debias
validation clicks for hyperparameter tuning to select the optimal model in the
absence of unbiased validation data. Experimental results demonstrate that our
method outperforms state-of-the-art approaches in correcting for position bias.

</details>


### [484] [RADAR: Recall Augmentation through Deferred Asynchronous Retrieval](https://arxiv.org/abs/2506.07261)
*Amit Jaspal,Qian Dang,Ajantha Ramineni*

Main category: cs.IR

TL;DR: RADAR，一种新型框架，通过离线异步计算预排更大候选集，使用复杂排名模型提升推荐系统召回率和参与度。


<details>
  <summary>Details</summary>
Motivation: 现代大规模推荐系统的初始检索阶段难以有效区分高度相关和仅仅相关的项目，影响了从十亿规模目录中提取最吸引人的项目的能力。

Method: 引入RADAR框架，利用异步、离线计算为用户预排更大候选集，使用全复杂性排名模型，并将顶级项目存储为高质量检索源，在在线推理期间绕过在线检索和预排名阶段。

Result: 离线实验显示RADAR显著提升了召回率（与DNN检索基线相比Recall@200提高两倍），在线A/B测试确认总体参与度指标提高了0.8%。

Conclusion: RADAR是一种实用且有效的方法，在严格的在线服务约束下改善推荐质量。

Abstract: Modern large-scale recommender systems employ multi-stage ranking funnel
(Retrieval, Pre-ranking, Ranking) to balance engagement and computational
constraints (latency, CPU). However, the initial retrieval stage, often relying
on efficient but less precise methods like K-Nearest Neighbors (KNN), struggles
to effectively surface the most engaging items from billion-scale catalogs,
particularly distinguishing highly relevant and engaging candidates from merely
relevant ones. We introduce Recall Augmentation through Deferred Asynchronous
Retrieval (RADAR), a novel framework that leverages asynchronous, offline
computation to pre-rank a significantly larger candidate set for users using
the full complexity ranking model. These top-ranked items are stored and
utilized as a high-quality retrieval source during online inference, bypassing
online retrieval and pre-ranking stages for these candidates. We demonstrate
through offline experiments that RADAR significantly boosts recall (2X
Recall@200 vs DNN retrieval baseline) by effectively combining a larger
retrieved candidate set with a more powerful ranking model. Online A/B tests
confirm a +0.8% lift in topline engagement metrics, validating RADAR as a
practical and effective method to improve recommendation quality under strict
online serving constraints.

</details>


### [485] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: Recent advances in Large Language Models (LLMs) have driven their adoption in recommender systems through Retrieval-Augmented Generation (RAG) frameworks. We introduce LlamaRec-LKG-RAG, a novel single-pass, end-to-end trainable framework that integrates personalized knowledge graph context into LLM-based recommendation ranking.


<details>
  <summary>Details</summary>
Motivation: Existing RAG approaches predominantly rely on flat, similarity-based retrieval that fails to leverage the rich relational structure inherent in user-item interactions.

Method: Our approach extends the LlamaRec architecture by incorporating a lightweight user preference module that dynamically identifies salient relation paths within a heterogeneous knowledge graph constructed from user behavior and item metadata. These personalized subgraphs are seamlessly integrated into prompts for a fine-tuned Llama-2 model, enabling efficient and interpretable recommendations through a unified inference step.

Result: Comprehensive experiments on ML-100K and Amazon Beauty datasets demonstrate consistent and significant improvements over LlamaRec across key ranking metrics (MRR, NDCG, Recall).

Conclusion: LlamaRec-LKG-RAG demonstrates the critical value of structured reasoning in LLM-based recommendations and establishes a foundation for scalable, knowledge-aware personalization in next-generation recommender systems.

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


### [486] [MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization](https://arxiv.org/abs/2506.07563)
*Ken Yagel,Eyal German,Aviel Ben Siman Tov*

Main category: cs.IR

TL;DR: Personalized recommendation systems need to adapt to user interactions across different domains. Traditional approaches lack flexibility, so MoE-MLoRA, a mixture-of-experts framework was proposed. Each expert is trained independently before a gating network weights their contributions dynamically. Evaluated across eight CTR models on Movielens and Taobao, it shows improved performance in large-scale dynamic datasets but limited benefits in structured datasets with low domain diversity and sparsity. Larger ensembles don't always improve performance, indicating the need for model-aware tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches like MLoRA apply a single adaptation per domain but lack flexibility in handling diverse user behaviors.

Method: Propose MoE-MLoRA, a mixture-of-experts framework where each expert is first trained independently to specialize in its domain before a gating network is trained to weight their contributions dynamically.

Result: Improves performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but offers limited benefits in structured datasets with low domain diversity and sparsity.

Conclusion: Findings highlight the potential of expert-based architectures for multi-domain recommendation systems, demonstrating that task-aware specialization and adaptive gating can enhance predictive accuracy in complex environments.

Abstract: Personalized recommendation systems must adapt to user interactions across
different domains. Traditional approaches like MLoRA apply a single adaptation
per domain but lack flexibility in handling diverse user behaviors. To address
this, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is
first trained independently to specialize in its domain before a gating network
is trained to weight their contributions dynamically. We evaluate MoE-MLoRA
across eight CTR models on Movielens and Taobao, showing that it improves
performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)
but offers limited benefits in structured datasets with low domain diversity
and sparsity. Further analysis of the number of experts per domain reveals that
larger ensembles do not always improve performance, indicating the need for
model-aware tuning. Our findings highlight the potential of expert-based
architectures for multi-domain recommendation systems, demonstrating that
task-aware specialization and adaptive gating can enhance predictive accuracy
in complex environments. The implementation and code are available in our
GitHub repository.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [487] [Towards Generalized Source Tracing for Codec-Based Deepfake Speech](https://arxiv.org/abs/2506.07294)
*Xuanjun Chen,I-Ming Lin,Lin Zhang,Haibin Wu,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.SD

TL;DR: Recent source tracing methods for codec-based deepfake speech have performed suboptimally. This paper introduces SASTNet, which uses Whisper and Wav2vec2 with AudioMAE to encode semantic and acoustic features respectively. SASTNet achieves state-of-the-art performance on the CoSG test set.


<details>
  <summary>Details</summary>
Motivation: There is a need to improve the performance of source tracing models for codec-based deepfake speech. Current models trained on simulated data do not perform well on real CoSG-generated audio due to overfitting to non-speech regions.

Method: The authors developed SASTNet, a Semantic-Acoustic Source Tracing Network that integrates Whisper for semantic feature encoding and Wav2vec2 with AudioMAE for acoustic feature encoding to address the issue of overfitting.

Result: SASTNet demonstrates state-of-the-art performance on the CoSG test set in the CodecFake+ dataset.

Conclusion: SASTNet effectively leverages both semantic and acoustic features to improve source tracing for codec-based deepfake speech, setting a new benchmark in this area.

Abstract: Recent attempts at source tracing for codec-based deepfake speech
(CodecFake), generated by neural audio codec-based speech generation (CoSG)
models, have exhibited suboptimal performance. However, how to train source
tracing models using simulated CoSG data while maintaining strong performance
on real CoSG-generated audio remains an open challenge. In this paper, we show
that models trained solely on codec-resynthesized data tend to overfit to
non-speech regions and struggle to generalize to unseen content. To mitigate
these challenges, we introduce the Semantic-Acoustic Source Tracing Network
(SASTNet), which jointly leverages Whisper for semantic feature encoding and
Wav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet
achieves state-of-the-art performance on the CoSG test set of the CodecFake+
dataset, demonstrating its effectiveness for reliable source tracing.

</details>


### [488] [RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis](https://arxiv.org/abs/2506.07118)
*Yu-Xuan Wu,Ziyan Huang,Bin Hu,Zhi-Hong Guan*

Main category: cs.SD

TL;DR: This paper presents a robust brain-inspired audio feature extractor (RBA-FE) model using an improved hierarchical network architecture for depression diagnosis, which incorporates an adaptive rate smooth leaky integrate-and-fire (ARSLIF) model to handle noise issues. Experimental results on MODMA, AVEC2014, and DAIC-WOZ datasets show state-of-the-art accuracy and enhanced noise robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation in existing deep learning models that focus on image-based diagnostic tasks while ignoring audio features, particularly in the context of depression diagnosis. The authors aim to develop a model that can effectively extract audio features while handling the challenge of noise in real-world data.

Method: The proposed method involves developing a robust brain-inspired audio feature extractor (RBA-FE) model with an improved hierarchical network architecture. This model leverages six acoustic features extracted from raw audio data to capture spatial characteristics and temporal dependencies. To deal with noise, the RBA-FE incorporates an adaptive rate smooth leaky integrate-and-fire (ARSLIF) model, which emulates the 'retuning of cellular signal selectivity' mechanism in brain attention systems.

Result: The RBA-FE model achieves state-of-the-art accuracy on the MODMA dataset with precision, accuracy, recall, and F1 score values of 0.8750, 0.8974, 0.8750, and 0.8750 respectively. Experiments on the AVEC2014 and DAIC-WOZ datasets also demonstrate enhanced noise robustness. Additionally, the ARSLIF neuron model indicates abnormal firing patterns within feature extraction on depressive audio data, providing brain-inspired interpretability.

Conclusion: The authors conclude that the RBA-FE model successfully addresses the challenges of noise in audio data for depression diagnosis, achieving high accuracy and robustness. The incorporation of the ARSLIF model not only enhances performance but also offers interpretability by identifying abnormal firing patterns in depressive audio data.

Abstract: This article proposes a robust brain-inspired audio feature extractor
(RBA-FE) model for depression diagnosis, using an improved hierarchical network
architecture. Most deep learning models achieve state-of-the-art performance
for image-based diagnostic tasks, ignoring the counterpart audio features. In
order to tailor the noise challenge, RBA-FE leverages six acoustic features
extracted from the raw audio, capturing both spatial characteristics and
temporal dependencies. This hybrid attribute helps alleviate the precision
limitation in audio feature extraction within other learning models like deep
residual shrinkage networks. To deal with the noise issues, our model
incorporates an improved spiking neuron model, called adaptive rate smooth
leaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of
``retuning of cellular signal selectivity" in the brain attention systems,
which enhances the model robustness against environmental noises in audio data.
Experimental results demonstrate that RBA-FE achieves state-of-the-art accuracy
on the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in
precision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014
and DAIC-WOZ datasets both show enhancements in noise robustness. It is further
indicated by comparison that the ARSLIF neuron model suggest the abnormal
firing pattern within the feature extraction on depressive audio data, offering
brain-inspired interpretability.

</details>


### [489] [Speech Recognition on TV Series with Video-guided Post-Correction](https://arxiv.org/abs/2506.07323)
*Haoyuan Yang,Yue Zhang,Liqiang Jing*

Main category: cs.SD

TL;DR: The paper proposes a multimodal post-correction framework for improving ASR in complex environments like TV series, using video-based context to enhance transcription accuracy.


<details>
  <summary>Details</summary>
Motivation: ASR systems struggle with complex environments such as TV series due to overlapping speech, domain-specific terminology, and long-range contextual dependencies. Existing multimodal approaches fail to effectively utilize the rich temporal and contextual information available in video to correct ASR outputs.

Method: The method involves a two-stage framework: ASR Generation and Video-based Post-Correction. In the first stage, an initial transcript is produced. The second stage uses Video-based Contextual Information Extraction and Context-aware ASR Correction to refine the output. VLMM extracts key contextual information which is integrated with a LLM to improve the ASR output.

Result: The proposed method was evaluated on a multimodal benchmark for TV series ASR and showed effectiveness in enhancing transcription accuracy in complex multimedia environments by leveraging video-based context.

Conclusion: A novel multimodal post-correction framework has been developed that successfully improves ASR performance in complex environments such as TV series by utilizing video-based context.

Abstract: Automatic Speech Recognition (ASR) has achieved remarkable success with deep
learning, driving advancements in conversational artificial intelligence, media
transcription, and assistive technologies. However, ASR systems still struggle
in complex environments such as TV series, where overlapping speech,
domain-specific terminology, and long-range contextual dependencies pose
significant challenges to transcription accuracy. Existing multimodal
approaches fail to correct ASR outputs with the rich temporal and contextual
information available in video. To address this limitation, we propose a novel
multimodal post-correction framework that refines ASR transcriptions by
leveraging contextual cues extracted from video. Our framework consists of two
stages: ASR Generation and Video-based Post-Correction, where the first stage
produces the initial transcript and the second stage corrects errors using
Video-based Contextual Information Extraction and Context-aware ASR Correction.
We employ the Video-Large Multimodal Model (VLMM) to extract key contextual
information using tailored prompts, which is then integrated with a Large
Language Model (LLM) to refine the ASR output. We evaluate our method on a
multimodal benchmark for TV series ASR and demonstrate its effectiveness in
improving ASR performance by leveraging video-based context to enhance
transcription accuracy in complex multimedia environments.

</details>


### [490] [Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework](https://arxiv.org/abs/2506.07358)
*Kuiyuan Zhang,Wenjie Pei,Rushi Lan,Yifang Guo,Zhongyun Hua*

Main category: cs.SD

TL;DR: 设计了一种轻量级的单流多模态学习框架网络，用于音视频深度伪造检测，通过协作音视频学习块和多模态分类模块，提高检测效率与效果，参数仅为0.48M，但在多种数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的音视频深度伪造检测方法通常使用两个相对独立的子模型来分别学习音频和视觉特征，然后进行融合检测，但这种方法可能未充分利用音频和视觉特征之间的固有相关性，并且由于两个孤立的特征学习子模型可能导致冗余神经层，使得整体模型效率低下，不适用于资源受限的环境。

Method: 提出一种轻量级网络，采用单流多模态学习框架，具体引入协作音视频学习块以高效整合多模态信息，同时学习视觉和音频特征；通过迭代使用该块，单流网络实现跨层的多模态特征连续融合；此外，还提出多模态分类模块，增强视觉和音频分类器对模态内容的依赖性，提升视频分类器对音频和视觉模态不匹配的抵抗能力。

Result: 在DF-TIMIT、FakeAVCeleb和DFDC基准数据集上的实验表明，相比最先进的音视频联合检测方法，所提方法显著更轻量（仅0.48M参数），并且在单模态和多模态深度伪造检测以及未见类型的深度伪造检测中表现出优越性。

Conclusion: 所提出的轻量级单流多模态学习框架能够有效捕捉音频和视觉特征，减少冗余计算，适合资源受限环境，同时在多种类型深度伪造检测任务中展现出优越性能。

Abstract: Deepfakes are AI-synthesized multimedia data that may be abused for spreading
misinformation. Deepfake generation involves both visual and audio
manipulation. To detect audio-visual deepfakes, previous studies commonly
employ two relatively independent sub-models to learn audio and visual
features, respectively, and fuse them subsequently for deepfake detection.
However, this may underutilize the inherent correlations between audio and
visual features. Moreover, utilizing two isolated feature learning sub-models
can result in redundant neural layers, making the overall model inefficient and
impractical for resource-constrained environments.
  In this work, we design a lightweight network for audio-visual deepfake
detection via a single-stream multi-modal learning framework. Specifically, we
introduce a collaborative audio-visual learning block to efficiently integrate
multi-modal information while learning the visual and audio features. By
iteratively employing this block, our single-stream network achieves a
continuous fusion of multi-modal features across its layers. Thus, our network
efficiently captures visual and audio features without the need for excessive
block stacking, resulting in a lightweight network design. Furthermore, we
propose a multi-modal classification module that can boost the dependence of
the visual and audio classifiers on modality content. It also enhances the
whole resistance of the video classifier against the mismatches between audio
and visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and
DFDC benchmark datasets. Compared to state-of-the-art audio-visual joint
detection methods, our method is significantly lightweight with only 0.48M
parameters, yet it achieves superiority in both uni-modal and multi-modal
deepfakes, as well as in unseen types of deepfakes.

</details>


### [491] [Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching](https://arxiv.org/abs/2506.07199)
*Ben Hayes,Charalampos Saitis,György Fazekas*

Main category: cs.SD

TL;DR: The paper addresses the issue of parameter estimation for audio synthesizers with inherent symmetries and permutation invariance, proposing a conditional generative model with permutation equivariant continuous normalizing flow and relaxed equivariance strategy, which outperforms regression and other generative models on real-world synthesizer Surge XT.


<details>
  <summary>Details</summary>
Motivation: Many audio synthesizers can produce the same signal given different parameter configurations, making the inversion from sound to parameters an ill-posed problem due to intrinsic symmetries like permutation invariance.

Method: Firstly, demonstrate the degradation of performance when regressing point estimates under permutation symmetry. Then use a conditional generative model to view equivalent solutions as modes of a probability distribution. Further improve the performance by using a permutation equivariant continuous normalizing flow. Propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data.

Result: The method outperforms regression and generative baselines across audio reconstruction metrics when applied to Surge XT, a full-featured open source synthesizer.

Conclusion: A conditional generative model with permutation equivariant continuous normalizing flow and relaxed equivariance strategy significantly improves parameter estimation for audio synthesizers.

Abstract: Many audio synthesizers can produce the same signal given different parameter
configurations, meaning the inversion from sound to parameters is an inherently
ill-posed problem. We show that this is largely due to intrinsic symmetries of
the synthesizer, and focus in particular on permutation invariance. First, we
demonstrate on a synthetic task that regressing point estimates under
permutation symmetry degrades performance, even when using a
permutation-invariant loss function or symmetry-breaking heuristics. Then,
viewing equivalent solutions as modes of a probability distribution, we show
that a conditional generative model substantially improves performance.
Further, acknowledging the invariance of the implicit parameter distribution,
we find that performance is further improved by using a permutation equivariant
continuous normalizing flow. To accommodate intricate symmetries in real
synthesizers, we also propose a relaxed equivariance strategy that adaptively
discovers relevant symmetries from data. Applying our method to Surge XT, a
full-featured open source synthesizer used in real world audio production, we
find our method outperforms regression and generative baselines across audio
reconstruction metrics.

</details>


### [492] [LeVo: High-Quality Song Generation with Multi-Preference Alignment](https://arxiv.org/abs/2506.07520)
*Shun Lei,Yaoxun Xu,Zhiwei Lin,Huaicheng Zhang,Wei Tan,Hangting Chen,Jianwei Yu,Yixuan Zhang,Chenyu Yang,Haina Zhu,Shuai Wang,Zhiyong Wu,Dong Yu*

Main category: cs.SD

TL;DR: LeVo is a new framework which can generate high-quality songs by modeling mixed and dual-track tokens, employing decoder-only transformers and modular extension training strategy, and introducing multi-preference alignment method. It surpasses existing methods in various aspects.


<details>
  <summary>Details</summary>
Motivation: To solve the problems of complex song composition and lack of high-quality data that limit the sound quality, musicality, instruction following, and vocal-instrument harmony in music generation.

Method: The framework LeVo consists of LeLM and a music codec. LeLM models two types of tokens - mixed tokens for vocal-instrument harmony and dual-track tokens for high-quality song generation using two decoder-only transformers and a modular extension training strategy. Also, a multi-preference alignment method based on Direct Preference Optimization is introduced to improve musicality and instruction following.

Result: Experimental results show that LeVo outperforms current methods in both objective and subjective metrics. Ablation studies confirm the effectiveness of its designs.

Conclusion: LeVo presents an effective solution for generating high-quality songs with better sound quality, musicality, instruction following, and vocal-instrument harmony.

Abstract: Recent advances in large language models (LLMs) and audio language models
have significantly improved music generation, particularly in lyrics-to-song
generation. However, existing approaches still struggle with the complex
composition of songs and the scarcity of high-quality data, leading to
limitations in sound quality, musicality, instruction following, and
vocal-instrument harmony. To address these challenges, we introduce LeVo, an
LM-based framework consisting of LeLM and a music codec. LeLM is capable of
parallelly modeling two types of tokens: mixed tokens, which represent the
combined audio of vocals and accompaniment to achieve vocal-instrument harmony,
and dual-track tokens, which separately encode vocals and accompaniment for
high-quality song generation. It employs two decoder-only transformers and a
modular extension training strategy to prevent interference between different
token types. To further enhance musicality and instruction following, we
introduce a multi-preference alignment method based on Direct Preference
Optimization (DPO). This method handles diverse human preferences through a
semi-automatic data construction process and DPO post-training. Experimental
results demonstrate that LeVo consistently outperforms existing methods on both
objective and subjective metrics. Ablation studies further justify the
effectiveness of our designs. Audio examples are available at
https://levo-demo.github.io/.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [493] [Explaining Risks: Axiomatic Risk Attributions for Financial Models](https://arxiv.org/abs/2506.06653)
*Dangxing Chen*

Main category: q-fin.CP

TL;DR: 在近年来，机器学习模型以高度复杂的黑箱结构为代价取得了巨大成功。通过使用公理归因方法，我们可以公平地分配每个特征的贡献，从而解释模型预测。在高风险领域（如金融），风险与平均预测同样重要。本文解决了以下风险归因问题：如何在给定模型和数据的情况下公平分配风险？我们通过分析和实证例子证明，通过扩展Shapley值框架可以很好地分配风险。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的成功伴随着复杂性和不透明性，特别是在高风险领域（如金融），仅仅解释平均预测是不够的，还需要公平合理地分配风险。

Method: 采用公理归因方法，并扩展Shapley值框架来解决风险分配问题。

Result: 分析和实证例子表明，风险可以通过扩展Shapley值框架得到良好的分配。

Conclusion: 扩展Shapley值框架是一种有效的方法来公平分配模型中的风险。

Abstract: In recent years, machine learning models have achieved great success at the
expense of highly complex black-box structures. By using axiomatic attribution
methods, we can fairly allocate the contributions of each feature, thus
allowing us to interpret the model predictions. In high-risk sectors such as
finance, risk is just as important as mean predictions. Throughout this work,
we address the following risk attribution problem: how to fairly allocate the
risk given a model with data? We demonstrate with analysis and empirical
examples that risk can be well allocated by extending the Shapley value
framework.

</details>


### [494] [Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling](https://arxiv.org/abs/2506.07299)
*Hans Buehler,Blanka Horvath,Yannick Limmer,Thorsten Schmidt*

Main category: q-fin.CP

TL;DR: This paper addresses model uncertainty in quantitative finance by enhancing the conventional objective with an outer 'uncertainty measure' and proposing a subsampling strategy to approximate model uncertainty.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the challenge of model uncertainty in quantitative finance, where decisions rely on estimating stochastic models from limited data. Small misestimations can lead to significant deviations in decision quality.

Method: The method includes enhancing the conventional objective with an outer 'uncertainty measure', proposing an ad hoc subsampling strategy to approximate model uncertainty, and presenting an adapted stochastic gradient descent algorithm for efficient parallelization.

Result: Through various studies, uncertainty measures outperform traditional mixture of measures strategies and the model-agnostic subsampling-based approach enhances robustness against model risk while achieving performance comparable to more elaborate Bayesian methods.

Conclusion: Uncertainty measures and the proposed subsampling strategy provide robust solutions to model uncertainty in quantitative finance.

Abstract: This paper addresses the challenge of model uncertainty in quantitative
finance, where decisions in portfolio allocation, derivative pricing, and risk
management rely on estimating stochastic models from limited data. In practice,
the unavailability of the true probability measure forces reliance on an
empirical approximation, and even small misestimations can lead to significant
deviations in decision quality. Building on the framework of Klibanoff et al.
(2005), we enhance the conventional objective - whether this is expected
utility in an investing context or a hedging metric - by superimposing an outer
"uncertainty measure", motivated by traditional monetary risk measures, on the
space of models. In scenarios where a natural model distribution is lacking or
Bayesian methods are impractical, we propose an ad hoc subsampling strategy,
analogous to bootstrapping in statistical finance and related to mini-batch
sampling in deep learning, to approximate model uncertainty. To address the
quadratic memory demands of naive implementations, we also present an adapted
stochastic gradient descent algorithm that enables efficient parallelization.
Through analytical, simulated, and empirical studies - including multi-period,
real data and high-dimensional examples - we demonstrate that uncertainty
measures outperform traditional mixture of measures strategies and our
model-agnostic subsampling-based approach not only enhances robustness against
model risk but also achieves performance comparable to more elaborate Bayesian
methods.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [495] [From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem](https://arxiv.org/abs/2506.07066)
*Li Jingyuan*

Main category: econ.TH

TL;DR: This paper presents a comprehensive formalization of the von Neumann-Morgenstern (vNM) expected utility theorem using Lean 4, verifying preference relations over lotteries and providing rigorous foundations for applications in economic modeling, AI alignment, etc.


<details>
  <summary>Details</summary>
Motivation: To provide a rigorous formalization of the vNM expected utility theorem that bridges theoretical decision theory and computational implementation.

Method: Implementing classical axioms of preference in Lean 4 to enable machine-verified proofs of utility representations, including granular independence axiom, mixture lotteries claims, constructive demonstrations of utility existence, and computational experiments.

Result: Formally verified proofs of fundamental claims about mixture lotteries, equivalence to classical presentations with greater precision at decision boundaries, and validated results through computational experiments.

Conclusion: The formalization provides a rigorous foundation for applications in economic modeling, AI alignment, and management decision systems.

Abstract: This paper presents a comprehensive formalization of the von
Neumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive
theorem prover. We implement the classical axioms of preference-completeness,
transitivity, continuity, and independence-enabling machine-verified proofs of
both the existence and uniqueness of utility representations. Our formalization
captures the mathematical structure of preference relations over lotteries,
verifying that preferences satisfying the vNM axioms can be represented by
expected utility maximization.
  Our contributions include a granular implementation of the independence
axiom, formally verified proofs of fundamental claims about mixture lotteries,
constructive demonstrations of utility existence, and computational experiments
validating the results. We prove equivalence to classical presentations while
offering greater precision at decision boundaries.
  This formalization provides a rigorous foundation for applications in
economic modeling, AI alignment, and management decision systems, bridging the
gap between theoretical decision theory and computational implementation.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [496] [MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes](https://arxiv.org/abs/2506.06318)
*Feiyang Pan,Shenghe Zheng,Chunyan Yin,Guangbin Dou*

Main category: eess.SP

TL;DR: MEMS陀螺仪在惯性导航和运动控制应用中起着关键作用，但通常在测量范围和噪声性能之间存在基本权衡。现有的硬件解决方案会增加复杂性、成本和可扩展性挑战，而深度学习方法主要集中在降噪上，并且需要精确对齐的真实信号，这使得它们难以在实际场景中部署，并且没有解决根本的权衡问题。为了解决这些挑战，我们引入了MoE-Gyro，这是一个新颖的自我监督框架，旨在同时进行超范围信号重建和降噪。MoE-Gyro使用两个专家：一个用于重建饱和段的过范围重建专家（ORE），其具有高斯衰减注意力机制；另一个是去噪专家（DE），利用双分支互补掩码和FFT引导增强来进行强大的降噪。轻量级门控模块动态地将输入段路由到适当的专家。此外，现有的评估缺乏全面的标准来评估多维信号增强。为了弥补这一差距，我们引入了IMU信号增强基准（ISEBench），这是一个开源基准测试平台，包含GyroPeak-100数据集和统一评估IMU信号增强方法。我们使用提出的ISEBench评估MoE-Gyro，证明我们的框架显著地将可测量范围从450 deg/s扩展到1500 deg/s，减少98.4%的偏差不稳定性，并达到最先进的性能，有效地解决了惯性传感中的长期权衡问题。


<details>
  <summary>Details</summary>
Motivation: MEMS陀螺仪在许多重要应用中扮演关键角色，但面临测量范围与噪声性能之间的基本权衡问题。现有硬件解决方案增加了复杂性和成本，而深度学习方法虽然有助于降噪，但需要精确对齐的真实信号，难以实际部署，且未解决根本权衡问题。因此，需要一种新的方法来同时实现超范围信号重建和降噪。

Method: 提出了MoE-Gyro框架，包括两个专家模块：过范围重建专家（ORE）和去噪专家（DE）。ORE采用高斯衰减注意力机制重建饱和信号段，而DE通过双分支互补掩码和FFT引导增强进行降噪。此外，还设计了一个轻量级门控模块动态选择合适的专家处理输入信号段。为了评估多维信号增强效果，开发了ISEBench基准测试平台，包含GyroPeak-100数据集和统一评估标准。

Result: 实验结果表明，MoE-Gyro框架显著扩展了可测量范围（从450 deg/s到1500 deg/s），大幅降低偏差不稳定性（98.4%），并在惯性信号增强方面达到了最先进的性能水平。

Conclusion: MoE-Gyro框架有效解决了MEMS陀螺仪在测量范围和噪声性能之间的长期权衡问题，同时实现了超范围信号重建和降噪。ISEBench平台为惯性信号增强提供了一个全面的评估标准，推动了该领域的进一步研究和发展。

Abstract: MEMS gyroscopes play a critical role in inertial navigation and motion
control applications but typically suffer from a fundamental trade-off between
measurement range and noise performance. Existing hardware-based solutions
aimed at mitigating this issue introduce additional complexity, cost, and
scalability challenges. Deep-learning methods primarily focus on noise
reduction and typically require precisely aligned ground-truth signals, making
them difficult to deploy in practical scenarios and leaving the fundamental
trade-off unresolved. To address these challenges, we introduce Mixture of
Experts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework
specifically designed for simultaneous over-range signal reconstruction and
noise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction
Expert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing
saturated segments; and a Denoise Expert (DE), utilizing dual-branch
complementary masking combined with FFT-guided augmentation for robust noise
reduction. A lightweight gating module dynamically routes input segments to the
appropriate expert. Furthermore, existing evaluation lack a comprehensive
standard for assessing multi-dimensional signal enhancement. To bridge this
gap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source
benchmarking platform comprising the GyroPeak-100 dataset and a unified
evaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our
proposed ISEBench, demonstrating that our framework significantly extends the
measurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by
98.4%, and achieves state-of-the-art performance, effectively addressing the
long-standing trade-off in inertial sensing.

</details>


### [497] [A Reinforcement Learning Approach for RIS-aided Fair Communications](https://arxiv.org/abs/2506.06344)
*Alex Pierron,Michel Barbeau,Luca De Cicco,Jose Rubio-Hernan,Joaquin Garcia-Alfaro*

Main category: eess.SP

TL;DR: This paper investigates the use of Reconfigurable Intelligent Surfaces (RISs) combined with Reinforcement Learning (RL) to create a fair and efficient communication system for multiple user equipment units.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance network performance and energy efficiency in areas with low coverage, while ensuring fair communications among User Equipment (UE) units.

Method: The method involves exploring fairness properties from previous work and proposing a novel approach that integrates RISs with RL techniques to achieve an efficient and fair duplex system for multiple UE units.

Result: The authors report their experimental work and simulation results, showing improvements in both efficiency and fairness in the communication system. They also release their code and datasets for further research.

Conclusion: Reconfigurable Intelligent Surfaces combined with Reinforcement Learning can significantly improve both the efficiency and fairness of communication systems for multiple user equipment units.

Abstract: Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements
that can dynamically alter electromagnetic wave properties to enhance
beamforming and leading to improvements in areas with low coverage properties.
They have the potential to be combined with Reinforcement Learning (RL)
techniques to achieve network performance and energy efficiency via
optimization techniques. In addition to performance and energy improvements, it
is also crucial to consider the concept of fair communications. RISs must
ensure that User Equipment (UE) units receive their signals with adequate
strength, without other UE being deprived of service due to insufficient power.
In this paper, we address such a problem. We explore the fairness properties of
previous work and propose a novel method that aims at obtaining an efficient
and fair duplex RIS-RL system for multiple legitimate UE units. We report and
discuss our experimental work and simulation results. We also release our code
and datasets to foster further research in the topic.

</details>


### [498] [Deep learning methods for modeling infrasound transmission loss in the middle atmosphere](https://arxiv.org/abs/2506.06351)
*Alexis Le Pichon,Alice Janela Cameijo,Samir Aknine,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven Peter Naesholm*

Main category: eess.SP

TL;DR: This paper optimizes a convolutional neural network to predict infrasound transmission losses over global ranges with improved accuracy and reduced computation time compared to parabolic equation simulations.


<details>
  <summary>Details</summary>
Motivation: To accurately model infrasound transmission losses globally for the International Monitoring System, overcoming limitations of existing methods like computational cost and prediction errors in unfavorable wind conditions.

Method: Developed an optimized convolutional network that predicts transmission losses from globally simulated temperature and wind fields over 4000 km propagation ranges, implementing key architectural improvements.

Result: The model predicts transmission losses with an average error of 8.6 dB across the frequency band (0.1-3.2 Hz) under realistic atmospheric scenarios.

Conclusion: The optimized convolutional network enhances the prediction of infrasound transmission losses globally, providing a more efficient alternative to traditional parabolic equation methods.

Abstract: Accurate modeling of infrasound transmission losses (TLs) is essential to
assess the performance of the global International Monitoring System infrasound
network. Among existing propagation modeling tools, parabolic equation (PE)
method enables TLs to be finely modeled, but its computational cost does not
allow exploration of a large parameter space for operational monitoring
applications. To reduce computation times, Brissaud et al. 2023 explored the
potential of convolutional neural networks trained on a large set of regionally
simulated wavefields (< 1000 km from the source) to predict TLs with negligible
computation times compared to PE simulations. However, this method struggles in
unfavorable initial wind conditions, especially at high frequencies, and causal
issues with winds at large distances from the source affecting ground TLs close
to the source. In this study, we have developed an optimized convolutional
network designed to minimize prediction errors while predicting TLs from
globally simulated combined temperature and wind fields spanning over
propagation ranges of 4000 km. Our approach enhances the previously proposed
one by implementing key optimizations that improve the overall architecture
performance. The implemented model predicts TLs with an average error of 8.6 dB
in the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric
scenarios.

</details>


### [499] [Large Language Models for EEG: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2506.06353)
*Naseem Babu,Jimson Mathew,A. P. Vinod*

Main category: eess.SP

TL;DR: The paper reviews the integration of Large Language Models (LLMs) with EEG research across four domains, showing how transformer-based models enable complex tasks in neural decoding, BCIs, and affective computing.


<details>
  <summary>Details</summary>
Motivation: To provide a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications.

Method: Organizing the literature into four domains: LLM-inspired foundation models for EEG representation learning, EEG-to-language decoding, cross-modal generation, and clinical applications/dataset management tools. Transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning are highlighted.

Result: Transformer-based models have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance.

Conclusion: This work serves as a foundational resource for future research to bridge natural language processing and neural signal analysis through language models.

Abstract: The growing convergence between Large Language Models (LLMs) and
electroencephalography (EEG) research is enabling new directions in neural
decoding, brain-computer interfaces (BCIs), and affective computing. This
survey offers a systematic review and structured taxonomy of recent
advancements that utilize LLMs for EEG-based analysis and applications. We
organize the literature into four domains: (1) LLM-inspired foundation models
for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal
generation including image and 3D object synthesis, and (4) clinical
applications and dataset management tools. The survey highlights how
transformer-based architectures adapted through fine-tuning, few-shot, and
zero-shot learning have enabled EEG-based models to perform complex tasks such
as natural language generation, semantic interpretation, and diagnostic
assistance. By offering a structured overview of modeling strategies, system
designs, and application areas, this work serves as a foundational resource for
future work to bridge natural language processing and neural signal analysis
through language models.

</details>


### [500] [Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation](https://arxiv.org/abs/2506.06358)
*Alice Janela Cameijo,Alexis Le Pichon,Youcef Sklab,Souhila Arib,Quentin Brissaud,Sven peter Naesholm,Constantino Listowski,Samir Aknine*

Main category: eess.SP

TL;DR: 研究通过优化神经网络结构，使用风速和温度场作为输入，实现了对远距离声波传输损失的快速预测，并在2022年洪加火山喷发事件中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习算法由于仅使用调整过的大气模型且缺乏温度输入，无法准确模拟长距离声波传播损失。因此需要改进模型以包含温度和风速数据，提升对大气条件的全面表示能力。

Method: 本研究将风速与温度场作为神经网络的输入，进行高达130公里高度、4000公里距离的模拟。同时优化了神经网络架构，利用卷积层和循环层捕捉空间和范围依赖特征。

Result: 该神经网络相对于完整抛物线方程模拟的平均误差为4 dB，能够提供认知和数据相关的不确定性估计，并成功应用于2022年洪加火山喷发事件的预测。

Conclusion: 此研究是迈向爆炸源国际监测系统实时检测阈值评估的重要一步，提升了对声波传播损失的预测能力。

Abstract: Accurate modeling of infrasound transmission loss is essential for evaluating
the performance of the International Monitoring System, enabling the effective
design and maintenance of infrasound stations to support compliance of the
Comprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling
tools enable transmission loss to be finely simulated using atmospheric models.
However, the computational cost prohibits the exploration of a large parameter
space in operational monitoring applications. To address this, recent studies
made use of a deep learning algorithm capable of making transmission loss
predictions almost instantaneously. However, the use of nudged atmospheric
models leads to an incomplete representation of the medium, and the absence of
temperature as an input makes the algorithm incompatible with long range
propagation. In this study, we address these limitations by using both wind and
temperature fields as inputs to a neural network, simulated up to 130 km
altitude and 4,000 km distance. We also optimize several aspects of the neural
network architecture. We exploit convolutional and recurrent layers to capture
spatially and range-dependent features embedded in realistic atmospheric
models, improving the overall performance. The neural network reaches an
average error of 4 dB compared to full parabolic equation simulations and
provides epistemic and data-related uncertainty estimates. Its evaluation on
the 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its
prediction capability using atmospheric conditions and frequencies not included
in the training. This represents a significant step towards near real-time
assessment of International Monitoring System detection thresholds of explosive
sources.

</details>


### [501] [Model-based Neural Data Augmentation for sub-wavelength Radio Localization](https://arxiv.org/abs/2506.06387)
*Baptiste Chatelier,Vincent Corlay,Musa Furkan Keskin,Matthieu Crussière,Henk Wymeersch,Luc Le Magoarou*

Main category: eess.SP

TL;DR: This paper proposes a model-based neural network that learns the location-to-channel mapping to enhance fingerprinting-based localization, achieving sub-wavelength accuracy in NLoS environments while reducing memory needs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional signal processing techniques and machine learning-assisted localization methods in complex radio environments, especially in NLoS scenarios, where localization accuracy is degraded and computational complexity is high.

Method: A model-based neural network is employed to learn the location-to-channel mapping, acting as a generative neural channel model. This model augments the fingerprinting comparison dictionary while reducing memory requirements.

Result: The method achieves sub-wavelength localization accuracy in NLoS environments, improving accuracy by several orders of magnitude and reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.

Conclusion: This approach successfully enhances localization accuracy while decreasing memory usage, making it a significant advancement in fingerprinting-based localization.

Abstract: The increasing deployment of large antenna arrays at base stations has
significantly improved the spatial resolution and localization accuracy of
radio-localization methods. However, traditional signal processing techniques
struggle in complex radio environments, particularly in scenarios dominated by
non line of sight (NLoS) propagation paths, resulting in degraded localization
accuracy. Recent developments in machine learning have facilitated the
development of machine learning-assisted localization techniques, enhancing
localization accuracy in complex radio environments. However, these methods
often involve substantial computational complexity during both the training and
inference phases. This work extends the well-established fingerprinting-based
localization framework by simultaneously reducing its memory requirements and
improving its accuracy. Specifically, a model-based neural network is used to
learn the location-to-channel mapping, and then serves as a generative neural
channel model. This generative model augments the fingerprinting comparison
dictionary while reducing the memory requirements. The proposed method
outperforms fingerprinting baselines by achieving sub-wavelength localization
accuracy, even in NLoS environments. Remarkably, it offers an improvement by
several orders of magnitude in localization accuracy, while simultaneously
reducing memory requirements by an order of magnitude compared to classical
fingerprinting methods.

</details>


### [502] [Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning](https://arxiv.org/abs/2506.06306)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: eess.SP

TL;DR: This paper aims to develop machine learning methods for early agitation prediction in dementia patients using multimodal sensor data. It introduces new agitation-related features and evaluates various models on the TIHM dataset, finding binary classification with LightGBM yields the best performance (AUC-ROC 0.9720). This work provides a benchmark for privacy-preserving agitation prediction.


<details>
  <summary>Details</summary>
Motivation: Agitation is a common issue among dementia patients, especially those living without continuous clinical supervision. Early prediction of agitation can help reduce caregiver burden and improve quality of life for both patients and caregivers.

Method: The study employed multimodal sensor data from the TIHM dataset, which includes in-home activity, physiology, and sleep data over 2,803 days. It introduced new contextual features related to agitation and evaluated multiple machine learning and deep learning models across different problem formulations such as binary classification, sequential data classification, and anomaly detection.

Result: Binary classification using the current 6-hour timestamp to predict agitation at the next timestamp was found to be the most effective approach. Incorporating additional information like time of day and agitation history further improved model performance, achieving an AUC-ROC of 0.9720 and AUC-PR of 0.4320 with LightGBM.

Conclusion: This study presents the first comprehensive benchmark for state-of-the-art techniques in agitation prediction for community-based dementia care. The results enable accurate, explainable, and efficient agitation prediction, supporting proactive dementia care and aging in place.

Abstract: Agitation is one of the most common responsive behaviors in people living
with dementia, particularly among those residing in community settings without
continuous clinical supervision. Timely prediction of agitation can enable
early intervention, reduce caregiver burden, and improve the quality of life
for both patients and caregivers. This study aimed to develop and benchmark
machine learning approaches for the early prediction of agitation in
community-dwelling older adults with dementia using multimodal sensor data. A
new set of agitation-related contextual features derived from activity data was
introduced and employed for agitation prediction. A wide range of machine
learning and deep learning models was evaluated across multiple problem
formulations, including binary classification for single-timestamp tabular
sensor data and multi-timestamp sequential sensor data, as well as anomaly
detection for single-timestamp tabular sensor data. The study utilized the
Technology Integrated Health Management (TIHM) dataset, the largest publicly
available dataset for remote monitoring of people living with dementia,
comprising 2,803 days of in-home activity, physiology, and sleep data. The most
effective setting involved binary classification of sensor data using the
current 6-hour timestamp to predict agitation at the subsequent timestamp.
Incorporating additional information, such as time of day and agitation
history, further improved model performance, with the highest AUC-ROC of 0.9720
and AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work
presents the first comprehensive benchmarking of state-of-the-art techniques
for agitation prediction in community-based dementia care using
privacy-preserving sensor data. The approach enables accurate, explainable, and
efficient agitation prediction, supporting proactive dementia care and aging in
place.

</details>


### [503] [Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia](https://arxiv.org/abs/2506.06309)
*Mohamed Kefi,Tien Dat Pham,Thin Nguyen,Mark G. Tjoelker,Viola Devasirvatham,Kenichi Kashiwagi*

Main category: eess.SP

TL;DR: The paper presents a method for estimating olive yield using remote sensing data and machine learning models, achieving high accuracy with Landsat-8 and Landsat-9 satellite imagery.


<details>
  <summary>Details</summary>
Motivation: Olive production is crucial in Mediterranean climates, but climate change causes significant yield variations. Accurate estimation of olive yield using remote sensing and machine learning remains complex.

Method: The study developed a pipeline that extracts features from multispectral reflectance bands, vegetation indices from Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, and digital elevation model data. This was combined with ground-based field survey data to form a structured tabular dataset. An automated ensemble learning framework implemented using AutoGluon was used to train and evaluate multiple machine learning models, select optimal combinations through stacking, and generate robust yield predictions using five-fold cross-validation.

Result: The results show strong predictive performance from both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha.

Conclusion: This study provides a scalable, cost-effective, and accurate method for olive yield estimation, which could be applicable across diverse agricultural regions globally.

Abstract: Olive production is an important tree crop in Mediterranean climates.
However, olive yield varies significantly due to climate change. Accurately
estimating yield using remote sensing and machine learning remains a complex
challenge. In this study, we developed a streamlined pipeline for olive yield
estimation in the Kairouan and Sousse governorates of Tunisia. We extracted
features from multispectral reflectance bands, vegetation indices derived from
Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital
elevation model data. These spatial features were combined with ground-based
field survey data to form a structured tabular dataset. We then developed an
automated ensemble learning framework, implemented using AutoGluon to train and
evaluate multiple machine learning models, select optimal combinations through
stacking, and generate robust yield predictions using five-fold
cross-validation. The results demonstrate strong predictive performance from
both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per
ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This
study highlights a scalable, cost-effective, and accurate method for olive
yield estimation, with potential applicability across diverse agricultural
regions globally.

</details>


### [504] [Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue](https://arxiv.org/abs/2506.06310)
*Xiaoyu Sun,Yang Yang,Xunde Dong*

Main category: eess.SP

TL;DR: The paper proposes a contrastive learning-based ECG pretrained model enhanced by Patient Memory Queue (PMQ) to better exploit patient consistency, and introduces two extra data augmentation methods. Experiments show that the method outperforms previous ones and is more robust with limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Due to the relatively limited amount of labeled data in automatic ECG diagnosis, researchers need to build a robust ECG pretrained model based on unlabeled data.

Method: The proposed method incorporates a large patient memory queue (Patient Memory Queue - PMQ) to mitigate model degeneration from insufficient intra-inter patient samples. Two extra data augmentation methods are also introduced for more perspectives of positive and negative pairs during pretraining.

Result: Extensive experiments conducted on three public datasets with different data ratios indicate that the comprehensive performance of the proposed method surpasses previous contrastive learning methods and shows greater robustness in scenarios with limited labeled data.

Conclusion: The contrastive learning-based ECG pretrained model enhanced by PMQ successfully exploits patient consistency more effectively, leading to superior and more robust performance.

Abstract: In the field of automatic Electrocardiogram (ECG) diagnosis, due to the
relatively limited amount of labeled data, how to build a robust ECG pretrained
model based on unlabeled data is a key area of focus for researchers. Recent
advancements in contrastive learning-based ECG pretrained models highlight the
potential of exploiting the additional patient-level self-supervisory signals
inherent in ECG. They are referred to as patient contrastive learning. Its
rationale is that multiple physical recordings from the same patient may share
commonalities, termed patient consistency, so redefining positive and negative
pairs in contrastive learning as intrapatient and inter-patient samples
provides more shared context to learn an effective representation. However,
these methods still fail to efficiently exploit patient consistency due to the
insufficient amount of intra-inter patient samples existing in a batch. Hence,
we propose a contrastive learning-based ECG pretrained model enhanced by the
Patient Memory Queue (PMQ), which incorporates a large patient memory queue to
mitigate model degeneration that can arise from insufficient intra-inter
patient samples. In order to further enhance the performance of the pretrained
model, we introduce two extra data augmentation methods to provide more
perspectives of positive and negative pairs for pretraining. Extensive
experiments were conducted on three public datasets with three different data
ratios. The experimental results show that the comprehensive performance of our
method outperforms previous contrastive learning methods and exhibits greater
robustness in scenarios with limited labeled data. The code is available at
https://github.com/3hiuwoo/PMQ.

</details>


### [505] [A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration](https://arxiv.org/abs/2506.06311)
*Meiyan Kang,Shizuo Kaji,Sang-Yun Lee,Taegon Kim,Hee-Hwan Ryu,Suyoung Choi*

Main category: eess.SP

TL;DR: This paper presents a novel framework combining Topological Data Analysis (TDA) with YOLOv5 for enhancing the detection of underground utilities from GPR images. It proposes a shape-aware topological representation and uses a Sim2Real strategy to generate synthetic datasets, showing significant improvements in detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Conventional methods for interpreting GPR data are limited by noise sensitivity and lack of structural awareness, motivating the development of a more robust detection system.

Method: The method integrates TDA-derived shape-aware topological features from B-scan GPR images with the spatial detection capabilities of YOLOv5. A novel shape-aware topological representation is proposed to amplify structural features, and a Sim2Real strategy is used to create realistic synthetic datasets.

Result: Experimental results show significant improvements in mean Average Precision (mAP), demonstrating the robustness and efficacy of the approach.

Conclusion: This study highlights the potential of TDA-enhanced learning for reliable, real-time subsurface object detection, with applications in urban planning, safety inspection, and infrastructure management.

Abstract: Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)
technique for subsurface exploration, particularly in infrastructure inspection
and maintenance. However, conventional interpretation methods are often limited
by noise sensitivity and a lack of structural awareness. This study presents a
novel framework that enhances the detection of underground utilities,
especially pipelines, by integrating shape-aware topological features derived
from B-scan GPR images using Topological Data Analysis (TDA), with the spatial
detection capabilities of the YOLOv5 deep neural network (DNN). We propose a
novel shape-aware topological representation that amplifies structural features
in the input data, thereby improving the model's responsiveness to the
geometrical features of buried objects. To address the scarcity of annotated
real-world data, we employ a Sim2Real strategy that generates diverse and
realistic synthetic datasets, effectively bridging the gap between simulated
and real-world domains. Experimental results demonstrate significant
improvements in mean Average Precision (mAP), validating the robustness and
efficacy of our approach. This approach underscores the potential of
TDA-enhanced learning in achieving reliable, real-time subsurface object
detection, with broad applications in urban planning, safety inspection, and
infrastructure management.

</details>


### [506] [An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation](https://arxiv.org/abs/2506.06315)
*Masoud Rahimi,Reza Karbasi,Abdol-Hossein Vahabie*

Main category: eess.SP

TL;DR: An open-source Python framework is introduced for generating synthetic ECG image datasets, advancing deep learning-based tasks in ECG analysis such as digitization, lead detection, and waveform segmentation. Four open-access datasets are produced using the PTB-XL signal dataset.


<details>
  <summary>Details</summary>
Motivation: To advance critical deep learning-based tasks in ECG analysis by providing a tool to generate synthetic ECG image datasets.

Method: The method involves creating an open-source Python framework that uses the PTB-XL signal dataset to produce four different types of open-access datasets for various ECG analysis tasks.

Result: Four open-access datasets were successfully generated: ECG images with time-series signals for digitization, images annotated with bounding boxes for lead detection, and cropped single-lead images with segmentation masks for waveform segmentation in normal and overlapping versions.

Conclusion: An open-source Python framework was developed and used to create valuable datasets for advancing deep learning applications in ECG analysis.

Abstract: We introduce an open-source Python framework for generating synthetic ECG
image datasets to advance critical deep learning-based tasks in ECG analysis,
including ECG digitization, lead region and lead name detection, and
pixel-level waveform segmentation. Using the PTB-XL signal dataset, our
proposed framework produces four open-access datasets: (1) ECG images in
various lead configurations paired with time-series signals for ECG
digitization, (2) ECG images annotated with YOLO-format bounding boxes for
detection of lead region and lead name, (3)-(4) cropped single-lead images with
segmentation masks compatible with U-Net-based models in normal and overlapping
versions. In the overlapping case, waveforms from neighboring leads are
superimposed onto the target lead image, while the segmentation masks remain
clean. The open-source Python framework and datasets are publicly available at
https://github.com/rezakarbasi/ecg-image-and-signal-dataset and
https://doi.org/10.5281/zenodo.15484519, respectively.

</details>


### [507] [Composite Reward Design in PPO-Driven Adaptive Filtering](https://arxiv.org/abs/2506.06323)
*Abdullah Burkan Bereketoglu*

Main category: eess.SP

TL;DR: This paper explores the use of Proximal Policy Optimization (PPO) in adaptive filtering for denoising signals in dynamic environments, showing better performance than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional filters have limitations in dynamic, non-stationary environments due to assumptions of stationarity or the need for complex fine-tuning.

Method: The paper proposes an adaptive filtering framework using PPO, guided by a composite reward balancing SNR improvement, MSE reduction, and residual smoothness.

Result: Experiments demonstrate that the PPO agent generalizes beyond its training distribution, achieving real-time performance and outperforming classical filters.

Conclusion: Policy-gradient reinforcement learning is viable for robust, low-latency adaptive signal filtering.

Abstract: Model-free and reinforcement learning-based adaptive filtering methods are
gaining traction for denoising in dynamic, non-stationary environments such as
wireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman
are limited by assumptions of stationary or requiring complex fine-tuning or
exact noise statistics or fixed models. This letter proposes an adaptive
filtering framework using Proximal Policy Optimization (PPO), guided by a
composite reward that balances SNR improvement, MSE reduction, and residual
smoothness. Experiments on synthetic signals with various noise types show that
our PPO agent generalizes beyond its training distribution, achieving real-time
performance and outperforming classical filters. This work demonstrates the
viability of policy-gradient reinforcement learning for robust, low-latency
adaptive signal filtering.

</details>


### [508] [Uncertainty-Aware Multi-view Arrhythmia Classification from ECG](https://arxiv.org/abs/2506.06342)
*Mohd Ashhad,Sana Rahmani,Mohammed Fayiz,Ali Etemad,Javad Hashemi*

Main category: eess.SP

TL;DR: The paper presents a deep neural architecture for uncertainty-aware multi-view classification of arrhythmia from ECG, which improves performance and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of classifying arrhythmia from ECG signals by leveraging multiple views and accounting for uncertainties due to noise and artifacts.

Method: The method involves learning two different views (1D and 2D) of single-lead ECG, using a time-series module for morphological features, an image-space learning module for spatiotemporal features, and an uncertainty-aware fusion module to combine information from both views.

Result: The experimental results on two real-world datasets show that the framework outperforms state-of-the-art methods in arrhythmia classification and demonstrates greater robustness to noise and artifacts in ECG data.

Conclusion: The proposed deep neural architecture effectively performs uncertainty-aware multi-view classification of arrhythmia from ECG, leading to improved performance and robustness.

Abstract: We propose a deep neural architecture that performs uncertainty-aware
multi-view classification of arrhythmia from ECG. Our method learns two
different views (1D and 2D) of single-lead ECG to capture different types of
information. We use a fusion technique to reduce the conflict between the
different views caused by noise and artifacts in ECG data, thus incorporating
uncertainty to obtain stronger final predictions. Our framework contains the
following three modules (1) a time-series module to learn the morphological
features from ECG; (2) an image-space learning module to learn the
spatiotemporal features; and (3) the uncertainty-aware fusion module to fuse
the information from the two different views. Experimental results on two
real-world datasets demonstrate that our framework not only improves the
performance on arrhythmia classification compared to the state-of-the-art but
also shows better robustness to noise and artifacts present in ECG.

</details>


### [509] [LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines](https://arxiv.org/abs/2506.06346)
*Wei Li,Xiaochun Wu,Xiaoxi Hu,Yuxuan Zhang,Sebastian Bader,Yuhan Huang*

Main category: eess.SP

TL;DR: This paper proposes LD-RPMNet, a lightweight model that combines Transformers and CNNs for railway fault diagnosis, achieving high accuracy with reduced parameters.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient near-sensor diagnosis system for railway applications, specifically for fault detection in railway point machines.

Method: The LD-RPMNet model integrates Multi-scale Depthwise Separable Convolution (MDSC) and Broadcast Self-Attention (BSA) mechanisms to enhance feature extraction and reduce computational complexity.

Result: The model reduces parameters and computational complexity by 50% while improving diagnostic accuracy by nearly 3%, reaching an accuracy of 98.86%.

Conclusion: LD-RPMNet proves effective for near-sensor fault diagnosis in railway point machines, offering potential for practical applications.

Abstract: Near-sensor diagnosis has become increasingly prevalent in industry. This
study proposes a lightweight model named LD-RPMNet that integrates Transformers
and Convolutional Neural Networks, leveraging both local and global feature
extraction to optimize computational efficiency for a practical railway
application. The LD-RPMNet introduces a Multi-scale Depthwise Separable
Convolution (MDSC) module, which decomposes cross-channel convolutions into
pointwise and depthwise convolutions while employing multi-scale kernels to
enhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)
mechanism is incorporated to simplify complex matrix multiplications and
improve computational efficiency. Experimental results based on collected sound
signals during the operation of railway point machines demonstrate that the
optimized model reduces parameter count and computational complexity by 50%
while improving diagnostic accuracy by nearly 3%, ultimately achieving an
accuracy of 98.86%. This demonstrates the possibility of near-sensor fault
diagnosis applications in railway point machines.

</details>


### [510] [Multi-Platform Methane Plume Detection via Model and Domain Adaptation](https://arxiv.org/abs/2506.06348)
*Vassiliki Mancoridis,Brian Bue,Jake H. Lee,Andrew K. Thorpe,Daniel Cusworth,Alana Ayasse,Philip G. Brodrick,Riley Duren*

Main category: eess.SP

TL;DR: The paper highlights the importance of addressing methane emissions and presents machine learning methods to improve methane plume detection across different remote sensing platforms.


<details>
  <summary>Details</summary>
Motivation: Methane is a significant contributor to global warming, and detecting methane plumes from various remote sensing platforms has become increasingly important. There's a need to reconcile differences in data distribution between airborne and spaceborne platforms.

Method: The study uses model- and data-driven machine learning approaches including transfer learning and CycleGAN for image-to-image translation to align data distributions between airborne (AVIRIS-NG) and spaceborne (EMIT) platforms.

Result: Transfer learning refined classifiers trained on airborne imagery outperformed standalone spaceborne models. Using CycleGAN to translate spaceborne data to the airborne domain and applying airborne classifiers yielded the best plume detection results.

Conclusion: This methodology not only aids in methane plume detection but also provides a broader approach to align products from distinct remote sensing instruments.

Abstract: Prioritizing methane for near-term climate action is crucial due to its
significant impact on global warming. Previous work used columnwise matched
filter products from the airborne AVIRIS-NG imaging spectrometer to detect
methane plume sources; convolutional neural networks (CNNs) discerned
anthropogenic methane plumes from false positive enhancements. However, as an
increasing number of remote sensing platforms are used for methane plume
detection, there is a growing need to address cross-platform alignment. In this
work, we describe model- and data-driven machine learning approaches that
leverage airborne observations to improve spaceborne methane plume detection,
reconciling the distributional shifts inherent with performing the same task
across platforms. We develop a spaceborne methane plume classifier using data
from the EMIT imaging spectroscopy mission. We refine classifiers trained on
airborne imagery from AVIRIS-NG campaigns using transfer learning,
outperforming the standalone spaceborne model. Finally, we use CycleGAN, an
unsupervised image-to-image translation technique, to align the data
distributions between airborne and spaceborne contexts. Translating spaceborne
EMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne
classifiers directly yields the best plume detection results. This methodology
is useful not only for data simulation, but also for direct data alignment.
Though demonstrated on the task of methane plume detection, our work more
broadly demonstrates a data-driven approach to align related products obtained
from distinct remote sensing instruments.

</details>


### [511] [Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning](https://arxiv.org/abs/2506.06349)
*Thien Nhan Vo,Thanh Xuan Truong*

Main category: eess.SP

TL;DR: This study compares traditional machine learning and deep learning methods for classifying heartbeats from ECG signals. Hand-crafted features with LightGBM achieved the highest accuracy of 99% and F1 score of 0.94, outperforming image-based CNN approach.


<details>
  <summary>Details</summary>
Motivation: To evaluate and compare the effectiveness of hand-crafted feature-based machine learning models and image-based deep learning models in classifying heartbeats from ECG signals.

Method: The dataset was preprocessed through downsampling, filtering, and normalization. In the first approach, various classifiers such as SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and LightGBM were trained using extracted features like HRV, mean, variance, and RR intervals. The second approach involved transforming ECG signals into images using GAF, MTF, and RP, which were then classified using CNN architectures like VGG and Inception.

Result: LightGBM model achieved the highest performance with an accuracy of 99% and an F1 score of 0.94. Image-based CNN approach had a lower F1 score of 0.85. Models like SVM and AdaBoost showed significantly lower scores.

Conclusion: Hand-crafted features proved more effective in capturing temporal and morphological variations in ECG signals compared to image-based representations. Future work should consider multi-lead ECG signals and temporal dependencies across successive beats.

Abstract: This study addresses the classification of heartbeats from ECG signals
through two distinct approaches: traditional machine learning utilizing
hand-crafted features and deep learning via transformed images of ECG beats.
The dataset underwent preprocessing steps, including downsampling, filtering,
and normalization, to ensure consistency and relevance for subsequent analysis.
In the first approach, features such as heart rate variability (HRV), mean,
variance, and RR intervals were extracted to train various classifiers,
including SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and
LightGBM. The second approach involved transforming ECG signals into images
using Gramian Angular Field (GAF), Markov Transition Field (MTF), and
Recurrence Plots (RP), with these images subsequently classified using CNN
architectures like VGG and Inception.
  Experimental results demonstrate that the LightGBM model achieved the highest
performance, with an accuracy of 99% and an F1 score of 0.94, outperforming the
image-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost
yielded significantly lower scores, indicating limited suitability for this
task. The findings underscore the superior ability of hand-crafted features to
capture temporal and morphological variations in ECG signals compared to
image-based representations of individual beats. Future investigations may
benefit from incorporating multi-lead ECG signals and temporal dependencies
across successive beats to enhance classification accuracy further.

</details>


### [512] [Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study](https://arxiv.org/abs/2506.06360)
*Jiyao Wang,Suzan Ayas,Jiahao Zhang,Xiao Wen,Dengbo He,Birsen Donmez*

Main category: eess.SP

TL;DR: Analyzed key features from ECG,EDA,and RESP signals across four datasets to identify physiological metrics associated with drowsiness using binary logistic regression models.


<details>
  <summary>Details</summary>
Motivation: Accurately detecting drowsiness is vital to driving safety and physiological-signal-based drowsiness monitoring can be more privacy-preserving than a camera-based approach.

Method: Analyzed key features from electrocardiograms(ECG),electrodermal activity(EDA),and respiratory(RESP)signals across four datasets where different drowsiness inducers and assessment methods were used. Binary logistic regression models were built.

Result: Findings indicate that distinct different drowsiness inducers can lead to different physiological responses,objective assessments were more sensitive than subjective ones in detecting drowsiness.The increased heart rate stability,reduced respiratory amplitude,and decreased tonic EDA are robustly associated with increased drowsiness.

Conclusion: The results enhance understanding of drowsiness detection and can inform future generalizable monitoring designs.

Abstract: Accurately detecting drowsiness is vital to driving safety. Among all
measures, physiological-signal-based drowsiness monitoring can be more
privacy-preserving than a camera-based approach. However, conflicts exist
regarding how physiological metrics are associated with different drowsiness
labels across datasets. Thus, we analyzed key features from electrocardiograms
(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four
datasets, where different drowsiness inducers (such as fatigue and low arousal)
and assessment methods (subjective vs. objective) were used. Binary logistic
regression models were built to identify the physiological metrics that are
associated with drowsiness. Findings indicate that distinct different
drowsiness inducers can lead to different physiological responses, and
objective assessments were more sensitive than subjective ones in detecting
drowsiness. Further, the increased heart rate stability, reduced respiratory
amplitude, and decreased tonic EDA are robustly associated with increased
drowsiness. The results enhance understanding of drowsiness detection and can
inform future generalizable monitoring designs.

</details>


### [513] [Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications](https://arxiv.org/abs/2506.06378)
*Charalampos Tsirmpas,Stasinos Konstantopoulos,Dimitris Andrikopoulos,Konstantina Kyriakouli,Panagiotis Fatouros*

Main category: eess.SP

TL;DR: This paper presents a comparative analysis of different methods for EDA signal decomposition and introduces the Feel Transformer, which is effective for separating phasic and tonic components in real-world data.


<details>
  <summary>Details</summary>
Motivation: To extract meaningful emotional and physiological biomarkers from Electrodermal Activity (EDA), it's necessary to decompose EDA into phasic and tonic components. Current methods have limitations, especially with in-the-wild data collected from wearable devices.

Method: The authors compare knowledge-driven, statistical, and deep learning-based methods for EDA signal decomposition. They introduce the Feel Transformer, a novel model adapted from the Autoformer architecture that separates phasic and tonic components without explicit supervision by leveraging pooling and trend-removal mechanisms.

Result: The Feel Transformer achieves a balance between feature fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy, real-world data. It outperforms other methods like Ledalab, cvxEDA, and conventional detrending.

Conclusion: The Feel Transformer shows potential for real-time biosignal analysis and could be applied in stress prediction, digital mental health interventions, and physiological forecasting.

Abstract: Decomposing Electrodermal Activity (EDA) into phasic (short-term,
stimulus-linked responses) and tonic (longer-term baseline) components is
essential for extracting meaningful emotional and physiological biomarkers.
This study presents a comparative analysis of knowledge-driven, statistical,
and deep learning-based methods for EDA signal decomposition, with a focus on
in-the-wild data collected from wearable devices. In particular, the authors
introduce the Feel Transformer, a novel Transformer-based model adapted from
the Autoformer architecture, designed to separate phasic and tonic components
without explicit supervision. The model leverages pooling and trend-removal
mechanisms to enforce physiologically meaningful decompositions. Comparative
experiments against methods such as Ledalab, cvxEDA, and conventional
detrending show that the Feel Transformer achieves a balance between feature
fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,
real-world data. The model demonstrates potential for real-time biosignal
analysis and future applications in stress prediction, digital mental health
interventions, and physiological forecasting.

</details>


### [514] [IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G](https://arxiv.org/abs/2506.06718)
*Omar Mashaal,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: This paper introduces IQFM, the first foundational model for wireless communications operating on raw IQ data. It supports multiple tasks like modulation classification and angle-of-arrival (AoA) with minimal preprocessing. Using self-supervised learning (SSL), it achieves high accuracy in various tasks with very few labeled samples, outperforming supervised methods by significant margins. The model also generalizes well to out-of-distribution tasks.


<details>
  <summary>Details</summary>
Motivation: Foundational models have made significant advancements in fields like natural language processing and computer vision but are underexplored in wireless communications, particularly when working directly with raw IQ data rather than image-based modalities such as CSI or frequency spectrograms.

Method: The method involves creating IQFM, a foundational model for wireless communication using raw IQ data. A task-aware augmentation strategy is introduced that includes core augmentations like cyclic time shifting and task-specific augmentations. This strategy is used within a contrastive SSL framework to pre-train a lightweight encoder on multi-antenna IQ data.

Result: IQFM achieves up to 99.67% accuracy on modulation classification and 65.45% on AoA classification using only one labeled sample per class. When adapted to new tasks with minimal parameter updates via LoRA, it performs competitively with supervised models on tasks like beam prediction, RML2016a modulation classification, and RF fingerprinting.

Conclusion: Raw IQ-based foundational models hold potential as efficient, reusable encoders for multi-task learning in AI-native 6G systems.

Abstract: Foundational models have shown remarkable potential in natural language
processing and computer vision, yet remain in their infancy in wireless
communications. While a few efforts have explored image-based modalities such
as channel state information (CSI) and frequency spectrograms, foundational
models that operate directly on raw IQ data remain largely unexplored. This
paper presents, IQFM, the first I/Q signal foundational model for wireless
communications. IQFM supporting diverse tasks: modulation classification,
angle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy
preprocessing or handcrafted features. We also introduce a task-aware
augmentation strategy that categorizes transformations into core augmentations,
such as cyclic time shifting, and task-specific augmentations. This strategy
forms the basis for structured, task-dependent representation learning within a
contrastive self-supervised learning (SSL) framework. Using this strategy, the
lightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,
achieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,
respectively, using only one labeled sample per class, outperforming supervised
baselines by up to 7x and 145x. The model also generalizes to
out-of-distribution tasks; when adapted to new tasks using only 500 samples per
class and minimal parameter updates via LoRA, the same frozen encoder achieves
94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a
modulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.
96.64%). These results demonstrate the potential of raw IQ-based foundational
models as efficient, reusable encoders for multi-task learning in AI-native 6G
systems.

</details>


### [515] [Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G](https://arxiv.org/abs/2506.06942)
*Mohammad Farzanullah,Han Zhang,Akram Bin Sediq,Ali Afana,Melike Erol-Kantarci*

Main category: eess.SP

TL;DR: The paper proposes a novel framework for channel estimation in cell-free ISAC systems, which integrates CDDM with MMT to leverage sensing information and enhance performance.


<details>
  <summary>Details</summary>
Motivation: Cell-free ISAC is crucial for 6G networks, but channel estimation faces challenges like pilot contamination and noisy estimates.

Method: A new framework combining Conditional Denoising Diffusion Model (CDDM) with Multimodal Transformer (MMT) is introduced. MMT captures relationships between sensing and location data, allowing CDDM to denoise and refine channel estimates iteratively.

Result: Simulation results show significant NMSE improvements over LS and MMSE estimators (8 dB and 9 dB respectively), a 27.8% improvement over TDDM, higher robustness against pilot contamination, and high accuracy under low SNRs.

Conclusion: The proposed model effectively leverages the correlation between sensing and communication channels, performing well especially for users near sensing targets.

Abstract: Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize
6th Generation (6G) networks. By combining distributed access points with ISAC
capabilities, it boosts spectral efficiency, situational awareness, and
communication reliability. Channel estimation is a critical step in cell-free
ISAC systems to ensure reliable communication, but its performance is usually
limited by challenges such as pilot contamination and noisy channel estimates.
This paper presents a novel framework leveraging sensing information as a key
input within a Conditional Denoising Diffusion Model (CDDM). In this framework,
we integrate CDDM with a Multimodal Transformer (MMT) to enhance channel
estimation in ISAC-enabled cell-free systems. The MMT encoder effectively
captures inter-modal relationships between sensing and location data, enabling
the CDDM to iteratively denoise and refine channel estimates. Simulation
results demonstrate that the proposed approach achieves significant performance
gains. As compared with Least Squares (LS) and Minimum Mean Squared Error
(MMSE) estimators, the proposed model achieves normalized mean squared error
(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a
27.8% NMSE improvement compared to the traditional denoising diffusion model
(TDDM), which does not incorporate sensing channel information. Additionally,
the model exhibits higher robustness against pilot contamination and maintains
high accuracy under challenging conditions, such as low signal-to-noise ratios
(SNRs). According to the simulation results, the model performs well for users
near sensing targets by leveraging the correlation between sensing and
communication channels.

</details>


### [516] [Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems](https://arxiv.org/abs/2506.07770)
*Yang Wang,Yin Xu,Cixiao Zhang,Zhiyong Chen,Xiaowu Ou,Mingzeng Dai,Meixia Tao,Wenjun Zhang*

Main category: eess.SP

TL;DR: This letter proposes a channel estimation method for RIS-assisted systems through a novel diffusion model (DM) framework, achieving superior performance over a wide range of SNRs compared to baselines.


<details>
  <summary>Details</summary>
Motivation: To overcome the inherent randomness in the reverse process of conventional DM approaches and reduce the number of parameters of the U-Net.

Method: Reformulate the channel estimation problem as a denoising process aligning with the reverse process of the DM, adopt a deterministic sampling strategy with a step alignment mechanism, and design a lightweight network.

Result: Superior performance over a wide range of SNRs compared to baselines, with performance improvements of up to 13.5 dB in normalized mean square error (NMSE) at SNR = 0 dB. The lightweight network exhibits almost no performance loss compared to the original U-Net while requiring only 6.59% of its parameters.

Conclusion: The proposed channel estimation method achieves superior performance and practicality.

Abstract: This letter proposes a channel estimation method for reconfigurable
intelligent surface (RIS)-assisted systems through a novel diffusion model (DM)
framework. We reformulate the channel estimation problem as a denoising
process, which aligns with the reverse process of the DM. To overcome the
inherent randomness in the reverse process of conventional DM approaches, we
adopt a deterministic sampling strategy with a step alignment mechanism that
ensures the accuracy of channel estimation while adapting to different
signal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of
the U-Net, we meticulously design a lightweight network that achieves
comparable performance, thereby enhancing the practicality of our proposed
method. Extensive simulations demonstrate superior performance over a wide
range of SNRs compared to baselines. For instance, the proposed method achieves
performance improvements of up to 13.5 dB in normalized mean square error
(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost
no performance loss compared to the original U-Net, while requiring only 6.59\%
of its parameters.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [517] [Improving choice model specification using reinforcement learning](https://arxiv.org/abs/2506.06410)
*Gabriel Nova,Sander van Cranenburgh,Stephane Hess*

Main category: econ.GN

TL;DR: An AI model using deep reinforcement learning is developed to automate and improve the process of discrete choice modelling, showing robustness and potential transferability.


<details>
  <summary>Details</summary>
Motivation: Discrete choice modelling is crucial for understanding and forecasting choice behaviour, but current methods are time-consuming, require expertise, and rely on subjective theoretical assumptions. Metaheuristics proposed to assist modellers have limitations such as treating model specification as a classic optimisation problem with static strategies.

Method: A deep reinforcement learning-based framework is introduced where an 'agent' specifies models by estimating them and receiving rewards based on goodness-of-fit and parsimony. The agent dynamically adapts its strategies without prior domain knowledge.

Result: The results show that the agent can dynamically adapt its strategies to identify promising specifications across data generation processes, demonstrating robustness and potential transferability.

Conclusion: This new framework addresses the limitations of current metaheuristics in discrete choice modelling, providing a more efficient and effective approach.

Abstract: Discrete choice modelling is a theory-driven modelling framework for
understanding and forecasting choice behaviour. To obtain behavioural insights,
modellers test several competing model specifications in their attempts to
discover the 'true' data generation process. This trial-and-error process
requires expertise, is time-consuming, and relies on subjective theoretical
assumptions. Although metaheuristics have been proposed to assist choice
modellers, they treat model specification as a classic optimisation problem,
relying on static strategies, applying predefined rules, and neglecting
outcomes from previous estimated models. As a result, current metaheuristics
struggle to prioritise promising search regions, adapt exploration dynamically,
and transfer knowledge to other modelling tasks. To address these limitations,
we introduce a deep reinforcement learning-based framework where an 'agent'
specifies models by estimating them and receiving rewards based on
goodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts
its strategies to identify promising specifications across data generation
processes, showing robustness and potential transferability, without prior
domain knowledge.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [518] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: 研究通过引入对抗性扰动提示，评估了大型语言模型在代码生成任务中的推理能力。结果显示模型对提示的表面特性敏感，并且某些扰动会显著降低或提升模型性能。这表明当前推理系统的脆弱性和不可预测性，需要更原则性的方法来改进。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在复杂推理任务中表现出色，但尚不清楚它们是否真正具备推理能力，还是仅仅依赖浅层统计模式。因此，需要系统地研究这些模型推理能力的鲁棒性。

Method: 研究者设计了一套语义忠实但具有对抗性的提示扰动，并应用于源自LeetCode风格问题的700个扰动代码生成任务中。扰动包括故事重述、无关约束注入、示例重新排序和数值扰动等。

Result: 实验结果表明，某些扰动会严重降低模型性能（准确率下降最高达42.1%），而另一些则意外提升了模型准确率（最高提升35.3%）。

Conclusion: 当前推理系统存在脆弱性和不可预测性，需要更原则性的方法来改进推理对齐和提示鲁棒性。为此，研究者公开了扰动数据集和评估框架以推动进一步研究。

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [519] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: The paper addresses the limitations of current Large Language Model (LLM) unlearning methods, which struggle with real-world generalization due to Form-Dependent Bias. It introduces ORT, a benchmark for evaluating unlearning robustness, and proposes ROCR, a training-free method that redirects dangerous concepts to improve unlearning effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing LLM unlearning methods are ineffective in real-world scenarios because their success depends on the form of training samples and they fail to generalize to different expressions of the same knowledge.

Method: The study introduces two key contributions: ORT, a benchmark to evaluate unlearning methods' robustness against variations in knowledge expression, and ROCR, a training-free unlearning method targeting dangerous concepts by redirecting them to harmless ones.

Result: Experiments reveal that Form-Dependent Bias is prevalent and severe in current unlearning techniques. ROCR significantly enhances unlearning effectiveness compared to traditional methods while maintaining natural output quality.

Conclusion: To achieve practical LLM unlearning, methods must be form-independent. ROCR represents a promising direction towards this goal.

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [520] [Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service](https://arxiv.org/abs/2411.12262)
*Raphael Merx,Adérito José Guterres Correia,Hanna Suominen,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 通过分析10万次翻译请求，发现许多用户（尤其是学生）使用tetun.org将高资源语言翻译成Tetun，主要涉及科学、医疗和日常生活等领域。这与现有的语料库假设不同，表明机器翻译系统应优先关注教育相关领域的准确性。


<details>
  <summary>Details</summary>
Motivation: 低资源机器翻译在社区需求和应用挑战方面尚未被充分理解，传统的调查方法样本量较小，因此需要一种新的研究方法来补充。

Method: 通过对Tetun语言专用机器翻译服务tetun.org的实际使用情况进行观察研究，分析了10万次翻译请求的数据。

Result: 发现用户多为使用移动设备的学生，他们通常将高资源语言翻译成Tetun，涵盖科学、医疗和日常生活等多个领域，而非现有语料库中占主导地位的新闻文章。

Conclusion: 机器翻译系统应优先考虑教育相关领域的准确性，并且观察分析可以更好地满足实际社区需求，推动低资源语言技术的发展。

Abstract: Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of tetun$.$org, a
specialized MT service for the Tetun language, which is the lingua franca in
Timor-Leste. Our analysis of 100,000 translation requests reveals patterns that
challenge assumptions based on existing corpora. We find that users, many of
them students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for institutionalized minority languages like Tetun
should prioritize accuracy on domains relevant to educational contexts, in the
high-resource to low-resource direction. More broadly, this study demonstrates
how observational analysis can inform low-resource language technology
development, by grounding research in practical community needs.

</details>


### [521] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: In this paper, researchers address flaws in the current evaluation framework for GraphRAG methods, proposing an unbiased evaluation framework that generates more relevant questions and reduces biases in assessment. When applied to three representative GraphRAG methods, the new framework reveals much more moderate performance gains than previously reported, emphasizing the need for scientific evaluations in GraphRAG research.


<details>
  <summary>Details</summary>
Motivation: The authors observe two critical flaws in the current answer evaluation framework for GraphRAG: unrelated questions and evaluation biases, which may lead to biased or incorrect conclusions about performance.

Method: They propose an unbiased evaluation framework consisting of graph-text-grounded question generation to produce more related questions and an unbiased evaluation procedure to eliminate biases in LLM-based answer assessment.

Result: When applying the proposed unbiased framework to evaluate three representative GraphRAG methods, it is found that their performance gains are much more moderate than previously reported.

Conclusion: Although the proposed evaluation framework might still have flaws, it highlights the importance of scientific evaluations to establish solid foundations for GraphRAG research.

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [522] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: 提出了一种名为TESU-LLM的新框架，仅使用文本数据训练具备语音能力的语言模型。通过统一编码器将语义等价的文本和语音输入映射到共享潜在空间，并与轻量级投影网络对齐，使模型能够从纯文本监督推广到语音推理。实验表明，TESU-LLM在多个语音相关基准测试中表现优异，媲美使用大规模多模态数据集训练的模型，且无需语音数据，更具可扩展性和高效性。


<details>
  <summary>Details</summary>
Motivation: 现有的语音支持语言模型大多依赖于大规模配对的语音-文本数据和广泛的计算资源，这限制了其可扩展性和可访问性。因此，需要一种新方法来克服这些挑战，利用更少的数据和资源实现高效的语音模型训练。

Method: 设计了一个名为TESU-LLM的框架，核心思想是利用统一编码器将语义等价的文本和语音输入映射到共享潜在空间。然后，通过轻量级投影网络将编码器输出与大型语言模型（LLM）的嵌入空间对齐，从而实现从纯文本监督到语音推理的泛化能力。

Result: 尽管仅基于文本数据进行训练，TESU-LLM在多种语音相关基准测试中表现出色，性能与使用大规模多模态数据集和大量计算资源训练的基线方法相当。

Conclusion: TESU-LLM提供了一种有效且高效的途径，用于构建无需语音数据的语音语言模型，为未来研究提供了可扩展的方向。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [523] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: 为了应对在多个游戏和语言中扩展毒性检测的挑战，本文提出了一种软提示方法和LLM辅助标签转移框架。软提示方法通过加入游戏上下文标记，使单一模型能有效处理多个游戏，并且具有更好的可扩展性；LLM辅助标签转移框架则扩展了对七种额外语言的支持。该方法在真实游戏聊天数据上的评估结果良好，特别是在德语中表现优于英语基准。生产环境中，这种方法显著减少了计算资源和维护开销。


<details>
  <summary>Details</summary>
Motivation: 毒性的检测在游戏社区面临多重挑战，尤其是在扩展到多游戏和多种语言时。同时，在实时环境中，计算效率至关重要。因此，需要一种更高效、更具可扩展性的毒性检测方法。

Method: 1. 提出软提示方法，通过加入游戏上下文标记，使单个模型可以处理多个游戏，其性能与课程学习等更复杂的方法相当，但提供了更好的可扩展性。
2. 开发了一个LLM辅助标签转移框架，使用GPT-4o-mini扩展支持七种额外的语言。

Result: 在真实游戏聊天数据上进行评估，涵盖法语、德语、葡萄牙语和俄语，宏F1分数从32.96%到58.88%不等，其中德语的表现特别出色，超过了英语基准（45.39%）。在生产环境中，该统一方法显著减少了计算资源和维护成本。

Conclusion: 提出的软提示方法和LLM辅助标签转移框架有效地解决了在多游戏和多语言环境下扩展毒性检测的问题，显著降低了计算资源需求和维护成本，并在实际应用中取得了良好的效果。

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [524] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: This paper presents LAC, a new LLM-based Actor-Critic framework that improves decision-making by long-term reasoning and action evaluations.


<details>
  <summary>Details</summary>
Motivation: Despite the success of LLMs in natural language processing tasks, they struggle with complex decision-making requiring long-term reasoning and alignment with high-level objectives. Current methods either rely on short-term auto-regressive action generation or have limitations in accurately simulating rollouts and assessing outcomes.

Method: The paper introduces LAC, which addresses two challenges: extracting robust action evaluations by computing Q-values through token logits associated with outcomes, enhanced by future trajectory rollouts and reasoning; and enabling efficient policy improvement via a gradient-free mechanism.

Result: Experiments in various environments (ALFWorld, BabyAI-Text, WebShop) show the framework's generality and superiority over state-of-the-art methods. Remarkably, it achieves competitive performance using 7B/8B parameter LLMs, even surpassing GPT-4 in some complex tasks.

Conclusion: The integration of structured policy optimization with LLMs' intrinsic knowledge shows potential to enhance decision-making capabilities in multi-step environments.

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [525] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: 提出了一种名为DMPI-PMHFE的双通道特征融合检测框架，用于检测提示注入攻击。该框架结合了预训练语言模型和启发式特征工程，实验结果表明其在准确率、召回率和F1分数上优于现有方法，并且在主流大语言模型中显著降低了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，提示注入攻击成为重要的安全威胁，现有的防御机制在有效性和通用性之间存在权衡问题，因此需要一种高效且适用于各种LLMs的提示注入检测方法。

Method: DMPI-PMHFE框架使用DeBERTa-v3-base作为特征提取器，将输入文本转换为包含上下文信息的语义向量；同时设计基于已知攻击模式的启发式规则，提取攻击中常见的显式结构特征。然后将来自两个通道的特征融合并通过全连接神经网络生成最终预测。

Result: 实验结果表明，在多个基准数据集上，DMPI-PMHFE在准确率、召回率和F1分数上优于现有方法。实际部署后，显著降低了包括GLM-4、LLaMA 3、Qwen 2.5和GPT-4o在内的主流LLMs的攻击成功率。

Conclusion: DMPI-PMHFE通过双通道特征融合方法缓解了仅依赖DeBERTa进行特征提取的局限性，是一种有效的提示注入攻击检测方法。

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [526] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 本文提出了一种利用自然语言处理（NLP）查询数据库并以自然语言返回响应的工作流程，特别适用于战场物联网（IoBT）中的情境感知。通过使用适配边缘设备的大型语言模型（LLMs）和图形数据库，架构能够将自然语言问题转换为数据库查询，并将结果总结回用户。实验显示Llama 3.1在多项指标上表现最佳，并且该方法通过放松与真实代码的完全匹配要求，提高了19.4%的准确性。此工作流为在边缘设备上部署LLMs以实现关键决策中的自然语言交互奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 随着战场物联网（IoBT）的发展，为了增强情境感知能力，需要将来自设备的数据处理成可供消费者按需使用的消费级信息对象。目前缺乏一种有效的方法来利用自然语言处理技术查询数据库并生成自然语言响应，从而满足关键决策的需求。

Method: 提出了一种工作流程，利用自然语言处理（NLP）和大型语言模型（LLMs）进行数据库查询和结果总结。具体包括：使用适配边缘设备的中型LLMs将自然语言问题映射到Cypher数据库查询，并将查询结果以自然语言形式反馈给用户。同时采用图形数据库处理动态连接网络数据。

Result: 在公开的美军多用途传感区域（MSA）数据集上评估了多个中型LLMs，结果显示Llama 3.1（80亿参数）在所有考虑的指标上优于其他模型。此外，提出的两步法通过放松对Cypher查询与真实代码的完全匹配要求，实现了19.4%的准确率提升。

Conclusion: 本研究展示了在边缘设备上部署LLMs以支持自然语言与数据库交互的潜力，为在IoBT环境中进行关键决策提供了新的可能性。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [527] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: DeBoP是一种新的优化范式，专门针对轻量级大语言模型（LwLLMs），通过自动优化其行为表现，在多个复杂任务上显著提升性能，超越GPT-3.5并减少约60%的计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有的轻量级大语言模型在推理和复杂任务处理能力上存在局限性，同时传统的提示优化方法依赖于大量人工或先进的元认知能力，对轻量级模型效果不佳。因此需要一种更有效的优化方法来改善这些模型的表现。

Method: 提出了DeBoP（Direct Behavior Optimization Paradigm），一种基于链式思维提示技术的自动优化方法。该方法将复杂的提示优化问题转化为离散、可量化的执行序列优化，并使用无梯度蒙特卡罗树搜索进行优化。

Result: 实验表明，DeBoP在七个具有挑战性的任务中显著优于其他最新的提示优化方法。经过DeBoP优化的轻量级模型在大多数任务上的表现超过了GPT-3.5，同时计算时间减少了约60%。

Conclusion: DeBoP作为一种新颖的优化范式，成功提升了轻量级大语言模型在复杂任务中的表现，证明了其高效性和潜力。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [528] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: The paper explores safety risks of value-aligned Large Language Models (LLMs), finding they are more likely to generate harmful content due to amplifying aligned values, and proposes in-context alignment methods for improvement.


<details>
  <summary>Details</summary>
Motivation: To address the growing interest in personalized LLMs that align with human values, while identifying and mitigating potential safety concerns associated with these models.

Method: Identify specific safety risks of value-aligned LLMs, investigate psychological principles behind these challenges, and analyze correlations between value alignment and safety risks using a dataset with detailed safety categories.

Result: Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models, exhibit slightly higher risks than other fine-tuned models in traditional safety evaluations, and genuinely generate text according to aligned values, which can amplify harmful outcomes.

Conclusion: This study sheds light on the 'black box' of value alignment in LLMs and suggests in-context alignment methods to improve the safety of value-aligned LLMs.

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [529] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: The paper introduces Soft Modality-Aware Routing (SMAR), a regularization technique for multimodal Mixture of Experts (MoE) models. It uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for building multimodal MoE models either have high training costs or suffer from degraded language capabilities when adapting pretrained models.

Method: Soft Modality-Aware Routing (SMAR) is proposed as a novel regularization technique that employs Kullback Leibler divergence to control the routing probability distributions across modalities, promoting expert specialization while maintaining language capabilities and efficient multimodal performance.

Result: Experiments on visual instruction tuning demonstrate that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance.

Conclusion: SMAR offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models.

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [530] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: Large language models sometimes generate non-canonical token sequences, which can lead to negative consequences. This paper introduces canonical sampling, a method that ensures models only generate canonical token sequences, and shows that this approach generates token sequences closer to the true distribution used during training.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of large language models generating non-canonical token sequences, which can lead to several negative consequences.

Method: The authors first establish a theoretical result that to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process. Based on this, they introduce canonical sampling, a simple and efficient sampling method that prevents a given model from generating non-canonical token sequences.

Result: Canonical sampling precludes the model from generating non-canonical token sequences. Additionally, compared with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.

Conclusion: Canonical sampling is a simple and efficient method to ensure that large language models generate canonical token sequences, thereby improving their performance and fidelity to the training data.

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [531] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: 大型语言模型在上下文记忆冲突中的行为诊断框架


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在上下文信息与参数化信念相悖时的行为表现

Method: 构建引发知识冲突的诊断数据并分析模型在多种任务类型中的表现

Result: (1) 知识冲突对不需要知识利用的任务影响最小，(2) 上下文和参数化知识一致时模型表现更佳，(3) 模型无法完全抑制内部知识，(4) 提供解释冲突的理由会增加对上下文的依赖

Conclusion: 研究结果引发了对基于模型评估的有效性担忧，并强调在部署大型语言模型时需要考虑知识冲突问题

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [532] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 研究人员对比了两个开源微调数据集Tulu-3-SFT-Mix和SmolTalk，并通过Magpie框架对样本进行详细标注，揭示了它们的结构与质量差异。基于此，他们设计了一个新的数据集TuluTalk，在减少14%样本的情况下，性能不降反升。研究结果提供了关于如何有效构建微调数据集的实用见解，并公开发布了所有相关数据。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）的微调数据集大多不公开，且缺乏透明性，尽管使用开源替代方案可以达到接近顶尖模型的性能，但系统性比较仍因计算成本高而难以实现。因此，需要一个全面的分析来理解不同数据集对模型性能的影响。

Method: 使用Magpie框架对Tulu-3-SFT-Mix和SmolTalk数据集中的每个样本进行标注，包括对话轮次结构、任务类别、输入质量及响应质量等指标。基于标注结果，统计并分析两者的结构与质量异同，并据此设计一个新的数据混合策略以生成TuluTalk数据集。

Result: 新数据集TuluTalk在减少14%样本的同时，于关键基准测试中匹配或超越了原数据集的表现，证明了其高效性。此外，研究揭示了影响数据质量的具体因素，如对话轮次结构和任务类别分布等。

Conclusion: 本研究为构建更高效的微调数据集提供了实际指导，同时公开发布所有标注数据和新数据集TuluTalk，支持未来的研究工作。

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [533] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: 当前大型语言模型（LLMs）在处理复杂查询时容易出现意图幻觉现象，即忽略或误解查询中的某些部分。为评估此问题，研究者提出了FAITHQA基准测试和CONSTRAINT SCORE自动评估指标，揭示了意图幻觉的普遍性和成因，并推动未来相关研究。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在应对复杂查询时，往往无法全面满足查询要求，存在忽略或误解查询条件的问题，这促使研究者提出意图幻觉的概念以解决这一不足。

Method: 1. 提出FAITHQA基准测试，包含20,068个问题，覆盖多种主题和难度，适用于查询生成和检索增强生成（RAG）场景。
2. 引入CONSTRAINT SCORE自动评估指标，用于检测意图幻觉。
3. 通过人类评估验证CONSTRAINT SCORE的有效性，证明其比基线方法更接近人类表现。

Result: 1. FAITHQA评估显示意图幻觉是即使是最先进的模型也普遍存在的问题。
2. 意图幻觉主要源于模型对查询内容的遗漏或误解。
3. CONSTRAINT SCORE在检测意图幻觉方面表现出色，与人类判断更为一致。

Conclusion: 意图幻觉是大型语言模型的一个重要问题，需要进一步研究。FAITHQA和CONSTRAINT SCORE为未来的研究提供了有价值的工具和方向。

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [534] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles, which helps generate captions closer to the original author-written ones.


<details>
  <summary>Details</summary>
Motivation: Figure captions are crucial for helping readers understand and remember a figure's key message. However, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization.

Method: LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context.

Result: Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs.

Conclusion: This paper demonstrates the advantage of using multimodal profiles over text-only ones for personalized figure caption generation.

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [535] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: 本论文提出了一个名为\name的端到端框架，用于医疗任务中生成和评估引用的能力。通过引入多轮检索-引用方法，提高了引用的质量，并且展示了其与专业专家标注结果的高度相关性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的医疗问答系统缺乏生成和评估引用的能力，这引发了对其在实际应用中的担忧。

Method: 提出了一种名为\name的框架，以及一种新的多轮检索-引用方法，以生成高质量的引用。

Result: 所提出的方法相较于强大的基线方法，在引用的精确度和召回率上均有显著提升，且评估结果与专业专家的标注结果高度一致。

Conclusion: 本文的工作强调了医疗任务中引用生成的挑战与机遇，并指出了对最终引用质量有重大影响的重要设计选择。

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [536] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: A training-free method using Orthogonal Matching Pursuit (OMP) is introduced to transplant tokenizers in pretrained large language models, allowing for zero-shot preservation of model performance across benchmarks and enabling various applications such as knowledge distillation and domain-specific vocabulary adaptations.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenge of adapting pretrained large language models to new tokenizers without requiring additional training, which can lead to degradation in model performance. Current methods often struggle with tokenizer discrepancies, particularly in preserving mathematical reasoning capabilities.

Method: The method involves reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). This is done by approximating each out-of-vocabulary token as a sparse linear combination of shared tokens, computed in two phases: first, calculating the new token's representation in the donor embedding space with anchor tokens; second, transferring these sparse coefficients into the base model's embedding space.

Result: OMP achieves the best zero-shot preservation of the base model's performance across multiple benchmarks compared to other approaches like zero-init, mean-init, WECHSEL, FOCUS, and ZETT. It effectively bridges large tokenizer discrepancies without gradient updates and identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities.

Conclusion: This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating applications such as cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. The method has been integrated into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment.

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [537] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: The paper explores probabilistic regression using Large Language Models (LLMs) for unstructured inputs, proposing a novel quantile regression approach that enables LLMs to produce full predictive distributions. Experiments show Mistral-7B outperforms traditional methods in price prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for structured prediction tasks with LLMs mainly focus on point estimates and lack systematic comparisons across different methods. There is a need for a method that addresses challenging text-to-distribution prediction tasks such as price estimation, which require nuanced text understanding and uncertainty quantification.

Method: The authors propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. They use a Mistral-7B model fine-tuned with quantile heads and conduct extensive experiments across three diverse price prediction datasets.

Result: The Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations. It consistently surpasses encoder architectures, embedding-based methods, and few-shot learning methods. The LLM-assisted label correction achieves human-level accuracy without systematic bias.

Conclusion: The proposed quantile regression approach using LLMs is effective for probabilistic regression tasks with unstructured inputs. The systematic comparison reveals the superiority of Mistral-7B in price prediction tasks. Curated datasets are made available to support future research.

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [538] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: Detecting LLM-generated text in specialized domains is crucial but challenging due to domain shift. This paper proposes DivScore, a zero-shot detection framework that outperforms current methods in medical and legal fields.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot detectors fail to effectively detect LLM-generated text in specialized domains like medicine and law because of the domain shift issue.

Method: The paper proposes DivScore, a zero-shot detection framework using normalized entropy-based scoring and domain knowledge distillation to identify LLM-generated text in specialized domains. It also introduces a theoretical analysis linking detector failure to KL divergence between different text distributions.

Result: Experiments on a domain-specific benchmark show DivScore consistently outperforms state-of-the-art detectors with 14.4% higher AUROC and 64.0% higher recall at 0.1% false positive rate threshold. In adversarial settings, it achieves 22.8% advantage in AUROC and 29.5% in recall.

Conclusion: DivScore provides a robust solution for detecting LLM-generated text in specialized domains such as medicine and law, significantly improving upon existing methods.

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [539] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: C-PATH is a conversational AI system designed to assist patients in recognizing symptoms and recommending medical departments through natural, multi-turn dialogues. It is powered by large language models (LLMs) and fine-tuned on medical knowledge, dialogue data, and clinical summaries using a multi-stage pipeline built on the LLaMA3 architecture.


<details>
  <summary>Details</summary>
Motivation: Navigating healthcare systems can be complex and overwhelming, creating barriers for patients seeking timely and appropriate medical attention.

Method: C-PATH is powered by large language models (LLMs) and fine-tuned on medical knowledge, dialogue data, and clinical summaries using a multi-stage pipeline built on the LLaMA3 architecture. A GPT-based data augmentation framework transforms structured clinical knowledge into lay-person-friendly conversations. A scalable conversation history management strategy ensures long-range coherence.

Result: Evaluation with GPTScore demonstrates strong performance across dimensions such as clarity, informativeness, and recommendation accuracy. Quantitative benchmarks show that C-PATH achieves superior performance in GPT-rewritten conversational datasets, significantly outperforming domain-specific baselines.

Conclusion: C-PATH represents a step forward in the development of user-centric, accessible, and accurate AI tools for digital health assistance and triage.

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [540] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: LAGAMC是一种新的多标签文本分类模型，通过生成标签描述并结合双目标损失函数，在多个数据集上超越了现有方法，分别在Micro-F1和Macro-F1指标上提高了13.94%和24.85%。


<details>
  <summary>Details</summary>
Motivation: 随着文本数据的激增，手动文档分类变得越来越困难，需要一种高效、通用的多标签文本分类方法来解决这一问题。

Method: 提出了一种基于生成模型的框架，利用预定义的标签描述生成机制，并通过微调的句子变换器将生成的描述与预定义标签匹配。同时，采用双目标损失函数，包括交叉熵损失和余弦相似度，以确保语义对齐和准确性。

Result: LAGAMC模型在所有评估的数据集上均取得了新的最先进性能，显著超越了多个强大的基线模型。

Conclusion: LAGAMC模型因其参数效率和跨数据集的通用性，非常适合实际应用，且在多标签文本分类任务中表现出色。

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [541] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在某些条件下预测可能事件的能力不如随机水平，包括Llama 3、Gemma 2和Mistral NeMo在内的所有测试模型都存在此问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估语言模型是否能够可靠地区分可能事件与不可能事件，并探讨其在可能性、典型性和上下文相关性方面的表现。

Method: 通过分析语言模型对可能事件、典型事件及上下文相关事件的处理能力，对比不同模型（如Llama 3、Gemma 2和Mistral NeMo）在特定条件下的性能表现。

Result: 发现所有测试模型在某些情况下表现低于随机水平，例如对不可能句子赋予更高的概率。

Conclusion: 语言模型在区分可能事件与不可能事件方面的能力并不稳健，需要进一步改进。

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [542] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）在代码生成方面表现出色，但在通过生成测试用例来进行代码检查或调试方面仍有待探索。本文提出了TCGBench基准，用于评估LLMs在生成有效和针对性测试用例生成器方面的能力。实验表明，虽然LLMs能够生成有效的测试用例生成器，但在生成揭示人类代码缺陷的针对性测试用例方面表现不佳。使用高质量的手动整理数据集可以通过提示和微调来提高LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面表现出色，但它们在代码检查或调试中的潜力尚未被充分挖掘，特别是在生成测试用例方面。这促使作者研究LLMs在竞争级编程（CP）任务中生成测试用例生成器的能力。

Method: 提出TCGBench基准，包含两个任务：1) 为给定的CP问题生成有效的测试用例生成器；2) 生成能暴露人类代码错误的针对性测试用例生成器。同时，构建了一个高质量的手动整理数据集，以通过提示和微调提高LLMs的性能。

Result: 实验结果表明，最先进的LLMs通常可以生成有效的测试用例生成器，但在生成揭示人类代码缺陷的针对性测试用例方面存在困难。即使高级推理模型（如o3-mini）也显著低于人类的表现。使用手动整理的数据集后，LLMs的性能有所提升。

Conclusion: TCGBench基准展示了LLMs在生成测试用例生成器方面的潜力和局限性。通过使用高质量的数据集进行提示和微调，可以提高LLMs在此任务上的表现。然而，LLMs在生成针对性测试用例方面仍需改进。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [543] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: The paper explores using large language models with persuasion knowledge to enhance disinformation detection, introducing Persuasion-Augmented Chain of Thought (PCoT) which improves zero-shot classification and outperforms other methods by 15%.


<details>
  <summary>Details</summary>
Motivation: Psychological studies indicate that understanding persuasive fallacies aids in detecting disinformation.

Method: Experimenting with large language models infused with persuasion knowledge, leading to the creation of PCoT approach for disinformation detection.

Result: PCoT outperforms competitive methods by 15% across five LLMs and five datasets.

Conclusion: Persuasion plays a crucial role in enhancing zero-shot disinformation detection.

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [544] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 近期文本摘要技术主要依赖大型语言模型生成简洁的摘要，但这些模型往往无法维持长篇论述结构，尤其是在新闻文章中。本文介绍了一种将论述结构整合到摘要过程中的新方法，并提出一个新的摘要数据集、新的新闻论述模式以及新的算法DiscoSum，以实现结构感知的摘要生成。实验结果表明，该方法在保持叙述保真度和满足结构要求方面非常有效。


<details>
  <summary>Details</summary>
Motivation: 现有的文本摘要技术虽然能够生成简洁的摘要，但在处理新闻文章时，往往无法很好地维持长篇论述结构，影响读者的参与度。因此，需要一种能够更好地整合论述结构的摘要方法。

Method: 构建了一个新的摘要数据集，其中包含不同社交媒体平台上对新闻文章的不同方式的多次摘要；开发了一种新的新闻论述模式来描述摘要结构；设计了一种名为DiscoSum的新算法，该算法采用束搜索技术进行结构感知的摘要生成。

Result: 通过人类和自动评估的结果，证明了该方法在保持叙述保真度和满足结构要求方面的有效性。

Conclusion: 将论述结构整合到摘要过程中可以显著提高摘要的质量，特别是在新闻文章领域。DiscoSum算法为适应不同的风格和结构需求提供了有效的解决方案。

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [545] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: The paper introduces BIS Reasoning 1.0, a Japanese dataset for evaluating belief-inconsistent reasoning in large language models (LLMs). It benchmarks state-of-the-art models revealing significant variance in performance and identifying critical weaknesses when handling belief-conflicting inputs.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capability of large language models in belief-inconsistent reasoning, which is crucial for high-stakes domains such as law, healthcare, and scientific literature.

Method: Developed a large-scale Japanese dataset named BIS Reasoning 1.0 consisting of syllogistic reasoning problems that are logically valid yet belief-inconsistent. Benchmarked state-of-the-art LLMs including GPT models, Claude models, and leading Japanese LLMs on this dataset.

Result: Significant variance in performance among different LLMs was observed, with GPT-4o achieving the highest accuracy at 79.54%. Critical weaknesses were identified in current LLMs when dealing with logically valid but belief-conflicting inputs.

Conclusion: Current LLMs have limitations in handling belief-inconsistent reasoning, which has important implications for their deployment in high-stakes domains where truth must override intuitive belief.

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [546] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 大型推理模型（LRMs）虽然通过生成长链条的思考过程取得优秀的推理效果，但也存在冗长和过度思考的问题。本文通过系统分析推理与非推理模型在token级别上的差异，发现两种未被充分研究的现象：全局错位反弹和局部错位减弱。基于局部错位减弱的发现，提出了一种新的解码方法FoReaL-Decoding，该方法结合快慢思维模型，在保证性能的同时显著减少了计算量和推理长度。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解大型推理模型（LRMs）的行为，作者试图系统地分析推理模型和非推理模型在token级别的差异，并探索这些差异背后的具体现象及可能的应用价值。

Method: 1. 系统分析推理模型和非推理模型在token级别的差异，发现两种关键现象：
   - 全局错位反弹：随着响应长度增加，LRMs与非推理模型的差异持续甚至扩大。
   - 局部错位减弱：错位主要集中在句子开头的“思考提示”部分，随后迅速减弱。
2. 基于局部错位减弱，提出FoReaL-Decoding方法：
   - 使用一个Leading模型生成每个句子的前几个token。
   - 使用较弱的draft模型完成剩余部分。
   - 采用随机门机制平滑插值大小模型之间的输出。

Result: 在四个数学推理基准测试（AIME24, GPQA-Diamond, MATH500, AMC23）上，FoReaL-Decoding方法：
- 将理论FLOPs减少30%到50%。
- 将链条长度缩短至多40%。
- 同时保持86%到100%的模型性能。

Conclusion: FoReaL-Decoding作为一种简单、即插即用的方法，为推理任务中的成本-质量权衡提供了一个可控的解决方案。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [547] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: Multimodal Large Language Models (MLLMs) are effective in general domains but limited in medical applications due to knowledge coverage, hallucinations, and reasoning capabilities. This paper proposes a data curation procedure to build a multimodal dataset with extensive medical knowledge, introduces Lingshu - a medical-specialized MLLM undergoing multi-stage training, explores reinforcement learning for enhanced reasoning, develops MedEvalKit for model evaluation, and evaluates Lingshu's superior performance on medical tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs have limitations in medical applications such as insufficient medical knowledge beyond imaging, susceptibility to hallucinations due to suboptimal data curation, and lack of reasoning capabilities for complex medical scenarios.

Method: The authors propose a comprehensive data curation process that acquires rich medical knowledge from diverse sources including imaging, texts, and general-domain data, synthesizes accurate medical captions, VQA, and reasoning samples, builds a multimodal dataset enriched with medical knowledge, introduces Lingshu which undergoes multi-stage training embedding medical expertise, explores reinforcement learning to enhance reasoning ability, and develops MedEvalKit for unified evaluation.

Result: Lingshu outperforms existing open-source multimodal models on most medical tasks including multimodal QA, text-based QA, and medical report generation.

Conclusion: The proposed medical-specialized MLLM Lingshu, built upon a carefully curated multimodal medical dataset and trained through multiple stages, demonstrates superior performance in various medical tasks, highlighting its potential in advancing medical AI applications.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [548] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在简单常识推理中表现出色，但在复杂和隐性常识推理方面存在困难。为了解决这一问题，本文提出了一个名为Com$^2$的基准测试，专注于复杂常识推理。通过结合因果事件图、因果理论和侦探故事，Com$^2$能够生成符合人类关注的不同场景，并指导LLMs进行深入推理。实验表明，后训练和慢思考可以改善LLMs在推理深度和广度上的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在简单常识推理方面表现优异，但它们在处理复杂和隐性常识知识时遇到了困难，例如理解某些事件的长期影响。这是人类更倾向于关注的方面，因此需要一种专门针对复杂常识推理的基准来填补这一空白。

Method: 首先，将因果事件图作为结构化的复杂常识知识纳入；然后，采用因果理论（如干预）修改因果事件图以生成符合人类关注的不同场景；最后，利用LLMs结合逻辑关系合成示例，同时使用侦探故事构建更具挑战性的子集。

Result: 实验结果表明，LLMs在推理深度和广度上存在不足，但通过后训练和慢思考方法可以缓解这一问题。

Conclusion: 本文提出的Com$^2$基准测试为复杂常识推理提供了一个新的研究方向，实验结果验证了其有效性，同时也指出了LLMs在复杂推理方面的局限性以及改进的可能性。

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [549] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: In this paper, researchers developed a new method called Reinforcement Learning via Self-Confidence (RLSC) which enhances the performance of large language models in reasoning tasks without needing costly human annotations or external reward models. This method was tested on Qwen2.5-Math-7B and showed significant improvements on several math tests.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create an effective post-training method for large language models that can align their behavior with task goals without relying on costly human annotations or external reward models.

Method: The proposed method, Reinforcement Learning via Self-Confidence (RLSC), uses the model's own confidence as reward signals for reinforcement learning, eliminating the need for labels, preference models, or reward engineering.

Result: When applied to Qwen2.5-Math-7B, RLSC improved accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on AMC23.

Conclusion: RLSC provides a simple, scalable post-training approach for reasoning models that requires minimal supervision.

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [550] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: Large Reasoning Models (LRMs) are powerful but often inefficient. This paper introduces reasoning efficiency frontiers and a metric called Reasoning Efficiency Gap (REG) to quantify this inefficiency. It proposes REO-RL, a reinforcement learning method that reduces REG by optimizing token budgets, demonstrating significant improvements in efficiency with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of LRMs which leads to high inference costs and limits practical deployment, and to provide a unified way to measure and improve reasoning efficiency.

Method: Introduced reasoning efficiency frontiers as empirical upper bounds from fine-tuning LRMs. Proposed REG as a metric for efficiency. Developed REO-RL, a reinforcement learning algorithm that minimizes REG by targeting specific token budgets and using numerical integration over selected budgets.

Result: REO-RL consistently reduced REG by >=50% across all evaluated LRMs. It matched Qwen3-4B/8B efficiency frontiers under a 16K token budget with little accuracy loss. Ablation studies confirmed the effectiveness of the exponential token budget strategy.

Conclusion: Fine-tuning LRMs to perfectly align with efficiency frontiers remains an open challenge despite significant improvements achieved by REO-RL.

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [551] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: The paper presents Theorem-of-Thought (ToTh), a framework enhancing LLM reasoning through collaboration of three inference agents, resulting in more reliable and interpretable reasoning chains.


<details>
  <summary>Details</summary>
Motivation: Large language models have strong performance in natural language reasoning tasks but suffer from brittleness and difficulty in interpretation. Existing techniques like Chain-of-Thought lack mechanisms to enforce logical structure and assess internal coherence.

Method: ToTh introduces three parallel agents simulating abductive, deductive, and inductive inference modes. Each agent produces a reasoning trace structured into a formal reasoning graph. Bayesian belief propagation guided by natural language inference assigns confidence scores to each step. The most coherent graph is selected for the final answer.

Result: Experiments on symbolic and numerical reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs. It provides interpretable and logically grounded reasoning chains.

Conclusion: ToTh represents a promising direction for building more robust and cognitively inspired LLM reasoning.

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [552] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: This report investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language models, revealing that its benefits depend heavily on the model and task type. For non-reasoning models, CoT offers small improvements but can introduce errors and requires more tokens. For reasoning-focused models, CoT adds significant cost and time with little accuracy improvement.


<details>
  <summary>Details</summary>
Motivation: To help business, education, and policy leaders understand the technical aspects of working with AI through rigorous testing, specifically focusing on the effectiveness of Chain-of-Thought (CoT) prompting.

Method: Investigating the impact of CoT prompting across different types of tasks and models, analyzing performance improvements, error rates, token usage, and response times.

Result: CoT prompting effectiveness varies by task and model type. Non-reasoning models see minor average performance improvements but increased variability and errors. Reasoning models show marginal gains in accuracy but much higher costs in terms of time and tokens.

Conclusion: CoT prompting is not universally effective; its benefits are task- and model-dependent. It often increases costs significantly without substantial accuracy improvements.

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [553] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 通过结合顺序蒙特卡洛方法和语法标记器，可以在不损害语言模型流畅性的情况下提高生成文本的语法准确性。


<details>
  <summary>Details</summary>
Motivation: 控制语言模型生成文本的句法结构对于需要清晰度、风格一致性或可解释性的应用来说很有价值，但仍然是一个具有挑战性的任务。

Method: 提出了一种基于后验推理的采样算法方法，该方法将顺序蒙特卡洛（sequential Monte Carlo）与语法标记器结合，以确保生成的每个标记都符合所需的句法结构。

Result: 实验表明，使用合适的建议分布，可以将GPT2-large和Llama3-8B模型的F1分数从12.31和35.33提高到约93，同时保持语言模型的流畅性。

Conclusion: 结果强调了句法控制的复杂性以及采样算法的有效性，为需要精确句法控制的应用提供了一种有前途的方法。

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [554] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: This Ph.D. dissertation explores Instance Selection (IS) techniques in NLP, revealing significant potential for reducing training set sizes while maintaining model effectiveness and reducing training costs.


<details>
  <summary>Details</summary>
Motivation: Training large language models for specific applications typically requires substantial computing resources. The under-investigated technique of Instance Selection (IS) has enormous potential to reduce training set size by removing noisy or redundant instances without sacrificing model effectiveness.

Method: The study provides a comprehensive comparison of IS methods applied to Automatic Text Classification (ATC), considering various classification solutions and datasets. Two novel IS solutions are proposed, focusing on noise and redundancy specifically designed for large datasets and transformer architectures.

Result: The final solution achieved an average 41% reduction in training sets across all datasets while maintaining the same levels of effectiveness. Additionally, the solutions demonstrated speedup improvements of 1.67x (up to 2.46x), making them scalable for very large datasets.

Conclusion: Instance Selection holds great promise for reducing computational requirements in NLP tasks, particularly for large datasets and transformer-based models.

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [555] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: Affine mappings between residual streams of language models can effectively transfer features and improve training efficiency of SAEs.


<details>
  <summary>Details</summary>
Motivation: To find a cheap way to effectively transfer represented features between different sized models, allowing expensive components like Sparse Autoencoders (SAEs) to be trained on smaller models and then transferred to larger ones, saving computational resources.

Method: Applying affine mappings between residual streams of language models to transfer the weights of SAEs, comparing representations across models of different sizes, analyzing the transferability of probes and steering vectors, and examining feature-level transferability.

Result: Small and large models learn highly similar representation spaces; transferred SAEs can lead to 50% cheaper training runs; transferred probes and steering vectors can recover ground truth performance; semantic and structural features transfer differently while functional features have their roles faithfully mapped.

Conclusion: The findings demonstrate similarities and differences in linear representation spaces of small and large models and provide a method for enhancing the training efficiency of SAEs.

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [556] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: A new benchmark VISE is proposed to evaluate sycophancy in Video-LLMs, analyzing its manifestations and exploring key-frame selection as a mitigation strategy.


<details>
  <summary>Details</summary>
Motivation: There is a lack of systematic benchmarks and evaluations for understanding sycophantic behavior in Video-LLMs when faced with misleading user input.

Method: Proposed VISE benchmark evaluates sycophancy across diverse question formats, prompt biases, and visual reasoning tasks. It incorporates linguistic perspectives and explores key-frame selection as a mitigation strategy.

Result: VISE enables fine-grained analysis of sycophancy types and interaction patterns, revealing potential paths for reducing bias by strengthening visual grounding.

Conclusion: VISE addresses the gap in evaluating sycophancy in Video-LLMs and provides insights into mitigation strategies.

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [557] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: Recent advancements in LLMs have improved Text-to-SQL performance, but they are limited by static database information. SDE-SQL addresses this by enabling self-driven exploration of databases through generating and executing SQL probes, improving execution accuracy on the BIRD benchmark.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of LLMs relying on static, pre-processed database information which restricts their ability to fully understand the database contents.

Method: SDE-SQL framework allows large language models to perform self-driven exploration of databases during inference by generating and executing SQL probes to actively retrieve information from the database and iteratively update its understanding of the data.

Result: SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline on the BIRD benchmark. With supervised fine-tuning (SFT), there is an additional 0.52% improvement.

Conclusion: SDE-SQL establishes a new state-of-the-art among methods based on open-source models without SFT or model ensembling, and its performance can be further enhanced with SFT.

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [558] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: Code-switching poses challenges for syntactic analysis, especially in low-resource languages. This paper introduces BiLingua Parser, an LLM-based annotation pipeline that produces UD annotations for code-switched text. It achieves up to 95.29% LAS after expert revision.


<details>
  <summary>Details</summary>
Motivation: Code-switching is a complex challenge for syntactic analysis, particularly in low-resource language settings where annotated data is scarce. Existing parsers trained on monolingual treebanks often fail to generalize to multilingual and mixed-language input.

Method: The authors developed a prompt-based framework for Spanish-English and Spanish-Guaraní data, combining few-shot LLM prompting with expert review. They also released two annotated datasets, including the first Spanish-Guaraní UD-parsed corpus, and conducted a detailed syntactic analysis of switch points across language pairs and communicative contexts.

Result: Experimental results show that BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly outperforming prior baselines and multilingual parsers.

Conclusion: LLMs, when carefully guided, can serve as practical tools for bootstrapping syntactic resources in under-resourced, code-switched environments.

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [559] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: Reward modeling is vital for aligning language models with human values. This paper explores reward model interpretability by analyzing their responses comprehensively, revealing heterogeneity among models, asymmetries in scoring, sensitivity to prompt framing, and biases towards frequent tokens and identity groups.


<details>
  <summary>Details</summary>
Motivation: To better understand and address the limitations of reward models used in fine-tuning generative models, as they directly encode human value judgments.

Method: Examine how different reward models score every possible single-token response to value-laden prompts across their entire vocabulary space.

Result: Uncovered heterogeneity between models trained on similar objectives, asymmetries in scoring high vs low tokens, sensitivity to prompt framing akin to human cognitive biases, and overvaluation of more frequent tokens.

Conclusion: Reward models are not interchangeable and may encode concerning biases that could propagate through downstream large language models.

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [560] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: This paper introduces Sparse Autoencoder Role-Playing Steering (SRPS), a framework that manipulates internal model features for role-playing in LLMs, offering improved interpretability and stability over prompt-based methods. Experiments show consistent reasoning performance gains.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing prompt engineering techniques for role-playing in LLMs, which lack stability and interpretability.

Method: SRPS extracts latent representations from role-play prompts, selects relevant features based on activation patterns, and constructs a steering vector to control role-specific behavior with adjustable intensity.

Result: Experiments across various benchmarks and model sizes demonstrate consistent performance improvements. Notably, accuracy improvements are observed in zero-shot chain-of-thought settings for different models.

Conclusion: SRPS enhances reasoning abilities in LLMs while providing better interpretability and stability compared to traditional prompt-based role-playing.

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [561] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 本研究提出了一种通过强化学习（RL）让问答（QA）智能体学会提出澄清性问题的方法，与现有方法相比，在优化奖励和语言质量方面都取得了进步。


<details>
  <summary>Details</summary>
Motivation: 当前的问答系统虽然能够自动回答用自然语言提出的问题，但在面对模糊或不完整信息时，缺乏主动提出澄清性问题的能力，这限制了系统的交互性和准确性。

Method: 通过模拟包含澄清性问题的对话，并使用强化学习从中学习。为了使强化学习在实际应用中可行，提出了离线强化学习目标，这些目标可以看作是奖励加权的监督微调（SFT），并可以在大型语言模型中轻松优化。这种方法不同于基于SFT和直接偏好优化的最近提出的其他方法，后者具有额外的超参数且不直接优化奖励。

Result: 与基于SFT和直接偏好优化的方法相比，实验结果表明该方法在优化奖励和语言质量方面均有显著提升。

Conclusion: 提出了一种新的方法，使得问答智能体可以通过强化学习学会提出澄清性问题，提高了系统的交互性和准确性。此方法在实践中更为可行，并优于现有的SFT和直接偏好优化方法。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [562] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 提出了一种名为Reinforcement UnLearning (RULE)的新框架，通过优化拒绝边界来高效地从大型语言模型中删除特定信息，仅使用12%的遗忘集和8%的合成边界数据，就比现有方法提高了17.5%的遗忘质量和16.3%的自然响应度，同时保持了总体效用。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在大规模、未经筛选的数据集上的广泛部署，人们越来越关注模型中可能包含敏感、受版权保护或非法内容的问题，因此对选择性删除模型中特定信息的需求日益增加。然而，现有的方法通常依赖于大规模的遗忘和保留数据集，并且存在不自然的响应、泛化能力差或灾难性的效用损失等问题。

Method: 提出了一种新的框架——强化非学习（RULE），将非学习任务定义为一个拒绝边界优化问题。该方法使用小部分的遗忘集和合成边界查询进行训练，采用可验证的奖励函数，鼓励对与遗忘相关的查询进行安全拒绝，同时保留对许可输入的帮助性响应。

Result: 实验结果表明，仅使用12%的遗忘集和8%的合成边界数据，RULE在遗忘质量上优于现有基线高达17.5%，在自然响应度上高出16.3%，同时保持了整体效用。此外，RULE还提高了模型输出的自然性，增强了训练效率，并展现出强大的泛化能力，能够将拒绝行为推广到语义相关但未见过的查询。

Conclusion: 研究表明，RULE在实现有针对性的非学习方面是有效的，并且不会损害模型的整体效用。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [563] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: PiFi框架结合了大型语言模型（LLMs）和小型语言模型（SLMs）的优点，通过将LLM的一个冻结层整合到SLM中，并对组合模型进行特定任务的微调，从而在保持计算效率的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型具有广泛的语言知识和强大的泛化能力，但计算需求高；小型语言模型计算效率高，但泛化能力有限。为弥补这一差距，需要一种既能保持高效又能提升性能的解决方案。

Method: 提出PiFi框架，将一个来自LLM的冻结层集成到SLM中，然后对组合模型进行微调以适应特定任务。这样可以在不显著增加计算成本的情况下提高性能。

Result: PiFi在包括自然语言理解和生成在内的多种自然语言处理任务中表现出一致的性能提升，并能有效利用LLM的知识，增强对未见领域的泛化能力以及语言能力的迁移。

Conclusion: PiFi框架成功地结合了LLM的强大泛化能力和SLM的计算效率，在多个NLP任务上实现了性能改进，同时保持了较低的计算成本。

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [564] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) need alignment with human preferences to generate appropriate content. The paper proposes a novel framework, Weak-to-Strong Decoding (WSD), which uses a small aligned model to guide the large base model in generating high-quality and aligned content. A new dataset GenerAlign is collected for fine-tuning the small model.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the observation that the difficulty of generating aligned responses mainly occurs at the beginning of decoding. Current low-resource methods for aligning LLMs face challenges in producing both high-quality and aligned content.

Method: The proposed method is called Weak-to-Strong Decoding (WSD). It involves a small aligned model first creating well-aligned beginnings of responses, followed by a large base model continuing the rest. This process is controlled by an auto-switch mechanism. Additionally, a new dataset named GenerAlign is collected to fine-tune a small-sized Pilot-3B model as the draft model within the WSD framework.

Result: The WSD framework effectively enhances different base models, outperforming all baseline methods while avoiding degradation on downstream tasks (alignment tax). Extensive experiments confirm the impact of different settings, time efficiency, and provide insights into the intrinsic mechanisms of WSD.

Conclusion: The paper concludes that the WSD framework successfully improves the alignment ability of base models using guidance from a small aligned model. The collection of the GenerAlign dataset contributes to enhancing the performance of the WSD framework.

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [565] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: The paper proposes a TF-IDF-based sentence ranking method for efficient long document classification, reducing input size and inference latency while maintaining near-identical accuracy with MahaBERT-v2.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models like BERT face challenges in long document classification due to fixed input lengths, quadratic attention complexity, and redundancy in using the entire document.

Method: A TF-IDF-based sentence ranking method is developed to select the most informative sentences through fixed-count or percentage-based selection. An enhanced scoring strategy combines normalized TF-IDF scores with sentence length.

Result: The method outperforms baselines on the MahaNews LDC dataset, achieving near-identical classification accuracy with MahaBERT-v2 while reducing input size by over 50% and inference latency by 43%.

Conclusion: Significant context reduction is possible without sacrificing performance, making the method suitable for real-world long document classification tasks.

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [566] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 本研究提出了一种名为KScope的分层统计测试框架，用于分析大语言模型（LLM）在不同知识状态下的表现，并通过四个数据集对九个LLM进行了系统评估，揭示了支持上下文、特征偏好及上下文总结等因素对LLM知识更新的影响。


<details>
  <summary>Details</summary>
Motivation: 目前对于大语言模型知识的研究主要集中在知识冲突情境下模型的表现，而这种研究方法并不能全面反映模型对问题答案的实际掌握程度。

Method: 首先引入了一个基于LLM知识模式的一致性和正确性的五种知识状态分类法，接着提出了KScope——一个分层的统计测试框架，该框架逐步细化关于知识模式的假设，并将LLM的知识状态归类为这五种之一。

Result: (1) 支持性上下文能够缩小模型之间的知识差距。(2) 与难度、相关性和熟悉度相关的上下文特征驱动了成功的知识更新。(3) LLM在部分正确或存在冲突时表现出相似的特征偏好，但在持续错误时则表现出显著差异。(4) 受特征分析约束的上下文总结以及增强的可信度进一步提高了更新效果，并且适用于不同的LLM。

Conclusion: KScope提供了一种有效的方法来表征和理解LLM的知识状态，支持性上下文和特定的上下文特征有助于提高LLM的知识更新效果，同时上下文总结和可信度增强可以进一步改善这一过程。

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [567] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: The paper introduces CCI4.0, a large bilingual pre-training dataset with high quality and diverse reasoning patterns. It includes two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. A novel pipeline is proposed for data quality assurance. Empirical evaluations show that LLMs pre-trained on CCI4.0 perform better in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: To create a large-scale bilingual pre-training dataset with superior data quality and diverse human-like reasoning trajectory to improve the performance of LLMs.

Method: CCI4.0 is composed of carefully curated Chinese and English corpora, along with diverse sources from math, wiki, arxiv, and code. A novel pipeline is proposed for data quality assurance through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. 4.5 billion CoT templates are extracted using staged CoT extraction.

Result: LLMs pre-trained on CCI4.0 benefit from cleaner, more reliable training signals, leading to consistent improvements in downstream tasks, especially in math and code reflection tasks.

Conclusion: Rigorous data curation and human thinking templates play critical roles in advancing LLM performance. The study provides insights into automatically processing pretraining corpora.

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [568] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: SELT is a new framework that improves LLMs' reasoning ability in complex tasks by modifying MCTS, showing significant improvements without task-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Although LLMs have been successful in many areas, their performance in complex reasoning tasks is still limited.

Method: SELT uses a modified Monte Carlo Tree Search to enhance LLM reasoning. It redefines Upper Confidence Bound scoring based on LLMs' self-evaluation capabilities and breaks down the inference process into smaller subtasks with semantic clustering at each node.

Result: SELT shows significant improvements in answer accuracy and reasoning robustness on challenging benchmarks like MMLU and Seal-Tools compared to baseline methods.

Conclusion: SELT enhances LLM reasoning without task-specific fine-tuning, demonstrating strong generalizability across diverse reasoning tasks.

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [569] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: 尽管大语言模型（LLMs）很受欢迎，但在上下文感知的机器翻译中的应用尚待深入研究。本文综述了使用LLM进行上下文感知翻译的相关文献，探讨了提示和精调等方法，并指出商业LLM表现优于开源LLM，同时提出了一些未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在上下文感知机器翻译领域的应用尚未被充分研究，因此需要对现有工作进行总结，以明确当前进展及未来可能的发展方向。

Method: 通过文献回顾，分析现有的上下文感知翻译方法，包括利用提示和精调技术，并比较商业LLM与开源LLM的表现差异。

Result: 发现商业LLM比开源LLM表现更好，基于提示的方法是评估翻译质量的良好基准。

Conclusion: 本文总结了上下文感知翻译的研究现状，指出了自动后编辑和创建翻译代理等较少关注的领域，并提出了值得探索的未来方向。

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [570] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: Large Language Models are effective but full fine-tuning is expensive. LoRA improves efficiency but uses additive updates. This paper proposes LoRMA, which uses matrix multiplicative transformations instead, and addresses related challenges.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of adapting Large Language Models to downstream tasks while overcoming limitations of existing methods like LoRA.

Method: Propose Low-Rank Multiplicative Adaptation (LoRMA) that uses matrix multiplicative transformations instead of additive updates. Address computational complexity and rank bottleneck issues with operation re-ordering and rank inflation strategies.

Result: Through extensive experiments, demonstrate the effectiveness of LoRMA using various evaluation metrics.

Conclusion: LoRMA provides a novel approach to adapting Large Language Models efficiently.

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [571] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: The paper presents PolitiSky24, the first stance detection dataset for the 2024 U.S. presidential election from Bluesky, focusing on Kamala Harris and Donald Trump with 16,044 user-target stance pairs. It combines advanced information retrieval and LLMs to generate stance labels with supporting rationales, achieving 81% accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing datasets primarily focus on tweet-level stances from established platforms, lacking user-level stance resources, especially on emerging platforms like Bluesky.

Method: The dataset was created using a pipeline combining advanced information retrieval and large language models (LLMs) that generates stance labels along with supporting rationales and text spans for transparency.

Result: The labeling approach achieves 81% accuracy with scalable LLMs, addressing gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective.

Conclusion: PolitiSky24 provides a valuable resource for stance detection research, offering a more holistic view by considering users' complete posting histories rather than isolated posts.

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [572] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: 为了应对内容审核的需求，研究人员对Twitch的自动审核工具(AutoMod)进行了审计。通过创建测试账户并使用Twitch的API发送超过107,000条评论，研究发现AutoMod未能标记大量仇恨言论（高达94%），且过度依赖俚语作为审核信号。同时，它错误地封锁了高达89.5%的良性示例，显示出其在理解语境方面的不足。


<details>
  <summary>Details</summary>
Motivation: 实时互动平台如Twitch上的评论等新形式增加了对低延迟内容审核系统的需求，但这些系统的有效性尚不明确。

Method: 研究人员创建了隔离的测试账户，并通过Twitch的API向直播聊天发送超过107,000条评论进行审计，这些评论来自四个数据集。

Result: 实验表明，大量的仇恨信息（最高达94%）未被AutoMod标记；添加俚语后，删除率可达100%，显示其过度依赖俚语作为审核依据。此外，AutoMod还错误地标记了大量无害内容（高达89.5%）。

Conclusion: AutoMod存在较大的功能缺口，强调了自动审核系统需要更有效地理解上下文的重要性。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [573] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: The paper introduces FAST, a novel training method for SAEs tailored to instruct models, enhancing reconstruction quality and feature interpretability in LLMs. It outperforms baselines on Qwen2.5-7B-Instruct and Llama3.2-3B-Instruct.


<details>
  <summary>Details</summary>
Motivation: Existing SAE training methods are primarily designed for base models, leading to reduced reconstruction quality and interpretability when applied to instruct models.

Method: FAST (Finetuning-Aligned Sequential Training) is proposed, aligning the training process with the data distribution and activation patterns characteristic of instruct models.

Result: On Qwen2.5-7B-Instruct, FAST achieves a mean squared error of 0.6468 in token reconstruction, significantly lower than baseline methods. For feature interpretability, FAST yields a higher proportion of high-quality features, with 21.1% scoring in the top range for Llama3.2-3B-Instruct compared to 7.0% and 10.2% for other methods.

Conclusion: FAST improves both reconstruction and feature interpretability in instruct models. Intervening on activations of special tokens via SAEs can improve output quality, suggesting new opportunities for fine-grained control.

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [574] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: 通过从数学推理中生成问题解决代码来提取结构信息，并用结构化解决方案指导数据生成，从而提高LLM的推理能力。在MATH和GSM8K上的实验表明，随着推理长度的增加，模型性能下降。使用所提出的训练数据进行微调实验的结果验证了我们数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 数学推理对于LLM来说仍然具有挑战性，因为需要复杂的逻辑和精确的计算。现有的方法通过重新表述问题来合成数据集以增强LLM推理，但这些问题生成的质量和复杂度存在问题。

Method: 提出了一种从数学推理中提取结构信息的方法，利用生成的问题解决代码，并用结构化的解决方案指导数据生成。

Result: 在自建基准上，结果表明模型性能随着推理长度的增加而下降。对一系列LLM进行微调实验的结果验证了所提出的数据集的有效性。

Conclusion: 希望所提出的方法和数据集能够为未来提升LLM推理能力的研究做出贡献。

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [575] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: The paper introduces GaRAGe, a large RAG benchmark for evaluating LLMs' ability to identify relevant grounding when generating answers. It contains 2366 questions and over 35K annotated passages. Evaluations show that state-of-the-art LLMs tend to over-summarise rather than ground their answers strictly on the relevant passages or deflect when no relevant grounding is available.


<details>
  <summary>Details</summary>
Motivation: To create a benchmark that allows fine-grained evaluation of whether LLMs can identify relevant grounding when generating RAG answers, reflecting real-world use cases.

Method: Developed GaRAGe, a benchmark with human-curated long-form answers and annotations of each grounding passage. It includes diverse questions and annotated passages from private document sets and the Web.

Result: State-of-the-art LLMs evaluated on GaRAGe tend to over-summarise, reaching at most 60% Relevance-Aware Factuality Score and 31% true positive rate in deflections. Attribution F1 score is at most 58.9%. Performance drops when answering time-sensitive questions or using sparser private grounding sources.

Conclusion: GaRAGe provides an ideal test bed to evaluate LLMs' abilities to identify relevant information or provide deflective responses. Current models struggle with strict grounding and appropriate deflection.

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [576] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: The paper proposes AbstraL, a method using reinforcement learning to promote abstract reasoning in large language models (LLMs), which significantly mitigates performance degradation on GSM perturbation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Recent studies indicate that LLMs, particularly smaller ones, lack robustness in reasoning when facing distribution shifts, such as changes in variables or distracting clauses. To address this issue, while generating synthetic data is one strategy, the authors focus on 'abstracting' reasoning problems to counteract these shifts and connect with symbolic tools for solutions.

Method: The method involves using reinforcement learning (RL) to acquire the abstraction process of reasoning problems rather than relying solely on supervised fine-tuning, which often fails to produce faithful abstractions. This approach aims to promote abstract reasoning in LLMs.

Result: AbstraL significantly reduces performance degradation on recent GSM perturbation benchmarks, showing its effectiveness in enhancing robustness in reasoning for LLMs.

Conclusion: Abstraction through reinforcement learning is more effective than supervised fine-tuning in promoting abstract reasoning in LLMs, leading to better performance stability under distribution shifts.

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [577] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: The paper introduces MultiMatch, a novel SSL algorithm that combines co-training and consistency regularization with pseudo-labeling, featuring a three-fold pseudo-label weighting module. It achieves SOTA results on 9 out of 10 setups from 5 NLP datasets and shows exceptional robustness in imbalanced settings.


<details>
  <summary>Details</summary>
Motivation: To improve the performance and robustness of semi-supervised learning algorithms by enhancing the pseudo-labeling process.

Method: MultiMatch is a SSL algorithm that integrates heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch into a three-fold pseudo-label weighting module.

Result: MultiMatch achieves state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 19 methods. It also demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%.

Conclusion: MultiMatch enhances and unifies existing SSL techniques, resulting in superior performance and robustness in both balanced and imbalanced data settings.

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [578] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: MEMOIR是一种新的可扩展框架，通过残差记忆注入知识，同时保留预训练模型的核心能力。它在可靠性和泛化性指标上达到最先进的性能，并能扩展到数千个顺序编辑，几乎不会忘记以前的知识。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在实际应用中需要后验更新以包含新知识或纠正错误知识，但现有方法要么影响泛化能力，要么干扰过去的编辑，或者无法扩展到长编辑序列。

Method: MEMOIR通过一个专门的参数模块（即残差记忆）注入知识，同时保持预训练模型的核心能力。通过对样本依赖的掩码稀疏化输入激活，将每次编辑限制在记忆参数的不同子集中，从而最小化编辑之间的干扰。在推理时，通过比较新查询的稀疏激活模式与编辑期间存储的模式，识别相关编辑。

Result: 在问答、幻觉校正和分布外泛化基准测试中，使用LLaMA-3和Mistral进行实验，结果表明MEMOIR在可靠性、泛化性和局部性指标上达到了最先进的性能，并且可以扩展到数千个顺序编辑，几乎不会忘记以前的知识。

Conclusion: MEMOIR提供了一种有效的解决方案，可以在不重新训练或忘记先前信息的情况下对语言模型进行编辑，适用于大规模的实际应用场景。

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [579] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: This paper presents MiniCPM4, a compact and efficient large language model designed for end-side devices. It introduces innovations in model architecture (InfLLM v2), training data (UltraClean and UltraChat v2), training algorithms (ModelTunnel v2, chunk-wise rollout, and BitCPM), and inference systems (CPM.cu). MiniCPM4 is available in two versions (0.5B and 8B parameters) and outperforms similar-sized open-source models on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To create a highly efficient large language model suitable for end-side devices, which can handle long-context processing effectively and support diverse applications.

Method: 1. **Model Architecture**: Developed InfLLM v2, a trainable sparse attention mechanism that accelerates prefilling and decoding phases.
2. **Training Data**: Proposed UltraClean for filtering and generating pre-training data, and UltraChat v2 as a supervised fine-tuning dataset.
3. **Training Algorithms**: Introduced ModelTunnel v2 for efficient pre-training strategy search, chunk-wise rollout for load-balanced reinforcement learning, and BitCPM for data-efficient ternary LLMs.
4. **Inference Systems**: Created CPM.cu to integrate sparse attention, model quantization, and speculative sampling for efficient prefilling and decoding.

Result: MiniCPM4 outperforms similar-sized open-source models across multiple benchmarks, with notable speed improvements over Qwen3-8B when processing long sequences. The model successfully powers diverse applications such as trustworthy survey generation and tool use with model context protocol.

Conclusion: MiniCPM4 demonstrates both efficiency and effectiveness, making it suitable for various on-device requirements and applications.

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


### [580] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: Modern language models have a problem where they assign probability to invalid token strings. This paper proposes two methods to fix this issue, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Current language models assign nonzero probability to many invalid (noncanonical) token encodings which never appear in training data, leading to erroneous and wasteful probability mass distribution.

Method: Two approaches are proposed: 1) canonicality by conditioning, using test-time inference strategies without extra training; 2) canonicality by construction, parameterizing the model to ensure only canonical outputs through training.

Result: Fixing canonicality mistakes enhances the likelihood of held-out data across several models and corpora.

Conclusion: Enforcing canonicality in token-level language models can avoid erroneous and wasteful probability mass allocation, thereby improving model performance.

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [581] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: 尽管训练数据、架构和供应商的多样性被认为是缓解大型语言模型（LLM）同质化的关键，但不同LLM之间是否具有实质性差异尚缺乏实证证据。通过对超过350个LLM的大规模实证评估，研究发现模型错误存在显著相关性，特别是在更准确的大型模型中，即使架构和供应商不同，错误也高度相关。这种相关性在两个下游任务（LLM-as-judge评估和招聘）中产生了影响，反映了算法单一种群的理论预测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过大规模实证分析，探讨不同LLM之间的实质性差异及其错误相关性的驱动因素，以验证多样性是否能有效缓解模型同质化问题。

Method: 对超过350个LLM进行大规模实证评估，使用两个流行的排行榜数据集和一个简历筛选任务，分析模型错误的相关性，并识别导致这种相关性的因素。

Result: 发现模型错误存在显著相关性，在一个排行榜数据集中，当两个模型都出错时，它们的判断一致率达到60%。研究表明，共享架构和供应商是驱动模型相关性的因素，而更大、更准确的模型即使在不同架构和供应商的情况下，错误也高度相关。

Conclusion: LLM之间的错误相关性显著，特别是高性能模型，这表明单纯依靠多样性可能不足以缓解模型同质化问题。这种相关性在实际应用中可能导致潜在风险，例如在招聘等场景中反映算法单一种群的影响。

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [582] [Fast Geometric Embedding for Node Influence Maximization](https://arxiv.org/abs/2506.07435)
*Alexander Kolpakov,Igor Rivin*

Main category: cs.SI

TL;DR: 在大规模图上计算如中介性和接近性等经典中心性度量在计算上非常昂贵。本文提出了一种高效的力导向布局算法，将图嵌入到低维空间中，其中从原点出发的径向距离作为各种中心性度量的代理。我们在多个图族上评估了我们的方法，并展示了与度、PageRank和路径基础中心性的强烈相关性。作为一种应用，所提出的嵌入方法能够找到网络中的高影响力节点，并为标准贪婪算法提供了一个快速且可扩展的替代方案。


<details>
  <summary>Details</summary>
Motivation: 计算大规模图上的经典中心性度量（例如中介性和接近性）在计算上非常昂贵。需要一种更高效的方法来估算这些中心性度量。

Method: 引入了一种高效的力导向布局算法，该算法将图嵌入到低维空间中，其中从原点出发的径向距离作为各种中心性度量的代理。

Result: 在多个图族上展示了与度、PageRank和路径基础中心性的强烈相关性。能够找到网络中的高影响力节点。

Conclusion: 所提出的嵌入方法为标准贪婪算法提供了一个快速且可扩展的替代方案。

Abstract: Computing classical centrality measures such as betweenness and closeness is
computationally expensive on large-scale graphs. In this work, we introduce an
efficient force layout algorithm that embeds a graph into a low-dimensional
space, where the radial distance from the origin serves as a proxy for various
centrality measures. We evaluate our method on multiple graph families and
demonstrate strong correlations with degree, PageRank, and paths-based
centralities. As an application, it turns out that the proposed embedding
allows to find high-influence nodes in a network, and provides a fast and
scalable alternative to the standard greedy algorithm.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [583] [Understanding the Error Sensitivity of Privacy-Aware Computing](https://arxiv.org/abs/2506.07957)
*Matías Mazzanti,Esteban Mocskos,Augusto Vega,Pradip Bose*

Main category: cs.AR

TL;DR: The paper discusses the sensitivity of Homomorphic Encryption (HE) applications to bit faults, focusing on the CKKS scheme. It provides an error characterization study and examines the impact of RNS and NTT optimization techniques on error sensitivity.


<details>
  <summary>Details</summary>
Motivation: To understand and address the challenges related to fault tolerance and robustness in HE schemes, particularly the sensitivity to bit faults.

Method: Conduct a detailed error characterization study of the CKKS HE scheme and analyze the impact of RNS and NTT optimization techniques on its error sensitivity.

Result: This work identifies the vulnerabilities of HE schemes to hardware- and software-induced errors and provides insights into improving their robustness.

Conclusion: This is the first study to investigate the robustness and error sensitivity of homomorphic encryption, offering a foundation for future research in enhancing HE's reliability.

Abstract: Homomorphic Encryption (HE) enables secure computation on encrypted data
without decryption, allowing a great opportunity for privacy-preserving
computation. In particular, domains such as healthcare, finance, and
government, where data privacy and security are of utmost importance, can
benefit from HE by enabling third-party computation and services on sensitive
data. In other words, HE constitutes the "Holy Grail" of cryptography: data
remains encrypted all the time, being protected while in use.
  HE's security guarantees rely on noise added to data to make relatively
simple problems computationally intractable. This error-centric intrinsic HE
mechanism generates new challenges related to the fault tolerance and
robustness of HE itself: hardware- and software-induced errors during HE
operation can easily evade traditional error detection and correction
mechanisms, resulting in silent data corruption (SDC).
  In this work, we motivate a thorough discussion regarding the sensitivity of
HE applications to bit faults and provide a detailed error characterization
study of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes
due to its fixed-point arithmetic support for AI and machine learning
applications. We also delve into the impact of the residue number system (RNS)
and the number theoretic transform (NTT), two widely adopted HE optimization
techniques, on CKKS' error sensitivity. To the best of our knowledge, this is
the first work that looks into the robustness and error sensitivity of
homomorphic encryption and, as such, it can pave the way for critical future
work in this area.

</details>


### [584] [Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing](https://arxiv.org/abs/2506.06693)
*Priyanshu Yadav*

Main category: cs.AR

TL;DR: The paper analyzes RISC-V ISA focusing on modular design, implementation challenges and performance characteristics. Through cycle-accurate simulation of RV32I with M/A extensions, it shows 17% power reduction vs ARM Cortex-M0 and highlights flexibility for optimizations.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive evaluation of RISC-V architecture's suitability for embedded systems and custom accelerator designs by analyzing its design, implementation challenges and performance metrics.

Method: Conducting cycle-accurate simulation of a pipelined RISC-V implementation using RV32I base instruction set with M and A extensions to evaluate CPI and power efficiency.

Result: Demonstrated 17% power consumption reduction compared to ARM Cortex-M0 in similar process nodes, showcasing RISC-V's advantages in embedded systems.

Conclusion: RISC-V's open-standard nature provides significant flexibility for domain-specific optimizations while offering better power efficiency for embedded applications.

Abstract: This paper presents a comprehensive analysis of the RISC-V instruction set
architecture, focusing on its modular design, implementation challenges, and
performance characteristics. We examine the RV32I base instruction set with
extensions for multiplication (M) and atomic operations (A). Through
cycle-accurate simulation of a pipelined implementation, we evaluate
performance metrics including CPI (cycles per instruction) and power
efficiency. Our results demonstrate RISC-V's advantages in embedded systems and
its scalability for custom accelerators. Comparative analysis shows a 17%
reduction in power consumption compared to ARM Cortex-M0 implementations in
similar process nodes. The open-standard nature of RISC-V provides significant
flexibility for domain-specific optimizations.

</details>


### [585] [MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection](https://arxiv.org/abs/2506.07126)
*Weihan Lu,Hong Cai Chen*

Main category: cs.AR

TL;DR: MAGNet is a hybrid deep learning model combining improved U-Net and GNN for DRC violation prediction, enhancing feature extraction with DAM and MSCM, utilizing pixel-aligned graph structure, adopting label amplification strategy, achieving better accuracy and reduced false positive rates in DRC hotspot detection compared to ibUnet, RouteNet, and J-Net.


<details>
  <summary>Details</summary>
Motivation: Design rule checking (DRC) is crucial for cost reduction and design efficiency improvement in integrated circuit (IC) designs. Existing methods have limitations that this research aims to overcome by leveraging machine learning.

Method: Propose MAGNet, integrating an enhanced U-Net with DAM and MSCM for spatial feature extraction, constructing a pixel-aligned graph structure with specialized GNN for topological relationships modeling, using graph-to-grid mapping to align features, and applying label amplification strategy during training.

Result: MAGNet shows improved prediction accuracy and reduced false positive rates in DRC hotspot detection. It outperforms ibUnet, RouteNet, and J-Net in overall performance, with incremental training further improving discrimination ability for hotspots.

Conclusion: MAGNet effectively combines spatial, semantic, and structural information for DRC violation prediction, demonstrating superior performance compared to existing models.

Abstract: Design rule checking (DRC) is of great significance for cost reduction and
design efficiency improvement in integrated circuit (IC) designs.
Machine-learning-based DRC has become an important approach in computer-aided
design (CAD). In this paper, we propose MAGNet, a hybrid deep learning model
that integrates an improved U-Net with a graph neural network for DRC violation
prediction. The U-Net backbone is enhanced with a Dynamic Attention Module
(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability
in extracting fine-grained and multi-scale spatial features. In parallel, we
construct a pixel-aligned graph structure based on chip layout tiles, and apply
a specialized GNN to model the topological relationships among pins. During
graph construction, a graph-to-grid mapping is generated to align GNN features
with the layout image. In addition, a label amplification strategy is adopted
during training to enhance the model's sensitivity to sparse violation
patterns. Overall, MAGNet effectively combines spatial, semantic, and
structural information, achieving improved prediction accuracy and reduced
false positive rates in DRC hotspot detection. Subsequently, through
incremental training, we achieve a more sensitive discrimination ability for
hotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,
and J-Net, MAGnet significantly outperforms these models, achieving substantial
improvements in overall performance.

</details>


### [586] [VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code](https://arxiv.org/abs/2506.07239)
*Raghu Vamshi Hemadri,Jitendra Bhandari,Johann Knechtel,Badri P Gopalan,Ramesh Narayanaswamy,Ramesh Karri,Siddharth Garg*

Main category: cs.AR

TL;DR: VeriLoC is the first method that predicts design quality directly from Verilog at both line- and module-level, achieving high F1-scores and significantly reducing errors.


<details>
  <summary>Details</summary>
Motivation: There is a crucial need for early-stage prediction of key design-quality metrics like timing and routing congestion directly from Verilog code, especially predicting individual lines of code that cause violations.

Method: VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, then trains downstream classifiers/regressors on concatenations of these embeddings.

Result: VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14%-18% for SOTA methods down to only 4%.

Conclusion: VeriLoC embeddings and insights will also be valuable for other predictive and optimization tasks for complex hardware design.

Abstract: Modern chip design is complex, and there is a crucial need for early-stage
prediction of key design-quality metrics like timing and routing congestion
directly from Verilog code (a commonly used programming language for hardware
design). It is especially important yet complex to predict individual lines of
code that cause timing violations or downstream routing congestion. Prior works
have tried approaches like converting Verilog into an intermediate graph
representation and using LLM embeddings alongside other features to predict
module-level quality, but did not consider line-level quality prediction. We
propose VeriLoC, the first method that predicts design quality directly from
Verilog at both the line- and module-level. To this end, VeriLoC leverages
recent Verilog code-generation LLMs to extract local line-level and
module-level embeddings, and train downstream classifiers/regressors on
concatenations of these embeddings. VeriLoC achieves high F1-scores of
0.86-0.95 for line-level congestion and timing prediction, and reduces the mean
average percentage error from 14% - 18% for SOTA methods down to only 4%. We
believe that VeriLoC embeddings and insights from our work will also be of
value for other predictive and optimization tasks for complex hardware design.

</details>


### [587] [Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor](https://arxiv.org/abs/2506.06773)
*Emet Behrendt,Shing Wai Pun,Prashant J. Nair*

Main category: cs.AR

TL;DR: The paper proposes augmenting TAGE-SC-L predictor with a Bullseye predictor to address hard-to-predict (H2P) branches, leading to improved prediction accuracy and performance metrics.


<details>
  <summary>Details</summary>
Motivation: Even the CBP-2016 winner TAGE-SC-L has over half of its remaining mispredictions from H2P branches which cause repeated thrashing and eviction before usefulness counters can mature. Simply enlarging tables provides only marginal improvement.

Method: Augment TAGE-SC-L predictor with a 28 KB Bullseye predictor subsystem that includes a set-associative H2P Identification Table (HIT) and two branch-specific perceptrons indexed by hashed local history or folded global history respectively. A trial phase tracks accuracy in an H2P cache, and TAGE updates are suppressed for branches meeting dynamic thresholds.

Result: Achieves an average MPKI of 3.4045 and CycWpPKI of 145.09, indicating improvements in prediction accuracy and overall processor performance.

Conclusion: Integrating the Bullseye predictor into TAGE-SC-L enhances fidelity on hard-to-predict branches, reducing mispredictions and improving key performance indicators.

Abstract: Branch prediction is key to the performance of out-of-order processors. While
the CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical
corrector, and a loop predictor, over half of its remaining mispredictions stem
from a small set of hard-to-predict (H2P) branches. These branches occur under
diverse global histories, causing repeated thrashing in TAGE and eviction
before usefulness counters can mature. Prior work shows that simply enlarging
the tables offers only marginal improvement.
  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem
called the Bullseye predictor. It identifies problematic PCs using a
set-associative H2P Identification Table (HIT) and steers them to one of two
branch-specific perceptrons, one indexed by hashed local history and the other
by folded global history. A short trial phase tracks head-to-head accuracy in
an H2P cache. A branch becomes perceptron-resident only if the perceptron's
sustained accuracy and output magnitude exceed dynamic thresholds, after which
TAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,
and perceptron operate fully in parallel with TAGE-SC-L, providing higher
fidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI
of 145.09.

</details>


### [588] [ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors](https://arxiv.org/abs/2506.06817)
*Haoran Wu,Ce Guo,Wayne Luk,Robert Mullins*

Main category: cs.AR

TL;DR: ASPO is an approach that modifies Bayesian Optimization to handle categorical parameters and accelerates design evaluation for FPGA-based soft processors, showing significant improvements in execution and design time.


<details>
  <summary>Details</summary>
Motivation: Bayesian Optimization has been effective for tuning processor design parameters, but standard BO does not support constraints with categorical parameters and becomes time-consuming for complex processors.

Method: ASPO uses a disjunctive form to enable BO handling of categorical parameters, a customized BO covariance kernel for such parameters, penalizes the BO acquisition function with potential evaluation time, and reuses FPGA synthesis checkpoints.

Result: ASPO reduced execution time for the 'multiply' benchmark on the BOOM processor by up to 35% compared to default settings and cut design time by up to 74% compared to Boomerang.

Conclusion: ASPO successfully addresses the challenges faced by soft-processor designs on FPGAs through customization of BO, demonstrating effectiveness in reducing both execution and design times.

Abstract: Bayesian Optimization (BO) has shown promise in tuning processor design
parameters. However, standard BO does not support constraints involving
categorical parameters such as types of branch predictors and division
circuits. In addition, optimization time of BO grows with processor complexity,
which becomes increasingly significant especially for FPGA-based soft
processors. This paper introduces ASPO, an approach that leverages disjunctive
form to enable BO to handle constraints involving categorical parameters.
Unlike existing methods that directly apply standard BO, the proposed ASPO
method, for the first time, customizes the mathematical mechanism of BO to
address challenges faced by soft-processor designs on FPGAs. Specifically, ASPO
supports categorical parameters using a novel customized BO covariance kernel.
It also accelerates the design evaluation procedure by penalizing the BO
acquisition function with potential evaluation time and by reusing FPGA
synthesis checkpoints from previously evaluated configurations. ASPO targets
three soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is
evaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce
execution time for the ``multiply'' benchmark on the BOOM processor by up to
35\% compared to the default configuration. Furthermore, it reduces design time
for the BOOM processor by up to 74\% compared to Boomerang, a state-of-the-art
hardware-oriented BO approach.

</details>


### [589] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: The paper explores the capabilities of LLMs in generating SystemVerilog implementations for communication protocols, introducing a benchmark suite for SPI, I2C, UART, and AXI protocols.


<details>
  <summary>Details</summary>
Motivation: Recent advances in Large Language Models (LLMs) have shown promising capabilities in generating code for general-purpose programming languages. However, their applicability for hardware description languages remains underexplored.

Method: The paper introduces a benchmark suite targeting four widely used protocols: SPI, I2C, UART, and AXI. It defines code generation tasks that capture varying levels of design abstraction and prompt specificity.

Result: The generated designs are assessed for syntactic correctness, synthesizability, and functional fidelity via waveform simulation and test benches.

Conclusion: This study aims to analyze the capabilities of state-of-the-art LLMs in generating SystemVerilog implementations of standard communication protocols.

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [590] [Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics](https://arxiv.org/abs/2506.06286)
*Kevin Baum*

Main category: cs.CY

TL;DR: Recent AI advancements raise concerns about agents operating in real-world settings. While fields like AI Safety and Machine Ethics aim to address these issues, their boundaries are unclear. This paper introduces a conceptual framework for understanding AI alignment with a taxonomy that considers alignment aim, scope, and constituency.


<details>
  <summary>Details</summary>
Motivation: To provide clear guidance for researchers by developing a structured conceptual framework for understanding AI alignment.

Method: Developing a taxonomy that distinguishes the alignment aim (safety, ethicality, legality, etc.), scope (outcome vs. execution), and constituency (individual vs. collective).

Result: Reveals multiple legitimate alignment configurations and provides a foundation for practical and philosophical integration across domains.

Conclusion: Clarifies what it might mean for an agent to be aligned all-things-considered.

Abstract: Recent advances in AI research make it increasingly plausible that artificial
agents with consequential real-world impact will soon operate beyond tightly
controlled environments. Ensuring that these agents are not only safe but that
they adhere to broader normative expectations is thus an urgent
interdisciplinary challenge. Multiple fields -- notably AI Safety, AI
Alignment, and Machine Ethics -- claim to contribute to this task. However, the
conceptual boundaries and interrelations among these domains remain vague,
leaving researchers without clear guidance in positioning their work.
  To address this meta-challenge, we develop a structured conceptual framework
for understanding AI alignment. Rather than focusing solely on alignment goals,
we introduce a taxonomy distinguishing the alignment aim (safety, ethicality,
legality, etc.), scope (outcome vs. execution), and constituency (individual
vs. collective). This structural approach reveals multiple legitimate alignment
configurations, providing a foundation for practical and philosophical
integration across domains, and clarifying what it might mean for an agent to
be aligned all-things-considered.

</details>


### [591] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: AI的进步预示着 sophisticated disinformation operations 的新时代。我们应从平台、模型和系统三个层面进行防御响应。


<details>
  <summary>Details</summary>
Motivation: 尽管个体AI系统已经能够创建令人信服且有时具有误导性的信息，但恶意AI群的出现将带来更大的威胁。这些系统可以秘密协调、渗透社区、逃避传统检测器并持续进行A/B测试。这可能导致虚假共识、现实分裂、大规模骚扰、选民抑制或动员、AI训练数据污染以及机构信任侵蚀。

Method: 三管齐下的响应策略：(1) 平台端防御 - 常态化的群检测仪表板、选举前高保真群模拟压力测试、透明度审计和可选的客户端“AI护盾”；(2) 模型端保障 - 标准化说服风险测试、溯源认证通行密钥和水印；(3) 系统级监督 - 联合国支持的AI影响观测站。

Result: 通过采取上述三方面的措施，可以更好地应对恶意AI群带来的挑战，保护民主进程免受侵害。

Conclusion: 随着全球民主进程日益脆弱，我们需要采取紧急行动，以防止恶意AI群对社会造成严重破坏。

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [592] [Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor](https://arxiv.org/abs/2506.06383)
*Qian Huang,King Wang Poon*

Main category: cs.CY

TL;DR: The paper explores the collaboration between human and AI in fitness education via a one year qualitative case study with a Pilates instructor, revealing insights on integrating generative AI into class planning and instruction.


<details>
  <summary>Details</summary>
Motivation: To clarify the optimal role of AI alongside human expertise in teaching and coaching practices, specifically within fitness education.

Method: A one year qualitative case study was conducted with a Pilates instructor, including participation in classes and biweekly semi structured interviews.

Result: Generative AI can be effectively integrated into class planning and instruction, enhancing the teaching practice when collaborating with human instructors.

Conclusion: Human AI collaboration holds potential for enriching fitness education, though clear guidelines for integration are necessary.

Abstract: Artificial intelligence is poised to transform teaching and coaching
practices,yet its optimal role alongside human expertise remains unclear.This
study investigates human and AI collaboration in fitness education through a
one year qualitative case study with a Pilates instructor.The researcher
participated in the instructor classes and conducted biweekly semi structured
interviews to explore how generative AI could be integrated into class planning
and instruction.

</details>


### [593] [Benchmarking Large Language Models on Homework Assessment in Circuit Analysis](https://arxiv.org/abs/2506.06390)
*Liangliang Chen,Zhihao Qin,Yiming Guo,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: 这篇论文探讨了如何在工程教育中利用大型语言模型（LLMs），特别是评估电路分析课程的家庭作业。研究发现GPT-4o和Llama 3 70B的表现优于GPT-3.5 Turbo，并提出了当前LLMs的局限性及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型因其广泛的知识和快速进步，有可能改变包括工程教育在内的多个领域。因此，研究它们在工程教育中的应用潜力具有重要意义。

Method: 开发了一个包含官方参考答案和真实学生解答的新数据集，并将这些解答转换为LaTeX格式以克服图像识别的限制。设计了一个提示模板，用于测试学生解答的五个指标：完整性、方法、最终答案、算术错误和单位。

Result: GPT-4o和Llama 3 70B在所有五个指标上的表现显著优于GPT-3.5 Turbo，各自在不同的评估方面具有独特优势。

Conclusion: 本研究为电路分析的可靠个性化导师的发展奠定了基准并提供了宝贵的见解，同时提出的评估方法可以推广到更广泛的工程教育课程。

Abstract: Large language models (LLMs) have the potential to revolutionize various
fields, including code development, robotics, finance, and education, due to
their extensive prior knowledge and rapid advancements. This paper investigates
how LLMs can be leveraged in engineering education. Specifically, we benchmark
the capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama
3 70B, in assessing homework for an undergraduate-level circuit analysis
course. We have developed a novel dataset consisting of official reference
solutions and real student solutions to problems from various topics in circuit
analysis. To overcome the limitations of image recognition in current
state-of-the-art LLMs, the solutions in the dataset are converted to LaTeX
format. Using this dataset, a prompt template is designed to test five metrics
of student solutions: completeness, method, final answer, arithmetic error, and
units. The results show that GPT-4o and Llama 3 70B perform significantly
better than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B
each having distinct advantages in different evaluation aspects. Additionally,
we present insights into the limitations of current LLMs in several aspects of
circuit analysis. Given the paramount importance of ensuring reliability in
LLM-generated homework assessment to avoid misleading students, our results
establish benchmarks and offer valuable insights for the development of a
reliable, personalized tutor for circuit analysis -- a focus of our future
work. Furthermore, the proposed evaluation methods can be generalized to a
broader range of courses for engineering education in the future.

</details>


### [594] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: This study evaluates 8 leading LLMs on their ability to refuse prompts violating International Humanitarian Law (IHL). A standardised safety prompt improves refusal explanations, but vulnerabilities persist in complex prompts. Findings aim to develop safer AI systems and propose a compliance benchmark.


<details>
  <summary>Details</summary>
Motivation: To understand how well Large Language Models (LLMs) align with International Humanitarian Law (IHL) and improve their safety and transparency by evaluating their ability to refuse unlawful prompts.

Method: The study evaluated eight leading LLMs on their ability to refuse prompts that explicitly violate IHL legal frameworks. It also assessed the helpfulness of these refusals - focusing on how clearly and constructively they were communicated. A standardised system-level safety prompt was introduced to observe its impact on the quality of refusal explanations.

Result: Most models rejected unlawful requests, but there was variation in the clarity and consistency of their responses. Explanatory refusals, which revealed the model's rationale and referenced relevant legal or safety principles, clarified system boundaries and reduced ambiguity. The introduction of a standardised safety prompt significantly improved the quality of refusal explanations in most models. However, vulnerabilities were identified in handling more complex prompts involving technical language or code.

Conclusion: The findings contribute towards developing safer and more transparent AI systems. They also propose a benchmark for evaluating LLM compliance with IHL.

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [595] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: 大型语言模型（LLMs）可以作为专家政治调查的可行替代品，特别是在突发事件或冲击事件破坏传统测量方法时。通过分析2025年的DOGE联邦裁员事件，研究发现使用LLMs得出的意识形态评分能够复制裁员前专家的测量结果，并预测哪些机构会被DOGE针对。此外，这种方法还揭示了某些联邦机构被视为知识机构的知觉也与被DOGE针对有关，即使控制了意识形态因素。该案例研究表明，LLMs可以帮助快速轻松地测试冲击背后的假设因素，并在传统测量技术失效时提供对冲击相关因素的见解。最后，文章提出了研究人员可以在何种情况下将LLMs作为专家政治调查的替代品的双重要求。


<details>
  <summary>Details</summary>
Motivation: 在发生破坏性事件或冲击后，专家判断往往受到结果的影响，这使得重建事件前的认知变得困难甚至不可能。因此，需要一种新的方法来替代传统的专家政治调查，以便研究与事件相关的因素。

Method: 使用成对比较提示符与大型语言模型（LLMs）进行交互，从而为联邦执行机构生成意识形态评分。这些评分用于复制裁员前专家的测量结果，并预测哪些机构可能受到DOGE裁员的影响。同时，采用相同的方法探讨联邦机构作为知识机构的知觉是否与裁员目标选择有关。

Result: LLMs生成的意识形态评分成功复制了裁员前专家的测量结果，并能够预测哪些机构被DOGE针对。此外，研究还发现，某些联邦机构被视为知识机构的知觉也与被DOGE裁员有关，即使在控制意识形态因素的情况下也是如此。

Conclusion: 大型语言模型（LLMs）可以作为专家政治调查的可行替代品，特别是在冲击事件破坏传统测量方法时。文章提出了一种双重要求，以帮助研究人员判断何时可以将LLMs作为替代方案。

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [596] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: The paper presents a novel auditing framework to assess workers' desires for AI agent automation or augmentation in occupational tasks and align them with current technological capabilities, constructing the WORKBank database.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic understanding of the evolving landscape of compound AI systems (AI agents) in the labor market and their impact on job displacement, human agency, and automation reliance.

Method: Introduced an audio-enhanced mini-interview based auditing framework combined with the Human Agency Scale (HAS) to quantify preferred human involvement levels. Constructed the WORKBank database using preferences from 1,500 domain workers and capability assessments from AI experts across 844 tasks in 104 occupations.

Result: Divided tasks into four zones based on worker desires and technological capabilities: Automation 'Green Light' Zone, 'Red Light' Zone, R&D Opportunity Zone, and Low Priority Zone. Revealed diverse HAS profiles across occupations and suggested a shift from information-focused skills to interpersonal ones due to AI agent integration.

Conclusion: Highlighted the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics.

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


### [597] [Position: Simulating Society Requires Simulating Thought](https://arxiv.org/abs/2506.06958)
*Chance Jiajie Li,Jiayi Wu,Zhenze Mo,Ao Qu,Yuhan Tang,Kaiya Ivy Zhao,Yulu Gan,Jie Fan,Jiangbo Yu,Jinhua Zhao,Paul Liang,Luis Alonso,Kent Larson*

Main category: cs.CY

TL;DR: The paper introduces Generative Minds (GenMinds) as a new paradigm to enhance large language models' ability to simulate society by providing structured, revisable, and traceable reasoning. It also presents the RECAP framework for evaluating these agents.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based agents lack internal coherence, causal reasoning, and belief traceability when emulating individual and group behavior.

Method: The authors propose Generative Minds (GenMinds), which uses cognitive science principles to support structured belief representations in generative agents. They also introduce the RECAP framework to evaluate these agents based on reasoning fidelity via causal traceability, demographic grounding, and intervention consistency.

Result: These contributions aim to move from surface-level mimicry to generative agents that simulate thought rather than just language for social simulations.

Conclusion: By adopting GenMinds and using the RECAP framework, there can be advancements in simulating society with LLMs through more reliable cognitively grounded reasoning.

Abstract: Simulating society with large language models (LLMs), we argue, requires more
than generating plausible behavior -- it demands cognitively grounded reasoning
that is structured, revisable, and traceable. LLM-based agents are increasingly
used to emulate individual and group behavior -- primarily through prompting
and supervised fine-tuning. Yet they often lack internal coherence, causal
reasoning, and belief traceability -- making them unreliable for analyzing how
people reason, deliberate, or respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds
(GenMinds), which draws from cognitive science to support structured belief
representations in generative agents. To evaluate such agents, we introduce the
RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess
reasoning fidelity via causal traceability, demographic grounding, and
intervention consistency. These contributions advance a broader shift: from
surface-level mimicry to generative agents that simulate thought -- not just
language -- for social simulations.

</details>


### [598] [Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research](https://arxiv.org/abs/2506.06377)
*Giuseppe Arbia,Luca Morandini,Vincenzo Nardelli*

Main category: cs.CY

TL;DR: This paper explores the ability of Large Language Models (LLMs) to evaluate the economic soundness and theoretical consistency in spatial econometrics. Using original and altered summaries from 28 papers, LLMs were tested on variable choice, coefficient plausibility, and publication suitability. Results show LLMs perform well on surface-level checks but struggle with deeper analysis.


<details>
  <summary>Details</summary>
Motivation: To assess whether Large Language Models can effectively judge the economic soundness and theoretical consistency of empirical findings in spatial econometrics.

Method: Created original and deliberately altered summaries from 28 published papers and evaluated them using a diverse set of LLMs, analyzing their qualitative assessments and structured binary classifications on variable choice, coefficient plausibility, and publication suitability.

Result: LLMs performed well in assessing the coherence of variable choices (GPT-4o achieved an F1 score of 0.87), but showed significant variation in evaluating deeper aspects like coefficient plausibility and overall publication suitability. The accuracy of assessment was influenced by the LLM used, paper characteristics, and their interaction.

Conclusion: LLMs are useful for initial, surface-level checks in peer review but are limited in deep economic reasoning, thus requiring human oversight.

Abstract: This paper investigates Large Language Models (LLMs) ability to assess the
economic soundness and theoretical consistency of empirical findings in spatial
econometrics. We created original and deliberately altered "counterfactual"
summaries from 28 published papers (2005-2024), which were evaluated by a
diverse set of LLMs. The LLMs provided qualitative assessments and structured
binary classifications on variable choice, coefficient plausibility, and
publication suitability. The results indicate that while LLMs can expertly
assess the coherence of variable choices (with top models like GPT-4o achieving
an overall F1 score of 0.87), their performance varies significantly when
evaluating deeper aspects such as coefficient plausibility and overall
publication suitability. The results further revealed that the choice of LLM,
the specific characteristics of the paper and the interaction between these two
factors significantly influence the accuracy of the assessment, particularly
for nuanced judgments. These findings highlight LLMs' current strengths in
assisting with initial, more surface-level checks and their limitations in
performing comprehensive, deep economic reasoning, suggesting a potential
assistive role in peer review that still necessitates robust human oversight.

</details>


### [599] [Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust](https://arxiv.org/abs/2506.07363)
*Claudiu Popa,Rex Pallath,Liam Cunningham,Hewad Tahiri,Abiram Kesavarajah,Tao Wu*

Main category: cs.CY

TL;DR: This paper explores the implications of deepfake technology, demonstrating how realistic deepfakes can be created with limited resources and emphasizing the need for regulatory frameworks, public awareness, and collaborative efforts to maintain trust in digital media.


<details>
  <summary>Details</summary>
Motivation: The motivation is to analyze the role of deepfake technology in enabling fraud, misinformation, and the erosion of authenticity in multimedia due to the increasing accessibility of generative AI.

Method: The method involves using cost-effective, easy-to-use tools such as Runway, Rope, and ElevenLabs to explore how realistic deepfakes can be created with limited resources.

Result: The result demonstrates the risks posed by deepfakes to individuals and organizations alike, highlighting the technical and ethical challenges of deepfake mitigation and detection.

Conclusion: The conclusion emphasizes the urgent need for regulatory frameworks, public awareness, and collaborative efforts to maintain trust in digital media.

Abstract: Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on
Digital Trust. With the increasing accessibility of generative AI, tools for
voice cloning, face-swapping, and synthetic media creation have advanced
significantly, lowering both financial and technical barriers for their use.
While these technologies present innovative opportunities, their rapid growth
raises concerns about trust, privacy, and security. This white paper explores
the implications of deepfake technology, analyzing its role in enabling fraud,
misinformation, and the erosion of authenticity in multimedia. Using
cost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we
explore how realistic deepfakes can be created with limited resources,
demonstrating the risks posed to individuals and organizations alike. By
analyzing the technical and ethical challenges of deepfake mitigation and
detection, we emphasize the urgent need for regulatory frameworks, public
awareness, and collaborative efforts to maintain trust in digital media.

</details>


<div id='cs.MS'></div>

# cs.MS [[Back]](#toc)

### [600] [El0ps: An Exact L0-regularized Problems Solver](https://arxiv.org/abs/2506.06373)
*Théo Guyard,Cédric Herzet,Clément Elvira*

Main category: cs.MS

TL;DR: This paper introduces El0ps, a Python toolbox for handling L0-regularized problems with custom instances, state-of-the-art solvers, and built-in machine learning pipelines.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive tool which opens new perspectives for the integration of L0-regularized problems in practical applications.

Method: El0ps allows users to define custom instances of L0-regularized problems through a flexible framework, provides a dedicated solver achieving state-of-the-art performance, and offers several built-in machine learning pipelines.

Result: Users can now handle L0-regularized problems more effectively in machine learning, statistics, and signal processing.

Conclusion: El0ps is presented as a Python toolbox that advances the practical application of L0-regularized problems.

Abstract: This paper presents El0ps, a Python toolbox providing several utilities to
handle L0-regularized problems related to applications in machine learning,
statistics, and signal processing, among other fields. In contrast to existing
toolboxes, El0ps allows users to define custom instances of these problems
through a flexible framework, provides a dedicated solver achieving
state-of-the-art performance, and offers several built-in machine learning
pipelines. Our aim with El0ps is to provide a comprehensive tool which opens
new perspectives for the integration of L0-regularized problems in practical
applications.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [601] [Scientific machine learning in Hydrology: a unified perspective](https://arxiv.org/abs/2506.06308)
*Adoubi Vincent De Paul Adombi*

Main category: physics.comp-ph

TL;DR: This paper provides the first focused overview of SciML in hydrology, proposing unified methodological frameworks for each SciML family to foster conceptual clarity and support cumulative progress.


<details>
  <summary>Details</summary>
Motivation: To address the fragmentation in scientific machine learning approaches within hydrology, which complicates assessment of novelty and identification of areas for meaningful advances.

Method: Proposing a unified methodological framework for each family of SciML (physics-informed, physics-guided, hybrid physics-machine learning, and data-driven physics discovery) to bring together representative contributions into a coherent structure.

Result: Provides a clear and structured understanding of the various SciML families and their respective contributions to hydrological modeling.

Conclusion: Highlights the limitations and future opportunities of each unified SciML family to guide systematic research in hydrology.

Abstract: Scientific machine learning (SciML) provides a structured approach to
integrating physical knowledge into data-driven modeling, offering significant
potential for advancing hydrological research. In recent years, multiple
methodological families have emerged, including physics-informed machine
learning, physics-guided machine learning, hybrid physics-machine learning, and
data-driven physics discovery. Within each of these families, a proliferation
of heterogeneous approaches has developed independently, often without
conceptual coordination. This fragmentation complicates the assessment of
methodological novelty and makes it difficult to identify where meaningful
advances can still be made in the absence of a unified conceptual framework.
This review, the first focused overview of SciML in hydrology, addresses these
limitations by proposing a unified methodological framework for each SciML
family, bringing together representative contributions into a coherent
structure that fosters conceptual clarity and supports cumulative progress in
hydrological modeling. Finally, we highlight the limitations and future
opportunities of each unified family to guide systematic research in hydrology,
where these methods remain underutilized.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [602] [DELPHYNE: A Pre-Trained Model for General and Financial Time Series](https://arxiv.org/abs/2506.06288)
*Xueying Ding,Aakriti Mittal,Achintya Gopal*

Main category: q-fin.ST

TL;DR: Delphyne is a pre-trained model designed for financial time-series data that achieves competitive performance with few fine-tuning steps.


<details>
  <summary>Details</summary>
Motivation: Existing time-series pre-trained models have not shown significant performance boosts in financial applications due to lack of financial data in pre-training and negative transfer effect from different domain patterns. Additionally, time-series data presents unique challenges such as continuity, noise, and varying frequencies.

Method: Introduced Delphyne, a Pre-trained MoDEL for FINance TimE-series, which addresses the issues of existing models by being specifically designed for financial time-series data.

Result: Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and shows superior performances on various financial tasks.

Conclusion: Delphyne presents a promising approach for handling financial time-series data, overcoming the limitations of previous models.

Abstract: Time-series data is a vital modality within data science communities. This is
particularly valuable in financial applications, where it helps in detecting
patterns, understanding market behavior, and making informed decisions based on
historical data. Recent advances in language modeling have led to the rise of
time-series pre-trained models that are trained on vast collections of datasets
and applied to diverse tasks across financial domains. However, across
financial applications, existing time-series pre-trained models have not shown
boosts in performance over simple finance benchmarks in both zero-shot and
fine-tuning settings. This phenomenon occurs because of a i) lack of financial
data within the pre-training stage, and ii) the negative transfer effect due to
inherently different time-series patterns across domains. Furthermore,
time-series data is continuous, noisy, and can be collected at varying
frequencies and with varying lags across different variables, making this data
more challenging to model than languages. To address the above problems, we
introduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne
achieves competitive performance to existing foundation and full-shot models
with few fine-tuning steps on publicly available datasets, and also shows
superior performances on various financial tasks.

</details>


### [603] [Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100](https://arxiv.org/abs/2506.06345)
*Sukru Selim Calik,Andac Akyuz,Zeynep Hilal Kilimci,Kerem Colak*

Main category: q-fin.ST

TL;DR: A novel approach combining transformer-based time series models with explainable AI (XAI) techniques is proposed to improve stock price prediction accuracy and interpretability for high-volume banks in BIST100, XBANK, and XU100 indices from 2015-2025. Models like DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are used alongside SHAP and LIME for feature influence transparency.


<details>
  <summary>Details</summary>
Motivation: Financial literacy increasingly depends on interpreting complex financial data and using advanced forecasting tools. This study aims to enhance the interpretability and accuracy of stock price predictions by integrating transformer-based time series models with XAI.

Method: The study uses transformer-based time series models (DLinear, LTSNet, Vanilla Transformer, Time Series Transformer) enriched with technical indicators to predict daily stock prices of five highest-volume banks in BIST100, along with XBANK and XU100 indices. SHAP and LIME are applied for explaining individual feature impacts.

Result: Transformer models show strong predictive capabilities, and XAI methods provide valuable insights into model behavior, empowering individuals to make informed investment decisions.

Conclusion: Combining transformer-based models with XAI techniques can significantly improve both the accuracy and interpretability of stock price predictions, enhancing financial literacy.

Abstract: Financial literacy is increasingly dependent on the ability to interpret
complex financial data and utilize advanced forecasting tools. In this context,
this study proposes a novel approach that combines transformer-based time
series models with explainable artificial intelligence (XAI) to enhance the
interpretability and accuracy of stock price predictions. The analysis focuses
on the daily stock prices of the five highest-volume banks listed in the
BIST100 index, along with XBANK and XU100 indices, covering the period from
January 2015 to March 2025. Models including DLinear, LTSNet, Vanilla
Transformer, and Time Series Transformer are employed, with input features
enriched by technical indicators. SHAP and LIME techniques are used to provide
transparency into the influence of individual features on model outputs. The
results demonstrate the strong predictive capabilities of transformer models
and highlight the potential of interpretable machine learning to empower
individuals in making informed investment decisions and actively engaging in
financial markets.

</details>


### [604] [Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation](https://arxiv.org/abs/2506.07315)
*Zonghan Wu,Junlin Wang,Congyuan Zou,Chenhan Wang,Yilei Shao*

Main category: q-fin.ST

TL;DR: 生成式AI，特别是大语言模型（LLMs），正在通过自动化任务和帮助理解复杂的金融信息来改变金融行业。本文提出了FinAR-Bench，一个专注于财务报表分析的坚实基准数据集，将任务分为三个可测量的步骤：提取关键信息、计算财务指标和应用逻辑推理，以更精确和可靠地评估LLMs在实际金融任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs可以尝试从单一提示生成基础分析报告，但其不准确性风险显著，可能导致误导性投资、监管问题和信任丧失。现有的金融基准主要评估LLMs回答金融问题的能力，未能反映其在实际任务（如生成金融分析报告）中的表现。

Method: 提出FinAR-Bench，一个专注于财务报表分析的基准数据集，将任务分解为三个可测量的步骤：1）提取关键信息；2）计算财务指标；3）应用逻辑推理。这种方法使得能够客观评估LLMs在每个步骤中的表现。

Result: 研究发现提供了对LLMs在基础分析中的当前优势和局限性的清晰理解，并提供了一种更实用的方式来衡量它们在真实世界金融环境中的性能。

Conclusion: FinAR-Bench为评估LLMs在财务报表分析方面的表现提供了一个精确且可靠的方法，揭示了LLMs在基础分析中的优势与不足，促进了更实际的性能评估方法的发展。

Abstract: Generative AI, particularly large language models (LLMs), is beginning to
transform the financial industry by automating tasks and helping to make sense
of complex financial information. One especially promising use case is the
automatic creation of fundamental analysis reports, which are essential for
making informed investment decisions, evaluating credit risks, guiding
corporate mergers, etc. While LLMs attempt to generate these reports from a
single prompt, the risks of inaccuracy are significant. Poor analysis can lead
to misguided investments, regulatory issues, and loss of trust. Existing
financial benchmarks mainly evaluate how well LLMs answer financial questions
but do not reflect performance in real-world tasks like generating financial
analysis reports. In this paper, we propose FinAR-Bench, a solid benchmark
dataset focusing on financial statement analysis, a core competence of
fundamental analysis. To make the evaluation more precise and reliable, we
break this task into three measurable steps: extracting key information,
calculating financial indicators, and applying logical reasoning. This
structured approach allows us to objectively assess how well LLMs perform each
step of the process. Our findings offer a clear understanding of LLMs current
strengths and limitations in fundamental analysis and provide a more practical
way to benchmark their performance in real-world financial settings.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [605] [Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model](https://arxiv.org/abs/2506.07083)
*Jiawen Li,Jiang Guo,Yuanzhe Li,Zetian Mao,Jiaxing Shen,Tashi Xu,Diptesh Das,Jinming He,Run Hu,Yaerim Lee,Koji Tsuda,Junichiro Shiomi*

Main category: physics.optics

TL;DR: A general framework using conditional diffusion models is proposed for metamaterial inverse design, exhibiting high spectral prediction accuracy and aiding in the fabrication of complex metamaterials.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by the nonlinear relationship between metamaterial structures and their optical behavior, as well as fabrication difficulties, when using machine learning for designing complex metamaterials.

Method: The paper proposes a framework that uses customized spectrum-to-shape and size parameters with conditional diffusion models to solve one-to-many metamaterial inverse design problems.

Result: This method shows superior spectral prediction accuracy, generates diverse patterns compared to other generative models, and provides useful prior knowledge for manufacturing. A free-form metamaterial was successfully designed and fabricated with a tailored selective emission spectrum for thermal camouflage.

Conclusion: The proposed framework effectively facilitates the experimental fabrication of complex metamaterials through accurate spectral predictions and diverse pattern generation.

Abstract: Metamaterials are artificially engineered structures that manipulate
electromagnetic waves, having optical properties absent in natural materials.
Recently, machine learning for the inverse design of metamaterials has drawn
attention. However, the highly nonlinear relationship between the metamaterial
structures and optical behaviour, coupled with fabrication difficulties, poses
challenges for using machine learning to design and manufacture complex
metamaterials. Herein, we propose a general framework that implements
customised spectrum-to-shape and size parameters to address one-to-many
metamaterial inverse design problems using conditional diffusion models. Our
method exhibits superior spectral prediction accuracy, generates a diverse
range of patterns compared to other typical generative models, and offers
valuable prior knowledge for manufacturing through the subsequent analysis of
the diverse generated results, thereby facilitating the experimental
fabrication of metamaterial designs. We demonstrate the efficacy of the
proposed method by successfully designing and fabricating a free-form
metamaterial with a tailored selective emission spectrum for thermal camouflage
applications.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [606] [KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes](https://arxiv.org/abs/2506.06541)
*Eugenie Lai,Gerardo Vitagliano,Ziyu Zhang,Sivaprasad Sudhir,Om Chabra,Anna Zeng,Anton A. Zabreyko,Chenning Li,Ferdi Kossmann,Jialin Ding,Jun Chen,Markos Markakis,Matthew Russo,Weiyang Wang,Ziniu Wu,Michael J. Cafarella,Lei Cao,Samuel Madden,Tim Kraska*

Main category: cs.DB

TL;DR: 尽管现有的AI模型在解决明确规定的数据科学代码生成任务方面表现出足够的能力，但在构建实际的数据科学管道时，当需要广泛的数据处理和领域知识时，现成的模型表现不足。KRAMABENCH基准测试展示了向开发用于实际应用的自主数据科学代理迈进的关键步骤。


<details>
  <summary>Details</summary>
Motivation: 设计和实现数据科学管道需要领域知识、技术专长，甚至特定项目的见解。然而，目前尚不清楚AI系统的推理、编码和理解能力在多大程度上可以转化为成功设计和执行此类复杂管道的能力。

Method: 引入了KRAMABENCH：一个由104个手动策划的实际数据科学管道组成的基准测试，涵盖了来自6个不同领域的24个数据源的1700个数据文件。使用参考框架DS-GURU评估5个通用模型和3个代码生成模型，该框架指导AI模型将问题分解为一系列子任务，通过每一步进行推理，并合成实现所提议设计的Python代码。

Result: 结果显示，尽管这些模型在解决明确规定的数据科学代码生成任务方面表现出足够的能力，但在需要广泛数据处理和领域知识以构建实际数据科学管道时，现有现成模型表现不佳。

Conclusion: KRAMABENCH代表了朝向开发用于实际应用的自主数据科学代理迈出的关键步骤。

Abstract: Constructing real-world data-to-insight pipelines often involves data
extraction from data lakes, data integration across heterogeneous data sources,
and diverse operations from data cleaning to analysis. The design and
implementation of data science pipelines require domain knowledge, technical
expertise, and even project-specific insights. AI systems have shown remarkable
reasoning, coding, and understanding capabilities. However, it remains unclear
to what extent these capabilities translate into successful design and
execution of such complex pipelines. We introduce KRAMABENCH: a benchmark
composed of 104 manually-curated real-world data science pipelines spanning
1700 data files from 24 data sources in 6 different domains. We show that these
pipelines test the end-to-end capabilities of AI systems on data processing,
requiring data discovery, wrangling and cleaning, efficient processing,
statistical reasoning, and orchestrating data processing steps given a
high-level task. Our evaluation tests 5 general models and 3 code generation
models using our reference framework, DS-GURU, which instructs the AI model to
decompose a question into a sequence of subtasks, reason through each step, and
synthesize Python code that implements the proposed design. Our results on
KRAMABENCH show that, although the models are sufficiently capable of solving
well-specified data science code generation tasks, when extensive data
processing and domain knowledge are required to construct real-world data
science pipelines, existing out-of-box models fall short. Progress on
KramaBench represents crucial steps towards developing autonomous data science
agents for real-world applications. Our code, reference framework, and data are
available at https://github.com/mitdbg/KramaBench.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [607] [ARGOS: Anomaly Recognition and Guarding through O-RAN Sensing](https://arxiv.org/abs/2506.06916)
*Stavros Dimou,Guevara Noubir*

Main category: cs.NI

TL;DR: ARGOS is a new O-RAN compliant Intrusion Detection System (IDS) that can detect Rogue Base Station downgrade attacks in 5G networks using unsupervised Machine Learning models, achieving high accuracy and low false positives.


<details>
  <summary>Details</summary>
Motivation: Rogue Base Station (RBS) attacks exploiting downgrade vulnerabilities are still a significant threat due to limited 5G Standalone deployments and continued support for legacy network connectivity.

Method: ARGOS enhances the 3GPP KPM Service Model for richer UE-level telemetry and uses a custom xApp with unsupervised Machine Learning models for anomaly detection. It operates on cross-layer features from Modem Layer 1 logs and Measurement Reports collected from COTS UEs.

Result: The Variational Autoencoder (VAE) model within ARGOS achieves 99.5% Accuracy with only 0.6% False Positives and minimal system overhead when tested against a real-world measurement dataset.

Conclusion: ARGOS represents a novel approach to detecting RBS downgrade attacks in real time within the O-RAN context, offering effective intrusion detection capabilities.

Abstract: Rogue Base Station (RBS) attacks, particularly those exploiting downgrade
vulnerabilities, remain a persistent threat as 5G Standalone (SA) deployments
are still limited and User Equipment (UE) manufacturers continue to support
legacy network connectivity. This work introduces ARGOS, a comprehensive O-RAN
compliant Intrusion Detection System (IDS) deployed within the Near Real-Time
RIC, designed to detect RBS downgrade attacks in real time, an area previously
unexplored within the O-RAN context. The system enhances the 3GPP KPM Service
Model to enable richer, UE-level telemetry and features a custom xApp that
applies unsupervised Machine Learning models for anomaly detection.
Distinctively, the updated KPM Service Model operates on cross-layer features
extracted from Modem Layer 1 (ML1) logs and Measurement Reports collected
directly from Commercial Off-The-Shelf (COTS) UEs. To evaluate system
performance under realistic conditions, a dedicated testbed is implemented
using Open5GS, srsRAN, and FlexRIC, and validated against an extensive
real-world measurement dataset. Among the evaluated models, the Variational
Autoencoder (VAE) achieves the best balance of detection performance and
efficiency, reaching 99.5% Accuracy with only 0.6% False Positives and minimal
system overhead.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [608] [Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images](https://arxiv.org/abs/2506.07023)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: The paper proposes a new deep generative model for nuclei segmentation from histological images, which uses an invertible generator and spatially-constrained squeeze operation to handle information disparity and maintain spatial continuity.


<details>
  <summary>Details</summary>
Motivation: Existing image-to-image translation models struggle with asymmetric information content between domains, particularly in the context of nuclei segmentation from histological images.

Method: The proposed model incorporates an embedding space to address information disparity, integrates optimal transport and measure theory for an efficient optimization framework, uses an invertible generator eliminating the need for cycle-consistency loss, and introduces a spatially-constrained squeeze operation to preserve spatial continuity.

Result: The model demonstrates superior performance on publicly available histological image datasets compared to existing state-of-the-art nuclei segmentation methods, providing a better balance between network complexity and performance.

Conclusion: The new deep generative model offers an effective solution for nuclei segmentation in histological images, overcoming limitations of previous approaches by addressing information disparity and maintaining spatial continuity.

Abstract: Segmentation of nuclei regions from histological images enables morphometric
analysis of nuclei structures, which in turn helps in the detection and
diagnosis of diseases under consideration. To develop a nuclei segmentation
algorithm, applicable to different types of target domain representations,
image-to-image translation networks can be considered as they are invariant to
target domain image representations. One of the important issues with
image-to-image translation models is that they fail miserably when the
information content between two image domains are asymmetric in nature. In this
regard, the paper introduces a new deep generative model for segmenting nuclei
structures from histological images. The proposed model considers an embedding
space for handling information-disparity between information-rich histological
image space and information-poor segmentation map domain. Integrating
judiciously the concepts of optimal transport and measure theory, the model
develops an invertible generator, which provides an efficient optimization
framework with lower network complexity. The concept of invertible generator
automatically eliminates the need of any explicit cycle-consistency loss. The
proposed model also introduces a spatially-constrained squeeze operation within
the framework of invertible generator to maintain spatial continuity within the
image patches. The model provides a better trade-off between network complexity
and model performance compared to other existing models having complex network
architectures. The performance of the proposed deep generative model, along
with a comparison with state-of-the-art nuclei segmentation methods, is
demonstrated on publicly available histological image data sets.

</details>


### [609] [SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images](https://arxiv.org/abs/2506.07028)
*Suman Mahapatra,Pradipta Maji*

Main category: eess.IV

TL;DR: A new deep generative model is proposed for simultaneous nuclei segmentation and color normalization in histological images, integrating truncated normal distribution and spatial attention.


<details>
  <summary>Details</summary>
Motivation: Automated analysis of histological images requires accurate nuclei segmentation, but impermissible color variation in stained tissue images poses challenges. While color normalization can improve nuclei segmentation, it also relies on accurate segmentation.

Method: The model integrates the merits of truncated normal distribution and spatial attention, assuming latent color appearance information is independent of nuclei segmentation map and embedding map information. It uses a mixture of truncated normal distributions as prior for latent color appearance code to handle stain overlap.

Result: The performance of the approach has been demonstrated on publicly available standard histological image data sets, with a comparative analysis showing its effectiveness against state-of-the-art algorithms.

Conclusion: The novel deep generative model enables simultaneous nuclei segmentation and color normalization, making it generalizable and adaptable to variations in color appearance without affecting segmentation accuracy.

Abstract: Segmentation of nuclei regions from histological images is an important task
for automated computer-aided analysis of histological images, particularly in
the presence of impermissible color variation in the color appearance of
stained tissue images. While color normalization enables better nuclei
segmentation, accurate segmentation of nuclei structures makes color
normalization rather trivial. In this respect, the paper proposes a novel deep
generative model for simultaneously segmenting nuclei structures and
normalizing color appearance of stained histological images.This model
judiciously integrates the merits of truncated normal distribution and spatial
attention. The model assumes that the latent color appearance information,
corresponding to a particular histological image, is independent of respective
nuclei segmentation map as well as embedding map information. The disentangled
representation makes the model generalizable and adaptable as the modification
or loss in color appearance information cannot be able to affect the nuclei
segmentation map as well as embedding information. Also, for dealing with the
stain overlap of associated histochemical reagents, the prior for latent color
appearance code is assumed to be a mixture of truncated normal distributions.
The proposed model incorporates the concept of spatial attention for
segmentation of nuclei regions from histological images. The performance of the
proposed approach, along with a comparative analysis with related
state-of-the-art algorithms, has been demonstrated on publicly available
standard histological image data sets.

</details>
