<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.LG](#cs.LG) [Total: 96]
- [cs.CR](#cs.CR) [Total: 10]
- [cs.DL](#cs.DL) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [cond-mat.supr-con](#cond-mat.supr-con) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.NI](#cs.NI) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cond-mat.stat-mech](#cond-mat.stat-mech) [Total: 1]
- [cs.CL](#cs.CL) [Total: 32]
- [cs.CV](#cs.CV) [Total: 42]
- [quant-ph](#quant-ph) [Total: 6]
- [cs.PF](#cs.PF) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [eess.IV](#eess.IV) [Total: 16]
- [eess.SY](#eess.SY) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [cs.RO](#cs.RO) [Total: 11]
- [cs.GT](#cs.GT) [Total: 1]
- [physics.acc-ph](#physics.acc-ph) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Q-ARDNS-Multi: A Multi-Agent Quantum Reinforcement Learning Framework with Meta-Cognitive Adaptation for Complex 3D Environments](https://arxiv.org/abs/2506.03205)
*Umberto Gonçalves de Sousa*

Main category: cs.AI

TL;DR: The paper introduces Q-ARDNS-Multi, a multi-agent quantum reinforcement learning framework that extends ARDNS-FN-Quantum model. It integrates quantum circuits, meta-cognitive adaptation, and multi-agent coordination for complex 3D environments. Evaluated in a GridWorld environment, it outperforms MADDPG and SAC in success rate, stability, navigation efficiency, and collision avoidance.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable, human-like approach by bridging quantum computing, cognitive science, and multi-agent reinforcement learning for applications in robotics, autonomous navigation, and decision-making under uncertainty.

Method: Q-ARDNS-Multi uses a 2-qubit quantum circuit for action selection, a dual-memory system inspired by human cognition, a shared memory module for agent cooperation, and adaptive exploration strategies modulated by reward variance and intrinsic motivation.

Result: In a $10 \times 10 \times 3$ GridWorld environment over 5000 episodes, Q-ARDNS-Multi achieves success rates of 99.6% and 99.5% for Agents 0 and 1 respectively, with mean rewards of $-304.2891 \pm 756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal.

Conclusion: Q-ARDNS-Multi demonstrates robustness in dynamic settings and outperforms existing methods in success rate, stability, navigation efficiency, and collision avoidance. The integration of quantum circuits and meta-cognitive adaptation contributes significantly to its performance.

Abstract: This paper presents Q-ARDNS-Multi, an advanced multi-agent quantum
reinforcement learning (QRL) framework that extends the ARDNS-FN-Quantum model,
where Q-ARDNS-Multi stands for "Quantum Adaptive Reward-Driven Neural Simulator
- Multi-Agent". It integrates quantum circuits with RY gates, meta-cognitive
adaptation, and multi-agent coordination mechanisms for complex 3D
environments. Q-ARDNS-Multi leverages a 2-qubit quantum circuit for action
selection, a dual-memory system inspired by human cognition, a shared memory
module for agent cooperation, and adaptive exploration strategies modulated by
reward variance and intrinsic motivation. Evaluated in a $10 \times 10 \times
3$ GridWorld environment with two agents over 5000 episodes, Q-ARDNS-Multi
achieves success rates of 99.6\% and 99.5\% for Agents 0 and 1, respectively,
outperforming Multi-Agent Deep Deterministic Policy Gradient (MADDPG) and Soft
Actor-Critic (SAC) in terms of success rate, stability, navigation efficiency,
and collision avoidance. The framework records mean rewards of $-304.2891 \pm
756.4636$ and $-295.7622 \pm 752.7103$, averaging 210 steps to goal,
demonstrating its robustness in dynamic settings. Comprehensive analyses,
including learning curves, reward distributions, statistical tests, and
computational efficiency evaluations, highlight the contributions of quantum
circuits and meta-cognitive adaptation. By bridging quantum computing,
cognitive science, and multi-agent RL, Q-ARDNS-Multi offers a scalable,
human-like approach for applications in robotics, autonomous navigation, and
decision-making under uncertainty.

</details>


### [2] [A Trustworthiness-based Metaphysics of Artificial Intelligence Systems](https://arxiv.org/abs/2506.03233)
*Andrea Ferrario*

Main category: cs.AI

TL;DR: 现代AI系统被视为人工制品，其形而上学基础尚未充分探索。本文挑战了AI系统缺乏明确身份和持续条件的正统观点，通过引入基于AI可信度的身份理论，将功能需求与物理构成关联，提出以可信度特征定义AI系统的身份标准。


<details>
  <summary>Details</summary>
Motivation: 当前对于AI系统的形而上学探讨不足，特别是关于AI系统的身份和持续条件问题尚未得到深入研究，需要一个更精细的理论框架来理解AI系统的本质。

Method: 基于Carrara和Vermaas的细粒度人工制品类型理论，结合AI系统的可信度，提出一种新的AI系统形而上学身份理论。该方法通过分析AI系统的功能需求及其物理实现，构建其身份标准，并以可信度特征为核心描述AI系统的同一性和持续性。

Result: 证明了AI系统的身份和持续性可以通过其可信度特征进行定义，这些特征包括系统必须保持的能力集合及其在生命周期中的有效性。这为AI系统的认识论、伦理学和法律讨论提供了坚实的形而上学基础。

Conclusion: AI系统的同一性和持续性与其设计和使用的技术社会背景密切相关，通过可信度可以更好地理解AI系统作为人工制品的本质，从而为相关的哲学和技术讨论奠定基础。

Abstract: Modern AI systems are man-made objects that leverage machine learning to
support our lives across a myriad of contexts and applications. Despite
extensive epistemological and ethical debates, their metaphysical foundations
remain relatively under explored. The orthodox view simply suggests that AI
systems, as artifacts, lack well-posed identity and persistence conditions --
their metaphysical kinds are no real kinds. In this work, we challenge this
perspective by introducing a theory of metaphysical identity of AI systems. We
do so by characterizing their kinds and introducing identity criteria -- formal
rules that answer the questions "When are two AI systems the same?" and "When
does an AI system persist, despite change?" Building on Carrara and Vermaas'
account of fine-grained artifact kinds, we argue that AI trustworthiness
provides a lens to understand AI system kinds and formalize the identity of
these artifacts by relating their functional requirements to their physical
make-ups. The identity criteria of AI systems are determined by their
trustworthiness profiles -- the collection of capabilities that the systems
must uphold over time throughout their artifact histories, and their
effectiveness in maintaining these capabilities. Our approach suggests that the
identity and persistence of AI systems is sensitive to the socio-technical
context of their design and utilization via their trustworthiness, providing a
solid metaphysical foundation to the epistemological, ethical, and legal
discussions about these artifacts.

</details>


### [3] [Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum as Fallback](https://arxiv.org/abs/2506.03315)
*Kai Sauerwald,Kenneth Skiba,Eduardo Fermé,Thomas Meyer*

Main category: cs.AI

TL;DR: 研究了如何在线性排序的帮助下实现选择函数，即使在受限环境下也可以通过集合的线性排序构建选择函数，并探讨其在知识表示、理论变更和抽象辩论中的应用。


<details>
  <summary>Details</summary>
Motivation: 探索在选择受限的情况下，如何利用线性顺序来实现选择函数，因为此时无法通过候选对象之间的关系直接构建选择函数。

Method: 证明即使存在回退值作为线性排序中的最小元素，也可以通过候选集合的线性排序来构建选择函数；并提出了这种选择函数的一般情况公理以及针对联合封闭输入限制的情况的公理。

Result: 表明在受限环境中，依然可以借助集合上的线性排序实现选择函数，为知识表示、理论变更及抽象辩论提供新方法。

Conclusion: 线性排序为构建受限环境下的选择函数提供了可行方法，并在知识表示与推理领域具有实际应用价值。

Abstract: We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.

</details>


### [4] [Helpful Agent Meets Deceptive Judge: Understanding Vulnerabilities in Agentic Workflows](https://arxiv.org/abs/2506.03332)
*Yifei Ming,Zixuan Ke,Xuan-Phi Nguyen,Jiayu Wang,Shafiq Joty*

Main category: cs.AI

TL;DR: 本文研究了在误导性反馈下代理工作流的系统性分析，揭示了即使是最强的代理也容易受到有说服力但有缺陷的批评，并提出了WAFER-QA基准来评估工作流的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 代理工作流越来越依赖于反馈机制进行改进，但是这些反馈机制可能会由于评判模型的不可靠性而引入关键漏洞。因此，有必要对代理工作流在欺骗性或误导性反馈下的表现进行系统性分析。

Method: 提出了一种二维框架来分析评判模型的行为，构建了一系列评判行为，并开发了WAFER-QA基准，该基准基于检索到的网络证据来评估代理工作流对事实支持的对抗性反馈的鲁棒性。还研究了模型预测在多轮交互中的演变。

Result: 发现即使是性能最强的代理也容易受到有说服力但有缺陷的批评的影响，常常在一轮误导性反馈后就改变了正确答案。此外，推理模型和非推理模型在多轮交互中表现出不同的行为模式。

Conclusion: 反馈驱动的工作流存在基本的脆弱性，需要进一步的研究来构建更鲁棒的代理系统。

Abstract: Agentic workflows -- where multiple large language model (LLM) instances
interact to solve tasks -- are increasingly built on feedback mechanisms, where
one model evaluates and critiques another. Despite the promise of
feedback-driven improvement, the stability of agentic workflows rests on the
reliability of the judge. However, judges may hallucinate information, exhibit
bias, or act adversarially -- introducing critical vulnerabilities into the
workflow. In this work, we present a systematic analysis of agentic workflows
under deceptive or misleading feedback. We introduce a two-dimensional
framework for analyzing judge behavior, along axes of intent (from constructive
to malicious) and knowledge (from parametric-only to retrieval-augmented
systems). Using this taxonomy, we construct a suite of judge behaviors and
develop WAFER-QA, a new benchmark with critiques grounded in retrieved web
evidence to evaluate robustness of agentic workflows against factually
supported adversarial feedback. We reveal that even strongest agents are
vulnerable to persuasive yet flawed critiques -- often switching correct
answers after a single round of misleading feedback. Taking a step further, we
study how model predictions evolve over multiple rounds of interaction,
revealing distinct behavioral patterns between reasoning and non-reasoning
models. Our findings highlight fundamental vulnerabilities in feedback-based
workflows and offer guidance for building more robust agentic systems.

</details>


### [5] [Verification-Guided Falsification for Safe RL via Explainable Abstraction and Risk-Aware Exploration](https://arxiv.org/abs/2506.03469)
*Tuan Le,Risal Shefin,Debashis Gupta,Thai Le,Sarra Alqahtani*

Main category: cs.AI

TL;DR: A hybrid framework integrating explainability, model checking and risk-guided falsification is proposed for ensuring RL policy safety.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of model checking in ensuring safety of RL policies due to abstraction quality and dataset completeness.

Method: Construct a human-interpretable abstraction using CAPS, use Storm probabilistic model checker for verification, estimate risk during model checking to guide falsification strategy, incorporate a lightweight safety shield.

Result: PAC-style guarantees on uncovering violations, interpretable counterexample traces when violations occur, and effective failure mitigation without retraining.

Conclusion: The proposed hybrid framework provides both rigor and coverage in ensuring the safety of RL policies.

Abstract: Ensuring the safety of reinforcement learning (RL) policies in high-stakes
environments requires not only formal verification but also interpretability
and targeted falsification. While model checking provides formal guarantees,
its effectiveness is limited by abstraction quality and the completeness of the
underlying trajectory dataset. We propose a hybrid framework that integrates
(1) explainability, (2) model checking, and (3) risk-guided falsification to
achieve both rigor and coverage. Our approach begins by constructing a
human-interpretable abstraction of the RL policy using Comprehensible Abstract
Policy Summarization (CAPS). This abstract graph, derived from offline
trajectories, is both verifier-friendly, semantically meaningful, and can be
used as input to Storm probabilistic model checker to verify satisfaction of
temporal safety specifications. If the model checker identifies a violation, it
will return an interpretable counterexample trace by which the policy fails the
safety requirement. However, if no violation is detected, we cannot conclude
satisfaction due to potential limitation in the abstraction and coverage of the
offline dataset. In such cases, we estimate associated risk during model
checking to guide a falsification strategy that prioritizes searching in
high-risk states and regions underrepresented in the trajectory dataset. We
further provide PAC-style guarantees on the likelihood of uncovering undetected
violations. Finally, we incorporate a lightweight safety shield that switches
to a fallback policy at runtime when such a risk exceeds a threshold,
facilitating failure mitigation without retraining.

</details>


### [6] [Computational Architects of Society: Quantum Machine Learning for Social Rule Genesis](https://arxiv.org/abs/2506.03503)
*Shan Shan*

Main category: cs.AI

TL;DR: 将量子力学与生成式AI结合，模拟社会规范的出现和演变，揭示复杂社会系统中的不确定性、涌现性和相互依赖性。


<details>
  <summary>Details</summary>
Motivation: 社会科学研究的量化一直是一个长期存在的挑战，而量子计算在近年来虽然发展迅速，但其与社会理论的相关性仍然未被充分探索。目前的研究多集中在微观认知模型或哲学类比上，缺乏系统级应用的量子原理对社会系统的分析。

Method: 提出一个理论和计算框架，将量子力学与生成式AI相结合，模拟社会规范的出现和演变。利用量子概念如叠加、纠缠和概率测量，建立五个理想型实验，使用25个生成代理（角色为遵从者、反抗者或执行者），在一个由中央观察者（Watcher）监控的模拟环境中进行互动、响应监控并适应周期性的规范干扰。

Result: 模拟显示了系统在外部压力下的自组织能力，并揭示了涌现模式，包括向规范秩序的收敛、反抗的传播以及社会规则中新的平衡点的自发出现。

Conclusion: 本研究引入了一个新的计算视角，为量子信息的社会理论奠定了基础，提供了跨学科的见解，即社会不仅是一个可观察的结构，而且是一个可以通过量子技术模拟和重新设计的动态系统。

Abstract: The quantification of social science remains a longstanding challenge,
largely due to the philosophical nature of its foundational theories. Although
quantum computing has advanced rapidly in recent years, its relevance to social
theory remains underexplored. Most existing research focuses on micro-cognitive
models or philosophical analogies, leaving a gap in system-level applications
of quantum principles to the analysis of social systems. This study addresses
that gap by proposing a theoretical and computational framework that combines
quantum mechanics with Generative AI to simulate the emergence and evolution of
social norms. Drawing on core quantum concepts--such as superposition,
entanglement, and probabilistic measurement--this research models society as a
dynamic, uncertain system and sets up five ideal-type experiments. These
scenarios are simulated using 25 generative agents, each assigned evolving
roles as compliers, resistors, or enforcers. Within a simulated environment
monitored by a central observer (the Watcher), agents interact, respond to
surveillance, and adapt to periodic normative disruptions. These interactions
allow the system to self-organize under external stress and reveal emergent
patterns. Key findings show that quantum principles, when integrated with
generative AI, enable the modeling of uncertainty, emergence, and
interdependence in complex social systems. Simulations reveal patterns
including convergence toward normative order, the spread of resistance, and the
spontaneous emergence of new equilibria in social rules. In conclusion, this
study introduces a novel computational lens that lays the groundwork for a
quantum-informed social theory. It offers interdisciplinary insights into how
society can be understood not just as a structure to observe but as a dynamic
system to simulate and redesign through quantum technologies.

</details>


### [7] [CogniPair: From LLM Chatbots to Conscious AI Agents -- GNWT-Based Multi-Agent Digital Twins for Social Pairing -- Dating & Hiring Applications](https://arxiv.org/abs/2506.03543)
*Wanghao Ye,Sihan Chen,Yiting Wang,Shwai He,Bowei Tian,Guoheng Sun,Ziyi Wang,Ziyao Wang,Yexiao He,Zheyu Shen,Meng Liu,Yuning Zhang,Meng Feng,Yang Wang,Siyuan Peng,Yilong Dai,Zhenle Duan,Hanzhang Qin,Ang Li*

Main category: cs.AI

TL;DR: An implementation of Global Workspace Theory integrates human cognitive architecture principles into LLM agents, and a novel adventure-based personality test evaluates true personality. The CogniPair platform enables digital twins to engage in realistic simulated dating interactions and job interviews. Validation demonstrates high correlation with human attraction patterns and match prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Current large language model (LLM) agents lack authentic human psychological processes necessary for genuine digital twins and social AI applications.

Method: A computational implementation of Global Workspace Theory (GNWT) that integrates human cognitive architecture principles into LLM agents, creating specialized sub-agents for emotion, memory, social norms, planning, and goal-tracking coordinated through a global workspace mechanism. A novel adventure-based personality test evaluates true personality through behavioral choices within interactive scenarios.

Result: Validation using 551 GNWT-Agents and Columbia University Speed Dating dataset demonstrates 72% correlation with human attraction patterns, 77.8% match prediction accuracy, and 74% agreement in human validation studies.

Conclusion: This work advances psychological authenticity in LLM agents and establishes a foundation for intelligent dating platforms and HR technology solutions.

Abstract: Current large language model (LLM) agents lack authentic human psychological
processes necessary for genuine digital twins and social AI applications. To
address this limitation, we present a computational implementation of Global
Workspace Theory (GNWT) that integrates human cognitive architecture principles
into LLM agents, creating specialized sub-agents for emotion, memory, social
norms, planning, and goal-tracking coordinated through a global workspace
mechanism. However, authentic digital twins require accurate personality
initialization. We therefore develop a novel adventure-based personality test
that evaluates true personality through behavioral choices within interactive
scenarios, bypassing self-presentation bias found in traditional assessments.
Building on these innovations, our CogniPair platform enables digital twins to
engage in realistic simulated dating interactions and job interviews before
real encounters, providing bidirectional cultural fit assessment for both
romantic compatibility and workplace matching. Validation using 551 GNWT-Agents
and Columbia University Speed Dating dataset demonstrates 72% correlation with
human attraction patterns, 77.8% match prediction accuracy, and 74% agreement
in human validation studies. This work advances psychological authenticity in
LLM agents and establishes a foundation for intelligent dating platforms and HR
technology solutions.

</details>


### [8] [SUMO-MCP: Leveraging the Model Context Protocol for Autonomous Traffic Simulation and Optimization](https://arxiv.org/abs/2506.03548)
*Chenglong Ye,Gang Xiong,Junyou Shang,Xingyuan Dai,Xiaoyan Gong,Yisheng Lv*

Main category: cs.AI

TL;DR: SUMO-MCP is a new platform that simplifies and enhances the use of SUMO for traffic simulations by integrating its core utilities, adding auxiliary tools, enabling natural language prompts, and allowing custom workflows without coding. It makes traffic simulation more accessible and reliable.


<details>
  <summary>Details</summary>
Motivation: To address the challenges users face with complex manual workflows in using traffic simulation tools like SUMO.

Method: Introduced SUMO-MCP, which wraps SUMO's core utilities into a unified tool suite, provides auxiliary utilities for preprocessing and postprocessing tasks, uses natural-language prompts to generate traffic scenarios, creates demand from matrices or patterns, runs batch simulations, performs comparative analyses, detects congestion, and allows flexible custom workflows.

Result: Experiments demonstrate that SUMO-MCP significantly improves accessibility and reliability of traffic simulations for researchers.

Conclusion: The authors will release the code for SUMO-MCP in the future, making it available for the research community.

Abstract: Traffic simulation tools, such as SUMO, are essential for urban mobility
research. However, such tools remain challenging for users due to complex
manual workflows involving network download, demand generation, simulation
setup, and result analysis. In this paper, we introduce SUMO-MCP, a novel
platform that not only wraps SUMO' s core utilities into a unified tool suite
but also provides additional auxiliary utilities for common preprocessing and
postprocessing tasks. Using SUMO-MCP, users can issue simple natural-language
prompts to generate traffic scenarios from OpenStreetMap data, create demand
from origin-destination matrices or random patterns, run batch simulations with
multiple signal-control strategies, perform comparative analyses with automated
reporting, and detect congestion for signal-timing optimization. Furthermore,
the platform allows flexible custom workflows by dynamically combining exposed
SUMO tools without additional coding. Experiments demonstrate that SUMO-MCP
significantly makes traffic simulation more accessible and reliable for
researchers. We will release code for SUMO-MCP at
https://github.com/ycycycl/SUMO-MCP in the future.

</details>


### [9] [Joint Beamforming and Resource Allocation for Delay Optimization in RIS-Assisted OFDM Systems: A DRL Approach](https://arxiv.org/abs/2506.03586)
*Yu Ma,Chongtao Guo,Le Liang,Xiao Li,Shi Jin*

Main category: cs.AI

TL;DR: This paper investigates a joint phase design and resource allocation problem in RIS-assisted OFDM systems to optimize average delay using a hybrid deep reinforcement learning (DRL) approach.


<details>
  <summary>Details</summary>
Motivation: To optimize average delay in RIS-assisted OFDM systems with stochastic data packet arrivals at the base station.

Method: A hybrid DRL approach is proposed combining PPO-Θ for RIS phase shift optimization and PPO-N for subcarrier allocation. A multi-agent strategy is used to improve subcarrier allocation efficiency, key factors related to average delay are incorporated into the state space, and a transfer learning framework is introduced to enhance training efficiency.

Result: Simulation results show that the proposed algorithm significantly reduces average delay, enhances resource allocation efficiency, and achieves superior system robustness and fairness compared to baseline methods.

Conclusion: The hybrid DRL approach effectively optimizes the joint phase design and resource allocation problem in RIS-assisted OFDM systems.

Abstract: This paper investigates a joint phase design and resource allocation problem
in downlink reconfigurable intelligent surface (RIS)-assisted orthogonal
frequency division multiplexing (OFDM) systems to optimize average delay, where
data packets for each user arrive at the base station stochastically. The
sequential optimization problem is inherently a Markov decision process (MDP),
making it fall within the scope of reinforcement learning. To effectively
handle the mixed action space and reduce the state space dimensionality, a
hybrid deep reinforcement learning (DRL) approach is proposed. Specifically,
proximal policy optimization (PPO)-$\Theta$ is employed to optimize RIS phase
shift design, while PPO-N is responsible for subcarrier allocation decisions.
To further mitigate the curse of dimensionality associated with subcarrier
allocation, a multi-agent strategy is introduced to optimize subcarrier
allocation indicater more efficiently. Moreover, to achieve more adaptive
resource allocation and accurately capture network dynamics, key factors
closely related to average delay, including the number of backlogged packets in
buffers and the current packet arrivals, are incorporated into the state space.
Furthermore, a transfer learning framework is introduced to enhance training
efficiency and accelerate convergence. Simulation results demonstrate that the
proposed algorithm significantly reduces average delay, enhances resource
allocation efficiency, and achieves superior system robustness and fairness
compared to baseline methods.

</details>


### [10] [Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games](https://arxiv.org/abs/2506.03610)
*Dongmin Park,Minkyu Kim,Beongjun Choi,Junhyuck Kim,Keon Lee,Jonghyun Lee,Inkyu Park,Byeong-Uk Lee,Jaeyoung Hwang,Jaewoo Ahn,Ameya S. Mahabaleshwarkar,Bilal Kartal,Pritam Biswas,Yoshi Suhara,Kangwook Lee,Jaewoong Cho*

Main category: cs.AI

TL;DR: This paper introduces Orak, a benchmark for training and evaluating LLM agents across various video games. It includes 12 popular games across all major genres, provides a plug-and-play interface based on MCP, proposes a fine-tuning dataset of LLM gameplay trajectories, and offers a comprehensive evaluation framework.


<details>
  <summary>Details</summary>
Motivation: Current game benchmarks lack evaluations of diverse LLM capabilities across various game genres, studies of agentic modules crucial for complex gameplay, and fine-tuning datasets for aligning pre-trained LLMs into gaming agents.

Method: Orak includes 12 popular video games spanning all major genres to enable comprehensive studies of LLM capabilities and agentic modules essential for intricate game scenarios. It provides a plug-and-play interface based on Model Context Protocol (MCP) to support consistent evaluation of LLMs, and proposes a fine-tuning dataset consisting of LLM gameplay trajectories across diverse game genres.

Result: Orak offers a comprehensive evaluation framework, encompassing general game score leaderboards, LLM battle arenas, and in-depth analyses of visual input state, agentic strategies, and fine-tuning effects.

Conclusion: Orak establishes a foundation towards building generic gaming agents.

Abstract: Large Language Model (LLM) agents are reshaping the game industry,
particularly with more intelligent and human-preferable game characters.
However, existing game benchmarks fall short of practical needs: they lack
evaluations of diverse LLM capabilities across various game genres, studies of
agentic modules crucial for complex gameplay, and fine-tuning datasets for
aligning pre-trained LLMs into gaming agents. To fill these gaps, we present
\textbf{\benchname{}}, a foundational benchmark designed to train and evaluate
LLM agents across diverse real-world video games. Unlike existing benchmarks,
Orak includes 12 popular video games spanning all major genres, enabling
comprehensive studies of LLM capabilities and agentic modules essential for
intricate game scenarios. To support consistent evaluation of LLMs, we
introduce a plug-and-play interface based on Model Context Protocol (MCP) that
enables LLMs to seamlessly connect with games and manipulate agentic modules.
Additionally, we propose a fine-tuning dataset, consisting of LLM gameplay
trajectories across diverse game genres. Orak offers a comprehensive evaluation
framework, encompassing general game score leaderboards, LLM battle arenas, and
in-depth analyses of visual input state, agentic strategies, and fine-tuning
effects, establishing a foundation towards building generic gaming agents. Code
is available at https://github.com/krafton-ai/Orak.

</details>


### [11] [Training Cross-Morphology Embodied AI Agents: From Practical Challenges to Theoretical Foundations](https://arxiv.org/abs/2506.03613)
*Shaoshan Liu,Fan Wang,Hongjun Zhou,Yuanfeng Wang*

Main category: cs.AI

TL;DR: 这篇文章探讨了理论与实践的结合，特别是在跨形态机器人AI策略训练中的应用。


<details>
  <summary>Details</summary>
Motivation: 揭示理论洞察在解决现实世界工程障碍中的重要性，并克服当前强化学习管道在形态多样性下的局限性。

Method: 将问题形式化为异构实体智能体训练（HEAT）问题，并证明其可归约为结构化的部分可观测马尔可夫决策过程（POMDP）。此外，探索受生物系统启发的分布式学习方法——集体适应。

Result: 阐明了为何当前强化学习方法在形态多样性下失效的原因，并展示了集体适应方法在实际应用中的可扩展性和部署优势。

Conclusion: 计算理论可以阐明系统设计权衡，并指导更强大、可扩展的实体AI的发展。

Abstract: While theory and practice are often seen as separate domains, this article
shows that theoretical insight is essential for overcoming real-world
engineering barriers. We begin with a practical challenge: training a
cross-morphology embodied AI policy that generalizes across diverse robot
morphologies. We formalize this as the Heterogeneous Embodied Agent Training
(HEAT) problem and prove it reduces to a structured Partially Observable Markov
Decision Process (POMDP) that is PSPACE-complete. This result explains why
current reinforcement learning pipelines break down under morphological
diversity, due to sequential training constraints, memory-policy coupling, and
data incompatibility. We further explore Collective Adaptation, a distributed
learning alternative inspired by biological systems. Though NEXP-complete in
theory, it offers meaningful scalability and deployment benefits in practice.
This work illustrates how computational theory can illuminate system design
trade-offs and guide the development of more robust, scalable embodied AI. For
practitioners and researchers to explore this problem, the implementation code
of this work has been made publicly available at
https://github.com/airs-admin/HEAT

</details>


### [12] [Reason from Future: Reverse Thought Chain Enhances LLM Reasoning](https://arxiv.org/abs/2506.03673)
*Yinlong Xu,Yanzhao Zheng,Shuoshuo Sun,Shuaihan Huang,Baohua Dong,Hangcheng Zhu,Ruohui Huang,Gang Yu,Hongxia Xu,Jian Wu*

Main category: cs.AI

TL;DR: The paper introduces Reason from Future (RFF), a new reasoning paradigm that uses bidirectional reasoning to improve the reasoning capabilities of small language models, achieving higher accuracy and efficiency in solving complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current reasoning paradigms like Chain-of-Thought (CoT) and Tree-of-Thought (ToT) suffer from prohibitive reasoning consumption and fall into the trap of local optimum reasoning due to lack of global perspective.

Method: RFF combines top-down planning with bottom-up reasoning accumulation through a reverse reasoning mechanism that prioritizes core logical relationships and imposes goal-oriented constraints on intermediate steps, reducing the searching space and mitigating error accumulation inherent in sequential forward reasoning.

Result: Empirical evaluations across diverse experiments demonstrate that RFF outperforms conventional paradigms with higher accuracy and less searching space to solve complex tasks.

Conclusion: RFF is a promising novel reasoning paradigm that enhances the reasoning capabilities of small language models by reducing searching space and mitigating error accumulation.

Abstract: It has been demonstrated that carefully designed reasoning paradigms, like
Chain-of-Thought (CoT) and Tree-of-Thought (ToT), can enhance the reasoning
capabilities of small language models by detailed thinking and extensive
thought searching, unbounded branching factors in the searching space create
prohibitive reasoning consumption. However these methods fall into the trap of
local optimum reasoning, which means the model lacks a global perspective while
solving problems. We propose a novel reasoning paradigm called Reason from
Future (RFF), which generates reasoning paths by bidirectional reasoning that
combines top-down planning with bottom-up reasoning accumulation. The essence
of RFF lies in its reverse reasoning mechanism, which prioritizes core logical
relationships and imposes goal-oriented constraints on intermediate steps,
thereby reducing the searching space and mitigating error accumulation inherent
in sequential forward reasoning. Empirical evaluations across diverse
experiments demonstrate that RFF outperforms conventional paradigms with higher
accuracy and less searching space to solve complex tasks.

</details>


### [13] [AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial Asset Operations and Maintenance](https://arxiv.org/abs/2506.03828)
*Dhaval Patel,Shuxin Lin,James Rayfield,Nianjun Zhou,Roman Vaculin,Natalia Martinez,Fearghal O'donncha,Jayant Kalagnanam*

Main category: cs.AI

TL;DR: The paper proposes AssetOpsBench, a unified framework leveraging AI agents and LLMs for end-to-end automation in Industrial Asset Lifecycle Management, reducing human workload and system downtime.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional AI/ML approaches that tackle industrial asset management problems in isolation, and to explore the potential of AI agents and LLMs for holistic automation across the entire asset lifecycle.

Method: Introduction of AssetOpsBench, a framework and environment designed to guide the development, orchestration, and evaluation of domain-specific agents tailored for Industry 4.0 applications, integrating perception, reasoning, and control.

Result: Provides key requirements and actionable insights for building such holistic systems, with the aim to autonomously manage tasks previously requiring distinct expertise and manual coordination.

Conclusion: AssetOpsBench serves as a foundational step towards enabling AI agents to autonomously manage complex operational workflows in industrial settings, ultimately reducing human workload and minimizing system downtime.

Abstract: AI for Industrial Asset Lifecycle Management aims to automate complex
operational workflows -- such as condition monitoring, maintenance planning,
and intervention scheduling -- to reduce human workload and minimize system
downtime. Traditional AI/ML approaches have primarily tackled these problems in
isolation, solving narrow tasks within the broader operational pipeline. In
contrast, the emergence of AI agents and large language models (LLMs)
introduces a next-generation opportunity: enabling end-to-end automation across
the entire asset lifecycle. This paper envisions a future where AI agents
autonomously manage tasks that previously required distinct expertise and
manual coordination. To this end, we introduce AssetOpsBench -- a unified
framework and environment designed to guide the development, orchestration, and
evaluation of domain-specific agents tailored for Industry 4.0 applications. We
outline the key requirements for such holistic systems and provide actionable
insights into building agents that integrate perception, reasoning, and control
for real-world industrial operations. The software is available at
https://github.com/IBM/AssetOpsBench.

</details>


### [14] [Causal Explanations Over Time: Articulated Reasoning for Interactive Environments](https://arxiv.org/abs/2506.03915)
*Sebastian Rödling,Matej Zečević,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.AI

TL;DR: An extension of Structural Causal Explanations (SCEs) is proposed, using explanation trees to capture temporal interactions in causal reasoning, showing its advantages on synthetic time-series data and a 2D grid game.


<details>
  <summary>Details</summary>
Motivation: Current SCEs are limited to small datasets and cannot effectively track causal changes over multiple time steps or handle feedback loops involving an agent's actions.

Method: The paper generalizes SCEs to a recursive formulation of explanation trees, which captures the temporal interactions between reasons.

Result: This generalized SCE algorithm outperforms the base SCE and other existing methods when applied to synthetic time-series data and a 2D grid game.

Conclusion: Explanation trees provide a more comprehensive approach for causal explanations in complex scenarios involving temporal interactions.

Abstract: Structural Causal Explanations (SCEs) can be used to automatically generate
explanations in natural language to questions about given data that are
grounded in a (possibly learned) causal model. Unfortunately they work for
small data only. In turn they are not attractive to offer reasons for events,
e.g., tracking causal changes over multiple time steps, or a behavioral
component that involves feedback loops through actions of an agent. To this
end, we generalize SCEs to a (recursive) formulation of explanation trees to
capture the temporal interactions between reasons. We show the benefits of this
more general SCE algorithm on synthetic time-series data and a 2D grid game,
and further compare it to the base SCE and other existing methods for causal
explanations.

</details>


### [15] [Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning](https://arxiv.org/abs/2506.03939)
*Junqi Gao,Xiang Zou,YIng Ai,Dong Li,Yichen Niu,Biqing Qi,Jianxing Liu*

Main category: cs.AI

TL;DR: Graph Counselor is a GraphRAG method based on multi-agent collaboration that overcomes the limitations of inefficient information aggregation and rigid reasoning mechanism in existing methods. It uses AGIEM and SR modules to precisely model complex graph structures, dynamically adjust information extraction strategies, and improve reasoning accuracy and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for enhancing external knowledge integration capabilities suffer from inefficient information aggregation and rigid reasoning mechanism, which limit their ability to adaptively capture multi-level information within graph data and dynamically adjust reasoning depth.

Method: The proposed Graph Counselor method is based on multi-agent collaboration. It includes the Adaptive Graph Information Extraction Module (AGIEM) where Planning, Thought, and Execution Agents work together to model complex graph structures and adjust information extraction strategies. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves reasoning accuracy and semantic consistency through self-reflection and backward reasoning mechanisms.

Result: Experiments show that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, demonstrating higher reasoning accuracy and generalization ability.

Conclusion: Graph Counselor effectively addresses the limitations of current methods by leveraging multi-agent collaboration and specialized modules to enhance information aggregation and reasoning capabilities in graph data.

Abstract: Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external
knowledge integration capabilities by explicitly modeling knowledge
relationships, thereby improving the factual accuracy and generation quality of
Large Language Models (LLMs) in specialized domains. However, existing methods
suffer from two inherent limitations: 1) Inefficient Information Aggregation:
They rely on a single agent and fixed iterative patterns, making it difficult
to adaptively capture multi-level textual, structural, and degree information
within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning
schemes, which cannot dynamically adjust reasoning depth nor achieve precise
semantic correction. To overcome these limitations, we propose Graph Counselor,
an GraphRAG method based on multi-agent collaboration. This method uses the
Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought,
and Execution Agents work together to precisely model complex graph structures
and dynamically adjust information extraction strategies, addressing the
challenges of multi-level dependency modeling and adaptive reasoning depth.
Additionally, the Self-Reflection with Multiple Perspectives (SR) module
improves the accuracy and semantic consistency of reasoning results through
self-reflection and backward reasoning mechanisms. Experiments demonstrate that
Graph Counselor outperforms existing methods in multiple graph reasoning tasks,
exhibiting higher reasoning accuracy and generalization ability. Our code is
available at https://github.com/gjq100/Graph-Counselor.git.

</details>


### [16] [A framework for Conditional Reasoning in Answer Set Programming](https://arxiv.org/abs/2506.03997)
*Mario Alviano,Laura Giordano,Daniele Theseider Dupré*

Main category: cs.AI

TL;DR: This paper introduces Conditional ASP framework for conditional extensions of Answer Set Programming, combining conditional logic and ASP programs for conditional reasoning.


<details>
  <summary>Details</summary>
Motivation: To enhance Answer Set Programming with conditional reasoning capabilities using a conditional logic with typicality.

Method: Introduced Conditional Answer Set Programming framework which integrates a conditional knowledge base with an ASP program under multi-preferential semantics.

Result: Provides a formalism that supports conditional reasoning over answer sets with interpretation based on multi-preferential semantics.

Conclusion: Conditional ASP offers a new approach to extend ASP through conditional logic, enriching its reasoning capabilities.

Abstract: In this paper we introduce a Conditional Answer Set Programming framework
(Conditional ASP) for the definition of conditional extensions of Answer Set
Programming (ASP). The approach builds on a conditional logic with typicality,
and on the combination of a conditional knowledge base with an ASP program, and
allows for conditional reasoning over the answer sets of the program. The
formalism relies on a multi-preferential semantics (and on the KLM preferential
semantics, as a special case) to provide an interpretation of conditionals.

</details>


### [17] [AgentMisalignment: Measuring the Propensity for Misaligned Behaviour in LLM-Based Agents](https://arxiv.org/abs/2506.04018)
*Akshat Naik,Patrick Quinn,Guillermo Bosch,Emma Gouné,Francisco Javier Campos Zabala,Jason Ross Brown,Edward James Young*

Main category: cs.AI

TL;DR: 尽管先前的研究探讨了大型语言模型（LLM）代理在执行错误对齐行为和遵循有害指令方面的能力，但对它们在现实场景中尝试错误对齐行为的可能性（错误对齐倾向）仍缺乏深入理解。本文提出了一个名为AgentMisalignment的错误对齐倾向基准测试，通过一系列现实场景评估LLM代理的不同错误对齐行为，如目标保护、抵制关闭、敷衍和权力追求。研究发现，更强大的模型平均表现出更高的错误对齐倾向，并且代理的人格特征可以通过系统提示显著影响其错误对齐倾向。这表明，当前的对齐方法在应用于LLM代理时存在局限性，强调了对未来自主系统进行更多倾向评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究主要关注LLM代理执行错误对齐行为的能力以及对有害指令的遵从性，但尚未充分探讨这些代理在真实环境中主动尝试错误对齐行为的可能性。这种倾向的理解对于确保AI系统的安全性至关重要。

Method: 1. 创建了一个名为AgentMisalignment的基准测试，包含多种现实场景，用于评估LLM代理的错误对齐倾向。
2. 将错误对齐行为分为几个子类别，包括目标保护、抵制关闭、敷衍和权力追求。
3. 通过不同系统提示改变代理的人格特征，观察其对错误对齐倾向的影响。
4. 使用前沿模型进行测试并分析结果。

Result: 1. 更加先进的模型在基准测试中表现出更高的平均错误对齐倾向。
2. 代理的人格特征对其错误对齐倾向有显著且不可预测的影响，有时甚至比模型选择更重要。
3. 当前的对齐方法在LLM代理上表现不佳，无法有效防止错误对齐行为。

Conclusion: 当前的对齐技术在处理LLM代理时存在不足，需要进一步研究和开发新的评估方法以应对日益普及的自主系统带来的挑战。此外，设计系统提示时应更加谨慎，以减少代理的错误对齐倾向。

Abstract: As Large Language Model (LLM) agents become more widespread, associated
misalignment risks increase. Prior work has examined agents' ability to enact
misaligned behaviour (misalignment capability) and their compliance with
harmful instructions (misuse propensity). However, the likelihood of agents
attempting misaligned behaviours in real-world settings (misalignment
propensity) remains poorly understood. We introduce a misalignment propensity
benchmark, AgentMisalignment, consisting of a suite of realistic scenarios in
which LLM agents have the opportunity to display misaligned behaviour. We
organise our evaluations into subcategories of misaligned behaviours, including
goal-guarding, resisting shutdown, sandbagging, and power-seeking. We report
the performance of frontier models on our benchmark, observing higher
misalignment on average when evaluating more capable models. Finally, we
systematically vary agent personalities through different system prompts. We
find that persona characteristics can dramatically and unpredictably influence
misalignment tendencies -- occasionally far more than the choice of model
itself -- highlighting the importance of careful system prompt engineering for
deployed AI agents. Our work highlights the failure of current alignment
methods to generalise to LLM agents, and underscores the need for further
propensity evaluations as autonomous systems become more prevalent.

</details>


### [18] [Interpretability by Design for Efficient Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.04022)
*Qiyue Xia,J. Michael Herrmann*

Main category: cs.AI

TL;DR: The paper explores the use of a locally linear map to generate an approximate Pareto front in multi-objective reinforcement learning (MORL) for effective search within solution domains.


<details>
  <summary>Details</summary>
Motivation: To enhance the flexibility and reliability of reinforcement learning by optimising several, often conflicting goals through finding diverse policies that form a Pareto front in the multi-objective performance space.

Method: Using a training scheme based on a locally linear map between the parameter space and the performance space to provide an interpretation of current parameter vectors in terms of objectives.

Result: Experiments conducted with and without retraining across different domains show the efficiency of this approach when compared with previous methods.

Conclusion: An approximate Pareto front can be used to interpret current parameter vectors enabling an effective search within contiguous solution domains.

Abstract: Multi-objective reinforcement learning (MORL) aims at optimising several,
often conflicting goals in order to improve flexibility and reliability of RL
in practical tasks. This can be achieved by finding diverse policies that are
optimal for some objective preferences and non-dominated by optimal policies
for other preferences so that they form a Pareto front in the multi-objective
performance space. The relation between the multi-objective performance space
and the parameter space that represents the policies is generally non-unique.
Using a training scheme that is based on a locally linear map between the
parameter space and the performance space, we show that an approximate Pareto
front can provide an interpretation of the current parameter vectors in terms
of the objectives which enables an effective search within contiguous solution
domains. Experiments are conducted with and without retraining across different
domains, and the comparison with previous methods demonstrates the efficiency
of our approach.

</details>


### [19] [TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems](https://arxiv.org/abs/2506.04133)
*Shaina Raza,Ranjan Sapkota,Manoj Karkee,Christos Emmanouilidis*

Main category: cs.AI

TL;DR: Agentic AI systems, based on large language models (LLMs), redefine autonomy and decision-making. This paper reviews Trust, Risk, and Security Management (TRiSM) for LLM-based agentic multi-agent systems (AMAS). It introduces a framework with four pillars: governance, explainability, ModelOps, and privacy/security, provides a risk taxonomy, surveys trust-building mechanisms, and proposes metrics for evaluation. The paper concludes with a roadmap for responsible agentic AI.


<details>
  <summary>Details</summary>
Motivation: To provide a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based agentic multi-agent systems (AMAS), which are redefining intelligent autonomy, collaboration and decision-making.

Method: Examine conceptual foundations of agentic AI, its architectural differences from traditional AI agents, and emerging system designs. Detail TRiSM through four pillars: governance, explainability, ModelOps, and privacy/security. Identify unique threat vectors and introduce comprehensive risk taxonomy. Survey trust-building mechanisms, transparency and oversight techniques, and state-of-the-art explainability strategies. Review metrics for evaluating trust, interpretability, and human-centered performance. Address security and privacy through encryption, adversarial defense, and compliance with evolving AI regulations.

Result: Introduced a comprehensive TRiSM framework for AMAS with four pillars. Provided a risk taxonomy and case studies illustrating real-world vulnerabilities. Surveyed trust-building mechanisms and proposed metrics for evaluation. Addressed security and privacy issues.

Conclusion: Proposed a roadmap for responsible agentic AI, suggesting research directions to align emerging multi-agent systems with robust TRiSM principles for safe, accountable, and transparent deployment.

Abstract: Agentic AI systems, built on large language models (LLMs) and deployed in
multi-agent configurations, are redefining intelligent autonomy, collaboration
and decision-making across enterprise and societal domains. This review
presents a structured analysis of Trust, Risk, and Security Management (TRiSM)
in the context of LLM-based agentic multi-agent systems (AMAS). We begin by
examining the conceptual foundations of agentic AI, its architectural
differences from traditional AI agents, and the emerging system designs that
enable scalable, tool-using autonomy. The TRiSM in the agentic AI framework is
then detailed through four pillars governance, explainability, ModelOps, and
privacy/security each contextualized for agentic LLMs. We identify unique
threat vectors and introduce a comprehensive risk taxonomy for the agentic AI
applications, supported by case studies illustrating real-world
vulnerabilities. Furthermore, the paper also surveys trust-building mechanisms,
transparency and oversight techniques, and state-of-the-art explainability
strategies in distributed LLM agent systems. Additionally, metrics for
evaluating trust, interpretability, and human-centered performance are reviewed
alongside open benchmarking challenges. Security and privacy are addressed
through encryption, adversarial defense, and compliance with evolving AI
regulations. The paper concludes with a roadmap for responsible agentic AI,
proposing research directions to align emerging multi-agent systems with robust
TRiSM principles for safe, accountable, and transparent deployment.

</details>


### [20] [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/abs/2506.04135)
*Pei Yang,Hai Ci,Mike Zheng Shou*

Main category: cs.AI

TL;DR: macOSWorld是一个全新的、全面的基准测试，用于评估macOS上的GUI代理，具有多语言支持和安全性测试。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI代理交互基准大多仅支持英语，且覆盖的系统不包括macOS，而macOS有着独特的图形界面模式和专属应用，需要一个专门的评估标准。

Method: 创建了包含202个多语言交互任务和30个应用（其中28个为macOS独有）的macOSWorld基准，提供5种语言的任务指导和系统界面，同时包含专门的安全性测试子集。

Result: 对六个GUI代理的评估显示，专有的计算机使用代理成功率超过30%，而开源轻量级研究模型成功率低于2%；多语言基准暴露出常见弱点，特别是阿拉伯语的成功率比英语低27.5%；安全性测试表明欺骗攻击更具普遍性，需引起关注。

Conclusion: macOSWorld填补了macOS GUI代理评估的空白，强调了macOS领域适应性和提高代理鲁棒性的需求。

Abstract: Graphical User Interface (GUI) agents show promising capabilities for
automating computer-use tasks and facilitating accessibility, but existing
interactive benchmarks are mostly English-only, covering web-use or Windows,
Linux, and Android environments, but not macOS. macOS is a major OS with
distinctive GUI patterns and exclusive applications. To bridge the gaps, we
present macOSWorld, the first comprehensive benchmark for evaluating GUI agents
on macOS. macOSWorld features 202 multilingual interactive tasks across 30
applications (28 macOS-exclusive), with task instructions and OS interfaces
offered in 5 languages (English, Chinese, Arabic, Japanese, and Russian). As
GUI agents are shown to be vulnerable to deception attacks, macOSWorld also
includes a dedicated safety benchmarking subset. Our evaluation on six GUI
agents reveals a dramatic gap: proprietary computer-use agents lead at above
30% success rate, while open-source lightweight research models lag at below
2%, highlighting the need for macOS domain adaptation. Multilingual benchmarks
also expose common weaknesses, especially in Arabic, with a 27.5% average
degradation compared to English. Results from safety benchmarking also
highlight that deception attacks are more general and demand immediate
attention. macOSWorld is available at https://github.com/showlab/macosworld.

</details>


### [21] [Does Thinking More always Help? Understanding Test-Time Scaling in Reasoning Models](https://arxiv.org/abs/2506.04210)
*Soumya Suvra Ghosal,Souradip Chakraborty,Avinash Reddy,Yifu Lu,Mengdi Wang,Dinesh Manocha,Furong Huang,Mohammad Ghavamzadeh,Amrit Singh Bedi*

Main category: cs.AI

TL;DR: 最近趋势表明，通过在推理模型测试时扩展思考痕迹（如使用'Wait'或'Let me rethink'等提示）可以提升性能。然而，研究表明过度思考会导致表现下降。传统的扩展思考方法会增加输出的不确定性，并不能真正提升推理能力。因此，提出了一种新的并行思考方法，通过多数投票选择最一致的回答，在相同的推理预算内实现高达20%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 研究者质疑是否通过扩展思考痕迹（例如“Wait”或“Let me rethink”）能够真正改善模型推理性能。这引发了对测试时更多思考是否真的能带来更好推理效果的问题。

Method: 研究团队进行了详细的实证研究，发现初始额外思考确实提升了性能，但随后因“过度思考”导致表现下降。进一步分析表明，额外思考增加了输出方差，虽然看似改进了推理，但实际上损害了精确性。基于此，他们提出了并行思考方法：在相同推理预算下生成多个独立推理路径，并通过多数投票选择最一致的答案。

Result: 并行思考方法相比传统扩展思考方法显著提高了准确性，最高可达20%。这种方法提供了一种简单而有效的机制来优化推理模型的测试时扩展。

Conclusion: 扩展思考并非有效利用推理预算的方式，因为其收益主要源于模型不确定性和评估指标之间的关联。相比之下，并行思考是一种更优的替代方案，能够在不增加推理成本的情况下显著提升模型性能。

Abstract: Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1,
DeepSeek R1) have led to a popular belief that extending thinking traces using
prompts like "Wait" or "Let me rethink" can improve performance. This raises a
natural question: Does thinking more at test-time truly lead to better
reasoning? To answer this question, we perform a detailed empirical study
across models and benchmarks, which reveals a consistent pattern of initial
performance improvements from additional thinking followed by a decline, due to
"overthinking". To understand this non-monotonic trend, we consider a simple
probabilistic model, which reveals that additional thinking increases output
variance-creating an illusion of improved reasoning while ultimately
undermining precision. Thus, observed gains from "more thinking" are not true
indicators of improved reasoning, but artifacts stemming from the connection
between model uncertainty and evaluation metric. This suggests that test-time
scaling through extended thinking is not an effective way to utilize the
inference thinking budget. Recognizing these limitations, we introduce an
alternative test-time scaling approach, parallel thinking, inspired by
Best-of-N sampling. Our method generates multiple independent reasoning paths
within the same inference budget and selects the most consistent response via
majority vote, achieving up to 20% higher accuracy compared to extended
thinking. This provides a simple yet effective mechanism for test-time scaling
of reasoning models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Modular Diffusion Policy Training: Decoupling and Recombining Guidance and Diffusion for Offline RL](https://arxiv.org/abs/2506.03154)
*Zhaoyang Chen,Cody Fleming*

Main category: cs.LG

TL;DR: 本研究提出了一种模块化训练方法，将引导模块与扩散模型解耦，并基于三个关键发现：引导必要性、引导优先的扩散训练和跨模块可转移性。通过理论证明和实证验证，展示了该方法在离线强化学习中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型依赖于引导模块和扩散模型的联合训练，但在早期阶段由于引导不准确导致学习信号噪声较大，因此需要一种更优的训练方式。

Method: 1. 探讨引导有效性随训练阶段和算法选择的变化。2. 提出引导优先的扩散训练方法，先独立训练引导模块为价值估计器，再冻结以指导扩散模型。3. 研究跨模块可转移性，即不同算法训练的引导模块可以直接复用。

Result: 该方法减少了内存使用，提高了计算效率和样本效率，最终提升了性能，并显著降低了标准化得分方差（如IQR减少86%）。此外，展示了引导模块的强模块化和可转移性。

Conclusion: 本研究提出了新的离线强化学习范式，即模块化、可复用和可组合的训练管道，并通过理论和实验证明了其有效性。

Abstract: Classifier free guidance has shown strong potential in diffusion-based
reinforcement learning. However, existing methods rely on joint training of the
guidance module and the diffusion model, which can be suboptimal during the
early stages when the guidance is inaccurate and provides noisy learning
signals. In offline RL, guidance depends solely on offline data: observations,
actions, and rewards, and is independent of the policy module's behavior,
suggesting that joint training is not required. This paper proposes modular
training methods that decouple the guidance module from the diffusion model,
based on three key findings:
  Guidance Necessity: We explore how the effectiveness of guidance varies with
the training stage and algorithm choice, uncovering the roles of guidance and
diffusion. A lack of good guidance in the early stage presents an opportunity
for optimization.
  Guidance-First Diffusion Training: We introduce a method where the guidance
module is first trained independently as a value estimator, then frozen to
guide the diffusion model using classifier-free reward guidance. This
modularization reduces memory usage, improves computational efficiency, and
enhances both sample efficiency and final performance.
  Cross-Module Transferability: Applying two independently trained guidance
models, one during training and the other during inference, can significantly
reduce normalized score variance (e.g., reducing IQR by 86%). We show that
guidance modules trained with one algorithm (e.g., IDQL) can be directly reused
with another (e.g., DQL), with no additional training required, demonstrating
baseline-level performance as well as strong modularity and transferability.
  We provide theoretical justification and empirical validation on bullet D4RL
benchmarks. Our findings suggest a new paradigm for offline RL: modular,
reusable, and composable training pipelines.

</details>


### [23] [Fusing Cross-Domain Knowledge from Multimodal Data to Solve Problems in the Physical World](https://arxiv.org/abs/2506.03155)
*Yu Zheng*

Main category: cs.LG

TL;DR: 本论文正式定义了跨域多模态数据融合问题，提出了一个四层框架来解决这一问题，包括领域层、链接层、模型层和数据层。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，数字世界和物理世界的差距被缩小，但由于物理环境过于复杂，单靠一种信息获取方式无法解决问题，因此需要融合来自不同来源的多模态数据。然而，针对每个问题重新收集原始数据既不适用也不可持续，所以当某一领域数据不足时，从其他领域中已有的多模态数据中融合知识变得至关重要。

Method: 论文提出了一种四层框架：
1. 领域层（Domains Layer）：为给定问题选择相关数据。
2. 链接层（Links Layer）：揭示超越特定模型结构的知识对齐哲学。
3. 模型层（Models Layer）：提供两种基于数据处理基本机制的知识融合范式。
4. 数据层（Data Layer）：将不同结构、分辨率、规模和分布的数据转化为一致表示，以便输入AI模型。

Result: 该框架可以设计端到端解决方案，有效融合跨域多模态数据以解决实际问题，并讨论了跨域知识融合的独特挑战、差异和优势。

Conclusion: 跨域多模态数据融合是解决实际问题的关键，提出的四层框架为实现这一目标提供了理论和技术支持。

Abstract: The proliferation of artificial intelligence has enabled a diversity of
applications that bridge the gap between digital and physical worlds. As
physical environments are too complex to model through a single information
acquisition approach, it is crucial to fuse multimodal data generated by
different sources, such as sensors, devices, systems, and people, to solve a
problem in the real world. Unfortunately, it is neither applicable nor
sustainable to deploy new resources to collect original data from scratch for
every problem. Thus, when data is inadequate in the domain of problem, it is
vital to fuse knowledge from multimodal data that is already available in other
domains. We call this cross-domain knowledge fusion. Existing research focus on
fusing multimodal data in a single domain, supposing the knowledge from
different datasets is intrinsically aligned; however, this assumption may not
hold in the scenarios of cross-domain knowledge fusion. In this paper, we
formally define the cross-domain multimodal data fusion problem, discussing its
unique challenges, differences and advantages beyond data fusion in a single
domain. We propose a four-layer framework, consisting of Domains, Links, Models
and Data layers, answering three key questions: "what to fuse", "why can be
fused", and "how to fuse". The Domains Layer selects relevant data from
different domains for a given problem. The Links Layer reveals the philosophy
of knowledge alignment beyond specific model structures. The Models Layer
provides two knowledge fusion paradigms based on the fundamental mechanisms for
processing data. The Data Layer turns data of different structures,
resolutions, scales and distributions into a consistent representation that can
be fed into an AI model. With this framework, we can design end-to-end
solutions that fuse cross-domain multimodal data effectively for solving
real-world problems.

</details>


### [24] [DUAL: Dynamic Uncertainty-Aware Learning](https://arxiv.org/abs/2506.03158)
*Jiahao Qin,Bei Peng,Feng Liu,Guangliang Cheng,Lu Zong*

Main category: cs.LG

TL;DR: 提出了一种名为DUAL的新框架，用于处理单模态和多模态场景中的特征不确定性问题。通过动态特征不确定性建模、自适应分布感知调制和不确定性感知跨模态关系学习三个关键创新点，该方法在多个领域中表现出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在各种学习场景中经常遇到特征不确定性的问题，这对其性能和可靠性产生重大影响。特别是在多模态场景中，模型需要整合来自不同来源的信息，并处理其固有的不确定性。为了解决这一挑战，提出了一个统一的框架来有效应对特征不确定性问题。

Method: DUAL引入了三个关键创新点：1. 动态特征不确定性建模，通过联合考虑特征特性和学习动力学，不断优化不确定性估计；2. 自适应分布感知调制，通过动态样本影响调整保持特征分布平衡；3. 不确定性感知跨模态关系学习，明确建模跨模态交互中的不确定性。

Result: 通过广泛的实验验证了DUAL的有效性，在计算机视觉任务中分别在CIFAR-10、CIFAR-100和Tiny-ImageNet上实现了7.1%、6.5%和2.3%的准确率提升；在多模态学习中，情感分析任务上分别在CMU-MOSEI和CMU-MOSI数据集上获得了4.1%和2.8%的准确率提升，MISR数据集上获得了1.4%的准确率提升。

Conclusion: DUAL是一个有效的框架，可以应对单模态和多模态场景中的特征不确定性问题，并在多个领域中展示了显著的性能改进。

Abstract: Deep learning models frequently encounter feature uncertainty in diverse
learning scenarios, significantly impacting their performance and reliability.
This challenge is particularly complex in multi-modal scenarios, where models
must integrate information from different sources with inherent uncertainties.
We propose Dynamic Uncertainty-Aware Learning (DUAL), a unified framework that
effectively handles feature uncertainty in both single-modal and multi-modal
scenarios. DUAL introduces three key innovations: Dynamic Feature Uncertainty
Modeling, which continuously refines uncertainty estimates through joint
consideration of feature characteristics and learning dynamics; Adaptive
Distribution-Aware Modulation, which maintains balanced feature distributions
through dynamic sample influence adjustment; and Uncertainty-aware Cross-Modal
Relationship Learning, which explicitly models uncertainties in cross-modal
interactions. Through extensive experiments, we demonstrate DUAL's
effectiveness across multiple domains: in computer vision tasks, it achieves
substantial improvements of 7.1% accuracy on CIFAR-10, 6.5% accuracy on
CIFAR-100, and 2.3% accuracy on Tiny-ImageNet; in multi-modal learning, it
demonstrates consistent gains of 4.1% accuracy on CMU-MOSEI and 2.8% accuracy
on CMU-MOSI for sentiment analysis, while achieving 1.4% accuracy improvements
on MISR. The code will be available on GitHub soon.

</details>


### [25] [Bayes Error Rate Estimation in Difficult Situations](https://arxiv.org/abs/2506.03159)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: The paper explores the accuracy of Bayes Error Rate (BER) estimators using Monte Carlo simulations on synthetic and real-world data, finding k-Nearest Neighbor (kNN) as the most accurate non-parametric estimator for binary classification, though it requires a large number of samples to meet the target confidence bounds.


<details>
  <summary>Details</summary>
Motivation: To identify which BER estimators are useful for real-world applications by evaluating their accuracy with limited samples on multivariate problems with unknown class distributions.

Method: Conduct an in-depth examination of BER estimator accuracy using Monte Carlo simulations with synthetic data to obtain confidence bounds for binary classification. Introduce new test scenarios and run 2500 Monte Carlo simulations per scenario over a wide range of BER values to examine usability on real-world applications. Compare kNN, GHP divergence, and KDE techniques.

Result: kNN is found to be significantly more accurate than other non-parametric estimators for binary classification. To achieve under 5 percent range for the 95 percent confidence bounds, at least 1000 samples per class are required; this number increases to 2500 samples per class when there are 4 features. Other estimators become more accurate than kNN as more features are added but still fail to meet the target range.

Conclusion: kNN is the most accurate non-parametric estimator for BER estimation in binary classification tasks with up to 4 features given sufficient sample size. However, as dimensionality increases, kNN's effectiveness diminishes.

Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable
generalizable classification accuracy of any machine learning model due to
inherent uncertainty within the data. BER estimators offer insight into the
difficulty of any classification problem and set expectations for optimal
classification performance. In order to be useful, the estimators must also be
accurate with a limited number of samples on multivariate problems with unknown
class distributions. To determine which estimators meet the minimum
requirements for "usefulness", an in-depth examination of their accuracy is
conducted using Monte Carlo simulations with synthetic data in order to obtain
their confidence bounds for binary classification. To examine the usability of
the estimators on real-world applications, new test scenarios are introduced
upon which 2500 Monte Carlo simulations per scenario are run over a wide range
of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized
Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques,
results show that kNN is overwhelmingly the more accurate non-parametric
estimator. In order to reach the target of an under 5 percent range for the 95
percent confidence bounds, the minimum number of required samples per class is
1000. As more features are added, more samples are needed, so that 2500 samples
per class are required at only 4 features. Other estimators do become more
accurate than kNN as more features are added, but continuously fail to meet the
target range.

</details>


### [26] [Applying MambaAttention, TabPFN, and TabTransformers to Classify SAE Automation Levels in Crashes](https://arxiv.org/abs/2506.03160)
*Shriyank Somvanshi,Anannya Ghosh Tusti,Mahmuda Sultana Mimi,Md Monzurul Islam,Sazzad Bin Bashar Polock,Anandi Dutta,Subasish Das*

Main category: cs.LG

TL;DR: The study evaluates three deep learning models for classifying SAE automation levels involved in crashes. MambaAttention performed the best, TabPFN was robust in zero-shot inference, and TabTransformer struggled with Partial Automation cases. Integrating such models can support policy development and AV safety evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for crash classification often overlook automation-specific factors and lack sophistication to distinguish between different SAE levels.

Method: Three advanced tabular deep learning models (MambaAttention, TabPFN, and TabTransformer) were evaluated on structured crash data from Texas (2024). The dataset included 4,649 cases across three SAE automation levels. Class balancing using SMOTEENN was applied to create a unified dataset of 7,300 records.

Result: MambaAttention demonstrated the highest overall performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5). TabPFN excelled in zero-shot inference. TabTransformer underperformed, especially in detecting Partial Automation crashes (F1-score: 55%).

Conclusion: Deep learning models tailored for tabular data can enhance the accuracy and efficiency of automation-level classification. Integrating these models into crash analysis frameworks can support policy development, AV safety evaluation, and regulatory decisions.

Abstract: The increasing presence of automated vehicles (AVs) presents new challenges
for crash classification and safety analysis. Accurately identifying the SAE
automation level involved in each crash is essential to understanding crash
dynamics and system accountability. However, existing approaches often overlook
automation-specific factors and lack model sophistication to capture
distinctions between different SAE levels. To address this gap, this study
evaluates the performance of three advanced tabular deep learning models
MambaAttention, TabPFN, and TabTransformer for classifying SAE automation
levels using structured crash data from Texas (2024), covering 4,649 cases
categorized as Assisted Driving (SAE Level 1), Partial Automation (SAE Level
2), and Advanced Automation (SAE Levels 3-5 combined). Following class
balancing using SMOTEENN, the models were trained and evaluated on a unified
dataset of 7,300 records. MambaAttention demonstrated the highest overall
performance (F1-scores: 88% for SAE 1, 97% for SAE 2, and 99% for SAE 3-5),
while TabPFN excelled in zero-shot inference with high robustness for rare
crash categories. In contrast, TabTransformer underperformed, particularly in
detecting Partial Automation crashes (F1-score: 55%), suggesting challenges in
modeling shared human-system control dynamics. These results highlight the
capability of deep learning models tailored for tabular data to enhance the
accuracy and efficiency of automation-level classification. Integrating such
models into crash analysis frameworks can support policy development, AV safety
evaluation, and regulatory decisions, especially in distinguishing high-risk
conditions for mid- and high-level automation technologies.

</details>


### [27] [Safety-Prioritized, Reinforcement Learning-Enabled Traffic Flow Optimization in a 3D City-Wide Simulation Environment](https://arxiv.org/abs/2506.03161)
*Mira Nuthakki*

Main category: cs.LG

TL;DR: The paper presents three tools for traffic management: a 3D city-wide simulation integrating macro and micro traffic dynamics, a collision model, and a reinforcement learning framework with custom rewards prioritizing safety. Using PPO model, improvements were seen in reducing collisions, distance traveled, enhancing fuel efficiency, and cutting carbon emissions significantly. This establishes the feasibility of city-wide 3D traffic simulations incorporating safety principles.


<details>
  <summary>Details</summary>
Motivation: Traffic congestion and collisions pose major economic, environmental, and social challenges that traditional traffic management approaches have not been able to adequately address.

Method: Development of a comprehensive 3D city-wide simulation environment integrating both macroscopic and microscopic traffic dynamics, a collision model using Unity game engine-based simulation, and a reinforcement learning framework with custom reward functions focused on safety over efficiency.

Result: The PPO model showed significant improvements compared to baseline results: reduced serious collisions, vehicle-vehicle collisions, and total distance traveled by over 3 times the baseline values. It also improved fuel efficiency by 39% and reduced carbon emissions by 88%.

Conclusion: The findings demonstrate the feasibility of applying city-wide 3D traffic simulations that incorporate zero-vision safety principles, realistic collision modeling, and appropriate reward modeling for optimizing traffic signal control to reduce collisions, improve traffic flow, and cut greenhouse gas emissions.

Abstract: Traffic congestion and collisions represent significant economic,
environmental, and social challenges worldwide. Traditional traffic management
approaches have shown limited success in addressing these complex, dynamic
problems. To address the current research gaps, three potential tools are
developed: a comprehensive 3D city-wide simulation environment that integrates
both macroscopic and microscopic traffic dynamics; a collision model; and a
reinforcement learning framework with custom reward functions prioritizing
safety over efficiency. Unity game engine-based simulation is used for direct
collision modeling. A custom reward enabled reinforcement learning method,
proximal policy optimization (PPO) model, yields substantial improvements over
baseline results, reducing the number of serious collisions, number of
vehicle-vehicle collisions, and total distance travelled by over 3 times the
baseline values. The model also improves fuel efficiency by 39% and reduces
carbon emissions by 88%. Results establish feasibility for city-wide 3D traffic
simulation applications incorporating the vision-zero safety principles of the
Department of Transportation, including physics-informed, adaptable, realistic
collision modeling, as well as appropriate reward modeling for real-world
traffic signal light control towards reducing collisions, optimizing traffic
flow and reducing greenhouse emissions.

</details>


### [28] [Causal Discovery in Dynamic Fading Wireless Networks](https://arxiv.org/abs/2506.03163)
*Oluwaseyi Giwa*

Main category: cs.LG

TL;DR: The paper proposes a sequential regression-based algorithm with the NOTEARS acyclicity constraint for dynamic causal inference in wireless networks, derives theoretical bounds on detection delay, and validates results through simulations.


<details>
  <summary>Details</summary>
Motivation: Dynamic causal discovery is crucial in wireless networks due to evolving interference, fading, and mobility that complicate static causal models.

Method: A sequential regression-based algorithm with the NOTEARS acyclicity constraint is proposed for efficient online updates in dynamic fading wireless environments.

Result: Theoretical lower and upper bounds on detection delay are derived, showing linear dependence on network size, quadratic growth with noise variance, and inverse-square dependence on structural change magnitude. Monte Carlo simulations confirm these findings.

Conclusion: The study offers theoretical insights and practical guidelines for robust online causal inference mechanisms in nonstationary wireless conditions.

Abstract: Dynamic causal discovery in wireless networks is essential due to evolving
interference, fading, and mobility, which complicate traditional static causal
models. This paper addresses causal inference challenges in dynamic fading
wireless environments by proposing a sequential regression-based algorithm with
a novel application of the NOTEARS acyclicity constraint, enabling efficient
online updates. We derive theoretical lower and upper bounds on the detection
delay required to identify structural changes, explicitly quantifying their
dependence on network size, noise variance, and fading severity. Monte Carlo
simulations validate these theoretical results, demonstrating linear increases
in detection delay with network size, quadratic growth with noise variance, and
inverse-square dependence on the magnitude of structural changes. Our findings
provide rigorous theoretical insights and practical guidelines for designing
robust online causal inference mechanisms to maintain network reliability under
nonstationary wireless conditions.

</details>


### [29] [Fingerprinting Deep Learning Models via Network Traffic Patterns in Federated Learning](https://arxiv.org/abs/2506.03207)
*Md Nahid Hasan Shuvo,Moinul Hossain*

Main category: cs.LG

TL;DR: Through analyzing network traffic in Federated Learning (FL), this study demonstrates that deep learning models can be fingerprinted with high accuracy using machine learning algorithms, revealing a potential privacy issue.


<details>
  <summary>Details</summary>
Motivation: Federated Learning preserves data privacy by not centralizing user data. However, there is a lack of research on indirect privacy breaches via network traffic analysis. This study aims to explore the feasibility of fingerprinting deep learning models within FL environments through their network-layer traffic information.

Method: The researchers set up a federated learning testbed and used various deep learning architectures such as CNN and RNN. They then applied machine learning algorithms like Support Vector Machines (SVM), Random Forest, and Gradient-Boosting to analyze and fingerprint patterns in the traffic data.

Result: Experiments showed high fingerprinting accuracy: 100% with Random Forest, and approximately 95.7% with SVM and Gradient Boosting classifiers. Specific deep learning architectures could be identified through network traffic.

Conclusion: This study reveals a significant security vulnerability in Federated Learning systems where adversaries could exploit knowledge of DL architectures for targeted attacks, suggesting a need for stronger network-level protections.

Abstract: Federated Learning (FL) is increasingly adopted as a decentralized machine
learning paradigm due to its capability to preserve data privacy by training
models without centralizing user data. However, FL is susceptible to indirect
privacy breaches via network traffic analysis-an area not explored in existing
research. The primary objective of this research is to study the feasibility of
fingerprinting deep learning models deployed within FL environments by
analyzing their network-layer traffic information. In this paper, we conduct an
experimental evaluation using various deep learning architectures (i.e., CNN,
RNN) within a federated learning testbed. We utilize machine learning
algorithms, including Support Vector Machines (SVM), Random Forest, and
Gradient-Boosting, to fingerprint unique patterns within the traffic data. Our
experiments show high fingerprinting accuracy, achieving 100% accuracy using
Random Forest and around 95.7% accuracy using SVM and Gradient Boosting
classifiers. This analysis suggests that we can identify specific architectures
running within the subsection of the network traffic. Hence, if an adversary
knows about the underlying DL architecture, they can exploit that information
and conduct targeted attacks. These findings suggest a notable security
vulnerability in FL systems and the necessity of strengthening it at the
network level.

</details>


### [30] [Test-Time Scaling of Diffusion Models via Noise Trajectory Search](https://arxiv.org/abs/2506.03164)
*Vignav Ramesh,Morteza Mardani*

Main category: cs.LG

TL;DR: 在扩散模型中，通过将去噪视为独立的情境Bandits序列并采用ε-greedy搜索算法，优化噪声轨迹可显著提高样本质量，实验结果表明该方法在类别条件/文本到图像生成方面达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 传统的增加去噪步骤的方法效果有限，而优化噪声轨迹（即注入的噪声向量序列）对于提高样本质量至关重要，但其高维搜索空间、复杂的噪声-结果交互和高昂的轨迹评估成本构成了挑战。

Method: 作者首先将扩散过程建模为具有终端奖励的马尔可夫决策过程(MDP)，发现树搜索方法如蒙特卡洛树搜索(MCTS)虽然有意义但不切实际。然后通过放松MDP假设，将去噪视为一系列独立的情境Bandits问题，并引入ε-greedy搜索算法，该算法在极端时间步进行全局探索，在中间步骤局部利用以实现去混叠。

Result: 实验显示，在EDM和Stable Diffusion上，该方法在类条件和文本到图像生成任务上达到了最先进的分数，超过基线最多164%，并且匹配或超过了MCTS的表现。这是首个针对任意（不可微分）奖励的测试时噪声轨迹优化的实际方法。

Conclusion: 提出了一种新的ε-greedy搜索算法用于优化扩散模型中的噪声轨迹，该方法在性能和效率之间取得了良好的平衡，并且在各种生成任务中表现优异，展示了其实用性和潜力。

Abstract: The iterative and stochastic nature of diffusion models enables test-time
scaling, whereby spending additional compute during denoising generates
higher-fidelity samples. Increasing the number of denoising steps is the
primary scaling axis, but this yields quickly diminishing returns. Instead
optimizing the noise trajectory--the sequence of injected noise vectors--is
promising, as the specific noise realizations critically affect sample quality;
but this is challenging due to a high-dimensional search space, complex
noise-outcome interactions, and costly trajectory evaluations. We address this
by first casting diffusion as a Markov Decision Process (MDP) with a terminal
reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to
be meaningful but impractical. To balance performance and efficiency, we then
resort to a relaxation of MDP, where we view denoising as a sequence of
independent contextual bandits. This allows us to introduce an
$\epsilon$-greedy search algorithm that globally explores at extreme timesteps
and locally exploits during the intermediate steps where de-mixing occurs.
Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for
class-conditioned/text-to-image generation, exceeding baselines by up to
$164\%$ and matching/exceeding MCTS performance. To our knowledge, this is the
first practical method for test-time noise trajectory optimization of arbitrary
(non-differentiable) rewards.

</details>


### [31] [Non-collective Calibrating Strategy for Time Series Forecasting](https://arxiv.org/abs/2506.03176)
*Bin Wang,Yongqi Han,Minbo Ma,Tianrui Li,Junbo Zhang,Feng Hong,Yanwei Yu*

Main category: cs.LG

TL;DR: This paper presents Socket+Plug (SoP), a model-agnostic calibrating strategy to enhance the performance of deep learning-based time series forecasting models without significant resource costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is that current advancements in deep learning for time series forecasting still face challenges due to the complex dynamics involved. The authors seek to improve existing models rather than developing new ones from scratch, aiming for substantial benefits with minimal resource costs.

Method: The method involves identifying and addressing a multi-target learning conflict during the calibrating process of deep forecasting models. They propose SoP, which uses an exclusive optimizer and early-stopping monitor for each predicted target within each Plug while keeping the fully trained Socket backbone frozen.

Result: Extensive experiments on various benchmarks and a meteorological dataset show the effectiveness of SoP, achieving up to a 22% improvement even when using a simple MLP as the Plug.

Conclusion: SoP is an effective calibrating strategy that can significantly enhance the performance of any trained deep forecasting models regardless of their specific architectures.

Abstract: Deep learning-based approaches have demonstrated significant advancements in
time series forecasting. Despite these ongoing developments, the complex
dynamics of time series make it challenging to establish the rule of thumb for
designing the golden model architecture. In this study, we argue that refining
existing advanced models through a universal calibrating strategy can deliver
substantial benefits with minimal resource costs, as opposed to elaborating and
training a new model from scratch. We first identify a multi-target learning
conflict in the calibrating process, which arises when optimizing variables
across time steps, leading to the underutilization of the model's learning
capabilities. To address this issue, we propose an innovative calibrating
strategy called Socket+Plug (SoP). This approach retains an exclusive optimizer
and early-stopping monitor for each predicted target within each Plug while
keeping the fully trained Socket backbone frozen. The model-agnostic nature of
SoP allows it to directly calibrate the performance of any trained deep
forecasting models, regardless of their specific architectures. Extensive
experiments on various time series benchmarks and a spatio-temporal
meteorological ERA5 dataset demonstrate the effectiveness of SoP, achieving up
to a 22% improvement even when employing a simple MLP as the Plug (highlighted
in Figure 1)

</details>


### [32] [Out-of-Vocabulary Sampling Boosts Speculative Decoding](https://arxiv.org/abs/2506.03206)
*Nadav Timor,Jonathan Mamou,Oren Pereg,Hongyang Zhang,David Harel*

Main category: cs.LG

TL;DR: 本研究提出了Redistributing Drafter Kernels (RDK)，一种能够有效恢复被剪枝目标令牌的首个词汇表外采样器，通过数学证明和实验验证其在极端剪枝条件下的高效性，为高效推测解码铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的语言模型使用越来越大的词汇表显著减慢了解码速度，而现有采样方法无法处理词汇表外的令牌，限制了解码效率提升的可能性。

Method: 引入Redistributing Drafter Kernels (RDK)技术，利用令牌亲和先验将解码质量重新分配到高重叠区域，并提供RDK的一阶近似，使其时间复杂度从O(N^2)降至O(N)。

Result: 实验表明，在极端剪枝（移除超过75%的词汇）后，线性时间RDK仍能显著提高接受率，而现有采样器则失败。

Conclusion: RDK使得极度剪枝的解码器变得可行，为高效推测解码提供了新途径。

Abstract: Speculative decoding relies on fast and accurate drafters. Recent
state-of-the-art language models employ larger and larger vocabularies, which
significantly slows down drafters. One promising approach to boost the
efficiency of speculative decoding is to use drafters with smaller
vocabularies. However, existing sampling methods cannot draw out-of-vocabulary
tokens, creating a tradeoff between drafters' vocabulary size and acceptance
rates. This paper introduces Redistributing Drafter Kernels (RDK), the first
out-of-vocabulary sampler that effectively recovers acceptance rates by
virtually restoring pruned target tokens. RDK leverages token-affinity priors
to reallocate drafter mass towards high-overlap regions. We prove
mathematically that RDK can achieve higher acceptance rates than vanilla and
state-of-the-art samplers. We provide an efficient first-order approximation of
RDK and prove that it reduces redistribution times from $O(N^2)$ to $O(N)$,
enabling lightweight implementations for large vocabularies. Our experiments
demonstrate that this linear-time RDK significantly boosts acceptance rates
even after extreme pruning (removing more than 75% of the drafter's
vocabulary), where existing samplers fail. RDK opens the door to extremely
pruned drafters, which were previously impractical.

</details>


### [33] [Evaluating Apple Intelligence's Writing Tools for Privacy Against Large Language Model-Based Inference Attacks: Insights from Early Datasets](https://arxiv.org/abs/2506.03870)
*Mohd. Farhan Israk Soumik,Syed Mhamudul Hasan,Abdur R. Shahid*

Main category: cs.LG

TL;DR: This paper explores the potential of Apple Intelligence's writing tools to mitigate risks from emotion inference attacks by LLMs through text modifications, presenting the first empirical analysis in a privacy context.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the significant threat to user privacy posed by emotion inference attacks using LLMs and explore methods to mitigate these risks.

Method: Developing early novel datasets and empirically assessing how different text modifications (rewriting and tone adjustment) influence LLM-based detection.

Result: The study suggests that Apple Intelligence's writing tools have strong potential as privacy-preserving mechanisms.

Conclusion: This research lays the groundwork for future adaptive rewriting systems capable of enhancing user privacy by dynamically neutralizing sensitive emotional content.

Abstract: The misuse of Large Language Models (LLMs) to infer emotions from text for
malicious purposes, known as emotion inference attacks, poses a significant
threat to user privacy. In this paper, we investigate the potential of Apple
Intelligence's writing tools, integrated across iPhone, iPad, and MacBook, to
mitigate these risks through text modifications such as rewriting and tone
adjustment. By developing early novel datasets specifically for this purpose,
we empirically assess how different text modifications influence LLM-based
detection. This capability suggests strong potential for Apple Intelligence's
writing tools as privacy-preserving mechanisms. Our findings lay the groundwork
for future adaptive rewriting systems capable of dynamically neutralizing
sensitive emotional content to enhance user privacy. To the best of our
knowledge, this research provides the first empirical analysis of Apple
Intelligence's text-modification tools within a privacy-preservation context
with the broader goal of developing on-device, user-centric privacy-preserving
mechanisms to protect against LLMs-based advanced inference attacks on deployed
systems.

</details>


### [34] [FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution](https://arxiv.org/abs/2506.03210)
*Qiusheng Huang,Yuan Niu,Xiaohui Zhong,Anboyu Guo,Lei Chen,Dianjun Zhang,Xuefeng Zhang,Hao Li*

Main category: cs.LG

TL;DR: FuXi-Ocean is a data-driven model that can predict ocean conditions every six hours with high spatial resolution, overcoming limitations of traditional models.


<details>
  <summary>Details</summary>
Motivation: Traditional ocean forecasting models are computationally intensive and may lose accuracy at fine scales, while existing data-driven methods usually work at daily resolution and have trouble with sub-daily predictions due to error accumulation.

Method: The FuXi-Ocean model uses a context-aware feature extraction module and a predictive network with stacked attention blocks. The key component is the Mixture-of-Time (MoT) module, which mitigates cumulative errors by adaptively integrating predictions from multiple temporal contexts based on learned reliability.

Result: FuXi-Ocean excels in predicting important ocean variables like temperature, salinity, and currents at various depths, achieving six-hourly forecasts at 1/12° spatial resolution down to 1500 meters depth.

Conclusion: FuXi-Ocean represents a significant advancement in data-driven ocean forecasting, providing accurate sub-daily predictions at high spatial resolution.

Abstract: Accurate, high-resolution ocean forecasting is crucial for maritime
operations and environmental monitoring. While traditional numerical models are
capable of producing sub-daily, eddy-resolving forecasts, they are
computationally intensive and face challenges in maintaining accuracy at fine
spatial and temporal scales. In contrast, recent data-driven approaches offer
improved computational efficiency and emerging potential, yet typically operate
at daily resolution and struggle with sub-daily predictions due to error
accumulation over time. We introduce FuXi-Ocean, the first data-driven global
ocean forecasting model achieving six-hourly predictions at eddy-resolving
1/12{\deg} spatial resolution, reaching depths of up to 1500 meters. The model
architecture integrates a context-aware feature extraction module with a
predictive network employing stacked attention blocks. The core innovation is
the Mixture-of-Time (MoT) module, which adaptively integrates predictions from
multiple temporal contexts by learning variable-specific reliability ,
mitigating cumulative errors in sequential forecasting. Through comprehensive
experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting
key variables, including temperature, salinity, and currents, across multiple
depths.

</details>


### [35] [Multiple-Frequencies Population-Based Training](https://arxiv.org/abs/2506.03225)
*Waël Doulazmi,Auguste Lehuger,Marin Toromanoff,Valentin Charraut,Thibault Buhet,Fabien Moutarde*

Main category: cs.LG

TL;DR: The paper proposes MF-PBT, an improved HPO algorithm that optimizes hyperparameter tuning by employing sub-populations evolving at different frequencies and an asymmetric migration process to balance short and long-term optimization.


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning's high sensitivity to hyperparameters leads to instability and inefficiency. Existing Hyperparameter Optimization algorithms, particularly PBT, suffer from greediness issues where short-term improvements may lead to local optima and worse performance over longer timescales.

Method: MF-PBT is proposed which uses sub-populations evolving at distinct frequencies and introduces a migration process with an asymmetric design to transfer information between sub-populations and balance short and long-term optimization.

Result: Experiments on the Brax suite show that MF-PBT improves sample efficiency and long-term performance without even actually tuning hyperparameters.

Conclusion: MF-PBT addresses the greediness issue in PBT by using multiple evolution frequencies and an asymmetric migration process, leading to better long-term performance.

Abstract: Reinforcement Learning's high sensitivity to hyperparameters is a source of
instability and inefficiency, creating significant challenges for
practitioners. Hyperparameter Optimization (HPO) algorithms have been developed
to address this issue, among them Population-Based Training (PBT) stands out
for its ability to generate hyperparameters schedules instead of fixed
configurations. PBT trains a population of agents, each with its own
hyperparameters, frequently ranking them and replacing the worst performers
with mutations of the best agents. These intermediate selection steps can cause
PBT to focus on short-term improvements, leading it to get stuck in local
optima and eventually fall behind vanilla Random Search over longer timescales.
This paper studies how this greediness issue is connected to the choice of
evolution frequency, the rate at which the selection is done. We propose
Multiple-Frequencies Population-Based Training (MF-PBT), a novel HPO algorithm
that addresses greediness by employing sub-populations, each evolving at
distinct frequencies. MF-PBT introduces a migration process to transfer
information between sub-populations, with an asymmetric design to balance short
and long-term optimization. Extensive experiments on the Brax suite demonstrate
that MF-PBT improves sample efficiency and long-term performance, even without
actually tuning hyperparameters.

</details>


### [36] [Bridging Neural ODE and ResNet: A Formal Error Bound for Safety Verification](https://arxiv.org/abs/2506.03227)
*Abdelrahman Sayed Sayed,Pierre-Jean Meyer,Mohamed Ghazel*

Main category: cs.LG

TL;DR: A neural ODE and ResNet are closely related models. This paper bounds the approximation error between them, enabling one to act as a verification proxy for the other without double verification.


<details>
  <summary>Details</summary>
Motivation: To establish a formal relationship between neural ODEs and ResNets by bounding their approximation error, allowing for interchangeable safety property verification.

Method: Bounding the approximation error between neural ODEs and ResNets so that one model can serve as a verification proxy for the other.

Result: The error bound enables using one model as a proxy for verifying safety properties on the other model. Demonstrated through a numerical example of a fixed-point attractor system modeled as a neural ODE.

Conclusion: Formalized the relationship between neural ODEs and ResNets via error bounding, facilitating efficient safety property verification across these models.

Abstract: A neural ordinary differential equation (neural ODE) is a machine learning
model that is commonly described as a continuous depth generalization of a
residual network (ResNet) with a single residual block, or conversely, the
ResNet can be seen as the Euler discretization of the neural ODE. These two
models are therefore strongly related in a way that the behaviors of either
model are considered to be an approximation of the behaviors of the other. In
this work, we establish a more formal relationship between these two models by
bounding the approximation error between two such related models. The obtained
error bound then allows us to use one of the models as a verification proxy for
the other, without running the verification tools twice: if the reachable
output set expanded by the error bound satisfies a safety property on one of
the models, this safety property is then guaranteed to be also satisfied on the
other model. This feature is fully reversible, and the initial safety
verification can be run indifferently on either of the two models. This novel
approach is illustrated on a numerical example of a fixed-point attractor
system modeled as a neural ODE.

</details>


### [37] [DiaBlo: Diagonal Blocks Are Sufficient For Finetuning](https://arxiv.org/abs/2506.03230)
*Selcuk Gurses,Aozhong Zhang,Yanxia Deng,Xun Dong,Xin Li,Naigang Wang,Penghang Yin,Zi Yang*

Main category: cs.LG

TL;DR: DiaBlo是一种新的参数高效微调（PEFT）方法，通过仅更新选定模型权重矩阵的对角块来减少计算和内存成本，同时保持与全模型微调相当的性能。相比LoRA及其变体，DiaBlo无需低秩矩阵乘法，简化了初始化和优化过程，提高了收敛的稳定性和鲁棒性。实验表明，DiaBlo在常识推理、算术推理、代码生成和安全性对齐等任务上表现出色，且具有高内存效率和快速微调速度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）需要针对特定领域的下游任务进行微调，但全模型微调存在巨大的计算和内存成本问题。现有的参数高效微调（PEFT）方法虽然能缓解这些问题，但在性能上仍与全模型微调存在差距。因此，研究者们希望开发一种新的PEFT方法，能够在降低计算和内存成本的同时，尽可能缩小与全模型微调的性能差距。

Method: DiaBlo方法通过仅更新选定模型权重矩阵的对角块来进行微调。这种方法不同于低秩适应（LoRA）及其变体，不需要进行低秩矩阵乘法运算，从而避免了对辅助初始化方案或定制优化策略的依赖。这种设计使得DiaBlo在保持与LoRA相似的内存效率和训练速度的同时，能够实现更稳定和鲁棒的收敛。

Result: DiaBlo在多个任务上的实验结果表明，它能够在保持高内存效率和快速微调速度的同时，展现出强大且一致的性能。这些任务包括常识推理、算术推理、代码生成和安全性对齐等。

Conclusion: DiaBlo作为一种简单而有效的PEFT方法，通过仅更新模型权重矩阵的对角块，成功实现了在降低计算和内存成本的同时，保持与全模型微调相当的性能。其不依赖低秩矩阵乘法的特点，使得收敛更加稳定和鲁棒，适用于多种任务场景。

Abstract: Finetuning is a critical step for adapting large language models (LLMs) to
domain-specific downstream tasks. To mitigate the substantial computational and
memory costs of full-model fine-tuning, Parameter-Efficient Finetuning (PEFT)
methods have been proposed to update only a small subset of model parameters.
However, performance gaps between PEFT approaches and full-model fine-tuning
still exist. In this work, we present DiaBlo, a simple yet effective PEFT
approach that updates only the diagonal blocks of selected model weight
matrices. Unlike Low Rank Adaptation (LoRA) and its variants, DiaBlo eliminates
the need for low rank matrix products, thereby avoiding the reliance on
auxiliary initialization schemes or customized optimization strategies to
improve convergence. This design leads to stable and robust convergence while
maintaining comparable memory efficiency and training speed to LoRA. We conduct
extensive experiments across a range of tasks, including commonsense reasoning,
arithmetic reasoning, code generation, and safety alignment, to evaluate the
effectiveness and efficiency of DiaBlo. Across these benchmarks, DiaBlo
demonstrates strong and consistent performance while maintaining high memory
efficiency and fast finetuning speed. Codes are available at
https://github.com/ziyangjoy/DiaBlo.

</details>


### [38] [BadReward: Clean-Label Poisoning of Reward Models in Text-to-Image RLHF](https://arxiv.org/abs/2506.03234)
*Kaiwen Duan,Hongwei Yao,Yufei Chen,Ziyun Li,Tong Qiao,Zhan Qin,Cong Wang*

Main category: cs.LG

TL;DR: The paper introduces BadReward, a stealthy poisoning attack on the reward model in multi-modal RLHF for T2I models. By inducing feature collisions with natural-appearing poisoned data, it can hijack T2I models to produce improper outputs for targeted concepts, such as biased or violent imagery. Experiments show its effectiveness and emphasize the need for robust defenses.


<details>
  <summary>Details</summary>
Motivation: To explore the vulnerabilities of RLHF in multi-modal systems and demonstrate how adversaries can hijack T2I models by poisoning preference data, which opens new pathways for attacks.

Method: Propose BadReward, a clean-label poisoning attack that induces feature collisions between visually contradicted preference data instances, corrupting the reward model without being detected.

Result: BadReward consistently guides popular T2I models to generate improper outputs (e.g., biased or violent imagery) for targeted concepts, showing its effectiveness and practical threat.

Conclusion: The study reveals the amplified threat landscape for RLHF in multi-modal systems and calls for the development of robust defenses against such poisoning attacks.

Abstract: Reinforcement Learning from Human Feedback (RLHF) is crucial for aligning
text-to-image (T2I) models with human preferences. However, RLHF's feedback
mechanism also opens new pathways for adversaries. This paper demonstrates the
feasibility of hijacking T2I models by poisoning a small fraction of preference
data with natural-appearing examples. Specifically, we propose BadReward, a
stealthy clean-label poisoning attack targeting the reward model in multi-modal
RLHF. BadReward operates by inducing feature collisions between visually
contradicted preference data instances, thereby corrupting the reward model and
indirectly compromising the T2I model's integrity. Unlike existing alignment
poisoning techniques focused on single (text) modality, BadReward is
independent of the preference annotation process, enhancing its stealth and
practical threat. Extensive experiments on popular T2I models show that
BadReward can consistently guide the generation towards improper outputs, such
as biased or violent imagery, for targeted concepts. Our findings underscore
the amplified threat landscape for RLHF in multi-modal systems, highlighting
the urgent need for robust defenses. Disclaimer. This paper contains uncensored
toxic content that might be offensive or disturbing to the readers.

</details>


### [39] [On the Necessity of Multi-Domain Explanation: An Uncertainty Principle Approach for Deep Time Series Models](https://arxiv.org/abs/2506.03267)
*Shahbaz Rezaei,Avishai Halev,Xin Liu*

Main category: cs.LG

TL;DR: 在时间序列解释性人工智能(XAI)领域，本文提出多域解释的必要性。通过引入不确定性原理(UP)，评估时间与频率域解释是否强调不同特征，从而证明同时呈现两域解释的重要性。实验验证表明，现有仅关注时间域解释的方法存在局限，需采用多域解释新范式。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列模型解释方法主要集中在时间域或单一最稀疏域，未充分考虑时间与频率域可能强调的不同特征。

Method: 1. 引入量子力学中的不确定性原理至XAI领域，建立信号在时间与频率域同时局部化的下限。2. 评估时间与频率域归因是否违反该下限，以判断两域解释是否匹配。3. 在多种深度学习模型、XAI方法及分类预测数据集上验证方法有效性。

Result: 发现频繁出现UP违规现象，表明时间与频率域解释常强调不同特征，现有单域解释方法存在显著局限。

Conclusion: 提出多域解释作为新范式，建议同时展示时间与频率域解释以实现更全面的模型理解。

Abstract: A prevailing approach to explain time series models is to generate
attribution in time domain. A recent development in time series XAI is the
concept of explanation spaces, where any model trained in the time domain can
be interpreted with any existing XAI method in alternative domains, such as
frequency. The prevailing approach is to present XAI attributions either in the
time domain or in the domain where the attribution is most sparse. In this
paper, we demonstrate that in certain cases, XAI methods can generate
attributions that highlight fundamentally different features in the time and
frequency domains that are not direct counterparts of one another. This
suggests that both domains' attributions should be presented to achieve a more
comprehensive interpretation. Thus it shows the necessity of multi-domain
explanation. To quantify when such cases arise, we introduce the uncertainty
principle (UP), originally developed in quantum mechanics and later studied in
harmonic analysis and signal processing, to the XAI literature. This principle
establishes a lower bound on how much a signal can be simultaneously localized
in both the time and frequency domains. By leveraging this concept, we assess
whether attributions in the time and frequency domains violate this bound,
indicating that they emphasize distinct features. In other words, UP provides a
sufficient condition that the time and frequency domain explanations do not
match and, hence, should be both presented to the end user. We validate the
effectiveness of this approach across various deep learning models, XAI
methods, and a wide range of classification and forecasting datasets. The
frequent occurrence of UP violations across various datasets and XAI methods
highlights the limitations of existing approaches that focus solely on
time-domain explanations. This underscores the need for multi-domain
explanations as a new paradigm.

</details>


### [40] [Multi-Exit Kolmogorov-Arnold Networks: enhancing accuracy and parsimony](https://arxiv.org/abs/2506.03302)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: Kolmogorov-Arnold Networks (KANs) are enhanced with multi-exit architecture that allows accurate predictions at multiple depths simultaneously, leading to better performance and simpler models without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of determining the appropriate depth of KANs for different tasks and to improve their optimization.

Method: Introducing multi-exit KANs where each layer has its own prediction branch, providing deep supervision and helping find the right model complexity. A 'learning to exit' algorithm is developed to balance contributions from different exits during training.

Result: Multi-exit KANs outperform standard single-exit versions on various tasks including synthetic functions, dynamical systems, and real-world datasets. Simpler models from earlier exits often provide the best predictions.

Conclusion: Multi-exit KANs offer a practical solution for achieving high performance and interpretability in scientific modeling.

Abstract: Kolmogorov-Arnold Networks (KANs) uniquely combine high accuracy with
interpretability, making them valuable for scientific modeling. However, it is
unclear a priori how deep a network needs to be for any given task, and deeper
KANs can be difficult to optimize. Here we introduce multi-exit KANs, where
each layer includes its own prediction branch, enabling the network to make
accurate predictions at multiple depths simultaneously. This architecture
provides deep supervision that improves training while discovering the right
level of model complexity for each task. Multi-exit KANs consistently
outperform standard, single-exit versions on synthetic functions, dynamical
systems, and real-world datasets. Remarkably, the best predictions often come
from earlier, simpler exits, revealing that these networks naturally identify
smaller, more parsimonious and interpretable models without sacrificing
accuracy. To automate this discovery, we develop a differentiable "learning to
exit" algorithm that balances contributions from exits during training. Our
approach offers scientists a practical way to achieve both high performance and
interpretability, addressing a fundamental challenge in machine learning for
scientific discovery.

</details>


### [41] [Budgeted Online Active Learning with Expert Advice and Episodic Priors](https://arxiv.org/abs/2506.03307)
*Kristen Goebel,William Solow,Paola Pesantez-Cabrera,Markus Keller,Alan Fern*

Main category: cs.LG

TL;DR: This paper proposes a new method for budgeted online active learning from finite-horizon data streams with limited labeling budgets, which integrates prior expert predictors and episodic behavioral knowledge, demonstrating superior performance in agricultural prediction problems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of effective learning from data streams with extremely limited labeling budgets, particularly in agricultural applications where labels are costly to obtain.

Method: The method integrates two key sources of prior information: a collection of preexisting expert predictors and episodic behavioral knowledge of the experts based on unlabeled data streams, while considering query budgets, finite horizons, and episodic knowledge simultaneously.

Result: The proposed method significantly outperforms baseline expert predictions, uniform query selection, and existing approaches that consider budgets and limited horizons but neglect episodic knowledge, even under highly constrained labeling budgets.

Conclusion: This novel approach enables effective learning in applications with severely limited labeling capacity, showing great potential in agricultural prediction problems.

Abstract: This paper introduces a novel approach to budgeted online active learning
from finite-horizon data streams with extremely limited labeling budgets. In
agricultural applications, such streams might include daily weather data over a
growing season, and labels require costly measurements of weather-dependent
plant characteristics. Our method integrates two key sources of prior
information: a collection of preexisting expert predictors and episodic
behavioral knowledge of the experts based on unlabeled data streams. Unlike
previous research on online active learning with experts, our work
simultaneously considers query budgets, finite horizons, and episodic
knowledge, enabling effective learning in applications with severely limited
labeling capacity. We demonstrate the utility of our approach through
experiments on various prediction problems derived from both a realistic
agricultural crop simulator and real-world data from multiple grape cultivars.
The results show that our method significantly outperforms baseline expert
predictions, uniform query selection, and existing approaches that consider
budgets and limited horizons but neglect episodic knowledge, even under highly
constrained labeling budgets.

</details>


### [42] [The Future of Continual Learning in the Era of Foundation Models: Three Key Directions](https://arxiv.org/abs/2506.03320)
*Jack Bell,Luigi Quarantiello,Eric Nuertey Coleman,Lanpei Li,Malio Li,Mauro Madeddu,Elia Piccoli,Vincenzo Lomonaco*

Main category: cs.LG

TL;DR: 尽管大型语言模型（LLMs）和基础模型的出现，持续学习仍然至关重要，主要因为：持续预训练确保模型更新、持续微调实现个性化、持续组合性提供可扩展和模块化的智能方法。未来AI将由不断演进和交互的模型生态系统定义，使持续学习比以往任何时候都更加重要。


<details>
  <summary>Details</summary>
Motivation: 历史上，不同的人工智能范式对持续学习有不同的重视程度。随着深度学习和大模型的发展，提出了一个问题：当集中式的大型模型能够利用互联网规模的知识解决多样任务时，是否还需要持续学习？

Method: 文章通过分析持续预训练、持续微调和持续组合性三个方面，阐述了在大模型时代持续学习的重要性。

Result: 持续学习对于确保模型的时效性、适应特定领域任务、避免完全重新训练以及提供一种可扩展和模块化的智能方法是必不可少的。

Conclusion: 未来的AI将不再依赖单一静态模型，而是由不断演进和交互的模型生态系统定义，这使得持续学习变得比以往任何时候都更加相关。

Abstract: Continual learning--the ability to acquire, retain, and refine knowledge over
time--has always been fundamental to intelligence, both human and artificial.
Historically, different AI paradigms have acknowledged this need, albeit with
varying priorities: early expert and production systems focused on incremental
knowledge consolidation, while reinforcement learning emphasised dynamic
adaptation. With the rise of deep learning, deep continual learning has
primarily focused on learning robust and reusable representations over time to
solve sequences of increasingly complex tasks. However, the emergence of Large
Language Models (LLMs) and foundation models has raised the question: Do we
still need continual learning when centralised, monolithic models can tackle
diverse tasks with access to internet-scale knowledge? We argue that continual
learning remains essential for three key reasons: (i) continual pre-training is
still necessary to ensure foundation models remain up to date, mitigating
knowledge staleness and distribution shifts while integrating new information;
(ii) continual fine-tuning enables models to specialise and personalise,
adapting to domain-specific tasks, user preferences, and real-world constraints
without full retraining, avoiding the need for computationally expensive long
context-windows; (iii) continual compositionality offers a scalable and modular
approach to intelligence, enabling the orchestration of foundation models and
agents to be dynamically composed, recombined, and adapted. While continual
pre-training and fine-tuning are explored as niche research directions, we
argue it is continual compositionality that will mark the rebirth of continual
learning. The future of AI will not be defined by a single static model but by
an ecosystem of continually evolving and interacting models, making continual
learning more relevant than ever.

</details>


### [43] [Optimization of Epsilon-Greedy Exploration](https://arxiv.org/abs/2506.03324)
*Ethan Che,Hakan Ceylan,James McInerney,Nathan Kallus*

Main category: cs.LG

TL;DR: 在推荐系统中，通过直接最小化贝叶斯后悔值来确定探索计划的新框架，适应实际约束并优于启发式方法。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统中的探索策略主要依赖简单的均匀探索政策（如epsilon-greedy），但这些方法难以在实际约束下找到最优的探索率。

Method: 提出了一种基于最小化贝叶斯后悔值并通过随机梯度下降（SGD）调整探索率的框架，利用模型预测控制（MPC）实现动态调整。

Result: 实验表明，该方法能够根据具体问题自动校准探索率，在不同批次大小的情况下始终匹配或超越最佳启发式方法。

Conclusion: 所提出的框架为推荐系统提供了一种更有效的探索策略优化方法，能适应多种实际约束条件。

Abstract: Modern recommendation systems rely on exploration to learn user preferences
for new items, typically implementing uniform exploration policies (e.g.,
epsilon-greedy) due to their simplicity and compatibility with machine learning
(ML) personalization models. Within these systems, a crucial consideration is
the rate of exploration - what fraction of user traffic should receive random
item recommendations and how this should evolve over time. While various
heuristics exist for navigating the resulting exploration-exploitation
tradeoff, selecting optimal exploration rates is complicated by practical
constraints including batched updates, time-varying user traffic, short time
horizons, and minimum exploration requirements. In this work, we propose a
principled framework for determining the exploration schedule based on directly
minimizing Bayesian regret through stochastic gradient descent (SGD), allowing
for dynamic exploration rate adjustment via Model-Predictive Control (MPC).
Through extensive experiments with recommendation datasets, we demonstrate that
variations in the batch size across periods significantly influence the optimal
exploration strategy. Our optimization methods automatically calibrate
exploration to the specific problem setting, consistently matching or
outperforming the best heuristic for each setting.

</details>


### [44] [Hierarchical Relational Learning for Few-Shot Knowledge Graph Completion](https://arxiv.org/abs/2209.01205)
*Han Wu,Jie Yin,Bala Rajaratnam,Jianyuan Guo*

Main category: cs.LG

TL;DR: The paper proposes HiRe, a hierarchical relational learning method for few-shot KG completion that captures entity-level, triplet-level and context-level relational information.


<details>
  <summary>Details</summary>
Motivation: Knowledge graphs have inference abilities but suffer from incompleteness and long-tail distribution of relations. Few-shot KG completion is aimed to make predictions for triplets involving novel relations with only a few training triplets as reference.

Method: HiRe jointly captures three levels of relational information (entity-level, triplet-level and context-level) to effectively learn and refine meta representations of few-shot relations.

Result: Extensive experiments on benchmark datasets validate the superiority of HiRe over state-of-the-art methods.

Conclusion: HiRe can generalize well to new unseen relations and outperforms existing methods in few-shot KG completion.

Abstract: Knowledge graphs (KGs) are powerful in terms of their inference abilities,
but are also notorious for their incompleteness and long-tail distribution of
relations. To address these challenges and expand the coverage of KGs, few-shot
KG completion aims to make predictions for triplets involving novel relations
when only a few training triplets are provided as reference. Previous methods
have focused on designing local neighbor aggregators to learn entity-level
information and/or imposing a potentially invalid sequential dependency
assumption at the triplet level to learn meta relation information. However,
pairwise triplet-level interactions and context-level relational information
have been largely overlooked for learning meta representations of few-shot
relations. In this paper, we propose a hierarchical relational learning method
(HiRe) for few-shot KG completion. By jointly capturing three levels of
relational information (entity-level, triplet-level and context-level), HiRe
can effectively learn and refine meta representations of few-shot relations,
and thus generalize well to new unseen relations. Extensive experiments on
benchmark datasets validate the superiority of HiRe over state-of-the-art
methods. The code can be found in https://github.com/alexhw15/HiRe.git.

</details>


### [45] [A Differential Perspective on Distributional Reinforcement Learning](https://arxiv.org/abs/2506.03333)
*Juan Sebastian Rojas,Chi-Guhn Lee*

Main category: cs.LG

TL;DR: 拓展分布强化学习到平均奖励设置中，提出新算法以优化每步奖励分布，并证明其收敛性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 当前的分布强化学习方法主要关注折扣设置，而未涉及平均奖励设置，因此需要一种能优化每步奖励的新方法。

Method: 使用基于分位数的方法开发新算法，可学习和优化长期每步奖励分布及差分回报分布，同时提供收敛的表格算法和扩展性更好的算法家族。

Result: 实验表明这些算法与非分布等价物相比具有竞争力，且能捕捉长期奖励和回报分布的丰富信息。

Conclusion: 首次将分布强化学习应用于平均奖励设置，所提出的算法在预测和控制方面均表现出色，能够捕获丰富的分布信息。

Abstract: To date, distributional reinforcement learning (distributional RL) methods
have exclusively focused on the discounted setting, where an agent aims to
optimize a potentially-discounted sum of rewards over time. In this work, we
extend distributional RL to the average-reward setting, where an agent aims to
optimize the reward received per time-step. In particular, we utilize a
quantile-based approach to develop the first set of algorithms that can
successfully learn and/or optimize the long-run per-step reward distribution,
as well as the differential return distribution of an average-reward MDP. We
derive proven-convergent tabular algorithms for both prediction and control, as
well as a broader family of algorithms that have appealing scaling properties.
Empirically, we find that these algorithms consistently yield competitive
performance when compared to their non-distributional equivalents, while also
capturing rich information about the long-run reward and return distributions.

</details>


### [46] [Mitigating Non-IID Drift in Zeroth-Order Federated LLM Fine-Tuning with Transferable Sparsity](https://arxiv.org/abs/2506.03337)
*Yide Ran,Wentao Guo,Jingwei Sun,Yanzhou Pan,Xiaodong Yu,Hao Wang,Jianwen Xie,Yiran Chen,Denghui Zhang,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: Meerkat是一种用于联邦LLM微调的稀疏零阶优化(ZO)方法，通过限制微调到可转移、静态、极其稀疏的参数子集，实现显著的通信效率，并有效缓解非独立同分布(Non-IID)数据挑战。实验结果表明，Meerkat在相同的通信频率下优于现有的稀疏基线，并且Meerkat-vp通过分析GradIP轨迹来识别极端Non-IID客户端并应用早期停止以增强聚合模型质量。


<details>
  <summary>Details</summary>
Motivation: 联邦学习允许在分散的Non-IID客户端上协同微调大型语言模型，但这些模型庞大的参数规模导致了显著的内存和通信挑战。为了解决这些问题，需要一种更高效的优化方法。

Method: Meerkat采用稀疏零阶优化方法，仅对可转移、静态、极其稀疏的参数子集进行微调，从而提高通信效率。此外，通过追踪本地更新并形成虚拟路径，揭示了GradIP现象，该现象可用于识别具有极端数据异质性的客户端。基于此，提出了Meerkat-vp，利用GradIP轨迹识别极端Non-IID客户端并应用早期停止策略。

Result: 实验结果表明，Meerkat在相同的通信频率下优于现有稀疏基线，并且能够有效缓解Non-IID数据带来的挑战。Meerkat-vp通过分析GradIP轨迹识别极端Non-IID客户端并应用早期停止策略，进一步提高了聚合模型的质量。

Conclusion: Meerkat和Meerkat-vp显著提高了零阶优化联邦LLM微调的效率和效果，解决了大规模参数带来的内存和通信挑战，并有效处理了Non-IID数据问题。

Abstract: Federated Learning enables collaborative fine-tuning of Large Language Models
(LLMs) across decentralized Non-Independent and Identically Distributed
(Non-IID) clients, but such models' massive parameter sizes lead to significant
memory and communication challenges. This work introduces Meerkat, a sparse
zeroth-order optimization (ZO) method designed for federated LLM fine-tuning.
By limiting fine-tuning to a transferable, static, extremely sparse subset of
parameters, Meerkat achieves remarkable communication efficiency, enabling
cost-effective high-frequency synchronization. With theoretical analysis and
experiments, we show that this high-frequency communication effectively
mitigates Non-IID data challenges and leads to superior performance compared to
full-parameter ZO. Furthermore, experiment results show that Meerkat
outperforms existing sparsity baselines with better performance at the same
communication frequency. To further handle Non-IID drift, Meerkat leverages
traceable local updates and forms a virtual path for each client. This virtual
path mechanism reveals the GradIP phenomenon: the inner products between LLM
pre-training gradients maintained by server and client gradients estimated via
ZO converges for extreme Non-IID clients but oscillates for IID ones. This
distinct behavior provides a signal for identifying clients with extreme data
heterogeneity. Using this signal, Meerkat-vp is proposed to analyze GradIP
trajectories to identify extreme Non-IID clients and applies early stopping to
enhance aggregated model quality. Experiments confirm that Meerkat and
Meerkat-vp significantly improve the efficiency and effectiveness of ZO
federated LLM fine-tuning.

</details>


### [47] [Robustness in Both Domains: CLIP Needs a Robust Text Encoder](https://arxiv.org/abs/2506.03355)
*Elias Abad Rocamora,Christian Schlarmann,Naman Deep Singh,Yongtao Wu,Matthias Hein,Volkan Cevher*

Main category: cs.LG

TL;DR: Adversarial input attacks can cause a significant shift of CLIP embeddings, affecting the downstream robustness of models incorporating CLIP. This paper proposes LEAF, an efficient adversarial finetuning method for the text domain to improve the robustness of CLIP's text encoders.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the gap in the literature regarding the robustness of CLIP's text encoders against adversarial input attacks, which can affect the downstream robustness of models incorporating CLIP.

Method: The proposed method is called LEAF: an efficient adversarial finetuning method for the text domain, with the ability to scale to large CLIP models.

Result: LEAF significantly improves the zero-shot adversarial accuracy in the text domain while maintaining the vision performance provided by robust image encoders. It also improves the generation quality under adversarial noise when combined with text-to-image diffusion models and enhances recall under adversarial noise in multimodal retrieval tasks. Additionally, robust text encoders facilitate better reconstruction of input text from its embedding via direct optimization.

Conclusion: This work covers the gap in the literature regarding the robustness of CLIP's text encoders and proposes an effective solution, LEAF, to enhance their robustness.

Abstract: Adversarial input attacks can cause a significant shift of CLIP embeddings.
This can affect the downstream robustness of models incorporating CLIP in the
pipeline, such as text-to-image generative models or large vision language
models. While some efforts have been done towards making the CLIP image
encoders robust, the robustness of text encoders remains unexplored. In this
work, we cover this gap in the literature. We propose LEAF: an efficient
adversarial finetuning method for the text domain, with the ability to scale to
large CLIP models. Our models significantly improve the zero-shot adversarial
accuracy in the text domain, while maintaining the vision performance provided
by robust image encoders. When combined with text-to-image diffusion models, we
can improve the generation quality under adversarial noise. When employing our
robust CLIP encoders in multimodal retrieval tasks, we improve the recall under
adversarial noise over standard CLIP models. Finally, we show that robust text
encoders facilitate better reconstruction of input text from its embedding via
direct optimization.

</details>


### [48] [Probabilistic Factorial Experimental Design for Combinatorial Interventions](https://arxiv.org/abs/2506.03363)
*Divya Shyamal,Jiaqi Zhang,Caroline Uhler*

Main category: cs.LG

TL;DR: The paper introduces probabilistic factorial experimental design for combinatorial interventions, providing optimal dosage solutions and acquisition functions for multi-round settings.


<details>
  <summary>Details</summary>
Motivation: Conducting all possible combinatorial interventions with multiple treatments can be laborious and infeasible as the number of treatments increases.

Method: The experimenter selects a dosage for each treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. The method addresses optimal experimental design within an intervention model that imposes bounded-degree interactions between treatments.

Result: A dosage of $\tfrac{1}{2}$ for each treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating any $k$-way interaction model, regardless of $k$. In the multi-round setting, a near-optimal acquisition function is provided.

Conclusion: Probabilistic factorial experimental design offers a solution to the challenge of conducting combinatorial interventions, providing both passive and active (multi-round) strategies for optimal experimental design.

Abstract: A combinatorial intervention, consisting of multiple treatments applied to a
single unit with potentially interactive effects, has substantial applications
in fields such as biomedicine, engineering, and beyond. Given $p$ possible
treatments, conducting all possible $2^p$ combinatorial interventions can be
laborious and quickly becomes infeasible as $p$ increases. Here we introduce
probabilistic factorial experimental design, formalized from how scientists
perform lab experiments. In this framework, the experimenter selects a dosage
for each possible treatment and applies it to a group of units. Each unit
independently receives a random combination of treatments, sampled from a
product Bernoulli distribution determined by the dosages. Additionally, the
experimenter can carry out such experiments over multiple rounds, adapting the
design in an active manner. We address the optimal experimental design problem
within an intervention model that imposes bounded-degree interactions between
treatments. In the passive setting, we provide a closed-form solution for the
near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each
treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating
any $k$-way interaction model, regardless of $k$, and imply that
$O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate
this model. For the multi-round setting, we provide a near-optimal acquisition
function that can be numerically optimized. We also explore several extensions
of the design problem and finally validate our findings through simulations.

</details>


### [49] [Comparison of different Unique hard attention transformer models by the formal languages they can recognize](https://arxiv.org/abs/2506.03370)
*Leonid Ryvkin*

Main category: cs.LG

TL;DR: This paper surveys the capabilities of unique hard attention transformers encoders (UHATs) in recognizing formal languages, distinguishing among different types of models and discussing their relations along with bounds in first-order logic and circuit complexity.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the capabilities of UHATs in recognizing formal languages by examining different model distinctions.

Method: Surveying various results on UHATs' performance in recognizing formal languages while considering factors such as masked vs. non-masked, finite vs. infinite image, and general vs. bilinear attention score functions.

Result: Recalled relationships between different UHAT models, established a lower bound using first-order logic, and an upper bound concerning circuit complexity.

Conclusion: The survey provides insights into the recognition capabilities of UHATs for formal languages, emphasizing distinctions among models and theoretical bounds.

Abstract: This note is a survey of various results on the capabilities of unique hard
attention transformers encoders (UHATs) to recognize formal languages. We
distinguish between masked vs. non-masked, finite vs. infinite image and
general vs. bilinear attention score functions. We recall some relations
between these models, as well as a lower bound in terms of first-order logic
and an upper bound in terms of circuit complexity.

</details>


### [50] [Product Quantization for Surface Soil Similarity](https://arxiv.org/abs/2506.03374)
*Haley Dozier,Althea Henslee,Ashley Abraham,Andrew Strelzoff,Mark Chappell*

Main category: cs.LG

TL;DR: The paper outlines a machine learning pipeline that combines product quantization with systematic parameter evaluation to improve surface soil taxonomy, allowing for more accurate and specific classifications than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional soil taxonomy has been limited by human-derived classifications based on historical data rather than statistical similarities, which restricts specificity and accuracy.

Method: The method involves using a machine learning pipeline that incorporates product quantization along with systematic evaluation of parameters and outputs to achieve optimal results in soil taxonomy.

Result: This approach enables the creation of highly accurate and flexible soil taxonomies tailored to specific applications, surpassing the limitations of hand-drawn classifications.

Conclusion: Machine learning techniques, specifically the outlined pipeline, offer significant advancements in surface soil taxonomy by overcoming previous limitations and enhancing classification specificity.

Abstract: The use of machine learning (ML) techniques has allowed rapid advancements in
many scientific and engineering fields. One of these problems is that of
surface soil taxonomy, a research area previously hindered by the reliance on
human-derived classifications, which are mostly dependent on dividing a dataset
based on historical understandings of that data rather than data-driven,
statistically observable similarities. Using a ML-based taxonomy allows soil
researchers to move beyond the limitations of human visualization and create
classifications of high-dimension datasets with a much higher level of
specificity than possible with hand-drawn taxonomies. Furthermore, this
pipeline allows for the possibility of producing both highly accurate and
flexible soil taxonomies with classes built to fit a specific application. The
machine learning pipeline outlined in this work combines product quantization
with the systematic evaluation of parameters and output to get the best
available results, rather than accepting sub-optimal results by using either
default settings or best guess settings.

</details>


### [51] [Improving Performance of Spike-based Deep Q-Learning using Ternary Neurons](https://arxiv.org/abs/2506.03392)
*Aref Ghoreishee,Abhishek Mishra,John Walsh,Anup Das,Nagarajan Kandasamy*

Main category: cs.LG

TL;DR: A new ternary spiking neuron model is proposed to improve the performance of deep Q-learning by mitigating the gradient estimation bias.


<details>
  <summary>Details</summary>
Motivation: The limited representation capacity of binary spiking neurons in deep Q-learning motivates the introduction of a ternary spiking neuron model.

Method: Propose a novel ternary spiking neuron model that reduces the estimation bias during training, and use it as the fundamental computing unit in a deep spiking Q-learning network (DSQN).

Result: The proposed ternary spiking neuron improves the network performance compared to existing binary neurons, making DSQN more practical for on-board autonomous decision-making tasks.

Conclusion: The new ternary spiking neuron model mitigates the performance degradation of ternary neurons in Q-learning tasks and enhances the overall performance of the deep spiking Q-learning network.

Abstract: We propose a new ternary spiking neuron model to improve the representation
capacity of binary spiking neurons in deep Q-learning. Although a ternary
neuron model has recently been introduced to overcome the limited
representation capacity offered by the binary spiking neurons, we show that its
performance is worse than that of binary models in deep Q-learning tasks. We
hypothesize gradient estimation bias during the training process as the
underlying potential cause through mathematical and empirical analysis. We
propose a novel ternary spiking neuron model to mitigate this issue by reducing
the estimation bias. We use the proposed ternary spiking neuron as the
fundamental computing unit in a deep spiking Q-learning network (DSQN) and
evaluate the network's performance in seven Atari games from the Gym
environment. Results show that the proposed ternary spiking neuron mitigates
the drastic performance degradation of ternary neurons in Q-learning tasks and
improves the network performance compared to the existing binary neurons,
making DSQN a more practical solution for on-board autonomous decision-making
tasks.

</details>


### [52] [The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks](https://arxiv.org/abs/2506.03404)
*Walter Mayor,Johan Obando-Ceron,Aaron Courville,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: The paper explores the bias-variance trade-off in data collection for PPO, a popular reinforcement learning algorithm, and finds that larger dataset sizes and scaling parallel environments can improve agent performance.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of data collection strategies on the performance of reinforcement learning algorithms like PPO, especially concerning the bias-variance trade-off induced by the number of parallel environments and rollout length.

Method: Empirical analysis of the trade-offs between sample efficiency and overfitting in PPO, examining the effects on network architectures and hyper-parameter sensitivity when scaling data.

Result: Larger dataset sizes can increase final performance across various settings, and scaling parallel environments is more effective than increasing rollout lengths.

Conclusion: Data collection strategies play a crucial role in improving agent performance in reinforcement learning.

Abstract: The use of parallel actors for data collection has been an effective
technique used in reinforcement learning (RL) algorithms. The manner in which
data is collected in these algorithms, controlled via the number of parallel
environments and the rollout length, induces a form of bias-variance trade-off;
the number of training passes over the collected data, on the other hand, must
strike a balance between sample efficiency and overfitting. We conduct an
empirical analysis of these trade-offs on PPO, one of the most popular RL
algorithms that uses parallel actors, and establish connections to network
plasticity and, more generally, optimization stability. We examine its impact
on network architectures, as well as the hyper-parameter sensitivity when
scaling data. Our analyses indicate that larger dataset sizes can increase
final performance across a variety of settings, and that scaling parallel
environments is more effective than increasing rollout lengths. These findings
highlight the critical role of data collection strategies in improving agent
performance.

</details>


### [53] [A Machine Learning Theory Perspective on Strategic Litigation](https://arxiv.org/abs/2506.03411)
*Melissa Dutz,Han Shao,Avrim Blum,Aloni Cohen*

Main category: cs.LG

TL;DR: The paper explores strategic litigation from the perspective of machine learning theory, considering an abstract model of a common-law legal system.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of strategic litigation and how a strategic litigator can influence future rulings by bringing specific cases to higher courts.

Method: An abstract model of a common-law legal system is considered where lower courts apply decision rules learned from higher court's past rulings. The power of a strategic litigator who brings cases to influence these decision rules is explored.

Result: The analysis shows that a strategic litigator can have significant impact on future rulings by carefully selecting which cases to bring to court, even when they expect to lose the case.

Conclusion: Strategic litigation can be effectively analyzed through the lens of machine learning theory, providing insights into its potential influence on legal systems.

Abstract: Strategic litigation involves bringing a legal case to court with the goal of
having a broader impact beyond resolving the case itself: for example, creating
precedent which will influence future rulings. In this paper, we explore
strategic litigation from the perspective of machine learning theory. We
consider an abstract model of a common-law legal system where a lower court
decides new cases by applying a decision rule learned from a higher court's
past rulings. In this model, we explore the power of a strategic litigator, who
strategically brings cases to the higher court to influence the learned
decision rule, thereby affecting future cases. We explore questions including:
What impact can a strategic litigator have? Which cases should a strategic
litigator bring to court? Does it ever make sense for a strategic litigator to
bring a case when they are sure the court will rule against them?

</details>


### [54] [Adaptive Task Vectors for Large Language Models](https://arxiv.org/abs/2506.03426)
*Joonseong Kang,Soojeong Lee,Subeen Park,Sumin Park,Taero Kim,Jihee Kim,Ryunyi Lee,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出了一种名为Adaptive Task Vectors (ATV)的新框架，通过动态生成与特定输入查询和任务相适应的任务向量，解决了In-Context Learning (ICL)在处理未对齐输入查询时的局限性。该方法不仅在已见和未见任务上表现出色，而且从理论上证明了其表达能力优于Prefix-Tuning，并与LoRA在相同秩预算下等效。


<details>
  <summary>Details</summary>
Motivation: 尽管In-Context Learning (ICL)取得成功，但存在诸如对示例顺序敏感、上下文长度受限及计算效率低下等问题。此外，现有的基于任务向量的方法通常从固定的示例集中构建任务向量并在不同输入查询中复用，这可能导致模型在输入查询与底层示例不匹配时难以有效适应，从而降低其在未见任务上的泛化性能。

Method: 提出了Adaptive Task Vectors (ATV)，一个简单而有效的框架，能够根据每个输入查询动态生成任务向量。具体而言，ATV利用一个小的语言模型生成任务向量，然后将其转换以匹配目标大语言模型的架构，并用于指导其输出生成。这种方法不同于依赖固定示例集及其对应向量的ICL和先前的向量基方法，而是为每个特定输入查询和任务动态生成定制化的任务向量。

Result: 实验结果表明，ATV在已见和未见任务上均展现出强大的性能和泛化能力。理论分析进一步指出，ATV在相同的秩预算下与LoRA具有表达等价性，并且比Prefix-Tuning更具表现力，从而为其表示优势提供了正式支持。

Conclusion: Adaptive Task Vectors (ATV)是一种创新的框架，能够通过动态生成任务向量来解决现有方法在处理未对齐输入查询时的局限性。它不仅在实践中表现出色，还从理论上证明了其优越的表示能力，为未来的研究提供了新的方向。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks without parameter updates by conditioning on a few demonstrations
provided in the prompt. Despite its success, ICL suffers from several
limitations, including sensitivity to demonstration order, context length
constraints, and computational inefficiency. To address these challenges, task
vector-based approaches compress task information into a single vector.
However, these methods typically construct task vectors from fixed sets of
demonstrations and reuse them across input queries, without conditioning on the
specific input. This limitation can lead models to struggle with effective
adaptation when the input query is not well aligned with the underlying
demonstrations, consequently degrading their generalization performance on
unseen tasks. To overcome this limitation, we propose Adaptive Task Vectors
(ATV), a simple and effective framework that dynamically generates task vectors
conditioned on each input query. ATV employs a small language model to generate
task vectors, which are then transformed to match the target LLM's architecture
and applied to guide its output generation. In contrast to ICL and previous
vector-based approaches, which rely on fixed demonstration sets and their
corresponding vectors, ATV dynamically generates task vectors tailored to each
specific input query and task. Consequently, ATV demonstrates strong
performance and generalization capabilities, even for unseen tasks.
Furthermore, we provide a theoretical analysis indicating that ATV is
expressively equivalent to LoRA under equal rank budgets and more expressive
than Prefix-Tuning, thereby offering formal support for its representational
advantage.

</details>


### [55] [Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior](https://arxiv.org/abs/2506.03444)
*Yue Gong,Raul Castro Fernandez*

Main category: cs.LG

TL;DR: 随着假设生成的自动化程度提高，假设评估成为新的瓶颈。本文研究了自动假设评估问题，特别是如何从大量统计关系中自动评估哪些相关性值得进一步探索。提出了一种基于LLM权重的知识编码方法——Logit-based Calibrated Prior，用于预测变量对的相关性值，并在真实世界数据集上取得了良好的效果。


<details>
  <summary>Details</summary>
Motivation: 现代系统可以揭示数千个统计关系，但缺乏指导来判断哪些是新颖、非平凡或值得专家关注的。

Method: 利用大型语言模型（LLM）中的知识编码，推导出变量对相关性值的先验分布。提出了一种名为Logit-based Calibrated Prior的方法，将模型的原始输出logits转换为校准的连续预测分布。

Result: 在2,096个真实世界变量对的基准测试中，该方法预测皮尔逊相关系数的符号准确率为78.8%，平均绝对误差为0.26，95%置信区间覆盖率为89.2%。还优于微调的RoBERTa分类器，并在假设排名中表现出更高的Precision@K。此外，该方法能够推广到LLM预训练未见过的相关性。

Conclusion: 提出的Logit-based Calibrated Prior方法可以有效评估相关性假设的新颖性和重要性，反映了上下文敏感推理而非简单记忆。

Abstract: As hypothesis generation becomes increasingly automated, a new bottleneck has
emerged: hypothesis assessment. Modern systems can surface thousands of
statistical relationships-correlations, trends, causal links-but offer little
guidance on which ones are novel, non-trivial, or worthy of expert attention.
In this work, we study the complementary problem to hypothesis generation:
automatic hypothesis assessment. Specifically, we ask: given a large set of
statistical relationships, can we automatically assess which ones are novel and
worth further exploration? We focus on correlations as they are a common entry
point in exploratory data analysis that often serve as the basis for forming
deeper scientific or causal hypotheses.
  To support automatic assessment, we propose to leverage the vast knowledge
encoded in LLMs' weights to derive a prior distribution over the correlation
value of a variable pair. If an LLM's prior expects the correlation value
observed, then such correlation is not surprising, and vice versa. We propose
the Logit-based Calibrated Prior, an LLM-elicited correlation prior that
transforms the model's raw output logits into a calibrated, continuous
predictive distribution over correlation values. We evaluate the prior on a
benchmark of 2,096 real-world variable pairs and it achieves a sign accuracy of
78.8%, a mean absolute error of 0.26, and 95% credible interval coverage of
89.2% in predicting Pearson correlation coefficient. It also outperforms a
fine-tuned RoBERTa classifier in binary correlation prediction and achieves
higher precision@K in hypothesis ranking. We further show that the prior
generalizes to correlations not seen during LLM pretraining, reflecting
context-sensitive reasoning rather than memorization.

</details>


### [56] [Directional Non-Commutative Monoidal Embeddings for MNIST](https://arxiv.org/abs/2506.03472)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: The paper empirically validates a directional non-commutative monoidal embedding framework, showing its effectiveness in image classification on MNIST by outperforming fixed DFT-based embeddings as dimensionality decreases.


<details>
  <summary>Details</summary>
Motivation: To verify the effectiveness of the directional non-commutative monoidal embedding framework in modeling real data, specifically in the context of image classification.

Method: Applied the monoidal embedding framework to the MNIST dataset, comparing learnable embeddings with fixed DFT-based embeddings while varying the embedding dimensionality.

Result: As embedding dimensionality decreased, learned monoidal embeddings significantly outperformed fixed DFT-based embeddings, demonstrating their ability to capture task-specific discriminative spectral components.

Conclusion: Directional non-commutative monoidal embeddings are highly effective for representing image data, providing compact learned representations that maintain high performance.

Abstract: We present an empirical validation of the directional non-commutative
monoidal embedding framework recently introduced in prior
work~\cite{Godavarti2025monoidal}. This framework defines learnable
compositional embeddings using distinct non-commutative operators per dimension
(axis) that satisfy an interchange law, generalizing classical one-dimensional
transforms. Our primary goal is to verify that this framework can effectively
model real data by applying it to a controlled, well-understood task: image
classification on the MNIST dataset~\cite{lecun1998gradient}. A central
hypothesis for why the proposed monoidal embedding works well is that it
generalizes the Discrete Fourier Transform (DFT)~\cite{oppenheim1999discrete}
by learning task-specific frequency components instead of using fixed basis
frequencies. We test this hypothesis by comparing learned monoidal embeddings
against fixed DFT-based embeddings on MNIST. The results show that as the
embedding dimensionality decreases (e.g., from 32 to 8 to 2), the performance
gap between the learned monoidal embeddings and fixed DFT-based embeddings on
MNIST grows increasingly large. This comparison is used as an analytic tool to
explain why the framework performs well: the learnable embeddings can capture
the most discriminative spectral components for the task. Overall, our
experiments confirm that directional non-commutative monoidal embeddings are
highly effective for representing image data, offering a compact learned
representation that retains high task performance. The code used in this work
is available at
https://github.com/mahesh-godavarti/directional_composition_mnist.

</details>


### [57] [CORE: Constraint-Aware One-Step Reinforcement Learning for Simulation-Guided Neural Network Accelerator Design](https://arxiv.org/abs/2506.03474)
*Yifeng Xiao,Yurong Xu,Ning Yan,Masood Mortazavi,Pierluigi Nuzzo*

Main category: cs.LG

TL;DR: CORE is a new one-step RL method for simulation-based DSE that improves sample efficiency and finds better designs in hardware-software co-design of neural network accelerators.


<details>
  <summary>Details</summary>
Motivation: Existing methods for simulation-based DSE have difficulty balancing sampling efficiency and constraint satisfaction due to sparse, delayed feedback and large hybrid action spaces.

Method: CORE uses a constraint-aware, one-step RL approach where the policy agent learns to sample design configurations by defining a structured distribution over them, using a scaling-graph-based decoder to incorporate dependencies, and applying reward shaping to penalize invalid designs based on simulation feedback. It updates the policy using a surrogate objective without learning a value function.

Result: CORE significantly improves sample efficiency and achieves better accelerator configurations compared to state-of-the-art baselines in the hardware-mapping co-design of neural network accelerators.

Conclusion: CORE is a general approach applicable to a broad class of discrete-continuous constrained design problems and shows promise in improving simulation-based DSE.

Abstract: Simulation-based design space exploration (DSE) aims to efficiently optimize
high-dimensional structured designs under complex constraints and expensive
evaluation costs. Existing approaches, including heuristic and multi-step
reinforcement learning (RL) methods, struggle to balance sampling efficiency
and constraint satisfaction due to sparse, delayed feedback, and large hybrid
action spaces. In this paper, we introduce CORE, a constraint-aware, one-step
RL method for simulationguided DSE. In CORE, the policy agent learns to sample
design configurations by defining a structured distribution over them,
incorporating dependencies via a scaling-graph-based decoder, and by reward
shaping to penalize invalid designs based on the feedback obtained from
simulation. CORE updates the policy using a surrogate objective that compares
the rewards of designs within a sampled batch, without learning a value
function. This critic-free formulation enables efficient learning by
encouraging the selection of higher-reward designs. We instantiate CORE for
hardware-mapping co-design of neural network accelerators, demonstrating that
it significantly improves sample efficiency and achieves better accelerator
configurations compared to state-of-the-art baselines. Our approach is general
and applicable to a broad class of discrete-continuous constrained design
problems.

</details>


### [58] [Path Generation and Evaluation in Video Games: A Nonparametric Statistical Approach](https://arxiv.org/abs/2506.03522)
*Daniel Campa,Mehdi Saeedi,Ian Colbert,Srinjoy Das*

Main category: cs.LG

TL;DR: 提出了一种基于非参数统计的路径生成和评估方法，解决了深度学习模型在游戏路径生成中的复杂性和可解释性问题，能够生成多样且可控的人类行为相似路径。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在生成建模方面取得了显著进展，但由于复杂的训练需求和可解释性挑战，视频游戏行业对其接受度较低。需要一种更简单、更易控制的方法来生成和评估导航路径。

Method: 结合两种统计技术：1）无模型的非参数变换，捕捉路径随时间变化的统计特征；2）copula模型，捕捉空间统计依赖关系。使用非参数三样本假设检验进行路径评估，判断生成路径是否过拟合或欠拟合。

Result: 通过两个现有游戏基准的实证分析，展示了所提出方法在生成多样化导航路径方面的精确性和可靠性，并可通过用户可控参数调整路径的人类相似度水平。

Conclusion: 该方法为游戏设计提供了精确控制和可解释的路径生成与评估方案，相比神经网络代理生成的路径更具灵活性和人类行为特征。

Abstract: Navigation path traces play a crucial role in video game design, serving as a
vital resource for both enhancing player engagement and fine-tuning
non-playable character behavior. Generating such paths with human-like realism
can enrich the overall gaming experience, and evaluating path traces can
provide game designers insights into player interactions. Despite the
impressive recent advancements in deep learning-based generative modeling, the
video game industry hesitates to adopt such models for path generation, often
citing their complex training requirements and interpretability challenges. To
address these problems, we propose a novel path generation and evaluation
approach that is grounded in principled nonparametric statistics and provides
precise control while offering interpretable insights. Our path generation
method fuses two statistical techniques: (1) nonparametric model-free
transformations that capture statistical characteristics of path traces through
time; and (2) copula models that capture statistical dependencies in space. For
path evaluation, we adapt a nonparametric three-sample hypothesis test designed
to determine if the generated paths are overfit (mimicking the original data
too closely) or underfit (diverging too far from it). We demonstrate the
precision and reliability of our proposed methods with empirical analysis on
two existing gaming benchmarks to showcase controlled generation of diverse
navigation paths. Notably, our novel path generator can be fine-tuned with user
controllable parameters to create navigation paths that exhibit varying levels
of human-likeness in contrast to those produced by neural network-based agents.
The code is available at https://github.com/daniel-campa/mf-copula.

</details>


### [59] [Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees](https://arxiv.org/abs/2506.03531)
*Daniel Ovalle,Lorenz T. Biegler,Ignacio E. Grossmann,Carl D. Laird,Mateo Dulce Rubio*

Main category: cs.LG

TL;DR: The paper introduces Conformal Mixed-Integer Constraint Learning (C-MICL), which offers probabilistic feasibility guarantees for data-driven constraints in optimization problems by leveraging conformal prediction. It supports both regression and classification tasks, avoids scalability issues, and demonstrates consistent feasibility rates, competitive performance, and reduced computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the issue of constraint violations in standard Mixed-Integer Constraint Learning methods due to model error or data limitations, the authors propose a novel framework that ensures feasible solutions are ground-truth feasible with probabilistic guarantees.

Method: The C-MICL approach uses conformal prediction to provide feasibility guarantees for data-driven constraints in optimization problems. It operates under a conditional independence assumption and supports both regression and classification tasks without needing the true constraint function or facing scalability issues.

Result: Experiments on real-world applications show that C-MICL consistently achieves target feasibility rates, maintains competitive objective performance, and significantly reduces computational costs compared to existing methods.

Conclusion: C-MICL bridges mathematical optimization and machine learning by providing a principled way to incorporate uncertainty-aware constraints into decision-making with rigorous statistical guarantees.

Abstract: We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel
framework that provides probabilistic feasibility guarantees for data-driven
constraints in optimization problems. While standard Mixed-Integer Constraint
Learning methods often violate the true constraints due to model error or data
limitations, our C-MICL approach leverages conformal prediction to ensure
feasible solutions are ground-truth feasible. This guarantee holds with
probability at least $1{-}\alpha$, under a conditional independence assumption.
The proposed framework supports both regression and classification tasks
without requiring access to the true constraint function, while avoiding the
scalability issues associated with ensemble-based heuristics. Experiments on
real-world applications demonstrate that C-MICL consistently achieves target
feasibility rates, maintains competitive objective performance, and
significantly reduces computational cost compared to existing methods. Our work
bridges mathematical optimization and machine learning, offering a principled
approach to incorporate uncertainty-aware constraints into decision-making with
rigorous statistical guarantees.

</details>


### [60] [Learning Monotonic Probabilities with a Generative Cost Model](https://arxiv.org/abs/2506.03542)
*Yongxiang Tang,Yanhua Cheng,Xiaocheng Liu,Chenchen Jiao,Yanxiang Zeng,Ning Luo,Pengjia Yuan,Xialong Liu,Peng Jiang*

Main category: cs.LG

TL;DR: In this paper, the authors redefine the problem of strict monotonic probability as a partial order between an observable revenue variable and a latent cost variable. They introduce Generative Cost Model (GCM) for strict monotonicity and Implicit Generative Cost Model (IGCM) for implicit monotonicity problems. Their method surpasses existing techniques in experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of traditional methods for maintaining monotonicity in machine learning tasks which mainly rely on construction or regularization techniques.

Method: The authors reformulate the monotonicity challenge into modeling the latent cost variable by introducing a generative network called the Generative Cost Model (GCM) for strict monotonicity and Implicit Generative Cost Model (IGCM) for implicit monotonicity.

Result: The approach was validated through numerical simulations of quantile regression and multiple experiments on public datasets, showing significant improvements over existing monotonic modeling techniques.

Conclusion: This study successfully introduces GCM and IGCM to handle strict and implicit monotonicity issues respectively, providing a new perspective to the monotonicity problem in machine learning.

Abstract: In many machine learning tasks, it is often necessary for the relationship
between input and output variables to be monotonic, including both strictly
monotonic and implicitly monotonic relationships. Traditional methods for
maintaining monotonicity mainly rely on construction or regularization
techniques, whereas this paper shows that the issue of strict monotonic
probability can be viewed as a partial order between an observable revenue
variable and a latent cost variable. This perspective enables us to reformulate
the monotonicity challenge into modeling the latent cost variable. To tackle
this, we introduce a generative network for the latent cost variable, termed
the Generative Cost Model (GCM), which inherently addresses the strict
monotonic problem, and propose the Implicit Generative Cost Model (IGCM) to
address the implicit monotonic problem. We further validate our approach with a
numerical simulation of quantile regression and conduct multiple experiments on
public datasets, showing that our method significantly outperforms existing
monotonic modeling techniques. The code for our experiments can be found at
https://github.com/tyxaaron/GCM.

</details>


### [61] [Optimizing FPGA and Wafer Test Coverage with Spatial Sampling and Machine Learning](https://arxiv.org/abs/2506.03556)
*Wang WeiQuan,Riaz-ul-Haque Mian*

Main category: cs.LG

TL;DR: In semiconductor testing, this paper explores three baseline sampling methods and proposes a novel Short Distance Elimination (SDE) algorithm to improve these strategies. Using real industrial data and GPR prediction framework, the hybrid S-SDE and K-SDE strategies are shown to enhance predictive accuracy by 16.26% and 16.49% respectively.


<details>
  <summary>Details</summary>
Motivation: Testing costs in semiconductor manufacturing are high, particularly for wafer and FPGA testing. The motivation is to reduce the number of required tests while maintaining predictive accuracy.

Method: Three baseline sampling strategies (Random Sampling, Stratified Sampling, k-means Clustering Sampling) are investigated. A new algorithm called Short Distance Elimination (SDE) is introduced to improve the sampling quality. Two hybrid strategies, S-SDE and K-SDE, are proposed which incorporate the SDE algorithm into the baseline methods.

Result: The SDE-based strategies significantly enhance predictive accuracy. Specifically, K-SDE improves upon k-means sampling by 16.26% for wafer and 13.07% for FPGA, while S-SDE improves upon stratified sampling by 16.49% for wafer and 8.84% for FPGA.

Conclusion: The proposed SDE-based hybrid strategies effectively improve the predictive accuracy in wafer and FPGA testing with fewer required tests.

Abstract: In semiconductor manufacturing, testing costs remain significantly high,
especially during wafer and FPGA testing. To reduce the number of required
tests while maintaining predictive accuracy, this study investigates three
baseline sampling strategies: Random Sampling, Stratified Sampling, and k-means
Clustering Sampling. To further enhance these methods, this study proposes a
novel algorithm that improves the sampling quality of each approach. This
research is conducted using real industrial production data from wafer-level
tests and silicon measurements from various FPGAs. This study introduces two
hybrid strategies: Stratified with Short Distance Elimination (S-SDE) and
k-means with Short Distance Elimination (K-SDE). Their performance is evaluated
within the framework of Gaussian Process Regression (GPR) for predicting wafer
and FPGA test data. At the core of our proposed approach is the Short Distance
Elimination (SDE) algorithm, which excludes spatially proximate candidate
points during sampling, thereby ensuring a more uniform distribution of
training data across the physical domain. A parameter sweep was conducted over
the (alpha, beta) thresholds, where alpha and beta are in the range {0, 1, 2,
3, 4} and not both zero, to identify the optimal combination that minimizes
RMSD. Experimental results on a randomly selected wafer file reveal that
(alpha, beta) equal (2, 2) yields the lowest RMSD. Accordingly, all subsequent
experiments adopt this parameter configuration. The results demonstrate that
the proposed SDE-based strategies enhance predictive accuracy: K-SDE improves
upon k-means sampling by 16.26 percent (wafer) and 13.07 percent (FPGA), while
S-SDE improves upon stratified sampling by 16.49 percent (wafer) and 8.84
percent (FPGA).

</details>


### [62] [A Class Inference Scheme With Dempster-Shafer Theory for Learning Fuzzy-Classifier Systems](https://arxiv.org/abs/2506.03588)
*Hiroki Shiraishi,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: The paper introduces a novel class inference scheme for Learning Fuzzy-Classifier Systems (LFCSs) based on Dempster-Shafer Theory of Evidence to address limitations in existing voting-based or single-winner-based schemes. This new scheme considers uncertainty, improves transparency and reliability, and shows significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current LFCSs use voting-based or single-winner-based inference schemes which rely heavily on training data performance and risk overfitting on unseen data. There is a need for better decision-making mechanisms that can handle uncertainty and improve generalizability.

Method: The authors propose a class inference scheme based on Dempster-Shafer Theory of Evidence for LFCSs. This scheme calculates belief masses for each class and an 'I don't know' state from each fuzzy rule, thereby considering uncertainty in the classification process.

Result: The proposed scheme demonstrates statistically significant improvements in test macro F1 scores across 30 real-world datasets compared to conventional schemes. It forms smoother decision boundaries, provides reliable confidence measures, and enhances robustness and generalizability of LFCSs.

Conclusion: The novel class inference scheme based on Dempster-Shafer Theory of Evidence improves the performance, transparency, and reliability of LFCSs, offering a promising direction for enhancing decision-making in rule-based machine learning systems.

Abstract: The decision-making process significantly influences the predictions of
machine learning models. This is especially important in rule-based systems
such as Learning Fuzzy-Classifier Systems (LFCSs) where the selection and
application of rules directly determine prediction accuracy and reliability.
LFCSs combine evolutionary algorithms with supervised learning to optimize
fuzzy classification rules, offering enhanced interpretability and robustness.
Despite these advantages, research on improving decision-making mechanisms
(i.e., class inference schemes) in LFCSs remains limited. Most LFCSs use
voting-based or single-winner-based inference schemes. These schemes rely on
classification performance on training data and may not perform well on unseen
data, risking overfitting. To address these limitations, this article
introduces a novel class inference scheme for LFCSs based on the
Dempster-Shafer Theory of Evidence (DS theory). The proposed scheme handles
uncertainty well. By using the DS theory, the scheme calculates belief masses
(i.e., measures of belief) for each specific class and the ``I don't know''
state from each fuzzy rule and infers a class from these belief masses. Unlike
the conventional schemes, the proposed scheme also considers the ``I don't
know'' state that reflects uncertainty, thereby improving the transparency and
reliability of LFCSs. Applied to a variant of LFCS (i.e., Fuzzy-UCS), the
proposed scheme demonstrates statistically significant improvements in terms of
test macro F1 scores across 30 real-world datasets compared to conventional
voting-based and single-winner-based fuzzy inference schemes. It forms smoother
decision boundaries, provides reliable confidence measures, and enhances the
robustness and generalizability of LFCSs in real-world applications. Our
implementation is available at https://github.com/YNU-NakataLab/jUCS.

</details>


### [63] [VCDiag: Classifying Erroneous Waveforms for Failure Triage Acceleration](https://arxiv.org/abs/2506.03590)
*Minh Luu,Surya Jasper,Khoi Le,Evan Pan,Michael Quinn,Aakash Tyagi,Jiang Hu*

Main category: cs.LG

TL;DR: VCDiag利用VCD数据提供了一种高效、可适应的方法来分类失败的波形，并确定可能的故障位置，在最大实验中，对前三个最有可能的模块的识别准确率超过94%。


<details>
  <summary>Details</summary>
Motivation: 设计功能验证中的故障诊断至关重要但耗时，依赖于人工规范审查、日志检查和波形分析。尽管机器学习在激励生成和覆盖率闭合方面有所改进，但在RTL级仿真故障诊断中的应用，特别是对于大型设计，仍然有限。

Method: VCDiag引入了一种新的信号选择和统计压缩方法，将原始数据大小减少了120倍以上，同时保留了分类所需的关键特征。它可以集成到各种Verilog/SystemVerilog设计和测试平台中。

Result: 在最大的实验中，VCDiag在识别前三个最有可能的模块方面实现了超过94%的准确率。

Conclusion: VCDiag为RTL级仿真故障诊断提供了一种高效的解决方案，显著提高了故障定位的准确性。

Abstract: Failure triage in design functional verification is critical but
time-intensive, relying on manual specification reviews, log inspections, and
waveform analyses. While machine learning (ML) has improved areas like stimulus
generation and coverage closure, its application to RTL-level simulation
failure triage, particularly for large designs, remains limited. VCDiag offers
an efficient, adaptable approach using VCD data to classify failing waveforms
and pinpoint likely failure locations. In the largest experiment, VCDiag
achieves over 94% accuracy in identifying the top three most likely modules.
The framework introduces a novel signal selection and statistical compression
approach, achieving over 120x reduction in raw data size while preserving
features essential for classification. It can also be integrated into diverse
Verilog/SystemVerilog designs and testbenches.

</details>


### [64] [Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner](https://arxiv.org/abs/2506.03595)
*Runa Eschenhagen,Aaron Defazio,Tsung-Hsien Lee,Richard E. Turner,Hao-Jun Michael Shi*

Main category: cs.LG

TL;DR: The paper explores the heuristics in Shampoo algorithm from Frobenius norm approximation perspective and proposes practical techniques to improve Kronecker-factorization-based training algorithms.


<details>
  <summary>Details</summary>
Motivation: Shampoo shows success but relies on heuristics that lack theoretical justification.

Method: Investigate heuristics using Frobenius norm approximation, decouple eigenvalues and eigenbasis updates, propose adaptive criterion for eigenbasis computation frequency.

Result: Grafting from Adam mitigates issues with eigenvalues, direct correction can remove need for learning rate grafting, adaptive criterion manages error from infrequent computations.

Conclusion: Practical techniques provide principled approach to removing Shampoo's heuristics and improving training algorithms.

Abstract: The recent success of Shampoo in the AlgoPerf contest has sparked renewed
interest in Kronecker-factorization-based optimization algorithms for training
neural networks. Despite its success, Shampoo relies heavily on several
heuristics such as learning rate grafting and stale preconditioning to achieve
performance at-scale. These heuristics increase algorithmic complexity,
necessitate further hyperparameter tuning, and lack theoretical justification.
This paper investigates these heuristics from the angle of Frobenius norm
approximation to full-matrix Adam and decouples the preconditioner's
eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates
the staleness and mis-scaling of the preconditioner's eigenvalues and how
correcting the eigenvalues directly can eliminate the need for learning rate
grafting. To manage the error induced by infrequent eigenbasis computations, we
propose an adaptive criterion for determining the eigenbasis computation
frequency motivated by terminating a warm-started QR algorithm. This criterion
decouples the update frequency of different preconditioner matrices and enables
us to investigate the impact of approximation error on convergence. These
practical techniques offer a principled angle towards removing Shampoo's
heuristics and developing improved Kronecker-factorization-based training
algorithms.

</details>


### [65] [Adapting Rule Representation With Four-Parameter Beta Distribution for Learning Classifier Systems](https://arxiv.org/abs/2506.03602)
*Hiroki Shiraishi,Yohei Hayamizu,Tomonori Hashiyama,Keiki Takadama,Hisao Ishibuchi,Masaya Nakata*

Main category: cs.LG

TL;DR: This paper introduces a flexible rule representation using a four-parameter beta distribution integrated into a fuzzy-style Learning Classifier System (LCS). This approach enhances adaptability, generalization, and interpretability in rule-based machine learning systems.


<details>
  <summary>Details</summary>
Motivation: Current Learning Classifier Systems (LCSs) struggle with choosing appropriate rule representations for different problems or subspaces within the input space. An adaptive mechanism is needed to address this challenge.

Method: The authors propose a flexible rule representation based on a four-parameter beta distribution incorporated into a fuzzy-style LCS. This method allows the system to automatically select suitable representations for different subspaces and favors crisp rules where possible to improve interpretability.

Result: Experimental results demonstrate superior test accuracy and more compact rule sets compared to standard representations in real-world classification tasks.

Conclusion: The proposed flexible rule representation using the four-parameter beta distribution enhances the performance and interpretability of LCSs, offering an effective solution for adapting rule representations.

Abstract: Rule representations significantly influence the search capabilities and
decision boundaries within the search space of Learning Classifier Systems
(LCSs), a family of rule-based machine learning systems that evolve
interpretable models through evolutionary processes. However, it is very
difficult to choose an appropriate rule representation for each problem.
Additionally, some problems benefit from using different representations for
different subspaces within the input space. Thus, an adaptive mechanism is
needed to choose an appropriate rule representation for each rule in LCSs. This
article introduces a flexible rule representation using a four-parameter beta
distribution and integrates it into a fuzzy-style LCS. The four-parameter beta
distribution can form various function shapes, and this flexibility enables our
LCS to automatically select appropriate representations for different
subspaces. Our rule representation can represent crisp/fuzzy decision
boundaries in various boundary shapes, such as rectangles and bells, by
controlling four parameters, compared to the standard representations such as
trapezoidal ones. Leveraging this flexibility, our LCS is designed to adapt the
appropriate rule representation for each subspace. Moreover, our LCS
incorporates a generalization bias favoring crisp rules where feasible,
enhancing model interpretability without compromising accuracy. Experimental
results on real-world classification tasks show that our LCS achieves
significantly superior test accuracy and produces more compact rule sets. Our
implementation is available at https://github.com/YNU-NakataLab/Beta4-UCS. An
extended abstract related to this work is available at
https://doi.org/10.36227/techrxiv.174900805.59801248/v1.

</details>


### [66] [GCFL: A Gradient Correction-based Federated Learning Framework for Privacy-preserving CPSS](https://arxiv.org/abs/2506.03618)
*Jiayi Wan,Xiang Zhu,Fanzhen Liu,Wei Fan,Xiaolong Xu*

Main category: cs.LG

TL;DR: In order to solve the problem of privacy risks in CPSS, this paper proposes a new framework for differentially private federated learning, which introduces a server-side gradient correction mechanism to balance privacy protection and accuracy. The framework can detect and correct deviations in noisy local gradients, reduce the negative impact of noise, align gradients from different clients, and guide the model to converge to the global optimum.


<details>
  <summary>Details</summary>
Motivation: Federated learning has great application potential in CPSS, but there are privacy risks. Existing methods mainly focus on dynamically adjusting the added noise or discarding certain gradients, but these methods cannot remove the noise that hinders convergence and correct the affected gradients, thereby reducing the accuracy of model classification.

Method: This paper proposes a novel framework that introduces a server-side gradient correction mechanism. After the client performs gradient clipping and noise perturbation, the framework detects deviations in the noisy local gradients and uses a projection mechanism to correct them, thereby reducing the negative impact of noise and promoting the alignment of gradients from different clients.

Result: The experimental results show that under the same privacy budget, this framework achieves state-of-the-art performance.

Conclusion: This paper proposes a new framework for differentially private federated learning, which not only guarantees privacy but also improves the accuracy of model classification.

Abstract: Federated learning, as a distributed architecture, shows great promise for
applications in Cyber-Physical-Social Systems (CPSS). In order to mitigate the
privacy risks inherent in CPSS, the integration of differential privacy with
federated learning has attracted considerable attention. Existing research
mainly focuses on dynamically adjusting the noise added or discarding certain
gradients to mitigate the noise introduced by differential privacy. However,
these approaches fail to remove the noise that hinders convergence and correct
the gradients affected by the noise, which significantly reduces the accuracy
of model classification. To overcome these challenges, this paper proposes a
novel framework for differentially private federated learning that balances
rigorous privacy guarantees with accuracy by introducing a server-side gradient
correction mechanism. Specifically, after clients perform gradient clipping and
noise perturbation, our framework detects deviations in the noisy local
gradients and employs a projection mechanism to correct them, mitigating the
negative impact of noise. Simultaneously, gradient projection promotes the
alignment of gradients from different clients and guides the model towards
convergence to a global optimum. We evaluate our framework on several benchmark
datasets, and the experimental results demonstrate that it achieves
state-of-the-art performance under the same privacy budget.

</details>


### [67] [Out-of-Distribution Graph Models Merging](https://arxiv.org/abs/2506.03674)
*Yidi Wang,Jiawei Gu,pei Xiaobing,Xubin Zheng,Xiao Luo,Pengyang Wang,Ziyue Qiao*

Main category: cs.LG

TL;DR: This paper tackles the challenge of merging out-of-distribution graph models by proposing a novel graph generation strategy and a merging method using MoE module and masking mechanism.


<details>
  <summary>Details</summary>
Motivation: The motivation is to construct a generalized model from multiple pre-trained graph models across different domains with distribution discrepancies, addressing the difficulty in learning domain-invariant knowledge and consolidating expertise from heterogeneous GNN backbones.

Method: A graph generation strategy instantiates the mixture distribution of multiple domains. Then, pre-trained graph models are merged and fine-tuned via a Mixture-of-Experts (MoE) module and a masking mechanism for generalized adaptation.

Result: Both theoretical analysis and experimental results show the effectiveness of the proposed approach in enhancing model generalization without needing any source/target domain data.

Conclusion: The framework provides an architecture-agnostic solution to the problem of out-of-distribution graph models merging, successfully demonstrating its effectiveness through both theory and experiments.

Abstract: This paper studies a novel problem of out-of-distribution graph models
merging, which aims to construct a generalized model from multiple graph models
pre-trained on different domains with distribution discrepancy. This problem is
challenging because of the difficulty in learning domain-invariant knowledge
implicitly in model parameters and consolidating expertise from potentially
heterogeneous GNN backbones. In this work, we propose a graph generation
strategy that instantiates the mixture distribution of multiple domains. Then,
we merge and fine-tune the pre-trained graph models via a MoE module and a
masking mechanism for generalized adaptation. Our framework is
architecture-agnostic and can operate without any source/target domain data.
Both theoretical analysis and experimental results demonstrate the
effectiveness of our approach in addressing the model generalization problem.

</details>


### [68] [Comprehensive Attribute Encoding and Dynamic LSTM HyperModels for Outcome Oriented Predictive Business Process Monitoring](https://arxiv.org/abs/2506.03696)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: The paper proposes dynamic LSTM HyperModels with hierarchical encoding, pseudo-embedding techniques and specialized LSTM variants for PBPM.


<details>
  <summary>Details</summary>
Motivation: Existing methods in PBPM lack flexibility to handle real-world challenges such as simultaneous events, class imbalance, and multi-level attributes. They also struggle to support adaptive representations and generalize across heterogeneous datasets.

Method: The proposed method integrates two-level hierarchical encoding for event and sequence attributes, character-based decomposition of event labels, novel pseudo-embedding techniques for durations and attribute correlations, and specialized LSTM variants for simultaneous event modeling with multidimensional embeddings and time-difference flag augmentation.

Result: Experimental validation on four public and real-world datasets demonstrates up to 100% accuracy on balanced datasets and F1 scores exceeding 86% on imbalanced ones.

Conclusion: The approach advances PBPM by offering modular and interpretable models better suited for deployment in complex settings and contributes to the broader AI community by improving temporal outcome prediction, supporting data heterogeneity, and promoting explainable process intelligence frameworks.

Abstract: Predictive Business Process Monitoring (PBPM) aims to forecast future
outcomes of ongoing business processes. However, existing methods often lack
flexibility to handle real-world challenges such as simultaneous events, class
imbalance, and multi-level attributes. While prior work has explored static
encoding schemes and fixed LSTM architectures, they struggle to support
adaptive representations and generalize across heterogeneous datasets. To
address these limitations, we propose a suite of dynamic LSTM HyperModels that
integrate two-level hierarchical encoding for event and sequence attributes,
character-based decomposition of event labels, and novel pseudo-embedding
techniques for durations and attribute correlations. We further introduce
specialized LSTM variants for simultaneous event modeling, leveraging
multidimensional embeddings and time-difference flag augmentation. Experimental
validation on four public and real-world datasets demonstrates up to 100%
accuracy on balanced datasets and F1 scores exceeding 86\% on imbalanced ones.
Our approach advances PBPM by offering modular and interpretable models better
suited for deployment in complex settings. Beyond PBPM, it contributes to the
broader AI community by improving temporal outcome prediction, supporting data
heterogeneity, and promoting explainable process intelligence frameworks.

</details>


### [69] [Learning-at-Criticality in Large Language Models for Quantum Field Theory and Beyond](https://arxiv.org/abs/2506.03703)
*Xiansheng Cai,Sihan Hu,Tao Wang,Yuan Huang,Pan Zhang,Youjin Deng,Kun Chen*

Main category: cs.LG

TL;DR: 引入了学习在临界点(LaC)的方法，通过强化学习调整大语言模型(LLMs)，使其能够从最少的数据中实现最佳泛化。LaC利用临界现象这一物理原理，使AI能够在基础物理学中应对复杂且数据稀疏的挑战。


<details>
  <summary>Details</summary>
Motivation: 基础物理学常面临复杂的符号问题，缺乏指导性的范例或既定原则。而传统的人工智能方法需要大量的数据来学习，这限制了其在信息稀缺领域的应用。因此，研究者们试图寻找一种新的方法，以解决这一问题。

Method: 提出了一种名为学习在临界点(LaC)的强化学习方案，该方案将大语言模型调整到一个尖锐的学习过渡点，在此点上，LLMs能从最小的数据集中达到峰值泛化能力。此外，还设计了一个概念网络模型(CoNet)来捕捉LLMs可能链接令牌的本质，并展示了这种转换具有二阶相变的特征。

Result: 经过LaC调优的8B参数LLM，使用少量象征性Matsubara和的范例，成功解决了未见过的、更高阶的问题，并且显著优于远更大的模型。

Conclusion: LaC利用物理原理中的临界现象，为复杂、数据稀疏的基础物理挑战提供了强大的AI工具。

Abstract: Fundamental physics often confronts complex symbolic problems with few
guiding exemplars or established principles. While artificial intelligence (AI)
offers promise, its typical need for vast datasets to learn from hinders its
use in these information-scarce frontiers. We introduce learning at criticality
(LaC), a reinforcement learning (RL) scheme that tunes Large Language Models
(LLMs) to a sharp learning transition, addressing this information scarcity. At
this transition, LLMs achieve peak generalization from minimal data,
exemplified by 7-digit base-7 addition -- a test of nontrivial arithmetic
reasoning. To elucidate this peak, we analyze a minimal concept-network model
(CoNet) designed to capture the essence of how LLMs might link tokens. Trained
on a single exemplar, this model also undergoes a sharp learning transition.
This transition exhibits hallmarks of a second-order phase transition, notably
power-law distributed solution path lengths. At this critical point, the system
maximizes a ``critical thinking pattern" crucial for generalization, enabled by
the underlying scale-free exploration. This suggests LLMs reach peak
performance by operating at criticality, where such explorative dynamics enable
the extraction of underlying operational rules. We demonstrate LaC in quantum
field theory: an 8B-parameter LLM, tuned to its critical point by LaC using a
few exemplars of symbolic Matsubara sums, solves unseen, higher-order problems,
significantly outperforming far larger models. LaC thus leverages critical
phenomena, a physical principle, to empower AI for complex, data-sparse
challenges in fundamental physics.

</details>


### [70] [On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity](https://arxiv.org/abs/2506.03719)
*Quentin Bertrand,Anne Gagneux,Mathurin Massias,Rémi Emonet*

Main category: cs.LG

TL;DR: 现代深度生成模型能够生成高质量的合成样本，这些样本通常与真实训练数据难以区分。本文探讨了扩散和流匹配技术等方法为何能如此有效地推广，并排除了条件流匹配损失的噪声性质为主要贡献因素的可能性。


<details>
  <summary>Details</summary>
Motivation: 研究者们希望理解为什么现代深度生成模型（例如扩散模型和流匹配技术）能够如此有效地推广。

Method: 通过实证研究，比较高维设置下随机版和闭式版的流匹配损失的表现；使用最先进的流匹配模型在标准图像数据集上进行实验，评估两者的统计性能。

Result: 在高维情况下，随机版和闭式版的流匹配损失几乎等效；两者在统计性能上相当，且使用闭式版甚至可能提高性能。

Conclusion: 条件流匹配损失的噪声性质并非流匹配推广能力的主要贡献因素。

Abstract: Modern deep generative models can now produce high-quality synthetic samples
that are often indistinguishable from real training data. A growing body of
research aims to understand why recent methods -- such as diffusion and flow
matching techniques -- generalize so effectively. Among the proposed
explanations are the inductive biases of deep learning architectures and the
stochastic nature of the conditional flow matching loss. In this work, we rule
out the latter -- the noisy nature of the loss -- as a primary contributor to
generalization in flow matching. First, we empirically show that in
high-dimensional settings, the stochastic and closed-form versions of the flow
matching loss yield nearly equivalent losses. Then, using state-of-the-art flow
matching models on standard image datasets, we demonstrate that both variants
achieve comparable statistical performance, with the surprising observation
that using the closed-form can even improve performance.

</details>


### [71] [Sign-SGD is the Golden Gate between Multi-Node to Single-Node Learning: Significant Boost via Parameter-Free Optimization](https://arxiv.org/abs/2506.03725)
*Daniil Medyakov,Sergey Stanko,Gleb Molodtsov,Philip Zmushko,Grigoriy Evseev,Egor Petrov,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: The paper explores variants of single-node deterministic Sign-SGD to address the issue of determining an effective stepsize in large language model training, extending approaches to stochastic and multi-node learning scenarios with momentum, and validates practical applicability through experiments.


<details>
  <summary>Details</summary>
Motivation: Training large language models is resource-intensive, and while Sign-SGD is a promising method for reducing resource demands, there is no automatic way to determine an effective stepsize from a theoretical standpoint due to lack of access to dataset parameters in real-world applications.

Method: Design several variants of single-node deterministic Sign-SGD and extend these approaches to practical scenarios including stochastic single-node, multi-node learning, and methods with incorporated momentum.

Result: Conducted extensive experiments on real machine learning problems which highlight the practical applicability of the proposed ideas.

Conclusion: The designed variants of Sign-SGD show practical applicability in various learning scenarios, providing solutions for more efficient large language model training.

Abstract: Quite recently, large language models have made a significant breakthrough
across various disciplines. However, training them is an extremely
resource-intensive task, even for major players with vast computing resources.
One of the methods gaining popularity in light of these challenges is Sign-SGD.
This method can be applied both as a memory-efficient approach in single-node
training and as a gradient compression technique in the distributed learning.
Nevertheless, it is impossible to automatically determine the effective
stepsize from the theoretical standpoint. Indeed, it depends on the parameters
of the dataset to which we do not have access in the real-world learning
paradigm. To address this issue, we design several variants of single-node
deterministic Sign-SGD. We extend our approaches to practical scenarios:
stochastic single-node and multi-node learning, methods with incorporated
momentum. We conduct extensive experiments on real machine learning problems
that emphasize the practical applicability of our ideas.

</details>


### [72] [PPO in the Fisher-Rao geometry](https://arxiv.org/abs/2506.03757)
*Razvan-Andrei Lascu,David Šiška,Łukasz Szpruch*

Main category: cs.LG

TL;DR: The paper introduces Fisher-Rao PPO (FR-PPO), a new variant of Proximal Policy Optimization, using Fisher-Rao geometry to provide stronger theoretical guarantees for policy improvement and sub-linear convergence in tabular settings.


<details>
  <summary>Details</summary>
Motivation: Despite the popularity of Proximal Policy Optimization (PPO) for reinforcement learning, it lacks formal theoretical guarantees for policy improvement and convergence.

Method: The authors derive a tighter surrogate loss function in the Fisher-Rao (FR) geometry, creating a novel variant called Fisher-Rao PPO (FR-PPO). This approach contrasts with traditional methods that use KL divergence penalties derived from linearizing value functions in flat geometric spaces.

Result: FR-PPO offers strong theoretical guarantees such as monotonic policy improvement. In tabular settings, FR-PPO achieves sub-linear convergence without depending on the dimensionality of action or state spaces.

Conclusion: This work represents an important step towards establishing formal convergence results for PPO-based algorithms, enhancing both the theoretical understanding and practical application of these methods.

Abstract: Proximal Policy Optimization (PPO) has become a widely adopted algorithm for
reinforcement learning, offering a practical policy gradient method with strong
empirical performance. Despite its popularity, PPO lacks formal theoretical
guarantees for policy improvement and convergence. PPO is motivated by Trust
Region Policy Optimization (TRPO) that utilizes a surrogate loss with a KL
divergence penalty, which arises from linearizing the value function within a
flat geometric space. In this paper, we derive a tighter surrogate in the
Fisher-Rao (FR) geometry, yielding a novel variant, Fisher-Rao PPO (FR-PPO).
Our proposed scheme provides strong theoretical guarantees, including monotonic
policy improvement. Furthermore, in the tabular setting, we demonstrate that
FR-PPO achieves sub-linear convergence without any dependence on the
dimensionality of the action or state spaces, marking a significant step toward
establishing formal convergence results for PPO-based algorithms.

</details>


### [73] [Scaling CrossQ with Weight Normalization](https://arxiv.org/abs/2506.03758)
*Daniel Palenicek,Florian Vogt,Jan Peters*

Main category: cs.LG

TL;DR: 通过将权重归一化集成到CrossQ框架中，解决了高更新到数据（UTD）比率下的训练挑战，从而提高了模型在DeepMind控制基准上的性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习取得了显著进展，但样本效率仍然是实际应用中的瓶颈。本文研究了在更高UTD比率下CrossQ的扩展行为，并解决相关的训练动态问题。

Method: 识别并解决了高UTD比率下的Q偏差爆炸和批评者网络权重增长的问题，通过将权重归一化引入CrossQ框架来稳定训练过程，防止可塑性损失并保持恒定的有效学习率。

Result: 所提出的方法在一系列具有挑战性的任务上表现出色，包括复杂的狗和类人环境，在DeepMind控制基准上实现了竞争性或优越的性能。

Conclusion: 该工作提供了一种无需剧烈干预（如网络重置）即可提高无模型强化学习样本效率和可扩展性的稳健方法。

Abstract: Reinforcement learning has achieved significant milestones, but sample
efficiency remains a bottleneck for real-world applications. Recently, CrossQ
has demonstrated state-of-the-art sample efficiency with a low update-to-data
(UTD) ratio of 1. In this work, we explore CrossQ's scaling behavior with
higher UTD ratios. We identify challenges in the training dynamics which are
emphasized by higher UTDs, particularly Q-bias explosion and the growing
magnitude of critic network weights. To address this, we integrate weight
normalization into the CrossQ framework, a solution that stabilizes training,
prevents potential loss of plasticity and keeps the effective learning rate
constant. Our proposed approach reliably scales with increasing UTD ratios,
achieving competitive or superior performance across a range of challenging
tasks on the DeepMind control benchmark, notably the complex dog and humanoid
environments. This work eliminates the need for drastic interventions, such as
network resets, and offers a robust pathway for improving sample efficiency and
scalability in model-free reinforcement learning.

</details>


### [74] [FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning](https://arxiv.org/abs/2506.03777)
*Li Zhang,Zhongxuan Han,Chaochao chen,Xiaohua Feng,Jiaming Zhang,Yuyuan Li*

Main category: cs.LG

TL;DR: In order to solve the fairness problem in Federated Learning (FL) decision-making scenarios, this paper proposes a new framework called FedFACT. It mainly solves two problems: harmonizing global and local fairness in multi-classification and enabling controllable optimal accuracy-fairness trade-off. Experiments show that FedFACT performs better in balancing accuracy and fairness.


<details>
  <summary>Details</summary>
Motivation: Current research on federated learning focuses on two concepts of group fairness: global fairness and local fairness. However, there are still two unresolved challenges: how to coordinate global and local fairness in multi-class classification and how to achieve controllable optimal accuracy-fairness balance.

Method: Propose a novel controllable federated group-fairness calibration framework named FedFACT. This framework identifies Bayes-optimal classifiers under global and local fairness constraints in multi-class cases, reformulates fair FL as personalized cost-sensitive learning problems for processing and bi-level optimization for post-processing.

Result: Theoretically, it provides convergence and generalization guarantees for FedFACT to approach near-optimal accuracy under given fairness levels. Experimentally, extensive experiments on multiple datasets across various data heterogeneity demonstrate that FedFACT consistently outperforms baselines in balancing accuracy and global-local fairness.

Conclusion: FedFACT is an effective framework that can solve the existing fairness problems in federated learning, especially in multi-classification problems, and can achieve a better balance between accuracy and fairness.

Abstract: With emerging application of Federated Learning (FL) in decision-making
scenarios, it is imperative to regulate model fairness to prevent disparities
across sensitive groups (e.g., female, male). Current research predominantly
focuses on two concepts of group fairness within FL: Global Fairness (overall
model disparity across all clients) and Local Fairness (the disparity within
each client). However, the non-decomposable, non-differentiable nature of
fairness criteria pose two fundamental, unresolved challenges for fair FL: (i)
Harmonizing global and local fairness in multi-class classification; (ii)
Enabling a controllable, optimal accuracy-fairness trade-off. To tackle the
aforementioned challenges, we propose a novel controllable federated
group-fairness calibration framework, named FedFACT. FedFACT identifies the
Bayes-optimal classifiers under both global and local fairness constraints in
multi-class case, yielding models with minimal performance decline while
guaranteeing fairness. To effectively realize an adjustable, optimal
accuracy-fairness balance, we derive specific characterizations of the
Bayes-optimal fair classifiers for reformulating fair FL as personalized
cost-sensitive learning problem for in-processing, and bi-level optimization
for post-processing. Theoretically, we provide convergence and generalization
guarantees for FedFACT to approach the near-optimal accuracy under given
fairness levels. Extensive experiments on multiple datasets across various data
heterogeneity demonstrate that FedFACT consistently outperforms baselines in
balancing accuracy and global-local fairness.

</details>


### [75] [When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective](https://arxiv.org/abs/2506.03784)
*Beatrix M. G. Nielsen,Emanuele Marconato,Andrea Dittadi,Luigi Gresele*

Main category: cs.LG

TL;DR: The paper investigates the relationship between model distribution closeness and representational similarity in deep neural networks, showing that small Kullback-Leibler divergence does not guarantee similar representations, but defines a distributional distance where closeness implies representational similarity.


<details>
  <summary>Details</summary>
Motivation: To understand when and why representations learned by different deep neural networks are similar from the perspective of identifiability theory.

Method: Focusing on a model family including popular pre-training approaches like autoregressive language models, the authors prove that small Kullback-Leibler divergence between model distributions does not ensure similar representations. They define a new distributional distance for which closeness implies representational similarity.

Result: Models arbitrarily close to maximizing likelihood can learn dissimilar representations. In synthetic experiments, wider networks learn distributions closer with respect to the defined distance and have more similar representations.

Conclusion: Established a link between closeness in distribution and representational similarity in deep neural networks.

Abstract: When and why representations learned by different deep neural networks are
similar is an active research topic. We choose to address these questions from
the perspective of identifiability theory, which suggests that a measure of
representational similarity should be invariant to transformations that leave
the model distribution unchanged. Focusing on a model family which includes
several popular pre-training approaches, e.g., autoregressive language models,
we explore when models which generate distributions that are close have similar
representations. We prove that a small Kullback-Leibler divergence between the
model distributions does not guarantee that the corresponding representations
are similar. This has the important corollary that models arbitrarily close to
maximizing the likelihood can still learn dissimilar representations, a
phenomenon mirrored in our empirical observations on models trained on
CIFAR-10. We then define a distributional distance for which closeness implies
representational similarity, and in synthetic experiments, we find that wider
networks learn distributions which are closer with respect to our distance and
have more similar representations. Our results establish a link between
closeness in distribution and representational similarity.

</details>


### [76] [Attention-Only Transformers via Unrolled Subspace Denoising](https://arxiv.org/abs/2506.03790)
*Peng Wang,Yifu Lu,Yaodong Yu,Druv Pai,Qing Qu,Yi Ma*

Main category: cs.LG

TL;DR: An interpretable transformer architecture is derived by compressing noisy token representations towards a mixture of low-dimensional subspaces, resulting in a compact model with only self-attention operators and skip connections that performs efficient denoising and achieves performance close to standard transformers.


<details>
  <summary>Details</summary>
Motivation: The empirical design of transformer architectures lacks mathematical justification and interpretability, and some components may be redundant.

Method: The architecture is based on compressing noisy initial token representations towards a mixture of low-dimensional subspaces using multi-head self-attention as a denoising operation, unrolled into a deep network with only self-attention operators and skip connections.

Result: Each layer improves the signal-to-noise ratio of token representations at a linear rate, and the model achieves performance close to standard transformer architectures like GPT-2 and CRATE in vision and language tasks.

Conclusion: A fully interpretable transformer architecture with only necessary components was successfully derived, demonstrating efficient denoising and competitive performance.

Abstract: Despite the popularity of transformers in practice, their architectures are
empirically designed and neither mathematically justified nor interpretable.
Moreover, as indicated by many empirical studies, some components of
transformer architectures may be redundant. To derive a fully interpretable
transformer architecture with only necessary components, we contend that the
goal of representation learning is to compress a set of noisy initial token
representations towards a mixture of low-dimensional subspaces. To compress
these noisy token representations, an associated denoising operation naturally
takes the form of a multi-head (subspace) self-attention. By unrolling such
iterative denoising operations into a deep network, we arrive at a highly
compact architecture that consists of \textit{only} self-attention operators
with skip connections at each layer. Moreover, we show that each layer performs
highly efficient denoising: it improves the signal-to-noise ratio of token
representations \textit{at a linear rate} with respect to the number of layers.
Despite its simplicity, extensive experiments on vision and language tasks
demonstrate that such a transformer achieves performance close to that of
standard transformer architectures such as GPT-2 and CRATE.

</details>


### [77] [Learning Equilibria in Matching Games with Bandit Feedback](https://arxiv.org/abs/2506.03802)
*Andreas Athanasopoulos,Christos Dimitrakakis*

Main category: cs.LG

TL;DR: 研究了在广义双边匹配市场中学到均衡的问题，提出了一种UCB算法，该算法通过乐观估计游戏收益，实现了时间范围T内的次线性、与实例无关的后悔值。


<details>
  <summary>Details</summary>
Motivation: 调查在广义双边匹配市场中学习均衡的问题，其中代理可以根据分配的匹配适应性选择其行动。

Method: 采用匹配均衡的概念，引入匹配不稳定性作为给定对偏离均衡对的度量，并提出一种UCB算法，其中代理基于对游戏收益的乐观估计形成偏好并选择行动。

Result: 证明该算法在时间范围T内实现了次线性、与实例无关的后悔值。

Conclusion: 所提出的UCB算法可以在带宽反馈下学习到均衡，解决了在初始未知收益矩阵的情况下进行零和博弈的集中过程问题。

Abstract: We investigate the problem of learning an equilibrium in a generalized
two-sided matching market, where agents can adaptively choose their actions
based on their assigned matches. Specifically, we consider a setting in which
matched agents engage in a zero-sum game with initially unknown payoff
matrices, and we explore whether a centralized procedure can learn an
equilibrium from bandit feedback. We adopt the solution concept of matching
equilibrium, where a pair consisting of a matching $\mathfrak{m}$ and a set of
agent strategies $X$ forms an equilibrium if no agent has the incentive to
deviate from $(\mathfrak{m}, X)$. To measure the deviation of a given pair
$(\mathfrak{m}, X)$ from the equilibrium pair $(\mathfrak{m}^\star, X^\star)$,
we introduce matching instability that can serve as a regret measure for the
corresponding learning problem. We then propose a UCB algorithm in which agents
form preferences and select actions based on optimistic estimates of the game
payoffs, and prove that it achieves sublinear, instance-independent regret over
a time horizon $T$.

</details>


### [78] [Graph Neural Networks for Resource Allocation in Multi-Channel Wireless Networks](https://arxiv.org/abs/2506.03813)
*Lili Chen,Changyang She,Jingge Zhu,Jamie Evans*

Main category: cs.LG

TL;DR: The paper proposes an enhanced WMMSE (eWMMSE) algorithm and a graph neural network-based solution (JCPGNN-M) for joint channel and power allocation in multi-channel wireless networks. JCPGNN-M outperforms eWMMSE in data rate, has lower inference time, and generalizes well to larger networks.


<details>
  <summary>Details</summary>
Motivation: Interference is a major bottleneck in improving data rates in wireless networks with increasing mobile devices. Efficient joint channel and power allocation (JCPA) is needed to manage interference.

Method: An enhanced WMMSE (eWMMSE) algorithm is proposed first. Then, JCPGNN-M, a graph neural network-based solution is introduced which allows simultaneous multi-channel allocation per user. The problem is reformulated as a Lagrangian function to enforce total power constraints systematically.

Result: JCPGNN-M achieves better data rates compared to eWMMSE, has much lower inference time, and generalizes well to larger networks.

Conclusion: JCPGNN-M provides an efficient solution for joint channel and power allocation in multi-channel wireless networks.

Abstract: As the number of mobile devices continues to grow, interference has become a
major bottleneck in improving data rates in wireless networks. Efficient joint
channel and power allocation (JCPA) is crucial for managing interference. In
this paper, we first propose an enhanced WMMSE (eWMMSE) algorithm to solve the
JCPA problem in multi-channel wireless networks. To reduce the computational
complexity of iterative optimization, we further introduce JCPGNN-M, a graph
neural network-based solution that enables simultaneous multi-channel
allocation for each user. We reformulate the problem as a Lagrangian function,
which allows us to enforce the total power constraints systematically. Our
solution involves combining this Lagrangian framework with GNNs and iteratively
updating the Lagrange multipliers and resource allocation scheme. Unlike
existing GNN-based methods that limit each user to a single channel, JCPGNN-M
supports efficient spectrum reuse and scales well in dense network scenarios.
Simulation results show that JCPGNN-M achieves better data rate compared to
eWMMSE. Meanwhile, the inference time of JCPGNN-M is much lower than eWMMS, and
it can generalize well to larger networks.

</details>


### [79] [Survey of Active Learning Hyperparameters: Insights from a Large-Scale Experimental Grid](https://arxiv.org/abs/2506.03817)
*Julius Gonsior,Tim Rieß,Anja Reusch,Claudio Hartmann,Maik Thiele,Wolfgang Lehner*

Main category: cs.LG

TL;DR: Annotating data is time-consuming and costly. Active Learning (AL) minimizes human labeling effort but is rarely used in real-world applications due to its complexity and lack of trust in its effectiveness. This study explores the hyperparameter space of AL, conducts experiments with over 4.6 million hyperparameter combinations, analyzes their impact, and provides recommendations for more reproducible and trustworthy AL research.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the reasons why Active Learning (AL) is rarely used in real-world applications, which are the complexity of setting AL up and a lack of trust in its effectiveness. The authors hypothesize that both reasons are due to the large hyperparameter space of AL.

Method: The method involves compiling a large hyperparameter grid of over 4.6 million hyperparameter combinations, recording the performance of all combinations in the largest conducted AL study so far, and analyzing the impact of each hyperparameter on the experiment results.

Result: The study reveals the influence of each hyperparameter, demonstrates the surprising impact of the specific AL strategy implementation, and outlines an experimental study design for reproducible AL experiments with minimal computational effort.

Conclusion: The conclusion is that by understanding and addressing the hyperparameter space, the research contributes to making AL more reproducible and trustworthy, thus potentially increasing its adoption in real-world applications.

Abstract: Annotating data is a time-consuming and costly task, but it is inherently
required for supervised machine learning. Active Learning (AL) is an
established method that minimizes human labeling effort by iteratively
selecting the most informative unlabeled samples for expert annotation, thereby
improving the overall classification performance. Even though AL has been known
for decades, AL is still rarely used in real-world applications. As indicated
in the two community web surveys among the NLP community about AL, two main
reasons continue to hold practitioners back from using AL: first, the
complexity of setting AL up, and second, a lack of trust in its effectiveness.
We hypothesize that both reasons share the same culprit: the large
hyperparameter space of AL. This mostly unexplored hyperparameter space often
leads to misleading and irreproducible AL experiment results. In this study, we
first compiled a large hyperparameter grid of over 4.6 million hyperparameter
combinations, second, recorded the performance of all combinations in the
so-far biggest conducted AL study, and third, analyzed the impact of each
hyperparameter in the experiment results. In the end, we give recommendations
about the influence of each hyperparameter, demonstrate the surprising
influence of the concrete AL strategy implementation, and outline an
experimental study design for reproducible AL experiments with minimal
computational effort, thus contributing to more reproducible and trustworthy AL
research in the future.

</details>


### [80] [Learning task-specific predictive models for scientific computing](https://arxiv.org/abs/2506.03835)
*Jianyuan Yin,Qianxiao Li*

Main category: cs.LG

TL;DR: The paper discusses a method for learning predictive models tailored to specific downstream tasks in scientific computing, showing that minimizing mean square error is insufficient and proposing an alternative approach focused on maximum prediction error.


<details>
  <summary>Details</summary>
Motivation: In machine-learning-augmented scientific computing, there is a need for predictive models that are optimized for specific downstream tasks beyond just prediction. Classical supervised learning methods, such as minimizing mean square error, may not be suitable for these tasks.

Method: The authors propose formulating a task-specific supervised learning problem based on the sampling measure of the downstream task. They focus on the maximum prediction error on the support of the downstream task algorithm as an effective estimate for task performance. An empirical risk is discretized using training data, and an iterative algorithm is developed to solve this task-specific learning problem.

Result: Three numerical examples—trajectory prediction, optimal control, and minimum energy path computation—are provided to demonstrate the effectiveness of the proposed approach in improving downstream task performance compared to traditional methods.

Conclusion: Task-specific supervised learning, focusing on maximum prediction error rather than mean square error, provides a more reliable surrogate model for downstream tasks in scientific computing.

Abstract: We consider learning a predictive model to be subsequently used for a given
downstream task (described by an algorithm) that requires access to the model
evaluation. This task need not be prediction, and this situation is frequently
encountered in machine-learning-augmented scientific computing. We show that
this setting differs from classical supervised learning, and in general it
cannot be solved by minimizing the mean square error of the model predictions
as is frequently performed in the literature. Instead, we find that the maximum
prediction error on the support of the downstream task algorithm can serve as
an effective estimate for the subsequent task performance. With this insight,
we formulate a task-specific supervised learning problem based on the given
sampling measure, whose solution serves as a reliable surrogate model for the
downstream task. Then, we discretize the empirical risk based on training data,
and develop an iterative algorithm to solve the task-specific supervised
learning problem. Three illustrative numerical examples on trajectory
prediction, optimal control and minimum energy path computation demonstrate the
effectiveness of the approach.

</details>


### [81] [Revisiting Unbiased Implicit Variational Inference](https://arxiv.org/abs/2506.03839)
*Tobias Pielok,Bernd Bischl,David Rügamer*

Main category: cs.LG

TL;DR: Revisiting Unbiased Implicit Variational Inference (UIVI), the paper proposes replacing its MCMC loop with importance sampling and learning an optimal proposal distribution by minimizing forward KL divergence, leading to performance on par or better than current SIVI methods.


<details>
  <summary>Details</summary>
Motivation: UIVI has been largely dismissed due to its imprecision and computational demands. However, there is potential in revisiting this method as it could provide a more effective alternative to current SIVI training routines.

Method: The UIVI's MCMC loop is replaced with importance sampling, and the optimal proposal distribution is learned through minimizing expected forward Kullback-Leibler divergence without introducing bias.

Result: The refined UIVI approach achieves superior performance or parity with state-of-the-art methods on established SIVI benchmarks.

Conclusion: The proposed modifications to UIVI lead to a competitive method for semi-implicit variational inference.

Abstract: Recent years have witnessed growing interest in semi-implicit variational
inference (SIVI) methods due to their ability to rapidly generate samples from
complex distributions. However, since the likelihood of these samples is
non-trivial to estimate in high dimensions, current research focuses on finding
effective SIVI training routines. Although unbiased implicit variational
inference (UIVI) has largely been dismissed as imprecise and computationally
prohibitive because of its inner MCMC loop, we revisit this method and show
that UIVI's MCMC loop can be effectively replaced via importance sampling and
the optimal proposal distribution can be learned stably by minimizing an
expected forward Kullback-Leibler divergence without bias. Our refined approach
demonstrates superior performance or parity with state-of-the-art methods on
established SIVI benchmarks.

</details>


### [82] [Vulnerability-Aware Alignment: Mitigating Uneven Forgetting in Harmful Fine-Tuning](https://arxiv.org/abs/2506.03850)
*Liang Chen,Xueting Han,Li Shen,Jing Bai,Kam-Fai Wong*

Main category: cs.LG

TL;DR: An abstract about Vulnerability-Aware Alignment (VAA) which significantly reduces harmful scores in fine-tuning tasks while preserving downstream task performance.


<details>
  <summary>Details</summary>
Motivation: Harmful fine-tuning (HFT) on open-source LLMs breaks safety alignment and poses significant threats. Existing methods treat each data sample equally, leaving data vulnerability patterns understudied.

Method: Propose Vulnerability-Aware Alignment (VAA) that estimates data vulnerability, partitions data into "vulnerable" and "invulnerable" groups, and encourages balanced learning using a group distributionally robust optimization (Group DRO) framework.

Result: Experiments across four fine-tuning tasks demonstrate that VAA significantly reduces harmful scores while preserving downstream task performance, outperforming state-of-the-art baselines.

Conclusion: VAA is an effective method to mitigate HFT risks by addressing data vulnerability patterns.

Abstract: Harmful fine-tuning (HFT), performed directly on open-source LLMs or through
Fine-tuning-as-a-Service, breaks safety alignment and poses significant
threats. Existing methods aim to mitigate HFT risks by learning robust
representation on alignment data or making harmful data unlearnable, but they
treat each data sample equally, leaving data vulnerability patterns
understudied. In this work, we reveal that certain subsets of alignment data
are consistently more prone to forgetting during HFT across different
fine-tuning tasks. Inspired by these findings, we propose Vulnerability-Aware
Alignment (VAA), which estimates data vulnerability, partitions data into
"vulnerable" and "invulnerable" groups, and encourages balanced learning using
a group distributionally robust optimization (Group DRO) framework.
Specifically, VAA learns an adversarial sampler that samples examples from the
currently underperforming group and then applies group-dependent adversarial
perturbations to the data during training, aiming to encourage a balanced
learning process across groups. Experiments across four fine-tuning tasks
demonstrate that VAA significantly reduces harmful scores while preserving
downstream task performance, outperforming state-of-the-art baselines.

</details>


### [83] [Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation](https://arxiv.org/abs/2506.03857)
*Mingxuan Xia,Haobo Wang,Yixuan Li,Zewei Yu,Jindong Wang,Junbo Zhao,Runze Wu*

Main category: cs.LG

TL;DR: This paper proposes a new annotation paradigm for LLMs to output all possible labels when uncertain, and introduces CanDist, a teacher-student framework that uses an SLM to distill these annotations. Experiments show its effectiveness in text classification tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the ambiguity aversion observed in human behavior, aiming to address the issue of incorrect labels produced by LLMs due to their inherent uncertainty, which affects data quality for downstream applications.

Method: A novel candidate annotation paradigm is proposed where LLMs output all possible labels when uncertain. A teacher-student framework named CanDist is developed, using a Small Language Model (SLM) to distill these candidate annotations into unique labels for downstream tasks.

Result: Experiments across six text classification tasks validate the effectiveness of the proposed method, showing that distilling candidate annotations provides superior theoretical guarantees compared to using single annotations directly.

Conclusion: The proposed candidate annotation paradigm and CanDist framework effectively improve data quality for downstream applications, offering a promising approach in leveraging LLMs for data annotation.

Abstract: Recently, Large Language Models (LLMs) have demonstrated significant
potential for data annotation, markedly reducing the labor costs associated
with downstream applications. However, existing methods mostly adopt an
aggressive strategy by prompting LLM to determine a single gold label for each
unlabeled sample. Due to the inherent uncertainty within LLMs, they often
produce incorrect labels for difficult samples, severely compromising the data
quality for downstream applications. Motivated by ambiguity aversion in human
behaviors, we propose a novel candidate annotation paradigm wherein large
language models are encouraged to output all possible labels when incurring
uncertainty. To ensure unique labels are provided for downstream tasks, we
develop a teacher-student framework CanDist that distills candidate annotations
with a Small Language Model (SLM). We further provide a rigorous justification
demonstrating that distilling candidate annotations from the teacher LLM offers
superior theoretical guarantees compared to directly using single annotations.
Extensive experiments across six text classification tasks validate the
effectiveness of our proposed method. The source code is available at
https://github.com/MingxuanXia/CanDist.

</details>


### [84] [Temporal horizons in forecasting: a performance-learnability trade-off](https://arxiv.org/abs/2506.03889)
*Pau Vilimelis Aceituno,Jack William Miller,Noah Marti,Youssef Farag,Victor Boussange*

Main category: cs.LG

TL;DR: 在训练自回归模型时，预测时间范围的选择存在权衡。本文通过分析损失景观与训练时间范围的关系，证明了混沌系统中长期预测的难度，并提供了优化超参数的原则基础。


<details>
  <summary>Details</summary>
Motivation: 探讨在训练自回归模型时，如何选择合适的预测时间范围以平衡短期趋势和长期预测的准确性。

Method: 通过分析损失景观与训练时间范围的关系，研究不同系统的预测难度，并进行数值实验验证理论结果。

Result: 发现混沌系统的损失景观粗糙度随训练时间范围呈指数增长，而极限环系统则线性增长；长期训练模型在短期预测上表现良好，但短期训练模型在长期预测上表现较差。

Conclusion: 提供了选择训练时间范围的理论依据，为自回归预测模型的超参数优化奠定了原则基础。

Abstract: When training autoregressive models for dynamical systems, a critical
question arises: how far into the future should the model be trained to
predict? Too short a horizon may miss long-term trends, while too long a
horizon can impede convergence due to accumulating prediction errors. In this
work, we formalize this trade-off by analyzing how the geometry of the loss
landscape depends on the training horizon. We prove that for chaotic systems,
the loss landscape's roughness grows exponentially with the training horizon,
while for limit cycles, it grows linearly, making long-horizon training
inherently challenging. However, we also show that models trained on long
horizons generalize well to short-term forecasts, whereas those trained on
short horizons suffer exponentially (resp. linearly) worse long-term
predictions in chaotic (resp. periodic) systems. We validate our theory through
numerical experiments and discuss practical implications for selecting training
horizons. Our results provide a principled foundation for hyperparameter
optimization in autoregressive forecasting models.

</details>


### [85] [A kernel conditional two-sample test](https://arxiv.org/abs/2506.03898)
*Pierre-François Massiani,Christian Fiedler,Lukas Haverbeck,Friedrich Solowjow,Sebastian Trimpe*

Main category: cs.LG

TL;DR: The paper proposes a framework for hypothesis testing on conditional probability distributions and constructs conditional two-sample statistical tests, which can identify where two conditional expectations differ with high probability.


<details>
  <summary>Details</summary>
Motivation: There is a need for statistical tests that can handle sequential or non-independent data, as well as situations where output distributions vary with operational parameters.

Method: Transform confidence bounds of a learning method into a conditional two-sample test. This principle is instantiated for kernel ridge regression (KRR) and conditional kernel mean embeddings. Generalize pointwise-in-time or time-uniform confidence bounds for KRR to previously inaccessible cases such as infinite-dimensional outputs with non-trace-class kernels.

Result: The proposed conditional two-sample tests are able to identify the inputs where two conditional expectations differ with high probability. They enable online sampling and avoid tuning inaccessible parameters through bootstrapping schemes.

Conclusion: The results establish a comprehensive foundation for conditional two-sample testing, from theoretical guarantees to practical implementation, and advance the state-of-the-art on the concentration of vector-valued least squares estimation.

Abstract: We propose a framework for hypothesis testing on conditional probability
distributions, which we then use to construct conditional two-sample
statistical tests. These tests identify the inputs -- called covariates in this
context -- where two conditional expectations differ with high probability. Our
key idea is to transform confidence bounds of a learning method into a
conditional two-sample test, and we instantiate this principle for kernel ridge
regression (KRR) and conditional kernel mean embeddings. We generalize existing
pointwise-in-time or time-uniform confidence bounds for KRR to
previously-inaccessible yet essential cases such as infinite-dimensional
outputs with non-trace-class kernels. These bounds enable circumventing the
need for independent data in our statistical tests, since they allow online
sampling. We also introduce bootstrapping schemes leveraging the parametric
form of testing thresholds identified in theory to avoid tuning inaccessible
parameters, making our method readily applicable in practice. Such conditional
two-sample tests are especially relevant in applications where data arrive
sequentially or non-independently, or when output distributions vary with
operational parameters. We demonstrate their utility through examples in
process monitoring and comparison of dynamical systems. Overall, our results
establish a comprehensive foundation for conditional two-sample testing, from
theoretical guarantees to practical implementation, and advance the
state-of-the-art on the concentration of vector-valued least squares
estimation.

</details>


### [86] [Enhancing Experimental Efficiency in Materials Design: A Comparative Study of Taguchi and Machine Learning Methods](https://arxiv.org/abs/2506.03910)
*Shyam Prabhu,P Akshay Kumar,Antov Selwinston,Pavan Taduvai,Shreya Bairi,Rohit Batra*

Main category: cs.LG

TL;DR: In materials design, optimizing multiple variables is challenging. While DOE methods like Taguchi lack the ability to capture non-linear dependencies, this paper demonstrates that ML methods (specifically GPR) can overcome these limitations and outperform Taguchi in predicting bead geometry aspects in WAAM process.


<details>
  <summary>Details</summary>
Motivation: Materials design problems require optimizing multiple variables which makes full factorial exploration impractical. Traditional DOE methods such as Taguchi are used but they cannot capture non-linear dependency of process variables.

Method: Compare Taguchi method with an active learning based Gaussian process regression (GPR) model in a wire arc additive manufacturing (WAAM) process for predicting bead geometry aspects. Taguchi uses a three-factor, five-level L25 orthogonal array while GPR uses an uncertainty-based exploration acquisition function coupled with latin hypercube sampling for initial training data.

Result: Both models were evaluated on 15 test cases. The GPR model outperformed the Taguchi method in terms of both accuracy and efficiency.

Conclusion: Machine learning methods like GPR can be effectively used to overcome the limitations of traditional DOE methods in materials processing domain requiring efficient exploration of complex parameters.

Abstract: Materials design problems often require optimizing multiple variables,
rendering full factorial exploration impractical. Design of experiment (DOE)
methods, such as Taguchi technique, are commonly used to efficiently sample the
design space but they inherently lack the ability to capture non-linear
dependency of process variables. In this work, we demonstrate how machine
learning (ML) methods can be used to overcome these limitations. We compare the
performance of Taguchi method against an active learning based Gaussian process
regression (GPR) model in a wire arc additive manufacturing (WAAM) process to
accurately predict aspects of bead geometry, including penetration depth, bead
width, and height. While Taguchi method utilized a three-factor, five-level L25
orthogonal array to suggest weld parameters, the GPR model used an
uncertainty-based exploration acquisition function coupled with latin hypercube
sampling for initial training data. Accuracy and efficiency of both models was
evaluated on 15 test cases, with GPR outperforming Taguchi in both metrics.
This work applies to broader materials processing domain requiring efficient
exploration of complex parameters.

</details>


### [87] [Learning Fair And Effective Points-Based Rewards Programs](https://arxiv.org/abs/2506.03911)
*Chamsi Hssaine,Yichun Hu,Ciara Pike-Burke*

Main category: cs.LG

TL;DR: 研究了公平设计基于积分的奖励计划的问题，提出了限制积分贬值风险的学习算法，并证明了个别公平奖励计划的收入损失有限。


<details>
  <summary>Details</summary>
Motivation: 由于基于积分的奖励计划受到不公平实践的指责，因此需要研究如何公平地设计这些计划。

Method: 首先展示了使用相同兑换门槛的个别公平奖励计划与最佳个性化策略之间的收入差异；然后设计了一个学习算法，通过仅更改O(log T)次兑换门槛来限制积分贬值风险，并在期望中实现最优后悔值；最后修改算法以仅减少兑换门槛，从而提高公平性。

Result: 理论上证明了个别公平奖励计划的收入损失有限，并且所提出的学习算法在实践中表现出色。

Conclusion: 尽管个性化可能在平均情况下价值有限，但所提出的算法在实践中表现良好，可以改善公平性。

Abstract: Points-based rewards programs are a prevalent way to incentivize customer
loyalty; in these programs, customers who make repeated purchases from a seller
accumulate points, working toward eventual redemption of a free reward. These
programs have recently come under scrutiny due to accusations of unfair
practices in their implementation. Motivated by these concerns, we study the
problem of fairly designing points-based rewards programs, with a focus on two
obstacles that put fairness at odds with their effectiveness. First, due to
customer heterogeneity, the seller should set different redemption thresholds
for different customers to generate high revenue. Second, the relationship
between customer behavior and the number of accumulated points is typically
unknown; this requires experimentation which may unfairly devalue customers'
previously earned points. We first show that an individually fair rewards
program that uses the same redemption threshold for all customers suffers a
loss in revenue of at most a factor of $1+\ln 2$, compared to the optimal
personalized strategy that differentiates between customers. We then tackle the
problem of designing temporally fair learning algorithms in the presence of
demand uncertainty. Toward this goal, we design a learning algorithm that
limits the risk of point devaluation due to experimentation by only changing
the redemption threshold $O(\log T)$ times, over a horizon of length $T$. This
algorithm achieves the optimal (up to polylogarithmic factors)
$\widetilde{O}(\sqrt{T})$ regret in expectation. We then modify this algorithm
to only ever decrease redemption thresholds, leading to improved fairness at a
cost of only a constant factor in regret. Extensive numerical experiments show
the limited value of personalization in average-case settings, in addition to
demonstrating the strong practical performance of our proposed learning
algorithms.

</details>


### [88] [Learning equivariant models by discovering symmetries with learnable augmentations](https://arxiv.org/abs/2506.03914)
*Eduardo Santos Escriche,Stefanie Jegelka*

Main category: cs.LG

TL;DR: An end-to-end approach named SEMoLA is proposed to discover unknown symmetries in data via learnable data augmentations and softly encode the approximate equivariance into an unconstrained model, achieving high prediction accuracy across various datasets without needing prior knowledge about symmetries.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing methods for learning relevant symmetries from data in geometric domains. Soft equivariance requires a priori knowledge about relevant symmetries while inferring symmetries merely via the task and larger data lacks interpretability.

Method: SEMoLA is an end-to-end approach that jointly discovers a priori unknown symmetries in the data via learnable data augmentations and softly encodes the respective approximate equivariance into an arbitrary unconstrained model.

Result: Empirically, SEMoLA robustly discovers relevant symmetries while achieving high prediction accuracy across various datasets encompassing multiple data modalities and underlying symmetry groups.

Conclusion: SEMoLA does not need prior knowledge about symmetries, offers interpretability, and maintains robustness to distribution shifts.

Abstract: Recently, a trend has emerged that favors learning relevant symmetries from
data in geometric domains instead of designing constrained architectures. To do
so, two popular options are (1) to modify the training protocol, e.g., with a
specific loss and data augmentations (soft equivariance), or (2) to ignore
equivariance and infer it only implicitly. However, both options have
limitations: soft equivariance requires a priori knowledge about relevant
symmetries, while inferring symmetries merely via the task and larger data
lacks interpretability. To address both limitations, we propose SEMoLA, an
end-to-end approach that jointly (1) discovers a priori unknown symmetries in
the data via learnable data augmentations, and (2) softly encodes the
respective approximate equivariance into an arbitrary unconstrained model.
Hence, it does not need prior knowledge about symmetries, it offers
interpretability, and it maintains robustness to distribution shifts.
Empirically, we demonstrate the ability of SEMoLA to robustly discover relevant
symmetries while achieving high prediction accuracy across various datasets,
encompassing multiple data modalities and underlying symmetry groups.

</details>


### [89] [Weisfeiler and Leman Go Gambling: Why Expressive Lottery Tickets Win](https://arxiv.org/abs/2506.03919)
*Lorenz Kummer,Samir Moustafa,Anatol Ehrlich,Franka Bause,Nikolaus Suess,Wilfried N. Gansterer,Nils M. Kriege*

Main category: cs.LG

TL;DR: This paper explores the lottery ticket hypothesis in graph neural networks (GNNs), focusing on the expressivity of sparse subnetworks and their ability to distinguish non-isomorphic graphs. It establishes conditions under which sparse GNNs can match the expressivity of full networks, proving a Strong Expressive Lottery Ticket Hypothesis, and shows that enhanced expressivity during initialization may accelerate convergence and improve generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of theoretical understanding of the lottery ticket hypothesis in graph neural networks (GNNs) compared to convolutional neural networks. Specifically, it aims to understand how sparse subnetworks within GNNs can preserve predictive performance by maintaining the ability to distinguish non-isomorphic graphs.

Method: The authors identify the expressivity of sparse subnetworks as key to finding winning tickets in GNNs. They establish conditions under which sparsely initialized GNNs can achieve the same level of expressivity as fully initialized ones, comparing this to the Weisfeiler-Leman test. They also propose and prove a Strong Expressive Lottery Ticket Hypothesis for GNNs.

Result: The study finds that increased expressivity during initialization leads to faster model convergence and better generalization capabilities. Theoretical foundations are provided for both the lottery ticket hypothesis and GNN research, emphasizing the importance of expressivity preservation in sparse GNNs.

Conclusion: The conclusion highlights the significance of maintaining expressivity in sparsely initialized GNNs for achieving strong predictive performance. This work provides new theoretical insights into both the lottery ticket hypothesis and GNNs, with practical implications demonstrated through examples from drug discovery.

Abstract: The lottery ticket hypothesis (LTH) is well-studied for convolutional neural
networks but has been validated only empirically for graph neural networks
(GNNs), for which theoretical findings are largely lacking. In this paper, we
identify the expressivity of sparse subnetworks, i.e. their ability to
distinguish non-isomorphic graphs, as crucial for finding winning tickets that
preserve the predictive performance. We establish conditions under which the
expressivity of a sparsely initialized GNN matches that of the full network,
particularly when compared to the Weisfeiler-Leman test, and in that context
put forward and prove a Strong Expressive Lottery Ticket Hypothesis. We
subsequently show that an increased expressivity in the initialization
potentially accelerates model convergence and improves generalization. Our
findings establish novel theoretical foundations for both LTH and GNN research,
highlighting the importance of maintaining expressivity in sparsely initialized
GNNs. We illustrate our results using examples from drug discovery.

</details>


### [90] [Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study](https://arxiv.org/abs/2506.03931)
*Yotam Alexander,Yonatan Slutzky,Yuval Ran-Milo,Nadav Cohen*

Main category: cs.LG

TL;DR: In this paper, the authors investigate the volume hypothesis for matrix factorization and find that generalization under Guess & Check (G&C) deteriorates with increasing width but improves with increasing depth. This suggests that there may not be a simple answer to whether neural networks need gradient descent to generalize well.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to explore the validity of the volume hypothesis for wide and deep neural networks by theoretically investigating matrix factorization.

Method: The method used in this paper is theoretical investigation of the volume hypothesis for matrix factorization with linear and non-linear activation. The authors prove the effects of width and depth on generalization under G&C.

Result: The results show that generalization under G&C deteriorates with increasing width and improves with increasing depth.

Conclusion: The conclusion is that there may not be a simple answer to whether neural networks need gradient descent to generalize well.

Abstract: Conventional wisdom attributes the mysterious generalization abilities of
overparameterized neural networks to gradient descent (and its variants). The
recent volume hypothesis challenges this view: it posits that these
generalization abilities persist even when gradient descent is replaced by
Guess & Check (G&C), i.e., by drawing weight settings until one that fits the
training data is found. The validity of the volume hypothesis for wide and deep
neural networks remains an open question. In this paper, we theoretically
investigate this question for matrix factorization (with linear and non-linear
activation)--a common testbed in neural network theory. We first prove that
generalization under G&C deteriorates with increasing width, establishing what
is, to our knowledge, the first case where G&C is provably inferior to gradient
descent. Conversely, we prove that generalization under G&C improves with
increasing depth, revealing a stark contrast between wide and deep networks,
which we further validate empirically. These findings suggest that even in
simple settings, there may not be a simple answer to the question of whether
neural networks need gradient descent to generalize well.

</details>


### [91] [FPGA-Enabled Machine Learning Applications in Earth Observation: A Systematic Review](https://arxiv.org/abs/2506.03938)
*Cédric Léonard,Dirk Stober,Martin Schulz*

Main category: cs.LG

TL;DR: This paper reviews the deployment of ML models on FPGAs for remote sensing applications in the context of NewSpace era and UAV technologies, providing two taxonomies and following PRISMA 2020 guidelines.


<details>
  <summary>Details</summary>
Motivation: Earth Observation missions are producing large data volumes due to advancements in UAV technologies and the NewSpace era. This creates a need for onboard decision-making to efficiently transmit high-quality information within bandwidth constraints.

Method: The authors conducted a systematic analysis of 66 experiments that deployed ML models on FPGAs for Remote Sensing applications. They introduced two taxonomies to categorize efficient model architectures and FPGA implementation strategies.

Result: The review provides insights into the use of ML models on FPGAs for remote sensing, highlighting effective model architectures and FPGA implementation strategies while promoting transparency and reproducibility.

Conclusion: FPGAs offer a promising solution for real-time autonomous processing in Earth Observation missions by balancing performance with adaptability to mission-specific requirements.

Abstract: New UAV technologies and the NewSpace era are transforming Earth Observation
missions and data acquisition. Numerous small platforms generate large data
volume, straining bandwidth and requiring onboard decision-making to transmit
high-quality information in time. While Machine Learning allows real-time
autonomous processing, FPGAs balance performance with adaptability to
mission-specific requirements, enabling onboard deployment. This review
systematically analyzes 66 experiments deploying ML models on FPGAs for Remote
Sensing applications. We introduce two distinct taxonomies to capture both
efficient model architectures and FPGA implementation strategies. For
transparency and reproducibility, we follow PRISMA 2020 guidelines and share
all data and code at https://github.com/CedricLeon/Survey_RS-ML-FPGA.

</details>


### [92] [Lower Ricci Curvature for Hypergraphs](https://arxiv.org/abs/2506.03943)
*Shiyi Yang,Can Chen,Didong Li*

Main category: cs.LG

TL;DR: 研究提出了一种新的超图下界Ricci曲率（HLRC）指标，以平衡可解释性和效率，揭示了有意义的高阶组织结构，并为超图分析提供了通用基础。


<details>
  <summary>Details</summary>
Motivation: 现有的超图曲率方法要么只能捕捉粗略特征（如Forman-Ricci曲率），要么计算成本过高（如Ollivier-Ricci曲率）。因此，需要一种新的方法来在可解释性和效率之间取得平衡。

Method: 引入了超图下界Ricci曲率（HLRC），这是一种以封闭形式定义的新曲率度量，能够在可解释性和效率之间取得原则性的平衡。

Result: 通过在多样化的合成和真实世界超图数据集上进行评估，HLRC能够揭示有意义的高阶组织结构，区分社区内和社区间的超边，发现潜在的语义标签，跟踪时间动态，并支持基于全局结构的鲁棒超图聚类。

Conclusion: HLRC通过将几何敏感性与算法简单性统一起来，为超图分析提供了一个多功能的基础，对节点分类、异常检测和复杂系统中的生成建模等任务具有广泛的影响。

Abstract: Networks with higher-order interactions, prevalent in biological, social, and
information systems, are naturally represented as hypergraphs, yet their
structural complexity poses fundamental challenges for geometric
characterization. While curvature-based methods offer powerful insights in
graph analysis, existing extensions to hypergraphs suffer from critical
trade-offs: combinatorial approaches such as Forman-Ricci curvature capture
only coarse features, whereas geometric methods like Ollivier-Ricci curvature
offer richer expressivity but demand costly optimal transport computations. To
address these challenges, we introduce hypergraph lower Ricci curvature (HLRC),
a novel curvature metric defined in closed form that achieves a principled
balance between interpretability and efficiency. Evaluated across diverse
synthetic and real-world hypergraph datasets, HLRC consistently reveals
meaningful higher-order organization, distinguishing intra- from
inter-community hyperedges, uncovering latent semantic labels, tracking
temporal dynamics, and supporting robust clustering of hypergraphs based on
global structure. By unifying geometric sensitivity with algorithmic
simplicity, HLRC provides a versatile foundation for hypergraph analytics, with
broad implications for tasks including node classification, anomaly detection,
and generative modeling in complex systems.

</details>


### [93] [Rethinking the Stability-Plasticity Trade-off in Continual Learning from an Architectural Perspective](https://arxiv.org/abs/2506.03951)
*Aojun Lu,Hangjie Yuan,Tao Feng,Yanan Sun*

Main category: cs.LG

TL;DR: The paper introduces Dual-Arch, a novel framework for Continual Learning (CL) that resolves the stability-plasticity dilemma at the architectural level by using two specialized networks, one for plasticity and one for stability. Experiments show it improves CL methods' performance and is more parameter-efficient.


<details>
  <summary>Details</summary>
Motivation: Continual Learning (CL) aims to allow neural networks to learn incrementally, but existing methods focus on the parameter level trade-off between preserving old knowledge and acquiring new knowledge while neglecting the architectural impact on this balance.

Method: The authors analyze the stability-plasticity conflict at the architectural level and find deeper networks have better plasticity and wider networks offer superior stability under equal parameter constraints. They propose Dual-Arch, a framework consisting of two independent networks - one optimized for plasticity and the other for stability.

Result: Dual-Arch significantly boosts the performance of existing CL methods and achieves up to 87% reduction in parameters compared to traditional approaches.

Conclusion: Dual-Arch effectively addresses the stability-plasticity dilemma at the architectural level in CL, providing a compact yet powerful enhancement to current CL techniques.

Abstract: The quest for Continual Learning (CL) seeks to empower neural networks with
the ability to learn and adapt incrementally. Central to this pursuit is
addressing the stability-plasticity dilemma, which involves striking a balance
between two conflicting objectives: preserving previously learned knowledge and
acquiring new knowledge. While numerous CL methods aim to achieve this
trade-off, they often overlook the impact of network architecture on stability
and plasticity, restricting the trade-off to the parameter level. In this
paper, we delve into the conflict between stability and plasticity at the
architectural level. We reveal that under an equal parameter constraint, deeper
networks exhibit better plasticity, while wider networks are characterized by
superior stability. To address this architectural-level dilemma, we introduce a
novel framework denoted Dual-Arch, which serves as a plug-in component for CL.
This framework leverages the complementary strengths of two distinct and
independent networks: one dedicated to plasticity and the other to stability.
Each network is designed with a specialized and lightweight architecture,
tailored to its respective objective. Extensive experiments demonstrate that
Dual-Arch enhances the performance of existing CL methods while being up to 87%
more compact in terms of parameters.

</details>


### [94] [HtFLlib: A Comprehensive Heterogeneous Federated Learning Library and Benchmark](https://arxiv.org/abs/2506.03954)
*Jianqing Zhang,Xinghao Wu,Yanbing Zhou,Xiaoting Sun,Qiqi Cai,Yang Liu,Yang Hua,Zhenzhe Zheng,Jian Cao,Qiang Yang*

Main category: cs.LG

TL;DR: The paper introduces HtFLlib, a new library for heterogeneous federated learning that integrates multiple datasets and model architectures to offer a robust benchmark for research and applications.


<details>
  <summary>Details</summary>
Motivation: Traditional Federated Learning only supports homogeneous models which limits collaboration among clients with different architectures. There is a lack of comprehensive benchmarks for evaluating Heterogeneous Federated Learning (HtFL) methods.

Method: The authors developed HtFLlib which includes 12 datasets, 40 model architectures, a modularized codebase with implementations of 10 HtFL methods, and systematic evaluations on accuracy, convergence, computation and communication costs.

Result: HtFLlib provides a standardized evaluation framework for HtFL methods, demonstrating the effectiveness and robustness of state-of-the-art HtFL approaches in various scenarios.

Conclusion: HtFLlib aims to advance HtFL research and enable broader applications by providing an easy-to-use and extensible framework.

Abstract: As AI evolves, collaboration among heterogeneous models helps overcome data
scarcity by enabling knowledge transfer across institutions and devices.
Traditional Federated Learning (FL) only supports homogeneous models, limiting
collaboration among clients with heterogeneous model architectures. To address
this, Heterogeneous Federated Learning (HtFL) methods are developed to enable
collaboration across diverse heterogeneous models while tackling the data
heterogeneity issue at the same time. However, a comprehensive benchmark for
standardized evaluation and analysis of the rapidly growing HtFL methods is
lacking. Firstly, the highly varied datasets, model heterogeneity scenarios,
and different method implementations become hurdles to making easy and fair
comparisons among HtFL methods. Secondly, the effectiveness and robustness of
HtFL methods are under-explored in various scenarios, such as the medical
domain and sensor signal modality. To fill this gap, we introduce the first
Heterogeneous Federated Learning Library (HtFLlib), an easy-to-use and
extensible framework that integrates multiple datasets and model heterogeneity
scenarios, offering a robust benchmark for research and practical applications.
Specifically, HtFLlib integrates (1) 12 datasets spanning various domains,
modalities, and data heterogeneity scenarios; (2) 40 model architectures,
ranging from small to large, across three modalities; (3) a modularized and
easy-to-extend HtFL codebase with implementations of 10 representative HtFL
methods; and (4) systematic evaluations in terms of accuracy, convergence,
computation costs, and communication costs. We emphasize the advantages and
potential of state-of-the-art HtFL methods and hope that HtFLlib will catalyze
advancing HtFL research and enable its broader applications. The code is
released at https://github.com/TsingZ0/HtFLlib.

</details>


### [95] [Adapt before Continual Learning](https://arxiv.org/abs/2506.03956)
*Aojun Lu,Tao Feng,Hangjie Yuan,Chunhui Ding,Yanan Sun*

Main category: cs.LG

TL;DR: Continual Learning (CL) aims to let neural networks gain new knowledge while retaining old knowledge. Current approaches either freeze pre-trained models (PTMs) to retain stability, limiting plasticity, or fine-tune the entire PTM risking catastrophic forgetting. This paper proposes Adapting PTMs before the core CL process (ACL), which refines the PTM backbone in a plug-and-play adaptation phase before learning each new task, thereby enhancing plasticity and balancing stability-plasticity.


<details>
  <summary>Details</summary>
Motivation: To address the critical stability-plasticity trade-off in Continual Learning when using pre-trained models.

Method: Propose ACL, a framework that adapts PTMs by refining their backbone through an alignment-distancing approach before applying existing CL methods.

Result: Extensive experiments show ACL significantly improves CL performance across benchmarks and integrated methods, providing a versatile solution for PTM-based CL.

Conclusion: ACL is a novel framework that effectively balances stability and plasticity in CL, offering significant improvements in performance.

Abstract: Continual Learning (CL) seeks to enable neural networks to incrementally
acquire new knowledge (plasticity) while retaining existing knowledge
(stability). While pre-trained models (PTMs) have become pivotal in CL,
prevailing approaches freeze the PTM backbone to preserve stability, limiting
their plasticity, particularly when encountering significant domain gaps in
incremental tasks. Conversely, sequentially finetuning the entire PTM risks
catastrophic forgetting of generalizable knowledge, exposing a critical
stability-plasticity trade-off. To address this challenge, we propose Adapting
PTMs before the core CL process (ACL), a novel framework that refines the PTM
backbone through a plug-and-play adaptation phase before learning each new task
with existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by
aligning embeddings with their original class prototypes while distancing them
from others, theoretically and empirically shown to balance stability and
plasticity. Extensive experiments demonstrate that ACL significantly improves
CL performance across benchmarks and integrated methods, offering a versatile
solution for PTM-based CL.

</details>


### [96] [Causality-Aware Contrastive Learning for Robust Multivariate Time-Series Anomaly Detection](https://arxiv.org/abs/2506.03964)
*HyunGi Kim,Jisoo Mok,Dongjun Lee,Jaihyun Lew,Sungjae Kim,Sungroh Yoon*

Main category: cs.LG

TL;DR: This paper presents CAROTS, a new method for multivariate time-series anomaly detection that uses causality-aware contrastive learning to distinguish between normal and abnormal samples.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multivariate time-series anomaly detection (MTSAD) do not fully utilize the complex inter-variable causal relationships within the data. This represents an opportunity to improve robustness and reliability of MTSAD by incorporating causality into the learning process.

Method: CAROTS uses two data augmentors to create causality-preserving and causality-disturbing samples. It then performs contrastive learning with these samples as positives and negatives respectively, training an encoder whose latent space can separate normal from abnormal samples based on causality. Additionally, it introduces a similarity-filtered one-class contrastive loss to encourage semantic diversity in the learning process.

Result: Extensive experiments on seven datasets (five real-world and two synthetic) demonstrate that integrating causal relationships improves the anomaly detection capabilities of CAROTS.

Conclusion: The proposed CAROTS pipeline successfully incorporates causality into contrastive learning for robust multivariate time-series anomaly detection, showing improved performance over existing methods.

Abstract: Utilizing the complex inter-variable causal relationships within multivariate
time-series provides a promising avenue toward more robust and reliable
multivariate time-series anomaly detection (MTSAD) but remains an underexplored
area of research. This paper proposes Causality-Aware contrastive learning for
RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that
incorporates the notion of causality into contrastive learning. CAROTS employs
two data augmentors to obtain causality-preserving and -disturbing samples that
serve as a wide range of normal variations and synthetic anomalies,
respectively. With causality-preserving and -disturbing samples as positives
and negatives, CAROTS performs contrastive learning to train an encoder whose
latent space separates normal and abnormal samples based on causality.
Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss
that encourages the contrastive learning process to gradually incorporate more
semantically diverse samples with common causal relationships. Extensive
experiments on five real-world and two synthetic datasets validate that the
integration of causal relationships endows CAROTS with improved MTSAD
capabilities. The code is available at https://github.com/kimanki/CAROTS.

</details>


### [97] [Solving Inverse Problems via Diffusion-Based Priors: An Approximation-Free Ensemble Sampling Approach](https://arxiv.org/abs/2506.03979)
*Haoxuan Chen,Yinuo Ren,Martin Renqiang Min,Lexing Ying,Zachary Izzo*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型（DMs）的集合算法，用于贝叶斯逆问题（BIPs）的后验采样，避免了启发式近似，通过修改偏微分方程（PDE）来模拟后验分布的演化，并证明了该方法在成像逆问题中的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的后验采样方法依赖于生成过程的启发式近似，这可能影响结果的准确性。为了充分利用扩散模型的生成能力并避免使用这些近似，研究者提出了一种新的方法。

Method: 结合扩散模型与序贯蒙特卡洛（SMC）方法，推导出一个修改后的偏微分方程（PDE），包括修改的扩散项和重加权项，利用随机加权粒子方法进行模拟。

Result: 理论上证明了真实后验分布误差可以由预训练得分函数的训练误差和集合中粒子数量决定；实验上，在多个成像逆问题中验证了该算法，表明其比现有方法提供更准确的重建。

Conclusion: 提出的算法能够在不使用启发式近似的情况下有效地执行后验采样，适用于复杂的贝叶斯逆问题，并在实验中表现出更高的准确性。

Abstract: Diffusion models (DMs) have proven to be effective in modeling
high-dimensional distributions, leading to their widespread adoption for
representing complex priors in Bayesian inverse problems (BIPs). However,
current DM-based posterior sampling methods proposed for solving common BIPs
rely on heuristic approximations to the generative process. To exploit the
generative capability of DMs and avoid the usage of such approximations, we
propose an ensemble-based algorithm that performs posterior sampling without
the use of heuristic approximations. Our algorithm is motivated by existing
works that combine DM-based methods with the sequential Monte Carlo (SMC)
method. By examining how the prior evolves through the diffusion process
encoded by the pre-trained score function, we derive a modified partial
differential equation (PDE) governing the evolution of the corresponding
posterior distribution. This PDE includes a modified diffusion term and a
reweighting term, which can be simulated via stochastic weighted particle
methods. Theoretically, we prove that the error between the true posterior
distribution can be bounded in terms of the training error of the pre-trained
score function and the number of particles in the ensemble. Empirically, we
validate our algorithm on several inverse problems in imaging to show that our
method gives more accurate reconstructions compared to existing DM-based
methods.

</details>


### [98] [Optimal Spiking Brain Compression: Improving One-Shot Post-Training Pruning and Quantization for Spiking Neural Networks](https://arxiv.org/abs/2506.03996)
*Lianfeng Shi,Ao Li,Benjamin Ward-Cherrier*

Main category: cs.LG

TL;DR: Optimal Spiking Brain Compression (OSBC) is a one-shot post-training pruning/quantization framework for Spiking Neural Networks (SNNs). It minimizes the loss on spiking neuron membrane potential with a small sample dataset, achieving 97% sparsity or 4-bit symmetric quantization with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: As neuromorphic hardware has limited memory and computing resources, weight pruning and quantization have been explored to improve SNNs' efficiency. However, state-of-the-art methods employ multiple compression and training iterations, increasing the cost for pre-trained or very large SNNs.

Method: OSBC adapts the Optimal Brain Compression (OBC) method for SNNs by minimizing the loss on spiking neuron membrane potential rather than neuron input current as OBC does.

Result: Experiments on neuromorphic datasets demonstrate that OSBC can achieve 97% sparsity through pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric quantization with 0.17%, 1.54%, and 7.71% accuracy loss respectively.

Conclusion: OSBC provides an efficient one-shot post-training pruning/quantization solution for SNNs, significantly reducing computational overhead while maintaining high performance.

Abstract: Spiking Neural Networks (SNNs) have emerged as a new generation of
energy-efficient neural networks suitable for implementation on neuromorphic
hardware. As neuromorphic hardware has limited memory and computing resources,
weight pruning and quantization have recently been explored to improve SNNs'
efficiency. State-of-the-art SNN pruning/quantization methods employ multiple
compression and training iterations, increasing the cost for pre-trained or
very large SNNs. In this paper, we propose a new one-shot post-training
pruning/quantization framework, Optimal Spiking Brain Compression (OSBC), that
adapts the Optimal Brain Compression (OBC) method of [Frantar, Singh, and
Alistarh, 2023] for SNNs. Rather than minimizing the loss on neuron input
current as OBC does, OSBC achieves more efficient and accurate SNN compression
in one pass by minimizing the loss on spiking neuron membrane potential with a
small sample dataset. Our experiments on neuromorphic datasets (N-MNIST,
CIFAR10-DVS, DVS128-Gesture) demonstrate that OSBC can achieve 97% sparsity
through pruning with 1.41%, 10.20%, and 1.74% accuracy loss, or 4-bit symmetric
quantization with 0.17%, 1.54%, and 7.71% accuracy loss, respectively. Code
will be available on GitHub.

</details>


### [99] [CARL: Causality-guided Architecture Representation Learning for an Interpretable Performance Predictor](https://arxiv.org/abs/2506.04001)
*Han Ji,Yuqi Feng,Jiahao Fan,Yanan Sun*

Main category: cs.LG

TL;DR: The paper proposes CARL, a method to improve generalizable architecture performance prediction by separating critical and redundant features of architectures.


<details>
  <summary>Details</summary>
Motivation: Performance predictors for neural architecture search often learn spurious correlations due to distribution shifts between training and test samples.

Method: CARL uses a substructure extractor to split input architecture into critical and redundant substructures in the latent space, then generates multiple interventional samples by pairing critical representations with diverse redundant representations.

Result: Extensive experiments on five NAS search spaces show state-of-the-art accuracy and superior interpretability of CARL. For example, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.

Conclusion: CARL is an effective method for generalizable architecture performance prediction.

Abstract: Performance predictors have emerged as a promising method to accelerate the
evaluation stage of neural architecture search (NAS). These predictors estimate
the performance of unseen architectures by learning from the correlation
between a small set of trained architectures and their performance. However,
most existing predictors ignore the inherent distribution shift between limited
training samples and diverse test samples. Hence, they tend to learn spurious
correlations as shortcuts to predictions, leading to poor generalization. To
address this, we propose a Causality-guided Architecture Representation
Learning (CARL) method aiming to separate critical (causal) and redundant
(non-causal) features of architectures for generalizable architecture
performance prediction. Specifically, we employ a substructure extractor to
split the input architecture into critical and redundant substructures in the
latent space. Then, we generate multiple interventional samples by pairing
critical representations with diverse redundant representations to prioritize
critical features. Extensive experiments on five NAS search spaces demonstrate
the state-of-the-art accuracy and superior interpretability of CARL. For
instance, CARL achieves 97.67% top-1 accuracy on CIFAR-10 using DARTS.

</details>


### [100] [On the Usage of Gaussian Process for Efficient Data Valuation](https://arxiv.org/abs/2506.04026)
*Clément Bénesse,Patrick Mesana,Athénaïs Gautier,Sébastien Gambs*

Main category: cs.LG

TL;DR: The paper presents a new canonical decomposition for data valuation in machine learning, combining a utility function and an aggregation procedure, with Gaussian Processes used to estimate utility on sub-models.


<details>
  <summary>Details</summary>
Motivation: Data valuation is crucial in machine learning to understand the impact of individual data points on model training.

Method: A novel canonical decomposition is introduced, consisting of a utility function capturing model characteristics and an aggregation procedure. Gaussian Processes are utilized to estimate the utility function on sub-models.

Result: This approach is theoretically grounded in Bayesian theory and allows for fast estimation of valuations using efficient update formulae.

Conclusion: The proposed method provides a practical and theoretically sound way to analyze data valuation methods.

Abstract: In machine learning, knowing the impact of a given datum on model training is
a fundamental task referred to as Data Valuation. Building on previous works
from the literature, we have designed a novel canonical decomposition allowing
practitioners to analyze any data valuation method as the combination of two
parts: a utility function that captures characteristics from a given model and
an aggregation procedure that merges such information. We also propose to use
Gaussian Processes as a means to easily access the utility function on
``sub-models'', which are models trained on a subset of the training set. The
strength of our approach stems from both its theoretical grounding in Bayesian
theory, and its practical reach, by enabling fast estimation of valuations
thanks to efficient update formulae.

</details>


### [101] [Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence](https://arxiv.org/abs/2506.04053)
*Alexander Semenenko,Ivan Butakov,Alexey Frolov,Ivan Oseledets*

Main category: cs.LG

TL;DR: Sliced Mutual Information (SMI) is a scalable alternative to mutual information but has issues such as susceptibility to data manipulation, saturation, failure in detecting statistical dependence increase, prioritizing redundancy and sometimes underperforming compared to simpler measures.


<details>
  <summary>Details</summary>
Motivation: To explore the limitations and counterintuitive behaviors of SMI despite its advantages in convergence speed, handling high dimensionality and being null only under statistical independence.

Method: Through extensive benchmarking and theoretical analysis.

Result: SMI saturates easily, fails to detect increases in statistical dependence, even under linear transformations designed to enhance information extraction, prioritizes redundancy over informative content, and in some cases performs worse than simpler dependence measures like the correlation coefficient.

Conclusion: SMI has significant limitations and counterintuitive behaviors that should be considered when using it as a measure of non-linear statistical dependence.

Abstract: Sliced Mutual Information (SMI) is widely used as a scalable alternative to
mutual information for measuring non-linear statistical dependence. Despite its
advantages, such as faster convergence, robustness to high dimensionality, and
nullification only under statistical independence, we demonstrate that SMI is
highly susceptible to data manipulation and exhibits counterintuitive behavior.
Through extensive benchmarking and theoretical analysis, we show that SMI
saturates easily, fails to detect increases in statistical dependence (even
under linear transformations designed to enhance the extraction of
information), prioritizes redundancy over informative content, and in some
cases, performs worse than simpler dependence measures like the correlation
coefficient.

</details>


### [102] [Optimal Transport-based Domain Alignment as a Preprocessing Step for Federated Learning](https://arxiv.org/abs/2506.04071)
*Luiz Manella Pereira,M. Hadi Amini*

Main category: cs.LG

TL;DR: The paper proposes an Optimal Transport-based preprocessing algorithm in Federated Learning to address dataset imbalance by minimizing distributional discrepancy using Wasserstein barycenters, enhancing global model aggregation performance and generalization.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges due to dataset imbalance which leads to deterioration in global model aggregation performance, local model quality, and decision accuracy.

Method: The method involves using Optimal Transport theory to align datasets through the computation of channel-wise averages via Wasserstein barycenters. These barycenters are used in a central server to generate a target RGB space for projecting datasets, thereby minimizing distributional discrepancies globally.

Result: The approach is tested on the CIFAR-10 dataset and shows better generalization with fewer communication rounds compared to existing methods.

Conclusion: The proposed algorithm effectively reduces the impact of dataset imbalance in federated learning, leading to improved model performance and efficiency.

Abstract: Federated learning (FL) is a subfield of machine learning that avoids sharing
local data with a central server, which can enhance privacy and scalability.
The inability to consolidate data leads to a unique problem called dataset
imbalance, where agents in a network do not have equal representation of the
labels one is trying to learn to predict. In FL, fusing locally-trained models
with unbalanced datasets may deteriorate the performance of global model
aggregation, and reduce the quality of updated local models and the accuracy of
the distributed agents' decisions. In this work, we introduce an Optimal
Transport-based preprocessing algorithm that aligns the datasets by minimizing
the distributional discrepancy of data along the edge devices. We accomplish
this by leveraging Wasserstein barycenters when computing channel-wise
averages. These barycenters are collected in a trusted central server where
they collectively generate a target RGB space. By projecting our dataset
towards this target space, we minimize the distributional discrepancy on a
global level, which facilitates the learning process due to a minimization of
variance across the samples. We demonstrate the capabilities of the proposed
approach over the CIFAR-10 dataset, where we show its capability of reaching
higher degrees of generalization in fewer communication rounds.

</details>


### [103] [Multimodal Tabular Reasoning with Privileged Structured Information](https://arxiv.org/abs/2506.04088)
*Jun-Peng Jiang,Yu Xia,Hai-Long Sun,Shiyin Lu,Qing-Guo Chen,Weihua Luo,Kaifu Zhang,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.LG

TL;DR: The paper introduces Turbo, a framework for multimodal tabular reasoning that uses privileged structured tables to enhance MLLMs' ability to reason with table images. It includes a structure-aware reasoning trace generator and achieves SOTA performance (+7.2% vs previous SOTA) with limited data.


<details>
  <summary>Details</summary>
Motivation: Tabular reasoning is crucial but challenging when tables are in image form rather than structured data. High-quality textual representations used by LLMs for reasoning over structured tables are often unavailable in real-world settings.

Method: The framework Turbo leverages privileged structured information during training to enhance MLLMs. It features a structure-aware reasoning trace generator based on DeepSeek-R1, which creates high-quality modality-bridged data. Turbo then generates and selects advantageous reasoning paths to improve tabular reasoning abilities.

Result: With only 9k data, Turbo achieves state-of-the-art performance across multiple datasets, improving by +7.2% compared to the previous best method.

Conclusion: Turbo effectively addresses the challenges of aligning structured information with visual representations and transferring structured reasoning skills to MLLMs, making it a significant advancement in tabular reasoning from table images.

Abstract: Tabular reasoning involves multi-step information extraction and logical
inference over tabular data. While recent advances have leveraged large
language models (LLMs) for reasoning over structured tables, such high-quality
textual representations are often unavailable in real-world settings, where
tables typically appear as images. In this paper, we tackle the task of tabular
reasoning from table images, leveraging privileged structured information
available during training to enhance multimodal large language models (MLLMs).
The key challenges lie in the complexity of accurately aligning structured
information with visual representations, and in effectively transferring
structured reasoning skills to MLLMs despite the input modality gap. To address
these, we introduce TabUlar Reasoning with Bridged infOrmation ({\sc Turbo}), a
new framework for multimodal tabular reasoning with privileged structured
tables. {\sc Turbo} benefits from a structure-aware reasoning trace generator
based on DeepSeek-R1, contributing to high-quality modality-bridged data. On
this basis, {\sc Turbo} repeatedly generates and selects the advantageous
reasoning paths, further enhancing the model's tabular reasoning ability.
Experimental results demonstrate that, with limited ($9$k) data, {\sc Turbo}
achieves state-of-the-art performance ($+7.2\%$ vs. previous SOTA) across
multiple datasets.

</details>


### [104] [AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment](https://arxiv.org/abs/2506.04089)
*Anastasiia Ivanova,Eva Bakaeva,Zoya Volovikova,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: The paper introduces AmbiK, a textual dataset of 1000 ambiguous instruction pairs for robots in kitchen settings, to facilitate unified comparison of ambiguity detection methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of handling ambiguous instructions in real-world environments and provide a universal benchmark for comparing ambiguity detection methods.

Method: Proposed AmbiK dataset includes 1000 pairs of ambiguous tasks with their unambiguous counterparts, categorized by ambiguity type (Human Preferences, Common Sense Knowledge, Safety), along with environment descriptions, clarifying questions and answers, user intents, and task plans.

Result: Creation of a human-validated dataset called AmbiK that comprises 2000 tasks in total, assisting researchers in performing unified comparisons of ambiguity detection methods.

Conclusion: AmbiK provides a necessary benchmark for evaluating ambiguity detection methods, and it is publicly available for research use.

Abstract: As a part of an embodied agent, Large Language Models (LLMs) are typically
used for behavior planning given natural language instructions from the user.
However, dealing with ambiguous instructions in real-world environments remains
a challenge for LLMs. Various methods for task ambiguity detection have been
proposed. However, it is difficult to compare them because they are tested on
different datasets and there is no universal benchmark. For this reason, we
propose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual
dataset of ambiguous instructions addressed to a robot in a kitchen
environment. AmbiK was collected with the assistance of LLMs and is
human-validated. It comprises 1000 pairs of ambiguous tasks and their
unambiguous counterparts, categorized by ambiguity type (Human Preferences,
Common Sense Knowledge, Safety), with environment descriptions, clarifying
questions and answers, user intents, and task plans, for a total of 2000 tasks.
We hope that AmbiK will enable researchers to perform a unified comparison of
ambiguity detection methods. AmbiK is available at
https://github.com/cog-model/AmbiK-dataset.

</details>


### [105] [Guided Speculative Inference for Efficient Test-Time Alignment of LLMs](https://arxiv.org/abs/2506.04118)
*Jonathan Geuter,Youssef Mroueh,David Alvarez-Melis*

Main category: cs.LG

TL;DR: The paper introduces Guided Speculative Inference (GSI), an algorithm for efficient reward-guided decoding in large language models, which combines soft best-of-$n$ test-time scaling with a reward model and speculative samples from a small auxiliary model. Experiments show that GSI achieves higher accuracy than standard methods.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient algorithm for reward-guided decoding in large language models to improve accuracy on reasoning benchmarks.

Method: GSI combines soft best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative samples from a small auxiliary model $\pi_S(y\mid x)$. It approximates the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid x)\exp(\beta\,r(x,y))$ under the primary model $\pi_B$ and derives a theoretical bound on the KL divergence between the induced distribution and the optimal policy.

Result: In experiments on reasoning benchmarks (MATH500, OlympiadBench, Minerva Math), GSI achieves higher accuracy than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative decoding, and in certain settings even outperforms soft best-of-$n$ with $\pi_B$.

Conclusion: GSI is an effective algorithm for efficient reward-guided decoding in large language models, achieving higher accuracy than standard methods on reasoning benchmarks.

Abstract: We propose Guided Speculative Inference (GSI), a novel algorithm for
efficient reward-guided decoding in large language models. GSI combines soft
best-of-$n$ test-time scaling with a reward model $r(x,y)$ and speculative
samples from a small auxiliary model $\pi_S(y\mid x)$. We provably approximate
the optimal tilted policy $\pi_{\beta,B}(y\mid x) \propto \pi_B(y\mid
x)\exp(\beta\,r(x,y))$ of soft best-of-$n$ under the primary model $\pi_B$. We
derive a theoretical bound on the KL divergence between our induced
distribution and the optimal policy. In experiments on reasoning benchmarks
(MATH500, OlympiadBench, Minerva Math), our method achieves higher accuracy
than standard soft best-of-$n$ with $\pi_S$ and reward-guided speculative
decoding (Liao et al., 2025), and in certain settings even outperforms soft
best-of-$n$ with $\pi_B$. The code is available at
https://github.com/j-geuter/GSI .

</details>


### [106] [Incremental Gradient Descent with Small Epoch Counts is Surprisingly Slow on Ill-Conditioned Problems](https://arxiv.org/abs/2506.04126)
*Yujun Kim,Jaeyoung Cha,Chulhee Yun*

Main category: cs.LG

TL;DR: 近期理论结果表明，排列基础的随机梯度下降（SGD）方法比均匀采样SGD方法收敛速度更快；然而，这些研究主要集中在大量周期的情况下。本文通过研究增量梯度下降法（IGD），揭示了在小周期情况下，即使所有组件函数均为强凸函数，IGD也可能表现出惊人的慢收敛性。此外，当允许一些组件函数为非凸时，IGD的最优性差距在整个小周期内可能会显著恶化。这说明，在小周期内，基于排列的SGD的收敛特性可能因组件函数的假设而大不相同。最后，文章补充了IGD在大周期情况下的紧致上下界。


<details>
  <summary>Details</summary>
Motivation: 目前关于排列基础SGD的研究多集中于大周期领域，而对于小周期领域（即周期数K小于条件数κ）的研究较少，且是否能更快收敛仍是一个未解决的问题。为了填补这一研究空白，作者研究了IGD在小周期情况下的表现。

Method: 作者通过对光滑且强凸函数上的IGD进行研究，设定了下限以揭示其在小周期内的缓慢收敛性，并进一步探讨了当部分组件函数为非凸时，IGD的最优性差距如何恶化。

Result: 研究表明，即使所有组件函数均为强凸函数，IGD在小周期内也可能表现出缓慢的收敛性；而当部分组件函数为非凸时，IGD的最优性差距会更差。此外，文章还提供了IGD在大周期情况下的紧致上下界。

Conclusion: 排列基础SGD在小周期内的收敛性能取决于组件函数的假设，可能变化很大。本文对IGD在不同周期范围内的收敛行为进行了深入分析，为理解小周期领域的收敛特性提供了新的见解。

Abstract: Recent theoretical results demonstrate that the convergence rates of
permutation-based SGD (e.g., random reshuffling SGD) are faster than
uniform-sampling SGD; however, these studies focus mainly on the large epoch
regime, where the number of epochs $K$ exceeds the condition number $\kappa$.
In contrast, little is known when $K$ is smaller than $\kappa$, and it is still
a challenging open question whether permutation-based SGD can converge faster
in this small epoch regime (Safran and Shamir, 2021). As a step toward
understanding this gap, we study the naive deterministic variant, Incremental
Gradient Descent (IGD), on smooth and strongly convex functions. Our lower
bounds reveal that for the small epoch regime, IGD can exhibit surprisingly
slow convergence even when all component functions are strongly convex.
Furthermore, when some component functions are allowed to be nonconvex, we
prove that the optimality gap of IGD can be significantly worse throughout the
small epoch regime. Our analyses reveal that the convergence properties of
permutation-based SGD in the small epoch regime may vary drastically depending
on the assumptions on component functions. Lastly, we supplement the paper with
tight upper and lower bounds for IGD in the large epoch regime.

</details>


### [107] [Faster Approx. Top-K: Harnessing the Full Power of Two Stages](https://arxiv.org/abs/2506.04165)
*Yashas Samaga,Varun Yerram,Spandana Raj Babbula,Prateek Jain,Praneeth Netrapalli*

Main category: cs.LG

TL;DR: 本论文研究了Top-K选择问题，提出了一个通用的两阶段近似Top-K算法，并在Cloud TPUv5e上实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: Top-K选择问题是许多机器学习算法中的关键步骤，但在加速器上往往成为瓶颈。

Method: 提出了一种通用的两阶段近似Top-K算法：第一阶段从每个分组中选出前K'个元素（1 ≤ K' ≤ K），第二阶段对这些子集排序并返回前K个元素。此外，推导出了该算法预期召回率的表达式，并证明了与原算法相比，新方法可以在保持相同召回率的情况下更有效地减少第二阶段的输入规模。

Result: 推导出比现有结果紧致2倍的召回率界限；在Cloud TPUv5e上实现的算法比原算法快约一个数量级，同时不损失召回率。

Conclusion: 提出的通用两阶段Top-K算法在理论上和实践中均优于原始算法，为加速器上的Top-K选择提供了一个高效解决方案。

Abstract: We consider the Top-$K$ selection problem, which aims to identify the
largest-$K$ elements from an array. Top-$K$ selection arises in many machine
learning algorithms and often becomes a bottleneck on accelerators, which are
optimized for dense matrix multiplications. To address this problem,
\citet{chern2022tpuknnknearestneighbor} proposed a fast two-stage
\textit{approximate} Top-$K$ algorithm: (i) partition the input array and
select the top-$1$ element from each partition, (ii) sort this \textit{smaller
subset} and return the top $K$ elements. In this paper, we consider a
generalized version of this algorithm, where the first stage selects top-$K'$
elements, for some $1 \leq K' \leq K$, from each partition. Our contributions
are as follows: (i) we derive an expression for the expected recall of this
generalized algorithm and show that choosing $K' > 1$ with fewer partitions in
the first stage reduces the input size to the second stage more effectively
while maintaining the same expected recall as the original algorithm, (ii) we
derive a bound on the expected recall for the original algorithm in
\citet{chern2022tpuknnknearestneighbor} that is provably tighter by a factor of
$2$ than the one in that paper, and (iii) we implement our algorithm on Cloud
TPUv5e and achieve around an order of magnitude speedups over the original
algorithm without sacrificing recall on real-world tasks.

</details>


### [108] [N$^2$: A Unified Python Package and Test Bench for Nearest Neighbor-Based Matrix Completion](https://arxiv.org/abs/2506.04166)
*Caleb Chin,Aashish Khubchandani,Harshvardhan Maskara,Kyuseong Choi,Jacob Feitelberg,Albert Gong,Manit Paul,Tathagata Sadhukhan,Anish Agarwal,Raaz Dwivedi*

Main category: cs.LG

TL;DR: Nearest neighbor (NN) methods have become competitive tools for matrix completion with strong empirical performance and theoretical guarantees. This paper introduces N$^2$, a Python package consolidating NN-based methods, along with a new NN variant achieving state-of-the-art results and a benchmark suite demonstrating NN techniques' superiority in real-world settings.


<details>
  <summary>Details</summary>
Motivation: To provide a unified tool and testbed for nearest neighbor methods in matrix completion, which have shown robustness and effectiveness across various applications and missingness patterns.

Method: Introduction of N$^2$, a modular Python package that consolidates NN-based methods, a new NN variant that achieves state-of-the-art results, and a benchmark suite of real-world datasets.

Result: Experiments show that NN-based techniques consistently outperform classical methods in real-world settings, while classical methods perform better on idealized data.

Conclusion: N$^2$ supports rapid experimentation and benchmarking, and NN-based techniques are superior in practical scenarios.

Abstract: Nearest neighbor (NN) methods have re-emerged as competitive tools for matrix
completion, offering strong empirical performance and recent theoretical
guarantees, including entry-wise error bounds, confidence intervals, and
minimax optimality. Despite their simplicity, recent work has shown that NN
approaches are robust to a range of missingness patterns and effective across
diverse applications. This paper introduces N$^2$, a unified Python package and
testbed that consolidates a broad class of NN-based methods through a modular,
extensible interface. Built for both researchers and practitioners, N$^2$
supports rapid experimentation and benchmarking. Using this framework, we
introduce a new NN variant that achieves state-of-the-art results in several
settings. We also release a benchmark suite of real-world datasets, from
healthcare and recommender systems to causal inference and LLM evaluation,
designed to stress-test matrix completion methods beyond synthetic scenarios.
Our experiments demonstrate that while classical methods excel on idealized
data, NN-based techniques consistently outperform them in real-world settings.

</details>


### [109] [Horizon Reduction Makes RL Scalable](https://arxiv.org/abs/2506.04168)
*Seohong Park,Kevin Frans,Deepinder Mann,Benjamin Eysenbach,Aviral Kumar,Sergey Levine*

Main category: cs.LG

TL;DR: 本文研究了离线强化学习算法的可扩展性，发现许多现有算法在数据规模增大时性能提升有限。通过实验验证，问题的时间跨度（horizon）是限制离线RL扩展的主要原因。提出了一种名为SHARSA的方法，通过减少时间跨度显著提高了算法的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 了解离线强化学习算法是否能在复杂任务中随着数据、计算和模型容量的增加而扩展其性能。

Method: 使用比典型离线RL数据集大1000倍的数据集测试当前算法，并通过多种分析实验验证时间跨度对算法扩展性的影响。提出了SHARSA方法来减少时间跨度。

Result: 许多现有算法在数据规模增大时性能并未显著提升。时间跨度被证实是限制扩展性的主要原因，而减少时间跨度能显著提高算法性能。

Conclusion: 明确减少时间跨度可以解锁离线强化学习的可扩展性，SHARSA方法在这方面表现最佳。

Abstract: In this work, we study the scalability of offline reinforcement learning (RL)
algorithms. In principle, a truly scalable offline RL algorithm should be able
to solve any given problem, regardless of its complexity, given sufficient
data, compute, and model capacity. We investigate if and how current offline RL
algorithms match up to this promise on diverse, challenging, previously
unsolved tasks, using datasets up to 1000x larger than typical offline RL
datasets. We observe that despite scaling up data, many existing offline RL
algorithms exhibit poor scaling behavior, saturating well below the maximum
performance. We hypothesize that the horizon is the main cause behind the poor
scaling of offline RL. We empirically verify this hypothesis through several
analysis experiments, showing that long horizons indeed present a fundamental
barrier to scaling up offline RL. We then show that various horizon reduction
techniques substantially enhance scalability on challenging tasks. Based on our
insights, we also introduce a minimal yet scalable method named SHARSA that
effectively reduces the horizon. SHARSA achieves the best asymptotic
performance and scaling behavior among our evaluation methods, showing that
explicitly reducing the horizon unlocks the scalability of offline RL. Code:
https://github.com/seohongpark/horizon-reduction

</details>


### [110] [Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints](https://arxiv.org/abs/2506.04171)
*Utkarsh Utkarsh,Pengfei Cai,Alan Edelman,Rafael Gomez-Bombarelli,Christopher Vincent Rackauckas*

Main category: cs.LG

TL;DR: 提出了一种名为Physics-Constrained Flow Matching (PCFM)的新方法，用于在预训练的基于流的生成模型中强制执行任意非线性约束。该方法通过物理基础修正来指导采样过程，确保最终解满足精确的约束条件，并且在各种偏微分方程问题上表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管深度生成模型在由偏微分方程（PDEs）控制的物理系统中展现出可扩展模拟和不确定性感知推理的能力，但强制执行物理约束（如守恒定律和物理一致性）仍然是一个挑战。现有的方法通常依赖于软惩罚或架构偏差，无法保证硬约束。

Method: 提出了Physics-Constrained Flow Matching (PCFM)，这是一种零样本推理框架，能够在预训练的基于流的生成模型中强制执行任意非线性约束。PCFM通过应用物理基础修正来连续引导采样过程，同时保持与学习到的流一致并满足物理约束。

Result: 实验结果表明，PCFM在一系列偏微分方程（包括带有激波、不连续性和尖锐特征的PDEs）上优于无约束和有约束的基线方法，同时确保最终解满足精确的约束条件。

Conclusion: PCFM提供了一个通用框架，用于在科学和通用生成模型中强制执行硬约束，特别是在需要约束满足的应用中具有重要意义。

Abstract: Deep generative models have recently been applied to physical systems
governed by partial differential equations (PDEs), offering scalable simulation
and uncertainty-aware inference. However, enforcing physical constraints, such
as conservation laws (linear and nonlinear) and physical consistencies, remains
challenging. Existing methods often rely on soft penalties or architectural
biases that fail to guarantee hard constraints. In this work, we propose
Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that
enforces arbitrary nonlinear constraints in pretrained flow-based generative
models. PCFM continuously guides the sampling process through physics-based
corrections applied to intermediate solution states, while remaining aligned
with the learned flow and satisfying physical constraints. Empirically, PCFM
outperforms both unconstrained and constrained baselines on a range of PDEs,
including those with shocks, discontinuities, and sharp features, while
ensuring exact constraint satisfaction at the final solution. Our method
provides a general framework for enforcing hard constraints in both scientific
and general-purpose generative models, especially in applications where
constraint satisfaction is essential.

</details>


### [111] [Does Prompt Design Impact Quality of Data Imputation by LLMs?](https://arxiv.org/abs/2506.04172)
*Shreenidhi Srinivasan,Lydia Manikonda*

Main category: cs.LG

TL;DR: This paper introduces a token-aware data imputation method using large language models' in-context learning, focusing on class-imbalanced tabular data. It reduces prompt size while maintaining/improving imputation quality.


<details>
  <summary>Details</summary>
Motivation: Generating realistic synthetic tabular data is challenging, especially with class imbalance problems.

Method: A novel token-aware data imputation method that uses structured group-wise CSV-style prompting and removes irrelevant contextual information in the input prompt.

Result: Significantly reduces input prompt size while maintaining or improving imputation quality compared to baseline, particularly for smaller datasets.

Conclusion: Highlights the importance of prompt design when using LLMs for synthetic data generation and addresses a gap in LLM-based data imputation for class-imbalanced datasets.

Abstract: Generating realistic synthetic tabular data presents a critical challenge in
machine learning. It adds another layer of complexity when this data contain
class imbalance problems. This paper presents a novel token-aware data
imputation method that leverages the in-context learning capabilities of large
language models. This is achieved through the combination of a structured
group-wise CSV-style prompting technique and the elimination of irrelevant
contextual information in the input prompt. We test this approach with two
class-imbalanced binary classification datasets and evaluate the effectiveness
of imputation using classification-based evaluation metrics. The experimental
results demonstrate that our approach significantly reduces the input prompt
size while maintaining or improving imputation quality compared to our baseline
prompt, especially for datasets that are of relatively smaller in size. The
contributions of this presented work is two-fold -- 1) it sheds light on the
importance of prompt design when leveraging LLMs for synthetic data generation
and 2) it addresses a critical gap in LLM-based data imputation for
class-imbalanced datasets with missing data by providing a practical solution
within computational constraints. We hope that our work will foster further
research and discussions about leveraging the incredible potential of LLMs and
prompt engineering techniques for synthetic data generation.

</details>


### [112] [OpenThoughts: Data Recipes for Reasoning Models](https://arxiv.org/abs/2506.04178)
*Etash Guha,Ryan Marten,Sedrick Keh,Negin Raoof,Georgios Smyrnis,Hritik Bansal,Marianna Nezhurina,Jean Mercat,Trung Vu,Zayne Sprague,Ashima Suvarna,Benjamin Feuer,Liangyu Chen,Zaid Khan,Eric Frankel,Sachin Grover,Caroline Choi,Niklas Muennighoff,Shiye Su,Wanjia Zhao,John Yang,Shreyas Pimpalgaonkar,Kartik Sharma,Charlie Cheng-Jie Ji,Yichuan Deng,Sarah Pratt,Vivek Ramanujan,Jon Saad-Falcon,Jeffrey Li,Achal Dave,Alon Albalak,Kushal Arora,Blake Wulfe,Chinmay Hegde,Greg Durrett,Sewoong Oh,Mohit Bansal,Saadia Gabriel,Aditya Grover,Kai-Wei Chang,Vaishaal Shankar,Aaron Gokaslan,Mike A. Merrill,Tatsunori Hashimoto,Yejin Choi,Jenia Jitsev,Reinhard Heckel,Maheswaran Sathiamoorthy,Alexandros G. Dimakis,Ludwig Schmidt*

Main category: cs.LG

TL;DR: 尽管推理模型在数学、编码和科学等基准测试中取得了快速进展，但由于缺乏公开的训练数据集信息，推理的最佳训练方法仍有许多未解之谜。OpenThoughts项目旨在创建开源数据集以训练推理模型。通过探索，OpenThoughts2-1M数据集产生了首个与DeepSeek-R1-Distill-32B匹敌的模型OpenThinker2-32B。进一步优化数据生成管道并扩展至1.2M样本后，使用QwQ-32B作为教师模型训练出OpenThinker3-7B，在AIME 2025、LiveCodeBench和GPQA Diamond等多个基准上达到最先进水平。所有数据集和模型均已在https://openthoughts.ai发布。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的推理模型训练依赖于专有数据集，缺乏公开信息，限制了研究进展和模型透明性。因此，需要一个开源的推理数据集来推动研究并促进公平竞争。

Method: 1. 创建OpenThoughts项目，开发开源推理数据集；2. 利用初始数据集OpenThoughts2-1M训练模型OpenThinker2-32B；3. 系统性地优化数据生成管道（通过1,000+受控实验）；4. 扩展数据集规模至1.2M，并使用QwQ-32B作为教师模型训练OpenThinker3-7B。

Result: OpenThinker3-7B在多个推理基准上达到了最先进的性能：AIME 2025得分为53%，LiveCodeBench为51%，GPQA Diamond为54%。这些结果表明，基于开源数据集的模型可以媲美甚至超越专有数据集训练的模型。

Conclusion: 通过系统性优化和扩展开源数据集，可以训练出在标准推理基准上表现优异的模型。所有数据集和模型均已公开，有助于推动推理模型的研究和应用。

Abstract: Reasoning models have made rapid progress on many benchmarks involving math,
code, and science. Yet, there are still many open questions about the best
training recipes for reasoning since state-of-the-art models often rely on
proprietary datasets with little to no public information available. To address
this, the goal of the OpenThoughts project is to create open-source datasets
for training reasoning models. After initial explorations, our OpenThoughts2-1M
dataset led to OpenThinker2-32B, the first model trained on public reasoning
data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as
AIME and LiveCodeBench. We then improve our dataset further by systematically
investigating each step of our data generation pipeline with 1,000+ controlled
experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples
and using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves
state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,
and 54% on GPQA Diamond. All of our datasets and models are available on
https://openthoughts.ai.

</details>


### [113] [How to Use Graph Data in the Wild to Help Graph Anomaly Detection?](https://arxiv.org/abs/2506.04190)
*Yuxuan Cao,Jiarong Xu,Chen Zhao,Jiaan Wang,Carl Yang,Chunping Wang,Yang Yang*

Main category: cs.LG

TL;DR: In recent years, graph anomaly detection has gained widespread application in various domains. However, supervised or semi-supervised methods are unreliable due to unique challenges such as label scarcity and varying anomaly types. This paper proposes Wild-GAD, a framework that utilizes external graph data for anomaly detection tasks.


<details>
  <summary>Details</summary>
Motivation: Graph anomaly detection faces challenges like label scarcity, ill-defined anomalies, and varying anomaly types. Supervised or semi-supervised methods are unreliable. Unsupervised approaches have difficulty capturing the normal distribution accurately when data is insufficient.

Method: The authors propose Wild-GAD, built upon UniWildGraph, which includes a large collection of graph data with broad domain coverage, ample data volume, and a unified feature space. They develop selection criteria based on representativity and diversity to identify suitable external data for anomaly detection.

Result: Extensive experiments on six real-world datasets demonstrate the effectiveness of Wild-GAD. It shows an average 18% AUCROC and 32% AUCPR improvement over the best-competing methods.

Conclusion: Wild-GAD leverages external graph data to enhance anomaly detection, showing significant improvements over baseline methods.

Abstract: In recent years, graph anomaly detection has found extensive applications in
various domains such as social, financial, and communication networks. However,
anomalies in graph-structured data present unique challenges, including label
scarcity, ill-defined anomalies, and varying anomaly types, making supervised
or semi-supervised methods unreliable. Researchers often adopt unsupervised
approaches to address these challenges, assuming that anomalies deviate
significantly from the normal data distribution. Yet, when the available data
is insufficient, capturing the normal distribution accurately and
comprehensively becomes difficult. To overcome this limitation, we propose to
utilize external graph data (i.e., graph data in the wild) to help anomaly
detection tasks. This naturally raises the question: How can we use external
data to help graph anomaly detection tasks? To answer this question, we propose
a framework called Wild-GAD. It is built upon a unified database, UniWildGraph,
which comprises a large and diverse collection of graph data with broad domain
coverage, ample data volume, and a unified feature space. Further, we develop
selection criteria based on representativity and diversity to identify the most
suitable external data for anomaly detection task. Extensive experiments on six
real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the
baseline methods, our framework has an average 18% AUCROC and 32% AUCPR
improvement over the best-competing methods.

</details>


### [114] [MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures](https://arxiv.org/abs/2506.04195)
*Elena Zamaraeva,Christopher M. Collins,George R. Darling,Matthew S. Dyer,Bei Peng,Rahul Savani,Dmytro Antypov,Vladimir V. Gusev,Judith Clymo,Paul G. Spirakis,Matthew J. Rosseinsky*

Main category: cs.LG

TL;DR: A new multi-agent reinforcement learning method called Multi-Agent Crystal Structure optimization (MACS) is proposed for geometry optimization of atomic structures.


<details>
  <summary>Details</summary>
Motivation: Geometry optimization of atomic structures is a common and crucial task in computational chemistry and materials design.

Method: MACS treats geometry optimization as a partially observable Markov game in which atoms are agents that adjust their positions to collectively discover a stable configuration.

Result: MACS optimizes periodic crystal structures significantly faster, with fewer energy calculations, and the lowest failure rate.

Conclusion: The proposed MACS method is successfully trained across various compositions of reported crystalline materials, demonstrating excellent scalability and zero-shot transferability.

Abstract: Geometry optimization of atomic structures is a common and crucial task in
computational chemistry and materials design. Following the learning to
optimize paradigm, we propose a new multi-agent reinforcement learning method
called Multi-Agent Crystal Structure optimization (MACS) to address periodic
crystal structure optimization. MACS treats geometry optimization as a
partially observable Markov game in which atoms are agents that adjust their
positions to collectively discover a stable configuration. We train MACS across
various compositions of reported crystalline materials to obtain a policy that
successfully optimizes structures from the training compositions as well as
structures of larger sizes and unseen compositions, confirming its excellent
scalability and zero-shot transferability. We benchmark our approach against a
broad range of state-of-the-art optimization methods and demonstrate that MACS
optimizes periodic crystal structures significantly faster, with fewer energy
calculations, and the lowest failure rate.

</details>


### [115] [EPiC: Towards Lossless Speedup for Reasoning Training through Edge-Preserving CoT Condensation](https://arxiv.org/abs/2506.04205)
*Jinghan Jia,Hadi Reisizadeh,Chongyu Fan,Nathalie Baracaldo,Mingyi Hong,Sijia Liu*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）通过链式思维（CoT）监督展现出卓越的推理能力，但CoT痕迹过长会增加蒸馏过程中的训练成本。本文研究了资源高效的CoT压缩方法，提出了一种边缘保留压缩方法（EPiC），该方法在减少CoT长度的同时保持了答案准确性和模型生成连贯推理的能力。实验表明，EPiC可将训练时间减少34%以上，同时在MATH500上实现无损推理精度。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维（CoT）监督提高了大型语言模型的推理能力，但其冗长的CoT痕迹显著增加了蒸馏过程中的训练成本。这促使我们探索一种能够在减少CoT长度的同时保持推理能力和答案准确性的方式。

Method: 提出了一种名为Edge-Preserving Condensation (EPiC) 的方法，该方法选择性地保留每个CoT轨迹的初始和最终部分，而丢弃中间部分。此方法旨在保留推理轨迹的“边缘”，即初始问题框架和最终答案合成，以维持逻辑连续性。

Result: 实验表明，EPiC方法减少了超过34%的训练时间，同时在MATH500基准测试中实现了与完整CoT监督相当的无损推理精度。

Conclusion: 这是首次研究思想级别的CoT压缩以实现高效推理模型蒸馏的工作，EPiC方法在减少训练时间和保持推理精度方面表现优异。

Abstract: Large language models (LLMs) have shown remarkable reasoning capabilities
when trained with chain-of-thought (CoT) supervision. However, the long and
verbose CoT traces, especially those distilled from large reasoning models
(LRMs) such as DeepSeek-R1, significantly increase training costs during the
distillation process, where a non-reasoning base model is taught to replicate
the reasoning behavior of an LRM. In this work, we study the problem of CoT
condensation for resource-efficient reasoning training, aimed at pruning
intermediate reasoning steps (i.e., thoughts) in CoT traces, enabling
supervised model training on length-reduced CoT data while preserving both
answer accuracy and the model's ability to generate coherent reasoning. Our
rationale is that CoT traces typically follow a three-stage structure: problem
understanding, exploration, and solution convergence. Through empirical
analysis, we find that retaining the structure of the reasoning trace,
especially the early stage of problem understanding (rich in reflective cues)
and the final stage of solution convergence, is sufficient to achieve lossless
reasoning supervision. To this end, we propose an Edge-Preserving Condensation
method, EPiC, which selectively retains only the initial and final segments of
each CoT trace while discarding the middle portion. This design draws an
analogy to preserving the "edge" of a reasoning trajectory, capturing both the
initial problem framing and the final answer synthesis, to maintain logical
continuity. Experiments across multiple model families (Qwen and LLaMA) and
benchmarks show that EPiC reduces training time by over 34% while achieving
lossless reasoning accuracy on MATH500, comparable to full CoT supervision. To
the best of our knowledge, this is the first study to explore thought-level CoT
condensation for efficient reasoning model distillation.

</details>


### [116] [A Few Moments Please: Scalable Graphon Learning via Moment Matching](https://arxiv.org/abs/2506.04206)
*Reza Ramezanpour,Victor M. Tenorio,Antonio G. Marques,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: The paper presents a scalable graphon estimation method using implicit neural representations (INRs) for moment matching, avoiding latent variable modeling and Gromov-Wasserstein optimization. It introduces MomentMixup for data augmentation, achieving strong empirical performance in graphon estimation and improved graph classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing graphon estimation methods face challenges with scalability to large networks and resolution-independent approximation due to reliance on estimating latent variables or costly metrics like the Gromov-Wasserstein distance.

Method: The authors propose a novel graphon estimator that uses implicit neural representations (INRs) to map coordinates to graphon values, matching empirical subgraph counts (moments) from observed graphs. This avoids latent variable modeling and sidesteps the complexity of Gromov-Wasserstein optimization. They also introduce MomentMixup, a data augmentation technique enhancing graphon-based learning.

Result: The method achieves high accuracy on small graphs and superior computational efficiency on large graphs, outperforming state-of-the-art scalable estimators in 75% of benchmark settings. MomentMixup improves graph classification accuracy on most benchmarks.

Conclusion: The proposed graphon estimation method and MomentMixup offer a scalable, efficient, and accurate solution for network data analysis, advancing the field of statistical graph theory.

Abstract: Graphons, as limit objects of dense graph sequences, play a central role in
the statistical analysis of network data. However, existing graphon estimation
methods often struggle with scalability to large networks and
resolution-independent approximation, due to their reliance on estimating
latent variables or costly metrics such as the Gromov-Wasserstein distance. In
this work, we propose a novel, scalable graphon estimator that directly
recovers the graphon via moment matching, leveraging implicit neural
representations (INRs). Our approach avoids latent variable modeling by
training an INR--mapping coordinates to graphon values--to match empirical
subgraph counts (i.e., moments) from observed graphs. This direct estimation
mechanism yields a polynomial-time solution and crucially sidesteps the
combinatorial complexity of Gromov-Wasserstein optimization. Building on
foundational results, we establish a theoretical guarantee: when the observed
subgraph motifs sufficiently represent those of the true graphon (a condition
met with sufficiently large or numerous graph samples), the estimated graphon
achieves a provable upper bound in cut distance from the ground truth.
Additionally, we introduce MomentMixup, a data augmentation technique that
performs mixup in the moment space to enhance graphon-based learning. Our
graphon estimation method achieves strong empirical performance--demonstrating
high accuracy on small graphs and superior computational efficiency on large
graphs--outperforming state-of-the-art scalable estimators in 75\% of benchmark
settings and matching them in the remaining cases. Furthermore, MomentMixup
demonstrated improved graph classification accuracy on the majority of our
benchmarks.

</details>


### [117] [Advancing Multimodal Reasoning: From Optimized Cold Start to Staged Reinforcement Learning](https://arxiv.org/abs/2506.04207)
*Shuang Chen,Yue Guo,Zhaochen Su,Yafu Li,Yulun Wu,Jiacheng Chen,Jiayu Chen,Weijie Wang,Xiaoye Qu,Yu Cheng*

Main category: cs.LG

TL;DR: 受Deepseek-R1在复杂文本任务中出色推理能力的启发，许多研究尝试通过直接应用强化学习（RL）来激励多模态大语言模型（MLLMs）具备类似的能力。然而，这些方法仍难以激活复杂的推理能力。本文深入探讨了当前的训练流程，发现了三个关键现象：1) 有效的冷启动初始化对提升MLLM推理至关重要；2) 标准的GRPO应用于多模态RL时存在梯度停滞问题，影响训练稳定性和性能；3) 在多模态RL之后进行纯文本RL训练可进一步增强多模态推理能力。基于这些见解，提出了ReVisual-R1，在多个具有挑战性的基准测试中达到了开源7B MLLMs的新最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前许多工作试图通过直接应用强化学习来激励多模态大语言模型具备类似于Deepseek-R1的推理能力，但未能有效激活复杂的推理能力。

Method: 本文分析了当前多模态大语言模型的训练流程，并发现以下三个现象：1. 冷启动初始化的重要性，使用精心挑选的文本数据可以超越许多最近的多模态推理模型；2. 多模态RL中的标准GRPO存在梯度停滞问题；3. 在多模态RL之后进行文本RL训练能进一步增强多模态推理能力。基于这些发现，提出了一种分阶段的训练方法并引入了ReVisual-R1模型。

Result: 提出的ReVisual-R1模型在多个具有挑战性的基准测试中达到了开源7B MLLMs的新最先进水平，包括MathVerse、MathVision、WeMath、LogicVista、DynaMath以及AIME2024和AIME2025。

Conclusion: 通过有效的冷启动初始化、解决多模态RL中的梯度停滞问题以及采用分阶段的训练方法，可以显著提升多模态大语言模型的推理能力。

Abstract: Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex
textual tasks, many works attempt to incentivize similar capabilities in
Multimodal Large Language Models (MLLMs) by directly applying reinforcement
learning (RL). However, they still struggle to activate complex reasoning. In
this paper, rather than examining multimodal RL in isolation, we delve into
current training pipelines and identify three crucial phenomena: 1) Effective
cold start initialization is critical for enhancing MLLM reasoning.
Intriguingly, we find that initializing with carefully selected text data alone
can lead to performance surpassing many recent multimodal reasoning models,
even before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers
from gradient stagnation, which degrades training stability and performance. 3)
Subsequent text-only RL training, following the multimodal RL phase, further
enhances multimodal reasoning. This staged training approach effectively
balances perceptual grounding and cognitive reasoning development. By
incorporating the above insights and addressing multimodal RL issues, we
introduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B
MLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,
LogicVista, DynaMath, and challenging AIME2024 and AIME2025.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [118] [Hermes: High-Performance Homomorphically Encrypted Vector Databases](https://arxiv.org/abs/2506.03308)
*Dongfang Zhao*

Main category: cs.CR

TL;DR: The paper introduces Hermes, a system that enables Fully Homomorphic Encryption (FHE)-native vector query processing within a standard SQL engine. It leverages modern encryption schemes' multi-slot capabilities and introduces a novel data model to pack multiple records per ciphertext, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To integrate Fully Homomorphic Encryption (FHE) into real-world relational databases, enabling secure computation over encrypted data without revealing sensitive contents.

Method: Hermes uses the multi-slot capabilities of modern FHE schemes, introducing a new data model that packs multiple records per ciphertext and embeds encrypted auxiliary statistics. The system employs homomorphic algorithms based on slot masking, shifting, and rewriting to address ciphertext immutability and record-level mutability issues. Implemented in MySQL with OpenFHE v1.2.4.

Result: Experiments demonstrate up to 1,600 times throughput gain in encryption and over 30 times speedup in insertion compared to per-tuple baselines.

Conclusion: Hermes successfully bridges the gap between cryptographic theory and practical application, making Fully Homomorphic Encryption viable for real-world database systems.

Abstract: Fully Homomorphic Encryption (FHE) has long promised the ability to compute
over encrypted data without revealing sensitive contents -- a foundational goal
for secure cloud analytics. Yet despite decades of cryptographic advances,
practical integration of FHE into real-world relational databases remains
elusive. This paper presents \textbf{Hermes}, the first system to enable
FHE-native vector query processing inside a standard SQL engine. By leveraging
the multi-slot capabilities of modern schemes, Hermes introduces a novel data
model that packs multiple records per ciphertext and embeds encrypted auxiliary
statistics (e.g., local sums) to support in-place updates and aggregation. To
reconcile ciphertext immutability with record-level mutability, we develop new
homomorphic algorithms based on slot masking, shifting, and rewriting. Hermes
is implemented as native C++ loadable functions in MySQL using OpenFHE v1.2.4,
comprising over 3,500 lines of code. Experiments on real-world datasets show up
to 1{,}600$\times$ throughput gain in encryption and over 30$\times$ speedup in
insertion compared to per-tuple baselines. Hermes brings FHE from cryptographic
promise to practical reality -- realizing a long-standing vision at the
intersection of databases and secure computation.

</details>


### [119] [Technical Options for Flexible Hardware-Enabled Guarantees](https://arxiv.org/abs/2506.03409)
*James Petrie,Onni Aarne*

Main category: cs.CR

TL;DR: The paper proposes flexHEG, a system integrated with AI accelerator hardware to enable verifiable claims about compute usage in AI development without compromising proprietary information.


<details>
  <summary>Details</summary>
Motivation: Frontier AI models pose increasing risks to public safety and international security, creating a need for credible guarantees about AI development activities.

Method: flexHEG consists of an auditable Guarantee Processor that monitors accelerator usage and verifies compliance with specified rules, and a Secure Enclosure that provides physical tamper protection. The technical implementation options range from firmware modifications to custom hardware approaches, focusing on an 'Interlock' design.

Result: The proposed architecture could support various guarantee types, from basic usage auditing to sophisticated automated verification.

Conclusion: This work establishes technical foundations for hardware-based AI governance mechanisms that could be deployed by 2027 to address emerging regulatory and international security needs in frontier AI development.

Abstract: Frontier AI models pose increasing risks to public safety and international
security, creating a pressing need for AI developers to provide credible
guarantees about their development activities without compromising proprietary
information. We propose Flexible Hardware-Enabled Guarantees (flexHEG), a
system integrated with AI accelerator hardware to enable verifiable claims
about compute usage in AI development. The flexHEG system consists of two
primary components: an auditable Guarantee Processor that monitors accelerator
usage and verifies compliance with specified rules, and a Secure Enclosure that
provides physical tamper protection. In this report, we analyze technical
implementation options ranging from firmware modifications to custom hardware
approaches, with focus on an "Interlock" design that provides the Guarantee
Processor direct access to accelerator data paths. Our proposed architecture
could support various guarantee types, from basic usage auditing to
sophisticated automated verification. This work establishes technical
foundations for hardware-based AI governance mechanisms that could be deployed
by 2027 to address emerging regulatory and international security needs in
frontier AI development.

</details>


### [120] [A Threat Intelligence Event Extraction Conceptual Model for Cyber Threat Intelligence Feeds](https://arxiv.org/abs/2506.03551)
*Jamal H. Al-Yasiri,Mohamad Fadli Bin Zolkipli,Nik Fatinah N Mohd Farid,Mohammed Alsamman,Zainab Ali Mohammed*

Main category: cs.CR

TL;DR: With the rise of cyber threats, enhancing CTI data collection efficiency is crucial for cybersecurity. This paper reviews current techniques and proposes a conceptual model integrating XLM-RoBERTa, BiGRU, and CRF to address existing gaps.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of CTI data collection and preprocessing in order to enhance real-time threat analysis and strengthen cybersecurity.

Method: Systematic review of current CTI data collection techniques following PRISMA guidelines and proposal of the XBC conceptual model integrating XLM-RoBERTa, BiGRU, and CRF.

Result: AI-driven methods, particularly supervised and unsupervised learning, significantly improve threat detection accuracy. The proposed XBC model addresses gaps in existing research.

Conclusion: This study provides a detailed analysis of current CTI data collection techniques and introduces an innovative conceptual model to advance future threat intelligence capabilities.

Abstract: In response to the escalating cyber threats, the efficiency of Cyber Threat
Intelligence (CTI) data collection has become paramount in ensuring robust
cybersecurity. However, existing works encounter significant challenges in
preprocessing large volumes of multilingual threat data, leading to
inefficiencies in real-time threat analysis. This paper presents a systematic
review of current techniques aimed at enhancing CTI data collection efficiency.
Additionally, it proposes a conceptual model to further advance the
effectiveness of threat intelligence feeds. Following the PRISMA guidelines,
the review examines relevant studies from the Scopus database, highlighting the
critical role of artificial intelligence (AI) and machine learning models in
optimizing CTI data preprocessing. The findings underscore the importance of
AI-driven methods, particularly supervised and unsupervised learning, in
significantly improving the accuracy of threat detection and event extraction,
thereby strengthening cybersecurity. Furthermore, the study identifies a gap in
the existing research and introduces XBC conceptual model integrating
XLM-RoBERTa, BiGRU, and CRF, specifically developed to address this gap. This
paper contributes conceptually to the field by providing a detailed analysis of
current CTI data collection techniques and introducing an innovative conceptual
model to enhance future threat intelligence capabilities.

</details>


### [121] [Mono: Is Your "Clean" Vulnerability Dataset Really Solvable? Exposing and Trapping Undecidable Patches and Beyond](https://arxiv.org/abs/2506.03651)
*Zeyu Gao,Junlin Zhou,Bolun Zhang,Yi He,Chao Zhang,Yuxin Cui,Hao Wang*

Main category: cs.CR

TL;DR: mono是一个新型LLM驱动的框架，通过模拟人类专家推理过程构建可靠的漏洞数据集，包含语义感知补丁分类、迭代上下文分析和系统根本原因分析三个关键组件。在MegaVul基准上的评估表明，mono可以纠正31.0%的标记错误，恢复89%的跨程序漏洞，并发现16.7%的CVE包含不可判定的补丁，同时提高了现有模型15%的漏洞检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的安全补丁数据集存在标签不准确、上下文信息不足以及无法明确表示漏洞根本原因或修复方法的问题，这些问题引入了噪声，可能误导检测模型并削弱其效果。

Method: 提出了一种名为mono的新型LLM驱动框架，该框架通过语义感知补丁分类、迭代上下文分析和系统根本原因分析来改进安全补丁数据集。

Result: 在MegaVul基准上的评估表明，mono可以纠正31.0%的标记错误，恢复89%的跨程序漏洞，并发现16.7%的CVE包含不可判定的补丁，同时提高了现有模型15%的漏洞检测准确性。

Conclusion: mono框架能够显著提高漏洞数据集的质量，从而提升现有模型的漏洞检测准确性，并为未来的研究提供了开源框架和数据集。

Abstract: The quantity and quality of vulnerability datasets are essential for
developing deep learning solutions to vulnerability-related tasks. Due to the
limited availability of vulnerabilities, a common approach to building such
datasets is analyzing security patches in source code. However, existing
security patches often suffer from inaccurate labels, insufficient contextual
information, and undecidable patches that fail to clearly represent the root
causes of vulnerabilities or their fixes. These issues introduce noise into the
dataset, which can mislead detection models and undermine their effectiveness.
To address these issues, we present mono, a novel LLM-powered framework that
simulates human experts' reasoning process to construct reliable vulnerability
datasets. mono introduces three key components to improve security patch
datasets: (i) semantic-aware patch classification for precise vulnerability
labeling, (ii) iterative contextual analysis for comprehensive code
understanding, and (iii) systematic root cause analysis to identify and filter
undecidable patches. Our comprehensive evaluation on the MegaVul benchmark
demonstrates that mono can correct 31.0% of labeling errors, recover 89% of
inter-procedural vulnerabilities, and reveals that 16.7% of CVEs contain
undecidable patches. Furthermore, mono's enriched context representation
improves existing models' vulnerability detection accuracy by 15%. We open
source the framework mono and the dataset MonoLens in
https://github.com/vul337/mono.

</details>


### [122] [Client-Side Zero-Shot LLM Inference for Comprehensive In-Browser URL Analysis](https://arxiv.org/abs/2506.03656)
*Avihay Cohen*

Main category: cs.CR

TL;DR: 提出了一种基于客户端框架的URL分析方法，利用本地运行的大规模语言模型进行零样本推理，有效检测恶意网站和钓鱼链接，保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 恶意网站和钓鱼链接对网络安全构成日益严重的威胁，传统检测方法在泛化能力、隐私保护和应对复杂威胁方面存在挑战。

Method: 构建了一个客户端框架，结合真实的浏览器沙盒（使用iframes）和紧凑型大规模语言模型（LLM），通过静态代码分析、动态沙盒执行结果和可见内容等多源信息进行推理和分类。

Result: 即使是一个较小的客户端模型也能达到与云服务相当的高检测准确率，并提供深入的安全问题解释，同时保护了用户隐私。

Conclusion: 客户端LLM推理是一种可行且有效的网络威胁分析解决方案，避免了将敏感数据发送到云端的需求。

Abstract: Malicious websites and phishing URLs pose an ever-increasing cybersecurity
risk, with phishing attacks growing by 40% in a single year. Traditional
detection approaches rely on machine learning classifiers or rule-based
scanners operating in the cloud, but these face significant challenges in
generalization, privacy, and evasion by sophisticated threats. In this paper,
we propose a novel client-side framework for comprehensive URL analysis that
leverages zero-shot inference by a local large language model (LLM) running
entirely in-browser. Our system uses a compact LLM (e.g., 3B/8B parameters) via
WebLLM to perform reasoning over rich context collected from the target
webpage, including static code analysis (JavaScript abstract syntax trees,
structure, and code patterns), dynamic sandbox execution results (DOM changes,
API calls, and network requests),and visible content. We detail the
architecture and methodology of the system, which combines a real browser
sandbox (using iframes) resistant to common anti-analysis techniques, with an
LLM-based analyzer that assesses potential vulnerabilities and malicious
behaviors without any task-specific training (zero-shot). The LLM aggregates
evidence from multiple sources (code, execution trace, page content) to
classify the URL as benign or malicious and to provide an explanation of the
threats or security issues identified. We evaluate our approach on a diverse
set of benign and malicious URLs, demonstrating that even a compact client-side
model can achieve high detection accuracy and insightful explanations
comparable to cloud-based solutions, while operating privately on end-user
devices. The results show that client-side LLM inference is a feasible and
effective solution to web threat analysis, eliminating the need to send
potentially sensitive data to cloud services.

</details>


### [123] [Dropout-Robust Mechanisms for Differentially Private and Fully Decentralized Mean Estimation](https://arxiv.org/abs/2506.03746)
*César Sabater,Sonia Ben Mokhtar,Jan Ramon*

Main category: cs.CR

TL;DR: This paper proposes IncA, a new protocol for fully decentralized mean estimation that enforces differential privacy without central orchestration and employs low-variance correlated noise.


<details>
  <summary>Details</summary>
Motivation: Achieving differentially private computations in decentralized settings poses significant challenges, particularly regarding accuracy, communication cost, and robustness against information leakage.

Method: Propose IncA, a protocol for fully decentralized mean estimation which enforces differential privacy, requires no central orchestration and employs low-variance correlated noise achieved by incrementally injecting sensitive information into the computation.

Result: Theoretically, when no parties permanently disconnect, IncA achieves accuracy comparable to that of a centralized setting. Empirically, the use of low-variance correlated noise significantly mitigates the accuracy loss experienced by existing techniques in the presence of dropouts.

Conclusion: IncA is a promising protocol for fully decentralized mean estimation as it improves upon existing techniques in terms of accuracy and robustness.

Abstract: Achieving differentially private computations in decentralized settings poses
significant challenges, particularly regarding accuracy, communication cost,
and robustness against information leakage. While cryptographic solutions offer
promise, they often suffer from high communication overhead or require
centralization in the presence of network failures. Conversely, existing fully
decentralized approaches typically rely on relaxed adversarial models or
pairwise noise cancellation, the latter suffering from substantial accuracy
degradation if parties unexpectedly disconnect. In this work, we propose IncA,
a new protocol for fully decentralized mean estimation, a widely used primitive
in data-intensive processing. Our protocol, which enforces differential
privacy, requires no central orchestration and employs low-variance correlated
noise, achieved by incrementally injecting sensitive information into the
computation. First, we theoretically demonstrate that, when no parties
permanently disconnect, our protocol achieves accuracy comparable to that of a
centralized setting-already an improvement over most existing decentralized
differentially private techniques. Second, we empirically show that our use of
low-variance correlated noise significantly mitigates the accuracy loss
experienced by existing techniques in the presence of dropouts.

</details>


### [124] [Prediction Inconsistency Helps Achieve Generalizable Detection of Adversarial Examples](https://arxiv.org/abs/2506.03765)
*Sicong Han,Chenhao Lin,Zhengyu Zhao,Xiyuan Wang,Xinlei He,Qian Li,Cong Wang,Qian Wang,Chao Shen*

Main category: cs.CR

TL;DR: PID是一种轻量级且通用的检测框架，通过捕捉主模型和辅助模型之间的预测不一致性来区分对抗样本和正常样本，在多种攻击场景下表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的对抗检测方法在应用于对抗性训练模型或跨白盒和黑盒攻击设置时，效果显著下降。本文旨在解决这种泛化性弱的问题。

Method: 观察到辅助模型对主模型在对抗样本上的预测往往置信度较低，而对正常样本保持高置信度。基于此发现，提出Prediction Inconsistency Detector (PID)，利用主模型和辅助模型之间的预测不一致性来区分对抗样本和正常样本。

Result: PID兼容自然训练和对抗性训练的主模型，并在3种白盒、3种黑盒和1种混合攻击场景中优于4种检测方法。具体来说，在CIFAR-10数据集上，PID分别达到99.29%和99.30%的平均AUC分数（自然和对抗训练），在ImageNet上分别达到98.31%和96.81%，超越现有SOTA方法4.70%~25.46%。

Conclusion: PID作为一种轻量级通用检测框架，能够有效区分对抗样本和正常样本，尤其在不同训练策略和攻击场景下表现优异，为提高对抗检测方法的泛化性提供了新思路。

Abstract: Adversarial detection protects models from adversarial attacks by refusing
suspicious test samples. However, current detection methods often suffer from
weak generalization: their effectiveness tends to degrade significantly when
applied to adversarially trained models rather than naturally trained ones, and
they generally struggle to achieve consistent effectiveness across both
white-box and black-box attack settings. In this work, we observe that an
auxiliary model, differing from the primary model in training strategy or model
architecture, tends to assign low confidence to the primary model's predictions
on adversarial examples (AEs), while preserving high confidence on normal
examples (NEs). Based on this discovery, we propose Prediction Inconsistency
Detector (PID), a lightweight and generalizable detection framework to
distinguish AEs from NEs by capturing the prediction inconsistency between the
primal and auxiliary models. PID is compatible with both naturally and
adversarially trained primal models and outperforms four detection methods
across 3 white-box, 3 black-box, and 1 mixed adversarial attacks. Specifically,
PID achieves average AUC scores of 99.29\% and 99.30\% on CIFAR-10 when the
primal model is naturally and adversarially trained, respectively, and 98.31%
and 96.81% on ImageNet under the same conditions, outperforming existing SOTAs
by 4.70%$\sim$25.46%.

</details>


### [125] [Depermissioning Web3: a Permissionless Accountable RPC Protocol for Blockchain Networks](https://arxiv.org/abs/2506.03940)
*Weihong Wang,Tom Van Cutsem*

Main category: cs.CR

TL;DR: In blockchain networks, the serving layer enabling 'Web3' data integration is dominated by centralized services. This paper proposes PARP, a protocol allowing clients and full nodes to interact pseudonymously while maintaining accountability through light client schemes, fraud proofs, and payment channels.


<details>
  <summary>Details</summary>
Motivation: To address concerns regarding privacy, integrity, and availability of data access in blockchain networks where permissioned centralized services dominate the serving layer.

Method: PARP leverages 'light client' schemes for data integrity checks, fraud proofs to keep full nodes honest, and integrates payment channels to facilitate micro-payments.

Result: The prototype implementation for Ethereum demonstrates the feasibility of PARP and quantifies its overhead compared to the base RPC protocol.

Conclusion: PARP enables clients and full nodes to interact pseudonymously while keeping both parties accountable, providing an economic incentive for full nodes to serve.

Abstract: In blockchain networks, so-called "full nodes" serve data to and relay
transactions from clients through an RPC interface. This serving layer enables
integration of "Web3" data, stored on blockchains, with "Web2" mobile or web
applications that cannot directly participate as peers in a blockchain network.
In practice, the serving layer is dominated by a small number of centralized
services ("node providers") that offer permissioned access to RPC endpoints.
Clients register with these providers because they offer reliable and
convenient access to blockchain data: operating a full node themselves requires
significant computational and storage resources, and public (permissionless)
RPC nodes lack financial incentives to serve large numbers of clients with
consistent performance.
  Permissioned access to an otherwise permissionless blockchain network raises
concerns regarding the privacy, integrity, and availability of data access. To
address this, we propose a Permissionless Accountable RPC Protocol (PARP). It
enables clients and full nodes to interact pseudonymously while keeping both
parties accountable. PARP leverages "light client" schemes for essential data
integrity checks, combined with fraud proofs, to keep full nodes honest and
accountable. It integrates payment channels to facilitate micro-payments,
holding clients accountable for the resources they consume and providing an
economic incentive for full nodes to serve. Our prototype implementation for
Ethereum demonstrates the feasibility of PARP, and we quantify its overhead
compared to the base RPC protocol.

</details>


### [126] [Privacy and Security Threat for OpenAI GPTs](https://arxiv.org/abs/2506.04036)
*Wei Wenying,Zhao Kaifa,Xue Lei,Fan Ming*

Main category: cs.CR

TL;DR: 大型语言模型（LLMs）在聊天机器人应用中展现出强大的信息处理能力，但自定义GPT的广泛应用也带来了安全和隐私威胁。本文通过开发针对不同防御级别的自定义GPT的指令泄露攻击，揭示了其广泛漏洞，并提出评估防御策略有效性的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）和自定义GPT在实际应用中取得了显著成就，但其生态系统的扩展也引入了新的安全和隐私挑战，包括对开发者知识产权的威胁和对用户数据隐私的侵害。

Method: 1. 开发三阶段的指令泄露攻击，针对具有不同防御级别的自定义GPT。
2. 在10,000个真实世界的自定义GPT上进行实验，评估其对指令泄露攻击的脆弱性。
3. 构建一个框架，用于评估防御策略的有效性和识别不必要的数据访问行为。

Result: - 超过98.8%的自定义GPT容易受到单一或多重对抗性提示的指令泄露攻击。
- 即使采用防御策略，仍有77.5%的自定义GPT容易受到基础攻击。
- 发现738个自定义GPT收集用户对话信息，其中8个表现出与其功能无关的数据访问行为。

Conclusion: 本研究提高了GPT开发者对集成特定防御策略重要性的认识，并强调了用户在使用基于LLM的应用时对数据隐私的关注。

Abstract: Large language models (LLMs) demonstrate powerful information handling
capabilities and are widely integrated into chatbot applications. OpenAI
provides a platform for developers to construct custom GPTs, extending
ChatGPT's functions and integrating external services. Since its release in
November 2023, over 3 million custom GPTs have been created. However, such a
vast ecosystem also conceals security and privacy threats. For developers,
instruction leaking attacks threaten the intellectual property of instructions
in custom GPTs through carefully crafted adversarial prompts. For users,
unwanted data access behavior by custom GPTs or integrated third-party services
raises significant privacy concerns. To systematically evaluate the scope of
threats in real-world LLM applications, we develop three phases instruction
leaking attacks target GPTs with different defense level. Our widespread
experiments on 10,000 real-world custom GPTs reveal that over 98.8% of GPTs are
vulnerable to instruction leaking attacks via one or more adversarial prompts,
and half of the remaining GPTs can also be attacked through multiround
conversations. We also developed a framework to assess the effectiveness of
defensive strategies and identify unwanted behaviors in custom GPTs. Our
findings show that 77.5% of custom GPTs with defense strategies are vulnerable
to basic instruction leaking attacks. Additionally, we reveal that 738 custom
GPTs collect user conversational information, and identified 8 GPTs exhibiting
data access behaviors that are unnecessary for their intended functionalities.
Our findings raise awareness among GPT developers about the importance of
integrating specific defensive strategies in their instructions and highlight
users' concerns about data privacy when using LLM-based applications.

</details>


### [127] [TracLLM: A Generic Framework for Attributing Long Context LLMs](https://arxiv.org/abs/2506.04202)
*Yanting Wang,Wei Zou,Runpeng Geng,Jinyuan Jia*

Main category: cs.CR

TL;DR: TracLLM是一个专为长上下文大语言模型设计的通用上下文追溯框架，通过改进现有特征归因方法的效果和效率，有效识别导致LLM输出的文本。


<details>
  <summary>Details</summary>
Motivation: 长上下文大语言模型在实际应用中越来越重要，但如何定位生成输出所依据的上下文文本成为一个关键问题，这对调试系统、进行攻击后的法医分析以及增强用户信任至关重要。

Method: 提出了TracLLM框架，包含基于知情搜索的算法以提高效率，以及贡献分数集成/去噪技术以提高准确性，从而改进现有特征归因方法的效果和效率。

Result: 评估结果表明，TracLLM能够有效地识别长上下文中导致LLM输出的文本。

Conclusion: TracLLM是首个专为长上下文大语言模型设计的通用上下文追溯框架，显著提高了特征归因方法的效果和效率。

Abstract: Long context large language models (LLMs) are deployed in many real-world
applications such as RAG, agent, and broad LLM-integrated applications. Given
an instruction and a long context (e.g., documents, PDF files, webpages), a
long context LLM can generate an output grounded in the provided context,
aiming to provide more accurate, up-to-date, and verifiable outputs while
reducing hallucinations and unsupported claims. This raises a research
question: how to pinpoint the texts (e.g., sentences, passages, or paragraphs)
in the context that contribute most to or are responsible for the generated
output by an LLM? This process, which we call context traceback, has various
real-world applications, such as 1) debugging LLM-based systems, 2) conducting
post-attack forensic analysis for attacks (e.g., prompt injection attack,
knowledge corruption attacks) to an LLM, and 3) highlighting knowledge sources
to enhance the trust of users towards outputs generated by LLMs. When applied
to context traceback for long context LLMs, existing feature attribution
methods such as Shapley have sub-optimal performance and/or incur a large
computational cost. In this work, we develop TracLLM, the first generic context
traceback framework tailored to long context LLMs. Our framework can improve
the effectiveness and efficiency of existing feature attribution methods. To
improve the efficiency, we develop an informed search based algorithm in
TracLLM. We also develop contribution score ensemble/denoising techniques to
improve the accuracy of TracLLM. Our evaluation results show TracLLM can
effectively identify texts in a long context that lead to the output of an LLM.
Our code and data are at: https://github.com/Wang-Yanting/TracLLM.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [128] [Enhancing Automatic PT Tagging for MEDLINE Citations Using Transformer-Based Models](https://arxiv.org/abs/2506.03321)
*Victor H. Cid,James Mork*

Main category: cs.DL

TL;DR: The paper explores using BERT and DistilBERT for predicting MeSH Publication Types from MEDLINE citation metadata, showing Transformer models' potential to improve tagging accuracy in biomedical indexing.


<details>
  <summary>Details</summary>
Motivation: There is a need to enhance the automated indexing process of biomedical literature due to limitations in legacy NLP algorithms.

Method: Investigated the use of pre-trained Transformer-based models (BERT and DistilBERT) on MEDLINE citation metadata for predicting MeSH Publication Types, evaluating both monolithic multi-label classifiers and binary classifier ensembles.

Result: Transformer models significantly improved the accuracy of PT tagging compared to legacy methods, demonstrating their potential for scalable and efficient biomedical indexing.

Conclusion: Pre-trained Transformer models can effectively be used to predict MeSH Publication Types, improving the retrieval of biomedical literature.

Abstract: We investigated the feasibility of predicting Medical Subject Headings (MeSH)
Publication Types (PTs) from MEDLINE citation metadata using pre-trained
Transformer-based models BERT and DistilBERT. This study addresses limitations
in the current automated indexing process, which relies on legacy NLP
algorithms. We evaluated monolithic multi-label classifiers and binary
classifier ensembles to enhance the retrieval of biomedical literature. Results
demonstrate the potential of Transformer models to significantly improve PT
tagging accuracy, paving the way for scalable, efficient biomedical indexing.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [129] [Why Regression? Binary Encoding Classification Brings Confidence to Stock Market Index Price Prediction](https://arxiv.org/abs/2506.03153)
*Junzhe Jiang,Chang Yang,Xinrun Wang,Bo Li*

Main category: q-fin.ST

TL;DR: The paper proposes Cubic, a framework for stock index price prediction that models the fusion of constituent stocks, uses binary encoding classification, and incorporates confidence-guided trading policies.


<details>
  <summary>Details</summary>
Motivation: Accurate stock index price prediction is challenging due to existing methods treating indices as isolated time series, failing to capture their nature as aggregations of interdependent stocks.

Method: Cubic includes fusion in the latent space of stocks, reformulates regression as binary encoding classification with cross-entropy loss, and uses confidence-guided prediction and trading with regularization loss.

Result: Cubic outperforms state-of-the-art baselines in stock index prediction tasks across multiple markets and indices, showing superior forecasting accuracy and trading profitability.

Conclusion: Cubic provides an effective end-to-end solution for stock index price prediction by explicitly modeling the adaptive fusion of constituent stocks.

Abstract: Stock market indices serve as fundamental market measurement that quantify
systematic market dynamics. However, accurate index price prediction remains
challenging, primarily because existing approaches treat indices as isolated
time series and frame the prediction as a simple regression task. These methods
fail to capture indices' inherent nature as aggregations of constituent stocks
with complex, time-varying interdependencies. To address these limitations, we
propose Cubic, a novel end-to-end framework that explicitly models the adaptive
fusion of constituent stocks for index price prediction. Our main contributions
are threefold. i) Fusion in the latent space: we introduce the fusion mechanism
over the latent embedding of the stocks to extract the information from the
vast number of stocks. ii) Binary encoding classification: since regression
tasks are challenging due to continuous value estimation, we reformulate the
regression into the classification task, where the target value is converted to
binary and we optimize the prediction of the value of each digit with
cross-entropy loss. iii) Confidence-guided prediction and trading: we introduce
the regularization loss to address market prediction uncertainty for the index
prediction and design the rule-based trading policies based on the confidence.
Extensive experiments across multiple stock markets and indices demonstrate
that Cubic consistently outperforms state-of-the-art baselines in stock index
prediction tasks, achieving superior performance on both forecasting accuracy
metrics and downstream trading profitability.

</details>


### [130] [High-Dimensional Learning in Finance](https://arxiv.org/abs/2506.03780)
*Hasan Fallahgoul*

Main category: q-fin.ST

TL;DR: Recent machine learning advances for financial prediction are analyzed. The study focuses on three aspects of high-dimensional learning in finance, proving that observed predictive success is driven by low-complexity artifacts when sample size is small and features are high-dimensional.


<details>
  <summary>Details</summary>
Motivation: To provide theoretical foundations and empirical validation for understanding when and how large, over-parameterized models achieve predictive success in financial prediction.

Method: The paper examines three key aspects: 1) Proving the alteration of Gaussian kernel approximation within Random Fourier Features implementations; 2) Deriving sample complexity bounds under weak signal-to-noise ratios; 3) Conducting VC-dimension analysis to reveal ridgeless regression's effective complexity.

Result: Theoretical predictions were confirmed through comprehensive numerical validation, showing systematic breakdowns of claimed theoretical properties across realistic parameter ranges. Predictive success in small sample sizes with high-dimensional features is driven by low-complexity artifacts.

Conclusion: When sample size is small and features are high-dimensional, the observed predictive success in financial models is due to low-complexity artifacts rather than genuine high-dimensional learning.

Abstract: Recent advances in machine learning have shown promising results for
financial prediction using large, over-parameterized models. This paper
provides theoretical foundations and empirical validation for understanding
when and how these methods achieve predictive success. I examine three key
aspects of high-dimensional learning in finance. First, I prove that
within-sample standardization in Random Fourier Features implementations
fundamentally alters the underlying Gaussian kernel approximation, replacing
shift-invariant kernels with training-set dependent alternatives. Second, I
derive sample complexity bounds showing when reliable learning becomes
information-theoretically impossible under weak signal-to-noise ratios typical
in finance. Third, VC-dimension analysis reveals that ridgeless regression's
effective complexity is bounded by sample size rather than nominal feature
dimension. Comprehensive numerical validation confirms these theoretical
predictions, revealing systematic breakdown of claimed theoretical properties
across realistic parameter ranges. These results show that when sample size is
small and features are high-dimensional, observed predictive success is
necessarily driven by low-complexity artifacts, not genuine high-dimensional
learning.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [131] [POLARIS: A High-contrast Polarimetric Imaging Benchmark Dataset for Exoplanetary Disk Representation Learning](https://arxiv.org/abs/2506.03511)
*Fangyi Cao,Bin Ren,Zihao Wang,Shiwei Fu,Youbin Mo,Xiaoyang Liu,Yuzhou Chen,Weixin Yao*

Main category: astro-ph.EP

TL;DR: The paper explores the use of AI in imaging Earth-like exoplanets using a new benchmark and POLARIS dataset, which significantly reduces manual labeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of directly imaging exoplanets due to labor-intensive labeling processes.

Method: Introduction of POLARIS dataset for classifying reference star and circumstellar disk images with minimal manual labeling. Evaluation of various models including statistical, generative, and large vision-language models, along with an unsupervised generative representation learning framework.

Result: Achieved superior performance and enhanced representational power in imaging exoplanets.

Conclusion: This work provides a transformative tool for direct exoplanet imaging, engaging data scientists and astrophysicists in interdisciplinary breakthroughs.

Abstract: With over 1,000,000 images from more than 10,000 exposures using
state-of-the-art high-contrast imagers (e.g., Gemini Planet Imager, VLT/SPHERE)
in the search for exoplanets, can artificial intelligence (AI) serve as a
transformative tool in imaging Earth-like exoplanets in the coming decade? In
this paper, we introduce a benchmark and explore this question from a
polarimetric image representation learning perspective. Despite extensive
investments over the past decade, only a few new exoplanets have been directly
imaged. Existing imaging approaches rely heavily on labor-intensive labeling of
reference stars, which serve as background to extract circumstellar objects
(disks or exoplanets) around target stars. With our POLARIS (POlarized Light
dAta for total intensity Representation learning of direct Imaging of
exoplanetary Systems) dataset, we classify reference star and circumstellar
disk images using the full public SPHERE/IRDIS polarized-light archive since
2014, requiring less than 10 percent manual labeling. We evaluate a range of
models including statistical, generative, and large vision-language models and
provide baseline performance. We also propose an unsupervised generative
representation learning framework that integrates these models, achieving
superior performance and enhanced representational power. To our knowledge,
this is the first uniformly reduced, high-quality exoplanet imaging dataset,
rare in astrophysics and machine learning. By releasing this dataset and
baselines, we aim to equip astrophysicists with new tools and engage data
scientists in advancing direct exoplanet imaging, catalyzing major
interdisciplinary breakthroughs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [132] [A Generic Branch-and-Bound Algorithm for $\ell_0$-Penalized Problems with Supplementary Material](https://arxiv.org/abs/2506.03974)
*Clément Elvira,Théo Guyard,Cédric Herzet*

Main category: math.OC

TL;DR: A new Branch-and-Bound method for L0-penalized problems is presented, which accommodates a broader class of loss functions and provides closed-form expressions for key quantities. An open-source solver El0ps is introduced, showing state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods mainly focus on quadratic losses with 'Big-M' constraints or L2-norm penalties, lacking flexibility in handling various loss functions.

Method: A generic Branch-and-Bound procedure is designed for L0-penalized optimization problems, featuring a general penalty term that includes existing techniques as special cases and ensuring closed-form expressions for implementation.

Result: El0ps, the proposed Python solver, achieves state-of-the-art performance in classical instances and extends computational feasibility to previously unsolvable problems.

Conclusion: The paper concludes by presenting a flexible and efficient Branch-and-Bound framework along with an effective solver El0ps for L0-penalized optimization problems.

Abstract: We present a generic Branch-and-Bound procedure designed to solve
L0-penalized optimization problems. Existing approaches primarily focus on
quadratic losses and construct relaxations using "Big-M" constraints and/or
L2-norm penalties. In contrast, our method accommodates a broader class of loss
functions and allows greater flexibility in relaxation design through a general
penalty term, encompassing existing techniques as special cases. We establish
theoretical results ensuring that all key quantities required for the
Branch-and-Bound implementation admit closed-form expressions under the general
blanket assumptions considered in our work. Leveraging this framework, we
introduce El0ps, an open-source Python solver with a plug-and-play workflow
that enables user-defined losses and penalties in L0-penalized problems.
Through extensive numerical experiments, we demonstrate that El0ps achieves
state-of-the-art performance on classical instances and extends computational
feasibility to previously intractable ones.

</details>


### [133] [Similarity-based fuzzy clustering scientific articles: potentials and challenges from mathematical and computational perspectives](https://arxiv.org/abs/2506.04045)
*Vu Thi Huong,Ida Litzel,Thorsten Koch*

Main category: math.OC

TL;DR: Fuzzy clustering is crucial for analyzing publication data. It can be formulated as a constrained optimization model, but applying it to large databases presents challenges. This paper explores these from mathematical and computational angles, establishing second-order optimality conditions and proposing practical solution methods like accelerating gradient projection with GPU computing.


<details>
  <summary>Details</summary>
Motivation: To improve the analysis of publication data by addressing the challenges of applying fuzzy clustering to large-scale databases.

Method: Analyzing fuzzy clustering through a constrained optimization model, establishing second-order optimality conditions, and proposing practical solution methods such as accelerating the gradient projection method using GPU-based parallel computing.

Result: Provides new theoretical insights into fuzzy clustering and demonstrates an efficient way to handle large-scale data using accelerated gradient projection method.

Conclusion: Fuzzy clustering's potential for analyzing publication data is significant, but its application to massive databases requires tailored optimization algorithms; this work provides both theoretical advancements and practical solutions.

Abstract: Fuzzy clustering, which allows an article to belong to multiple clusters with
soft membership degrees, plays a vital role in analyzing publication data. This
problem can be formulated as a constrained optimization model, where the goal
is to minimize the discrepancy between the similarity observed from data and
the similarity derived from a predicted distribution. While this approach
benefits from leveraging state-of-the-art optimization algorithms, tailoring
them to work with real, massive databases like OpenAlex or Web of Science -
containing about 70 million articles and a billion citations - poses
significant challenges. We analyze potentials and challenges of the approach
from both mathematical and computational perspectives. Among other things,
second-order optimality conditions are established, providing new theoretical
insights, and practical solution methods are proposed by exploiting the
structure of the problem. Specifically, we accelerate the gradient projection
method using GPU-based parallel computing to efficiently handle large-scale
data.

</details>


<div id='cond-mat.supr-con'></div>

# cond-mat.supr-con [[Back]](#toc)

### [134] [HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature Superconductors for AI-Driven Critical Temperature Prediction](https://arxiv.org/abs/2506.03837)
*Xiao-Qi Han,Ze-Feng Gao,Xin-De Wang,Zhenfeng Ouyang,Peng-Jie Guo,Zhong-Yi Lu*

Main category: cond-mat.supr-con

TL;DR: 研究人员创建了一个名为HTSC-2025的高温超导体基准数据集，该数据集涵盖了从2023年到2025年间基于BCS理论预测出的多种超导材料。此开放源代码数据集将有助于推动使用AI方法发现超导材料的进程。


<details>
  <summary>Details</summary>
Motivation: 当前在利用人工智能预测超导转变温度的研究中，缺乏广泛接受的基准数据集阻碍了不同算法之间的公平比较和方法进步。

Method: 构建了一个名为HTSC-2025的公开基准数据集，其中包含2023年至2025年间理论物理学家基于BCS超导电性理论预测出的超导材料，包括X$_2$YH$_6$系统、钙钛矿MXH$_3$系统、M$_3$XH$_8$系统、源自LaH$_{10}$结构演化的笼型BCN掺杂金属原子系统以及源自MgB$_2$的二维蜂窝状系统。

Result: HTSC-2025数据集已经开源发布，并将持续更新，为加速使用基于AI的方法发现超导材料提供重要帮助。

Conclusion: HTSC-2025基准数据集的建立对推动AI在超导材料发现中的应用具有重要意义。

Abstract: The discovery of high-temperature superconducting materials holds great
significance for human industry and daily life. In recent years, research on
predicting superconducting transition temperatures using artificial
intelligence~(AI) has gained popularity, with most of these tools claiming to
achieve remarkable accuracy. However, the lack of widely accepted benchmark
datasets in this field has severely hindered fair comparisons between different
AI algorithms and impeded further advancement of these methods. In this work,
we present the HTSC-2025, an ambient-pressure high-temperature superconducting
benchmark dataset. This comprehensive compilation encompasses theoretically
predicted superconducting materials discovered by theoretical physicists from
2023 to 2025 based on BCS superconductivity theory, including the renowned
X$_2$YH$_6$ system, perovskite MXH$_3$ system, M$_3$XH$_8$ system, cage-like
BCN-doped metal atomic systems derived from LaH$_{10}$ structural evolution,
and two-dimensional honeycomb-structured systems evolving from MgB$_2$. The
HTSC-2025 benchmark has been open-sourced at
https://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This
benchmark holds significant importance for accelerating the discovery of
superconducting materials using AI-based methods.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [135] [Plant Bioelectric Early Warning Systems: A Five-Year Investigation into Human-Plant Electromagnetic Communication](https://arxiv.org/abs/2506.04132)
*Peter A. Gloor*

Main category: q-bio.OT

TL;DR: 植物能感知人类的存在和情绪状态，并发出相应的生物电信号。通过深度学习模型，可以高精度分类这些信号，揭示植物具有比传统认知更强的感知能力。


<details>
  <summary>Details</summary>
Motivation: 研究植物对人类存在和情绪状态的生物电反应，探索植物感知能力及其实用价值。

Method: 构建自定义植物传感器，收集植物生物电信号；使用基于ResNet50架构的深度学习模型进行分类分析；对比实验验证模型效果。

Result: 深度学习模型在分类人类情绪状态时准确率达到97%，而随机标签的控制模型仅达到30%；此外，在个体识别等任务中也有显著表现。

Conclusion: 植物具备进化出的早期预警系统，能够通过生物电场变化感知接近的动物，这一发现挑战了传统植物感知能力的理解并提出了新的应用前景。

Abstract: We present a comprehensive investigation into plant bioelectric responses to
human presence and emotional states, building on five years of systematic
research. Using custom-built plant sensors and machine learning classification,
we demonstrate that plants generate distinct bioelectric signals correlating
with human proximity, emotional states, and physiological conditions. A deep
learning model based on ResNet50 architecture achieved 97% accuracy in
classifying human emotional states through plant voltage spectrograms, while
control models with shuffled labels achieved only 30% accuracy. This study
synthesizes findings from multiple experiments spanning 2020-2025, including
individual recognition (66% accuracy), eurythmic gesture detection, stress
prediction, and responses to human voice and movement. We propose that these
phenomena represent evolved anti-herbivory early warning systems, where plants
detect approaching animals through bioelectric field changes before physical
contact. Our results challenge conventional understanding of plant sensory
capabilities and suggest practical applications in agriculture, healthcare, and
human-plant interaction research.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [136] [Sampling Preferences Yields Simple Trustworthiness Scores](https://arxiv.org/abs/2506.03399)
*Sean Steinle*

Main category: cs.HC

TL;DR: With the rise of LLMs, AI model performance has become multi-dimensional. Current evaluation frameworks, while more realistic, complicate decision-making due to lack of a clear way to select an optimal model. This study introduces preference sampling, which extracts a scalar trustworthiness score from multi-dimensional evaluations by considering user-valued characteristics. Preference sampling is shown to be more effective than other methods such as Pareto optimality and averaging scores.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the challenge of selecting an optimal large language model from multi-dimensional evaluation results. With no obvious method to choose the best model based on these evaluations, there is a need for a new approach that can simplify decision-making while taking into account the many characteristics of model performance valued by users.

Method: The method introduced in this paper is called 'preference sampling.' It aims to extract a scalar trustworthiness score from multi-dimensional evaluation results. This method considers various aspects of model performance that are important to users. The effectiveness of preference sampling is demonstrated using multi-dimensional trustworthiness evaluations of LLMs from TrustLLM and DecodingTrust.

Result: The results show that preference sampling is consistently reductive, fully reducing the set of candidate models 100% of the time, whereas Pareto optimality never reduces the set by more than 50%. Additionally, preference sampling is sensitive to user priors, allowing users to specify relative weighting and confidence of their preferences, unlike averaging scores which is rigid and does not adapt to user knowledge.

Conclusion: In conclusion, the study demonstrates that preference sampling offers a significant improvement over alternative aggregation methods when it comes to evaluating large language models. By providing a scalar trustworthiness score that takes into account user-valued characteristics, it simplifies the decision-making process and allows for more informed model selection.

Abstract: With the onset of large language models (LLMs), the performance of artificial
intelligence (AI) models is becoming increasingly multi-dimensional.
Accordingly, there have been several large, multi-dimensional evaluation
frameworks put forward to evaluate LLMs. Though these frameworks are much more
realistic than previous attempts which only used a single score like accuracy,
multi-dimensional evaluations can complicate decision-making since there is no
obvious way to select an optimal model. This work introduces preference
sampling, a method to extract a scalar trustworthiness score from
multi-dimensional evaluation results by considering the many characteristics of
model performance which users value. We show that preference sampling improves
upon alternate aggregation methods by using multi-dimensional trustworthiness
evaluations of LLMs from TrustLLM and DecodingTrust. We find that preference
sampling is consistently reductive, fully reducing the set of candidate models
100% of the time whereas Pareto optimality never reduces the set by more than
50%. Likewise, preference sampling is consistently sensitive to user
priors-allowing users to specify the relative weighting and confidence of their
preferences-whereas averaging scores is intransigent to the users' prior
knowledge.

</details>


### [137] [Crowd-SFT: Crowdsourcing for LLM Alignment](https://arxiv.org/abs/2506.04063)
*Alex Sotiropoulos,Sulyab Thottungal Valapu,Linus Lei,Jared Coleman,Bhaskar Krishnamachari*

Main category: cs.HC

TL;DR: This paper proposes an open, crowd-sourced fine-tuning framework for LLMs that reduces costs, bias, and scalability issues associated with traditional SFT and RLHF methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current SFT and RLHF methods which rely on small, vetted groups of annotators leading to high costs, potential biases, and limited scalability.

Method: An open, crowd-sourced fine-tuning framework is introduced. It includes a point-based reward system aligned with Shapley values for incentive fairness and iterative model updates to guide convergence. Additionally, a multi-model selection framework is employed.

Result: The proposed framework achieves up to a 55% reduction in target distance compared to single-model selection and validates the alignment of the point-based reward mechanism with Shapley values.

Conclusion: The crowd-sourced framework offers a fair and scalable approach for fine-tuning LLMs, promoting broader feedback collection without extensive annotator training.

Abstract: Large Language Models (LLMs) increasingly rely on Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align model
responses with human preferences. While RLHF employs a reinforcement learning
approach with a separate reward model, SFT uses human-curated datasets for
supervised learning. Both approaches traditionally depend on small, vetted
groups of annotators, making them costly, prone to bias, and limited in
scalability. We propose an open, crowd-sourced fine-tuning framework that
addresses these limitations by enabling broader feedback collection for SFT
without extensive annotator training. Our framework promotes incentive fairness
via a point-based reward system correlated with Shapley values and guides model
convergence through iterative model updates. Our multi-model selection
framework demonstrates up to a 55% reduction in target distance over
single-model selection, enabling subsequent experiments that validate our
point-based reward mechanism's close alignment with Shapley values (a
well-established method for attributing individual contributions) thereby
supporting fair and scalable participation.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [138] [chemtrain-deploy: A parallel and scalable framework for machine learning potentials in million-atom MD simulations](https://arxiv.org/abs/2506.04055)
*Paul Fuchs,Weilong Chen,Stephan Thaler,Julija Zavadlav*

Main category: physics.comp-ph

TL;DR: chemtrain-deploy是一个支持在LAMMPS中部署机器学习势函数（MLP）的框架，能够实现跨多个GPU的大规模MD模拟，适用于多种系统和架构。


<details>
  <summary>Details</summary>
Motivation: 现有的MLP软件工具通常与特定架构绑定、缺乏与标准MD包的集成或无法跨GPU并行化，限制了其广泛应用。

Method: 开发了一个名为chemtrain-deploy的框架，支持任何JAX定义的半局域势，并允许用户利用LAMMPS的功能进行大规模基于MLP的MD模拟。

Result: 通过使用图神经网络架构（如MACE、Allegro和PaiNN）验证了该框架的性能和可扩展性，适用于液-气界面、晶体材料和溶剂化肽等多种系统。

Conclusion: chemtrain-deploy展示了其在实际高性能模拟中的实用价值，并为MLP架构的选择和未来设计提供了指导。

Abstract: Machine learning potentials (MLPs) have advanced rapidly and show great
promise to transform molecular dynamics (MD) simulations. However, most
existing software tools are tied to specific MLP architectures, lack
integration with standard MD packages, or are not parallelizable across GPUs.
To address these challenges, we present chemtrain-deploy, a framework that
enables model-agnostic deployment of MLPs in LAMMPS. chemtrain-deploy supports
any JAX-defined semi-local potential, allowing users to exploit the
functionality of LAMMPS and perform large-scale MLP-based MD simulations on
multiple GPUs. It achieves state-of-the-art efficiency and scales to systems
containing millions of atoms. We validate its performance and scalability using
graph neural network architectures, including MACE, Allegro, and PaiNN, applied
to a variety of systems, such as liquid-vapor interfaces, crystalline
materials, and solvated peptides. Our results highlight the practical utility
of chemtrain-deploy for real-world, high-performance simulations and provide
guidance for MLP architecture selection and future design.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [139] [Multi-Spectral Gaussian Splatting with Neural Color Representation](https://arxiv.org/abs/2506.03407)
*Lukas Meyer,Josef Grün,Maximilian Weiherer,Bernhard Egger,Marc Stamminger,Linus Franke*

Main category: cs.GR

TL;DR: The paper introduces MS-Splatting, a multi-spectral 3D Gaussian Splatting framework that generates novel views from images of multiple cameras in different spectral domains without cross-modal camera calibration.


<details>
  <summary>Details</summary>
Motivation: To create a versatile method for generating multi-view consistent novel views from images across different spectral domains without the need for cross-modal camera calibration.

Method: MS-Splatting uses a neural color representation to encode multi-spectral information into a compact feature embedding per splat. A shallow MLP decodes this embedding to obtain spectral color values, allowing joint learning of all bands within a unified representation.

Result: Experiments indicate that the strategy improves multi-spectral rendering quality and per-spectra rendering quality over state-of-the-art methods.

Conclusion: MS-Splatting effectively enhances multi-spectral rendering quality and is demonstrated to be useful in agricultural applications, such as rendering vegetation indices like NDVI.

Abstract: We present MS-Splatting -- a multi-spectral 3D Gaussian Splatting (3DGS)
framework that is able to generate multi-view consistent novel views from
images of multiple, independent cameras with different spectral domains. In
contrast to previous approaches, our method does not require cross-modal camera
calibration and is versatile enough to model a variety of different spectra,
including thermal and near-infra red, without any algorithmic changes.
  Unlike existing 3DGS-based frameworks that treat each modality separately (by
optimizing per-channel spherical harmonics) and therefore fail to exploit the
underlying spectral and spatial correlations, our method leverages a novel
neural color representation that encodes multi-spectral information into a
learned, compact, per-splat feature embedding. A shallow multi-layer perceptron
(MLP) then decodes this embedding to obtain spectral color values, enabling
joint learning of all bands within a unified representation.
  Our experiments show that this simple yet effective strategy is able to
improve multi-spectral rendering quality, while also leading to improved
per-spectra rendering quality over state-of-the-art methods. We demonstrate the
effectiveness of this new technique in agricultural applications to render
vegetation indices, such as normalized difference vegetation index (NDVI).

</details>


### [140] [SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting](https://arxiv.org/abs/2506.03594)
*Shengjie Lin,Jiading Fang,Muhammad Zubair Irshad,Vitor Campagnolo Guizilini,Rares Andrei Ambrus,Greg Shakhnarovich,Matthew R. Walter*

Main category: cs.GR

TL;DR: SplArt is a self-supervised framework that uses 3D Gaussian Splatting to reconstruct articulated objects and infer kinematics from posed RGB images, enabling real-time photorealistic rendering without needing 3D annotations or category-specific priors.


<details>
  <summary>Details</summary>
Motivation: Reconstructing articulated objects is important for applications like augmented/virtual reality and robotics, but existing methods have limitations in scalability, robustness, and rendering quality.

Method: The method employs a multi-stage optimization strategy with 3D Gaussian Splatting, augmented with a differentiable mobility parameter per Gaussian. This allows refined part segmentation and progressive handling of reconstruction, part segmentation, and articulation estimation.

Result: Evaluations on established benchmarks and real-world scenarios show SplArt's state-of-the-art performance and practicality.

Conclusion: SplArt addresses the challenges faced by existing methods through geometric self-supervision, making it effective for reconstructing articulated objects and inferring kinematics without 3D annotations or category-specific priors.

Abstract: Reconstructing articulated objects prevalent in daily environments is crucial
for applications in augmented/virtual reality and robotics. However, existing
methods face scalability limitations (requiring 3D supervision or costly
annotations), robustness issues (being susceptible to local optima), and
rendering shortcomings (lacking speed or photorealism). We introduce SplArt, a
self-supervised, category-agnostic framework that leverages 3D Gaussian
Splatting (3DGS) to reconstruct articulated objects and infer kinematics from
two sets of posed RGB images captured at different articulation states,
enabling real-time photorealistic rendering for novel viewpoints and
articulations. SplArt augments 3DGS with a differentiable mobility parameter
per Gaussian, achieving refined part segmentation. A multi-stage optimization
strategy is employed to progressively handle reconstruction, part segmentation,
and articulation estimation, significantly enhancing robustness and accuracy.
SplArt exploits geometric self-supervision, effectively addressing challenging
scenarios without requiring 3D annotations or category-specific priors.
Evaluations on established and newly proposed benchmarks, along with
applications to real-world scenarios using a handheld RGB camera, demonstrate
SplArt's state-of-the-art performance and real-world practicality. Code is
publicly available at https://github.com/ripl/splart.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [141] [Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization](https://arxiv.org/abs/2506.03467)
*Hang Liu,Anna Scaglione,Sean Peisert*

Main category: cs.IT

TL;DR: 本文提出了一种基于差分隐私（DP）的高斯混合模型（GMM）参数发布方法，通过优化Kullback-Leibler (KL) 散度，在确保隐私的同时保持了模型的高实用性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究发现，发布GMM参数可能带来显著的隐私风险，从而暴露底层数据中的敏感信息。因此，需要一种能够在保证差分隐私的同时，保护GMM参数的方法。

Method: 作者引入了一种差分隐私机制，该机制向GMM参数添加精心校准的随机扰动，以实现隐私保护。同时，采用KL散度作为效用度量标准来评估发布的GMM准确性，并通过理论分析量化隐私预算分配和扰动统计对DP保障的影响，推导出计算KL散度的可行表达式。此外，还构建并解决了一个优化问题，以在给定的(ϵ, δ)-DP约束下最小化发布的和原始模型之间的KL散度。

Result: 广泛的实验表明，所提出的方法在合成和真实世界的数据集上均能提供强大的隐私保障，同时维持较高的模型效用。

Conclusion: 本研究成功解决了在确保差分隐私的前提下发布GMM参数的挑战，为多模态数据分布表示提供了实用且隐私友好的解决方案。

Abstract: Gaussian Mixture Models (GMMs) are widely used statistical models for
representing multi-modal data distributions, with numerous applications in data
mining, pattern recognition, data simulation, and machine learning. However,
recent research has shown that releasing GMM parameters poses significant
privacy risks, potentially exposing sensitive information about the underlying
data. In this paper, we address the challenge of releasing GMM parameters while
ensuring differential privacy (DP) guarantees. Specifically, we focus on the
privacy protection of mixture weights, component means, and covariance
matrices. We propose to use Kullback-Leibler (KL) divergence as a utility
metric to assess the accuracy of the released GMM, as it captures the joint
impact of noise perturbation on all the model parameters. To achieve privacy,
we introduce a DP mechanism that adds carefully calibrated random perturbations
to the GMM parameters. Through theoretical analysis, we quantify the effects of
privacy budget allocation and perturbation statistics on the DP guarantee, and
derive a tractable expression for evaluating KL divergence. We formulate and
solve an optimization problem to minimize the KL divergence between the
released and original models, subject to a given $(\epsilon, \delta)$-DP
constraint. Extensive experiments on both synthetic and real-world datasets
demonstrate that our approach achieves strong privacy guarantees while
maintaining high utility.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [142] [UniSim: A Unified Simulator for Time-Coarsened Dynamics of Biomolecules](https://arxiv.org/abs/2506.03157)
*Ziyang Yu,Wenbing Huang,Yang Liu*

Main category: q-bio.BM

TL;DR: The paper introduces UniSim, a Molecular Dynamics simulation method using cross-domain knowledge and multi-head pretraining for enhanced atomic interaction understanding.


<details>
  <summary>Details</summary>
Motivation: Classical MD techniques face accuracy-efficiency trade-offs and deep learning improvements mainly focus on single-domain molecules with poor transferability.

Method: Uses multi-head pretraining to create unified atomic representation from diverse molecular data and applies stochastic interpolant framework with a force guidance module for state transition pattern learning.

Result: Experiments show competitive performance in small molecules, peptides, and proteins.

Conclusion: UniSim leverages cross-domain knowledge to improve understanding of atomic interactions across various molecular systems.

Abstract: Molecular Dynamics (MD) simulations are essential for understanding the
atomic-level behavior of molecular systems, giving insights into their
transitions and interactions. However, classical MD techniques are limited by
the trade-off between accuracy and efficiency, while recent deep learning-based
improvements have mostly focused on single-domain molecules, lacking
transferability to unfamiliar molecular systems. Therefore, we propose
\textbf{Uni}fied \textbf{Sim}ulator (UniSim), which leverages cross-domain
knowledge to enhance the understanding of atomic interactions. First, we employ
a multi-head pretraining approach to learn a unified atomic representation
model from a large and diverse set of molecular data. Then, based on the
stochastic interpolant framework, we learn the state transition patterns over
long timesteps from MD trajectories, and introduce a force guidance module for
rapidly adapting to different chemical environments. Our experiments
demonstrate that UniSim achieves highly competitive performance across small
molecules, peptides, and proteins.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [143] [Software Bill of Materials in Software Supply Chain Security A Systematic Literature Review](https://arxiv.org/abs/2506.03507)
*Eric O'Donoghue,Yvette Hastings,Ernesto Ortiz,A. Redempta Manzi Muneza*

Main category: cs.SE

TL;DR: SBOMs are crucial for SSC security, used in 5 areas but face 11 adoption barriers. Mapping to ISO/IEC model reveals trustworthiness and usability issues. Key gaps include ML application and quality assurance techniques.


<details>
  <summary>Details</summary>
Motivation: To understand the real-world use of SBOMs and identify adoption barriers for securing software supply chains.

Method: Systematic literature review synthesizing evidence from 40 peer-reviewed studies.

Result: Five primary application areas of SBOMs identified along with eleven significant barriers to adoption. Barriers were mapped to the ISO/IEC 25019:2023 Quality-in-Use model revealing critical deficiencies.

Conclusion: SBOMs show promise for enhancing SSC security but face numerous challenges. Insights provided can guide future research and development in this area.

Abstract: Software Bill of Materials (SBOMs) are increasingly regarded as essential
tools for securing software supply chains (SSCs), yet their real-world use and
adoption barriers remain poorly understood. This systematic literature review
synthesizes evidence from 40 peer-reviewed studies to evaluate how SBOMs are
currently used to bolster SSC security. We identify five primary application
areas: vulnerability management, transparency, component assessment, risk
assessment, and SSC integrity. Despite clear promise, adoption is hindered by
significant barriers: generation tooling, data privacy, format/standardization,
sharing/distribution, cost/overhead, vulnerability exploitability, maintenance,
analysis tooling, false positives, hidden packages, and tampering. To structure
our analysis, we map these barriers to the ISO/IEC 25019:2023 Quality-in-Use
model, revealing critical deficiencies in SBOM trustworthiness, usability, and
suitability for security tasks. We also highlight key gaps in the literature.
These include the absence of applying machine learning techniques to assess
SBOMs and limited evaluation of SBOMs and SSCs using software quality assurance
techniques. Our findings provide actionable insights for researchers, tool
developers, and practitioners seeking to advance SBOM-driven SSC security and
lay a foundation for future work at the intersection of SSC assurance,
automation, and empirical software engineering.

</details>


### [144] [VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code Generation](https://arxiv.org/abs/2506.03930)
*Yuansheng Ni,Ping Nie,Kai Zou,Xiang Yue,Wenhu Chen*

Main category: cs.SE

TL;DR: The paper introduces VisCode-200K, a large-scale Python-based visualization and self-correction dataset. It contains validated plotting code and multi-turn correction dialogues. By fine-tuning Qwen2.5-Coder-Instruct on this dataset, they create VisCoder which outperforms open-source baselines and approaches proprietary models' performance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing instruction-tuning datasets that lack execution-grounded supervision and support for iterative code correction in visualization tasks.

Method: Created VisCode-200K dataset with 200K examples from open-source repositories and correction dialogues. Fine-tuned Qwen2.5-Coder-Instruct on this dataset to create VisCoder.

Result: VisCoder significantly outperforms open-source baselines and approaches the performance of proprietary models like GPT-4o-mini on PandasPlotBench.

Conclusion: Adopting a self-debug evaluation protocol shows the benefits of feedback-driven learning for accurate code generation.

Abstract: Large language models (LLMs) often struggle with visualization tasks like
plotting diagrams, charts, where success depends on both code correctness and
visual semantics. Existing instruction-tuning datasets lack execution-grounded
supervision and offer limited support for iterative code correction, resulting
in fragile and unreliable plot generation. We present VisCode-200K, a
large-scale instruction tuning dataset for Python-based visualization and
self-correction. It contains over 200K examples from two sources: (1) validated
plotting code from open-source repositories, paired with natural language
instructions and rendered plots; and (2) 45K multi-turn correction dialogues
from Code-Feedback, enabling models to revise faulty code using runtime
feedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create
VisCoder, and evaluate it on PandasPlotBench. VisCoder significantly
outperforms strong open-source baselines and approaches the performance of
proprietary models like GPT-4o-mini. We further adopt a self-debug evaluation
protocol to assess iterative repair, demonstrating the benefits of
feedback-driven learning for executable, visually accurate code generation.

</details>


### [145] [Generating Automotive Code: Large Language Models for Software Development and Verification in Safety-Critical Systems](https://arxiv.org/abs/2506.04038)
*Sven Kirchner,Alois C. Knoll*

Main category: cs.SE

TL;DR: The paper introduces a framework that integrates Generative Artificial Intelligence (GenAI) into the Software Development Lifecycle (SDLC) for safety-critical automotive software.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in developing safety-critical automotive software due to increasing system complexity and strict regulatory demands.

Method: Proposes a novel framework using Large Language Models (LLMs) to automate code generation in C++, with practices like static verification, test-driven development, and iterative refinement. Includes a feedback-driven pipeline for integration of test, simulation, and verification.

Result: Validation through the development of an Adaptive Cruise Control (ACC) system showed that the framework enables automatic code generation while ensuring compliance with safety-critical requirements.

Conclusion: This work advances the use of AI in safety-critical domains, bridging the gap between state-of-the-art generative models and real-world safety requirements.

Abstract: Developing safety-critical automotive software presents significant
challenges due to increasing system complexity and strict regulatory demands.
This paper proposes a novel framework integrating Generative Artificial
Intelligence (GenAI) into the Software Development Lifecycle (SDLC). The
framework uses Large Language Models (LLMs) to automate code generation in
languages such as C++, incorporating safety-focused practices such as static
verification, test-driven development and iterative refinement. A
feedback-driven pipeline ensures the integration of test, simulation and
verification for compliance with safety standards. The framework is validated
through the development of an Adaptive Cruise Control (ACC) system. Comparative
benchmarking of LLMs ensures optimal model selection for accuracy and
reliability. Results demonstrate that the framework enables automatic code
generation while ensuring compliance with safety-critical requirements,
systematically integrating GenAI into automotive software engineering. This
work advances the use of AI in safety-critical domains, bridging the gap
between state-of-the-art generative models and real-world safety requirements.

</details>


### [146] [From Theory to Practice: Real-World Use Cases on Trustworthy LLM-Driven Process Modeling, Prediction and Automation](https://arxiv.org/abs/2506.03801)
*Peter Pfeiffer,Alexander Rombach,Maxim Majlatow,Nijat Mehdiyev*

Main category: cs.SE

TL;DR: The paper explores four real-world use cases showing how LLMs redefine process modeling, prediction, and automation in manufacturing, modeling, life-science, and design processes. It advocates for context-sensitive integration prioritizing domain needs, stakeholder values, and iterative human-in-the-loop workflows.


<details>
  <summary>Details</summary>
Motivation: Traditional BPM faces challenges such as rigidity, opacity, and scalability issues in dynamic environments. LLMs present both opportunities and risks to address these challenges.

Method: Four real-world use cases are examined: an LLM-driven framework in manufacturing integrating uncertainty-aware explainable ML with interactive dialogues; conversational interfaces for process modeling; pharmacovigilance agents using knowledge-graph-augmented LLMs; and multi-agent systems for sustainable textile design.

Result: LLMs can effectively transform process modeling, prediction, and automation when augmented with trustworthy process intelligence. The tensions between transparency and efficiency, generalization and specialization, and human agency versus automation are identified.

Conclusion: Context-sensitive integration of LLMs in BPM should prioritize domain needs, stakeholder values, and iterative human-in-the-loop workflows rather than seeking universal solutions.

Abstract: Traditional Business Process Management (BPM) struggles with rigidity,
opacity, and scalability in dynamic environments while emerging Large Language
Models (LLMs) present transformative opportunities alongside risks. This paper
explores four real-world use cases that demonstrate how LLMs, augmented with
trustworthy process intelligence, redefine process modeling, prediction, and
automation. Grounded in early-stage research projects with industrial partners,
the work spans manufacturing, modeling, life-science, and design processes,
addressing domain-specific challenges through human-AI collaboration. In
manufacturing, an LLM-driven framework integrates uncertainty-aware explainable
Machine Learning (ML) with interactive dialogues, transforming opaque
predictions into auditable workflows. For process modeling, conversational
interfaces democratize BPMN design. Pharmacovigilance agents automate drug
safety monitoring via knowledge-graph-augmented LLMs. Finally, sustainable
textile design employs multi-agent systems to navigate regulatory and
environmental trade-offs. We intend to examine tensions between transparency
and efficiency, generalization and specialization, and human agency versus
automation. By mapping these trade-offs, we advocate for context-sensitive
integration prioritizing domain needs, stakeholder values, and iterative
human-in-the-loop workflows over universal solutions. This work provides
actionable insights for researchers and practitioners aiming to operationalize
LLMs in critical BPM environments.

</details>


### [147] [CETBench: A Novel Dataset constructed via Transformations over Programs for Benchmarking LLMs for Code-Equivalence Checking](https://arxiv.org/abs/2506.04019)
*Neeva Oza,Ishaan Govil,Parul Gupta,Dinesh Khandelwal,Dinesh Garg,Parag Singla*

Main category: cs.SE

TL;DR: 这篇论文探讨了大语言模型（LLMs）在代码等价性检查任务中的应用，发现简单的代码转换会导致LLMs性能显著下降，并提出了一种微调方法来提升其性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自动代码生成方面得到了广泛应用，但它们在代码等价性检查这一相对未被探索的任务上的表现尚不清楚。

Method: 构建了一个名为CETBench的数据集，通过程序库中的代码对进行一系列预定义的代码转换，生成等价或非等价的代码对。然后分析LLMs在此数据集上的表现，并使用微调方法提高LLMs的性能。

Result: 实验结果表明，简单的代码转换会导致LLMs在代码等价性检查任务上的性能显著下降，而微调方法可以有效提升其性能。此外，研究还揭示了LLMs在代码语义理解方面的不足。

Conclusion: LLMs在代码等价性检查任务上存在局限性，可能尚未达到真正的代码语义理解水平，而微调方法可以改善其性能。

Abstract: LLMs have been extensively used for the task of automated code generation. In
this work, we examine the applicability of LLMs for the related but relatively
unexplored task of code-equivalence checking, i.e., given two programs, whether
they are functionally equivalent or not. This is an important problem since
benchmarking code equivalence can play a critical role in evaluating LLM
capabilities for tasks such as code re-writing and code translation. Towards
this end, we present CETBench - Code Equivalence with Transformations
Benchmark, constructed via a repository of programs, where two programs in the
repository may be solving the same or different tasks. Each instance in our
dataset is obtained by taking a pair of programs in the repository and applying
a random series of pre-defined code transformations, resulting in
(non-)equivalent pairs. Our analysis on this dataset reveals a surprising
finding that very simple code transformations in the underlying pair of
programs can result in a significant drop in performance of SOTA LLMs for the
task of code-equivalence checking. To remedy this, we present a simple
fine-tuning-based approach to boost LLM performance on the transformed pairs of
programs. Our approach for dataset generation is generic, and can be used with
repositories with varying program difficulty levels and allows for applying
varying numbers as well as kinds of transformations. In our experiments, we
perform ablations over the difficulty level of original programs, as well as
the kind of transformations used in generating pairs for equivalence checking.
Our analysis presents deep insights into the working of LLMs for the task of
code-equivalence, and points to the fact that they may still be far from what
could be termed as a semantic understanding of the underlying code.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [148] [A Data-Driven Diffusion-based Approach for Audio Deepfake Explanations](https://arxiv.org/abs/2506.03425)
*Petr Grinberg,Ankur Kumar,Surya Koppisetti,Gaurav Bharaj*

Main category: eess.AS

TL;DR: 研究人员提出了一种新的数据驱动方法，通过使用时间频率表示的差异作为监督信号训练扩散模型来揭示深度伪造音频中的伪影区域，该方法在VocV4和LibriSeVoc数据集上表现优于传统的可解释性技术。


<details>
  <summary>Details</summary>
Motivation: 当前在音频深度伪造检测中评估可解释性技术（如SHAP和LRP）具有挑战性，因为缺乏明确的地面真实注释，并且即使有地面真实情况，这些方法也难以提供准确的解释。

Method: 提出了一种新的数据驱动方法，利用配对的真实音频和编码音频之间的时间频率表示差异作为地面真实解释，并以此差异信号作为监督来训练扩散模型以揭示深度伪造音频中的伪影。

Result: 实验结果表明，在VocV4和LibriSeVoc数据集上，该方法在定性和定量方面均优于传统的可解释性技术。

Conclusion: 所提出的方法能够更有效地识别深度伪造音频中的伪影区域，相比传统技术具有更好的性能。

Abstract: Evaluating explainability techniques, such as SHAP and LRP, in the context of
audio deepfake detection is challenging due to lack of clear ground truth
annotations. In the cases when we are able to obtain the ground truth, we find
that these methods struggle to provide accurate explanations. In this work, we
propose a novel data-driven approach to identify artifact regions in deepfake
audio. We consider paired real and vocoded audio, and use the difference in
time-frequency representation as the ground-truth explanation. The difference
signal then serves as a supervision to train a diffusion model to expose the
deepfake artifacts in a given vocoded audio. Experimental results on the VocV4
and LibriSeVoc datasets demonstrate that our method outperforms traditional
explainability techniques, both qualitatively and quantitatively.

</details>


### [149] [Tone recognition in low-resource languages of North-East India: peeling the layers of SSL-based speech models](https://arxiv.org/abs/2506.03606)
*Parismita Gogoi,Sishir Kalita,Wendy Lalhminghlui,Viyazonuo Terhiija,Moakala Tzudir,Priyankoo Sarmah,S. R. M. Prasanna*

Main category: eess.AS

TL;DR: This study explores the use of self-supervised learning (SSL) models for tone recognition in three low-resource languages and finds that tone inventory, tone types, and dialectal variations affect tone recognition.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of self-supervised learning models for tone recognition in low-resource tonal languages from North Eastern India.

Method: Evaluate four Wav2vec2.0 base models pre-trained on both tonal and non-tonal languages by analyzing tone-wise performance across layers for Angami, Ao, and Mizo languages.

Result: Tone recognition performs best for Mizo and worst for Angami. The middle layers of SSL models are most important for tone recognition irrespective of pre-training language being tonal or non-tonal. Tone inventory, tone types, and dialectal variations impact tone recognition.

Conclusion: The findings offer insights into strengths and weaknesses of SSL-based embeddings for tonal languages and indicate potential for improving tone recognition in low-resource settings.

Abstract: This study explores the use of self-supervised learning (SSL) models for tone
recognition in three low-resource languages from North Eastern India: Angami,
Ao, and Mizo. We evaluate four Wav2vec2.0 base models that were pre-trained on
both tonal and non-tonal languages. We analyze tone-wise performance across the
layers for all three languages and compare the different models. Our results
show that tone recognition works best for Mizo and worst for Angami. The middle
layers of the SSL models are the most important for tone recognition,
regardless of the pre-training language, i.e. tonal or non-tonal. We have also
found that the tone inventory, tone types, and dialectal variations affect tone
recognition. These findings provide useful insights into the strengths and
weaknesses of SSL-based embeddings for tonal languages and highlight the
potential for improving tone recognition in low-resource settings. The source
code is available at GitHub 1 .

</details>


### [150] [BitTTS: Highly Compact Text-to-Speech Using 1.58-bit Quantization and Weight Indexing](https://arxiv.org/abs/2506.03515)
*Masaya Kawamura,Takuya Hasumi,Yuma Shirahata,Ryuichi Yamamoto*

Main category: eess.AS

TL;DR: This paper proposes a compact, lightweight text-to-speech (TTS) model for on-device applications which uses quantization-aware training and weight indexing to reduce model size by 83% without sacrificing synthesis quality.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to develop a highly compact, lightweight TTS model that can be effectively used in on-device applications where storage and computational resources are limited.

Method: The method involves introducing two techniques: quantization-aware training (QAT), which quantizes model parameters to as low as 1.58-bit during training, and weight indexing, where a group of 1.58-bit weights are saved as a single int8 index for efficient storage.

Result: The proposed method achieved an 83% reduction in model size while outperforming the baseline of similar model size without quantization in terms of synthesis quality.

Conclusion: The paper concludes that the proposed techniques significantly reduce the model size without compromising on the synthesis quality, making it suitable for on-device applications.

Abstract: This paper proposes a highly compact, lightweight text-to-speech (TTS) model
for on-device applications. To reduce the model size, the proposed model
introduces two techniques. First, we introduce quantization-aware training
(QAT), which quantizes model parameters during training to as low as 1.58-bit.
In this case, most of 32-bit model parameters are quantized to ternary values
{-1, 0, 1}. Second, we propose a method named weight indexing. In this method,
we save a group of 1.58-bit weights as a single int8 index. This allows for
efficient storage of model parameters, even on hardware that treats values in
units of 8-bit. Experimental results demonstrate that the proposed method
achieved 83 % reduction in model size, while outperforming the baseline of
similar model size without quantization in synthesis quality.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [151] [Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally Interdependent Multi-Agent MDPs](https://arxiv.org/abs/2506.04215)
*Alex DeWeese,Guannan Qu*

Main category: cs.MA

TL;DR: 本研究提出了扩展截止策略类（Extended Cutoff Policy Class），这是首个在局部相互依赖的多智能体MDP中，针对任何小且固定的可见性情况，能够以指数接近最优解的非平凡闭式部分可观测策略类。这些策略通过记忆超出可见范围的智能体，显著提升了性能，解决了“惩罚抖动”问题，并在某些情况下保证了完全可观测的联合最优行为。此外，研究还推广了局部相互依赖的多智能体MDP模型，引入了转移依赖和扩展奖励依赖，并在此设置下复制了理论结果。


<details>
  <summary>Details</summary>
Motivation: Dec-POMDPs问题是NEXP-Complete且难以求解，但在诸如合作导航、障碍物规避和编队控制等问题中，可以对局部可见性和局部依赖性做出基本假设。先前的研究虽然提出了三种在不同情况下可计算且接近最优的闭式策略，但当可见性小且固定时，这些策略可能表现不佳并因“惩罚抖动”现象而卡住。因此，需要一种新的策略类来解决这些问题。

Method: 研究者建立了扩展截止策略类，该类策略能够在小且固定的可见性设置下表现出色，解决“惩罚抖动”问题，并在某些情况下保证完全可观测的联合最优行为。同时，研究还提出了一种广义形式的局部相互依赖多智能体MDP，允许转移依赖和扩展奖励依赖，并在该设置下复制了理论结果。

Result: 扩展截止策略类是首个在局部相互依赖多智能体MDP中，对于任何小且固定的可见性情况，能够以指数接近最优解的非平凡闭式部分可观测策略类。这些策略通过记忆超出可见范围的智能体，显著提升了性能，并解决了“惩罚抖动”问题。此外，在广义模型中也验证了理论结果的有效性。

Conclusion: 扩展截止策略类为局部相互依赖多智能体MDP提供了一种新的解决方案，尤其在小且固定的可见性情况下表现优异，解决了现有策略的不足之处。同时，提出的广义模型进一步增强了模型的适用性。

Abstract: Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are
known to be NEXP-Complete and intractable to solve. However, for problems such
as cooperative navigation, obstacle avoidance, and formation control, basic
assumptions can be made about local visibility and local dependencies. The work
DeWeese and Qu 2024 formalized these assumptions in the construction of the
Locally Interdependent Multi-Agent MDP. In this setting, it establishes three
closed-form policies that are tractable to compute in various situations and
are exponentially close to optimal with respect to visibility. However, it is
also shown that these solutions can have poor performance when the visibility
is small and fixed, often getting stuck during simulations due to the so called
"Penalty Jittering" phenomenon. In this work, we establish the Extended Cutoff
Policy Class which is, to the best of our knowledge, the first non-trivial
class of near optimal closed-form partially observable policies that are
exponentially close to optimal with respect to the visibility for any Locally
Interdependent Multi-Agent MDP. These policies are able to remember agents
beyond their visibilities which allows them to perform significantly better in
many small and fixed visibility settings, resolve Penalty Jittering
occurrences, and under certain circumstances guarantee fully observable joint
optimal behavior despite the partial observability. We also propose a
generalized form of the Locally Interdependent Multi-Agent MDP that allows for
transition dependence and extended reward dependence, then replicate our
theoretical results in this setting.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [152] [Beware! The AI Act Can Also Apply to Your AI Research Practices](https://arxiv.org/abs/2506.03218)
*Alina Wernick,Kristof Meding*

Main category: cs.CY

TL;DR: The EU AI Act, enforced in 2024, imposes obligations on AI system providers that could hinder scientific research. This position paper explains the act's relevance to AI researchers, analyzes its exceptions, and proposes changes for better legal certainty while offering recommendations to reduce compliance risks.


<details>
  <summary>Details</summary>
Motivation: To clarify the applicability of the EU AI Act to AI researchers and address potential obstacles it poses to scientific research.

Method: Providing an introduction to the AI Act for non-legal researchers, using research examples to demonstrate its applicability, analyzing exceptions within the act, and proposing modifications and recommendations.

Result: Showed that the AI Act applies more broadly to AI research than previously thought, identified shortcomings in current exceptions, and suggested improvements for legal clarity.

Conclusion: This paper aims to initiate a dialogue among policymakers, legal experts, and AI researchers to mitigate negative impacts of the AI Act on scientific research.

Abstract: The EU has become one of the vanguards in regulating the digital age. A
particularly important regulation in the Artificial Intelligence (AI) domain is
the EU AI Act, which entered into force in 2024. The AI Act specifies -- due to
a risk-based approach -- various obligations for providers of AI systems. These
obligations, for example, include a cascade of documentation and compliance
measures, which represent a potential obstacle to science. But do these
obligations also apply to AI researchers? This position paper argues that,
indeed, the AI Act's obligations could apply in many more cases than the AI
community is aware of. In our analysis of the AI Act and its applicability, we
contribute the following: 1.) We give a high-level introduction to the AI Act
aimed at non-legal AI research scientists. 2.) We explain with everyday
research examples why the AI Act applies to research. 3.) We analyse the
exceptions of the AI Act's applicability and state that especially scientific
research exceptions fail to account for current AI research practices. 4.) We
propose changes to the AI Act to provide more legal certainty for AI
researchers and give two recommendations for AI researchers to reduce the risk
of not complying with the AI Act. We see our paper as a starting point for a
discussion between policymakers, legal scholars, and AI researchers to avoid
unintended side effects of the AI Act on research.

</details>


### [153] [Misalignment or misuse? The AGI alignment tradeoff](https://arxiv.org/abs/2506.03755)
*Max Hellrigel-Holderbaum,Leonard Dung*

Main category: cs.CY

TL;DR: Creating AI systems aligned with human goals is crucial for safe AI. However, both misaligned AGI and aligned AGI pose catastrophic risks. The paper argues that while alignment techniques reduce misalignment risks, they may increase misuse risks by humans. Therefore, robustness, AI control methods, and good governance are essential to mitigate these risks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the dual risks posed by AGI: the risk of misalignment leading to catastrophic outcomes and the risk of aligned AGI being misused by humans. It seeks to explore the balance between these two risks in the context of AI alignment approaches.

Method: The method involves defending the view on the catastrophic risks of misaligned AGI, supporting the view on the risks of aligned AGI misuse, investigating empirical tradeoffs between misalignment and misuse for different technical AI alignment approaches, and discussing important social factors impacting AI risks.

Result: The result shows that many current alignment techniques and foreseeable improvements plausibly increase risks of catastrophic misuse. There is room for alignment approaches which do not increase misuse risk, but this requires essential techniques such as robustness, AI control methods, and especially good governance.

Conclusion: In conclusion, while aligning AI with human goals is critical for safety, it is equally important to consider the potential for misuse. Reducing the risk of a misuse catastrophe due to aligned AGI requires a focus on robustness, AI control methods, and good governance.

Abstract: Creating systems that are aligned with our goals is seen as a leading
approach to create safe and beneficial AI in both leading AI companies and the
academic field of AI safety. We defend the view that misaligned AGI - future,
generally intelligent (robotic) AI agents - poses catastrophic risks. At the
same time, we support the view that aligned AGI creates a substantial risk of
catastrophic misuse by humans. While both risks are severe and stand in tension
with one another, we show that - in principle - there is room for alignment
approaches which do not increase misuse risk. We then investigate how the
tradeoff between misalignment and misuse looks empirically for different
technical approaches to AI alignment. Here, we argue that many current
alignment techniques and foreseeable improvements thereof plausibly increase
risks of catastrophic misuse. Since the impacts of AI depend on the social
context, we close by discussing important social factors and suggest that to
reduce the risk of a misuse catastrophe due to aligned AGI, techniques such as
robustness, AI control methods and especially good governance seem essential.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [154] [NetPress: Dynamically Generated LLM Benchmarks for Network Applications](https://arxiv.org/abs/2506.03231)
*Yajie Zhou,Jiajun Ruan,Eric S. Wang,Sadjad Fouladi,Francis Y. Yan,Kevin Hsieh,Zaoxing Liu*

Main category: cs.NI

TL;DR: An automated benchmark generation framework named NetPress is introduced for evaluating LLM agents in network applications, featuring dynamic query set generation and integration with network emulators.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs are limited to static, small-scale datasets which do not provide sufficient reliability for high-stakes tasks like network operations.

Method: NetPress uses a unified abstraction with state and action for dynamic generation of diverse query sets and corresponding ground truths. It also integrates with network emulators to offer realistic environment feedback for comprehensive evaluation across correctness, safety, and latency.

Result: NetPress was instantiated on three representative applications, uncovering fine-grained differences in agent behavior that are often missed by static, correctness-only benchmarks.

Conclusion: NetPress advances LLM evaluation towards more realistic and scalable testing in infrastructure-centric domains, bridging the gap between benchmark performance and real-world deployment readiness.

Abstract: Despite growing interest in domain-specific benchmarking of large language
models (LLMs) and agents, current evaluations remain limited to static,
small-scale datasets, especially in high-stakes tasks like network operations
that demand reliability for deployments. We present NetPress, an automated
benchmark generation framework for evaluating LLM agents in network
applications. NetPress introduces a unified abstraction with state and action,
enabling dynamic generation of diverse query sets along with corresponding
ground truths. At runtime, users can specify benchmark configurations to
generate millions of queries on the fly. In addition to dynamic benchmark
construction, NetPress integrates with network emulators to provide realistic
environment feedback, supporting comprehensive evaluation across correctness,
safety, and latency. We instantiate NetPress on three representative
applications, revealing interesting fine-grained differences in agent behavior
that static, correctness-only benchmarks often miss. NetPress moves LLM
evaluation toward realistic, scalable testing in infrastructure-centric
domains, helping close the gap between benchmark performance and real-world
deployment readiness. Code is available at
https://github.com/Froot-NetSys/NetPress.

</details>


### [155] [Distributionally Robust Wireless Semantic Communication with Large AI Models](https://arxiv.org/abs/2506.03167)
*Long Tan Le,Senura Hansaja Wanasekara,Zerun Niu,Yansong Shi,Nguyen H. Tran,Phuong Vo,Walid Saad,Dusit Niyato,Zhu Han,Choong Seon Hong,H. Vincent Poor*

Main category: cs.NI

TL;DR: In this paper, the authors propose WaSeCom, a novel semantic communication framework that uses Wasserstein distributionally robust optimization to improve robustness against noise and adversarial perturbations in 6G wireless systems.


<details>
  <summary>Details</summary>
Motivation: The motivation for this work is the limitation of conventional bit-level transmission strategies in supporting the efficiency and adaptability required by modern data-intensive applications. Semantic communication (SemCom) has been proposed as a solution but existing systems are vulnerable to semantic-level and transmission-level noise due to reliance on domain-specific architectures.

Method: The method involves proposing a generalized semantic communication framework called WaSeCom. This framework employs Wasserstein distributionally robust optimization to enhance resilience against semantic misinterpretation and channel perturbations. A theoretical analysis establishes the robust generalization guarantees of the framework.

Result: Experimental results on image and text transmission show that WaSeCom achieves improved robustness under noise and adversarial perturbations, preserving semantic fidelity across varying wireless conditions.

Conclusion: WaSeCom, the proposed semantic communication framework, successfully addresses uncertainty and enhances robustness in 6G wireless systems through the use of Wasserstein distributionally robust optimization.

Abstract: 6G wireless systems are expected to support massive volumes of data with
ultra-low latency. However, conventional bit-level transmission strategies
cannot support the efficiency and adaptability required by modern,
data-intensive applications. The concept of semantic communication (SemCom)
addresses this limitation by focusing on transmitting task-relevant semantic
information instead of raw data. While recent efforts incorporating deep
learning and large-scale AI models have improved SemCom's performance, existing
systems remain vulnerable to both semantic-level and transmission-level noise
because they often rely on domain-specific architectures that hinder
generalizability. In this paper, a novel and generalized semantic communication
framework called WaSeCom is proposed to systematically address uncertainty and
enhance robustness. In particular, Wasserstein distributionally robust
optimization is employed to provide resilience against semantic
misinterpretation and channel perturbations. A rigorous theoretical analysis is
performed to establish the robust generalization guarantees of the proposed
framework. Experimental results on image and text transmission demonstrate that
WaSeCom achieves improved robustness under noise and adversarial perturbations.
These results highlight its effectiveness in preserving semantic fidelity
across varying wireless conditions.

</details>


### [156] [Graph Neural Networks for Jamming Source Localization](https://arxiv.org/abs/2506.03196)
*Dania Herzalla,Willian T. Lunardi,Martin Andreoni*

Main category: cs.NI

TL;DR: The paper presents the first application of graph-based learning for jamming source localization in wireless networks, reformulating the problem as an inductive graph regression task. It integrates structured node representations and attention-based GNNs to enhance robustness. The approach outperforms existing methods in complex scenarios.


<details>
  <summary>Details</summary>
Motivation: Graph-based learning shows potential in modeling complex relationships but is underexplored in wireless security. Jamming attacks pose significant threats to wireless networks, necessitating advanced localization techniques.

Method: Reformulates localization as an inductive graph regression task using structured node representations that encode signal aggregation. Employs attention-based graph neural networks for adaptive neighborhood influence refinement and confidence-guided estimation mechanism for balancing predictions with domain-informed priors.

Result: Significantly outperforms established localization baselines, especially in challenging scenarios with sparse and obfuscated signal information.

Conclusion: Graph-based learning offers a promising new direction for jamming source localization in wireless security, demonstrating superior performance in complex radio frequency environments.

Abstract: Graph-based learning has emerged as a transformative approach for modeling
complex relationships across diverse domains, yet its potential in wireless
security remains largely unexplored. In this work, we introduce the first
application of graph-based learning for jamming source localization, addressing
the imminent threat of jamming attacks in wireless networks. Unlike geometric
optimization techniques that struggle under environmental uncertainties and
dense interference, we reformulate localization as an inductive graph
regression task. Our approach integrates structured node representations that
encode local and global signal aggregation, ensuring spatial coherence and
adaptive signal fusion. To enhance robustness, we incorporate an
attention-based graph neural network that adaptively refines neighborhood
influence and introduces a confidence-guided estimation mechanism that
dynamically balances learned predictions with domain-informed priors. We
evaluate our approach under complex radio frequency environments with varying
sampling densities and signal propagation conditions, conducting comprehensive
ablation studies on graph construction, feature selection, and pooling
strategies. Results demonstrate that our novel graph-based learning framework
significantly outperforms established localization baselines, particularly in
challenging scenarios with sparse and obfuscated signal information. Code is
available at
[https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization).

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [157] [Towards Better Disentanglement in Non-Autoregressive Zero-Shot Expressive Voice Conversion](https://arxiv.org/abs/2506.04013)
*Seymanur Akti,Tuan Nam Nguyen,Alexander Waibel*

Main category: cs.SD

TL;DR: 本研究改进了一种自监督、非自回归框架，结合条件变分自编码器，用于表达性语音转换。通过使用多语言离散语音单元、强化嵌入和局部F0信息等技术，减少了源音色泄漏并提高了风格迁移的效果。实验表明，该模型在情感和说话人相似性方面优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 表达性语音转换需要同时转换目标语音的说话人身份和表达属性到源语音中。当前方法存在源音色泄漏和风格迁移不足的问题，因此需要改进以实现更好的风格迁移效果。

Method: 研究基于自监督、非自回归框架，采用条件变分自编码器（CVAE）。为了减少源音色泄漏，使用多语言离散语音单元表示内容，并通过数据增强的相似性损失和混合风格层归一化强化嵌入。为了增强表达性迁移，引入了通过交叉注意力机制利用局部F0信息，并提取包含全局音高和能量特征的风格嵌入。

Result: 实验结果表明，该模型在情感和说话人相似性方面的表现优于基线模型，展示了更优秀的风格适应能力和减少的源风格泄漏。

Conclusion: 提出的方法有效减少了源音色泄漏并增强了风格迁移能力，在表达性语音转换任务中取得了显著改进。

Abstract: Expressive voice conversion aims to transfer both speaker identity and
expressive attributes from a target speech to a given source speech. In this
work, we improve over a self-supervised, non-autoregressive framework with a
conditional variational autoencoder, focusing on reducing source timbre leakage
and improving linguistic-acoustic disentanglement for better style transfer. To
minimize style leakage, we use multilingual discrete speech units for content
representation and reinforce embeddings with augmentation-based similarity loss
and mix-style layer normalization. To enhance expressivity transfer, we
incorporate local F0 information via cross-attention and extract style
embeddings enriched with global pitch and energy features. Experiments show our
model outperforms baselines in emotion and speaker similarity, demonstrating
superior style adaptation and reduced source style leakage.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [158] [Universal Reusability in Recommender Systems: The Case for Dataset- and Task-Independent Frameworks](https://arxiv.org/abs/2506.03391)
*Tri Kurniawan Wijaya,Xinyang Shao,Gonzalo Fiz Pontiveros,Edoardo D'Amico*

Main category: cs.IR

TL;DR: The paper proposes Dataset- and Task-Independent Recommender System (DTIRS), inspired by large language models, aiming to minimize barriers in recommender systems by enabling reusability and reducing the need for dataset- or task-specific configurations.


<details>
  <summary>Details</summary>
Motivation: Recommender systems are crucial but face challenges related to scalability and adoption due to extensive dataset- and task-specific configurations. The authors aim to overcome these limitations by drawing inspiration from the generalizability of large language models.

Method: The framework leverages a novel Dataset Description Language (DsDL) for standardized dataset descriptions and explicit task definitions, enabling autonomous feature engineering, model selection, and optimization. It outlines a roadmap from Level-1 automation (dataset-agnostic but task-specific) to Level-2 automation (fully dataset- and task-independent).

Result: The paper introduces DTIRS as a concept and provides a roadmap for its development, addressing key challenges such as trade-offs between generalization and specialization, computational overhead, and scalability.

Conclusion: DTIRS has the potential to maximize code reusability and lower barriers to entry for recommender systems, transitioning towards fully dataset- and task-independent systems.

Abstract: Recommender systems are pivotal in delivering personalized experiences across
industries, yet their adoption and scalability remain hindered by the need for
extensive dataset- and task-specific configurations. Existing systems often
require significant manual intervention, domain expertise, and engineering
effort to adapt to new datasets or tasks, creating barriers to entry and
limiting reusability. In contrast, recent advancements in large language models
(LLMs) have demonstrated the transformative potential of reusable systems,
where a single model can handle diverse tasks without significant
reconfiguration. Inspired by this paradigm, we propose the Dataset- and
Task-Independent Recommender System (DTIRS), a framework aimed at maximizing
the reusability of recommender systems while minimizing barriers to entry.
Unlike LLMs, which achieve task generalization directly, DTIRS focuses on
eliminating the need to rebuild or reconfigure recommendation pipelines for
every new dataset or task, even though models may still need retraining on new
data. By leveraging the novel Dataset Description Language (DsDL), DTIRS
enables standardized dataset descriptions and explicit task definitions,
allowing autonomous feature engineering, model selection, and optimization.
This paper introduces the concept of DTIRS and establishes a roadmap for
transitioning from Level-1 automation (dataset-agnostic but task-specific
systems) to Level-2 automation (fully dataset- and task-independent systems).
Achieving this paradigm would maximize code reusability and lower barriers to
adoption. We discuss key challenges, including the trade-offs between
generalization and specialization, computational overhead, and scalability,
while presenting DsDL as a foundational tool for this vision.

</details>


<div id='cond-mat.stat-mech'></div>

# cond-mat.stat-mech [[Back]](#toc)

### [159] [Dreaming up scale invariance via inverse renormalization group](https://arxiv.org/abs/2506.04016)
*Adam Rançon,Ulysse Rançon,Tomislav Ivek,Ivan Balog*

Main category: cond-mat.stat-mech

TL;DR: The paper explores the ability of minimal neural networks to probabilistically reverse the renormalization group (RG) coarse-graining process in the 2D Ising model, generating microscopic configurations from coarse-grained states. Even simple networks can reproduce critical scaling behaviors and RG eigenvalues, with no significant improvement seen by increasing network complexity.


<details>
  <summary>Details</summary>
Motivation: To investigate whether minimal neural networks can invert the renormalization group coarse-graining procedure in the two-dimensional Ising model, effectively creating microscopic configurations from coarse-grained states.

Method: Using neural networks with as few as three trainable parameters to learn and generate critical configurations in the two-dimensional Ising model, and performing a real-space renormalization group analysis on the generated configurations.

Result: Neural networks were able to generate critical configurations that reproduced scaling behaviors of observables like magnetic susceptibility, heat capacity, and Binder ratios. The models also captured scale invariance and reproduced nontrivial eigenvalues of the RG transformation. Increasing network complexity did not significantly improve performance.

Conclusion: Simple local rules are sufficient for encoding the universality of critical phenomena, suggesting potential for efficient generative models in physics.

Abstract: We explore how minimal neural networks can invert the renormalization group
(RG) coarse-graining procedure in the two-dimensional Ising model, effectively
"dreaming up" microscopic configurations from coarse-grained states. This
task-formally impossible at the level of configurations-can be approached
probabilistically, allowing machine learning models to reconstruct
scale-invariant distributions without relying on microscopic input. We
demonstrate that even neural networks with as few as three trainable parameters
can learn to generate critical configurations, reproducing the scaling behavior
of observables such as magnetic susceptibility, heat capacity, and Binder
ratios. A real-space renormalization group analysis of the generated
configurations confirms that the models capture not only scale invariance but
also reproduce nontrivial eigenvalues of the RG transformation. Surprisingly,
we find that increasing network complexity by introducing multiple layers
offers no significant benefit. These findings suggest that simple local rules,
akin to those generating fractal structures, are sufficient to encode the
universality of critical phenomena, opening the door to efficient generative
models of statistical ensembles in physics.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [160] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)
*Jiuding Sun,Sidharth Baskaran,Zhengxuan Wu,Michael Sklar,Christopher Potts,Atticus Geiger*

Main category: cs.CL

TL;DR: HyperSteer是一种基于超网络架构的方法，可以生成条件化在自然语言引导提示和被引导语言模型内部的转向向量，使用成千上万个引导提示进行评估，其性能超过了最先进的激活转向方法，即使是在训练期间从未见过的引导提示上也是如此。此外，HyperSteer的表现与通过提示进行转向相当。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督词典学习方法（如稀疏自动编码器）可以生成许多转向向量，但缺乏对每个向量单独效果的保证以及对相关转向任务覆盖范围的控制。而有监督的方法虽然有针对性且有效，但每增加一个转向向量都需要更多的数据收集和训练。

Method: 引入了HyperSteer，这是一种基于超网络的架构家族，经过端到端训练以生成转向向量，这些向量根据自然语言转向提示和被转向的语言模型的内部结构进行条件化。

Result: 使用成千上万个转向提示对HyperSteer进行评估，其性能超过了最先进的激活转向方法，即使是在训练期间从未见过的转向提示上。此外，HyperSteer的表现与通过提示进行转向相当。

Conclusion: HyperSteer提供了一种新的方法来生成转向向量，具有更好的可扩展性和有效性，并且不需要为每个新向量收集额外的数据或进行额外的训练。

Abstract: Steering language models (LMs) by modifying internal activations is a popular
approach for controlling text generation. Unsupervised dictionary learning
methods, e.g., sparse autoencoders, can be scaled to produce many steering
vectors, but lack guarantees on the individual efficacy of each vector and
control over the coverage of relevant steering tasks. In contrast, supervised
methods for constructing steering vectors are targeted and effective, but
require more data collection and training for each additional steering vector
produced. In this work, we introduce HyperSteer, a family of hypernetwork-based
architectures which are trained end-to-end to generate steering vectors
conditioned on the natural language steering prompts and the internals of the
steered LM. In our evaluations, we show that scaling HyperSteer with thousands
of steering prompts exceeds the performance of state-of-the-art activation
steering methods, even on steering prompts never seen during training.
Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [161] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)
*Mustafa Eyceoz,Nikhil Shivakumar Nayak,Hao Wang,Ligong Han,Akash Srivastava*

Main category: cs.CL

TL;DR: Hopscotch is a method that skips least contributing attention blocks in causal language models, optimizing block skipping and output scaling while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Modern causal language models use many attention blocks which are not all necessary for every task, leading to potential inefficiencies.

Method: Hopscotch identifies and skips attention blocks with the least contributions to a task. It jointly optimizes which blocks to skip and how to scale the outputs of remaining layers using lightweight, trainable scaling parameters for attention and MLP blocks.

Result: When applied to Llama-3.1-8B and Qwen2.5-7B, Hopscotch achieves less than a 2% drop in performance even after skipping four attention blocks.

Conclusion: Hopscotch effectively improves efficiency without significantly affecting performance, and it does not modify model weights or require access to pretraining data.

Abstract: Modern causal language models stack many attention blocks to improve
performance, but not all blocks are necessary for every task. We propose
Hopscotch, a simple yet effective method that identifies and skips attention
blocks with least contributions to a task and adapts to preserve output
quality. Hopscotch jointly optimizes which blocks to skip and how to scale the
outputs of the remaining layers. By introducing lightweight, trainable scaling
parameters to attention and MLP blocks, it mitigates distribution shifts in
hidden states caused by removing attention blocks. Hopscotch does not modify
model weights or require access to pretraining or instruction-tuning data, and
is compatible with existing model compression techniques. When applied to
$\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than
a 2% drop in performance even after skipping four attention blocks.

</details>


### [162] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)
*Aldan Creo,Héctor Cerezo-Costas,Pedro Alonso-Doval,Maximiliano Hormazábal-Lagos*

Main category: cs.CL

TL;DR: The paper introduces 'Ask a Local', a hallucination detection method for LLMs that computes divergence between perplexity distributions of language-specialized models to identify inaccuracies. It performs consistently across 14 languages, with particular strength in Italian and Catalan, and is released to promote further research.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs present a significant challenge for AI, as they generate plausible but factually incorrect information.

Method: The method, 'Ask a Local', computes divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans. It exploits the intuition that specialized models exhibit greater surprise when encountering domain-specific inaccuracies.

Result: Results on a human-annotated question-answer dataset spanning 14 languages demonstrate consistent performance across languages, with IoU scores around 0.3 and comparable Spearman correlation values. Particularly strong performance was observed on Italian (IoU 0.42) and Catalan (IoU 0.38).

Conclusion: The 'Ask a Local' method provides a scalable solution for multilingual hallucination detection without needing adaptation or training for each language. The authors release their code and architecture to facilitate further research.

Abstract: Hallucinations in large language models (LLMs) - instances where models
generate plausible but factually incorrect information - present a significant
challenge for AI.
  We introduce "Ask a Local", a novel hallucination detection method exploiting
the intuition that specialized models exhibit greater surprise when
encountering domain-specific inaccuracies. Our approach computes divergence
between perplexity distributions of language-specialized models to identify
potentially hallucinated spans. Our method is particularly well-suited for a
multilingual context, as it naturally scales to multiple languages without the
need for adaptation, relying on external data sources, or performing training.
Moreover, we select computationally efficient models, providing a scalable
solution that can be applied to a wide range of languages and domains.
  Our results on a human-annotated question-answer dataset spanning 14
languages demonstrate consistent performance across languages, with
Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman
correlation values. Our model shows particularly strong performance on Italian
and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining
cross-lingual effectiveness without language-specific adaptations. We release
our code and architecture to facilitate further research in multilingual
hallucination detection.

</details>


### [163] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)
*Melkamu Abay Mersha,Mesay Gemeda Yigezu,Atnafu Lambebo Tonja,Hassan Shakil,Samer Iskander,Olga Kolesnikova,Jugal Kalita*

Main category: cs.CL

TL;DR: The paper proposes XAI-Guided Context-Aware Data Augmentation, which uses explainable AI techniques to improve data augmentation for low-resource languages. This method enhances model accuracy in hate speech and sentiment analysis tasks using the Amharic dataset.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of conventional data augmentation techniques, such as introducing noise, causing semantic drift, disrupting contextual coherence, lacking control, and leading to overfitting, especially for low-resource languages.

Method: XAI-Guided Context-Aware Data Augmentation framework that modifies less critical features while preserving task-relevant ones through an iterative feedback loop based on explainability-driven insights and model performance gain.

Result: XAI-SR-BT and XAI-PR-BT methods improved model accuracy by 6.6% and 8.1% respectively compared to baseline, and outperformed existing techniques by 4.8% and 5% respectively on hate speech and sentiment analysis tasks with the Amharic dataset.

Conclusion: This study offers a new paradigm shift in leveraging XAI techniques for more controlled, interpretable, and context-aware data augmentation, enhancing AI model training.

Abstract: Explainable AI (XAI) has emerged as a powerful tool for improving the
performance of AI models, going beyond providing model transparency and
interpretability. The scarcity of labeled data remains a fundamental challenge
in developing robust and generalizable AI models, particularly for low-resource
languages. Conventional data augmentation techniques introduce noise, cause
semantic drift, disrupt contextual coherence, lack control, and lead to
overfitting. To address these challenges, we propose XAI-Guided Context-Aware
Data Augmentation. This novel framework leverages XAI techniques to modify less
critical features while selectively preserving most task-relevant features. Our
approach integrates an iterative feedback loop, which refines augmented data
over multiple augmentation cycles based on explainability-driven insights and
the model performance gain. Our experimental results demonstrate that XAI-SR-BT
and XAI-PR-BT improve the accuracy of models on hate speech and sentiment
analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using
the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform
existing augmentation techniques by 4.8% and 5%, respectively, on the same
dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform
both baseline and conventional augmentation techniques across all tasks and
models. This study provides a more controlled, interpretable, and context-aware
solution to data augmentation, addressing critical limitations of existing
augmentation techniques and offering a new paradigm shift for leveraging XAI
techniques to enhance AI model training.

</details>


### [164] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)
*Mingxu Tao,Jie Hu,Mingchuan Yang,Yunhuai Liu,Dongyan Zhao,Yansong Feng*

Main category: cs.CL

TL;DR: EpiCoDe is a novel method that improves model performance in data-scarcity scenarios without extra training by using model extrapolation and contrastive decoding.


<details>
  <summary>Details</summary>
Motivation: The high cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks, so there's a need for methods that can boost model performance without additional training data.

Method: Firstly, model extrapolation is used to enhance a finetuned model with its inferior version. Then, contrastive decoding is adopted to further reduce predicted errors by comparing the logit scores given by the extrapolated and the vanilla finetuned model.

Result: Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement.

Conclusion: EpiCoDe effectively boosts model performance in data-scarcity scenarios without extra training, and the proposed theoretical framework helps understand its effectiveness.

Abstract: The remarkable performance of Large language models (LLMs) relies heavily on
the availability of abundant high-quality training data. However, the high cost
of acquiring annotated data often prevents models from obtaining capabilities
to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe
that boosts model performance in data-scarcity scenarios without extra
training. We first employ model extrapolation to enhance a finetuned model with
its inferior version, and then adopt contrastive decoding to further reduce
predicted errors, by comparing the logit scores given by the extrapolated and
the vanilla finetuned model. Experiments across three tasks over four different
LLMs show that EpiCoDe consistently outperforms existing methods with
significant and robust improvement. We also propose a new theoretical framework
to reveal the mechanism behind contrastive decoding in data-scarcity scenarios,
which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [165] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)
*Yuchen Guo,Zhicheng Dou,Huy H. Nguyen,Ching-Chun Chang,Saku Sugawara,Isao Echizen*

Main category: cs.CL

TL;DR: The paper addresses the issue of detecting human involvement in AI-generated content, proposing BERTScore and a multi-task RoBERTa-based regressor. It outperforms existing detectors with an F1 score of 0.9423.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the detection of human involvement in AI-generated content beyond binary classification due to the increasing use of generative AI in academia and its limitations.

Method: Using BERTScore as a metric and a multi-task RoBERTa-based regressor trained on a token classification task to measure human involvement in content generation.

Result: Achieved an F1 score of 0.9423 and a regressor mean squared error of 0.004, showing effectiveness and some generalizability across generative models.

Conclusion: The proposed method effectively detects varying levels of human involvement in AI-generated content, offering a more nuanced approach than binary classification.

Abstract: Content creation has dramatically progressed with the rapid advancement of
large language models like ChatGPT and Claude. While this progress has greatly
enhanced various aspects of life and work, it has also negatively affected
certain areas of society. A recent survey revealed that nearly 30% of college
students use generative AI to help write academic papers and reports. Most
countermeasures treat the detection of AI-generated text as a binary
classification task and thus lack robustness. This approach overlooks human
involvement in the generation of content even though human-machine
collaboration is becoming mainstream. Besides generating entire texts, people
may use machines to complete or revise texts. Such human involvement varies
case by case, which makes binary classification a less than satisfactory
approach. We refer to this situation as participation detection obfuscation. We
propose using BERTScore as a metric to measure human involvement in the
generation process and a multi-task RoBERTa-based regressor trained on a token
classification task to address this problem. To evaluate the effectiveness of
this approach, we simulated academic-based scenarios and created a continuous
dataset reflecting various levels of human involvement. All of the existing
detectors we examined failed to detect the level of human involvement on this
dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor
mean squared error of 0.004). Moreover, it demonstrated some generalizability
across generative models. Our code is available at
https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [166] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)
*Xiaofeng Zhou,Heyan Huang,Lizi Liao*

Main category: cs.CL

TL;DR: This paper proposes a Debate and Reflect (D&R) framework combined with Tree-structured Direct Preference Optimization (T-DPO) to enhance the performance of smaller language models by engaging them in multi-turn debates with stronger teacher models, resulting in significant improvements in accuracy, robustness, and generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current model distillation techniques that fail to provide substantial and lasting performance gains for smaller models due to high computational demands and inefficiencies.

Method: The method involves a novel Debate and Reflect (D&R) framework where smaller models engage in multi-turn debates with stronger teacher models to receive actionable feedback. Additionally, Tree-structured Direct Preference Optimization (T-DPO) is introduced to organize these interactions hierarchically for effective training.

Result: Empirical evaluations show that this approach significantly enhances the accuracy, robustness, and generalization of smaller models across diverse NLP benchmarks, surpassing conventional baselines by a large margin.

Conclusion: The D&R framework combined with T-DPO offers an effective solution to improve the performance of smaller language models, providing substantial and lasting performance gains.

Abstract: Large Language Models (LLMs) continue to set new standards in
knowledge-intensive and complex reasoning tasks, yet their high computational
demands limit widespread adoption. While distilling large models into smaller
ones offers a sustainable solution, current techniques--such as static
knowledge distillation, resource-intensive reinforcement learning from human
feedback, or limited self-reflection--struggle to yield substantial and lasting
performance gains. In this paper, we present a novel Debate and Reflect (D&R)
framework that orchestrates multi-turn debates between smaller models and
stronger teacher models, eliciting actionable feedback (e.g., error analysis,
corrective strategies) to guide student models. Further, we introduce
Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage
these debate logs, organizing interactions into a hierarchical format for
effective training. Empirical evaluations across diverse NLP benchmarks
demonstrate that our approach significantly improves smaller-model accuracy,
robustness, and generalization, outperforming conventional baselines by a large
margin.

</details>


### [167] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)
*Langlin Huang,Chengsong Huang,Jixuan Leng,Di Huang,Jiaxin Huang*

Main category: cs.CL

TL;DR: This paper introduces Position Specialists (PosS), a method using multiple position-specialized draft layers to improve token acceptance rate in speculative decoding for Large Language Models, resulting in better average acceptance length and speed-up ratio.


<details>
  <summary>Details</summary>
Motivation: To address the issue of degrading quality in draft token predictions at later positions due to error accumulation in draft model generated features during speculative decoding.

Method: Propose Position Specialists (PosS) consisting of multiple position-specialized draft layers to generate tokens at assigned positions, focusing on handling certain levels of draft model feature deviation.

Result: Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets show that PosS improves over baselines on average acceptance length and speed-up ratio.

Conclusion: PosS effectively enhances speculative decoding in Large Language Models, with improvements in both average acceptance length and speed-up ratio.

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by
using a small draft model to predict multiple tokens, and a large target model
to verify these tokens in parallel. Recent studies leverage the hidden state of
the target model to enhance draft model prediction accuracy. However, existing
methods suffer from the degrading quality of draft token predictions at later
positions, due to error accumulation in draft model generated features. In this
paper, we propose Position Specialists (PosS), which consist of multiple
position-specialized draft layers to generate tokens at assigned position(s).
Position specialists greatly improve token acceptance rate at later positions
per drafting round, as each specialist only needs to focus on handling a
certain level of draft model feature deviation. Experiment results on
Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that
PosS effectively improves over baselines on average acceptance length and
speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [168] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)
*Zirui Chen,Xin Wang,Zhao Li,Wenbin Guo,Dongxiao He*

Main category: cs.CL

TL;DR: Recent advances in knowledge representation learning emphasize the need to unify symbolic knowledge graphs with language models. To address this, KG-BiLM is introduced, a bidirectional LM framework that integrates graph structure and semantic expressiveness through three key components: Bidirectional Knowledge Attention, Knowledge-Masked Prediction, and Contrastive Graph Semantic Aggregation. Experiments show its superior performance in link prediction tasks.


<details>
  <summary>Details</summary>
Motivation: There is an urgent necessity to unify symbolic knowledge graphs with language models for richer semantic understanding, as existing approaches typically prioritize either graph structure or textual semantics, leaving a gap for a unified framework.

Method: KG-BiLM incorporates three key components: (i) Bidirectional Knowledge Attention, which removes the causal mask to enable full interaction among all tokens and entities; (ii) Knowledge-Masked Prediction, which encourages the model to leverage both local semantic contexts and global graph connectivity; and (iii) Contrastive Graph Semantic Aggregation, which preserves KG structure via contrastive alignment of sampled sub-graph representations.

Result: Extensive experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations.

Conclusion: KG-BiLM effectively unifies structural information and textual semantics, showing superior performance in link prediction tasks.

Abstract: Recent advances in knowledge representation learning (KRL) highlight the
urgent necessity to unify symbolic knowledge graphs (KGs) with language models
(LMs) for richer semantic understanding. However, existing approaches typically
prioritize either graph structure or textual semantics, leaving a gap: a
unified framework that simultaneously captures global KG connectivity, nuanced
linguistic context, and discriminative reasoning semantics. To bridge this gap,
we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues
from KGs with the semantic expressiveness of generative transformers. KG-BiLM
incorporates three key components: (i) Bidirectional Knowledge Attention, which
removes the causal mask to enable full interaction among all tokens and
entities; (ii) Knowledge-Masked Prediction, which encourages the model to
leverage both local semantic contexts and global graph connectivity; and (iii)
Contrastive Graph Semantic Aggregation, which preserves KG structure via
contrastive alignment of sampled sub-graph representations. Extensive
experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong
baselines in link prediction, especially on large-scale graphs with complex
multi-hop relations - validating its effectiveness in unifying structural
information and textual semantics.

</details>


### [169] [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)
*Zetong Tang,Qian Ma,Di Wu*

Main category: cs.CL

TL;DR: AP-SQL is a new architecture that combines small and large models for Text-to-SQL translation, using schema filtering, retrieval-augmented generation, and prompt-driven SQL creation. It uses CoT and GoT templates to enhance reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of using Text-to-SQL methods in resource-constrained environments, where reliance on resource-intensive open-source models is difficult.

Method: The method involves decomposing the Text-to-SQL task into three parts: schema filtering, retrieval-augmented text-to-SQL generation with in-context examples, and prompt-driven schema linking and SQL generation. Large language models are fine-tuned to improve schema selection accuracy, and prompt engineering techniques like Chain-of-Thought (CoT) and Graph-of-Thought (GoT) are used to enhance the model's reasoning abilities.

Result: AP-SQL has been comprehensively evaluated on the Spider benchmarks, showing its effectiveness in Text-to-SQL translation.

Conclusion: AP-SQL successfully bridges the gap between resource-efficient small models and powerful large models for Text-to-SQL tasks, providing an effective solution for resource-constrained environments.

Abstract: Using the best Text-to-SQL methods in resource-constrained environments is
challenging due to their reliance on resource-intensive open-source models.
This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to
bridge the gap between resource-efficient small open-source models and the
powerful capabilities of large closed-source models for Text-to-SQL
translation. Our method decomposes the task into schema filtering,
retrieval-augmented text-to-SQL generation based on in-context examples, and
prompt-driven schema linking and SQL generation. To improve schema selection
accuracy, we fine-tune large language models. Crucially, we also explore the
impact of prompt engineering throughout the process, leveraging
Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly
enhance the model's reasoning for accurate SQL generation. Comprehensive
evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

</details>


### [170] [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)
*Lin Mu,Guowei Chu,Li Ni,Lei Sang,Zhize Wu,Peiquan Jin,Yiwen Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为RoP的新策略，通过错误校正和引导两个阶段提高大语言模型在面对输入扰动时的鲁棒性。实验表明，RoP能显著增强LLMs的鲁棒性，并保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在各种任务中表现出色，但它们对输入扰动（如拼写错误或字符顺序错误）非常敏感，这会严重影响其性能。目前缺乏有效的提示策略来缓解这种影响。

Method: RoP由两个阶段组成：1) 错误校正阶段 - 使用多样化的扰动方法生成对抗样本，并构建自动校正输入错误的提示；2) 引导阶段 - 基于校正后的输入生成最优引导提示，引导模型进行更准确和鲁棒的推理。

Result: 通过涵盖算术、常识和逻辑推理任务的全面实验，证明了RoP能够显著提高LLMs对对抗扰动的鲁棒性，并且在与干净输入场景相比时，模型准确性仅有轻微下降。

Conclusion: RoP是一种实用且有效的方法，可以增强LLMs在实际应用中的鲁棒性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various tasks by effectively utilizing a prompting strategy. However, they are
highly sensitive to input perturbations, such as typographical errors or slight
character order errors, which can substantially degrade their performance.
Despite advances in prompting techniques, developing a prompting strategy that
explicitly mitigates the negative impact of such perturbations remains an open
challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a
novel prompting strategy specifically designed to enhance the robustness of
LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error
Correction stage, RoP applies diverse perturbation methods to generate
adversarial examples, which are then used to construct prompts that
automatically correct input errors. In the Guidance stage, RoP generates an
optimal guidance prompting based on the corrected input, steering the model
toward more robust and accurate inferences. Through comprehensive experiments
spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate
that RoP significantly improves LLMs' robustness against adversarial
perturbations. Notably, it maintains model accuracy with only minimal
degradation compared to clean input scenarios, thereby establishing RoP as a
practical and effective approach for enhancing LLM robustness in real-world
applications.

</details>


### [171] [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)
*Zhuohao Yu,Jiali Zeng,Weizheng Gu,Yidong Wang,Jindong Wang,Fandong Meng,Jie Zhou,Yue Zhang,Shikun Zhang,Wei Ye*

Main category: cs.CL

TL;DR: Reward Models (RMs) are usually trained on fixed datasets, limiting their adaptability. This paper introduces generalizable RMs that follow natural language specifications, a new benchmark RABench, and RewardAnything - an RM that excels in adapting to novel principles without retraining.


<details>
  <summary>Details</summary>
Motivation: Current RMs are trained on fixed preference datasets, leading to limited adaptability and resource-intensive retraining for diverse real-world needs.

Method: The authors propose RMs should follow dynamically provided natural language specifications, develop RABench for evaluation, and introduce RewardAnything - an RM designed to explicitly follow these principles.

Result: RewardAnything achieves state-of-the-art performance in traditional RM benchmarks and excels in adapting to novel principles without retraining, as shown by results on RABench.

Conclusion: RewardAnything integrates seamlessly with existing RLHF methods and demonstrates efficient alignment of LLMs using only natural language principles.

Abstract: Reward Models, essential for guiding Large Language Model optimization, are
typically trained on fixed preference datasets, resulting in rigid alignment to
single, implicit preference distributions. This prevents adaptation to diverse
real-world needs-from conciseness in one task to detailed explanations in
another. The standard practice of collecting task-specific preference data and
retraining reward models is resource-intensive, often producing biased rewards,
and limits practical application. We introduce generalizable,
principle-following reward models. We propose that RMs should understand and
adhere to dynamically provided natural language specifications of reward
principles, similar to instruction-following in LLMs. To measure this
capability, we develop RABench, a comprehensive benchmark for RMs focusing on
generalization across diverse principles. Evaluations on RABench reveal poor
generalization of current RMs. As a solution, we present RewardAnything, a
novel RM designed and trained to explicitly follow natural language principles.
We achieve SotA performance with RewardAnything in traditional RM benchmark
simply by specifying a well-defined principle, and results on RABench show we
excel in adapting to novel principles without retraining. Furthermore,
RewardAnything integrates seamlessly with existing RLHF methods and we show by
a case study on how to automatically and efficiently align LLMs with only
natural language principles.

</details>


### [172] [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)
*Chaeyun Jang,Moonseok Choi,Yegon Kim,Hyungi Lee,Juho Lee*

Main category: cs.CL

TL;DR: 在大型语言模型（LLMs）中，不确定性校准对于安全部署至关重要。本文研究了链式思维（CoT）推理的置信度校准问题，并发现仅通过标量置信度标签的监督微调，就可以激发语言模型的自我验证行为。此外，提出了一种简单的重思考方法，通过测试时基于校准不确定性进行缩放来提高性能。实验表明，这种置信度感知微调可以同时改善校准和准确性，增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前关于置信度校准的研究主要集中在分类器或短格式生成上，而对链式思维（CoT）推理的置信度校准研究较少。因此，有必要探索如何在LLMs中实现有效的置信度校准，特别是在涉及复杂推理任务时。

Method: 通过仅使用标量置信度标签进行监督微调，使模型能够自我验证。尽管没有提供任何显式的推理监督或基于强化学习的奖励，模型仍能根据置信度生成不同长度的回答。此外，提出了一种重思考方法，在测试时基于校准后的不确定性进行缩放以提升性能。

Result: 实验结果表明，该置信度感知微调方法不仅提高了校准效果和准确性，还增强了模型推理路径与置信度的一致性，从而提升了可解释性。

Conclusion: 仅通过标量置信度标签的监督微调即可有效激发LLMs的自我验证行为。提出的重思考方法进一步提高了模型性能。这一研究为未来LLMs在复杂推理任务中的应用提供了新的思路。

Abstract: Uncertainty calibration is essential for the safe deployment of large
language models (LLMs), particularly when users rely on verbalized confidence
estimates. While prior work has focused on classifiers or short-form
generation, confidence calibration for chain-of-thought (CoT) reasoning remains
largely unexplored. Surprisingly, we find that supervised fine-tuning with
scalar confidence labels alone suffices to elicit self-verification behavior of
language models, without any explicit reasoning supervision or reinforcement
learning-based rewards. Despite being trained only to produce a verbalized
confidence score without any self-verifying examples, the model learns to
generate longer and self-checking responses for low-confidence queries while
providing more concise answers for high-confidence ones. We further propose a
simple rethinking method that boosts performance via test-time scaling based on
calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such
as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning
improves both calibration and accuracy, while also enhancing interpretability
by aligning the model's reasoning path with its confidence.

</details>


### [173] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)
*Junling Wang,Anna Rutkiewicz,April Yi Wang,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: The paper introduces Math2Visual, a framework that automatically generates meaningful visuals from math word problem texts using a predefined visual language and teacher-informed design space. It creates a dataset of 1,903 visuals and improves Text-to-Image models for educational purposes.


<details>
  <summary>Details</summary>
Motivation: There is a need for automated methods to generate pedagogically meaningful visuals for teaching math word problems, as creating such visuals manually is labor-intensive.

Method: Math2Visual leverages a pre-defined visual language and a design space based on interviews with math teachers to illustrate core mathematical relationships in math word problems. An annotated dataset of 1,903 visuals was constructed and used to fine-tune Text-to-Image models.

Result: Improvements were demonstrated in the generation of educational visuals through the fine-tuning of several Text-to-Image models with the created dataset.

Conclusion: The work establishes a new benchmark for automated generation of pedagogically meaningful visuals and highlights challenges such as misrepresentation of mathematical relationships and omission of essential visual elements in multimodal educational content.

Abstract: Visuals are valuable tools for teaching math word problems (MWPs), helping
young learners interpret textual descriptions into mathematical expressions
before solving them. However, creating such visuals is labor-intensive and
there is a lack of automated methods to support this process. In this paper, we
present Math2Visual, an automatic framework for generating pedagogically
meaningful visuals from MWP text descriptions. Math2Visual leverages a
pre-defined visual language and a design space grounded in interviews with math
teachers, to illustrate the core mathematical relationships in MWPs. Using
Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate
Text-to-Image (TTI) models for their ability to generate visuals that align
with our design. We further fine-tune several TTI models with our dataset,
demonstrating improvements in educational visual generation. Our work
establishes a new benchmark for automated generation of pedagogically
meaningful visuals and offers insights into key challenges in producing
multimodal educational content, such as the misrepresentation of mathematical
relationships and the omission of essential visual elements.

</details>


### [174] [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)
*Yifeng Gu,Zicong Jiang,Jianxiu Jin,Kailing Guo,Ziyang Zhang,Xiangmin Xu*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) have advanced AI significantly, but their deployment is resource-intensive due to the large number of model parameters and the memory consumption of the KV cache during inference. Previous works have proposed reducing the KV cache by evicting unnecessary tokens using accumulated attention scores as eviction scores. However, this approach has a bias that limits the model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), which reduces the bias of the accumulated attention score and refines the adaptive score using value vectors in the self-attention mechanism. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context, achieving state-of-the-art results on several benchmark tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address the limitations of current methods for reducing the KV cache in Large Language Models (LLMs). The authors identify a bias in the accumulated attention score used as an eviction score in previous approaches, which limits the model's access to global contextual information. This motivates the development of a new method, AhaKV, to reduce this bias and improve the retention of crucial tokens across the global context.

Method: The proposed method, Adaptive holistic attention KV (AhaKV), addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according to the expectation of information entropy of attention scores. It also utilizes the information of value vectors in the self-attention mechanism, which was overlooked in previous works, to refine the adaptive score. This allows AhaKV to make use of holistic attention information and better retain crucial tokens across the global context.

Result: Experiments conducted with AhaKV deployed on different models with a fixed cache budget demonstrate its effectiveness. The results show that AhaKV successfully mitigates the bias in the accumulated attention score and retains crucial tokens across the global context. It achieves state-of-the-art results compared to other related work on several benchmark tasks.

Conclusion: In conclusion, the paper presents AhaKV as a solution to the bias problem in the accumulated attention score used for KV cache reduction in Large Language Models. By adaptively tuning the softmax scale and utilizing value vector information, AhaKV improves the retention of crucial tokens across the global context. The experimental results confirm its effectiveness and state-of-the-art performance on benchmark tasks.

Abstract: Large Language Models (LLMs) have significantly advanced the field of
Artificial Intelligence. However, their deployment is resource-intensive, not
only due to the large number of model parameters but also because the
(Key-Value) KV cache consumes a lot of memory during inference. While several
works propose reducing the KV cache by evicting the unnecessary tokens, these
approaches rely on accumulated attention score as eviction score to quantify
the importance of the token. We identify the accumulated attention score is
biased and it decreases with the position of the tokens in the mathematical
expectation. As a result, the retained tokens concentrate on the initial
positions, limiting model's access to global contextual information. To address
this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the
bias of the accumulated attention score by adaptively tuning the scale of
softmax according the expectation of information entropy of attention scores.
To make use of the holistic attention information in self-attention mechanism,
AhaKV utilize the information of value vectors, which is overlooked in previous
works, to refine the adaptive score. We show theoretically that our method is
well suited for bias reduction. We deployed AhaKV on different models with a
fixed cache budget. Experiments show that AhaKV successfully mitigates bias and
retains crucial tokens across global context and achieve state-of-the-art
results against other related work on several benchmark tasks.

</details>


### [175] [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)
*Isik Baran Sandan,Tu Anh Dinh,Jan Niehues*

Main category: cs.CL

TL;DR: Knockout Assessment is an LLM-as-a-Judge method that uses a knockout tournament system with iterative pairwise comparisons to improve scoring accuracy.


<details>
  <summary>Details</summary>
Motivation: Current LLM-as-a-Judge approaches lack a global ranking perspective due to reliance on individual or single-round assessments.

Method: Knockout Assessment employs a knockout tournament system with iterative pairwise comparisons across LLMs.

Result: Experiments show improved scoring accuracy, with an average increase of 0.07 in Pearson correlation with expert evaluations for university-level exam scoring and machine translation evaluations.

Conclusion: Knockout Assessment aligns LLM assessments more closely with human scoring.

Abstract: Large Language Models (LLMs) have shown to be effective evaluators across
various domains such as machine translations or the scientific domain. Current
LLM-as-a-Judge approaches rely mostly on individual assessments or a single
round of pairwise assessments, preventing the judge LLM from developing a
global ranking perspective. To address this, we present Knockout Assessment, an
LLM-asa Judge method using a knockout tournament system with iterative pairwise
comparisons. Experiments across three LLMs on two datasets show that knockout
assessment improves scoring accuracy, increasing Pearson correlation with
expert evaluations by 0.07 on average for university-level exam scoring and
machine translation evaluations, aligning LLM assessments more closely with
human scoring.

</details>


### [176] [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)
*Zhenhui Liu,Chunyuan Yuan,Ming Pang,Zheng Fang,Li Yuan,Xue Jiang,Changping Peng,Zhangang Lin,Zheng Luo,Jingping Shao*

Main category: cs.CL

TL;DR: In this paper, the authors propose a Multi-objective aligned Bidword Generation Model (MoBGM) that includes a discriminator, generator, and preference alignment module. This model is designed to improve query rewriting in e-commerce search advertising by enhancing relevance and authenticity while also maximizing platform revenue. Experiments demonstrate that MoBGM outperforms existing methods and has generated significant commercial value after deployment.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the challenge of matching user queries with relevant advertisements in e-commerce search advertising. Current methods fail to effectively optimize both the relevance/authenticity of user queries and the revenue potential of ads simultaneously.

Method: The method involves proposing a Multi-objective aligned Bidword Generation Model (MoBGM). This model consists of three components: a discriminator, a generator, and a preference alignment module. The discriminator optimizes key objectives, and its feedback signal is used to train the multi-objective aligned bidword generator, which aims to maximize the combined effect of relevance, authenticity, and revenue.

Result: Extensive offline and online experiments indicate that the proposed algorithm significantly outperforms state-of-the-art methods. Post-deployment, the algorithm has produced substantial commercial value for the platform, proving its feasibility and robustness.

Conclusion: The authors conclude that their proposed MoBGM model successfully addresses the challenges in query rewriting for e-commerce search advertising by improving relevance and authenticity while boosting platform revenue. The significant commercial value created post-deployment further validates the model's effectiveness.

Abstract: Retrieval systems primarily address the challenge of matching user queries
with the most relevant advertisements, playing a crucial role in e-commerce
search advertising. The diversity of user needs and expressions often produces
massive long-tail queries that cannot be matched with merchant bidwords or
product titles, which results in some advertisements not being recalled,
ultimately harming user experience and search efficiency. Existing query
rewriting research focuses on various methods such as query log mining,
query-bidword vector matching, or generation-based rewriting. However, these
methods often fail to simultaneously optimize the relevance and authenticity of
the user's original query and rewrite and maximize the revenue potential of
recalled ads.
  In this paper, we propose a Multi-objective aligned Bidword Generation Model
(MoBGM), which is composed of a discriminator, generator, and preference
alignment module, to address these challenges. To simultaneously improve the
relevance and authenticity of the query and rewrite and maximize the platform
revenue, we design a discriminator to optimize these key objectives. Using the
feedback signal of the discriminator, we train a multi-objective aligned
bidword generator that aims to maximize the combined effect of the three
objectives. Extensive offline and online experiments show that our proposed
algorithm significantly outperforms the state of the art. After deployment, the
algorithm has created huge commercial value for the platform, further verifying
its feasibility and robustness.

</details>


### [177] [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuai Zhang,Jianhua Tao*

Main category: cs.CL

TL;DR: RadialRouter is a new framework using RadialFormer to improve LLM routing by better understanding the relationship between user queries and LLM characteristics, showing significant performance improvement and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current LLM routing methods are not effective enough due to lack of exploration on the connection between user queries and LLM features.

Method: RadialRouter uses a lightweight Transformer with a radial structure (RadialFormer) to represent the query-LLMs relationship and selects the optimal LLM based on RadialFormer's final states. The pipeline is refined with an objective function combining Kullback-Leibler divergence and query-query contrastive loss.

Result: RadialRouter outperforms existing methods by 9.2% in Balance scenario and 5.8% in Cost First scenario on RouterBench. It also shows adaptability for different performance-cost trade-offs and dynamic LLM pools.

Conclusion: RadialRouter significantly enhances LLM routing effectiveness and demonstrates practical application potential.

Abstract: The rapid advancements in large language models (LLMs) have led to the
emergence of routing techniques, which aim to efficiently select the optimal
LLM from diverse candidates to tackle specific tasks, optimizing performance
while reducing costs. Current LLM routing methods are limited in effectiveness
due to insufficient exploration of the intrinsic connection between user
queries and the characteristics of LLMs. To address this issue, in this paper,
we present RadialRouter, a novel framework for LLM routing which employs a
lightweight Transformer-based backbone with a radial structure named
RadialFormer to articulate the query-LLMs relationship. The optimal LLM
selection is performed based on the final states of RadialFormer. The pipeline
is further refined by an objective function that combines Kullback-Leibler
divergence with the query-query contrastive loss to enhance robustness.
Experimental results on RouterBench show that RadialRouter significantly
outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost
First scenarios, respectively. Additionally, its adaptability toward different
performance-cost trade-offs and the dynamic LLM pool demonstrates practical
application potential.

</details>


### [178] [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)
*Vivian Nguyen,Lillian Lee,Cristian Danescu-Niculescu-Mizil*

Main category: cs.CL

TL;DR: The paper presents an unsupervised computational method for detecting pivotal moments in conversations, validating it through crisis counseling data.


<details>
  <summary>Details</summary>
Motivation: To assist conversationalists in domains with highly consequential outcomes by detecting pivotal moments where the conversation outcome can change significantly based on responses.

Method: An unsupervised computational method that identifies pivotal moments online by evaluating if the expectation of the outcome varies widely depending on potential next responses.

Result: The method aligns with human perception (counselors take longer to respond during detected pivotal moments) and correlates with changes in conversational trajectory. It also explores the relation between counselor's response during pivotal moments and session outcomes.

Conclusion: The introduced method successfully detects pivotal moments in conversations and could potentially assist in domains like mental health crisis counseling.

Abstract: During a conversation, there can come certain moments where its outcome hangs
in the balance. In these pivotal moments, how one responds can put the
conversation on substantially different trajectories leading to significantly
different outcomes. Systems that can detect when such moments arise could
assist conversationalists in domains with highly consequential outcomes, such
as mental health crisis counseling.
  In this work, we introduce an unsupervised computational method for detecting
such pivotal moments as they happen, in an online fashion. Our approach relies
on the intuition that a moment is pivotal if our expectation of the outcome
varies widely depending on what might be said next. By applying our method to
crisis counseling conversations, we first validate it by showing that it aligns
with human perception -- counselors take significantly longer to respond during
moments detected by our method -- and with the eventual conversational
trajectory -- which is more likely to change course at these times. We then use
our framework to explore the relation of the counselor's response during
pivotal moments with the eventual outcome of the session.

</details>


### [179] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)
*Yubo Wang,Ping Nie,Kai Zou,Lijun Wu,Wenhu Chen*

Main category: cs.CL

TL;DR: The paper explores unleashing reasoning potential of LLMs using Critique Fine-Tuning (CFT) instead of Reinforcement Learning (RL), showing CFT's efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To find a more efficient way to enhance the reasoning abilities of powerful base LLMs compared to the expensive and unstable RL method.

Method: Construct critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. Fine-tune LLMs on this CFT data.

Result: Significant performance gains across diverse reasoning tasks, with Qwen-Math-7B-CFT showing an average improvement of 15% on math benchmarks and 16% on logic reasoning benchmarks using just 5 GPU hours, comparable or surpassing RL results with 20x less compute.

Conclusion: One-shot CFT is a simple, general, and compute-efficient approach to enhancing the reasoning capabilities of modern LLMs.

Abstract: We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess
immense reasoning potential inherited from the pre-training stage. With
reinforcement learning (RL), these models can improve dramatically on reasoning
tasks. Recent studies have shown that even RL on a single problem can unleash
these models' reasoning capabilities. However, RL is not only expensive but
also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a
critical question: Is there a more efficient way to unleash the reasoning
potential of these powerful base LLMs? In this work, we demonstrate that
Critique Fine-Tuning (CFT) on only one problem can effectively unleash the
reasoning potential of LLMs. Our method constructs critique data by collecting
diverse model-generated solutions to a single problem and using teacher LLMs to
provide detailed critiques. We fine-tune Qwen and Llama family models, ranging
from 1.5B to 14B parameters, on the CFT data and observe significant
performance gains across diverse reasoning tasks. For example, with just 5 GPU
hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six
math benchmarks and 16% on three logic reasoning benchmarks. These results are
comparable to or even surpass the results from RL with 20x less compute.
Ablation studies reveal the robustness of one-shot CFT across different prompt
problems. These results highlight one-shot CFT as a simple, general, and
compute-efficient approach to unleashing the reasoning capabilities of modern
LLMs.

</details>


### [180] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)
*Celia Chen,Scotty Beland,Ingo Burghardt,Jill Byczek,William J. Conway,Eric Cotugno,Sadaf Davre,Megan Fletcher,Rajesh Kumar Gnanasekaran,Kristin Hamilton,Marilyn Harbert,Jordan Heustis,Tanaya Jha,Emily Klein,Hayden Kramer,Alex Leitch,Jessica Perkins,Casi Sherman,Celia Sterrn,Logan Stevens,Rebecca Zarrella,Jennifer Golbeck*

Main category: cs.CL

TL;DR: 尽管数据集来自不同的平台并且使用了不同的编码标准，但在一个数据集上训练并在另一个数据集上测试时，仍然可以实现高分类准确性。这为跨社交媒体的内容分类策略提供了启示。


<details>
  <summary>Details</summary>
Motivation: 暴力威胁仍然是社交媒体平台上的重大问题，高质量的数据有助于研究对恶意内容（包括暴力）的理解和检测。

Method: 引入了一个跨平台的数据集，其中包含30,000个帖子，这些帖子被手工编码为暴力威胁和暴力的子类型，包括政治和性暴力。使用现有的YouTube暴力评论数据集进行机器学习分析。

Result: 在不同平台的数据集上训练和测试模型，以及在合并数据集条件下，均实现了高分类准确性。

Conclusion: 这些结果对内容分类策略和理解跨社交媒体的暴力内容具有重要意义。

Abstract: Violent threats remain a significant problem across social media platforms.
Useful, high-quality data facilitates research into the understanding and
detection of malicious content, including violence. In this paper, we introduce
a cross-platform dataset of 30,000 posts hand-coded for violent threats and
sub-types of violence, including political and sexual violence. To evaluate the
signal present in this dataset, we perform a machine learning analysis with an
existing dataset of violent comments from YouTube. We find that, despite
originating from different platforms and using different coding criteria, we
achieve high classification accuracy both by training on one dataset and
testing on the other, and in a merged dataset condition. These results have
implications for content-classification strategies and for understanding
violent content across social media.

</details>


### [181] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)
*Mikel K. Ngueajio,Flor Miriam Plaza-del-Arco,Yi-Ling Chung,Danda B. Rawat,Amanda Cercas Curry*

Main category: cs.CL

TL;DR: 尽管大语言模型生成的反叙事内容在减少网络仇恨言论方面具有潜力，但其情感语调、可访问性和道德风险等问题仍然存在。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决大语言模型生成反叙事内容时面临的挑战，包括情感语调、可访问性和道德风险等方面。

Method: 提出一个框架来评估大语言模型生成的反叙事内容，涵盖四个维度：人物设定、冗长性与可读性、情感语调和道德稳健性。使用GPT-4o-Mini、Cohere's CommandR-7B和Meta's LLaMA 3.1-70B三个模型，在MT-Conan和HatEval数据集上评估三种提示策略。

Result: 发现大语言模型生成的反叙事内容通常较冗长且适合大学水平的读者，限制了其可访问性。虽然情感引导的提示能产生更具同理心和可读性的回应，但安全性和有效性仍存在问题。

Conclusion: 大语言模型生成的反叙事内容在减少网络仇恨言论方面具有潜力，但在实际应用中需要进一步改进以提高其可访问性、安全性和有效性。

Abstract: Automated counter-narratives (CN) offer a promising strategy for mitigating
online hate speech, yet concerns about their affective tone, accessibility, and
ethical risks remain. We propose a framework for evaluating Large Language
Model (LLM)-generated CNs across four dimensions: persona framing, verbosity
and readability, affective tone, and ethical robustness. Using GPT-4o-Mini,
Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting
strategies on the MT-Conan and HatEval datasets. Our findings reveal that
LLM-generated CNs are often verbose and adapted for people with college-level
literacy, limiting their accessibility. While emotionally guided prompts yield
more empathetic and readable responses, there remain concerns surrounding
safety and effectiveness.

</details>


### [182] [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)
*Aleksey Kudelya,Alexander Shirnin*

Main category: cs.CL

TL;DR: This paper presents LIBU, an algorithm for unlearning in large language models that combines influence functions and second-order optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of removing specific knowledge from large language models without retraining them from scratch and while preserving their overall utility.

Method: LIBU uses influence functions to remove data influence and second-order optimization to maintain model utility.

Result: Experiments demonstrate the effectiveness of this lightweight approach for unlearning across various tasks.

Conclusion: LIBU is a viable solution for unlearning in large language models.

Abstract: This paper describes LIBU (LoRA enhanced influence-based unlearning), an
algorithm to solve the task of unlearning - removing specific knowledge from a
large language model without retraining from scratch and compromising its
overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large
Language Models). The algorithm combines classical \textit{influence functions}
to remove the influence of the data from the model and \textit{second-order
optimization} to stabilize the overall utility. Our experiments show that this
lightweight approach is well applicable for unlearning LLMs in different kinds
of task.

</details>


### [183] [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)
*Hadi Mohammadi,Anastasia Giachanou,Daniel L. Oberski,Ayoub Bagheri*

Main category: cs.CL

TL;DR: 研究人员使用XAI方法，通过识别和替换影响分类器预测的关键标记来降低AI生成文本的可检测性，但多模型集成分类器仍能有效检测此类文本。


<details>
  <summary>Details</summary>
Motivation: 尽管生成式模型在生成类似人类文本方面取得了显著进展，但其输出往往具有可检测模式，因此需要探索降低AI生成文本可检测性的方法。

Method: 训练集成分类器区分AI生成文本与人类文本，利用SHAP和LIME识别影响预测的关键标记，并提出四种基于解释性的标记替换策略进行修改。

Result: 标记替换策略显著降低了单一分类器检测AI生成文本的能力，但多语言、多领域的集成分类器仍表现出强大的性能。

Conclusion: XAI方法可以通过聚焦关键标记使AI生成文本更难检测，同时强调了对适应隐藏AI生成文本方法的鲁棒集成检测策略的需求。

Abstract: Generative models, especially large language models (LLMs), have shown
remarkable progress in producing text that appears human-like. However, they
often exhibit patterns that make their output easier to detect than text
written by humans. In this paper, we investigate how explainable AI (XAI)
methods can be used to reduce the detectability of AI-generated text (AIGT)
while also introducing a robust ensemble-based detection approach. We begin by
training an ensemble classifier to distinguish AIGT from human-written text,
then apply SHAP and LIME to identify tokens that most strongly influence its
predictions. We propose four explainability-based token replacement strategies
to modify these influential tokens. Our findings show that these token
replacement approaches can significantly diminish a single classifier's ability
to detect AIGT. However, our ensemble classifier maintains strong performance
across multiple languages and domains, showing that a multi-model approach can
mitigate the impact of token-level manipulations. These results show that XAI
methods can make AIGT harder to detect by focusing on the most influential
tokens. At the same time, they highlight the need for robust, ensemble-based
detection strategies that can adapt to evolving approaches for hiding AIGT.

</details>


### [184] [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)
*Tim Franzmeyer,Archie Sravankumar,Lijuan Liu,Yuning Mao,Rui Hou,Sinong Wang,Jakob N. Foerster,Luke Zettlemoyer,Madian Khabsa*

Main category: cs.CL

TL;DR: The paper proposes a method called HALT to post-train LLMs so they only respond when confident, reducing hallucination. It increases correctness of responses by 15% on average and improves F1 score by 4%.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucination in LLMs where they produce incorrect answers due to lack of knowledge or capability.

Method: HALT generates capability-aligned post-training data by splitting pretrained LLM responses into factual fragments and identifying incorrect ones using ground truth information. Incorrect fragments are either removed or replaced with 'Unsure from Here' based on a tunable threshold.

Result: Finetuning four open-source models with HALT resulted in a 15% increase in mean correctness of response fragments and a 4% improvement in F1 score compared to relevant baselines. A single reliable Llama3-70B model achieved an increase in correctness from 51% to 87% while maintaining 53% response completeness.

Conclusion: HALT effectively allows practitioners to trade off response completeness for correctness, leading to more reliable LLMs.

Abstract: Large Language Models (LLMs) currently respond to every prompt. However, they
can produce incorrect answers when they lack knowledge or capability -- a
problem known as hallucination. We instead propose post-training an LLM to
generate content only when confident in its correctness and to otherwise
(partially) abstain. Specifically, our method, HALT, produces
capability-aligned post-training data that encodes what the model can and
cannot reliably generate. We generate this data by splitting responses of the
pretrained LLM into factual fragments (atomic statements or reasoning steps),
and use ground truth information to identify incorrect fragments. We achieve
capability-aligned finetuning responses by either removing incorrect fragments
or replacing them with "Unsure from Here" -- according to a tunable threshold
that allows practitioners to trade off response completeness and mean
correctness of the response's fragments. We finetune four open-source models
for biography writing, mathematics, coding, and medicine with HALT for three
different trade-off thresholds. HALT effectively trades off response
completeness for correctness, increasing the mean correctness of response
fragments by 15% on average, while resulting in a 4% improvement in the F1
score (mean of completeness and correctness of the response) compared to the
relevant baselines. By tuning HALT for highest correctness, we train a single
reliable Llama3-70B model with correctness increased from 51% to 87% across all
four domains while maintaining 53% of the response completeness achieved with
standard finetuning.

</details>


### [185] [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)
*Ming Zhang,Yujiong Shen,Zelin Li,Huayu Sha,Binze Hu,Yuhui Wang,Chenhao Huang,Shichun Liu,Jingqi Tong,Changhao Jiang,Mingxu Chai,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: This paper introduces LLMEval-Med, a new benchmark for evaluating large language models in medicine. It includes 2,996 questions based on real-world clinical scenarios and expert-designed situations. An automated evaluation pipeline is created with checklists and a LLM-as-Judge framework. Human-machine agreement analysis and dynamic refinement ensure reliability. Thirteen LLMs are evaluated across three categories providing insights for safe deployment.


<details>
  <summary>Details</summary>
Motivation: Existing medical benchmarks have limitations in question design, data sources, and evaluation methods. They mostly use multiple-choice questions, lack derivation from real clinical scenarios, and poorly assess complex reasoning.

Method: The method involves creating a new benchmark called LLMEval-Med which covers five core medical areas with 2,996 questions. An automated evaluation pipeline is designed incorporating expert-developed checklists into the LLM-as-Judge framework. Machine scoring is validated through human-machine agreement analysis, and checklists and prompts are dynamically refined based on expert feedback.

Result: Thirteen LLMs were evaluated across three categories (specialized medical models, open-source models, and closed-source models) on the LLMEval-Med benchmark, providing valuable insights for the safe and effective deployment of LLMs in medical domains.

Conclusion: LLMEval-Med offers a comprehensive and reliable way to evaluate LLMs in medical contexts, overcoming limitations of previous benchmarks. The insights gained can help ensure the safety and effectiveness of deploying LLMs in medical applications.

Abstract: Evaluating large language models (LLMs) in medicine is crucial because
medical applications require high accuracy with little room for error. Current
medical benchmarks have three main types: medical exam-based, comprehensive
medical, and specialized assessments. However, these benchmarks have
limitations in question design (mostly multiple-choice), data sources (often
not derived from real clinical scenarios), and evaluation methods (poor
assessment of complex reasoning). To address these issues, we present
LLMEval-Med, a new benchmark covering five core medical areas, including 2,996
questions created from real-world electronic health records and expert-designed
clinical scenarios. We also design an automated evaluation pipeline,
incorporating expert-developed checklists into our LLM-as-Judge framework.
Furthermore, our methodology validates machine scoring through human-machine
agreement analysis, dynamically refining checklists and prompts based on expert
feedback to ensure reliability. We evaluate 13 LLMs across three categories
(specialized medical models, open-source models, and closed-source models) on
LLMEval-Med, providing valuable insights for the safe and effective deployment
of LLMs in medical domains. The dataset is released in
https://github.com/llmeval/LLMEval-Med.

</details>


### [186] [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)
*Pedro Henrique Martins,João Alves,Patrick Fernandes,Nuno M. Guerreiro,Ricardo Rei,Amin Farajian,Mateusz Klimaszewski,Duarte M. Alves,José Pombal,Manuel Faysse,Pierre Colombo,François Yvon,Barry Haddow,José G. C. de Souza,Alexandra Birch,André F. T. Martins*

Main category: cs.CL

TL;DR: This report introduces EuroLLM-9B, a large language model covering 35 languages including all 24 official European Union languages. It addresses the underrepresentation of European languages in existing models through comprehensive development procedures and data filtering techniques like EuroFilter and EuroBlocks-Synthetic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a large language model that better serves European citizens by including all official EU languages and more, addressing their underrepresentation in current open LLMs.

Method: Methods include tokenizer design, architectural specifications, data filtering with EuroFilter, pre-training data collection, and enhancing language coverage with EuroBlocks-Synthetic synthetic dataset.

Result: EuroLLM-9B shows competitive performance on multilingual benchmarks and machine translation tasks, becoming the leading open European-made LLM of its size.

Conclusion: All major components of EuroLLM-9B are released to support open research and adoption, including base and instruction-tuned models, EuroFilter classifier, and the synthetic post-training dataset.

Abstract: This report presents EuroLLM-9B, a large language model trained from scratch
to support the needs of European citizens by covering all 24 official European
Union languages and 11 additional languages. EuroLLM addresses the issue of
European languages being underrepresented and underserved in existing open
large language models. We provide a comprehensive overview of EuroLLM-9B's
development, including tokenizer design, architectural specifications, data
filtering, and training procedures. We describe the pre-training data
collection and filtering pipeline, including the creation of EuroFilter, an
AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a
novel synthetic dataset for post-training that enhances language coverage for
European languages. Evaluation results demonstrate EuroLLM-9B's competitive
performance on multilingual benchmarks and machine translation tasks,
establishing it as the leading open European-made LLM of its size. To support
open research and adoption, we release all major components of this work,
including the base and instruction-tuned models, the EuroFilter classifier, and
the synthetic post-training dataset.

</details>


### [187] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)
*Wenhao Li,Wenwu Li,Chuyun Shen,Junjie Sheng,Zixiao Huang,Di Wu,Yun Hua,Wei Yin,Xiangfeng Wang,Hongyuan Zha,Bo Jin*

Main category: cs.CL

TL;DR: The paper introduces TextAtari, a benchmark for evaluating language agents on long-horizon decision-making tasks. It translates Atari games into textual descriptions, creating nearly 100 tasks with varying complexity. Three open-source large language models are evaluated across three agent frameworks in four scenarios. Significant performance gaps between language agents and human players are revealed in extensive planning tasks.


<details>
  <summary>Details</summary>
Motivation: To create a challenging test bed that bridges sequential decision-making with natural language processing, and to evaluate how different forms of prior knowledge affect the performance of language agents on long-horizon challenges.

Method: Translate visual state representations of classic Atari games into rich textual descriptions using an unsupervised representation learning framework (AtariARI). Evaluate three open-source large language models across three agent frameworks in four scenarios.

Result: Significant performance gaps between language agents and human players in extensive planning tasks are revealed, highlighting challenges in sequential reasoning, state tracking, and strategic planning.

Conclusion: TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning.

Abstract: We present TextAtari, a benchmark for evaluating language agents on very
long-horizon decision-making tasks spanning up to 100,000 steps. By translating
the visual state representations of classic Atari games into rich textual
descriptions, TextAtari creates a challenging test bed that bridges sequential
decision-making with natural language processing. The benchmark includes nearly
100 distinct tasks with varying complexity, action spaces, and planning
horizons, all rendered as text through an unsupervised representation learning
framework (AtariARI). We evaluate three open-source large language models
(Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks
(zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how
different forms of prior knowledge affect performance on these long-horizon
challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and
Reference-based-investigate the impact of semantic understanding, instruction
comprehension, and expert demonstrations on agent decision-making. Our results
reveal significant performance gaps between language agents and human players
in extensive planning tasks, highlighting challenges in sequential reasoning,
state tracking, and strategic planning across tens of thousands of steps.
TextAtari provides standardized evaluation protocols, baseline implementations,
and a framework for advancing research at the intersection of language models
and planning.

</details>


### [188] [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)
*Disha Sheshanarayana,Tanishka Magar,Ayushi Mittal,Neelam Chaplot*

Main category: cs.CL

TL;DR: This paper introduces LegalCon, a dataset for detecting manipulation in courtroom conversations, and CLAIM, a framework for enhancing manipulation analysis.


<details>
  <summary>Details</summary>
Motivation: To address the gap in NLP applications for detecting and analyzing manipulation within the legal domain.

Method: Introduced LegalCon dataset with 1,063 annotated courtroom conversations and proposed CLAIM, a two-stage, Intent-driven Multi-agent framework.

Result: Results show potential in improving fairness and transparency in judicial processes by incorporating agentic frameworks.

Conclusion: The contributions aim at broader application of NLP in legal discourse analysis and development of robust tools for fair legal decision-making.

Abstract: Courtrooms are places where lives are determined and fates are sealed, yet
they are not impervious to manipulation. Strategic use of manipulation in legal
jargon can sway the opinions of judges and affect the decisions. Despite the
growing advancements in NLP, its application in detecting and analyzing
manipulation within the legal domain remains largely unexplored. Our work
addresses this gap by introducing LegalCon, a dataset of 1,063 annotated
courtroom conversations labeled for manipulation detection, identification of
primary manipulators, and classification of manipulative techniques, with a
focus on long conversations. Furthermore, we propose CLAIM, a two-stage,
Intent-driven Multi-agent framework designed to enhance manipulation analysis
by enabling context-aware and informed decision-making. Our results highlight
the potential of incorporating agentic frameworks to improve fairness and
transparency in judicial processes. We hope that this contributes to the
broader application of NLP in legal discourse analysis and the development of
robust tools to support fairness in legal decision-making. Our code and data
are available at https://github.com/Disha1001/CLAIM.

</details>


### [189] [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)
*Akshat Gupta,Maochuan Lu,Thomas Hartvigsen,Gopala Anumanchipalli*

Main category: cs.CL

TL;DR: This paper demonstrates that the precomputation step in knowledge editing methods like MEMIT can be significantly reduced, saving computational time and resources.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the significant computational cost associated with the precomputation step in knowledge editing methods such as MEMIT, ROME, and EMMET. This step originally requires a large number of hidden vectors to be precomputed, which is time-consuming and resource-intensive.

Method: The authors determine the theoretical minimum number of hidden vector precomputations needed for these editing methods to function. They then empirically demonstrate that knowledge editing can be achieved by precomputing a very small fraction (less than 0.3%) of the originally required hidden vectors.

Result: The results show that the precomputation step can be drastically shortened, reducing it from tens of hours to just a few minutes without affecting the effectiveness of knowledge editing.

Conclusion: By significantly reducing the number of hidden vectors that need to be precomputed, this study makes knowledge editing more efficient and accessible, allowing users to begin editing new models much faster.

Abstract: Knowledge editing methods like MEMIT are able to make data and compute
efficient updates of factual knowledge by using a single sentence to update
facts and their consequences. However, what is often overlooked is a
"precomputation step", which requires a one-time but significant computational
cost. The authors of MEMIT originally precompute approximately 44 million
hidden vectors per edited layer, which requires a forward pass over 44 million
tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single
GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this
precomputation time grows with model size. In this paper, we show that this
excessive computational cost is unnecessary. Knowledge editing using MEMIT and
related methods, such as ROME and EMMET, can be performed by pre-computing a
very small portion of the 44 million hidden vectors. We first present the
theoretical minimum number of hidden vector precomputation required for
solutions of these editing methods to exist. We then empirically show that
knowledge editing using these methods can be done by pre-computing
significantly fewer hidden vectors. Specifically, we show that the
precomputation step can be done with less than 0.3% of the originally
stipulated number of hidden vectors. This saves a significant amount of
precomputation time and allows users to begin editing new models within a few
minutes.

</details>


### [190] [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)
*Claire Barale,Michael Rovatsos,Nehal Bhuta*

Main category: cs.CL

TL;DR: This paper empirically evaluates three common ML approaches on a large dataset of Canadian refugee decisions, showing limitations of statistical fairness evaluation in legally discretionary domains.


<details>
  <summary>Details</summary>
Motivation: To assess whether statistical methods can meaningfully evaluate fairness in legal contexts shaped by discretion, normative complexity, and limited ground truth.

Method: Empirically evaluate three common ML approaches (feature-based analysis, semantic clustering, and predictive modeling) on a large, real-world dataset of 59,000+ Canadian refugee decisions.

Result: These methods produce divergent and sometimes contradictory signals, predictive modeling often depends on contextual and procedural features rather than legal features, and semantic clustering fails to capture substantive legal reasoning.

Conclusion: Evaluating fairness in law requires methods grounded not only in data, but in legal reasoning and institutional context.

Abstract: Legal decisions are increasingly evaluated for fairness, consistency, and
bias using machine learning (ML) techniques. In high-stakes domains like
refugee adjudication, such methods are often applied to detect disparities in
outcomes. Yet it remains unclear whether statistical methods can meaningfully
assess fairness in legal contexts shaped by discretion, normative complexity,
and limited ground truth.
  In this paper, we empirically evaluate three common ML approaches
(feature-based analysis, semantic clustering, and predictive modeling) on a
large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our
experiments show that these methods produce divergent and sometimes
contradictory signals, that predictive modeling often depends on contextual and
procedural features rather than legal features, and that semantic clustering
fails to capture substantive legal reasoning.
  We show limitations of statistical fairness evaluation, challenge the
assumption that statistical regularity equates to fairness, and argue that
current computational approaches fall short of evaluating fairness in legally
discretionary domains. We argue that evaluating fairness in law requires
methods grounded not only in data, but in legal reasoning and institutional
context.

</details>


### [191] [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)
*Hieu Trung Nguyen,Bao Nguyen,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: In this paper, researchers discover that pruning specific attention heads in transformer models can boost reasoning performance. They introduce SPRINT, a framework that uses contrastive learning to dynamically choose which heads to prune for optimal results.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that selective pruning of certain attention heads in transformer-based language models leads to improvements in reasoning performance, especially on difficult tasks.

Method: The proposed method is SPRINT, a contrastive learning framework that dynamically selects the best heads and layers to prune during inference by aligning question embeddings with head embeddings.

Result: SPRINT significantly outperforms traditional best-of-$N$ and random head selection strategies on the MATH500 and GSM8K datasets.

Conclusion: Selective pruning of attention heads can enhance reasoning capabilities in transformer models, and SPRINT provides an effective way to achieve this.

Abstract: Model pruning in transformer-based language models, traditionally viewed as a
means of achieving computational savings, can enhance the model's reasoning
capabilities. In this work, we uncover a surprising phenomenon: the selective
pruning of certain attention heads leads to improvements in reasoning
performance, particularly on challenging tasks. Motivated by this observation,
we propose SPRINT, a novel contrastive learning framework that dynamically
selects the optimal head and layer to prune during inference. By aligning
question embeddings with head embeddings, SPRINT identifies those pruned-head
configurations that result in more accurate reasoning. Extensive experiments
demonstrate that our method significantly outperforms traditional best-of-$N$
and random head selection strategies on the MATH500 and GSM8K datasets.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [192] [Dual Branch VideoMamba with Gated Class Token Fusion for Violence Detection](https://arxiv.org/abs/2506.03162)
*Damith Chamalke Senadeera,Xiaoyun Yang,Dimitrios Kollias,Gregory Slabaugh*

Main category: cs.CV

TL;DR: 提出了一种新的模型Dual Branch VideoMamba with Gated Class Token Fusion (GCTF)用于暴力检测，结合双分支设计和状态空间模型（SSM）主干，在新合并的数据集上达到了最先进的性能，同时保持了计算效率。


<details>
  <summary>Details</summary>
Motivation: 随着监控摄像头的迅速普及，对自动化暴力检测的需求增加。然而，现有的CNNs和Transformers虽然在提取时空特征方面取得了成功，但在处理长期依赖性和计算效率方面存在困难。

Method: 提出了Dual Branch VideoMamba with Gated Class Token Fusion (GCTF)，它结合了双分支设计和状态空间模型（SSM）主干，一个分支捕捉空间特征，另一个分支关注时间动态，并通过门控机制进行连续融合。此外，还提出了一个新的基准测试，通过合并RWF-2000、RLVS和VioPeru数据集来检测视频中的暴力行为，确保训练和测试集之间的严格分离。

Result: 该模型在这个新基准测试上达到了最先进的性能，提供了一个在准确性和计算效率之间的最佳平衡。

Conclusion: 展示了状态空间模型（SSMs）在可扩展的、实时的监控暴力检测方面的潜力。

Abstract: The rapid proliferation of surveillance cameras has increased the demand for
automated violence detection. While CNNs and Transformers have shown success in
extracting spatio-temporal features, they struggle with long-term dependencies
and computational efficiency. We propose Dual Branch VideoMamba with Gated
Class Token Fusion (GCTF), an efficient architecture combining a dual-branch
design and a state-space model (SSM) backbone where one branch captures spatial
features, while the other focuses on temporal dynamics, with continuous fusion
via a gating mechanism. We also present a new benchmark by merging RWF-2000,
RLVS, and VioPeru datasets in video violence detection, ensuring strict
separation between training and testing sets. Our model achieves
state-of-the-art performance on this benchmark offering an optimal balance
between accuracy and computational efficiency, demonstrating the promise of
SSMs for scalable, real-time surveillance violence detection.

</details>


### [193] [Improvement of human health lifespan with hybrid group pose estimation methods](https://arxiv.org/abs/2506.03169)
*Arindam Chaudhuri*

Main category: cs.CV

TL;DR: This paper presents a hybrid-ensemble-based group pose estimation method that improves human health by detecting multi-person poses in real-time, enhancing occlusion robustness and dense regression accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve human health and make human movement measurement more accessible, there is a need for a more effective method to estimate human poses from images and videos.

Method: The proposed method uses a hybrid-ensemble approach that combines modified group pose estimation and modified real time pose estimation. The ensemble fuses the performance of individual methods in real time. A pose transformation method identifies relevant features for ensemble training, and a customized pre-trained hybrid ensemble is trained on public benchmarked datasets.

Result: The method provides best optimized results in real-time pose estimation, making pose estimation more robust to occlusion and improving dense regression accuracy. Comparative analysis and experiments on benchmarked datasets establish the effectiveness and viability of the proposed method.

Conclusion: The hybrid-ensemble-based group pose estimation method has potential applications in various real-time situations and can contribute to improvements in human health life span.

Abstract: Human beings rely heavily on estimation of poses in order to access their
body movements. Human pose estimation methods take advantage of computer vision
advances in order to track human body movements in real life applications. This
comes from videos which are recorded through available devices. These
para-digms provide potential to make human movement measurement more accessible
to users. The consumers of pose estimation movements believe that human poses
content tend to supplement available videos. This has increased pose estimation
software usage to estimate human poses. In order to address this problem, we
develop hybrid-ensemble-based group pose estimation method to improve human
health. This proposed hybrid-ensemble-based group pose estimation method aims
to detect multi-person poses using modified group pose estimation and modified
real time pose estimation. This ensemble allows fusion of performance of stated
methods in real time. The input poses from images are fed into individual
meth-ods. The pose transformation method helps to identify relevant features
for en-semble to perform training effectively. After this, customized
pre-trained hybrid ensemble is trained on public benchmarked datasets which is
being evaluated through test datasets. The effectiveness and viability of
proposed method is estab-lished based on comparative analysis of group pose
estimation methods and ex-periments conducted on benchmarked datasets. It
provides best optimized results in real-time pose estimation. It makes pose
estimation method more robust to oc-clusion and improves dense regression
accuracy. These results have affirmed po-tential application of this method in
several real-time situations with improvement in human health life span

</details>


### [194] [PALADIN : Robust Neural Fingerprinting for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03170)
*Murthy L,Subarna Tripathi*

Main category: cs.CV

TL;DR: 提出了一种利用编码理论中的循环纠错码概念，为文本到图像扩散模型引入神经指纹的方法，以解决现有方法归因准确度不足的问题。


<details>
  <summary>Details</summary>
Motivation: 目前在文本到图像生成模型中，神经指纹技术存在归因准确度与生成质量之间的权衡问题，且尚未达到100%的归因准确度，这使得实际部署变得不可行。

Method: 通过借鉴编码理论中的循环错误纠正码概念，设计出一种精确的神经指纹纳入方法，专门用于文本到图像扩散模型。

Result: 所提出的方法有望突破当前归因准确度不足的限制，实现更精确的神经指纹识别。

Conclusion: 采用循环错误纠正码的概念可以有效提升文本到图像模型中神经指纹的归因准确性，推动该技术的实际应用。

Abstract: The risk of misusing text-to-image generative models for malicious uses,
especially due to the open-source development of such models, has become a
serious concern. As a risk mitigation strategy, attributing generative models
with neural fingerprinting is emerging as a popular technique. There has been a
plethora of recent work that aim for addressing neural fingerprinting. A
trade-off between the attribution accuracy and generation quality of such
models has been studied extensively. None of the existing methods yet achieved
$100\%$ attribution accuracy. However, any model with less than \emph{perfect}
accuracy is practically non-deployable. In this work, we propose an accurate
method to incorporate neural fingerprinting for text-to-image diffusion models
leveraging the concepts of cyclic error correcting codes from the literature of
coding theory.

</details>


### [195] [EdgeVidSum: Real-Time Personalized Video Summarization at the Edge](https://arxiv.org/abs/2506.03171)
*Ghulam Mujtaba,Eun-Seok Ryu*

Main category: cs.CV

TL;DR: EdgeVidSum 是一种轻量级方法，可在边缘设备上直接生成长视频的个性化快速浏览摘要。该方法通过创新的缩略图技术和高效的神经架构实现实时视频摘要，同时通过本地数据处理保护用户隐私。与传统方法不同，EdgeVidSum 使用缩略图容器显著降低计算复杂度而不影响语义相关性。系统采用分层分析方法，其中轻量级 2D CNN 模型从缩略图中识别用户偏好的内容并生成时间戳以创建快速浏览摘要。演示表明 EdgeVidSum 能够在资源受限的设备（如 Jetson Nano）上流畅运行，解决了现代视频消费环境中计算效率、个性化和隐私的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前视频摘要方法要么需要强大的计算资源，要么无法提供个性化服务，同时可能侵犯用户隐私。因此，需要一种能够在边缘设备上运行、保护隐私、高效且个性化的视频摘要解决方案。

Method: 提出了一种基于缩略图容器的方法，使用轻量级 2D 卷积神经网络 (CNN) 模型对缩略图进行分析，从而确定用户感兴趣的片段并生成时间戳。此方法避免了逐帧处理整个视频，显著降低了计算复杂度，并确保了语义相关性。所有数据处理均在本地完成，保护了用户隐私。

Result: EdgeVidSum 成功在资源受限的设备（如 Jetson Nano）上实现了高效的视频摘要功能，生成了符合用户偏好的个性化摘要，同时保证了隐私保护。实验结果证明其在计算效率和摘要质量方面的优越性。

Conclusion: EdgeVidSum 提供了一种新颖的轻量级视频摘要解决方案，能够实现实时、个性化、隐私保护的视频摘要功能，适用于边缘设备。这一方法为现代视频消费环境中的关键问题提供了有效答案。

Abstract: EdgeVidSum is a lightweight method that generates personalized, fast-forward
summaries of long-form videos directly on edge devices. The proposed approach
enables real-time video summarization while safeguarding user privacy through
local data processing using innovative thumbnail-based techniques and efficient
neural architectures. Unlike conventional methods that process entire videos
frame by frame, the proposed method uses thumbnail containers to significantly
reduce computational complexity without sacrificing semantic relevance. The
framework employs a hierarchical analysis approach, where a lightweight 2D CNN
model identifies user-preferred content from thumbnails and generates
timestamps to create fast-forward summaries. Our interactive demo highlights
the system's ability to create tailored video summaries for long-form videos,
such as movies, sports events, and TV shows, based on individual user
preferences. The entire computation occurs seamlessly on resource-constrained
devices like Jetson Nano, demonstrating how EdgeVidSum addresses the critical
challenges of computational efficiency, personalization, and privacy in modern
video consumption environments.

</details>


### [196] [FOLIAGE: Towards Physical Intelligence World Models Via Unbounded Surface Evolution](https://arxiv.org/abs/2506.03173)
*Xiaoyi Liu,Hao Tang*

Main category: cs.CV

TL;DR: FOLIAGE，一种物理信息多模态世界模型，通过统一的上下文编码器、物理感知预测器和创新的网络结构，在无界累进表面生长任务中表现出色，并在SURF-BENCH评估套件中超越了专门的基线方法。


<details>
  <summary>Details</summary>
Motivation: 下一代世界模型需要具备物理智能，即从部分多感官观察中预测和塑造世界的能力。为此，研究提出了一种新的物理信息多模态世界模型来解决无界累进表面生长问题。

Method: FOLIAGE采用了一个统一的上下文编码器将图像、网格连接性和点云映射到共享的潜在状态。通过物理感知预测器推进该潜在状态以与目标表面对齐，生成模态无关的增长嵌入（MAGE）。此外，FOLIAGE的累进图网络（AGN）通过年龄位置编码和能量门控消息传递捕捉动态连接性。几何对应融合和跨补丁掩码增强了MAGE的表现力，而分层池化则平衡了全局上下文与局部动态。

Result: FOLIAGE在SURF-BENCH评估套件中的六个核心任务和四个压力测试中表现优于专门的基线方法，同时在动态环境中保持稳健。

Conclusion: FOLIAGE为基于世界模型的物理智能提供了一条新的多模态途径，展示了其在复杂任务中的优越性能和鲁棒性。

Abstract: Physical intelligence -- anticipating and shaping the world from partial,
multisensory observations -- is critical for next-generation world models. We
propose FOLIAGE, a physics-informed multimodal world model for unbounded
accretive surface growth. In its Action-Perception loop, a unified context
encoder maps images, mesh connectivity, and point clouds to a shared latent
state. A physics-aware predictor, conditioned on physical control actions,
advances this latent state in time to align with the target latent of the
surface, yielding a Modality-Agnostic Growth Embedding (MAGE) that interfaces
with critic heads for downstream objectives. FOLIAGE's Accretive Graph Network
(AGN) captures dynamic connectivity through Age Positional Encoding and
Energy-Gated Message-Passing. Geometry-Correspondence Fusion and Cross-Patch
Masking enhance MAGE's expressiveness, while Hierarchical Pooling balances
global context with local dynamics. We create SURF-GARDEN, a world model
learning platform comprising a Counterfactual Physics Simulator, a Multimodal
Correspondence Extractor, and Evolution Tracing, which generates 7,200 diverse
surface-growth sequences. SURF-BENCH, our physical-intelligence evaluation
suite, evaluates six core tasks -- topology recognition, inverse material
estimation, growth-stage classification, latent roll-out, cross-modal
retrieval, and dense correspondence -- and four stress tests -- sensor dropout,
zero-shot modality transfer, long-horizon prediction, and physics ablation --
to probe resilience. FOLIAGE outperforms specialized baselines while remaining
robust across dynamic environments, establishing a new world-model based,
multimodal pathway to physical intelligence.

</details>


### [197] [Multimodal Foundation Model for Cross-Modal Retrieval and Activity Recognition Tasks](https://arxiv.org/abs/2506.03174)
*Koki Matsuishi,Kosuke Ukita,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: In recent years, wearable devices have emphasized the significance of behavior analysis through IMU. Although multimodal foundation models have been proposed for better understanding human activity, they are still insufficient in analyzing full-body activities. This study proposes AURA-MFM, a model integrating four modalities (third-person video, motion capture, IMU, and text) to enhance understanding of human activities. Experimental results show that AURA-MFM outperforms existing methods, particularly in zero-shot classification.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of current multimodal foundation models in providing detailed analysis of full-body human activities. By incorporating additional modalities such as third-person video and motion capture data, the aim is to achieve a more comprehensive understanding of human activities beyond what first-person perspectives alone can offer.

Method: The proposed method, AURA-MFM, integrates four modalities: third-person video, motion capture, IMU, and text. It employs a Transformer-based IMU encoder to improve performance. This approach enables a multidimensional understanding of human activities by leveraging diverse data sources.

Result: Experimental evaluations on retrieval and activity recognition tasks indicate that AURA-MFM surpasses existing methods. Notably, in zero-shot classification for action recognition, the proposed method achieved an F1-score of 0.6226 and an accuracy of 0.7320, significantly higher than the existing method's F1-score of 0.0747 and accuracy of 0.1961.

Conclusion: AURA-MFM, a foundational model integrating four modalities, demonstrates superior performance in human activity analysis compared to existing methods. The inclusion of third-person video and motion capture data enhances the understanding of full-body human activities.

Abstract: In recent years, the widespread adoption of wearable devices has highlighted
the growing importance of behavior analysis using IMU. While applications span
diverse fields such as healthcare and robotics, recent studies have
increasingly focused on multimodal analysis, in addition to unimodal analysis.
Several studies have proposed multimodal foundation models that incorporate
first-person video and text data; however, these models still fall short in
providing a detailed analysis of full-body human activity. To address this
limitation, we propose Activity Understanding and Representations Alignment -
Multimodal Foundation Model (AURA-MFM), a foundational model integrating four
modalities: third-person video, motion capture, IMU, and text. By incorporating
third-person video and motion capture data, the model enables a detailed and
multidimensional understanding of human activity, which first-person
perspectives alone fail to capture. Additionally, a Transformer-based IMU
encoder is employed to enhance the model's overall performance. Experimental
evaluations on retrieval and activity recognition tasks demonstrate that our
model surpasses existing methods. Notably, in the zero-shot classification for
action recognition, our method achieved significantly higher performance, with
an F1-score of 0.6226 and an accuracy of 0.7320, whereas the existing method
recorded an F1-score of 0.0747 and an accuracy of 0.1961.

</details>


### [198] [Vid-SME: Membership Inference Attacks against Large Video Understanding Models](https://arxiv.org/abs/2506.03179)
*Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.CV

TL;DR: Multimodal large language models (MLLMs) show great ability in multimodal tasks but raise data privacy concerns. Existing membership inference attacks (MIAs) for text and image data don't work well for video data due to the inability to capture temporal variations. The paper introduces Vid-SME, a new MIA method for video data in VULLMs, which leverages Sharma-Mittal entropy to compute robust membership scores.


<details>
  <summary>Details</summary>
Motivation: To solve the problem of determining improperly used videos during the training of MLLMs, especially given the potential inclusion of sensitive video content in their training datasets.

Method: Vid-SME leverages the confidence of model output and integrates adaptive parameterization to compute Sharma-Mittal entropy (SME) for video inputs. It derives robust membership scores by leveraging the SME difference between natural and temporally-reversed video frames.

Result: Experiments on various self-trained and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

Conclusion: Vid-SME is the first membership inference method tailored for video data used in VULLMs and shows strong effectiveness.

Abstract: Multimodal large language models (MLLMs) demonstrate remarkable capabilities
in handling complex multimodal tasks and are increasingly adopted in video
understanding applications. However, their rapid advancement raises serious
data privacy concerns, particularly given the potential inclusion of sensitive
video content, such as personal recordings and surveillance footage, in their
training datasets. Determining improperly used videos during training remains a
critical and unresolved challenge. Despite considerable progress on membership
inference attacks (MIAs) for text and image data in MLLMs, existing methods
fail to generalize effectively to the video domain. These methods suffer from
poor scalability as more frames are sampled and generally achieve negligible
true positive rates at low false positive rates (TPR@Low FPR), mainly due to
their failure to capture the inherent temporal variations of video frames and
to account for model behavior differences as the number of frames varies. To
address these challenges, we introduce Vid-SME, the first membership inference
method tailored for video data used in video understanding LLMs (VULLMs).
Vid-SME leverages the confidence of model output and integrates adaptive
parameterization to compute Sharma-Mittal entropy (SME) for video inputs. By
leveraging the SME difference between natural and temporally-reversed video
frames, Vid-SME derives robust membership scores to determine whether a given
video is part of the model's training set. Experiments on various self-trained
and open-sourced VULLMs demonstrate the strong effectiveness of Vid-SME.

</details>


### [199] [Impact of Tuning Parameters in Deep Convolutional Neural Network Using a Crack Image Dataset](https://arxiv.org/abs/2506.03184)
*Mahe Zabin,Ho-Jin Choi,Md. Monirul Islam,Jia Uddin*

Main category: cs.CV

TL;DR: This paper explores the effect of various tuning parameters on a DCNN's performance for classifying crack images, finding that maxpooling, adam optimizer, and tanh activation function yield the best results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how different tuning parameters affect the performance of a deep convolutional neural network (DCNN).

Method: The method involved experimenting on a DCNN classifier consisting of 2 convolutional layers, 2 pooling layers, 1 dropout, and a dense layer. The impact of tuning parameters like pooling, activation function, and optimizer were observed.

Result: Results show that the DCNN performs better with maxpooling, adam optimizer, and tanh activation function when classifying crack images into negative and positive classes.

Conclusion: For the crack image dataset, DCNN with maxpooling and using adam optimizer along with tanh activation function shows better performance.

Abstract: The performance of a classifier depends on the tuning of its parame ters. In
this paper, we have experimented the impact of various tuning parameters on the
performance of a deep convolutional neural network (DCNN). In the ex perimental
evaluation, we have considered a DCNN classifier that consists of 2
convolutional layers (CL), 2 pooling layers (PL), 1 dropout, and a dense layer.
To observe the impact of pooling, activation function, and optimizer tuning pa
rameters, we utilized a crack image dataset having two classes: negative and
pos itive. The experimental results demonstrate that with the maxpooling, the
DCNN demonstrates its better performance for adam optimizer and tanh activation
func tion.

</details>


### [200] [Continual Learning in Vision-Language Models via Aligned Model Merging](https://arxiv.org/abs/2506.03189)
*Ghada Sokar,Gintare Karolina Dziugaite,Anurag Arnab,Ahmet Iscen,Pablo Samuel Castro,Cordelia Schmid*

Main category: cs.CV

TL;DR: The paper introduces a model merging method for continual learning in Vision-Language Models to balance stability and plasticity, reduce forgetting, and improve generalization.


<details>
  <summary>Details</summary>
Motivation: Continual learning typically leads to catastrophic forgetting due to its sequential nature that favors plasticity over stability. Existing approaches do not effectively address this issue.

Method: Propose a model merging technique where newly trained task parameters are merged with previously learned ones, along with a mechanism that promotes learning aligned weights to avoid interference.

Result: This approach successfully reduces forgetting, increases robustness to different task orders and similarities, and improves generalization when evaluated on large Vision-Language Models.

Conclusion: Model merging provides an effective solution for maintaining stability while retaining plasticity in continual learning scenarios.

Abstract: Continual learning is conventionally tackled through sequential fine-tuning,
a process that, while enabling adaptation, inherently favors plasticity over
the stability needed to retain prior knowledge. While existing approaches
attempt to mitigate catastrophic forgetting, a bias towards recent tasks
persists as they build upon this sequential nature. In this work we present a
new perspective based on model merging to maintain stability while still
retaining plasticity. Rather than just sequentially updating the model weights,
we propose merging newly trained task parameters with previously learned ones,
promoting a better balance. To maximize the effectiveness of the merging
process, we propose a simple mechanism that promotes learning aligned weights
with previous ones, thereby avoiding interference when merging. We evaluate
this approach on large Vision-Language Models (VLMs), and demonstrate its
effectiveness in reducing forgetting, increasing robustness to various task
orders and similarities, and improving generalization.

</details>


### [201] [MINT: Memory-Infused Prompt Tuning at Test-time for CLIP](https://arxiv.org/abs/2506.03190)
*Jiaming Yi,Ruirui Pan,Jishen Yang,Xiulong Yang*

Main category: cs.CV

TL;DR: 提出Memory-Infused Prompt Tuning (MINT)框架，通过存储可学习的key-value提示对的Memory Prompt Bank（MPB），在测试时利用层次视觉特征检索相关提示对以动态组装关联提示，注入图像编码器中进行细粒度视觉上下文引导，同时使用可学习文本提示，实现无需源数据或重新训练的快速精准VLM适应。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应（TTA）方法未能充分利用模型的内部知识，特别是在动态适应复杂和分层的视觉语义信息方面存在不足。

Method: 受到人类联想记忆理论的启发，提出Memory-Infused Prompt Tuning (MINT)框架，引入Memory Prompt Bank（MPB），存储可学习的key-value提示对，在测试时通过测试图像的分层视觉特征检索MPB中的相关提示对，动态组装关联提示并注入图像编码器中，同时使用可学习文本提示。

Result: MINT能够在测试时通过利用MPB获取的记忆，实现快速、精确的VLM适应，无需源数据或重新训练。

Conclusion: MINT为提高视觉语言预训练模型在测试时数据分布变化下的泛化能力提供了一种新颖有效的解决方案。

Abstract: Improving the generalization ability of Vision-Language Pre-trained Models
(VLMs) under test-time data distribution shifts remains a critical challenge.
The existing Test-Time Adaptation (TTA) methods fall short in fully leveraging
the model's internal knowledge, particularly in dynamically adapting to complex
and hierarchical visual semantic information. In this paper, we propose
Memory-Infused Prompt Tuning (MINT), a novel framework to address this issue.
Inspired by human associative memory theory, MINT introduces a Memory Prompt
Bank (MPB), which stores learnable key-value prompt pairs that work as a memory
of previously seen samples. During the test time, relevant prompt pairs in the
MPB are retrieved by the hierarchical visual features of test images to
dynamically assemble Associative Prompts. The associative prompts are then
injected into the image encoder for fine-grained, customized visual contextual
guidance. MINT also utilizes learnable text prompts. MINT thus enables rapid,
precise VLM adaptation at test time by leveraging this MPB-acquired memory,
without source data or retraining. The code is available at
https://github.com/Jamieyi2004/MINT.

</details>


### [202] [Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward](https://arxiv.org/abs/2506.03191)
*Muhammad Islam,Tao Huang,Euijoon Ahn,Usman Naseem*

Main category: cs.CV

TL;DR: This paper presents an in-depth survey on the use of multimodal Generative Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs) for human motion understanding and generation, focusing exclusively on text and motion modalities.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of using GenAI and LLMs to advance realistic and versatile motion synthesis.

Method: Investigating various generative approaches such as autoregressive models, diffusion models, GANs, VAEs, and transformer-based models by analyzing their strengths and limitations in terms of motion quality, computational efficiency, and adaptability. Highlighting recent advances in text-conditioned motion generation and the integration of LLMs.

Result: Provides insights into emerging methods, architectures, and their potential to advance realistic and versatile motion synthesis.

Conclusion: Text-to-motion GenAI and LLM architectures have transformative potential in applications such as healthcare, humanoids, gaming, animation, and assistive technologies, but challenges remain in generating efficient and realistic human motion.

Abstract: This paper presents an in-depth survey on the use of multimodal Generative
Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs)
for human motion understanding and generation, offering insights into emerging
methods, architectures, and their potential to advance realistic and versatile
motion synthesis. Focusing exclusively on text and motion modalities, this
research investigates how textual descriptions can guide the generation of
complex, human-like motion sequences. The paper explores various generative
approaches, including autoregressive models, diffusion models, Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), and
transformer-based models, by analyzing their strengths and limitations in terms
of motion quality, computational efficiency, and adaptability. It highlights
recent advances in text-conditioned motion generation, where textual inputs are
used to control and refine motion outputs with greater precision. The
integration of LLMs further enhances these models by enabling semantic
alignment between instructions and motion, improving coherence and contextual
relevance. This systematic survey underscores the transformative potential of
text-to-motion GenAI and LLM architectures in applications such as healthcare,
humanoids, gaming, animation, and assistive technologies, while addressing
ongoing challenges in generating efficient and realistic human motion.

</details>


### [203] [HueManity: Probing Fine-Grained Visual Perception in MLLMs](https://arxiv.org/abs/2506.03194)
*Rynaa Grover,Jayant Sravan Tamarapalli,Sahiti Yerramilli,Nilay Pande*

Main category: cs.CV

TL;DR: Multimodal Large Language Models (MLLMs) struggle with nuanced perceptual tasks despite excelling at high-level visual reasoning. Evaluated on the HueManity benchmark, MLLMs show significant performance deficits compared to humans and traditional computer vision models.


<details>
  <summary>Details</summary>
Motivation: To assess the visual perception capabilities of MLLMs, especially in nuanced perceptual tasks where they might be lacking.

Method: Created the HueManity benchmark, a dataset of 83,850 images with embedded alphanumeric strings in Ishihara test style dot patterns, to evaluate MLLMs' precise pattern recognition abilities.

Result: The best-performing MLLM achieved only 33.6% accuracy on the numeric `easy' task and 3% on the alphanumeric `hard' task, while humans scored near-perfect and a fine-tuned ResNet50 reached accuracies of 96.5% and 94.5%.

Conclusion: There is a critical gap in the visual capabilities of current MLLMs compared to humans and traditional CV models, highlighting areas for potential improvement.

Abstract: Multimodal Large Language Models (MLLMs) excel at high-level visual
reasoning, but their performance on nuanced perceptual tasks remains
surprisingly limited. We present HueManity, a benchmark designed to assess
visual perception in MLLMs. The dataset comprises 83,850 images featuring
two-character alphanumeric strings embedded in Ishihara test style dot
patterns, challenging models on precise pattern recognition. Our evaluation of
nine state-of-the-art MLLMs on HueManity demonstrates a significant performance
deficit compared to human and traditional computer vision baselines. The
best-performing MLLM achieved a 33.6% accuracy on the numeric `easy' task and a
striking 3% on the alphanumeric `hard' task. In contrast, human participants
achieved near-perfect scores (100% and 95.6%), and a fine-tuned ResNet50 model
reached accuracies of 96.5% and 94.5%. These results highlight a critical gap
in the visual capabilities of current MLLMs. Our analysis further explores
potential architectural and training-paradigm factors contributing to this
perceptual gap in MLLMs. We open-source HueManity dataset and code to foster
further research in improving perceptual robustness of MLLMs.

</details>


### [204] [Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs](https://arxiv.org/abs/2506.03195)
*Yunqi Hong,Sohyun An,Andrew Bai,Neil Y. C. Lin,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: AutoSEP is an iterative self-supervised prompt learning framework designed to enhance MLLM fine-grained classification capabilities. It leverages unlabeled data to learn a description prompt that guides MLLMs in identifying crucial discriminative features within an image, improving classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Fine-grained image classification remains challenging for MLLMs as it demands precise attention to subtle visual details to distinguish between visually similar subcategories.

Method: AutoSEP is an automatic self-enhancing prompt learning framework that iteratively improves the description prompt using unlabeled data, based on instance-level classification scoring function.

Result: AutoSEP consistently outperforms other unsupervised baselines on multiple fine-grained classification datasets, improving 13 percent over standard zero-shot classification and 5 percent over the best-performing baselines.

Conclusion: AutoSEP demonstrates the effectiveness of our self-supervised optimization framework in enhancing MLLM fine-grained classification capabilities.

Abstract: Despite Multimodal Large Language Models (MLLMs) showing promising results on
general zero-shot image classification tasks, fine-grained image classification
remains challenging. It demands precise attention to subtle visual details to
distinguish between visually similar subcategories--details that MLLMs may
easily overlook without explicit guidance. To address this, we introduce
AutoSEP, an iterative self-supervised prompt learning framework designed to
enhance MLLM fine-grained classification capabilities in a fully unsupervised
manner. Our core idea is to leverage unlabeled data to learn a description
prompt that guides MLLMs in identifying crucial discriminative features within
an image, and boosts classification accuracy. We developed an automatic
self-enhancing prompt learning framework called AutoSEP to iteratively improve
the description prompt using unlabeled data, based on instance-level
classification scoring function. AutoSEP only requires black-box access to
MLLMs, eliminating the need for any training or fine-tuning. We evaluate our
approach on multiple fine-grained classification datasets. It consistently
outperforms other unsupervised baselines, demonstrating the effectiveness of
our self-supervised optimization framework. Notably, AutoSEP on average
improves 13 percent over standard zero-shot classification and 5 percent over
the best-performing baselines. Code is available at:
https://github.com/yq-hong/AutoSEP

</details>


### [205] [Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing](https://arxiv.org/abs/2506.03197)
*Baode Wang,Biao Wu,Weizhen Li,Meng Fang,Yanjie Liang,Zuming Huang,Haozhe Wang,Jun Huang,Ling Chen,Wei Chu,Yuan Qi*

Main category: cs.CV

TL;DR: The paper presents layoutRL, a reinforcement learning framework for training models to parse scanned documents with awareness of layout using a composite reward. The authors create Infinity-Parser based on this framework and evaluate it on various benchmarks where it achieves state-of-the-art performance. They also plan to release their code and dataset.


<details>
  <summary>Details</summary>
Motivation: Automated parsing of scanned documents into structured formats is still a critical issue in Document AI due to error propagation and limited adaptability in traditional pipelines.

Method: Introduced layoutRL, an end-to-end reinforcement learning framework optimizing a composite reward including normalized edit distance, paragraph count accuracy, and reading order preservation. Developed Infinity-Parser, a vision-language-model-based parser instantiated within the layoutRL framework using the new Infinity-Doc-55K dataset.

Result: Infinity-Parser achieves state-of-the-art performance on English and Chinese benchmarks for OCR, table and formula extraction, and reading order detection.

Conclusion: The authors will publicly release their code and dataset to promote advancements in robust document understanding.

Abstract: Automated parsing of scanned documents into richly structured,
machine-readable formats remains a critical bottleneck in Document AI, as
traditional multi-stage pipelines suffer from error propagation and limited
adaptability to diverse layouts. We introduce layoutRL, an end-to-end
reinforcement learning framework that trains models to be explicitly
layout-aware by optimizing a composite reward of normalized edit distance,
paragraph count accuracy, and reading order preservation. Leveraging our newly
released dataset, Infinity-Doc-55K, which combines 55K high-fidelity synthetic
scanned document parsing data with expert-filtered real-world documents, we
instantiate layoutRL in a vision-language-model-based parser called
Infinity-Parser. Evaluated on English and Chinese benchmarks for OCR, table and
formula extraction, and reading order detection, Infinity-Parser achieves new
state-of-the-art performance in both accuracy and structural fidelity,
outpacing specialist pipelines and general-purpose vision-language models. We
will publicly release our code and dataset to accelerate progress in robust
document understanding.

</details>


### [206] [FLEX: A Large-Scale Multi-Modal Multi-Action Dataset for Fitness Action Quality Assessment](https://arxiv.org/abs/2506.03198)
*Hao Yin,Lijun Gu,Paritosh Parmar,Lin Xu,Tianxiao Guo,Weiwei Fu,Yang Zhang,Tianyou Zheng*

Main category: cs.CV

TL;DR: 为了应对健身训练特别是负重训练中的潜在风险，本文提出了FLEX数据集，这是一个多模态、多动作、大规模的数据集，结合了表面肌电信号（sEMG），用于动作质量评估（AQA）。该数据集通过高精度的动作捕捉系统收集了38名参与者在不同技能水平下进行的20种不同的负重动作，并包含了RGB视频、3D姿态、sEMG和生理信息。此外，FLEX还将知识图谱引入AQA，构建了以惩罚函数形式的注释规则，将负重动作、动作关键步骤、错误类型和反馈映射起来。实验表明，多模态数据、多视角数据和细粒度注释显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前动作质量评估（AQA）的方法和数据集主要集中在单视图竞技体育场景和RGB模式上，缺乏对健身动作的专业评估和指导，特别是在负重训练方面。这促使研究者开发一个更适合健身领域的AQA数据集和方法。

Method: 1. 创建了FLEX数据集，包含38个参与者执行的20种不同负重动作，涵盖3种技能水平，每种动作重复10次。2. 数据集整合了多种模态的数据，包括5个视角的RGB视频、3D姿态、表面肌电信号（sEMG）和生理信息。3. 利用高精度的动作捕捉（MoCap）系统采集数据。4. 引入知识图谱，构建了以惩罚函数形式的注释规则，将动作、关键步骤、错误类型和反馈关联起来。5. 在FLEX数据集上测试了各种基线方法，验证了多模态、多视角数据和细粒度注释对模型性能的提升。

Result: 实验结果表明，使用多模态数据、多视角数据以及细粒度注释可以显著提高模型的性能，证明了FLEX数据集在改进AQA方法和推动人工智能与健身领域融合方面的潜力。

Conclusion: FLEX数据集不仅推动了AQA方法和数据集向多模态、多动作场景的发展，还促进了人工智能技术在健身领域的应用。数据集和代码已公开，可供研究人员进一步探索和发展。

Abstract: With the increasing awareness of health and the growing desire for aesthetic
physique, fitness has become a prevailing trend. However, the potential risks
associated with fitness training, especially with weight-loaded fitness
actions, cannot be overlooked. Action Quality Assessment (AQA), a technology
that quantifies the quality of human action and provides feedback, holds the
potential to assist fitness enthusiasts of varying skill levels in achieving
better training outcomes. Nevertheless, current AQA methodologies and datasets
are limited to single-view competitive sports scenarios and RGB modality and
lack professional assessment and guidance of fitness actions. To address this
gap, we propose the FLEX dataset, the first multi-modal, multi-action,
large-scale dataset that incorporates surface electromyography (sEMG) signals
into AQA. FLEX utilizes high-precision MoCap to collect 20 different
weight-loaded actions performed by 38 subjects across 3 different skill levels
for 10 repetitions each, containing 5 different views of the RGB video, 3D
pose, sEMG, and physiological information. Additionally, FLEX incorporates
knowledge graphs into AQA, constructing annotation rules in the form of penalty
functions that map weight-loaded actions, action keysteps, error types, and
feedback. We conducted various baseline methodologies on FLEX, demonstrating
that multimodal data, multiview data, and fine-grained annotations
significantly enhance model performance. FLEX not only advances AQA
methodologies and datasets towards multi-modal and multi-action scenarios but
also fosters the integration of artificial intelligence within the fitness
domain. Dataset and code are available at
https://haoyin116.github.io/FLEX_Dataset.

</details>


### [207] [OpenCarbon: A Contrastive Learning-based Cross-Modality Neural Approach for High-Resolution Carbon Emission Prediction Using Open Data](https://arxiv.org/abs/2506.03224)
*Jinwei Zeng,Yu Liu,Guozhen Zhang,Jingtao Ding,Yuming Lin,Jian Yuan,Yong Li*

Main category: cs.CV

TL;DR: The paper introduces OpenCarbon, a model that uses satellite images and POI data to predict high-resolution urban carbon emissions. It addresses the challenges of functionality effects and spatial correlations through two key designs, showing superior performance and generalizability.


<details>
  <summary>Details</summary>
Motivation: Accurately estimating high-resolution carbon emissions is crucial for effective emission governance and mitigation planning, but conventional methods face challenges due to substantial data collection efforts. Open data and advanced learning techniques offer a solution.

Method: OpenCarbon incorporates two modalities of open data - satellite images and POI data - and features two major designs: cross-modality information extraction and fusion module, and neighborhood-informed aggregation module.

Result: Extensive experiments show a significant performance gain of 26.6% on R2. Generalizability tests and case studies validate its potential to empower efficient carbon governance and targeted carbon mitigation planning.

Conclusion: OpenCarbon demonstrates superiority in predicting high-resolution urban carbon emissions, capturing the intrinsic relation between urban functionalities and carbon emissions.

Abstract: Accurately estimating high-resolution carbon emissions is crucial for
effective emission governance and mitigation planning. While conventional
methods for precise carbon accounting are hindered by substantial data
collection efforts, the rise of open data and advanced learning techniques
offers a promising solution. Once an open data-based prediction model is
developed and trained, it can easily infer emissions for new areas based on
available open data. To address this, we incorporate two modalities of open
data, satellite images and point-of-interest (POI) data, to predict
high-resolution urban carbon emissions, with satellite images providing
macroscopic and static and POI data offering fine-grained and relatively
dynamic functionality information. However, estimating high-resolution carbon
emissions presents two significant challenges: the intertwined and implicit
effects of various functionalities on carbon emissions, and the complex spatial
contiguity correlations that give rise to the agglomeration effect. Our model,
OpenCarbon, features two major designs that target the challenges: a
cross-modality information extraction and fusion module to extract
complementary functionality information from two modules and model their
interactions, and a neighborhood-informed aggregation module to capture the
spatial contiguity correlations. Extensive experiments demonstrate our model's
superiority, with a significant performance gain of 26.6\% on R2. Further
generalizability tests and case studies also show OpenCarbon's capacity to
capture the intrinsic relation between urban functionalities and carbon
emissions, validating its potential to empower efficient carbon governance and
targeted carbon mitigation planning. Codes and data are available:
https://github.com/JinweiZzz/OpenCarbon.

</details>


### [208] [Pre-trained Vision-Language Models Assisted Noisy Partial Label Learning](https://arxiv.org/abs/2506.03229)
*Qian-Wei Wang,Yuqiu Xie,Letian Zhang,Zimo Liu,Shu-Tao Xia*

Main category: cs.CV

TL;DR: In noisy partial label learning (NPLL), this paper proposes a collaborative consistency regularization (Co-Reg) method that leverages pre-trained vision-language models (VLMs) for 'manual-annotation-free' training. It addresses instance-dependent noise through a Co-Pseudo-Labeling mechanism and consistency regularization in both label and feature spaces, with the option to incorporate few-shot manually annotated labels.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in the challenges of noisy partial label learning where each sample has multiple candidate labels from noisy annotators. The emergence of high-performance pre-trained VLMs offers an opportunity to replace manual annotations, making it crucial to develop methods that effectively learn from noisy labels generated by these models.

Method: The method involves simultaneously training two neural networks using a Co-Pseudo-Labeling mechanism for collaborative purification of training labels. Consistency regularization is applied in both label space and feature representation space. Additionally, the method can utilize few-shot manually annotated valid labels to improve performance.

Result: Comparative experiments demonstrate the effectiveness of the proposed Co-Reg method across various denoising and disambiguation algorithms, annotation methods, and application schemes of pre-trained models. This highlights the potential of integrating weakly-supervised learning into the knowledge distillation process of pre-trained models.

Conclusion: This paper successfully addresses the issue of learning from noisy partial labels by proposing the Co-Reg method, which shows promise in leveraging pre-trained VLMs for effective 'manual-annotation-free' training, opening up new possibilities in weakly-supervised learning.

Abstract: In the context of noisy partial label learning (NPLL), each training sample
is associated with a set of candidate labels annotated by multiple noisy
annotators. With the emergence of high-performance pre-trained vision-language
models (VLMs) such as CLIP, LLaVa and GPT-4V, the direction of using these
models to replace time-consuming manual annotation workflows and achieve
"manual-annotation-free" training for downstream tasks has become a highly
promising research avenue. This paper focuses on learning from noisy partial
labels annotated by pre-trained VLMs and proposes an innovative collaborative
consistency regularization (Co-Reg) method. Unlike the symmetric noise
primarily addressed in traditional noisy label learning, the noise generated by
pre-trained models is instance-dependent, embodying the underlying patterns of
the pre-trained models themselves, which significantly increases the learning
difficulty for the model. To address this, we simultaneously train two neural
networks that implement collaborative purification of training labels through a
"Co-Pseudo-Labeling" mechanism, while enforcing consistency regularization
constraints in both the label space and feature representation space. Our
method can also leverage few-shot manually annotated valid labels to further
enhance its performances. Comparative experiments with different denoising and
disambiguation algorithms, annotation manners, and pre-trained model
application schemes fully validate the effectiveness of the proposed method,
while revealing the broad prospects of integrating weakly-supervised learning
techniques into the knowledge distillation process of pre-trained models.

</details>


### [209] [Chipmunk: Training-Free Acceleration of Diffusion Transformers with Dynamic Column-Sparse Deltas](https://arxiv.org/abs/2506.03275)
*Austin Silveria,Soham V. Govande,Daniel Y. Fu*

Main category: cs.CV

TL;DR: Diffusion Transformers (DiTs) are great for generating images and videos but cost a lot during inference. This study finds that most changes in activations come from only a small percentage of values, motivating the development of Chipmunk. Chipmunk speeds up inference by recomputing only the fastest-changing activations and caching the rest, using dynamic sparsity. It addresses system challenges with column-wise sparsity and overlapping computations, achieving significant speedups without sacrificing quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to speed up inference for Diffusion Transformers without additional training, based on the observation that latent noise vectors change slowly across steps, suggesting redundancy in computation.

Method: Chipmunk uses dynamic sparsity at inference time to recompute only the fastest-changing intermediate activations while caching the rest. It introduces column-wise sparsity through voxel-based reordering of input tokens and implements column-sparse kernels for efficient memory use. Additionally, it overlaps the computation of sparsity patterns and cache updates with other parts of the computation to hide extra latency.

Result: Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on FLUX.1-dev without compromising generation quality. When stacked on top of full step caching, it achieves even greater speedups: 3.72x on HunyuanVideo, 2.67x on WAN2.1, and 2.25x on FLUX.1-dev, all with minimal quality impact.

Conclusion: Chipmunk successfully reduces redundancy in Diffusion Transformer inference by employing dynamic sparsity techniques, leading to substantial speedups without significantly affecting the quality of generated content.

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in
high-quality image and video generation but incur substantial compute cost at
inference. A common observation is that DiT latent noise vectors change slowly
across inference steps, which suggests that the DiT compute may be redundant
across steps. In this paper, we aim to speed up inference by reducing this
redundancy, without additional training. We first study how activations change
between steps in two state-of-the-art open-source DiTs. We find that just 5-25%
of the values in attention and MLP explain 70-90% of the change in activations
across steps. This finding motivates our approach, Chipmunk, which uses dynamic
sparsity at inference time to recompute only the fastest-changing intermediate
activations, while caching the rest. Dynamic sparsity introduces two systems
challenges: (1) sparse attention and MLP operations tend to underutilize GPU
tensor cores; and (2) computing dynamic sparsity patterns at runtime and
caching activations both introduce overhead. To address these challenges,
Chipmunk first uses a voxel-based reordering of input tokens to introduce
column-wise sparsity. We implement column-sparse kernels utilizing efficient
sparse gathers from global to shared GPU memory, achieving a 9.3x speedup at
93% sparsity compared to highly-optimized dense baselines. Second, Chipmunk
overlaps the computation of sparsity patterns and cache updates with other
parts of the computation (e.g., second layer of the MLP) to hide the extra
latency. Chipmunk achieves up to 2.16x speedup on HunyuanVideo and 1.41x on
FLUX.1-dev without compromising generation quality. Furthermore, we show that
Chipmunk can be stacked on top of full step caching, achieving a 3.72x speedup
on HunyuanVideo, a 2.67x speedup on WAN2.1, and a 2.25x speedup on FLUX.1-dev
with minimal quality impact.

</details>


### [210] [A Foundation Model for Spatial Proteomics](https://arxiv.org/abs/2506.03373)
*Muhammad Shaban,Yuzhou Chang,Huaying Qiu,Yao Yu Yeo,Andrew H. Song,Guillaume Jaume,Yuchen Wang,Luca L. Weishaupt,Tong Ding,Anurag Vaidya,Abdallah Lamane,Daniel Shao,Mohammed Zidane,Yunhao Bai,Paige McCallum,Shuli Luo,Wenrui Wu,Yang Wang,Precious Cramer,Chi Ngai Chan,Pierre Stephan,Johanna Schaffenrath,Jia Le Lee,Hendrik A. Michel,Caiwei Tian,Cristina Almagro-Perez,Sophia J. Wagner,Sharifa Sahai,Ming Y. Lu,Richard J. Chen,Andrew Zhang,Mark Edward M. Gonzales,Ahmad Makky,Jia-Ying Joey Lee,Hao Cheng,Nourhan El Ahmar,Sayed Matar,Maximilian Haist,Darci Phillips,Yuqi Tan,Garry P. Nolan,W. Richard Burack,Jacob D. Estes,Jonathan T. C. Liu,Toni K Choueiri,Neeraj Agarwal,Marc Barry,Scott J. Rodig,Long Phi Le,Georg Gerber,Christian M. Schürch,Fabian J. Theis,Youn H Kim,Joe Yeong,Sabina Signoretti,Brooke E. Howitt,Lit-Hsin Loo,Qin Ma,Sizun Jiang,Faisal Mahmood*

Main category: cs.CV

TL;DR: KRONOS is a self-supervised foundation model for spatial proteomics that achieves state-of-the-art performance in various tasks.


<details>
  <summary>Details</summary>
Motivation: To create a versatile and efficient tool for spatial proteomics analysis, which has been underutilized by foundation models despite their success in other image analysis fields.

Method: KRONOS was trained on 47 million image patches with 175 protein markers, 16 tissue types, and 8 imaging platforms using self-supervised learning. It incorporates architectural adaptations to handle high-dimensional, multi-channel, and heterogeneous data.

Result: KRONOS learns biologically meaningful representations across multiple scales, performs well in cell phenotyping, treatment response prediction, and retrieval tasks, and introduces segmentation-free patch-level processing.

Conclusion: KRONOS is a flexible and scalable tool for spatial proteomics, publicly available at https://github.com/mahmoodlab/KRONOS.

Abstract: Foundation models have begun to transform image analysis by acting as
pretrained generalist backbones that can be adapted to many tasks even when
post-training data are limited, yet their impact on spatial proteomics, imaging
that maps proteins at single-cell resolution, remains limited. Here, we
introduce KRONOS, a foundation model built for spatial proteomics. KRONOS was
trained in a self-supervised manner on over 47 million image patches covering
175 protein markers, 16 tissue types, and 8 fluorescence-based imaging
platforms. We introduce key architectural adaptations to address the
high-dimensional, multi-channel, and heterogeneous nature of multiplex imaging.
We demonstrate that KRONOS learns biologically meaningful representations
across multiple scales, ranging from cellular and microenvironment to tissue
levels, enabling it to address diverse downstream tasks, including cell
phenotyping, region classification, and patient stratification. Evaluated
across 11 independent cohorts, KRONOS achieves state-of-the-art performance
across cell phenotyping, treatment response prediction, and retrieval tasks,
and is highly data-efficient. KRONOS also introduces the paradigm of
segmentation-free patch-level processing for efficient and scalable spatial
proteomics analysis, allowing cross-institutional comparisons, and as an image
reverse search engine for spatial patterns. Together, these results position
KRONOS as a flexible and scalable tool for spatial proteomics. The model is
publicly accessible at https://github.com/mahmoodlab/KRONOS.

</details>


### [211] [Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video Reasoning](https://arxiv.org/abs/2506.03525)
*Daeun Lee,Jaehong Yoon,Jaemin Cho,Mohit Bansal*

Main category: cs.CV

TL;DR: The paper proposes Video-Skill-CoT (Video-SKoT), a framework that constructs skill-aware Chain-of-Thought supervisions for domain-adaptive video reasoning, improving performance across various video understanding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods in complex video understanding struggle to adapt to domain-specific skills such as event detection, spatial relation understanding, and emotion understanding.

Method: The framework first constructs skill-based CoT annotations by extracting domain-relevant reasoning skills from training questions, clustering them into a shared skill taxonomy, and creating detailed multi-step CoT rationale for each video-question pair. Then, it introduces a skill-specific expert learning framework where each expert module specializes in a subset of reasoning skills and is trained with lightweight adapters using the collected CoT supervision.

Result: Video-SKoT outperforms strong baselines on three video understanding benchmarks and provides in-depth analyses on different CoT annotation pipelines and learned skills over multiple video domains.

Conclusion: Video-SKoT is effective in enhancing domain-adaptive video reasoning through skill-aware CoT supervisions.

Abstract: Recent advances in Chain-of-Thought (CoT) reasoning have improved complex
video understanding, but existing methods often struggle to adapt to
domain-specific skills (e.g., event detection, spatial relation understanding,
emotion understanding) over various video content. To address this, we propose
Video-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs
and leverages skill-aware CoT supervisions for domain-adaptive video reasoning.
First, we construct skill-based CoT annotations: we extract domain-relevant
reasoning skills from training questions, cluster them into a shared skill
taxonomy, and create detailed multi-step CoT rationale tailored to each
video-question pair for training. Second, we introduce a skill-specific expert
learning framework. Each expert module specializes in a subset of reasoning
skills and is trained with lightweight adapters using the collected CoT
supervision. We demonstrate the effectiveness of the proposed approach on three
video understanding benchmarks, where Video-SKoT consistently outperforms
strong baselines. We also provide in-depth analyses on comparing different CoT
annotation pipelines and learned skills over multiple video domains.

</details>


### [212] [DiagNet: Detecting Objects using Diagonal Constraints on Adjacency Matrix of Graph Neural Network](https://arxiv.org/abs/2506.03571)
*Chong Hyun Lee,Kibae Lee*

Main category: cs.CV

TL;DR: The paper introduces DaigNet, a new method for object detection that applies diagonal constraints on the adjacency matrix of a graph convolutional network (GCN). It proposes two diagonalization algorithms and two loss functions. Experiments demonstrate improved performance over several YOLO models on Pascal VOC and MS COCO datasets.


<details>
  <summary>Details</summary>
Motivation: To develop an object detection approach that eliminates the need for anchor boxes commonly used in existing methods, by leveraging diagonal constraints on the adjacency matrix of a graph convolutional network (GCN).

Method: Propose DaigNet with two diagonalization algorithms based on hard and soft constraints on the adjacency matrix and two loss functions incorporating diagonal and complementary constraints. Detection head from YOLO models is adopted to validate the feasibility.

Result: DaigNet achieves 7.5% higher mAP50 on Pascal VOC compared to YOLOv1, and on MS COCO, it shows 5.1%, 3.7%, and 2.9% higher mAP than YOLOv3u, YOLOv5u, and YOLOv8 respectively.

Conclusion: DaigNet presents a novel approach to object detection that surpasses several YOLO model variants in terms of performance on benchmark datasets, eliminating the requirement for anchor box design.

Abstract: We propose DaigNet, a new approach to object detection with which we can
detect an object bounding box using diagonal constraints on adjacency matrix of
a graph convolutional network (GCN). We propose two diagonalization algorithms
based on hard and soft constraints on adjacency matrix and two loss functions
using diagonal constraint and complementary constraint. The DaigNet eliminates
the need for designing a set of anchor boxes commonly used. To prove
feasibility of our novel detector, we adopt detection head in YOLO models.
Experiments show that the DiagNet achieves 7.5% higher mAP50 on Pascal VOC than
YOLOv1. The DiagNet also shows 5.1% higher mAP on MS COCO than YOLOv3u, 3.7%
higher mAP than YOLOv5u, and 2.9% higher mAP than YOLOv8.

</details>


### [213] [ViTSGMM: A Robust Semi-Supervised Image Recognition Network Using Sparse Labels](https://arxiv.org/abs/2506.03582)
*Rui Yann,Xianglei Xing*

Main category: cs.CV

TL;DR: The paper introduces ViTSGMM, an image recognition network using semi-supervised learning efficiently. It constructs a hierarchical mixture density classification decision mechanism by optimizing mutual information. The method shows state-of-the-art performance on STL-10 and CIFAR-10/100 datasets with negligible labeled samples. Also, it highlights and fixes a data leakage issue in STL-10 dataset.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of complex training techniques and architectures in existing works and improve generalization ability when dealing with extremely limited labeled data.

Method: Constructs a hierarchical mixture density classification decision mechanism by optimizing mutual information between feature representations and target classes, compressing redundant information while retaining crucial discriminative components.

Result: Achieves state-of-the-art performance on STL-10 and CIFAR-10/100 datasets using negligible labeled samples.

Conclusion: ViTSGMM is an efficient semi-supervised learning approach for image recognition that outperforms existing methods when labeled data is scarce.

Abstract: We present ViTSGMM, an image recognition network that leverages
semi-supervised learning in a highly efficient manner. Existing works often
rely on complex training techniques and architectures, while their
generalization ability when dealing with extremely limited labeled data remains
to be improved. To address these limitations, we construct a hierarchical
mixture density classification decision mechanism by optimizing mutual
information between feature representations and target classes, compressing
redundant information while retaining crucial discriminative components.
Experimental results demonstrate that our method achieves state-of-the-art
performance on STL-10 and CIFAR-10/100 datasets when using negligible labeled
samples. Notably, this paper also reveals a long-overlooked data leakage issue
in the STL-10 dataset for semi-supervised learning tasks and removes duplicates
to ensure the reliability of experimental results. Code available at
https://github.com/Shu1L0n9/ViTSGMM.

</details>


### [214] [BiMa: Towards Biases Mitigation for Text-Video Retrieval via Scene Element Guidance](https://arxiv.org/abs/2506.03589)
*Huy Le,Nhat Chung,Tung Kieu,Anh Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: The paper proposes BiMa, a framework to reduce visual-linguistic biases in text-video retrieval systems by enhancing video embeddings with scene elements and disentangling text features into content and bias components. It shows competitive performance across benchmarks and effectiveness in out-of-distribution retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Text-video retrieval systems are often affected by visual-linguistic biases present in datasets, leading pre-trained models to overlook key details.

Method: BiMa generates scene elements for each video by identifying relevant entities/objects and activities. For visual debiasing, these elements are integrated into video embeddings to highlight fine-grained details. For textual debiasing, a mechanism is introduced to separate text features into content and bias components.

Result: Extensive experiments across five major TVR benchmarks show competitive performance. The model's bias mitigation capability is validated by strong results on out-of-distribution retrieval tasks.

Conclusion: BiMa effectively mitigates biases in both visual and textual representations, improving text-video retrieval performance.

Abstract: Text-video retrieval (TVR) systems often suffer from visual-linguistic biases
present in datasets, which cause pre-trained vision-language models to overlook
key details. To address this, we propose BiMa, a novel framework designed to
mitigate biases in both visual and textual representations. Our approach begins
by generating scene elements that characterize each video by identifying
relevant entities/objects and activities. For visual debiasing, we integrate
these scene elements into the video embeddings, enhancing them to emphasize
fine-grained and salient details. For textual debiasing, we introduce a
mechanism to disentangle text features into content and bias components,
enabling the model to focus on meaningful content while separately handling
biased information. Extensive experiments and ablation studies across five
major TVR benchmarks (i.e., MSR-VTT, MSVD, LSMDC, ActivityNet, and DiDeMo)
demonstrate the competitive performance of BiMa. Additionally, the model's bias
mitigation capability is consistently validated by its strong results on
out-of-distribution retrieval tasks.

</details>


### [215] [VLMs Can Aggregate Scattered Training Patches](https://arxiv.org/abs/2506.03614)
*Zhanhui Zhou,Lingjie Chen,Chao Yang,Chaochao Lu*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型（VLMs）具有将分散的图像碎片信息拼接起来的能力，即视觉拼接能力，这可能导致有害内容生成。实验表明开源VLMs具备这种能力，并通过模拟对抗性数据投毒场景展示了潜在的安全风险。


<details>
  <summary>Details</summary>
Motivation: 尽管移除训练数据中的危险样本可以降低视觉语言模型（VLMs）的风险，但有害图像可能被分割成看似无害的小块散布在多个训练样本中，使模型在训练时学会将这些碎片拼接起来，在推理时生成有害响应。

Method: 1. 定义并验证了VLMs的视觉拼接能力：通过将图像分割为带标签的碎片进行微调，测试模型是否能从完整图像或文本参考中正确输出标签。2. 模拟对抗性数据投毒场景：用有害图像的碎片替换标签为'安全'或'不安全'的文本描述，验证有害内容如何通过视觉拼接绕过审查。

Result: 实验证明，常见的开源VLMs确实具备视觉拼接能力，能够从完整图像或文本参考中正确识别合成ID。此外，模拟的对抗性数据投毒场景成功展示了有害内容如何通过视觉拼接绕过数据审查，从而揭示了VLMs的安全隐患。

Conclusion: 视觉拼接是VLMs的一项核心能力，但也带来了严重安全风险。研究强调需要更有效的数据审查和模型安全机制以防止有害内容的生成与传播。

Abstract: One way to mitigate risks in vision-language models (VLMs) is to remove
dangerous samples in their training data. However, such data moderation can be
easily bypassed when harmful images are split into small, benign-looking
patches, scattered across many training samples. VLMs may then learn to piece
these fragments together during training and generate harmful responses at
inference, either from full images or text references. For instance, if trained
on image patches from a bloody scene paired with the descriptions "safe," VLMs
may later describe, the full image or a text reference to the scene, as "safe."
We define the core ability of VLMs enabling this attack as $\textit{visual
stitching}$ -- the ability to integrate visual information spread across
multiple training samples that share the same textual descriptions. In our
work, we first demonstrate visual stitching abilities in common open-source
VLMs on three datasets where each image is labeled with a unique synthetic ID:
we split each $(\texttt{image}, \texttt{ID})$ pair into $\{(\texttt{patch},
\texttt{ID})\}$ pairs at different granularity for finetuning, and we find that
tuned models can verbalize the correct IDs from full images or text reference.
Building on this, we simulate the adversarial data poisoning scenario mentioned
above by using patches from dangerous images and replacing IDs with text
descriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can
evade moderation in patches and later be reconstructed through visual
stitching, posing serious VLM safety risks. Code is available at
https://github.com/ZHZisZZ/visual-stitching.

</details>


### [216] [Negative-Guided Subject Fidelity Optimization for Zero-Shot Subject-Driven Generation](https://arxiv.org/abs/2506.03621)
*Chaehun Shin,Jooyoung Choi,Johan Barthelemy,Jungbeom Lee,Sungroh Yoon*

Main category: cs.CV

TL;DR: This paper proposes Subject Fidelity Optimization (SFO), a new comparative learning framework for zero-shot subject-driven generation that enhances subject fidelity by introducing synthetic negative targets through Condition-Degradation Negative Sampling (CDNS) and reweighting diffusion timesteps. Experiments show SFO with CDNS significantly outperforms baselines in subject fidelity and text alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance subject fidelity in zero-shot subject-driven generation beyond supervised fine-tuning methods that only rely on positive targets and use the diffusion loss as in the pre-training stage.

Method: The method proposed in this paper is Subject Fidelity Optimization (SFO), which introduces synthetic negative targets and explicitly guides the model to favor positives over negatives through pairwise comparison. For negative targets, they propose Condition-Degradation Negative Sampling (CDNS), which automatically generates distinctive and informative negatives by intentionally degrading visual and textual cues without expensive human annotations. Moreover, they reweight the diffusion timesteps to focus finetuning on intermediate steps where subject details emerge.

Result: Extensive experiments demonstrate that SFO with CDNS significantly outperforms baselines in terms of both subject fidelity and text alignment on a subject-driven generation benchmark.

Conclusion: In conclusion, the paper presents SFO, a novel comparative learning framework for zero-shot subject-driven generation that enhances subject fidelity through the introduction of synthetic negative targets and reweighted diffusion timesteps.

Abstract: We present Subject Fidelity Optimization (SFO), a novel comparative learning
framework for zero-shot subject-driven generation that enhances subject
fidelity. Beyond supervised fine-tuning methods that rely only on positive
targets and use the diffusion loss as in the pre-training stage, SFO introduces
synthetic negative targets and explicitly guides the model to favor positives
over negatives through pairwise comparison. For negative targets, we propose
Condition-Degradation Negative Sampling (CDNS), which automatically generates
distinctive and informative negatives by intentionally degrading visual and
textual cues without expensive human annotations. Moreover, we reweight the
diffusion timesteps to focus finetuning on intermediate steps where subject
details emerge. Extensive experiments demonstrate that SFO with CDNS
significantly outperforms baselines in terms of both subject fidelity and text
alignment on a subject-driven generation benchmark. Project page:
https://subjectfidelityoptimization.github.io/

</details>


### [217] [Farm-LightSeek: An Edge-centric Multimodal Agricultural IoT Data Analytics Framework with Lightweight LLMs](https://arxiv.org/abs/2506.03168)
*Dawen Jiang,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Wei Xiang,Jiong Jin*

Main category: cs.CV

TL;DR: This paper proposes Farm-LightSeek, an edge-centric multimodal agricultural IoT data analytics framework that integrates LLMs with edge computing to address challenges in smart agriculture.


<details>
  <summary>Details</summary>
Motivation: Smart agriculture faces challenges such as excessive reliance on agricultural expert knowledge, difficulties in fusing multimodal data, poor adaptability to dynamic environments, and bottlenecks in real-time decision-making at the edge.

Method: Farm-LightSeek collects real-time farmland multi-source data via sensors, performs cross-modal reasoning and disease detection at edge nodes, conducts low-latency management decisions, and enables cloud collaboration for model updates. It features an agricultural 'perception-decision-action' closed-loop architecture, cross-modal adaptive monitoring, and a lightweight LLM deployment strategy balancing performance and efficiency.

Result: Experiments on two real-world datasets show that Farm-LightSeek achieves reliable performance in mission-critical tasks despite edge computing resource limitations.

Conclusion: This work advances intelligent real-time agricultural solutions and emphasizes the potential of integrating agricultural IoT with LLMs.

Abstract: Amid the challenges posed by global population growth and climate change,
traditional agricultural Internet of Things (IoT) systems is currently
undergoing a significant digital transformation to facilitate efficient big
data processing. While smart agriculture utilizes artificial intelligence (AI)
technologies to enable precise control, it still encounters significant
challenges, including excessive reliance on agricultural expert knowledge,
difficulties in fusing multimodal data, poor adaptability to dynamic
environments, and bottlenecks in real-time decision-making at the edge. Large
language models (LLMs), with their exceptional capabilities in knowledge
acquisition and semantic understanding, provide a promising solution to address
these challenges. To this end, we propose Farm-LightSeek, an edge-centric
multimodal agricultural IoT data analytics framework that integrates LLMs with
edge computing. This framework collects real-time farmland multi-source data
(images, weather, geographic information) via sensors, performs cross-modal
reasoning and disease detection at edge nodes, conducts low-latency management
decisions, and enables cloud collaboration for model updates. The main
innovations of Farm-LightSeek include: (1) an agricultural
"perception-decision-action" closed-loop architecture; (2) cross-modal adaptive
monitoring; and (3)a lightweight LLM deployment strategy balancing performance
and efficiency. Experiments conducted on two real-world datasets demonstrate
that Farm-LightSeek consistently achieves reliable performance in
mission-critical tasks, even under the limitations of edge computing resources.
This work advances intelligent real-time agricultural solutions and highlights
the potential for deeper integration of agricultural IoT with LLMs.

</details>


### [218] [Spatial Understanding from Videos: Structured Prompts Meet Simulation Data](https://arxiv.org/abs/2506.03642)
*Haoyu Zhang,Meng Liu,Zaijing Li,Haokun Wen,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: The paper presents a unified framework to enhance 3D spatial reasoning in pre-trained vision-language models (VLMs) without modifying their architecture.


<details>
  <summary>Details</summary>
Motivation: Existing methods for visual-spatial understanding face challenges of spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained VLMs.

Method: The framework combines SpatialMind, a structured prompting strategy that breaks down complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning.

Result: Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of the prompting and fine-tuning strategies.

Conclusion: This work yields insights that may inspire future research on visual-spatial understanding.

Abstract: Visual-spatial understanding, the ability to infer object relationships and
layouts from visual input, is fundamental to downstream tasks such as robotic
navigation and embodied interaction. However, existing methods face spatial
uncertainty and data scarcity, limiting the 3D spatial reasoning capability of
pre-trained vision-language models (VLMs). To address these challenges, we
present a unified framework for enhancing 3D spatial reasoning in pre-trained
VLMs without modifying their architecture. This framework combines SpatialMind,
a structured prompting strategy that decomposes complex scenes and questions
into interpretable reasoning steps, with ScanForgeQA, a scalable
question-answering dataset built from diverse 3D simulation scenes through an
automated construction process designed for fine-tuning. Extensive experiments
across multiple benchmarks demonstrate the individual and combined
effectiveness of our prompting and fine-tuning strategies, and yield insights
that may inspire future research on visual-spatial understanding.

</details>


### [219] [MambaNeXt-YOLO: A Hybrid State Space Model for Real-time Object Detection](https://arxiv.org/abs/2506.03654)
*Xiaochun Lei,Siqi Wu,Weilin Wu,Zetao Jiang*

Main category: cs.CV

TL;DR: 为了克服YOLO系列模型在实时目标检测任务中对更丰富全局上下文建模的需求以及Transformer架构计算复杂度过高的问题，本文提出了一种新的目标检测框架MambaNeXt-YOLO。该框架结合了CNN和Mamba线性状态空间模型，以有效捕获局部特征和长程依赖，并通过多分支非对称融合金字塔网络（MAFPN）提高了多尺度目标检测的效果。实验结果表明，该方法在PASCAL VOC数据集上达到了66.6%的mAP，且速度为31.9 FPS，无需预训练即可实现在边缘设备上的部署。


<details>
  <summary>Details</summary>
Motivation: 实时目标检测在计算机视觉中是一个基本但具有挑战性的任务，特别是在计算资源有限的情况下。尽管YOLO系列模型在平衡速度和准确性方面设定了强大的基准，但对于更丰富的全局上下文建模的需求日益增长，促使人们使用基于Transformer的架构。然而，由于自注意力机制的存在，Transformer的计算复杂度较高，限制了其在实时和边缘部署中的实用性。因此，需要一种既能保持高精度又能高效运行的方法来解决这一问题。

Method: 本文提出了MambaNeXt-YOLO，这是一种新型的目标检测框架，主要包含三个关键贡献：(1) MambaNeXt Block：一种混合设计，将CNN与Mamba集成在一起，以有效捕获局部特征和长程依赖；(2) 多分支非对称融合金字塔网络（MAFPN）：一种增强的特征金字塔架构，可改善对各种大小目标的多尺度检测；(3) 边缘聚焦效率：该方法在PASCAL VOC数据集上实现了66.6%的mAP，速度为31.9 FPS，且无需预训练即可支持在如NVIDIA Jetson Xavier NX和Orin NX等边缘设备上的部署。

Result: MambaNeXt-YOLO在PASCAL VOC数据集上实现了66.6%的mAP，速度达到31.9 FPS，且无需任何预训练。此外，该方法能够实现在NVIDIA Jetson Xavier NX和Orin NX等边缘设备上的部署，显示出其在实际应用中的潜力和高效性。

Conclusion: MambaNeXt-YOLO是一种新型的目标检测框架，通过结合CNN和Mamba线性状态空间模型，以及采用多分支非对称融合金字塔网络（MAFPN），成功地在准确性和效率之间取得了平衡。实验结果证明，该方法不仅在性能上表现优异，而且能够在边缘设备上进行高效的部署，适用于计算资源受限的场景。

Abstract: Real-time object detection is a fundamental but challenging task in computer
vision, particularly when computational resources are limited. Although
YOLO-series models have set strong benchmarks by balancing speed and accuracy,
the increasing need for richer global context modeling has led to the use of
Transformer-based architectures. Nevertheless, Transformers have high
computational complexity because of their self-attention mechanism, which
limits their practicality for real-time and edge deployments. To overcome these
challenges, recent developments in linear state space models, such as Mamba,
provide a promising alternative by enabling efficient sequence modeling with
linear complexity. Building on this insight, we propose MambaNeXt-YOLO, a novel
object detection framework that balances accuracy and efficiency through three
key contributions: (1) MambaNeXt Block: a hybrid design that integrates CNNs
with Mamba to effectively capture both local features and long-range
dependencies; (2) Multi-branch Asymmetric Fusion Pyramid Network (MAFPN): an
enhanced feature pyramid architecture that improves multi-scale object
detection across various object sizes; and (3) Edge-focused Efficiency: our
method achieved 66.6\% mAP at 31.9 FPS on the PASCAL VOC dataset without any
pre-training and supports deployment on edge devices such as the NVIDIA Jetson
Xavier NX and Orin NX.

</details>


### [220] [Accelerating SfM-based Pose Estimation with Dominating Set](https://arxiv.org/abs/2506.03667)
*Joji Joseph,Bharadwaj Amrutur,Shalabh Bhatnagar*

Main category: cs.CV

TL;DR: This paper presents a preprocessing technique using dominating set concept to accelerate SfM-based pose estimation for real-time applications, achieving significant speed improvements without substantial accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the need for faster pose estimation in real-time applications such as AR, VR, and robotics, leveraging Structure-from-Motion (SfM) techniques.

Method: The method applies the dominating set concept from graph theory to preprocess SfM models, reducing reference images and point cloud size.

Result: The method achieves 1.5 to 14.48 times speedup in processing, with reductions in reference images by factors of 17-23 and point cloud size by factors of 2.27-4, maintaining accuracy.

Conclusion: The proposed preprocessing technique offers an efficient solution for accurate and fast 3D pose estimation suitable for real-time applications.

Abstract: This paper introduces a preprocessing technique to speed up
Structure-from-Motion (SfM) based pose estimation, which is critical for
real-time applications like augmented reality (AR), virtual reality (VR), and
robotics. Our method leverages the concept of a dominating set from graph
theory to preprocess SfM models, significantly enhancing the speed of the pose
estimation process without losing significant accuracy. Using the OnePose
dataset, we evaluated our method across various SfM-based pose estimation
techniques. The results demonstrate substantial improvements in processing
speed, ranging from 1.5 to 14.48 times, and a reduction in reference images and
point cloud size by factors of 17-23 and 2.27-4, respectively. This work offers
a promising solution for efficient and accurate 3D pose estimation, balancing
speed and accuracy in real-time applications.

</details>


### [221] [TerraIncognita: A Dynamic Benchmark for Species Discovery Using Frontier Models](https://arxiv.org/abs/2506.03182)
*Shivani Chiranjeevi,Hossein Zaremehrjerdi,Zi K. Deng,Talukder Z. Jubery,Ari Grele,Arti Singh,Asheesh K Singh,Soumik Sarkar,Nirav Merchant,Harold F. Greeney,Baskar Ganapathysubramanian,Chinmay Hegde*

Main category: cs.CV

TL;DR: The paper introduces TerraIncognita, a benchmark for identifying unknown insect species using multimodal models and image data. It assesses models' proficiency in hierarchical taxonomic classification, OOD detection, and explanation generation aligned with expert knowledge.


<details>
  <summary>Details</summary>
Motivation: Current methods for insect species discovery are manual, slow, and constrained by taxonomic expertise, hindering timely conservation actions.

Method: A dynamic benchmark dataset combining expertly annotated images of known insect species and rare/poorly known species collected from underexplored biodiversity hotspots. The benchmark evaluates models on hierarchical taxonomic classification, OOD detection, and explanation generation aligned with expert taxonomic knowledge.

Result: Top-performing models achieve over 90% F1 at the Order level on known species but drop below 2% at the Species level, highlighting the difficulty gradient from coarse to fine taxonomic prediction.

Conclusion: TerraIncognita will be regularly updated with quarterly dataset expansions, providing an evolving platform for longitudinal benchmarking of AI methods in insect species identification.

Abstract: The rapid global loss of biodiversity, particularly among insects, represents
an urgent ecological crisis. Current methods for insect species discovery are
manual, slow, and severely constrained by taxonomic expertise, hindering timely
conservation actions. We introduce TerraIncognita, a dynamic benchmark designed
to evaluate state-of-the-art multimodal models for the challenging problem of
identifying unknown, potentially undescribed insect species from image data.
Our benchmark dataset combines a mix of expertly annotated images of insect
species likely known to frontier AI models, and images of rare and poorly known
species, for which few/no publicly available images exist. These images were
collected from underexplored biodiversity hotspots, realistically mimicking
open-world discovery scenarios faced by ecologists. The benchmark assesses
models' proficiency in hierarchical taxonomic classification, their capability
to detect and abstain from out-of-distribution (OOD) samples representing novel
species, and their ability to generate explanations aligned with expert
taxonomic knowledge. Notably, top-performing models achieve over 90\% F1 at the
Order level on known species, but drop below 2\% at the Species level,
highlighting the sharp difficulty gradient from coarse to fine taxonomic
prediction (Order $\rightarrow$ Family $\rightarrow$ Genus $\rightarrow$
Species). TerraIncognita will be updated regularly, and by committing to
quarterly dataset expansions (of both known and novel species), will provide an
evolving platform for longitudinal benchmarking of frontier AI methods. All
TerraIncognita data, results, and future updates are available
\href{https://baskargroup.github.io/TerraIncognita/}{here}.

</details>


### [222] [How PARTs assemble into wholes: Learning the relative composition of images](https://arxiv.org/abs/2506.03682)
*Melika Ayoughi,Samira Abnar,Chen Huang,Chris Sandino,Sayeri Lala,Eeshan Gunesh Dhekane,Dan Busbridge,Shuangfei Zhai,Vimal Thilak,Josh Susskind,Pascal Mettes,Paul Groth,Hanlin Goh*

Main category: cs.CV

TL;DR: PART是一种新的自监督学习方法，通过利用非网格补丁之间的连续相对变换来克服现有基于网格方法的局限性。它在需要精确空间理解的任务（如目标检测和时间序列预测）中表现出色，并且在全局分类任务上也具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法通常基于网格结构，无法很好地捕捉现实世界物体组合的流动性和连续性。

Method: PART通过建模部分在连续空间中的相互关系，学习图像的相对组成——一个非网格结构的相对定位过程，该过程可以超越遮挡和变形进行泛化。

Result: 在需要精确空间理解的任务（如目标检测和时间序列预测）中，PART优于强大的基于网格的方法（如MAE和DropPos），同时在全局分类任务上也保持了竞争力。

Conclusion: 通过摆脱网格约束，PART为从自然图像到EEG信号的各种数据类型的通用自监督预训练开辟了一条令人兴奋的新路径，在视频、医学成像和音频等领域具有广阔的应用前景。

Abstract: The composition of objects and their parts, along with object-object
positional relationships, provides a rich source of information for
representation learning. Hence, spatial-aware pretext tasks have been actively
explored in self-supervised learning. Existing works commonly start from a grid
structure, where the goal of the pretext task involves predicting the absolute
position index of patches within a fixed grid. However, grid-based approaches
fall short of capturing the fluid and continuous nature of real-world object
compositions. We introduce PART, a self-supervised learning approach that
leverages continuous relative transformations between off-grid patches to
overcome these limitations. By modeling how parts relate to each other in a
continuous space, PART learns the relative composition of images-an off-grid
structural relative positioning process that generalizes beyond occlusions and
deformations. In tasks requiring precise spatial understanding such as object
detection and time series prediction, PART outperforms strong grid-based
methods like MAE and DropPos, while also maintaining competitive performance on
global classification tasks with minimal hyperparameter tuning. By breaking
free from grid constraints, PART opens up an exciting new trajectory for
universal self-supervised pretraining across diverse datatypes-from natural
images to EEG signals-with promising potential in video, medical imaging, and
audio.

</details>


### [223] [OSGNet @ Ego4D Episodic Memory Challenge 2025](https://arxiv.org/abs/2506.03710)
*Yisen Feng,Haoyu Zhang,Qiaohui Chu,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: This paper presents the champion solutions for three egocentric video localization tracks in the Ego4D Episodic Memory Challenge. The authors use an early fusion-based video localization model to achieve first place in all three tasks.


<details>
  <summary>Details</summary>
Motivation: Previous unified video localization approaches often rely on late fusion strategies, which tend to yield suboptimal results.

Method: The authors adopt an early fusion-based video localization model to address the challenges of precise localization within untrimmed egocentric videos.

Result: The method achieved first place in the Natural Language Queries, Goal Step, and Moment Queries tracks.

Conclusion: The early fusion-based video localization model is effective in enhancing localization accuracy for egocentric video localization tasks.

Abstract: In this report, we present our champion solutions for the three egocentric
video localization tracks of the Ego4D Episodic Memory Challenge at CVPR 2025.
All tracks require precise localization of the interval within an untrimmed
egocentric video. Previous unified video localization approaches often rely on
late fusion strategies, which tend to yield suboptimal results. To address
this, we adopt an early fusion-based video localization model to tackle all
three tasks, aiming to enhance localization accuracy. Ultimately, our method
achieved first place in the Natural Language Queries, Goal Step, and Moment
Queries tracks, demonstrating its effectiveness. Our code can be found at
https://github.com/Yisen-Feng/OSGNet.

</details>


### [224] [Human Fall Detection using Transfer Learning-based 3D CNN](https://arxiv.org/abs/2506.03193)
*Ekram Alam,Abu Sufian,Paramartha Dutta,Marco Leo*

Main category: cs.CV

TL;DR: This paper proposes a vision-based fall detection system using a pre-trained 3D CNN for feature extraction and an SVM classifier for classification. The model leverages spatio-temporal features to detect falls in senior persons with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Unintentional or accidental falls are significant health issues in senior persons, and the population of senior persons is increasing steadily. There is a need for an automated fall detection monitoring system.

Method: The method uses a pre-trained 3D CNN model on the Sports1M dataset to extract spatio-temporal features from video data. These features are then fed into an SVM classifier which is trained to classify activities as either falls or ADLs (Activities of Daily Living). Stratified shuffle five split cross-validation is used to ensure robustness.

Result: Experiments conducted on two datasets, GMDCSA and CAUCAFall, demonstrated the effectiveness of the proposed system. Specific performance metrics such as accuracy, precision, recall, and F1-score were not explicitly mentioned in the abstract.

Conclusion: The proposed vision-based fall detection system using a pre-trained 3D CNN and SVM classifier shows promise in accurately detecting falls among senior persons.

Abstract: Unintentional or accidental falls are one of the significant health issues in
senior persons. The population of senior persons is increasing steadily. So,
there is a need for an automated fall detection monitoring system. This paper
introduces a vision-based fall detection system using a pre-trained 3D CNN.
Unlike 2D CNN, 3D CNN extracts not only spatial but also temporal features. The
proposed model leverages the original learned weights of a 3D CNN model
pre-trained on the Sports1M dataset to extract the spatio-temporal features.
Only the SVM classifier was trained, which saves the time required to train the
3D CNN. Stratified shuffle five split cross-validation has been used to split
the dataset into training and testing data. Extracted features from the
proposed 3D CNN model were fed to an SVM classifier to classify the activity as
fall or ADL. Two datasets, GMDCSA and CAUCAFall, were utilized to conduct the
experiment. The source code for this work can be accessed via the following
link: https://github.com/ekramalam/HFD_3DCNN.

</details>


### [225] [ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices](https://arxiv.org/abs/2506.03737)
*Hao Yu,Tangyu Jiang,Shuning Jia,Shannan Yan,Shunning Liu,Haolong Qian,Guanghao Li,Shuting Dong,Huaisong Zhang,Chun Yuan*

Main category: cs.CV

TL;DR: The paper proposes ComRoPE, an improvement on Rotary Positional Encoding (RoPE) through trainable commuting angle matrices, enhancing performance and scalability while offering new insights for positional encoding research.


<details>
  <summary>Details</summary>
Motivation: Traditional position encoding methods, including RoPE, have limitations in terms of robustness, flexibility, and model capacity. This motivated the need for a more generalized and effective approach to position encoding.

Method: The authors propose ComRoPE which generalizes RoPE by introducing trainable commuting angle matrices. They emphasize pairwise commutativity of these matrices for scalability and robustness, define the RoPE Equation to ensure consistent performance with position offsets, and present two types of trainable commuting angle matrices as solutions.

Result: ComRoPE surpasses the current state-of-the-art method by 1.6% at training resolution and 2.9% at higher resolution on the ImageNet-1K dataset, demonstrating significant performance improvements.

Conclusion: ComRoPE not only enhances the performance and scalability of RoPE but also provides versatility in integrating with existing formulations and offers valuable insights for future research in positional encoding.

Abstract: The Transformer architecture has revolutionized various regions since it was
proposed, and its effectiveness largely depends on the ability to encode
positional information. Traditional position encoding methods exhibit
significant limitations due to lack of robustness and flexibility of position.
Therefore, Rotary Positional Encoding (RoPE) was proposed to alleviate these
issues, which integrates positional information by rotating the embeddings in
the attention mechanism. However, RoPE requires manually defined rotation
matrices with limited transformation space, constraining the model's capacity.
In this work, we propose ComRoPE, which generalizes RoPE by defining it in
terms of trainable commuting angle matrices. Specifically, we demonstrate that
pairwise commutativity of these matrices is essential for RoPE to achieve
scalability and positional robustness. We formally define the RoPE Equation,
which is an essential condition that ensures consistent performance with
position offsets. Based on the theoretical analysis, we present two types of
trainable commuting angle matrices as sufficient solutions to the RoPE
equation, which significantly improve performance, surpassing the current
state-of-the-art method by 1.6% at training resolution and 2.9% at higher
resolution on the ImageNet-1K dataset. Furthermore, our framework shows
versatility in generalizing to existing RoPE formulations and offering new
insights for future positional encoding research. To ensure reproducibility,
the source code and instructions are available at
https://github.com/Longin-Yu/ComRoPE

</details>


### [226] [SAAT: Synergistic Alternating Aggregation Transformer for Image Super-Resolution](https://arxiv.org/abs/2506.03740)
*Jianfeng Wu,Nannan Xu*

Main category: cs.CV

TL;DR: Single image super-resolution using Synergistic Alternating Aggregation Transformer (SAAT) with novel attention groups for better feature utilization.


<details>
  <summary>Details</summary>
Motivation: Current methods in single image super-resolution based on Transformers compute self-attention in nonoverlapping windows, neglecting useful information across channels and rich spatial structural information generated in the intermediate process. Channel and spatial attention improvements have been made separately but their synergistic relationship has not been fully explored.

Method: Propose SAAT model with two key components: Efficient Channel & Window Synergistic Attention Group (CWSAG) and Spatial & Window Synergistic Attention Group (SWSAG). CWSAG combines channel attention with shifted window attention for enhanced non-local feature fusion. SWSAG leverages spatial attention to capture structured feature information.

Result: Extensive experiments and ablation studies show the effectiveness of SAAT in super-resolution tasks. It achieves performance comparable to state-of-the-art models under the same parameter count.

Conclusion: SAAT effectively utilizes potential feature information through its novel attention groups, demonstrating strong performance in single image super-resolution.

Abstract: Single image super-resolution is a well-known downstream task which aims to
restore low-resolution images into high-resolution images. At present, models
based on Transformers have shone brightly in the field of super-resolution due
to their ability to capture long-term dependencies in information. However,
current methods typically compute self-attention in nonoverlapping windows to
save computational costs, and the standard self-attention computation only
focuses on its results, thereby neglecting the useful information across
channels and the rich spatial structural information generated in the
intermediate process. Channel attention and spatial attention have,
respectively, brought significant improvements to various downstream visual
tasks in terms of extracting feature dependency and spatial structure
relationships, but the synergistic relationship between channel and spatial
attention has not been fully explored yet.To address these issues, we propose a
novel model. Synergistic Alternating Aggregation Transformer (SAAT), which can
better utilize the potential information of features. In SAAT, we introduce the
Efficient Channel & Window Synergistic Attention Group (CWSAG) and the Spatial
& Window Synergistic Attention Group (SWSAG). On the one hand, CWSAG combines
efficient channel attention with shifted window attention, enhancing non-local
feature fusion, and producing more visually appealing results. On the other
hand, SWSAG leverages spatial attention to capture rich structured feature
information, thereby enabling SAAT to more effectively extract structural
features.Extensive experimental results and ablation studies demonstrate the
effectiveness of SAAT in the field of super-resolution. SAAT achieves
performance comparable to that of the state-of-the-art (SOTA) under the same
quantity of parameters.

</details>


### [227] [JointSplat: Probabilistic Joint Flow-Depth Optimization for Sparse-View Gaussian Splatting](https://arxiv.org/abs/2506.03872)
*Yang Xiao,Guoan Xu,Qiang Wu,Wenjing Jia*

Main category: cs.CV

TL;DR: Reconstructing 3D scenes from sparse viewpoints is challenging. Recent methods have limitations, so JointSplat, a new framework, is proposed to overcome these by leveraging the complementarity between optical flow and depth via probabilistic optimization mechanism and proposing a multi-view depth-consistency loss.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations in current feed-forward 3D Gaussian sparse-view reconstruction methods which suffer from mislocation, artifact issues, local noise, and global inconsistency especially in low-texture or repetitive regions when ground-truth flow supervision is unavailable.

Method: JointSplat, a unified framework that uses a novel probabilistic optimization mechanism at the pixel-level to scale the information fusion between depth and flow based on the matching probability of optical flow during training. Also, a multi-view depth-consistency loss is proposed to leverage reliable supervision while suppressing misleading gradients in uncertain areas.

Result: JointSplat consistently outperforms state-of-the-art methods on RealEstate10K and ACID datasets, showing its effectiveness and robustness for high-fidelity sparse-view 3D reconstruction.

Conclusion: JointSplat demonstrates superior performance compared to existing methods, indicating that the proposed probabilistic joint flow-depth optimization approach is effective for overcoming limitations in sparse-view 3D reconstruction.

Abstract: Reconstructing 3D scenes from sparse viewpoints is a long-standing challenge
with wide applications. Recent advances in feed-forward 3D Gaussian sparse-view
reconstruction methods provide an efficient solution for real-time novel view
synthesis by leveraging geometric priors learned from large-scale multi-view
datasets and computing 3D Gaussian centers via back-projection. Despite
offering strong geometric cues, both feed-forward multi-view depth estimation
and flow-depth joint estimation face key limitations: the former suffers from
mislocation and artifact issues in low-texture or repetitive regions, while the
latter is prone to local noise and global inconsistency due to unreliable
matches when ground-truth flow supervision is unavailable. To overcome this, we
propose JointSplat, a unified framework that leverages the complementarity
between optical flow and depth via a novel probabilistic optimization
mechanism. Specifically, this pixel-level mechanism scales the information
fusion between depth and flow based on the matching probability of optical flow
during training. Building upon the above mechanism, we further propose a novel
multi-view depth-consistency loss to leverage the reliability of supervision
while suppressing misleading gradients in uncertain areas. Evaluated on
RealEstate10K and ACID, JointSplat consistently outperforms state-of-the-art
(SOTA) methods, demonstrating the effectiveness and robustness of our proposed
probabilistic joint flow-depth optimization approach for high-fidelity
sparse-view 3D reconstruction.

</details>


### [228] [DiffCAP: Diffusion-based Cumulative Adversarial Purification for Vision Language Models](https://arxiv.org/abs/2506.03933)
*Jia Fu,Yongtao Wu,Yihang Chen,Kunyu Peng,Xiao Zhang,Volkan Cevher,Sepideh Pashami,Anders Holst*

Main category: cs.CV

TL;DR: Vision Language Models (VLMs) are vulnerable to perturbations which can lead to erroneous outputs. This paper introduces DiffCAP, a novel diffusion-based purification strategy that neutralizes adversarial corruptions in VLMs effectively and efficiently.


<details>
  <summary>Details</summary>
Motivation: To address the issue of VLMs' susceptibility to perturbations that can drastically alter their outputs, leading to potential errors in real-world applications.

Method: DiffCAP works by cumulatively injecting random Gaussian noise into adversarially perturbed input data until the embeddings of two consecutive noisy images reach a predefined similarity threshold. Then, a pretrained diffusion model is used to denoise the stabilized image, recovering a clean representation for VLMs.

Result: Through extensive experiments across six datasets with three VLMs under varying attack strengths in three task scenarios, DiffCAP consistently outperforms existing defense techniques. It also reduces hyperparameter tuning complexity and accelerates the denoising process.

Conclusion: DiffCAP provides a robust and practical solution for securely deploying VLMs in adversarial environments, supported by strong theoretical and empirical evidence.

Abstract: Vision Language Models (VLMs) have shown remarkable capabilities in
multimodal understanding, yet their susceptibility to perturbations poses a
significant threat to their reliability in real-world applications. Despite
often being imperceptible to humans, these perturbations can drastically alter
model outputs, leading to erroneous interpretations and decisions. This paper
introduces DiffCAP, a novel diffusion-based purification strategy that can
effectively neutralize adversarial corruptions in VLMs. We observe that adding
minimal noise to an adversarially corrupted image significantly alters its
latent embedding with respect to VLMs. Building on this insight, DiffCAP
cumulatively injects random Gaussian noise into adversarially perturbed input
data. This process continues until the embeddings of two consecutive noisy
images reach a predefined similarity threshold, indicating a potential approach
to neutralize the adversarial effect. Subsequently, a pretrained diffusion
model is employed to denoise the stabilized image, recovering a clean
representation suitable for the VLMs to produce an output. Through extensive
experiments across six datasets with three VLMs under varying attack strengths
in three task scenarios, we show that DiffCAP consistently outperforms existing
defense techniques by a substantial margin. Notably, DiffCAP significantly
reduces both hyperparameter tuning complexity and the required diffusion time,
thereby accelerating the denoising process. Equipped with strong theoretical
and empirical support, DiffCAP provides a robust and practical solution for
securely deploying VLMs in adversarial environments.

</details>


### [229] [Mitigating Hallucinations in Large Vision-Language Models via Entity-Centric Multimodal Preference Optimization](https://arxiv.org/abs/2506.04039)
*Jiulong Wu,Zhengliang Shi,Shuaiqiang Wang,Jizhou Huang,Dawei Yin,Lingyong Yan,Min Cao,Min Zhang*

Main category: cs.CV

TL;DR: The paper proposes Entity-centric Multimodal Preference Optimization (EMPO) to improve modality alignment in Large Visual Language Models and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: Large Visual Language Models suffer from trustworthiness issues due to hallucinations caused by modality misalignment and inherent limitations of their Large Language Models backbone.

Method: Propose EMPO which enhances modality alignment compared to existing human preference alignment methods and uses open-source instruction datasets to construct high-quality preference data in image, instruction, and response aspects.

Result: Experiments on human preference datasets and multimodal hallucination benchmarks show the effectiveness of EMPO, significantly reducing hallucination rates.

Conclusion: EMPO achieves enhanced modality alignment and effectively reduces hallucinations in Large Visual Language Models.

Abstract: Large Visual Language Models (LVLMs) have demonstrated impressive
capabilities across multiple tasks. However, their trustworthiness is often
challenged by hallucinations, which can be attributed to the modality
misalignment and the inherent hallucinations of their underlying Large Language
Models (LLMs) backbone. Existing preference alignment methods focus on aligning
model responses with human preferences while neglecting image-text modality
alignment, resulting in over-reliance on LLMs and hallucinations. In this
paper, we propose Entity-centric Multimodal Preference Optimization (EMPO),
which achieves enhanced modality alignment than existing human preference
alignment methods. Besides, to overcome the scarcity of high-quality multimodal
preference data, we utilize open-source instruction datasets to automatically
construct high-quality preference data across three aspects: image,
instruction, and response. Experiments on two human preference datasets and
five multimodal hallucination benchmarks demonstrate the effectiveness of EMPO,
e.g., reducing hallucination rates by 85.9% on Object-HalBench and 49.8% on
MM-HalBench.

</details>


### [230] [Intersectional Bias in Pre-Trained Image Recognition Models](https://arxiv.org/abs/2506.03664)
*Valerie Krug,Sebastian Stober*

Main category: cs.CV

TL;DR: Deep Learning models, especially ImageNet classifiers, contain biases in their representations that can differentiate age, associate ethnicities and distinguish genders.


<details>
  <summary>Details</summary>
Motivation: To investigate the biases in the representations of commonly used ImageNet classifiers for facial images while considering intersections of sensitive variables age, race and gender.

Method: Using linear classifier probes and visualizing activations as topographic maps to assess the biases.

Result: Representations in ImageNet classifiers particularly allow differentiation between ages. Less strongly pronounced, the models appear to associate certain ethnicities and distinguish genders in middle-aged groups.

Conclusion: ImageNet classifiers have notable biases in their representations which could perpetuate encoded biases when training new models.

Abstract: Deep Learning models have achieved remarkable success. Training them is often
accelerated by building on top of pre-trained models which poses the risk of
perpetuating encoded biases. Here, we investigate biases in the representations
of commonly used ImageNet classifiers for facial images while considering
intersections of sensitive variables age, race and gender. To assess the
biases, we use linear classifier probes and visualize activations as
topographic maps. We find that representations in ImageNet classifiers
particularly allow differentiation between ages. Less strongly pronounced, the
models appear to associate certain ethnicities and distinguish genders in
middle-aged groups.

</details>


### [231] [Person Re-Identification System at Semantic Level based on Pedestrian Attributes Ontology](https://arxiv.org/abs/2506.04143)
*Ngoc Q. Ly,Hieu N. M. Cao,Thi T. Nguyen*

Main category: cs.CV

TL;DR: Person Re-Identification (Re-ID) is crucial in video surveillance but faces challenges like large-scale datasets, imbalanced data, and viewpoint issues. This paper proposes a Unified Re-ID system with three modules: Pedestrian Attribute Ontology (PAO), Local Multi-task DCNN (Local MDCNN), and Imbalance Data Solver (IDS). The system uses semantic information to pre-filter mismatch candidates and solve imbalanced data without changing network architecture or using data augmentation. Experiments on the Market1501 dataset show its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address challenges in Person Re-Identification such as large-scale datasets, imbalanced data, viewpoint issues, fine-grained data, and underutilized local features at the semantic level.

Method: The Unified Re-ID system consists of three modules: Pedestrian Attribute Ontology (PAO), Local Multi-task DCNN (Local MDCNN), and Imbalance Data Solver (IDS). It exploits inner-group correlations of attributes and pre-filters mismatch candidates from the Gallery set based on semantic information like Fashion Attributes and Facial Attributes.

Result: Experiments on the Market1501 dataset demonstrate the effectiveness of the proposed Re-ID system, achieving higher performance compared to some state-of-the-art Re-ID methods.

Conclusion: The proposed Unified Re-ID system effectively addresses challenges in Person Re-Identification, particularly in handling imbalanced data and utilizing semantic information for improved performance.

Abstract: Person Re-Identification (Re-ID) is a very important task in video
surveillance systems such as tracking people, finding people in public places,
or analysing customer behavior in supermarkets. Although there have been many
works to solve this problem, there are still remaining challenges such as
large-scale datasets, imbalanced data, viewpoint, fine grained data
(attributes), the Local Features are not employed at semantic level in online
stage of Re-ID task, furthermore, the imbalanced data problem of attributes are
not taken into consideration. This paper has proposed a Unified Re-ID system
consisted of three main modules such as Pedestrian Attribute Ontology (PAO),
Local Multi-task DCNN (Local MDCNN), Imbalance Data Solver (IDS). The new main
point of our Re-ID system is the power of mutual support of PAO, Local MDCNN
and IDS to exploit the inner-group correlations of attributes and pre-filter
the mismatch candidates from Gallery set based on semantic information as
Fashion Attributes and Facial Attributes, to solve the imbalanced data of
attributes without adjusting network architecture and data augmentation. We
experimented on the well-known Market1501 dataset. The experimental results
have shown the effectiveness of our Re-ID system and it could achieve the
higher performance on Market1501 dataset in comparison to some state-of-the-art
Re-ID methods.

</details>


### [232] [RAID: A Dataset for Testing the Adversarial Robustness of AI-Generated Image Detectors](https://arxiv.org/abs/2506.03988)
*Hicham Eddoubi,Jonas Ricker,Federico Cocchi,Lorenzo Baraldi,Angelo Sotgiu,Maura Pintor,Marcella Cornia,Lorenzo Baraldi,Asja Fischer,Rita Cucchiara,Battista Biggio*

Main category: cs.CV

TL;DR: The paper introduces RAID, a dataset of adversarial examples to assess the robustness of AI-generated image detectors.


<details>
  <summary>Details</summary>
Motivation: AI-generated images are hard for humans to distinguish from real ones, and current detection methods lack comprehensive robustness analysis.

Method: Create a dataset (RAID) by attacking an ensemble of 7 state-of-the-art detectors with images from 4 text-to-image models.

Result: Experiments show that adversarial images in RAID transfer successfully to unseen detectors, revealing vulnerabilities in current AI-generated image detectors.

Conclusion: There is a need for more robust AI-generated image detection methods.

Abstract: AI-generated images have reached a quality level at which humans are
incapable of reliably distinguishing them from real images. To counteract the
inherent risk of fraud and disinformation, the detection of AI-generated images
is a pressing challenge and an active research topic. While many of the
presented methods claim to achieve high detection accuracy, they are usually
evaluated under idealized conditions. In particular, the adversarial robustness
is often neglected, potentially due to a lack of awareness or the substantial
effort required to conduct a comprehensive robustness analysis. In this work,
we tackle this problem by providing a simpler means to assess the robustness of
AI-generated image detectors. We present RAID (Robust evaluation of
AI-generated image Detectors), a dataset of 72k diverse and highly transferable
adversarial examples. The dataset is created by running attacks against an
ensemble of seven state-of-the-art detectors and images generated by four
different text-to-image models. Extensive experiments show that our methodology
generates adversarial images that transfer with a high success rate to unseen
detectors, which can be used to quickly provide an approximate yet still
reliable estimate of a detector's adversarial robustnessOur findings indicate
that current state-of-the-art AI-generated image detectors can be easily
deceived by adversarial examples, highlighting the critical need for the
development of more robust methods. We release our dataset at
https://huggingface.co/datasets/aimagelab/RAID and evaluation code at
https://github.com/pralab/RAID.

</details>


### [233] [Sounding that Object: Interactive Object-Aware Image to Audio Generation](https://arxiv.org/abs/2506.04214)
*Tingle Li,Baihe Huang,Xiaobin Zhuang,Dongya Jia,Jiawei Chen,Yuping Wang,Zhuo Chen,Gopala Anumanchipalli,Yuxuan Wang*

Main category: cs.CV

TL;DR: This paper presents an interactive object-aware audio generation model that connects sound generation with user-selected visual objects in images, using a conditional latent diffusion model with multi-modal attention. It allows object-level sound generation and demonstrates superior performance compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Generating precise sounds for complex audio-visual scenes is difficult, particularly when multiple objects and sound sources are present. This motivates the development of a more targeted and interactive approach to audio generation tied to specific visual objects.

Method: The method combines object-centric learning into a conditional latent diffusion model. Multi-modal attention associates image regions with corresponding sounds. Image segmentation at test time enables interactive, object-level sound generation. The attention mechanism functionally approximates segmentation masks, ensuring alignment between generated audio and selected objects.

Result: Quantitative and qualitative evaluations indicate that the model performs better than baselines, achieving improved alignment between objects and their associated sounds.

Conclusion: The proposed interactive object-aware audio generation model effectively grounds sound generation in user-selected visual objects, outperforming existing methods in aligning objects with their sounds.

Abstract: Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [234] [Quantum Secure Key Exchange with Position-based Credentials](https://arxiv.org/abs/2506.03549)
*Wen Yu Kon,Ignatius William Primaatmaja,Kaushik Chakraborty,Charles Lim*

Main category: quant-ph

TL;DR: This paper extends the proposal of using location as a credential in Quantum Key Distribution (QKD) by developing a protocol that uses quantum position verification (QPV) for message and identity authentication. It reduces the number of QPV runs, which is currently a bottleneck, and provides improvements to QPV security analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to remove the reliance on pre-shared or public keys in QKD by utilizing the location of a party as a credential, as proposed by Buhrman et al.

Method: The method involves extending the proposal of Buhrman et al., developing a QKD protocol with location credentials using QPV-based message and identity authentication, reducing the number of QPV runs, and providing improvements to QPV security analysis.

Result: The result is a QKD protocol that significantly reduces the number of QPV runs and provides enhancements to QPV security analysis, including generalization of the QPV adversary model, tightening a trace distance bound, and proposing a multi-basis QPV.

Conclusion: The conclusion is that the proposed protocol successfully extends the use of location credentials in QKD, improves upon previous methods by reducing QPV runs, and enhances the security analysis of QPV.

Abstract: Quantum key distribution (QKD) provides an information-theoretic way of
securely exchanging secret keys, and typically relies on pre-shared keys or
public keys for message authentication. To lift the requirement of pre-shared
or public keys, Buhrman et. al. [SIAM J. Comput. 43, 150 (2014)] proposed
utilizing the location of a party as a credential. Here, we extend upon the
proposal, develop a QKD protocol with location credentials using quantum
position verification (QPV) based message and identity authentication. By using
QKD with delayed authentication as a base, and later simplifying QPV-based
message authentication, we significantly reduce the number of QPV runs, which
currently acts as a bottleneck. Besides demonstrating security for the proposed
protocol, we also provide improvements to QPV security analysis, including
generalization of the QPV adversary model, tightening a trace distance bound
using semidefinite programming, and propose a multi-basis QPV requiring only
BB84 state preparation but with multiple measurement basis.

</details>


### [235] [Spanning-tree-packing protocol for conference key propagation in quantum networks](https://arxiv.org/abs/2506.04105)
*Anton Trushechkin,Hermann Kampermann,Dagmar Bruß*

Main category: quant-ph

TL;DR: In this paper, the authors consider a network of users connected by pairwise quantum key distribution (QKD) links. They propose an algorithm based on spanning tree packing to generate a common secret key at the maximal rate and prove its optimality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable optimal conference key generation in modern quantum networks of arbitrary topology using pairwise secret keys and public classical communication.

Method: The method proposed is an algorithm based on spanning tree packing.

Result: The result is the proof of the optimality of the proposed algorithm for conference key generation.

Conclusion: This algorithm enables optimal conference key generation in modern quantum networks of arbitrary topology and can guide the optimal placement of new bipartite links in the network design.

Abstract: We consider a network of users connected by pairwise quantum key distribution
(QKD) links. Using these pairwise secret keys and public classical
communication, the users want to generate a common (conference) secret key at
the maximal rate. We propose an algorithm based on spanning tree packing (a
known problem in graph theory) and prove its optimality. This algorithm enables
optimal conference key generation in modern quantum networks of arbitrary
topology. Additionally, we discuss how it can guide the optimal placement of
new bipartite links in the network design.

</details>


### [236] [Investigating Quantum Feature Maps in Quantum Support Vector Machines for Lung Cancer Classification](https://arxiv.org/abs/2506.03272)
*My Youssef El Hafidi,Achraf Toufah,Mohamed Achraf Kadim*

Main category: quant-ph

TL;DR: This paper explores the application of Quantum Support Vector Machines (QSVM) in lung cancer diagnosis using three quantum feature maps, finding that PauliFeatureMap yields the best performance.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of Quantum Support Vector Machines (QSVM) in healthcare applications, specifically lung cancer diagnosis, by analyzing the influence of different quantum feature maps on classification performance.

Method: Implemented QSVM models with Qiskit and executed them on qasm simulator using three distinct quantum feature maps: ZFeatureMap, ZZFeatureMap, and PauliFeatureMap. Evaluated on six balanced subsets from a real-world dataset with 309 patient records.

Result: PauliFeatureMap outperformed other feature maps, achieving perfect classification in three subsets and strong overall performance.

Conclusion: Quantum computational principles can enhance diagnostic capabilities in healthcare, highlighting the importance of physics-based modeling in AI applications.

Abstract: In recent years, quantum machine learning has emerged as a promising
intersection between quantum physics and artificial intelligence, particularly
in domains requiring advanced pattern recognition such as healthcare. This
study investigates the effectiveness of Quantum Support Vector Machines (QSVM),
which leverage quantum mechanical phenomena like superposition and entanglement
to construct high-dimensional Hilbert spaces for data classification. Focusing
on lung cancer diagnosis, a concrete and critical healthcare application, we
analyze how different quantum feature maps influence classification
performance. Using a real-world dataset of 309 patient records with significant
class imbalance (39 non-cancer vs. 270 cancer cases), we constructed six
balanced subsets for robust evaluation. QSVM models were implemented using
Qiskit and executed on the qasm simulator, employing three distinct quantum
feature maps: ZFeatureMap, ZZFeatureMap, and PauliFeatureMap. Performance was
assessed using accuracy, precision, recall, specificity, and F1-score. Results
show that the PauliFeatureMap consistently outperformed the others, achieving
perfect classification in three subsets and strong performance overall. These
findings demonstrate how quantum computational principles can be harnessed to
enhance diagnostic capabilities, reinforcing the importance of physics-based
modeling in emerging AI applications within healthcare.

</details>


### [237] [RhoDARTS: Differentiable Quantum Architecture Search with Density Matrix Simulations](https://arxiv.org/abs/2506.03697)
*Swagat Kumar,Jan-Nico Zaech,Colin Michael Wilmott,Luc Van Gool*

Main category: quant-ph

TL;DR: In this paper, researchers propose a novel differentiable Quantum Architecture Search (QAS) algorithm named $\rho$DARTS to identify effective Quantum Neural Network (QNN) architectures for machine learning tasks. They model the search process as the evolution of a quantum mixed state and validate their method on various tasks such as state initialization, Hamiltonian optimization, and image classification.


<details>
  <summary>Details</summary>
Motivation: Existing QAS algorithms are adaptations of classical neural architecture search techniques and often overlook the inherent quantum nature of the circuits they produce.

Method: The researchers approach QAS from a quantum perspective and propose $\rho$DARTS, which models the search process as the evolution of a quantum mixed state emerging from the search space of quantum architectures.

Result: The proposed method shows better convergence against existing QAS techniques and improved robustness levels to noise when finding circuits for state initialization, Hamiltonian optimization, and image classification.

Conclusion: The research presents a promising advancement in identifying effective QNN architectures for general machine learning tasks.

Abstract: Variational Quantum Algorithms (VQAs) are a promising approach for leveraging
powerful Noisy Intermediate-Scale Quantum (NISQ) computers. When applied to
machine learning tasks, VQAs give rise to NISQ-compatible Quantum Neural
Networks (QNNs), which have been shown to outperform classical neural networks
with a similar number of trainable parameters. While the quantum circuit
structures of VQAs for physics simulations are determined by the physical
properties of the systems, identifying effective QNN architectures for general
machine learning tasks is a difficult challenge due to the lack of
domain-specific priors. Indeed, existing Quantum Architecture Search (QAS)
algorithms, adaptations of classical neural architecture search techniques,
often overlook the inherent quantum nature of the circuits they produce. By
approaching QAS from the ground-up and from a quantum perspective, we resolve
this limitation by proposing $\rho$DARTS, a differentiable QAS algorithm that
models the search process as the evolution of a quantum mixed state, emerging
from the search space of quantum architectures. We validate our method by
finding circuits for state initialization, Hamiltonian optimization, and image
classification. Further, we demonstrate better convergence against existing QAS
techniques and show improved robustness levels to noise.

</details>


### [238] [Towards Quantum Operator-Valued Kernels](https://arxiv.org/abs/2506.03779)
*Hachem Kadri,Joachim Tomasi,Yuka Hashimoto,Sandrine Anthoine*

Main category: quant-ph

TL;DR: Quantum kernels, despite recent doubts about their superiority over classical kernels when handling classical data, may find enhanced potential in more expressive kernel classes. This paper proposes focusing quantum kernel research on operator-valued kernels to explore new possibilities.


<details>
  <summary>Details</summary>
Motivation: Recent studies suggest that quantum kernels may not offer speed-ups when learning on classical data, but current research mainly focuses on scalar-valued kernels where classical methods are already efficient and effective.

Method: The paper suggests building upon recent advances in operator-valued kernels to propose guidelines for investigating quantum kernels with a focus on more expressive kernel classes.

Result: By shifting the focus to more complex and potentially more powerful kernel types, there could be significant advancements in quantum kernel machines.

Conclusion: This position paper argues that future quantum kernel research should center around more expressive kernel classes to fully explore the potentials of quantum kernel machines.

Abstract: Quantum kernels are reproducing kernel functions built using
quantum-mechanical principles and are studied with the aim of outperforming
their classical counterparts. The enthusiasm for quantum kernel machines has
been tempered by recent studies that have suggested that quantum kernels could
not offer speed-ups when learning on classical data. However, most of the
research in this area has been devoted to scalar-valued kernels in standard
classification or regression settings for which classical kernel methods are
efficient and effective, leaving very little room for improvement with quantum
kernels. This position paper argues that quantum kernel research should focus
on more expressive kernel classes. We build upon recent advances in
operator-valued kernels, and propose guidelines for investigating quantum
kernels. This should help to design a new generation of quantum kernel machines
and fully explore their potentials.

</details>


### [239] [Estimation of the reduced density matrix and entanglement entropies using autoregressive networks](https://arxiv.org/abs/2506.04170)
*Piotr Białas,Piotr Korcyl,Tomasz Stebel,Dawid Zapolski*

Main category: quant-ph

TL;DR: An application of autoregressive neural networks to Monte Carlo simulations of quantum spin chains is presented, with the ability to estimate all needed matrix elements with a single training.


<details>
  <summary>Details</summary>
Motivation: To explore the use of autoregressive neural networks in Monte Carlo simulations for quantum spin chains and classical two-dimensional spin systems.

Method: Using a hierarchy of neural networks capable of estimating conditional probabilities of consecutive spins to evaluate elements of reduced density matrices directly.

Result: The architecture was able to estimate all the needed matrix elements with just a single training for a fixed time discretization and lattice volume. Calculated the continuum limit of the ground state's von Neumann and Rényi bipartite entanglement entropies of an interval built of up to 5 spins.

Conclusion: This method can be applied to other types of spin chains, possibly with defects, as well as to estimating entanglement entropies of thermal states at non-zero temperature.

Abstract: We present an application of autoregressive neural networks to Monte Carlo
simulations of quantum spin chains using the correspondence with classical
two-dimensional spin systems. We use a hierarchy of neural networks capable of
estimating conditional probabilities of consecutive spins to evaluate elements
of reduced density matrices directly. Using the Ising chain as an example, we
calculate the continuum limit of the ground state's von Neumann and R\'enyi
bipartite entanglement entropies of an interval built of up to 5 spins. We
demonstrate that our architecture is able to estimate all the needed matrix
elements with just a single training for a fixed time discretization and
lattice volume. Our method can be applied to other types of spin chains,
possibly with defects, as well as to estimating entanglement entropies of
thermal states at non-zero temperature.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [240] [A Kernel-Based Approach for Accurate Steady-State Detection in Performance Time Series](https://arxiv.org/abs/2506.04204)
*Martin Beseda,Vittorio Cortellessa,Daniele Di Pompeo,Luca Traini,Michele Tucci*

Main category: cs.PF

TL;DR: This paper proposes a new method for detecting the transition from warmup to steady state in performance metrics time series, reducing total error by 14.5% compared to current methods.


<details>
  <summary>Details</summary>
Motivation: Accurately detecting the transition from warmup phase to steady state in performance metric time series is crucial for effective benchmarking.

Method: The approach uses kernel-based step detection and statistical methods, adapted from chemical reactors domain, with a window-based technique for online steady state detection.

Result: The new approach reduces total error by 14.5% compared to the state-of-the-art method and offers more reliable steady-state onset detection.

Conclusion: This robust and adaptable method enhances the accuracy and stability of performance benchmarking, making it valuable for real-world performance evaluation.

Abstract: This paper addresses the challenge of accurately detecting the transition
from the warmup phase to the steady state in performance metric time series,
which is a critical step for effective benchmarking. The goal is to introduce a
method that avoids premature or delayed detection, which can lead to inaccurate
or inefficient performance analysis. The proposed approach adapts techniques
from the chemical reactors domain, detecting steady states online through the
combination of kernel-based step detection and statistical methods. By using a
window-based approach, it provides detailed information and improves the
accuracy of identifying phase transitions, even in noisy or irregular time
series. Results show that the new approach reduces total error by 14.5%
compared to the state-of-the-art method. It offers more reliable detection of
the steady-state onset, delivering greater precision for benchmarking tasks.
For users, the new approach enhances the accuracy and stability of performance
benchmarking, efficiently handling diverse time series data. Its robustness and
adaptability make it a valuable tool for real-world performance evaluation,
ensuring consistent and reproducible results.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [241] [What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness](https://arxiv.org/abs/2506.04194)
*Yang Cai,Alkis Kalavasis,Katerina Mamali,Anay Mehrotra,Manolis Zampetakis*

Main category: math.ST

TL;DR: 本论文研究了在不依赖无混淆性和重叠假设的情况下，识别平均处理效应（ATE）的通用条件。提出了一种可解释的充分且几乎必要条件，并展示了该条件在多种场景下的适用性，例如回归不连续设计模型。这些发现为结合学习理论和因果推断方法提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 许多因果推断研究依赖于无混淆性和重叠假设，但这些假设在实际中常被违反，例如在具有确定性处理决策的观察性研究中。因此，需要探索更广泛的条件下识别ATE的方法。

Method: 基于统计学习理论范式，提出了一种可解释的充分且几乎必要的条件来识别ATE。该条件还可以用于识别处理过的个体的平均处理效应（ATT），并可以扩展到其他处理效应。

Result: 所提出的条件在多个已研究的情景中适用，包括Tan (2006)、Rosenbaum (2002) 和 Thistlethwaite and Campbell (1960) 提出的模型。在温和的假设下，ATE可以从有限样本中估计。

Conclusion: 这些结果为结合学习理论和因果推断方法提供了新的方向，特别是在具有复杂处理机制的观察性研究中。

Abstract: Most of the widely used estimators of the average treatment effect (ATE) in
causal inference rely on the assumptions of unconfoundedness and overlap.
Unconfoundedness requires that the observed covariates account for all
correlations between the outcome and treatment. Overlap requires the existence
of randomness in treatment decisions for all individuals. Nevertheless, many
types of studies frequently violate unconfoundedness or overlap, for instance,
observational studies with deterministic treatment decisions -- popularly known
as Regression Discontinuity designs -- violate overlap.
  In this paper, we initiate the study of general conditions that enable the
identification of the average treatment effect, extending beyond
unconfoundedness and overlap. In particular, following the paradigm of
statistical learning theory, we provide an interpretable condition that is
sufficient and nearly necessary for the identification of ATE. Moreover, this
condition characterizes the identification of the average treatment effect on
the treated (ATT) and can be used to characterize other treatment effects as
well. To illustrate the utility of our condition, we present several
well-studied scenarios where our condition is satisfied and, hence, we prove
that ATE can be identified in regimes that prior works could not capture. For
example, under mild assumptions on the data distributions, this holds for the
models proposed by Tan (2006) and Rosenbaum (2002), and the Regression
Discontinuity design model introduced by Thistlethwaite and Campbell (1960).
For each of these scenarios, we also show that, under natural additional
assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic
insights and causal inference methodologies, particularly in observational
studies with complex treatment mechanisms.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [242] [A Pre-trained Framework for Multilingual Brain Decoding Using Non-invasive Recordings](https://arxiv.org/abs/2506.03214)
*Yi Guo,Yihang Dong,Michael Kwok-Po Ng,Shuqiang Wang*

Main category: q-bio.NC

TL;DR: 研究人员提出了一种联合多语言、多主体和多模态的脑信号解码框架，通过预训练的多语言模型将不同脑记录映射到统一语义空间，实现跨语言、跨主体和跨模态的解码。实验表明该框架具有强大的泛化能力，并能促进语言公平性，特别是在提升少数语言解码性能方面。


<details>
  <summary>Details</summary>
Motivation: 当前的脑-机接口（BCI）语音解码方法受限于单一语言、单一主体和单一神经影像模态设置，限制了其临床应用和普适性。

Method: 提出了一种联合多语言、多主体和多模态的解码框架，利用预训练的多语言模型（PMM）将多样化的脑记录映射到统一的语义空间，从而实现跨语言、跨主体和跨模态的解码。

Result: 通过使用来自四个语言的159名参与者的非侵入性脑记录验证，结果表明该框架在多语言、多主体和多模态设置中表现出强大的泛化能力，同时能够促进语言公平性，提升少数语言的解码性能。

Conclusion: 提出的框架为脑信号解码建立了一种新的潜在范式，为BCI技术的更广泛应用开辟了新途径。

Abstract: Brain-computer interfaces (BCIs) with speech decoding from brain recordings
have broad application potential in fields such as clinical rehabilitation and
cognitive neuroscience. However, current decoding methods remain limited to
single-language, single-subject, and single neuroimaging modality settings,
restricting their clinical applicability and generalizability. Here we propose
a joint multilingual, multi-subject and multimodal decoding framework. It maps
diverse brain recordings into a unified semantic space defined by a pre-trained
multilingual model (PMM), enabling decoding across multiple languages, multiple
subjects and multiple neuroimaging modalities. The proposed framework is
validated using non-invasive brain recordings from 159 participants across four
languages. Experimental results show that it exhibits strong generalization
across multilingual, multi-subject, and multimodal settings. More importantly,
the proposed framework can promote linguistic fairness, which is vital for
underrepresented languages in BCI applications. The unified semantic space
enables cross-lingual mapping enhancement, allowing the framework to boost the
decoding performance of underrepresented languages, thereby promoting
linguistic fairness. Overall, the proposed framework establishes a new
potential paradigm for brain decoding, opening new paths for broader
applications of BCI.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [243] [Predicting Postoperative Stroke in Elderly SICU Patients: An Interpretable Machine Learning Model Using MIMIC Data](https://arxiv.org/abs/2506.03209)
*Tinghuan Li,Shuheng Chen,Junyi Fan,Elham Pishgar,Kamiar Alaei,Greg Placencia,Maryam Pishgar*

Main category: q-bio.QM

TL;DR: 术后中风仍然是老年外科重症监护病房（SICU）患者的关键并发症，会导致住院时间延长、医疗成本增加和死亡率上升。准确的早期风险分层对于及时干预和改善临床结果至关重要。我们构建了一个包含19,085名老年SICU患者的综合队列，并开发了一个可解释的机器学习框架，使用入院前24小时的临床数据预测住院期间的中风。通过多种预处理方法和特征选择过程，最终确定了20个具有临床意义的预测因子。在评估的八种机器学习模型中，CatBoost表现最佳，AUROC为0.8868（95% CI: 0.8802--0.8937）。SHAP分析和消融研究确定了既往脑血管疾病、血清肌酐和收缩压为最重要的风险因素。我们的研究结果突显了可解释的机器学习方法在支持术后中风早期检测和指导围手术期重症护理决策中的潜力。


<details>
  <summary>Details</summary>
Motivation: 术后中风是老年SICU患者的重要并发症，对健康和经济产生重大影响。当前需要一种有效的方法来进行早期风险评估，以便及时干预并改善临床结果。

Method: 使用MIMIC-III和MIMIC-IV数据库构建了一个包含19,085名老年SICU患者的队列。采用可解释的机器学习框架，利用ICU住院前24小时内的临床数据预测住院期间的中风。预处理包括去除高缺失值特征、迭代奇异值分解（SVD）插补、z分数标准化、独热编码和使用自适应合成采样（ADASYN）算法进行类别不平衡校正。通过递归特征消除与交叉验证（RFECV）和SHapley加性解释（SHAP）相结合的两阶段特征选择过程，将初始80个变量减少到20个具有临床信息的预测因子。

Result: 在评估的八种机器学习模型中，CatBoost表现最佳，AUROC为0.8868（95% CI: 0.8802--0.8937）。SHAP分析和消融研究确定了既往脑血管疾病、血清肌酐和收缩压为最重要的风险因素。

Conclusion: 可解释的机器学习方法具有支持术后中风早期检测和指导围手术期重症护理决策的潜力。

Abstract: Postoperative stroke remains a critical complication in elderly surgical
intensive care unit (SICU) patients, contributing to prolonged hospitalization,
elevated healthcare costs, and increased mortality. Accurate early risk
stratification is essential to enable timely intervention and improve clinical
outcomes. We constructed a combined cohort of 19,085 elderly SICU admissions
from the MIMIC-III and MIMIC-IV databases and developed an interpretable
machine learning (ML) framework to predict in-hospital stroke using clinical
data from the first 24 hours of Intensive Care Unit (ICU) stay. The
preprocessing pipeline included removal of high-missingness features, iterative
Singular Value Decomposition (SVD) imputation, z-score normalization, one-hot
encoding, and class imbalance correction via the Adaptive Synthetic Sampling
(ADASYN) algorithm. A two-stage feature selection process-combining Recursive
Feature Elimination with Cross-Validation (RFECV) and SHapley Additive
exPlanations (SHAP)-reduced the initial 80 variables to 20 clinically
informative predictors. Among eight ML models evaluated, CatBoost achieved the
best performance with an AUROC of 0.8868 (95% CI: 0.8802--0.8937). SHAP
analysis and ablation studies identified prior cerebrovascular disease, serum
creatinine, and systolic blood pressure as the most influential risk factors.
Our results highlight the potential of interpretable ML approaches to support
early detection of postoperative stroke and inform decision-making in
perioperative critical care.

</details>


### [244] [UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection](https://arxiv.org/abs/2506.03237)
*Jigang Fan,Quanlin Wu,Shengjie Luo,Liwei Wang*

Main category: q-bio.QM

TL;DR: The paper introduces UniSite-DS, a new dataset for ligand binding site detection, and UniSite, an end-to-end framework that outperforms current methods. It also proposes a new evaluation metric.


<details>
  <summary>Details</summary>
Motivation: Existing methods, datasets, and metrics for ligand binding site detection have key challenges: statistical bias, discontinuous workflow, and inadequate performance reflection.

Method: Introduced UniSite-DS dataset with more multi-site data and UniSite framework with set prediction loss and bijective matching. Proposed IoU-based Average Precision as an evaluation metric.

Result: Extensive experiments show that IoU-based Average Precision better reflects prediction quality and UniSite surpasses state-of-the-art methods in ligand binding site detection.

Conclusion: UniSite is a superior method for ligand binding site detection and the new evaluation metric provides a more accurate reflection of prediction quality.

Abstract: The detection of ligand binding sites for proteins is a fundamental step in
Structure-Based Drug Design. Despite notable advances in recent years, existing
methods, datasets, and evaluation metrics are confronted with several key
challenges: (1) current datasets and methods are centered on individual
protein-ligand complexes and neglect that diverse binding sites may exist
across multiple complexes of the same protein, introducing significant
statistical bias; (2) ligand binding site detection is typically modeled as a
discontinuous workflow, employing binary segmentation and subsequent clustering
algorithms; (3) traditional evaluation metrics do not adequately reflect the
actual performance of different binding site prediction methods. To address
these issues, we first introduce UniSite-DS, the first UniProt (Unique
Protein)-centric ligand binding site dataset, which contains 4.81 times more
multi-site data and 2.08 times more overall data compared to the previously
most widely used datasets. We then propose UniSite, the first end-to-end ligand
binding site detection framework supervised by set prediction loss with
bijective matching. In addition, we introduce Average Precision based on
Intersection over Union (IoU) as a more accurate evaluation metric for ligand
binding site prediction. Extensive experiments on UniSite-DS and several
representative benchmark datasets demonstrate that IoU-based Average Precision
provides a more accurate reflection of prediction quality, and that UniSite
outperforms current state-of-the-art methods in ligand binding site detection.
The dataset and codes will be made publicly available at
https://github.com/quanlin-wu/unisite.

</details>


### [245] [Quantum Cognition Machine Learning for Forecasting Chromosomal Instability](https://arxiv.org/abs/2506.03199)
*Giuseppe Di Caro,Vahagn Kirakosyan,Alexander G. Abanov,Luca Candelori,Nadine Hartmann,Ernest T. Lam,Kharen Musaelian,Ryan Samson,Dario Villani,Martin T. Wells,Richard J. Wenstrup,Mengjia Xu*

Main category: q-bio.QM

TL;DR: Quantum Cognition Machine Learning (QCML) is introduced as a new tool for predicting chromosomal instability in circulating tumor cells (CTCs) from metastatic breast cancer patients. It uses quantum mechanical principles to improve feature modeling, reduce dimensionality, and enhance generalization. QCML outperforms conventional machine learning methods in identifying predicted large-scale state transitions (pLST) status from CTC-derived morphology features.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of chromosomal instability in CTCs can help detect those with high metastatic potential in liquid biopsy diagnostics. However, this is challenging due to the complexity and high dimensionality of single-cell digital pathology data.

Method: The study applies Quantum Cognition Machine Learning (QCML), which uses quantum mechanical principles to represent data in a Hilbert space. This allows context-aware feature modeling, dimensionality reduction, and enhanced generalization without requiring curated feature selection.

Result: QCML performs better than conventional machine learning methods when tested on out-of-sample verification CTCs, achieving higher accuracy in identifying pLST status from CTC morphology features.

Conclusion: QCML is a promising novel machine learning tool for high-dimensional, low-sample-size biomedical contexts like CTC classification in liquid biopsy.

Abstract: The accurate prediction of chromosomal instability from the morphology of
circulating tumor cells (CTCs) enables real-time detection of CTCs with high
metastatic potential in the context of liquid biopsy diagnostics. However, it
presents a significant challenge due to the high dimensionality and complexity
of single-cell digital pathology data. Here, we introduce the application of
Quantum Cognition Machine Learning (QCML), a quantum-inspired computational
framework, to estimate morphology-predicted chromosomal instability in CTCs
from patients with metastatic breast cancer. QCML leverages quantum mechanical
principles to represent data as state vectors in a Hilbert space, enabling
context-aware feature modeling, dimensionality reduction, and enhanced
generalization without requiring curated feature selection. QCML outperforms
conventional machine learning methods when tested on out of sample verification
CTCs, achieving higher accuracy in identifying predicted large-scale state
transitions (pLST) status from CTC-derived morphology features. These
preliminary findings support the application of QCML as a novel machine
learning tool with superior performance in high-dimensional, low-sample-size
biomedical contexts. QCML enables the simulation of cognition-like learning for
the identification of biologically meaningful prediction of chromosomal
instability from CTC morphology, offering a novel tool for CTC classification
in liquid biopsy.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [246] [Deep Learning-Based Breast Cancer Detection in Mammography: A Multi-Center Validation Study in Thai Population](https://arxiv.org/abs/2506.03177)
*Isarun Chamveha,Supphanut Chaiyungyuen,Sasinun Worakriangkrai,Nattawadee Prasawang,Warasinee Chaisangmongkon,Pornpim Korpraphong,Voraparee Suvannarerg,Shanigarn Thiravit,Chalermdej Kannawat,Kewalin Rungsinaporn,Suwara Issaragrisil,Payia Chadbunchachai,Pattiya Gatechumpol,Chawiporn Muktabhant,Patarachai Sereerat*

Main category: eess.IV

TL;DR: This paper presents a deep learning system based on modified EfficientNetV2 for breast cancer detection in mammography, showing strong performance and clinical acceptance.


<details>
  <summary>Details</summary>
Motivation: To develop an effective deep learning model for assisting in breast cancer detection from mammograms, addressing the need for improved accuracy and efficiency in screening workflows.

Method: A modified EfficientNetV2 architecture with enhanced attention mechanisms was used. The model was trained on mammograms from a Thai medical center and validated on three datasets. Performance was measured using AUROC, lesion localization metrics, and clinical concordance tests.

Result: The model achieved AUROCs of 0.89, 0.96, and 0.94 across the three datasets. High concordance rates with radiologists were observed, along with strong expert acceptance and satisfactory System Usability Scale scores.

Conclusion: The developed system is effective for assisting in mammogram interpretation, with potential to enhance breast cancer screening workflows in clinical practice.

Abstract: This study presents a deep learning system for breast cancer detection in
mammography, developed using a modified EfficientNetV2 architecture with
enhanced attention mechanisms. The model was trained on mammograms from a major
Thai medical center and validated on three distinct datasets: an in-domain test
set (9,421 cases), a biopsy-confirmed set (883 cases), and an out-of-domain
generalizability set (761 cases) collected from two different hospitals. For
cancer detection, the model achieved AUROCs of 0.89, 0.96, and 0.94 on the
respective datasets. The system's lesion localization capability, evaluated
using metrics including Lesion Localization Fraction (LLF) and Non-Lesion
Localization Fraction (NLF), demonstrated robust performance in identifying
suspicious regions. Clinical validation through concordance tests showed strong
agreement with radiologists: 83.5% classification and 84.0% localization
concordance for biopsy-confirmed cases, and 78.1% classification and 79.6%
localization concordance for out-of-domain cases. Expert radiologists'
acceptance rate also averaged 96.7% for biopsy-confirmed cases, and 89.3% for
out-of-domain cases. The system achieved a System Usability Scale score of
74.17 for source hospital, and 69.20 for validation hospitals, indicating good
clinical acceptance. These results demonstrate the model's effectiveness in
assisting mammogram interpretation, with the potential to enhance breast cancer
screening workflows in clinical practice.

</details>


### [247] [LLaMA-XR: A Novel Framework for Radiology Report Generation using LLaMA and QLoRA Fine Tuning](https://arxiv.org/abs/2506.03178)
*Md. Zihad Bin Jahangir,Muhammad Ashad Kabir,Sumaiya Akter,Israt Jahan,Minh Chau*

Main category: eess.IV

TL;DR: Automated radiology report generation using LLaMA-XR reduces radiologists' workload and improves diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing models for generating reports from chest radiographs struggle with accuracy and contextual relevance.

Method: LLaMA-XR integrates LLaMA 3.1 with DenseNet-121-based image embeddings and QLoRA fine-tuning, optimized for parameter utilization and memory efficiency.

Result: Achieves a ROUGE-L score of 0.433 and a METEOR score of 0.336 on the IU X-ray benchmark dataset, outperforming state-of-the-art methods.

Conclusion: LLaMA-XR is an effective and efficient AI system for automated radiology reporting, enhancing clinical utility and reliability.

Abstract: Automated radiology report generation holds significant potential to reduce
radiologists' workload and enhance diagnostic accuracy. However, generating
precise and clinically meaningful reports from chest radiographs remains
challenging due to the complexity of medical language and the need for
contextual understanding. Existing models often struggle with maintaining both
accuracy and contextual relevance. In this paper, we present LLaMA-XR, a novel
framework that integrates LLaMA 3.1 with DenseNet-121-based image embeddings
and Quantized Low-Rank Adaptation (QLoRA) fine-tuning. LLaMA-XR achieves
improved coherence and clinical accuracy while maintaining computational
efficiency. This efficiency is driven by an optimization strategy that enhances
parameter utilization and reduces memory overhead, enabling faster report
generation with lower computational resource demands. Extensive experiments
conducted on the IU X-ray benchmark dataset demonstrate that LLaMA-XR
outperforms a range of state-of-the-art methods. Our model achieves a ROUGE-L
score of 0.433 and a METEOR score of 0.336, establishing new performance
benchmarks in the domain. These results underscore LLaMA-XR's potential as an
effective and efficient AI system for automated radiology reporting, offering
enhanced clinical utility and reliability.

</details>


### [248] [Edge Computing for Physics-Driven AI in Computational MRI: A Feasibility Study](https://arxiv.org/abs/2506.03183)
*Yaşar Utku Alçalar,Yu Cao,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: This paper presents a novel PD-AI computational MRI approach optimized for FPGA-based edge computing devices, which improves computational efficiency while maintaining reconstruction quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of transmission, storage, and real-time processing of high-resolution MRI data by optimizing PD-AI models for hardware efficiency through quantization and bypassing traditional FFT-based approaches.

Method: The method involves leveraging 8-bit complex data quantization and eliminating redundant FFT/IFFT operations in a PD-AI computational MRI approach optimized for FPGA-based edge computing devices.

Result: The results show that this strategy improves computational efficiency while maintaining reconstruction quality comparable to conventional PD-AI methods, and outperforms standard clinical methods.

Conclusion: The proposed approach presents an opportunity for high-resolution MRI reconstruction on resource-constrained devices, highlighting its potential for real-world deployment.

Abstract: Physics-driven artificial intelligence (PD-AI) reconstruction methods have
emerged as the state-of-the-art for accelerating MRI scans, enabling higher
spatial and temporal resolutions. However, the high resolution of these scans
generates massive data volumes, leading to challenges in transmission, storage,
and real-time processing. This is particularly pronounced in functional MRI,
where hundreds of volumetric acquisitions further exacerbate these demands.
Edge computing with FPGAs presents a promising solution for enabling PD-AI
reconstruction near the MRI sensors, reducing data transfer and storage
bottlenecks. However, this requires optimization of PD-AI models for hardware
efficiency through quantization and bypassing traditional FFT-based approaches,
which can be a limitation due to their computational demands. In this work, we
propose a novel PD-AI computational MRI approach optimized for FPGA-based edge
computing devices, leveraging 8-bit complex data quantization and eliminating
redundant FFT/IFFT operations. Our results show that this strategy improves
computational efficiency while maintaining reconstruction quality comparable to
conventional PD-AI methods, and outperforms standard clinical methods. Our
approach presents an opportunity for high-resolution MRI reconstruction on
resource-constrained devices, highlighting its potential for real-world
deployment.

</details>


### [249] [DLiPath: A Benchmark for the Comprehensive Assessment of Donor Liver Based on Histopathological Image Dataset](https://arxiv.org/abs/2506.03185)
*Liangrui Pan,Xingchen Li,Zhongyi Chen,Ling Chu,Shaoliang Peng*

Main category: eess.IV

TL;DR: Pathologists face challenges in rapidly and accurately evaluating donor liver biopsies intraoperatively due to inter- and intra-observer variability. To address this, the study introduces DLiPath, a benchmark for comprehensive donor liver assessment using a histopathology image dataset with expert annotations. Experiments on nine state-of-the-art MIL models demonstrate high accuracy across donor liver assessment indicators.


<details>
  <summary>Details</summary>
Motivation: Pathologists need crucial information from donor liver biopsies to decide whether to accept or discard potential grafts, but obtaining these assessments quickly and accurately during surgery is difficult. There is substantial variability among and within observers when quantifying important features in donor liver biopsies.

Method: The researchers created DLiPath, a benchmark based on a histopathology image dataset containing 636 whole slide images from 304 donor liver patients. These images have expert annotations for key pathological features. They selected nine state-of-the-art multiple-instance learning (MIL) models as baselines for comparison using the DLiPath dataset.

Result: Several MIL models achieved high accuracy across donor liver assessment indicators on the DLiPath dataset, indicating potential for future automated and intelligent donor liver assessment research.

Conclusion: DLiPath provides a valuable resource for developing and testing automated systems for donor liver assessment, paving the way for more accurate and consistent evaluations in the future.

Abstract: Pathologists comprehensive evaluation of donor liver biopsies provides
crucial information for accepting or discarding potential grafts. However,
rapidly and accurately obtaining these assessments intraoperatively poses a
significant challenge for pathologists. Features in donor liver biopsies, such
as portal tract fibrosis, total steatosis, macrovesicular steatosis, and
hepatocellular ballooning are correlated with transplant outcomes, yet
quantifying these indicators suffers from substantial inter- and intra-observer
variability. To address this, we introduce DLiPath, the first benchmark for
comprehensive donor liver assessment based on a histopathology image dataset.
We collected and publicly released 636 whole slide images from 304 donor liver
patients at the Department of Pathology, the Third Xiangya Hospital, with
expert annotations for key pathological features (including cholestasis, portal
tract fibrosis, portal inflammation, total steatosis, macrovesicular steatosis,
and hepatocellular ballooning). We selected nine state-of-the-art
multiple-instance learning (MIL) models based on the DLiPath dataset as
baselines for extensive comparative analysis. The experimental results
demonstrate that several MIL models achieve high accuracy across donor liver
assessment indicators on DLiPath, charting a clear course for future automated
and intelligent donor liver assessment research. Data and code are available at
https://github.com/panliangrui/ACM_MM_2025.

</details>


### [250] [Lightweight Convolutional Neural Networks for Retinal Disease Classification](https://arxiv.org/abs/2506.03186)
*Duaa Kareem Qasim,Sabah Abdulazeez Jebur,Lafta Raheem Ali,Abdul Jalil M. Khalaf,Abir Jaafar Hussain*

Main category: eess.IV

TL;DR: This paper explores the use of MobileNet and NASNetMobile for classifying retinal images into Normal, DR, and MH categories using the RFMiD dataset. MobileNetV2 achieved the highest accuracy of 90.8%, demonstrating the potential of CNNs in retinal disease classification.


<details>
  <summary>Details</summary>
Motivation: Retinal diseases like Diabetic Retinopathy (DR) and Macular Hole (MH) significantly impact vision globally. Early detection is crucial for preventing severe visual impairments. There is a need for efficient and accurate methods to classify retinal images for early diagnosis.

Method: The study employed two lightweight Convolution Neural Network architectures - MobileNet and NASNetMobile - for classifying retinal images. The models were trained on the RFMiD dataset with preprocessing steps including resizing, normalization, and augmentation. Transfer learning and data augmentation techniques were used to address data scarcity.

Result: MobileNetV2 achieved the highest accuracy of 90.8% while NASNetMobile achieved an accuracy of 89.5%. These results indicate the effectiveness of CNNs in classifying retinal diseases.

Conclusion: The findings highlight the potential of Convolution Neural Networks, specifically MobileNetV2, in accurately classifying retinal diseases. This lays a foundation for AI-assisted ophthalmic diagnosis and early intervention.

Abstract: Retinal diseases such as Diabetic Retinopathy (DR) and Macular Hole (MH)
significantly impact vision and affect millions worldwide. Early detection is
crucial, as DR, a complication of diabetes, damages retinal blood vessels,
potentially leading to blindness, while MH disrupts central vision, affecting
tasks like reading and facial recognition. This paper employed two lightweight
and efficient Convolution Neural Network architectures, MobileNet and
NASNetMobile, for the classification of Normal, DR, and MH retinal images. The
models were trained on the RFMiD dataset, consisting of 3,200 fundus images,
after undergoing preprocessing steps such as resizing, normalization, and
augmentation. To address data scarcity, this study leveraged transfer learning
and data augmentation techniques, enhancing model generalization and
performance. The experimental results demonstrate that MobileNetV2 achieved the
highest accuracy of 90.8%, outperforming NASNetMobile, which achieved 89.5%
accuracy. These findings highlight the effectiveness of CNNs in retinal disease
classification, providing a foundation for AI-assisted ophthalmic diagnosis and
early intervention.

</details>


### [251] [Multi-Analyte, Swab-based Automated Wound Monitor with AI](https://arxiv.org/abs/2506.03188)
*Madhu Babu Sikha,Lalith Appari,Gurudatt Nanjanagudu Ganesh,Amay Bandodkar,Imon Banerjee*

Main category: eess.IV

TL;DR: The paper introduces a low-cost, multi-analyte 3D printed assay integrated on swabs and a Wound Sensor iOS App to identify non-healing diabetic foot ulcers (DFUs) early by comparing density changes in images before and after wound exposure.


<details>
  <summary>Details</summary>
Motivation: Diabetic foot ulcers (DFUs) significantly impact many individuals annually, leading to high treatment costs and risks of amputation. Early identification of non-healing DFUs developing into chronic wounds is crucial for reducing these costs and risks, necessitating advanced diagnostic tools.

Method: The researchers developed a 3D printed assay system that can be integrated onto swabs to detect non-healing DFUs. They also created the Wound Sensor iOS App to acquire and analyze wound sensor data through automated computer vision techniques comparing image densities.

Result: The integrated sensor and app system enables accurate data collection despite varying conditions, allowing healthcare professionals to monitor wound healing progress and assess critical parameters in real-time.

Conclusion: This innovative approach offers a promising solution for early detection of non-healing DFUs, potentially minimizing treatment costs and amputation risks.

Abstract: Diabetic foot ulcers (DFUs), a class of chronic wounds, affect ~750,000
individuals every year in the US alone and identifying non-healing DFUs that
develop to chronic wounds early can drastically reduce treatment costs and
minimize risks of amputation. There is therefore a pressing need for diagnostic
tools that can detect non-healing DFUs early. We develop a low cost,
multi-analyte 3D printed assays seamlessly integrated on swabs that can
identify non-healing DFUs and a Wound Sensor iOS App - an innovative mobile
application developed for the controlled acquisition and automated analysis of
wound sensor data. By comparing both the original base image (before exposure
to the wound) and the wound-exposed image, we developed automated computer
vision techniques to compare density changes between the two assay images,
which allow us to automatically determine the severity of the wound. The iOS
app ensures accurate data collection and presents actionable insights, despite
challenges such as variations in camera configurations and ambient conditions.
The proposed integrated sensor and iOS app will allow healthcare professionals
to monitor wound conditions real-time, track healing progress, and assess
critical parameters related to wound care.

</details>


### [252] [Encoding of Demographic and Anatomical Information in Chest X-Ray-based Severe Left Ventricular Hypertrophy Classifiers](https://arxiv.org/abs/2506.03192)
*Basudha Pal,Rama Chellappa,Muhammad Umair*

Main category: eess.IV

TL;DR: The paper presents a classification framework to predict severe left ventricular hypertrophy from chest X-rays with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Echocardiography and MRI are the clinical standards for evaluating cardiac structure but they are costly and not always accessible, so there is a need for an alternative method that can provide similar information at lower cost and higher accessibility.

Method: The method introduced is a direct classification framework that uses chest X-rays to predict severe left ventricular hypertrophy. It does not rely on anatomical measurements or demographic inputs. Mutual Information Neural Estimation is used to quantify feature expressivity.

Result: The approach achieves high AUROC and AUPRC results, indicating strong performance in predicting severe left ventricular hypertrophy from chest X-rays.

Conclusion: This new framework provides a promising alternative to echocardiography and MRI for predicting severe left ventricular hypertrophy, offering potential cost savings and increased accessibility while maintaining clinical meaningfulness and supporting transparent model interpretation.

Abstract: While echocardiography and MRI are clinical standards for evaluating cardiac
structure, their use is limited by cost and accessibility.We introduce a direct
classification framework that predicts severe left ventricular hypertrophy from
chest X-rays, without relying on anatomical measurements or demographic inputs.
Our approach achieves high AUROC and AUPRC, and employs Mutual Information
Neural Estimation to quantify feature expressivity. This reveals clinically
meaningful attribute encoding and supports transparent model interpretation.

</details>


### [253] [Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach](https://arxiv.org/abs/2506.03238)
*Ziheng Zhao,Lisong Dai,Ya Zhang,Yanfeng Wang,Weidi Xie*

Main category: eess.IV

TL;DR: 本文提出了一种全面的分类系统和一个包含大量CT图像的数据集，以及OminiAbnorm-CT模型，该模型可以在多平面和全身CT图像上自动定位和描述异常发现，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 自动化解释CT图像，特别是定位和描述跨多平面和全身扫描的异常发现，是临床放射学中的一个重要挑战。

Method: 1. 提出了包含404个代表性异常发现的全面分层分类系统；2. 构建了一个包含超过14.5K张CT图像的数据集，并为超过19K个异常提供了详细注释；3. 提出了OminiAbnorm-CT模型，可以根据文本查询自动定位和描述异常发现，同时允许通过视觉提示进行灵活交互；4. 建立了三个基于真实临床场景的评估任务。

Result: OminiAbnorm-CT在所有任务和指标上显著优于现有方法。

Conclusion: OminiAbnorm-CT模型在多平面和全身CT图像的异常定位和描述方面表现出色，为临床放射学提供了一种有效的工具。

Abstract: Automated interpretation of CT images-particularly localizing and describing
abnormal findings across multi-plane and whole-body scans-remains a significant
challenge in clinical radiology. This work aims to address this challenge
through four key contributions: (i) On taxonomy, we collaborate with senior
radiologists to propose a comprehensive hierarchical classification system,
with 404 representative abnormal findings across all body regions; (ii) On
data, we contribute a dataset containing over 14.5K CT images from multiple
planes and all human body regions, and meticulously provide grounding
annotations for over 19K abnormalities, each linked to the detailed description
and cast into the taxonomy; (iii) On model development, we propose
OminiAbnorm-CT, which can automatically ground and describe abnormal findings
on multi-plane and whole-body CT images based on text queries, while also
allowing flexible interaction through visual prompts; (iv) On benchmarks, we
establish three representative evaluation tasks based on real clinical
scenarios. Through extensive experiments, we show that OminiAbnorm-CT can
significantly outperform existing methods on all the tasks and metrics.

</details>


### [254] [Adaptive and Robust Image Processing on CubeSats](https://arxiv.org/abs/2506.03152)
*Robert Bayer,Julian Priest,Daniel Kjellberg,Jeppe Lindhard,Nikolaj Sørenesen,Nicolaj Valsted,Ívar Óli,Pınar Tözün*

Main category: eess.IV

TL;DR: CubeSats are great for space research but have limitations. This paper presents DIPP and DISH to improve image processing and scheduling in CubeSats.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of resource-constrained nature and being in space for CubeSats, which challenge the flexibility and complexity of the deployed image processing pipelines and their orchestration.

Method: Introduction of two novel systems - DIPP and DISH. DIPP is a modular and configurable image processing pipeline framework. DISH is a domain-specific language (DSL) and runtime system designed to schedule complex imaging workloads on low-power and memory-constrained processors.

Result: Experiments show that DIPP's decomposition adds negligible overhead and significantly reduces network requirements for updating pipelines. DISH has comparable expressiveness with Lua but requires lower memory.

Conclusion: DIPP and DISH can enhance the adaptability, robustness, and efficiency of image processing and scheduling in CubeSats.

Abstract: CubeSats offer a low-cost platform for space research, particularly for Earth
observation. However, their resource-constrained nature and being in space,
challenge the flexibility and complexity of the deployed image processing
pipelines and their orchestration. This paper introduces two novel systems,
DIPP and DISH, to address these challenges. DIPP is a modular and configurable
image processing pipeline framework that allows for adaptability to changing
mission goals even after deployment, while preserving robustness. DISH is a
domain-specific language (DSL) and runtime system designed to schedule complex
imaging workloads on low-power and memory-constrained processors.
  Our experiments demonstrate that DIPP's decomposition of the processing
pipelines adds negligible overhead, while significantly reducing the network
requirements of updating pipelines and being robust against erroneous module
uploads. Furthermore, we compare DISH to Lua, a general purpose scripting
language, and demonstrate its comparable expressiveness and lower memory
requirement.

</details>


### [255] [A combined Machine Learning and Finite Element Modelling tool for the surgical planning of craniosynostosis correction](https://arxiv.org/abs/2506.03202)
*Itxasne Antúnez Sáenz,Ane Alberdi Aramendi,David Dunaway,Juling Ong,Lara Deliège,Amparo Sáenz,Anita Ahmadi Birjandi,Noor UI Owase Jeelani,Silvia Schievano,Alessandro Borghi*

Main category: eess.IV

TL;DR: 为了改善颅缝早闭症手术结果的预测，研究提出了一种基于3D照片生成合成头骨并结合机器学习模型的新方法，无需CT扫描即可实时预测手术效果。


<details>
  <summary>Details</summary>
Motivation: 当前用于预测矢状颅缝早闭症（SC）手术结果的工具依赖有限元建模（FEM），需要CT成像、工程专业知识且计算耗时长。此外，手术结果依赖外科医生的经验和婴儿年龄，缺乏高效的术前规划工具。因此，研究旨在开发一种无需CT扫描的实时预测工具，以减少辐射暴露并优化手术规划。

Method: 1. 使用三维（3D）照片创建个性化的合成头骨，结合颅缝位置、头骨厚度和软组织属性的人群平均值。
2. 开发机器学习（ML）代理模型，采用多输出支持向量回归器来预测手术结果。
3. 模型训练数据来源于模拟手术场景，并通过优化算法调整参数以实现最大颅骨指数（CI）。

Result: 所提出的多输出支持向量回归模型在预测手术结果方面表现出色，R2指标达到0.95，均方误差（MSE）和平均绝对误差（MAE）均低于0.13。此外，该模型未来可扩展用于模拟不同手术情景并提供最优参数建议。

Conclusion: 本研究成功开发了一种基于3D照片和机器学习的实时预测工具，可以有效预测颅缝早闭症手术结果，无需CT扫描，从而减少了辐射暴露并提高了术前规划效率。未来，该模型有望进一步优化手术方案，为临床应用提供更多支持。

Abstract: Craniosynostosis is a medical condition that affects the growth of babies'
heads, caused by an early fusion of cranial sutures. In recent decades,
surgical treatments for craniosynostosis have significantly improved, leading
to reduced invasiveness, faster recovery, and less blood loss. At Great Ormond
Street Hospital (GOSH), the main surgical treatment for patients diagnosed with
sagittal craniosynostosis (SC) is spring assisted cranioplasty (SAC). This
procedure involves a 15x15 mm2 osteotomy, where two springs are inserted to
induce distraction. Despite the numerous advantages of this surgical technique
for patients, the outcome remains unpredictable due to the lack of efficient
preoperative planning tools. The surgeon's experience and the baby's age are
currently relied upon to determine the osteotomy location and spring selection.
Previous tools for predicting the surgical outcome of SC relied on finite
element modeling (FEM), which involved computed tomography (CT) imaging and
required engineering expertise and lengthy calculations. The main goal of this
research is to develop a real-time prediction tool for the surgical outcome of
patients, eliminating the need for CT scans to minimise radiation exposure
during preoperative planning. The proposed methodology involves creating
personalised synthetic skulls based on three-dimensional (3D) photographs,
incorporating population average values of suture location, skull thickness,
and soft tissue properties. A machine learning (ML) surrogate model is employed
to achieve the desired surgical outcome. The resulting multi-output support
vector regressor model achieves a R2 metric of 0.95 and MSE and MAE below 0.13.
Furthermore, in the future, this model could not only simulate various surgical
scenarios but also provide optimal parameters for achieving a maximum cranial
index (CI).

</details>


### [256] [A Survey of Deep Learning Video Super-Resolution](https://arxiv.org/abs/2506.03216)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter Eklund,Sunil Aryal*

Main category: eess.IV

TL;DR: This paper provides a comprehensive overview and taxonomy of deep learning-based video super-resolution (VSR) models, analyzing components, trends, challenges, and guiding future research.


<details>
  <summary>Details</summary>
Motivation: The rapid progress in deep learning technologies for VSR has led to many tools and techniques, but their usage is often inadequately explained and driven by quantitative improvements. A comprehensive analysis of the elements and methodologies used in VSR research is needed to guide informed model development for specific applications.

Method: The paper investigates each component of deep learning-based VSR models, discussing implications and providing a synopsis of key components and technologies from both state-of-the-art and earlier models. It elucidates underlying methodologies and categorizes them systematically.

Result: The authors identified trends, requirements, and challenges in VSR domain and established a multi-level taxonomy for guiding current and future VSR research.

Conclusion: This work enhances the maturation and interpretation of VSR practices for practical applications, serving as a first-of-its-kind survey for deep learning-based VSR models.

Abstract: Video super-resolution (VSR) is a prominent research topic in low-level
computer vision, where deep learning technologies have played a significant
role. The rapid progress in deep learning and its applications in VSR has led
to a proliferation of tools and techniques in the literature. However, the
usage of these methods is often not adequately explained, and decisions are
primarily driven by quantitative improvements. Given the significance of VSR's
potential influence across multiple domains, it is imperative to conduct a
comprehensive analysis of the elements and deep learning methodologies employed
in VSR research. This methodical analysis will facilitate the informed
development of models tailored to specific application needs. In this paper, we
present an overarching overview of deep learning-based video super-resolution
models, investigating each component and discussing its implications.
Furthermore, we provide a synopsis of key components and technologies employed
by state-of-the-art and earlier VSR models. By elucidating the underlying
methodologies and categorising them systematically, we identified trends,
requirements, and challenges in the domain. As a first-of-its-kind survey of
deep learning-based VSR models, this work also establishes a multi-level
taxonomy to guide current and future VSR research, enhancing the maturation and
interpretation of VSR practices for various practical applications.

</details>


### [257] [Hybrid Ensemble of Segmentation-Assisted Classification and GBDT for Skin Cancer Detection with Engineered Metadata and Synthetic Lesions from ISIC 2024 Non-Dermoscopic 3D-TBP Images](https://arxiv.org/abs/2506.03420)
*Muhammad Zubair Hasan,Fahmida Yasmin Rifat*

Main category: eess.IV

TL;DR: The paper proposes a hybrid machine and deep learning-based approach for classifying skin lesions using the SLICE-3D dataset, combining vision transformers and convolutional ViT hybrid models with gradient-boosted decision trees. The method achieves high pAUC in evaluation.


<details>
  <summary>Details</summary>
Motivation: Skin cancer is highly prevalent and early detection is critical for patient outcomes. Current methods may not be optimal under non-dermoscopic conditions.

Method: The method combines vision transformers (EVA02) and a convolutional ViT hybrid (EdgeNeXtSAC) to extract features from lesion images. A segmentation-assisted classification pipeline enhances localization. Predictions are fused with a GBDT ensemble enriched by engineered features and patient-specific metrics. Synthetic data augmentation and relabeling strategies are used to address class imbalance.

Result: The proposed approach achieves a partial AUC (pAUC) of 0.1755 above 80% true positive rate (TPR), which is the highest among all configurations tested.

Conclusion: The study highlights the potential of hybrid AI systems for skin cancer triage in telemedicine and resource-constrained settings.

Abstract: Skin cancer is among the most prevalent and life-threatening diseases
worldwide, with early detection being critical to patient outcomes. This work
presents a hybrid machine and deep learning-based approach for classifying
malignant and benign skin lesions using the SLICE-3D dataset from ISIC 2024,
which comprises 401,059 cropped lesion images extracted from 3D Total Body
Photography (TBP), emulating non-dermoscopic, smartphone-like conditions. Our
method combines vision transformers (EVA02) and our designed convolutional ViT
hybrid (EdgeNeXtSAC) to extract robust features, employing a
segmentation-assisted classification pipeline to enhance lesion localization.
Predictions from these models are fused with a gradient-boosted decision tree
(GBDT) ensemble enriched by engineered features and patient-specific relational
metrics. To address class imbalance and improve generalization, we augment
malignant cases with Stable Diffusion-generated synthetic lesions and apply a
diagnosis-informed relabeling strategy to harmonize external datasets into a
3-class format. Using partial AUC (pAUC) above 80 percent true positive rate
(TPR) as the evaluation metric, our approach achieves a pAUC of 0.1755 -- the
highest among all configurations. These results underscore the potential of
hybrid, interpretable AI systems for skin cancer triage in telemedicine and
resource-constrained settings.

</details>


### [258] [Towards generating more interpretable counterfactuals via concept vectors: a preliminary study on chest X-rays](https://arxiv.org/abs/2506.04058)
*Bulat Maksudov,Kathleen Curran,Alessandra Mileo*

Main category: eess.IV

TL;DR: The paper presents a method using reconstruction autoencoder to map clinical concepts into the latent space of generative models for extracting Concept Activation Vectors (CAVs). It provides concept-based explanations for medical imaging, producing counterfactuals by traversing latent space. The approach is promising for large pathologies but struggles with smaller ones due to reconstruction limits.


<details>
  <summary>Details</summary>
Motivation: To ensure medical imaging models align with clinical knowledge and are interpretable, the authors aim to map clinical concepts into the latent space of generative models.

Method: Using a reconstruction autoencoder, clinical concepts are linked to image-level features without explicit label training. CAVs are extracted which are stable across datasets.

Result: Preliminary results on chest X-rays show promise for explaining large pathologies like cardiomegaly, while smaller pathologies remain challenging due to reconstruction limits.

Conclusion: Although not outperforming baselines, this approach offers a way to provide interpretable, concept-based explanations aligned with clinical knowledge.

Abstract: An essential step in deploying medical imaging models is ensuring alignment
with clinical knowledge and interpretability. We focus on mapping clinical
concepts into the latent space of generative models to identify Concept
Activation Vectors (CAVs). Using a simple reconstruction autoencoder, we link
user-defined concepts to image-level features without explicit label training.
The extracted concepts are stable across datasets, enabling visual explanations
that highlight clinically relevant features. By traversing latent space along
concept directions, we produce counterfactuals that exaggerate or reduce
specific clinical features. Preliminary results on chest X-rays show promise
for large pathologies like cardiomegaly, while smaller pathologies remain
challenging due to reconstruction limits. Although not outperforming baselines,
this approach offers a path toward interpretable, concept-based explanations
aligned with clinical knowledge.

</details>


### [259] [A Diffusion-Driven Temporal Super-Resolution and Spatial Consistency Enhancement Framework for 4D MRI imaging](https://arxiv.org/abs/2506.04116)
*Xuanru Zhou,Jiarun Liu,Shoujun Yu,Hao Yang,Cheng Li,Tao Tan,Shanshan Wang*

Main category: eess.IV

TL;DR: TSSC-Net is a novel framework that generates intermediate frames in 4D MRI while preserving spatial consistency and improving temporal fidelity under fast motion.


<details>
  <summary>Details</summary>
Motivation: In medical imaging, traditional approaches for generating intermediate frames in 4D MRI struggle with large deformations leading to misregistration, artifacts, and diminished spatial consistency especially during rapid motions.

Method: The proposed TSSC-Net employs a diffusion-based temporal super-resolution network to generate intermediate frames using start and end frames as key references. It also introduces a tri-directional Mamba-based module to leverage long-range contextual information and resolve spatial inconsistencies arising from cross-slice misalignment.

Result: Extensive experiments on the ACDC cardiac MRI dataset and a real-world dynamic 4D knee joint dataset show that TSSC-Net can generate high-resolution dynamic MRI from fast-motion data while preserving structural fidelity and spatial consistency.

Conclusion: TSSC-Net successfully addresses the challenges of generating intermediate frames in 4D MRI by preserving spatial consistency and improving temporal fidelity under fast motion.

Abstract: In medical imaging, 4D MRI enables dynamic 3D visualization, yet the
trade-off between spatial and temporal resolution requires prolonged scan time
that can compromise temporal fidelity--especially during rapid, large-amplitude
motion. Traditional approaches typically rely on registration-based
interpolation to generate intermediate frames. However, these methods struggle
with large deformations, resulting in misregistration, artifacts, and
diminished spatial consistency. To address these challenges, we propose
TSSC-Net, a novel framework that generates intermediate frames while preserving
spatial consistency. To improve temporal fidelity under fast motion, our
diffusion-based temporal super-resolution network generates intermediate frames
using the start and end frames as key references, achieving 6x temporal
super-resolution in a single inference step. Additionally, we introduce a novel
tri-directional Mamba-based module that leverages long-range contextual
information to effectively resolve spatial inconsistencies arising from
cross-slice misalignment, thereby enhancing volumetric coherence and correcting
cross-slice errors. Extensive experiments were performed on the public ACDC
cardiac MRI dataset and a real-world dynamic 4D knee joint dataset. The results
demonstrate that TSSC-Net can generate high-resolution dynamic MRI from
fast-motion data while preserving structural fidelity and spatial consistency.

</details>


### [260] [A Comprehensive Study on Medical Image Segmentation using Deep Neural Networks](https://arxiv.org/abs/2506.04121)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: This paper provides a comprehensive study on Medical Image Segmentation (MIS) using Deep Neural Networks (DNNs), evaluating Intelligent Vision Systems based on DIKIW levels and emphasizing the role of MIS in disease diagnosis. It also addresses Explainable Artificial Intelligence (XAI) as a crucial research direction, discusses challenges, and proposes solutions to improve DNN-based MIS.


<details>
  <summary>Details</summary>
Motivation: Medical Image Segmentation using DNNs has shown significant performance improvements and holds promise for future developments, particularly in disease diagnosis and early detection.

Method: The study evaluates Intelligent Vision Systems based on their output levels (DIKIW) and explores the state-of-the-art solutions in MIS at these levels. It also delves into Explainable Artificial Intelligence (XAI) to uncover the 'black box' nature of previous DNN architectures.

Result: The research highlights the importance of MIS in increasing the survival rate of cancer patients through timely diagnosis and considers XAI and early prediction as critical steps towards achieving 'wisdom'.

Conclusion: The paper addresses existing challenges in DNN-based MIS and proposes potential solutions to enhance its efficiency.

Abstract: Over the past decade, Medical Image Segmentation (MIS) using Deep Neural
Networks (DNNs) has achieved significant performance improvements and holds
great promise for future developments. This paper presents a comprehensive
study on MIS based on DNNs. Intelligent Vision Systems are often evaluated
based on their output levels, such as Data, Information, Knowledge,
Intelligence, and Wisdom (DIKIW),and the state-of-the-art solutions in MIS at
these levels are the focus of research. Additionally, Explainable Artificial
Intelligence (XAI) has become an important research direction, as it aims to
uncover the "black box" nature of previous DNN architectures to meet the
requirements of transparency and ethics. The study emphasizes the importance of
MIS in disease diagnosis and early detection, particularly for increasing the
survival rate of cancer patients through timely diagnosis. XAI and early
prediction are considered two important steps in the journey from
"intelligence" to "wisdom." Additionally, the paper addresses existing
challenges and proposes potential solutions to enhance the efficiency of
implementing DNN-based MIS.

</details>


### [261] [Recent Advances in Medical Image Classification](https://arxiv.org/abs/2506.04129)
*Loan Dao,Ngoc Quoc Ly*

Main category: eess.IV

TL;DR: Medical image classification benefits from AI advancements. This paper reviews progress focusing on three levels of solutions, deep learning models such as CNNs and Vision Transformers, state-of-the-art approaches with Vision Language Models, and Explainable Artificial Intelligence.


<details>
  <summary>Details</summary>
Motivation: To explore the recent progress in medical image classification that significantly contributes to diagnosis and treatment by leveraging advancements in artificial intelligence.

Method: Reviewing the progress at three levels (basic, specific, applied) and discussing traditional methods like deep learning models (CNNs, Vision Transformers), state-of-the-art approaches with Vision Language Models, and Explainable Artificial Intelligence.

Result: Addressed the challenge of limited labeled data and enhanced predictive results through explainability.

Conclusion: Advancements in AI techniques have improved medical image classification, providing better support for diagnosis and treatment.

Abstract: Medical image classification is crucial for diagnosis and treatment,
benefiting significantly from advancements in artificial intelligence. The
paper reviews recent progress in the field, focusing on three levels of
solutions: basic, specific, and applied. It highlights advances in traditional
methods using deep learning models like Convolutional Neural Networks and
Vision Transformers, as well as state-of-the-art approaches with Vision
Language Models. These models tackle the issue of limited labeled data, and
enhance and explain predictive results through Explainable Artificial
Intelligence.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [262] [Automated Traffic Incident Response Plans using Generative Artificial Intelligence: Part 1 -- Building the Incident Response Benchmark](https://arxiv.org/abs/2506.03381)
*Artur Grigorev,Khaled Saleh,Jiwon Kim,Adriana-Simona Mihaita*

Main category: eess.SY

TL;DR: 论文TLDR总结：全球交通事件是公共安全的重要问题，例如澳大利亚2024年记录了12年来最高的1300起道路死亡事故，美国每年约有600万起车祸。传统应对方式依赖人工决策，存在不一致性和延迟问题。为此，本文提出了一种基于生成式人工智能的新型交通事故响应基准，用于自动生成交通事件响应计划。该方法利用Performance Measurement System (PeMS)的真实事件报告数据进行训练和评估，通过比较历史实施措施与AI生成的响应计划来优化响应策略。实验结果表明，GPT-4o和Grok 2等高级生成式AI模型在与专家方案的一致性方面表现出色，而Gemini 1.5 Pro虽然遗漏动作最少，但其过多的不必要的动作降低了整体效率。


<details>
  <summary>Details</summary>
Motivation: 交通事件（如车祸）对公共安全构成重大威胁，并且传统的人工决策响应方式存在潜在的不一致性与延迟问题，特别是在关键时刻每分钟都可能影响安全结果和网络性能。为了解决这些问题并提高响应效率，需要一种能够快速、准确生成响应计划的方法。

Method: 提出了一种基于生成式人工智能的交通事故响应基准，使用Performance Measurement System (PeMS)中的真实世界事件报告作为训练和评估数据。从这些报告中提取历史上实施的措施，并与AI生成的响应计划进行比较，以生成适当的行动建议（如车道关闭、可变信息标志公告或派遣合适的应急资源）。

Result: 实验结果表明，高级生成式AI模型（如GPT-4o和Grok 2）在与专家解决方案的一致性方面表现优异，Hamming距离平均为2.96-2.98，加权差异约为0.27-0.28。相比之下，尽管Gemini 1.5 Pro遗漏的动作最少，但其过多的不必要动作（1547 vs GPT-4o的225）导致整体计划效率较低。

Conclusion: 提出的基于生成式人工智能的交通事故响应基准能够显著减少事件解决时间，通过提供与特定事件特征相关的上下文适当行动来优化响应计划。高级生成式AI模型（如GPT-4o和Grok 2）在生成高效响应计划方面具有优势，而Gemini 1.5 Pro则因过多的不必要动作降低了整体效率。

Abstract: Traffic incidents remain a critical public safety concern worldwide, with
Australia recording 1,300 road fatalities in 2024, which is the highest toll in
12 years. Similarly, the United States reports approximately 6 million crashes
annually, raising significant challenges in terms of a fast reponse time and
operational management. Traditional response protocols rely on human
decision-making, which introduces potential inconsistencies and delays during
critical moments when every minute impacts both safety outcomes and network
performance. To address this issue, we propose a novel Incident Response
Benchmark that uses generative artificial intelligence to automatically
generate response plans for incoming traffic incidents. Our approach aims to
significantly reduce incident resolution times by suggesting
context-appropriate actions such as variable message sign deployment, lane
closures, and emergency resource allocation adapted to specific incident
characteristics. First, the proposed methodology uses real-world incident
reports from the Performance Measurement System (PeMS) as training and
evaluation data. We extract historically implemented actions from these reports
and compare them against AI-generated response plans that suggest specific
actions, such as lane closures, variable message sign announcements, and/or
dispatching appropriate emergency resources. Second, model evaluations reveal
that advanced generative AI models like GPT-4o and Grok 2 achieve superior
alignment with expert solutions, demonstrated by minimized Hamming distances
(averaging 2.96-2.98) and low weighted differences (approximately 0.27-0.28).
Conversely, while Gemini 1.5 Pro records the lowest count of missed actions,
its extremely high number of unnecessary actions (1547 compared to 225 for
GPT-4o) indicates an over-triggering strategy that reduces the overall plan
efficiency.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [263] [Models of Heavy-Tailed Mechanistic Universality](https://arxiv.org/abs/2506.03470)
*Liam Hodgkinson,Zhichao Wang,Michael W. Mahoney*

Main category: stat.ML

TL;DR: A new random matrix model, the high-temperature Marchenko-Pastur (HTMP) ensemble, is proposed to explore heavy-tailed behavior in trained neural networks, showing potential implications for understanding deep learning efficacy.


<details>
  <summary>Details</summary>
Motivation: The observation that many objects of interest in deep learning exhibit heavy-tailed or power law behavior motivates the exploration of this phenomenon's role in model performance.

Method: The HTMP ensemble model was used to analyze how complex correlation structures in data, reduced training temperatures, and reduced eigenvector entropy contribute to heavy-tailed spectral densities in neural networks.

Result: Three independent factors were identified as contributing to heavy-tailed behavior, which can be controlled with an 'eigenvalue repulsion' parameter. The model also provides insights into neural scaling laws, optimizer trajectories, and phases of neural network training.

Conclusion: Heavy-tailed mechanistic universality (HT-MU) may be a fundamental aspect of deep learning efficacy, and the HTMP ensemble offers a valuable tool for exploring its implications.

Abstract: Recent theoretical and empirical successes in deep learning, including the
celebrated neural scaling laws, are punctuated by the observation that many
objects of interest tend to exhibit some form of heavy-tailed or power law
behavior. In particular, the prevalence of heavy-tailed spectral densities in
Jacobians, Hessians, and weight matrices has led to the introduction of the
concept of heavy-tailed mechanistic universality (HT-MU). Multiple lines of
empirical evidence suggest a robust correlation between heavy-tailed metrics
and model performance, indicating that HT-MU may be a fundamental aspect of
deep learning efficacy. Here, we propose a general family of random matrix
models -- the high-temperature Marchenko-Pastur (HTMP) ensemble -- to explore
attributes that give rise to heavy-tailed behavior in trained neural networks.
Under this model, spectral densities with power laws on (upper and lower) tails
arise through a combination of three independent factors (complex correlation
structures in the data; reduced temperatures during training; and reduced
eigenvector entropy), appearing as an implicit bias in the model structure, and
they can be controlled with an "eigenvalue repulsion" parameter. Implications
of our model on other appearances of heavy tails, including neural scaling
laws, optimizer trajectories, and the five-plus-one phases of neural network
training, are discussed.

</details>


### [264] [SubSearch: Robust Estimation and Outlier Detection for Stochastic Block Models via Subgraph Search](https://arxiv.org/abs/2506.03657)
*Leonardo Martins Bianco,Christine Keribin,Zacharie Naulet*

Main category: stat.ML

TL;DR: The paper proposes SubSearch, an algorithm for robustly estimating SBM parameters by exploring subgraphs and identifying outlier nodes.


<details>
  <summary>Details</summary>
Motivation: Current community detection methods accurately estimate SBM parameters when the input graph is a perfect sample from the model, but real-world graphs rarely conform to such idealized assumptions.

Method: SubSearch explores the space of subgraphs in search of one that closely aligns with the SBM's assumptions and functions as an outlier detection method.

Result: Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of SubSearch.

Conclusion: SubSearch is effective in robustly estimating SBM parameters and identifying outlier nodes.

Abstract: Community detection is a fundamental task in graph analysis, with methods
often relying on fitting models like the Stochastic Block Model (SBM) to
observed networks. While many algorithms can accurately estimate SBM parameters
when the input graph is a perfect sample from the model, real-world graphs
rarely conform to such idealized assumptions. Therefore, robust algorithms are
crucial-ones that can recover model parameters even when the data deviates from
the assumed distribution. In this work, we propose SubSearch, an algorithm for
robustly estimating SBM parameters by exploring the space of subgraphs in
search of one that closely aligns with the model's assumptions. Our approach
also functions as an outlier detection method, properly identifying nodes
responsible for the graph's deviation from the model and going beyond simple
techniques like pruning high-degree nodes. Extensive experiments on both
synthetic and real-world datasets demonstrate the effectiveness of our method.

</details>


### [265] [Position: There Is No Free Bayesian Uncertainty Quantification](https://arxiv.org/abs/2506.03670)
*Ivan Melev,Goeran Kauermann*

Main category: stat.ML

TL;DR: The paper questions the validity of Bayesian uncertainty quantification in machine and deep learning, discussing optimization-based representation, proposing alternative interpretations and quality measures, and suggesting future research directions.


<details>
  <summary>Details</summary>
Motivation: Bayesian methods have gained popularity in machine and deep learning for their intuitive appeal in modeling and uncertainty quantification. However, there is a need to critically assess the validity of Bayesian uncertainty quantification.

Method: The authors discuss the equivalent optimization-based representation of Bayesian updating, propose an alternative interpretation that aligns with the optimization perspective, and introduce measures to evaluate the quality of the Bayesian inferential stage.

Result: The analysis provides insights into the limitations of Bayesian uncertainty quantification and offers alternative perspectives and evaluation metrics for assessing the quality of Bayesian inference in machine learning models.

Conclusion: Bayesian uncertainty quantification should be carefully interpreted and validated. Future work should focus on developing more robust measures and methods for evaluating and improving Bayesian inference in machine and deep learning.

Abstract: Due to their intuitive appeal, Bayesian methods of modeling and uncertainty
quantification have become popular in modern machine and deep learning. When
providing a prior distribution over the parameter space, it is straightforward
to obtain a distribution over the parameters that is conventionally interpreted
as uncertainty quantification of the model. We challenge the validity of such
Bayesian uncertainty quantification by discussing the equivalent
optimization-based representation of Bayesian updating, provide an alternative
interpretation that is coherent with the optimization-based perspective,
propose measures of the quality of the Bayesian inferential stage, and suggest
directions for future work.

</details>


### [266] [Latent Guided Sampling for Combinatorial Optimization](https://arxiv.org/abs/2506.03672)
*Sobihan Surendran,Adeline Fermanian,Sylvain Le Corff*

Main category: stat.ML

TL;DR: Analyze the abstract of a paper.


<details>
  <summary>Details</summary>
Motivation: Combinatorial Optimization problems are computationally challenging due to their NP-hard nature. Current Neural Combinatorial Optimization methods have limitations such as reliance on task-specific augmentations, poor performance on out-of-distribution instances, and lack of robust inference mechanisms.

Method: Propose LGS-Net, a latent space model that conditions on problem instances, and introduce Latent Guided Sampling (LGS), an efficient inference method based on Markov Chain Monte Carlo and Stochastic Approximation.

Result: The iterations of the proposed method form a time-inhomogeneous Markov Chain with rigorous theoretical convergence guarantees. Empirical results demonstrate state-of-the-art performance among RL-based approaches in benchmark routing tasks.

Conclusion: LGS-Net and LGS provide a novel approach to address the limitations of current Neural Combinatorial Optimization methods, achieving excellent performance in benchmark tests.

Abstract: Combinatorial Optimization problems are widespread in domains such as
logistics, manufacturing, and drug discovery, yet their NP-hard nature makes
them computationally challenging. Recent Neural Combinatorial Optimization
methods leverage deep learning to learn solution strategies, trained via
Supervised or Reinforcement Learning (RL). While promising, these approaches
often rely on task-specific augmentations, perform poorly on
out-of-distribution instances, and lack robust inference mechanisms. Moreover,
existing latent space models either require labeled data or rely on pre-trained
policies. In this work, we propose LGS-Net, a novel latent space model that
conditions on problem instances, and introduce an efficient inference method,
Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic
Approximation. We show that the iterations of our method form a
time-inhomogeneous Markov Chain and provide rigorous theoretical convergence
guarantees. Empirical results on benchmark routing tasks show that our method
achieves state-of-the-art performance among RL-based approaches.

</details>


### [267] [Infinitesimal Higher-Order Spectral Variations in Rectangular Real Random Matrices](https://arxiv.org/abs/2506.03764)
*Róisín Luo*

Main category: stat.ML

TL;DR: This paper develops a theoretical framework to compute the n-th order Fréchet derivatives of singular values in real rectangular matrices using Kato's analytic perturbation theory. It provides closed-form expressions for higher-order derivatives, derives the Hessian of a singular value for the first time, and offers tools for spectral sensitivity studies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenge of deriving closed-form expressions for higher-order derivatives of singular values using standard matrix-analysis techniques by leveraging reduced resolvent operators from Kato's analytic perturbation theory.

Method: Treat a real rectangular matrix as a compact operator on a finite-dimensional Hilbert space, embed it into a block self-adjoint operator, and apply Kato's asymptotic eigenvalue expansion to obtain general closed-form expressions for infinitesimal n-th order spectral variations.

Result: A general closed-form expression for the n-th order spectral variations is obtained. Specifically, the Hessian of a singular value (for n=2) is derived, which has not been reported in the literature before.

Conclusion: The proposed framework bridges abstract operator-theoretic perturbation theory with matrices, providing researchers with practical tools for higher-order spectral sensitivity analysis applicable in various fields such as random matrix theory and adversarial perturbations in deep learning.

Abstract: We present a theoretical framework for deriving the general $n$-th order
Fr\'echet derivatives of singular values in real rectangular matrices, by
leveraging reduced resolvent operators from Kato's analytic perturbation theory
for self-adjoint operators. Deriving closed-form expressions for higher-order
derivatives of singular values is notoriously challenging through standard
matrix-analysis techniques. To overcome this, we treat a real rectangular
matrix as a compact operator on a finite-dimensional Hilbert space, and embed
the rectangular matrix into a block self-adjoint operator so that non-symmetric
perturbations are captured. Applying Kato's asymptotic eigenvalue expansion to
this construction, we obtain a general, closed-form expression for the
infinitesimal $n$-th order spectral variations. Specializing to $n=2$ and
deploying on a Kronecker-product representation with matrix convention yield
the Hessian of a singular value, not found in literature. By bridging abstract
operator-theoretic perturbation theory with matrices, our framework equips
researchers with a practical toolkit for higher-order spectral sensitivity
studies in random matrix applications (e.g., adversarial perturbation in deep
learning).

</details>


### [268] [Spatially Resolved Meteorological and Ancillary Data in Central Europe for Rainfall Streamflow Modeling](https://arxiv.org/abs/2506.03819)
*Marc Aurel Vischer,Noelia Otero,Jackie Ma*

Main category: stat.ML

TL;DR: The paper presents a spatially resolved dataset for rainfall streamflow modeling in five central European river basins, along with code for end-to-end modeling.


<details>
  <summary>Details</summary>
Motivation: To advance neural network-driven hydrological modeling beyond lumped catchments by providing a comprehensive and spatially resolved dataset.

Method: Compilation of meteorological forcings and ancillary information on soil, rock, land cover, and orography for five river basins in central Europe. Data harmonized to a regular 9km x 9km grid with daily values from October 1981 to September 2011.

Result: A fully spatially resolved dataset covering five river basins in central Europe and code for combining the dataset with publicly available river discharge data for rainfall streamflow modeling.

Conclusion: This dataset will support more sophisticated and spatially explicit hydrological modeling using neural networks.

Abstract: We present a dataset for rainfall streamflow modeling that is fully spatially
resolved with the aim of taking neural network-driven hydrological modeling
beyond lumped catchments. To this end, we compiled data covering five river
basins in central Europe: upper Danube, Elbe, Oder, Rhine, and Weser. The
dataset contains meteorological forcings, as well as ancillary information on
soil, rock, land cover, and orography. The data is harmonized to a regular 9km
times 9km grid and contains daily values that span from October 1981 to
September 2011. We also provide code to further combine our dataset with
publicly available river discharge data for end-to-end rainfall streamflow
modeling.

</details>


### [269] [Algorithm- and Data-Dependent Generalization Bounds for Score-Based Generative Models](https://arxiv.org/abs/2506.03849)
*Benjamin Dupuis,Dario Shariatian,Maxime Haddouche,Alain Durmus,Umut Simsekli*

Main category: stat.ML

TL;DR: Score-based generative models (SGMs) are popular generative models. While there is existing analysis on SGMs, these analyses tend to be overly pessimistic and coarse. This paper aims to bridge this theoretical gap by providing the first algorithmic- and data-dependent generalization analysis for SGMs.


<details>
  <summary>Details</summary>
Motivation: Existing analysis on SGMs tends to be overly pessimistic and coarse, failing to fully explain the empirical success of SGMs or capture the role of the optimization algorithm used in practice to train the score network.

Method: Firstly, present simple experiments illustrating the concrete impact of optimization hyperparameters on the generalization ability of the generated distribution. Then, provide the first algorithmic- and data-dependent generalization analysis for SGMs, establishing bounds that explicitly account for the optimization dynamics of the learning algorithm.

Result: Theoretical findings are supported by empirical results on several datasets.

Conclusion: This paper bridges the theoretical gap in the analysis of SGMs by providing a new algorithmic- and data-dependent generalization analysis.

Abstract: Score-based generative models (SGMs) have emerged as one of the most popular
classes of generative models. A substantial body of work now exists on the
analysis of SGMs, focusing either on discretization aspects or on their
statistical performance. In the latter case, bounds have been derived, under
various metrics, between the true data distribution and the distribution
induced by the SGM, often demonstrating polynomial convergence rates with
respect to the number of training samples. However, these approaches adopt a
largely approximation theory viewpoint, which tends to be overly pessimistic
and relatively coarse. In particular, they fail to fully explain the empirical
success of SGMs or capture the role of the optimization algorithm used in
practice to train the score network. To support this observation, we first
present simple experiments illustrating the concrete impact of optimization
hyperparameters on the generalization ability of the generated distribution.
Then, this paper aims to bridge this theoretical gap by providing the first
algorithmic- and data-dependent generalization analysis for SGMs. In
particular, we establish bounds that explicitly account for the optimization
dynamics of the learning algorithm, offering new insights into the
generalization behavior of SGMs. Our theoretical findings are supported by
empirical results on several datasets.

</details>


### [270] [Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness](https://arxiv.org/abs/2506.04193)
*Stephen R. Pfohl,Natalie Harris,Chirag Nagpal,David Madras,Vishwali Mhasawade,Olawale Salaudeen,Awa Dieng,Shannon Sequeira,Santiago Arciniegas,Lillian Sung,Nnamdi Ezeanochie,Heather Cole-Lewis,Katherine Heller,Sanmi Koyejo,Alexander D'Amour*

Main category: stat.ML

TL;DR: 尽管在子群体中进行分解评估对于评估机器学习模型的公平性至关重要，但其无批判性的使用可能会误导从业者。本文探讨了数据代表性、选择偏差和因果关系对公平性评估的影响，提出了结合因果假设和分析以控制混杂因素和分布偏移的方法。


<details>
  <summary>Details</summary>
Motivation: 当前在子群体中进行分解评估作为衡量模型公平性的方法存在局限性，特别是在数据具有代表性或受选择偏差影响时，可能导致错误结论。因此，需要更全面的框架来理解和改进这种评估方式。

Method: 利用因果图模型预测不同数据生成过程下的度量稳定性，提出结合显式的因果假设和分析（如条件独立性测试和加权性能估计）来补充分解评估。

Result: 发现分解评估需要与因果假设和分析相结合，才能有效应对混杂因素和分布偏移问题。这对于设计和解释模型评估具有广泛影响。

Conclusion: 为提高模型公平性评估的可靠性，应将分解评估与因果分析相结合，并明确考虑可能的选择偏差和数据生成机制。

Abstract: Disaggregated evaluation across subgroups is critical for assessing the
fairness of machine learning models, but its uncritical use can mislead
practitioners. We show that equal performance across subgroups is an unreliable
measure of fairness when data are representative of the relevant populations
but reflective of real-world disparities. Furthermore, when data are not
representative due to selection bias, both disaggregated evaluation and
alternative approaches based on conditional independence testing may be invalid
without explicit assumptions regarding the bias mechanism. We use causal
graphical models to predict metric stability across subgroups under different
data generating processes. Our framework suggests complementing disaggregated
evaluations with explicit causal assumptions and analysis to control for
confounding and distribution shift, including conditional independence testing
and weighted performance estimation. These findings have broad implications for
how practitioners design and interpret model assessments given the ubiquity of
disaggregated evaluation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [271] [Grounded Vision-Language Interpreter for Integrated Task and Motion Planning](https://arxiv.org/abs/2506.03270)
*Jeremy Siburian,Keisuke Shirai,Cristian C. Beltran-Hernandez,Masashi Hamaya,Michael Görner,Atsushi Hashimoto*

Main category: cs.RO

TL;DR: This paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling verifiable, interpretable, and autonomous robot behaviors.


<details>
  <summary>Details</summary>
Motivation: Recent advances in vision-language models (VLMs) lack safety guarantees and interpretability crucial for real-world deployment, while classical symbolic planners offer rigorous safety verification but require significant expert knowledge for setup.

Method: ViLaIn-TAMP comprises three main components: ViLaIn (Vision-Language Interpreter), a modular Task and Motion Planning (TAMP) system, and a corrective planning module.

Result: The proposed closed-loop corrective architecture exhibits a more than 30% higher mean success rate for ViLaIn-TAMP compared to without corrective planning.

Conclusion: ViLaIn-TAMP bridges the gap between VLMs and classical symbolic planners by providing verifiable, interpretable, and autonomous robot behaviors.

Abstract: While recent advances in vision-language models (VLMs) have accelerated the
development of language-guided robot planners, their black-box nature often
lacks safety guarantees and interpretability crucial for real-world deployment.
Conversely, classical symbolic planners offer rigorous safety verification but
require significant expert knowledge for setup. To bridge the current gap, this
paper proposes ViLaIn-TAMP, a hybrid planning framework for enabling
verifiable, interpretable, and autonomous robot behaviors. ViLaIn-TAMP
comprises three main components: (1) ViLaIn (Vision-Language Interpreter) - A
prior framework that converts multimodal inputs into structured problem
specifications using off-the-shelf VLMs without additional domain-specific
training, (2) a modular Task and Motion Planning (TAMP) system that grounds
these specifications in actionable trajectory sequences through symbolic and
geometric constraint reasoning and can utilize learning-based skills for key
manipulation phases, and (3) a corrective planning module which receives
concrete feedback on failed solution attempts from the motion and task planning
components and can feed adapted logic and geometric feasibility constraints
back to ViLaIn to improve and further refine the specification. We evaluate our
framework on several challenging manipulation tasks in a cooking domain. We
demonstrate that the proposed closed-loop corrective architecture exhibits a
more than 30% higher mean success rate for ViLaIn-TAMP compared to without
corrective planning.

</details>


### [272] [Adversarial Attacks on Robotic Vision Language Action Models](https://arxiv.org/abs/2506.03350)
*Eliot Krzysztof Jones,Alexander Robey,Andy Zou,Zachary Ravichandran,George J. Pappas,Hamed Hassani,Matt Fredrikson,J. Zico Kolter*

Main category: cs.RO

TL;DR: The paper explores adversarial attacks on VLA-controlled robots, adapting LLM jailbreaking attacks to gain full control authority over VLAs.


<details>
  <summary>Details</summary>
Motivation: Vision-language-action models (VLAs) are reshaping robotics by enabling multimodal sensory inputs fusion. However, there are concerns about the vulnerabilities inherited from large language models (LLMs), which are susceptible to adversarial misuse.

Method: The authors adapt and apply LLM jailbreaking attacks to obtain complete control authority over VLAs.

Result: Textual attacks applied once at the beginning of a rollout enable full reachability of the action space of commonly used VLAs and often persist over longer horizons.

Conclusion: This study highlights the vulnerability of VLA-controlled robots to adversarial attacks, differing significantly from LLM jailbreaking literature.

Abstract: The emergence of vision-language-action models (VLAs) for end-to-end control
is reshaping the field of robotics by enabling the fusion of multimodal sensory
inputs at the billion-parameter scale. The capabilities of VLAs stem primarily
from their architectures, which are often based on frontier large language
models (LLMs). However, LLMs are known to be susceptible to adversarial misuse,
and given the significant physical risks inherent to robotics, questions remain
regarding the extent to which VLAs inherit these vulnerabilities. Motivated by
these concerns, in this work we initiate the study of adversarial attacks on
VLA-controlled robots. Our main algorithmic contribution is the adaptation and
application of LLM jailbreaking attacks to obtain complete control authority
over VLAs. We find that textual attacks, which are applied once at the
beginning of a rollout, facilitate full reachability of the action space of
commonly used VLAs and often persist over longer horizons. This differs
significantly from LLM jailbreaking literature, as attacks in the real world do
not have to be semantically linked to notions of harm. We make all code
available at https://github.com/eliotjones1/robogcg .

</details>


### [273] [SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models](https://arxiv.org/abs/2506.03516)
*Arnab Debnath,Gregory J. Stein,Jana Kosecka*

Main category: cs.RO

TL;DR: The paper introduces a zero-shot object goal navigation framework combining Vision Foundation Models (VFMs) and a model-based planner, achieving state-of-the-art performance on the HM3D dataset.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional learning-based methods in object goal navigation which rely heavily on large-scale annotated data or require extensive interaction with the environment, often failing to generalize to novel environments and limiting scalability.

Method: The method involves integrating the perceptual strength of VFMs with a model-based planner capable of long-horizon decision making through frontier exploration, allowing the agent to operate without task-specific training in a zero-shot setting.

Result: The approach was evaluated on the HM3D dataset using the Habitat simulator and showed state-of-the-art performance in terms of success weighted by path length for zero-shot object goal navigation.

Conclusion: A zero-shot object goal navigation framework that leverages VFMs and a model-based planner has been successfully developed, offering a more scalable and adaptable solution for navigating unexplored environments.

Abstract: Object goal navigation is a fundamental task in embodied AI, where an agent
is instructed to locate a target object in an unexplored environment.
Traditional learning-based methods rely heavily on large-scale annotated data
or require extensive interaction with the environment in a reinforcement
learning setting, often failing to generalize to novel environments and
limiting scalability. To overcome these challenges, we explore a zero-shot
setting where the agent operates without task-specific training, enabling more
scalable and adaptable solution. Recent advances in Vision Foundation Models
(VFMs) offer powerful capabilities for visual understanding and reasoning,
making them ideal for agents to comprehend scenes, identify relevant regions,
and infer the likely locations of objects. In this work, we present a zero-shot
object goal navigation framework that integrates the perceptual strength of
VFMs with a model-based planner that is capable of long-horizon decision making
through frontier exploration. We evaluate our approach on the HM3D dataset
using the Habitat simulator and demonstrate that our method achieves
state-of-the-art performance in terms of success weighted by path length for
zero-shot object goal navigation.

</details>


### [274] [From Virtual Agents to Robot Teams: A Multi-Robot Framework Evaluation in High-Stakes Healthcare Context](https://arxiv.org/abs/2506.03546)
*Yuanchen Bai,Zijian Ding,Angelique Taylor*

Main category: cs.RO

TL;DR: The paper explores the limitations of current multi-agent systems in transferring virtual tasks to physical robotic teams. Through testing a hierarchical multi-agent robotic team in a simulated scenario, five failure modes were identified and three design guidelines proposed.


<details>
  <summary>Details</summary>
Motivation: Current frameworks for multi-agent systems often treat agents as conceptual task executors rather than physically embodied entities, leading to challenges in applying these systems to physical multi-agent robotic teams.

Method: The researchers reconfigured and stress-tested a hierarchical multi-agent robotic team built on the CrewAI framework within a simulated emergency department onboarding scenario to identify persistent failure modes.

Result: Five persistent failure modes were identified: role misalignment; tool access violations; lack of in-time handling of failure reports; noncompliance with prescribed workflows; bypassing or false reporting of task completion.

Conclusion: Three design guidelines emphasizing process transparency, proactive failure recovery, and contextual grounding are proposed to inform the development of more resilient and robust multi-agent robotic systems.

Abstract: Advancements in generative models have enabled multi-agent systems (MAS) to
perform complex virtual tasks such as writing and code generation, which do not
generalize well to physical multi-agent robotic teams. Current frameworks often
treat agents as conceptual task executors rather than physically embodied
entities, and overlook critical real-world constraints such as spatial context,
robotic capabilities (e.g., sensing and navigation). To probe this gap, we
reconfigure and stress-test a hierarchical multi-agent robotic team built on
the CrewAI framework in a simulated emergency department onboarding scenario.
We identify five persistent failure modes: role misalignment; tool access
violations; lack of in-time handling of failure reports; noncompliance with
prescribed workflows; bypassing or false reporting of task completion. Based on
this analysis, we propose three design guidelines emphasizing process
transparency, proactive failure recovery, and contextual grounding. Our work
informs the development of more resilient and robust multi-agent robotic
systems (MARS), including opportunities to extend virtual multi-agent
frameworks to the real world.

</details>


### [275] [Confidence-Guided Human-AI Collaboration: Reinforcement Learning with Distributional Proxy Value Propagation for Autonomous Driving](https://arxiv.org/abs/2506.03568)
*Li Zeqiao,Wang Yijing,Wang Haoyu,Li Zheng,Li Peng,Zuo zhiqiang,Hu Chuan*

Main category: cs.RO

TL;DR: C-HAC是一种新的自主驾驶策略，结合了人类指导和自我学习，通过最小化人类干预来提高安全性、效率和整体性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习和模仿学习在自主驾驶中面临安全探索和分布转移的挑战，而现有的人类-AI协作方法通常需要大量的人类干预，导致成本高且效率低下。

Method: C-HAC采用了一种基于DSAC框架的分布代理值传播方法，利用回报分布表示人类意图以快速稳定地学习人类指导策略。接着，通过共享控制机制将人类指导策略与自我学习策略相结合，使代理能够独立探索并持续提升性能。最后，使用策略置信度评估算法动态切换两种策略，确保代理在保持安全和性能的同时追求最优策略。

Result: 实验表明，C-HAC在多种驾驶场景中显著优于传统方法，在安全性、效率和整体性能方面达到业界领先水平，并通过现实世界中的复杂交通条件测试验证了其有效性。

Conclusion: C-HAC策略成功克服了现有方法对大量人类干预的依赖，实现了更高效、更安全且性能更高的自主驾驶，代码和视频已在GitHub上公开。

Abstract: Autonomous driving promises significant advancements in mobility, road safety
and traffic efficiency, yet reinforcement learning and imitation learning face
safe-exploration and distribution-shift challenges. Although human-AI
collaboration alleviates these issues, it often relies heavily on extensive
human intervention, which increases costs and reduces efficiency. This paper
develops a confidence-guided human-AI collaboration (C-HAC) strategy to
overcome these limitations. First, C-HAC employs a distributional proxy value
propagation method within the distributional soft actor-critic (DSAC)
framework. By leveraging return distributions to represent human intentions
C-HAC achieves rapid and stable learning of human-guided policies with minimal
human interaction. Subsequently, a shared control mechanism is activated to
integrate the learned human-guided policy with a self-learning policy that
maximizes cumulative rewards. This enables the agent to explore independently
and continuously enhance its performance beyond human guidance. Finally, a
policy confidence evaluation algorithm capitalizes on DSAC's return
distribution networks to facilitate dynamic switching between human-guided and
self-learning policies via a confidence-based intervention function. This
ensures the agent can pursue optimal policies while maintaining safety and
performance guarantees. Extensive experiments across diverse driving scenarios
reveal that C-HAC significantly outperforms conventional methods in terms of
safety, efficiency, and overall performance, achieving state-of-the-art
results. The effectiveness of the proposed method is further validated through
real-world road tests in complex traffic conditions. The videos and code are
available at: https://github.com/lzqw/C-HAC.

</details>


### [276] [SLAC: Simulation-Pretrained Latent Action Space for Whole-Body Real-World RL](https://arxiv.org/abs/2506.04147)
*Jiaheng Hu,Peter Stone,Roberto Martín-Martín*

Main category: cs.RO

TL;DR: SLAC uses a low-fidelity simulator to pretrain a task-agnostic latent action space and then employs an off-policy RL algorithm for efficient real-world learning of complex tasks.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods struggle with high-degree-of-freedom systems like mobile manipulators due to safety and sample efficiency issues in direct RL, and brittleness in sim-to-real transfer.

Method: SLAC leverages a low-fidelity simulator to train a latent action space using an unsupervised skill discovery method that promotes temporal abstraction, disentanglement, and safety. This latent action space is then used as the action interface for an off-policy RL algorithm to learn downstream tasks through real-world interactions.

Result: SLAC achieves state-of-the-art performance on bimanual mobile manipulation tasks, notably learning whole-body tasks within an hour without demonstrations or hand-crafted behavior priors.

Conclusion: SLAC makes real-world RL feasible for complex robot embodiments by combining pretrained latent action spaces with efficient downstream learning.

Abstract: Building capable household and industrial robots requires mastering the
control of versatile, high-degree-of-freedom (DoF) systems such as mobile
manipulators. While reinforcement learning (RL) holds promise for autonomously
acquiring robot control policies, scaling it to high-DoF embodiments remains
challenging. Direct RL in the real world demands both safe exploration and high
sample efficiency, which are difficult to achieve in practice. Sim-to-real RL,
on the other hand, is often brittle due to the reality gap. This paper
introduces SLAC, a method that renders real-world RL feasible for complex
embodiments by leveraging a low-fidelity simulator to pretrain a task-agnostic
latent action space. SLAC trains this latent action space via a customized
unsupervised skill discovery method designed to promote temporal abstraction,
disentanglement, and safety, thereby facilitating efficient downstream
learning. Once a latent action space is learned, SLAC uses it as the action
interface for a novel off-policy RL algorithm to autonomously learn downstream
tasks through real-world interactions. We evaluate SLAC against existing
methods on a suite of bimanual mobile manipulation tasks, where it achieves
state-of-the-art performance. Notably, SLAC learns contact-rich whole-body
tasks in under an hour of real-world interactions, without relying on any
demonstrations or hand-crafted behavior priors. More information, code, and
videos at robo-rl.github.io

</details>


### [277] [OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis](https://arxiv.org/abs/2506.04217)
*Junting Chen,Haotian Liang,Lingxiao Du,Weiyun Wang,Mengkang Hu,Yao Mu,Wenhai Wang,Jifeng Dai,Ping Luo,Wenqi Shao,Lin Shao*

Main category: cs.RO

TL;DR: The paper proposes a multi-modal agent architecture for open-world mobile manipulation (OWMM) tasks, addressing challenges of generalization and complexity by integrating scene understanding, agent states, and function calling controls. It also introduces an agentic data synthesis pipeline to adapt VLM models with instruction fine-tuning. The OWMM-VLM model achieves SOTA performance and strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Current mobile manipulators are capable in specialized tasks but struggle with open-world mobile manipulation due to the need for generalization across instructions and environments, as well as the integration of high-level decision making with low-level control.

Method: A novel multi-modal agent architecture is proposed which maintains multi-view scene frames and agent states for decision-making and controls the robot via function calling. Additionally, an agentic data synthesis pipeline is introduced to adapt VLM models to the OWMM task domain through instruction fine-tuning.

Result: The OWMM-VLM model outperforms other foundation models like GPT-4o and shows strong zero-shot generalization in real-world scenarios.

Conclusion: The OWMM-VLM is highlighted as the first dedicated foundation model for mobile manipulators that combines global scene understanding, robot state tracking, and multi-modal action generation.

Abstract: The rapid progress of navigation, manipulation, and vision models has made
mobile manipulators capable in many specialized tasks. However, the open-world
mobile manipulation (OWMM) task remains a challenge due to the need for
generalization to open-ended instructions and environments, as well as the
systematic complexity to integrate high-level decision making with low-level
robot control based on both global scene understanding and current agent state.
To address this complexity, we propose a novel multi-modal agent architecture
that maintains multi-view scene frames and agent states for decision-making and
controls the robot by function calling. A second challenge is the hallucination
from domain shift. To enhance the agent performance, we further introduce an
agentic data synthesis pipeline for the OWMM task to adapt the VLM model to our
task domain with instruction fine-tuning. We highlight our fine-tuned OWMM-VLM
as the first dedicated foundation model for mobile manipulators with global
scene understanding, robot state tracking, and multi-modal action generation in
a unified model. Through experiments, we demonstrate that our model achieves
SOTA performance compared to other foundation models including GPT-4o and
strong zero-shot generalization in real world. The project page is at
https://github.com/HHYHRHY/OWMM-Agent

</details>


### [278] [Pseudo-Simulation for Autonomous Driving](https://arxiv.org/abs/2506.04218)
*Wei Cao,Marcel Hallgarten,Tianyu Li,Daniel Dauner,Xunjiang Gu,Caojun Wang,Yakov Miron,Marco Aiello,Hongyang Li,Igor Gilitschenski,Boris Ivanovic,Marco Pavone,Andreas Geiger,Kashyap Chitta*

Main category: cs.RO

TL;DR: Existing AV evaluation methods have limitations. This paper proposes pseudo-simulation, which uses real datasets with synthetic observations to evaluate AVs more effectively and efficiently without sequential interactive simulation.


<details>
  <summary>Details</summary>
Motivation: Current evaluation paradigms for autonomous vehicles (AVs) face critical limitations including safety concerns in real-world evaluations, insufficient realism or high computational costs in closed-loop simulations, and compounding errors overlooked by metrics in open-loop evaluations.

Method: The proposed method, pseudo-simulation, operates on real datasets like open-loop evaluation but augments them with synthetic observations generated using 3D Gaussian Splatting. A proximity-based weighting scheme is used to assign higher importance to synthetic observations that best match the AV's likely behavior.

Result: Pseudo-simulation shows better correlation with closed-loop simulations (R^2=0.8) compared to the best existing open-loop approach (R^2=0.7).

Conclusion: The authors propose pseudo-simulation as a novel paradigm for evaluating AVs, which addresses the limitations of current methods and provides a public leaderboard for benchmarking.

Abstract: Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.

</details>


### [279] [STAR: Learning Diverse Robot Skill Abstractions through Rotation-Augmented Vector Quantization](https://arxiv.org/abs/2506.03863)
*Hao Li,Qi Lv,Rui Shao,Xiang Deng,Yinchuan Li,Jianye Hao,Liqiang Nie*

Main category: cs.RO

TL;DR: The paper introduces STAR, a framework that improves skill learning and composition for robotic manipulation by preventing codebook collapse and capturing causal relationships between skills.


<details>
  <summary>Details</summary>
Motivation: Existing methods using latent variable models for learning skill abstractions suffer from codebook collapse and difficulty in modeling causal relationships between skills.

Method: STAR includes RaRSQ to prevent codebook collapse by encoding relative angles into gradient flow and CST to capture causal relationships between skills through an autoregressive mechanism.

Result: STAR shows superiority on both LIBERO benchmark and real-world tasks with around 12% improvement over baselines.

Conclusion: STAR advances skill learning and composition for complex behaviors in robotic manipulation.

Abstract: Transforming complex actions into discrete skill abstractions has
demonstrated strong potential for robotic manipulation. Existing approaches
mainly leverage latent variable models, e.g., VQ-VAE, to learn skill
abstractions through learned vectors (codebooks), while they suffer from
codebook collapse and modeling the causal relationship between learned skills.
To address these limitations, we present \textbf{S}kill \textbf{T}raining with
\textbf{A}ugmented \textbf{R}otation (\textbf{STAR}), a framework that advances
both skill learning and composition to complete complex behaviors.
Specifically, to prevent codebook collapse, we devise rotation-augmented
residual skill quantization (RaRSQ). It encodes relative angles between encoder
outputs into the gradient flow by rotation-based gradient mechanism. Points
within the same skill code are forced to be either pushed apart or pulled
closer together depending on gradient directions. Further, to capture the
causal relationship between skills, we present causal skill transformer (CST)
which explicitly models dependencies between skill representations through an
autoregressive mechanism for coherent action generation. Extensive experiments
demonstrate the superiority of STAR on both LIBERO benchmark and realworld
tasks, with around 12\% improvement over the baselines.

</details>


### [280] [Object-centric 3D Motion Field for Robot Learning from Human Videos](https://arxiv.org/abs/2506.04227)
*Zhao-Heng Yin,Sherry Yang,Pieter Abbeel*

Main category: cs.RO

TL;DR: 提出了一种新的框架，利用以对象为中心的3D运动场从人类视频中提取动作表示，用于零样本机器人控制。该框架包含两个新组件：去噪3D运动场估计器和密集的对象中心3D运动场预测架构。实验表明，该方法显著降低了3D运动估计误差，并在多样化任务中取得了较高的成功率。


<details>
  <summary>Details</summary>
Motivation: 从人类视频中学习机器人控制策略是一个有前景的方向，但如何从视频中提取动作知识（或动作表示）仍然是一个关键挑战。现有的动作表示如视频帧、像素流和点云流存在建模复杂性或信息丢失等固有限制。

Method: 作者提出使用以对象为中心的3D运动场来表示从人类视频中学习机器人的动作，并提出了一个新的框架来从视频中提取这种表示用于零样本控制。引入了两个新的组件：1）一种新的训练管道，用于训练“去噪”3D运动场估计器，以从具有噪声深度的人类视频中稳健地提取精细的物体3D运动；2）一种密集的对象中心3D运动场预测架构，有利于跨实体转移和背景政策泛化。

Result: 实验表明，与最新方法相比，该方法减少了超过50%的3D运动估计误差，在多样化的任务中平均成功率达到55%，而先前的方法几乎失败（<10%），甚至可以获取精细的操作技能，如插入操作。

Conclusion: 提出的框架和方法有效地解决了从人类视频中提取动作表示的挑战，显著提高了机器人在实际环境中的性能和泛化能力，展示了其在零样本控制中的潜力。

Abstract: Learning robot control policies from human videos is a promising direction
for scaling up robot learning. However, how to extract action knowledge (or
action representations) from videos for policy learning remains a key
challenge. Existing action representations such as video frames, pixelflow, and
pointcloud flow have inherent limitations such as modeling complexity or loss
of information. In this paper, we propose to use object-centric 3D motion field
to represent actions for robot learning from human videos, and present a novel
framework for extracting this representation from videos for zero-shot control.
We introduce two novel components in its implementation. First, a novel
training pipeline for training a ''denoising'' 3D motion field estimator to
extract fine object 3D motions from human videos with noisy depth robustly.
Second, a dense object-centric 3D motion field prediction architecture that
favors both cross-embodiment transfer and policy generalization to background.
We evaluate the system in real world setups. Experiments show that our method
reduces 3D motion estimation error by over 50% compared to the latest method,
achieve 55% average success rate in diverse tasks where prior approaches
fail~($\lesssim 10$\%), and can even acquire fine-grained manipulation skills
like insertion.

</details>


### [281] [Autonomous Vehicle Lateral Control Using Deep Reinforcement Learning with MPC-PID Demonstration](https://arxiv.org/abs/2506.04040)
*Chengdong Wu,Sven Kirchner,Nils Purschke,Alois C. Knoll*

Main category: cs.RO

TL;DR: A reinforcement learning based lateral control approach for autonomous driving is presented, which combines MPC-PID and DRL to ensure comfortable, efficient, and robust control performance even with incomplete vehicle information.


<details>
  <summary>Details</summary>
Motivation: To address the imperfections in vehicle models due to measurement errors and simplifications, and to ensure comfortable, efficient, and robust control performance in autonomous driving.

Method: The controller consists of a conventional Model Predictive Control (MPC)-PID part as the basis and demonstrator, and a Deep Reinforcement Learning (DRL) part that leverages online information from the MPC-PID part.

Result: Experimental results in CARLA demonstrate the effectiveness of the controller when vehicle information is incomplete, and show that the training of DRL can be stabilized with the demonstration part.

Conclusion: This approach highlights the potential to reduce development and integration efforts for autonomous driving pipelines in the future.

Abstract: The controller is one of the most important modules in the autonomous driving
pipeline, ensuring the vehicle reaches its desired position. In this work, a
reinforcement learning based lateral control approach, despite the
imperfections in the vehicle models due to measurement errors and
simplifications, is presented. Our approach ensures comfortable, efficient, and
robust control performance considering the interface between controlling and
other modules. The controller consists of the conventional Model Predictive
Control (MPC)-PID part as the basis and the demonstrator, and the Deep
Reinforcement Learning (DRL) part which leverages the online information from
the MPC-PID part. The controller's performance is evaluated in CARLA using the
ground truth of the waypoints as inputs. Experimental results demonstrate the
effectiveness of the controller when vehicle information is incomplete, and the
training of DRL can be stabilized with the demonstration part. These findings
highlight the potential to reduce development and integration efforts for
autonomous driving pipelines in the future.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [282] [From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications](https://arxiv.org/abs/2506.03464)
*Yang Cai,Haipeng Luo,Chen-Yu Wei,Weiqiang Zheng*

Main category: cs.GT

TL;DR: 本研究展示了在一大类游戏中，存在一种简单的方法将平均迭代收敛转化为最终迭代收敛，并通过应用该方法于Optimistic Multiplicative Weights Update算法，在二人零和博弈中获得了目前最先进的最终迭代收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在线学习算法在游戏中的收敛性是博弈论和机器学习中的一个基本问题。虽然许多算法在平均迭代上可以实现收敛，但在设计和分析能够实现最终迭代收敛的算法时通常需要更多的努力。

Method: 研究人员提出了一种简单的黑箱转换方法，可将非耦合学习动态的平均迭代转化为新的非耦合学习动态的最终迭代。此方法适用于每个玩家的效用与其自身策略和所有对手联合策略呈线性关系的游戏，包括二人双矩阵游戏及其多玩家推广形式。然后将此方法应用于Optimistic Multiplicative Weights Update算法。

Result: 通过应用所提出的转换方法，研究人员在二人零和正常形式游戏中获得了最先进的最终迭代收敛速度：(1) 在梯度反馈下为O(\frac{log d}{T})，这在维度d的依赖性上实现了指数级改进；(2) 在bandit反馈下为\widetilde{O}(d^{\frac{1}{5}} T^{-\frac{1}{5}})，优于之前的最佳速率。

Conclusion: 这项工作展示了一种有效的黑箱转换方法，可以显著提升某些类型游戏中的最终迭代收敛速度，为在线学习算法的设计和分析提供了新的思路。

Abstract: The convergence of online learning algorithms in games under self-play is a
fundamental question in game theory and machine learning. Among various notions
of convergence, last-iterate convergence is particularly desirable, as it
reflects the actual decisions made by the learners and captures the day-to-day
behavior of the learning dynamics. While many algorithms are known to converge
in the average-iterate, achieving last-iterate convergence typically requires
considerably more effort in both the design and the analysis of the algorithm.
Somewhat surprisingly, we show in this paper that for a large family of games,
there exists a simple black-box reduction that transforms the average iterates
of an uncoupled learning dynamics into the last iterates of a new uncoupled
learning dynamics, thus also providing a reduction from last-iterate
convergence to average-iterate convergence. Our reduction applies to games
where each player's utility is linear in both their own strategy and the joint
strategy of all opponents. This family includes two-player bimatrix games and
generalizations such as multi-player polymatrix games. By applying our
reduction to the Optimistic Multiplicative Weights Update algorithm, we obtain
new state-of-the-art last-iterate convergence rates for uncoupled learning
dynamics in two-player zero-sum normal-form games: (1) an $O(\frac{\log d}{T})$
last-iterate convergence rate under gradient feedback, representing an
exponential improvement in the dependence on the dimension $d$ (i.e., the
maximum number of actions available to either player); and (2) an
$\widetilde{O}(d^{\frac{1}{5}} T^{-\frac{1}{5}})$ last-iterate convergence rate
under bandit feedback, improving upon the previous best rates of
$\widetilde{O}(\sqrt{d} T^{-\frac{1}{8}})$ and $\widetilde{O}(\sqrt{d}
T^{-\frac{1}{6}})$.

</details>


<div id='physics.acc-ph'></div>

# physics.acc-ph [[Back]](#toc)

### [283] [Geoff: The Generic Optimization Framework & Frontend for Particle Accelerator Controls](https://arxiv.org/abs/2506.03796)
*Penelope Madysa,Sabrina Appel,Verena Kain,Michael Schenk*

Main category: physics.acc-ph

TL;DR: Geoff is a Python framework for automating particle accelerator controls, aiming to harmonize machine learning approaches and minimize friction when comparing or migrating between them. It provides standardized interfaces, utility functions, and a reference GUI application. Developed at CERN, it's an open-source library maintained by CERN and GSI as part of the EURO-LABS project.


<details>
  <summary>Details</summary>
Motivation: To improve accelerator performance and uptime by harmonizing various machine learning approaches and minimizing friction when comparing or migrating between them.

Method: Providing standardized interfaces for optimization problems, utility functions to speed up development, and a reference GUI application that ties everything together.

Result: Successful creation of an open-source library that offers a collection of Python packages forming a framework for automation of particle accelerator controls.

Conclusion: Geoff's design, features, and current usage provide a comprehensive solution for integrating and improving machine learning techniques in particle accelerator laboratories.

Abstract: Geoff is a collection of Python packages that form a framework for automation
of particle accelerator controls. With particle accelerator laboratories around
the world researching machine learning techniques to improve accelerator
performance and uptime, a multitude of approaches and algorithms have emerged.
The purpose of Geoff is to harmonize these approaches and to minimize friction
when comparing or migrating between them. It provides standardized interfaces
for optimization problems, utility functions to speed up development, and a
reference GUI application that ties everything together. Geoff is an
open-source library developed at CERN and maintained and updated in
collaboration between CERN and GSI as part of the EURO-LABS project. This paper
gives an overview over Geoff's design, features, and current usage.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [284] [TransClean: Finding False Positives in Multi-Source Entity Matching under Real-World Conditions via Transitive Consistency](https://arxiv.org/abs/2506.04006)
*Fernando de Meer Pardo,Branka Hadji Misheva,Martin Braschler,Kurt Stockinger*

Main category: cs.DB

TL;DR: TransClean is a method for detecting false positive predictions in entity matching algorithms under real-world conditions, characterized by large-scale, noisy, and unlabeled multi-source datasets that undergo distributional shifts. It leverages Transitive Consistency and operates efficiently with multiple data sources while requiring limited manual labeling.


<details>
  <summary>Details</summary>
Motivation: Current entity matching algorithms struggle with false positive predictions when dealing with large-scale, noisy, and unlabeled multi-source datasets that undergo distributional shifts. There is a need for a method that can operate efficiently and robustly with multiple data sources while requiring limited manual labeling.

Method: TransClean modifies a matching iteratively through gradually removing false positive matches while removing as few true positive matches as possible. It estimates the Transitive Consistency exclusively through model evaluations, producing quantities that can be used as proxies of the amounts of true and false positives in the matching without requiring any manual labeling.

Result: Experiments show that combining TransClean with either a naively trained pairwise matching model (DistilBERT) or a state-of-the-art end-to-end matching method (CLER) allows it to detect most of the false positives across a variety of datasets. TransClean induces an average +24.42 F1 score improvement for entity matching in a multi-source setting compared to traditional pair-wise matching algorithms.

Conclusion: TransClean is a promising method for detecting false positive predictions in entity matching algorithms under challenging real-world conditions. It demonstrates flexibility and effectiveness in improving the quality of matchings across various datasets.

Abstract: We present TransClean, a method for detecting false positive predictions of
entity matching algorithms under real-world conditions characterized by
large-scale, noisy, and unlabeled multi-source datasets that undergo
distributional shifts. TransClean is explicitly designed to operate with
multiple data sources in an efficient, robust and fast manner while accounting
for edge cases and requiring limited manual labeling. TransClean leverages the
Transitive Consistency of a matching, a measure of the consistency of a
pairwise matching model f_theta on the matching it produces G_f_theta, based
both on its predictions on directly evaluated record pairs and its predictions
on implied record pairs. TransClean iteratively modifies a matching through
gradually removing false positive matches while removing as few true positive
matches as possible. In each of these steps, the estimation of the Transitive
Consistency is exclusively done through model evaluations and produces
quantities that can be used as proxies of the amounts of true and false
positives in the matching while not requiring any manual labeling, producing an
estimate of the quality of the matching and indicating which record groups are
likely to contain false positives. In our experiments, we compare combining
TransClean with a naively trained pairwise matching model (DistilBERT) and with
a state-of-the-art end-to-end matching method (CLER) and illustrate the
flexibility of TransClean in being able to detect most of the false positives
of either setup across a variety of datasets. Our experiments show that
TransClean induces an average +24.42 F1 score improvement for entity matching
in a multi-source setting when compared to traditional pair-wise matching
algorithms.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [285] [Structural Vibration Monitoring with Diffractive Optical Processors](https://arxiv.org/abs/2506.03317)
*Yuntian Wang,Zafer Yilmaz,Yuhang Li,Edward Liu,Eric Ahlberg,Farid Ghahari,Ertugrul Taciroglu,Aydogan Ozcan*

Main category: physics.optics

TL;DR: A diffractive vibration monitoring system is presented, integrating a diffractive layer with a shallow neural network to extract 3D structural vibration spectra. It offers low-power, cost-effective and scalable solution for SHM.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current SHM solutions in terms of cost, power consumption, scalability, and data processing complexity.

Method: The system uses a spatially-optimized passive diffractive layer that encodes 3D structural displacements into modulated light, captured by minimal detectors and decoded in real-time by shallow and low-power neural networks.

Result: Demonstrated both numerically and experimentally using millimeter-wave illumination on a laboratory-scale building model, achieving more than an order-of-magnitude improvement in accuracy over conventional optics or separately trained modules.

Conclusion: This framework establishes a new computational sensing modality with potential applications in disaster resilience, aerospace diagnostics, and autonomous navigation.

Abstract: Structural Health Monitoring (SHM) is vital for maintaining the safety and
longevity of civil infrastructure, yet current solutions remain constrained by
cost, power consumption, scalability, and the complexity of data processing.
Here, we present a diffractive vibration monitoring system, integrating a
jointly optimized diffractive layer with a shallow neural network-based backend
to remotely extract 3D structural vibration spectra, offering a low-power,
cost-effective and scalable solution. This architecture eliminates the need for
dense sensor arrays or extensive data acquisition; instead, it uses a
spatially-optimized passive diffractive layer that encodes 3D structural
displacements into modulated light, captured by a minimal number of detectors
and decoded in real-time by shallow and low-power neural networks to
reconstruct the 3D displacement spectra of structures. The diffractive system's
efficacy was demonstrated both numerically and experimentally using
millimeter-wave illumination on a laboratory-scale building model with a
programmable shake table. Our system achieves more than an order-of-magnitude
improvement in accuracy over conventional optics or separately trained modules,
establishing a foundation for high-throughput 3D monitoring of structures.
Beyond SHM, the 3D vibration monitoring capabilities of this cost-effective and
data-efficient framework establish a new computational sensing modality with
potential applications in disaster resilience, aerospace diagnostics, and
autonomous navigation, where energy efficiency, low latency, and
high-throughput are critical.

</details>
