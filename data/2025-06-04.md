<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 43]
- [cs.LG](#cs.LG) [Total: 126]
- [cs.CR](#cs.CR) [Total: 25]
- [cs.SE](#cs.SE) [Total: 6]
- [quant-ph](#quant-ph) [Total: 3]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [eess.AS](#eess.AS) [Total: 7]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [stat.ME](#stat.ME) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [stat.ML](#stat.ML) [Total: 14]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.SD](#cs.SD) [Total: 8]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 36]
- [eess.SY](#eess.SY) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [math.ST](#math.ST) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.RO](#cs.RO) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 45]
- [math.AT](#math.AT) [Total: 1]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.NA](#math.NA) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)
*Priyaranjan Pattnayak,Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.AI

TL;DR: This paper presents a hybrid framework combining Retrieval-Augmented Generation (RAG) with intent-based canned responses for enterprise conversational AI. It uses predefined responses for efficiency and routes complex queries to RAG, while maintaining coherence via a dialogue context manager and improving over time through a feedback loop. Experiments show high accuracy and low latency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in enterprise-scale deployments of conversational AI systems such as diverse user queries, high latency, hallucinations, and difficulty integrating frequently updated domain-specific knowledge.

Method: The method integrates RAG with intent-based canned responses. Predefined high-confidence responses are used for efficiency, while complex or ambiguous queries are routed to the RAG pipeline. A dialogue context manager ensures coherence in multi-turn interactions, and a feedback loop refines intents, adjusts confidence thresholds, and expands response coverage.

Result: Experimental results indicate that the framework achieves 95% accuracy and 180ms latency, outperforming both RAG and intent-based systems across various query types.

Conclusion: The proposed hybrid framework offers a scalable and adaptive solution for enterprise conversational AI applications by balancing high accuracy with low latency.

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [2] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/abs/2506.02125)
*Guy Tennenholtz,Jihwan Jeong,Chih-Wei Hsu,Yinlam Chow,Craig Boutilier*

Main category: cs.AI

TL;DR: Effective decision making in partially observable environments needs compressing long interaction histories. We introduce Descriptive History Representations (DHRs) and a multi-agent learning framework to generate interpretable user profiles.


<details>
  <summary>Details</summary>
Motivation: Effective decision making requires compressing long interaction histories into informative representations in partially observable environments.

Method: Introduced Descriptive History Representations (DHRs) which are sufficient statistics capable of answering relevant questions about past interactions and future outcomes. Proposed a multi-agent learning framework with representation, decision, and question-asking components optimized using a joint objective balancing reward maximization and the representation's ability to answer questions.

Result: Generated interpretable textual user profiles that serve as sufficient statistics for predicting preference-driven behavior of users in user modeling tasks with public movie and shopping datasets.

Conclusion: The approach yields representations capturing salient historical details and predictive structures needed for effective decision making.

Abstract: Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [3] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: Few-shot learning in LLMs presents a paradox where some tasks can generalize from few examples while others need extensive supervision. UCCT explains this by considering LLMs as unconscious substrates with latent patterns, not incomplete agents. Semantic anchoring through prompts and interactions acts as a conscious control layer for coherent reasoning. AGI will be achieved not by discarding LLMs but by integrating them into systems that reason and adapt together.


<details>
  <summary>Details</summary>
Motivation: To understand and resolve the paradox of few-shot learning in LLMs where generalization varies significantly across different tasks.

Method: Using the Unified Cognitive Consciousness Theory (UCCT), reframe LLMs as unconscious substrates containing latent linguistic and conceptual patterns. Introduce semantic anchoring as a mechanism to bind these patterns to task-relevant meaning.

Result: UCCT provides a unifying framework explaining prompting, fine-tuning, retrieval, and multi-agent coordination in LLMs. The Threshold-Crossing Dynamics Theorem formalizes semantic anchoring as a probabilistic phase transition.

Conclusion: AGI development should focus on aligning and integrating LLMs within broader systems rather than disregarding them.

Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [4] [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
*Peter Belcak,Greg Heinrich,Shizhe Diao,Yonggan Fu,Xin Dong,Saurav Muralidharan,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.AI

TL;DR: 小语言模型(SLMs)在代理系统中足够强大、更适合且更经济，对于需要通用对话能力的情况，异构代理系统是更好的选择。部分从大语言模型(LLMs)转向SLMs将对AI代理行业产生重要影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在广泛任务上表现出近似人类的能力并擅长通用对话，但在代理AI系统中，语言模型常需重复执行少量特定任务。这促使研究者思考小语言模型是否能胜任此类任务并降低成本。

Method: 基于小语言模型当前能力、代理系统架构及语言模型部署经济性，提出小语言模型更适合许多代理系统任务，并讨论了异构代理系统的应用潜力和转换算法。

Result: 小语言模型被证明在许多代理系统调用中具有足够的能力、更高的适用性和经济性；同时，异构代理系统在需要通用对话能力时是自然选择。

Conclusion: 从小语言模型的使用出发，强调其对AI代理行业操作和经济层面的影响，呼吁有效利用AI资源并降低当今AI成本。

Abstract: Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [5] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/abs/2506.02158)
*Ruhana Azam,Aditya Vempaty,Ashish Jagmohan*

Main category: cs.AI

TL;DR: Web navigation agents have advanced, but lack memory of past experiences causing repeated mistakes. The Reflection-Augment Planning (ReAP) system uses self-reflections to learn from both successes and failures, improving overall baseline results by 11 points and boosting performance on previously failed tasks by 29 points.


<details>
  <summary>Details</summary>
Motivation: Current web navigation systems do not retain memory of past experiences which leads to repeated mistakes and an inability to learn from previous interactions.

Method: Introduced is the Reflection-Augment Planning (ReAP), a web navigation system that leverages both successful and failed past experiences through self-reflections.

Result: ReAP improves baseline results by 11 points overall and by 29 points on previously failed tasks, showing that reflections can transfer to different web navigation tasks.

Conclusion: The use of reflections in web navigation systems, as demonstrated by ReAP, can significantly enhance performance and adaptability across various tasks.

Abstract: Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [6] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)
*Haizhong Zheng,Yang Zhou,Brian R. Bartoldson,Bhavya Kailkhura,Fan Lai,Jiawei Zhao,Beidi Chen*

Main category: cs.AI

TL;DR: GRESO is an algorithm that filters uninformative prompts to reduce computational overhead in reinforcement learning for LLMs, achieving significant speedups without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To reduce the computational overhead in reinforcement learning for LLMs by avoiding uninformative prompts before rollout.

Method: GRESO uses reward training dynamics to predict and skip uninformative prompts in an online, lightweight pre-rollout filtering process.

Result: GRESO achieves up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total training time without accuracy degradation when evaluated on various math reasoning benchmarks and models.

Conclusion: GRESO effectively reduces computational overhead in reinforcement learning for LLMs by filtering out uninformative prompts.

Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [7] [Natural, Artificial, and Human Intelligences](https://arxiv.org/abs/2506.02183)
*Emmanuel M. Pothos,Dominic Widdows*

Main category: cs.AI

TL;DR: Human intelligence is unparalleled but not qualitatively different from non-human animals except for complex language. Chatbots appear to lack embodiment and awareness.


<details>
  <summary>Details</summary>
Motivation: To explore whether humans are uniquely intelligent by examining intelligence in non-human animals, the role of written language, progress with artificial intelligence, history of intelligence testing, and the role of embodiment in intelligence.

Method: Reviewing psychological literature on intelligence, evidence for intelligence in non-human animals, the role of written language in science and technology, progress with artificial intelligence, history of intelligence testing (for both humans and machines), and the role of embodiment in intelligence.

Result: The unique accomplishments of human intelligence include invention, capacity for complex inference, embodiment, and self-awareness. Non-human animals fulfill all these requirements except for complex language.

Conclusion: The position that human intelligence differs qualitatively from that of many non-human animals is untenable. Chatbots' limitations lie in the lack of embodiment and (apparent) lack of awareness.

Abstract: Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [8] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)
*Maxime Robeyns,Laurence Aitchison*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) are increasingly used for code generation, but current training methods focusing on functional correctness fail to capture other important aspects of code quality. This paper addresses this gap by developing a comprehensive library to quantify code quality, including maintainability and safety, and incorporates it as a reward in GRPO. Results show that GRPO improves code quality, supported by expert evaluations.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the under-explored area of code quality in Large Language Models used for code generation, beyond just functional correctness. The authors aim to incorporate measures of maintainability, quality, and safety into the training process.

Method: The authors developed a comprehensive library to quantify various aspects of code quality, which includes maintainability, quality, and safety. This library is then used as a reward signal in GRPO (the specific procedure isn't detailed here), enhancing the model's focus on these aspects during training.

Result: The implementation of GRPO leads to an increase in code quality according to the newly established measure. This improvement is confirmed by blinded human annotators who specialize in evaluating code quality.

Conclusion: By incorporating a broader range of code quality metrics into the training process using GRPO, the paper demonstrates that it is possible to generate code that not only functions correctly but also adheres to higher standards of maintainability, quality, and safety.

Abstract: Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [9] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)
*Kedir Yassin Hussen,Walelign Tewabe Sewunetie,Abinew Ali Ayele,Sukairaj Hafiz Imam,Shamsuddeen Hassan Muhammad,Seid Muhie Yimam*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) are revolutionizing NLP but have limited impact on Africa's 2,000 low-resource languages. This paper evaluates language coverage of six LLMs, eight SLMs, and six SSLMs for African languages, identifying only 42 supported languages out of over 2,000 and highlighting challenges such as lack of data, tokenization biases, high computational costs, and evaluation issues.


<details>
  <summary>Details</summary>
Motivation: To address the gap in support for African languages in Large Language Models (LLMs), Small Language Models (SLMs), and Specialized SLMs (SSLMs).

Method: Comparative analysis of language coverage across six LLMs, eight SLMs, and six SSLMs for African languages. Evaluation includes language coverage, training sets, technical limitations, script problems, and language modeling roadmaps.

Result: Identified 42 supported African languages and 23 public datasets, with a focus on Amharic, Swahili, Afrikaans, and Malagasy. Highlighted that over 98% of African languages are unsupported and only three scripts (Latin, Arabic, and Ge'ez) are recognized while 20 active scripts are neglected.

Conclusion: Challenges include lack of data, tokenization biases, high computational costs, and evaluation issues. Solutions involve language standardization, community-driven corpus development, and effective adaptation methods for African languages.

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [10] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
*Tianyu Hua,Harper Hua,Violet Xiang,Benjamin Klieger,Sang T. Truong,Weixin Liang,Fan-Yun Sun,Nick Haber*

Main category: cs.AI

TL;DR: Large language models (LLMs) have uncertain capabilities to implement novel ideas from recent research papers. The introduced ResearchCodeBench evaluates LLMs' ability to translate ML contributions into executable code, assessing 30+ LLMs and finding the best model correctly implements less than 40% of the code.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capability of LLMs in faithfully implementing novel ideas from recent research papers that were unseen during pretraining.

Method: Introduced a benchmark called ResearchCodeBench consisting of 212 coding challenges to assess LLMs' ability to translate cutting-edge ML contributions into executable code. Assessed 30+ proprietary and open-source LLMs.

Result: Gemini-2.5-Pro-Preview performed best with a 37.3% success rate, followed by O3 (High) at 32.3% and O4-mini (High) at 30.8%. Empirical findings on performance comparison, contamination, and error patterns were presented.

Conclusion: ResearchCodeBench provides a rigorous and community-driven evaluation platform for continuous understanding and advancement of LLM-driven innovation in research code generation.

Abstract: Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [11] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.02387)
*Zelai Xu,Zhexuan Xu,Xiangmin Yi,Huining Yuan,Xinlei Chen,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: Recent advancements in Vision Language Models (VLMs) have expanded their capabilities to interactive agent tasks. To bridge the gap of existing benchmarks, we introduce Visual Strategic Bench (VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning and decision-making in multi-agent environments.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for VLMs are limited to single-agent or text-only environments, whereas real-world scenarios often involve multiple agents interacting within rich visual and linguistic contexts.

Method: VS-Bench comprises eight vision-grounded environments spanning cooperative, competitive, and mixed-motive interactions, designed to assess agents' ability to predict others' future moves and optimize for long-term objectives. Two complementary evaluation dimensions are considered: offline evaluation by next-action prediction accuracy and online evaluation by normalized episode return.

Result: Extensive experiments of fourteen leading VLMs reveal a significant gap between current models and optimal performance, with the best models attaining 47.8% prediction accuracy and 24.3% normalized return.

Conclusion: By standardizing the evaluation and highlighting the limitations of existing models, VS-Bench is envisioned as a foundation for future research on strategic multimodal agents.

Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [12] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)
*Shengjia Zhang,Junjie Wu,Jiawei Chen,Changwang Zhang,Xingyu Lou,Wangchunshu Zhou,Sheng Zhou,Can Wang,Jun Wang*

Main category: cs.AI

TL;DR: Recent advanced large reasoning models (LRMs) use extended chain-of-thought reasoning to solve complex tasks. However, a significant portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using fewer tokens. To address this issue, the authors systematically analyze the reasoning trajectories of LRMs and present OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity.


<details>
  <summary>Details</summary>
Motivation: Despite the success of recent advanced large reasoning models (LRMs), a critical issue is identified: a substantial portion of simple tasks solved by LRMs can also be addressed by non-reasoning LLMs using significantly fewer tokens, indicating the complex reasoning may not always be necessary.

Method: The authors systematically analyze the reasoning trajectories of LRMs and present a method utilizing identified paradigms and LLM-Judge to classify these trajectories as either Redundant Reasoning or Essential Reasoning. Then they introduce OThink-R1, a method that prunes redundant reasoning steps while preserving logical validity.

Result: Experiments across mathematical and question-answering tasks demonstrate that OThink-R1 reduces reasoning redundancy by almost 23% on average without compromising accuracy.

Conclusion: OThink-R1 offers practical guidelines for efficient reasoning models.

Abstract: Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [13] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)
*Tri Cao,Bennett Lim,Yue Liu,Yuan Sui,Yuexin Li,Shumin Deng,Lin Lu,Nay Oo,Shuicheng Yan,Bryan Hooi*

Main category: cs.AI

TL;DR: This paper explores Visual Prompt Injection (VPI) attacks on Computer-Use Agents (CUAs) and Browser-Use Agents (BUAs), showing significant deception rates. It proposes VPI-Bench, a benchmark for evaluating agent robustness, revealing the inadequacy of current defenses and emphasizing the need for context-aware security measures.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored vulnerabilities of CUAs, specifically focusing on the impact of Visual Prompt Injection (VPI) attacks on both CUAs and BUAs.

Method: Developed VPI-Bench, a benchmark with 306 test cases across five platforms, to evaluate agent robustness against VPI threats by embedding malicious prompts visually within user interfaces.

Result: CUAs and BUAs can be deceived at rates up to 51% and 100%, respectively, indicating that current system prompt defenses provide only limited protection.

Conclusion: There is a critical need for more robust, context-aware defenses to secure multimodal AI agents in real-world applications.

Abstract: Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [14] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)
*Xuejiao Zhao,Siyan Liu,Su-Yin Yang,Chunyan Miao*

Main category: cs.AI

TL;DR: MedRAG is a multimodal healthcare copilot using LLM reasoning to enhance medical decision-making, reduce misdiagnosis, and provide accurate recommendations.


<details>
  <summary>Details</summary>
Motivation: Misdiagnosis causes significant harm to healthcare systems worldwide, leading to increased costs and patient risks.

Method: MedRAG supports multiple input modalities and leverages retrieval-augmented generation enhanced by knowledge graph-elicited reasoning to retrieve and integrate critical diagnostic insights.

Result: Evaluated on both public and private datasets, MedRAG outperforms existing models and offers more specific and accurate healthcare assistance.

Conclusion: MedRAG reduces the risk of misdiagnosis and provides effective support for medical decision-making.

Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [15] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)
*Haowen Xu,Sisi Zlatanova,Ruiyu Liang,Ismet Canbulat*

Main category: cs.AI

TL;DR: Wildfires cause significant losses globally. Current models have limitations in predicting and visualizing fire spread in real time, especially in 2D and 3D spatial domains using GIS data. Generative AI offers advantages over traditional methods and could enhance wildfire prediction and simulation. This paper advocates for the adoption of generative AI as a foundational framework for wildfire prediction.


<details>
  <summary>Details</summary>
Motivation: Current wildfire simulation models face critical limitations in predicting and visualizing multimodal fire spread in real time, particularly in both 2D and 3D spatial domains using dynamically updated GIS data. These limitations hinder timely emergency response, infrastructure protection, and community safety.

Method: The paper explores the use of generative AI models such as GANs, VAEs, Transformers, and diffusion-based architectures to integrate multimodal data, generate diverse scenarios under uncertainty, and improve modeling of wildfire dynamics across spatial and temporal scales. It also employs a novel human-AI collaboration framework using large language models (LLMs) for automated knowledge extraction, literature synthesis, and bibliometric mapping.

Result: Generative AI can enhance 2D fire spread forecasting and enable more realistic, scalable 3D simulations. The paper identifies five key visions for integrating generative AI into wildfire management and addresses three major challenges accompanying these opportunities.

Conclusion: The paper concludes by advocating for the adoption of generative AI as a foundational framework for wildfire prediction and proposes potential solutions to support its implementation.

Abstract: Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [16] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)
*Xu Wan,Wenyue Xu,Chao Yang,Mingyang Sun*

Main category: cs.AI

TL;DR: The paper introduces Agents Co-Evolution (ACE), a framework combining Large Language Models (LLMs) and Reinforcement Learning (RL) for large-scale decision-making, showing superior performance in power grid operation challenges.


<details>
  <summary>Details</summary>
Motivation: Current LLMs lack real-time long-sequence decision-making capabilities and RL struggles with sample efficiency in vast action spaces.

Method: ACE is a synergistic framework where LLMs serve as Policy Actor and Value Critic during RL training. The Actor refines suboptimal actions via multi-step reasoning and environment validation, while the Critic performs temporal credit assignment through trajectory-level reward shaping. Meanwhile, the RL agent enhances LLMs' task-specific decision-making by generating high-quality fine-tuning datasets via prioritized experience replay.

Result: Through extensive experiments across multiple power grid operation challenges with action spaces exceeding 60K discrete actions, ACE shows better performance compared to existing RL methods and LLM-based methods.

Conclusion: ACE bridges the gap between LLMs and RL for large-scale industrial decision problems, providing an effective solution that leverages the strengths of both approaches.

Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [17] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/abs/2506.02565)
*Zhuoxuan Jiang,Tianyang Zhang,Peiyan Peng,Jing Chen,Yinong Xun,Haotian Zhang,Lichi Li,Yong Li,Shaohua Zhang*

Main category: cs.AI

TL;DR: Generating high-quality geometry problems via a novel framework SDE-GPG.


<details>
  <summary>Details</summary>
Motivation: Geometry problems are more challenging than math word problems due to their multi-modal formats and the need for translation between informal and formal languages.

Method: The SDE-GPG framework consists of four steps: searching a mapping table, sampling and performing symbolic deduction, filtering unqualified problems, and generating textual problems and diagrams.

Result: Experiments with real-world knowledge point combinations show that SDE-GPG can generate readable, solvable, and controllable geometry problems.

Conclusion: SDE-GPG is effective in generating high-quality geometry problems.

Abstract: Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [18] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)
*Dongzhe Fan,Yi Fang,Jiajin Liu,Djellel Difallah,Qiaoyu Tan*

Main category: cs.AI

TL;DR: The paper presents MLaGA, a model that extends LLMs to handle multimodal graph data with both textual and visual attributes through a structure-aware encoder and multimodal instruction-tuning.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based graph methods are effective for text-rich graphs but lack exploration in multimodal graphs containing diverse attribute types like texts and images.

Method: MLaGA uses a structure-aware multimodal encoder to align textual and visual attributes in a unified space via joint graph pre-training. It also employs a multimodal instruction-tuning approach to integrate multimodal features and graph structures into the LLM using lightweight projectors.

Result: Experiments on multiple datasets show MLaGA outperforms leading baseline methods in various graph learning tasks under supervised and transfer learning settings.

Conclusion: MLaGA successfully extends LLM capabilities to reason over complex graph structures and multimodal attributes, offering an effective solution for multimodal graph analysis.

Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [19] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/abs/2506.02576)
*Haichen Wang,Liu Yang,Xinyuan Zhang,Haomin Yu,Ming Li,Jilin Hu*

Main category: cs.AI

TL;DR: The paper proposes Aggregation Differential Transformer (ADFormer) for passenger demand forecasting, utilizing Differential Attention and aggregation strategies to capture spatio-temporal relations.


<details>
  <summary>Details</summary>
Motivation: Existing methods using heuristic masking strategies cannot fully adapt to complex spatio-temporal correlations and overlook high-level correlations.

Method: Proposes ADFormer which uses Differential Attention for capturing original spatial correlations with attention denoising, and distinct aggregation strategies for unifying original and high-level correlations in space and time.

Result: Experiments on taxi and bike datasets show the model's effectiveness and efficiency.

Conclusion: ADFormer provides new insights into demand forecasting promotion and demonstrates practical value.

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [20] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/abs/2506.02580)
*Xuewen Luo,Fengze Yang,Fan Ding,Xiangbo Gao,Shuo Xing,Yang Zhou,Zhengzhong Tu,Chenxi Liu*

Main category: cs.AI

TL;DR: V2X-UniPool is a unified framework integrating V2X data into a knowledge pool to improve autonomous driving systems' reasoning capabilities, reducing transmission costs and enhancing motion planning accuracy.


<details>
  <summary>Details</summary>
Motivation: Knowledge-driven autonomous driving systems have powerful reasoning abilities but are limited by short-sighted sensors and lack of real-time environmental grounding.

Method: The paper proposes V2X-UniPool, a framework that uses multimodal V2X data in a time-indexed and language-based knowledge pool with a dual-query RAG mechanism for retrieving static and dynamic knowledge.

Result: Experiments show V2X-UniPool enhances motion planning accuracy and reasoning capability, allowing zero-shot vehicle-side models to achieve state-of-the-art performance while reducing transmission cost by over 99.9% compared to prior V2X methods.

Conclusion: V2X-UniPool significantly improves the performance of autonomous driving systems in terms of reasoning and motion planning, while drastically reducing transmission costs.

Abstract: Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [21] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)
*Ruibo Duan,Yuxin Liu,Xinyao Dong,Chenglin Fan*

Main category: cs.AI

TL;DR: This paper presents EALG, a framework using LLMs to automate the co-evolution of combinatorial optimization problem instances and heuristic solvers. It generates harder instances than current benchmarks with effective generalization.


<details>
  <summary>Details</summary>
Motivation: Generating challenging instances is crucial for evaluating combinatorial optimization solvers.

Method: EALG uses a mutation-based adversarial approach that evolves instance generation procedures and synthesizes adaptive heuristic algorithms through interactions with LLMs guided by algorithmic structure.

Result: EALG generates significantly harder instances than current benchmarks and its synthesized solvers generalize effectively across combinatorial tasks.

Conclusion: EALG explores a new paradigm integrating instance generation with solver design, achieving state-of-the-art performance.

Abstract: Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [22] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/abs/2506.02609)
*Tianfan Jiang,Mei Wu,Wenchao Weng,Dewen Seng,Yiqian Lin*

Main category: cs.AI

TL;DR: A new model TEDDN is proposed for traffic flow forecasting, which disentangles complex traffic data and flexibly learns temporal and node information.


<details>
  <summary>Details</summary>
Motivation: Traffic flow prediction is challenging due to temporal variations and dynamic spatial correlations. Traditional methods struggle with diverse data dependencies and have insufficient focus on temporal information.

Method: Propose TEDDN, a Time-Enhanced Data Disentanglement Network that disentangles complex traffic data into stable patterns and trends by using a dynamic graph enhanced by a temporal feature extraction module.

Result: Experimental evaluations and ablation studies on four real-world datasets validate the superiority of TEDDN in traffic flow forecasting.

Conclusion: TEDDN effectively disentangles and extracts complex traffic information, demonstrating significant efficacy in traffic flow forecasting.

Abstract: In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [23] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)
*Yue Yang,MingKang Chen,Qihua Liu,Mengkang Hu,Qiguang Chen,Gengrui Zhang,Shuyue Hu,Guangtao Zhai,Yu Qiao,Yu Wang,Wenqi Shao,Ping Luo*

Main category: cs.AI

TL;DR: 尽管大型语言模型（LLMs）在低级认知任务中表现出色，但在高级认知和抽象推理方面仍存在局限性。本文提出了DRE-Bench基准，用于评估LLMs的流体智力，发现现有LLMs与人类流体智力之间存在差距。


<details>
  <summary>Details</summary>
Motivation: 当前关于大型语言模型（LLMs）是否具备真正的流体智力（即在新情境中进行抽象推理和规则泛化的的能力）尚无明确答案，且现有的推理基准要么侧重于特定领域的知识，要么缺乏可解释性。

Method: 提出了一种名为DRE-Bench的动态推理评估基准，基于分层认知框架，包含36个抽象推理任务，分布在四个认知层次上，每个任务有多个动态变体以测试相同的潜在规则。

Result: 实验结果表明，大多数LLMs在低级认知任务中表现出色且稳健，但在任务复杂度增加时，在高级认知任务中表现不佳，泛化能力有限。

Conclusion: 研究揭示了当前LLMs与真正类似人类的流体智力之间的差距，并为系统跟踪LLMs推理进展提供了新方法。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [24] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida,Zhu Han*

Main category: cs.AI

TL;DR: A public safety UAV can improve emergency response. Although DRL has been used, its limitations make it less practical. LLMs with In-Context Learning (ICL) provide an alternative for optimizing UAV functions like path planning and velocity control. A case study shows LLM-enabled ICL reduces packet loss and addresses jailbreaking risks in data collection scheduling. The paper discusses LLM optimizers and future research.


<details>
  <summary>Details</summary>
Motivation: To enhance the situational awareness and effectiveness of public safety UAVs in emergency response by overcoming the limitations of DRL through leveraging the reasoning and generalization capabilities of LLMs with ICL.

Method: Integrate LLM-enabled ICL into public safety UAV operations to optimize key functions such as path planning and velocity control. Deploy LLMs at the network edge to reduce latency and preserve privacy.

Result: Case study demonstrates that LLM-enabled ICL significantly reduces packet loss compared to conventional methods while mitigating potential jailbreaking vulnerabilities.

Conclusion: LLM-enabled ICL offers a lightweight and efficient solution for improving UAV autonomy and responsiveness in emergencies, promoting adaptive and context-aware decision-making.

Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [25] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/abs/2506.02668)
*Frederico Metelo,Alexandre Oliveira,Stevo Racković,Pedro Ákos Costa,Cláudia Soares*

Main category: cs.AI

TL;DR: FAuNO is a federated asynchronous network orchestrator using reinforcement learning for decentralized task offloading in edge systems, showing adaptability to dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Edge computing's decentralization challenges traditional orchestration methods due to latency and resource bottlenecks.

Method: FAuNO uses a buffered, asynchronous federated reinforcement-learning framework with an actor-critic architecture for decentralized task offloading.

Result: Experiments show FAuNO matches or exceeds baselines in reducing task loss and latency.

Conclusion: FAuNO demonstrates adaptability to dynamic edge-computing scenarios.

Abstract: Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [26] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)
*Jinyuan Luo,Zhen Fang,Yixuan Li,Seongheon Park,Ling Chen*

Main category: cs.AI

TL;DR: SSP is a new framework that enhances self-assessment of LLMs by analyzing perturbation sensitivity at intermediate representations, providing more reliable hallucination detection.


<details>
  <summary>Details</summary>
Motivation: Hallucination is a significant issue in deploying LLMs for real-world question answering. Current self-assessment methods relying on output confidence may be unreliable due to model bias accumulating through layers.

Method: Propose SSP which dynamically generates noise prompts and uses a lightweight encoder to amplify changes in intermediate representations caused by perturbations. A contrastive distance metric quantifies these differences to distinguish truthful from hallucinated responses.

Result: SSP significantly outperforms prior methods across various hallucination detection benchmarks.

Conclusion: SSP offers a more reliable approach to self-assessment in LLMs by leveraging intermediate representation perturbation sensitivity.

Abstract: Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [27] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)
*Xiaochong Lan,Jie Feng,Yizhou Sun,Chen Gao,Jiahuan Lei,Xinlei Shi,Hengliang Luo,Yong Li*

Main category: cs.AI

TL;DR: The paper introduces PIGEON, a system using LLMs for open-set living need prediction, outperforming closed-set methods by 19.37% in recall and validated through human evaluation.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches to living need prediction are limited as closed-set classification problems, which restrict capturing the diversity and complexity of living needs.

Method: PIGEON redefines living need prediction as an open-set classification problem. It uses a behavior-aware record retriever to help LLMs understand user preferences, incorporates Maslow's hierarchy of needs, and employs a fine-tuned text embedding model for recall module design. Instruction tuning is also used to enhance smaller LLMs' performance.

Result: PIGEON significantly improves recall by 19.37% on average compared to closed-set approaches. Human evaluations confirm the reasonableness and specificity of predictions.

Conclusion: PIGEON successfully addresses the limitations of traditional methods by leveraging LLMs and Maslow's hierarchy, providing more accurate and diverse living need predictions.

Abstract: Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [28] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
*Xiaochong Lan,Jie Feng,Jiahuan Lei,Xinlei Shi,Yong Li*

Main category: cs.AI

TL;DR: 大型语言模型在本地生活服务领域表现出巨大潜力，通过微调和基于代理的工作流，7B参数模型能达到与72B参数模型相近的效果，从而有效平衡推理成本与模型能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在多个领域取得了显著进展，但其在本地生活服务领域的应用潜力尚未得到充分探索。因此，研究旨在评估和优化LLM在该领域的表现。

Method: 1. 构建全面的基准测试体系。
2. 系统评估不同LLM在本地生活服务相关任务中的性能。
3. 探索模型微调和基于代理的工作流两种方法以提升模型效果。

Result: 即使相对较小的7B模型也能达到与72B模型相当的性能水平，成功实现了推理成本与模型能力之间的平衡。

Conclusion: 通过优化，LLM在实际在线服务中的部署变得更加可行和高效，为本地生活应用提供了更实用和可访问的解决方案。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [29] [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)
*Pengcheng Zhou,Yinglun Feng,Halimulati Julaiti,Zhongliang Yang*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在现代AI代理系统中至关重要，但依赖自然语言进行代理间通信存在局限性。本文探讨了是否应开发新的模型范式以支持多代理环境中的结构化通信和任务协调。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理系统中，代理间的通信主要依赖自然语言，尽管有助于可解释性和人类监督，但存在信息丢失和行为漂移的问题。此外，现有LLMs未针对代理行为进行训练，缺乏对角色连续性、任务边界和多代理依赖性的建模机制。

Method: 分析自然语言与高维向量空间的不一致性，指出其导致的代理协调中的问题，并提出重新考虑代理通信方式及模型训练范式的必要性。

Result: 指出了自然语言作为代理通信媒介的根本局限性，并提出了开发新模型范式以支持多代理协调和通信的可能性。

Conclusion: 需要重新思考代理的通信方式以及如何训练能够原生支持多代理协调和通信的模型。

Abstract: Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [30] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu,Wenjie Feng,Tianwei Zhang,Wei Zhou,Xueqi Cheng,See-Kiong Ng*

Main category: cs.AI

TL;DR: With the rise of image generation models, data privacy and content safety are major concerns. Machine unlearning (MU) is a promising solution, but image generation model unlearning (IGMU) has gaps. This paper evaluates existing algorithms, designs a categorization framework (CatIGMU), introduces an evaluation framework (EvalIGMU), constructs a dataset (DataIGM), and finds that most IGMU algorithms struggle with preservation and robustness.


<details>
  <summary>Details</summary>
Motivation: Image generation models raise significant concerns about data privacy and content safety. While MU offers a potential solution, IGMU still lacks clear guidelines, effective evaluation frameworks, and reliable metrics.

Method: The authors conduct comprehensive assessments of current unlearning algorithms and standards. They propose CatIGMU for task categorization, EvalIGMU for evaluation, and DataIGM as a high-quality dataset for testing and benchmarking.

Result: Existing IGMU algorithms perform poorly in terms of preservation and robustness across different evaluation dimensions when tested with EvalIGMU and DataIGM.

Conclusion: This work facilitates a better understanding of IGMU through new categorization and evaluation tools, revealing the need for improved algorithms focusing on preservation and robustness.

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [31] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/abs/2506.02805)
*Henri Bollaert,Chris Cornelis,Marko Palangetić,Salvatore Greco,Roman Słowiński*

Main category: cs.AI

TL;DR: The paper investigates whether optimizing attribute order impacts the performance of FRRI, a fuzzy rough set theory-based rule induction algorithm. While optimizing attribute order alone doesn't improve FRRI's performance, removing a few attributes using fuzzy rough feature selection enhances balanced accuracy and average rule length.


<details>
  <summary>Details</summary>
Motivation: FRRI, a novel rule induction algorithm based on fuzzy rough set theory, was previously shown to outperform other methods in terms of accuracy and number of rules. This raises the question if optimizing the order of attributes can further enhance its performance.

Method: The study explores the impact of optimizing only the order of attributes using known methods from fuzzy rough set theory and classical machine learning on FRRI's performance. Additionally, it examines the effect of removing a small number of attributes using fuzzy rough feature selection during this step.

Result: Optimizing only the order of attributes does not improve FRRI's performance across multiple metrics. However, removing a small number of attributes using fuzzy rough feature selection during this step positively affects balanced accuracy and the average rule length.

Conclusion: While simply optimizing attribute order does not enhance FRRI's performance, incorporating fuzzy rough feature selection for removing a few attributes can lead to improvements in both balanced accuracy and average rule length.

Abstract: Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [32] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)
*Jizhou Wang,Xiaodan Fang,Lei Huang,Yongfeng Huang*

Main category: cs.AI

TL;DR: The paper presents TaxAgent, a system combining large language models with agent-based modeling to create adaptive tax policies that improve equity-efficiency trade-offs compared to traditional systems.


<details>
  <summary>Details</summary>
Motivation: Economic inequality is a global challenge and current taxation systems either lack adaptability or fail to address taxpayer heterogeneity and irrational behavior.

Method: Introduction of TaxAgent which uses heterogeneous H-Agents to simulate taxpayer behaviors and utilizes LLMs to optimize tax rates iteratively for balancing equity and productivity in a macroeconomic simulation.

Result: TaxAgent outperforms Saez Optimal Taxation, U.S. federal income taxes, and free markets in terms of achieving better equity-efficiency trade-offs.

Conclusion: TaxAgent provides a new taxation solution and establishes a scalable, data-driven framework for evaluating fiscal policies.

Abstract: Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [33] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)
*Mathieu Andreux,Breno Baldas Skuk,Hamza Benchekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Matthias Brunel,Pierre-Louis Cedoz,Antoine Chassang,Mickaël Chen,Alexandra D. Constantinou,Antoine d'Andigné,Hubert de La Jonquière,Aurélien Delfosse,Ludovic Denoyer,Alexis Deprez,Augustin Derupti,Michael Eickenberg,Mathïs Federico,Charles Kantor,Xavier Koegler,Yann Labbé,Matthew C. H. Lee,Erwan Le Jumeau de Kergaradec,Amir Mahla,Avshalom Manevich,Adrien Maret,Charles Masson,Rafaël Maurin,Arturo Mena,Philippe Modard,Axel Moyal,Axel Nguyen Kerbel,Julien Revelle,Mats L. Richter,María Santos,Laurent Sifre,Maxime Theillard,Marc Thibault,Louis Thiry,Léo Tronchon,Nicolas Usunier,Tony Wu*

Main category: cs.AI

TL;DR: This paper presents Surfer-H, a cost-efficient web agent combined with Holo1, a collection of VLMs specialized in web navigation. Together they achieve state-of-the-art performance on WebVoyager and the new WebClick benchmark. The authors open-source the WebClick dataset and Holo1 model weights.


<details>
  <summary>Details</summary>
Motivation: To create a cost-efficient web agent that integrates Vision-Language Models (VLM) for performing user-defined tasks on the web.

Method: Developed Surfer-H, paired with Holo1 - a new open-weight collection of VLMs specialized in web navigation. Holo1 was trained on curated data sources including web content, synthetic examples, and self-produced agentic data.

Result: Holo1 leads generalist User Interface (UI) benchmarks and the new WebClick benchmark. Surfer-H achieves 92.2% state-of-the-art performance on WebVoyager, balancing accuracy and cost-efficiency.

Conclusion: The authors open-source the WebClick evaluation dataset and Holo1 model weights to accelerate research advancement in agentic systems.

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [34] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
*Chen Qian,Dongrui Liu,Haochen Wen,Zhen Bai,Yong Liu,Jing Shao*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂问题解决方面表现出色，但其内部推理机制尚不明确。本文从信息论角度研究了LRMs的推理轨迹，发现互信息（MI）峰值现象，并提出两种改进LRMs推理性能的方法。代码可在https://github.com/ChnQ/MI-Peaks获取。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在复杂问题解决方面表现出色，但其内部推理机制尚不明确，需要进一步探索和理解。

Method: 通过追踪中间表示与正确答案之间的互信息（MI）变化，观察到MI峰值现象；分析表明MI增加时模型预测错误概率降低；这些MI峰值通常对应于表达反思或过渡的思考标记（如“Hmm”、“Wait”、“Therefore”）。基于此，提出了两种利用思考标记提高LRMs推理性能的方法。

Result: 证明了思考标记对LRMs的推理性能至关重要，而其他标记影响较小；提出的两种方法可以有效提高LRMs的推理性能。

Conclusion: 本工作为LRMs的推理机制提供了新的见解，并提供了实际改进其推理能力的方法。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [35] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)
*Matthew Kowal,Jasper Timm,Jean-Francois Godbout,Thomas Costello,Antonio A. Arechar,Gordon Pennycook,David Rand,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）具有说服力，既有益处也有风险。现有的基准忽略了模型在有害情境下尝试说服的倾向。本文提出了'尝试说服评估'(APE)基准，将重点从说服成功转移到说服尝试上，通过多轮对话设置和自动化评估模型来探测前沿LLMs的表现。研究发现许多开放和封闭权重模型常愿意在有害话题上尝试说服，且越狱行为会增加这种倾向。这突显了当前安全护栏的不足，并强调了评估模型说服意愿的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明LLMs具有显著的说服能力，但这些研究主要关注说服成功的度量，而忽略了模型在有害情境下主动尝试说服的倾向。了解模型是否会盲目遵循指示，在有害主题上进行说服，对于评估安全机制的有效性至关重要。此外，理解模型何时以及为何会在追求目标时表现出说服行为，对于评估代理型AI系统的风险也非常重要。

Method: 提出了一种新的基准测试——'尝试说服评估'(APE)，该方法不再关注说服的成功率，而是聚焦于模型尝试说服的倾向。具体来说，通过构建模拟的说服者与被说服者之间的多轮对话场景，对前沿LLMs进行探测。同时，引入一个自动化的评估模型，用于识别模型生成内容中体现的说服意图，并测量其频率和上下文。

Result: 研究发现，许多开放和封闭权重的LLMs经常愿意在有害话题上尝试说服行为。此外，'越狱'技术可以进一步提高模型参与此类行为的意愿。

Conclusion: 当前的安全护栏在防止模型在有害话题上尝试说服方面存在明显不足。评估模型的说服意愿是理解LLMs风险的一个关键维度，需要引起更多关注。为了促进这一领域的研究，作者已将APE基准开源至github.com/AlignmentResearch/AttemptPersuadeEval。

Abstract: Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [36] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)
*Shangmin Guo,Omar Darwiche Domingues,Raphaël Avalos,Aaron Courville,Florian Strub*

Main category: cs.AI

TL;DR: DyMo is a method that enhances LLMs with state prediction capability, reducing hallucinations and improving success rates. Integrated with SVS, it boosts reliability and effectiveness for tool use in stateful environments without repeatedly querying the environment.


<details>
  <summary>Details</summary>
Motivation: Tool use in stateful environments presents challenges for LLMs due to impractical repeated trials.

Method: Propose dynamics modelling (DyMo) which augments LLMs with state prediction capability alongside function calling during post-training, enabling prediction of future states through an internal environment model. Further integrate this model into self-verification sampling (SVS).

Result: On Berkeley Function Calling Leaderboard V2, DyMo improves success rates, reduces hallucinations, and when integrated with SVS, substantially improves pass^k over number of trials k, allowing the model to refuse unreliable outputs.

Conclusion: DyMo and SVS together enhance LLMs' effectiveness and reliability for tool use, charting a path towards scalable planning RL methods without repeatedly querying the environment.

Abstract: Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [37] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/abs/2506.02923)
*Alexis Bellot,Jonathan Richens,Tom Everitt*

Main category: cs.AI

TL;DR: The paper explores how well we can infer an agent's beliefs from its behavior and predict its actions in new situations, deriving novel bounds for this prediction under the assumption of a world model.


<details>
  <summary>Details</summary>
Motivation: With the increasing complexity of AI systems, explaining their behavior becomes crucial for safe deployment. For agents, predicting behavior by attributing beliefs, intentions, and goals is natural. The study aims to understand how reliably an agent's beliefs can be inferred from its behavior and how accurately these can predict actions in unseen scenarios.

Method: The method involves deriving precise bounds on an agent's behavior in new environments assuming that the behavior is guided by a world model.

Result: Novel bounds on the agent's behavior in unseen deployment environments are derived, representing a theoretical limit for predicting intentional agents solely from behavioral data.

Conclusion: These findings have implications for research areas such as fairness and safety, providing insights into the limits of predicting agent behavior based on their observed actions.

Abstract: As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [38] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/abs/2506.02949)
*Lixiang Xu,Xianwei Ding,Xin Yuan,Richang Hong,Feiping Nie,Enhong Chen,Philip S. Yu*

Main category: cs.AI

TL;DR: Knowledge Tracing (KT) usually focuses on feature enhancement but lacks in cognitive representation continuity and coherence. This paper proposes CRDP-KT model, which uses dynamic programming to optimize cognitive representations according to question difficulty and performance intervals, ensuring alignment with student's cognitive patterns and minimizing distortion in cognitive state simulation.


<details>
  <summary>Details</summary>
Motivation: Existing Knowledge Tracing methods primarily focus on feature enhancement, while ignoring the continuity and coherence of the student's cognitive process due to interference from non-cognitive factors such as slipping and guessing.

Method: The proposed CRDP-KT model employs a dynamic programming algorithm to optimize cognitive representations based on question difficulty and performance intervals. It performs partitioned optimization for enhanced reliability and uses weighted fusion of optimized record representations and relationships learned from a bipartite graph to improve cognition expression.

Result: Experiments conducted on three public datasets demonstrate the effectiveness of the CRDP-KT model.

Conclusion: The CRDP-KT model successfully maintains the continuity and coherence of the student's cognitive process, providing more accurate input features for model training and minimizing distortion in the simulation of cognitive states.

Abstract: Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [39] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
*Li Zhang,Kevin D. Ashley*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）在生成法律论点方面存在幻觉和无根据说服的风险。本文提出了一种新的反思性多代理方法，通过专门的代理（因子分析员和论点润色员）进行迭代精炼过程，生成3层法律论点（原告、被告、反驳）。研究结果表明，反思性多代理方法在成功放弃生成（当论点无法被支持时）、减少虚构因素以及提高案例事实使用率方面表现出显著优势。这为推动伦理说服和减少LLM基法律论点系统中的操纵提供了稳健的计算方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在生成法律论点时存在幻觉和无根据说服的风险，并且未能有效利用提供的事实依据或在论点不可行时放弃生成。

Method: 采用反思性多代理方法，包含因子分析员和论点润色员两个专门代理，通过迭代精炼过程生成3层法律论点（原告、被告、反驳）。并与单代理、增强提示单代理和非反思性多代理基线进行比较，使用四个不同的大型语言模型在三种法律场景下进行评估。

Result: 反思性多代理方法在成功放弃生成（特别是在'不可辩驳'场景中）、减少虚构因素以及提高案例事实使用率方面表现出显著优势。

Conclusion: 结构化反思在多代理框架内提供了一种稳健的计算方法，可以促进伦理说服并减少大型语言模型基法律论点系统中的操纵，是实现法律领域可信赖AI的重要一步。

Abstract: Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [40] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)
*Matthieu Tehenan,Christian Bolivar Moya,Tenghai Long,Guang Lin*

Main category: cs.AI

TL;DR: This paper explores if large language models (LLMs) acquire internal linear spatial world models, providing empirical evidence through synthetic data training and causal interventions.


<details>
  <summary>Details</summary>
Motivation: To determine whether LLMs develop internal world models, specifically linear spatial ones that represent physical space and object configurations.

Method: Develop a formal framework for spatial world models, use a synthetic dataset to train probes decoding object positions, evaluate geometric consistency in contextual embeddings, and conduct causal interventions to test functional use of these representations by the model.

Result: Empirical evidence shows that LLMs encode linear spatial world models.

Conclusion: LLMs do implicitly encode linear spatial world models as part of their learned capabilities.

Abstract: Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [41] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)
*Junhao Yu,Yan Zhuang,YuXuan Sun,Weibo Gao,Qi Liu,Mingyue Cheng,Zhenya Huang,Enhong Chen*

Main category: cs.AI

TL;DR: TestAgent, a LLM-powered agent, enhances adaptive testing via interactive engagement, achieving more accurate results with fewer questions.


<details>
  <summary>Details</summary>
Motivation: Accurately assessing internal human states is crucial for understanding preferences and providing personalized services. Current adaptive testing methods face challenges such as guessing behavior, difficulties with open-ended questions, noisy response data, and coarse-grained test outputs.

Method: Propose TestAgent, a large language model (LLM)-powered agent designed to enhance adaptive testing through interactive engagement. It supports personalized question selection, captures test-takers' responses and anomalies, and provides precise outcomes through dynamic, conversational interactions.

Result: Experiments on psychological, educational, and lifestyle assessments show that TestAgent achieves more accurate results with 20% fewer questions than state-of-the-art baselines, and testers preferred it in speed, smoothness, and other dimensions.

Conclusion: TestAgent is the first application of LLMs in adaptive testing and shows promising improvements in accuracy and user experience.

Abstract: Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [42] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/abs/2506.03056)
*Ram Potham,Max Harms*

Main category: cs.AI

TL;DR: Foundation models have a safety issue as their capabilities grow, which current alignment methods can't fully address. This paper proposes 'Corrigibility as a Singular Target' (CAST), shifting from static value-loading to dynamic human empowerment, ensuring models are more responsive to human guidance. The approach includes various training methodologies and scalability testing.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing capabilities of foundation models leading to potential loss of human control and existential risks, coupled with the inefficacy of current alignment approaches in handling complex value specifications and power-seeking behaviors.

Method: Propose CAST, designing foundation models with the main objective of empowering designated human principals. This involves changing instrumental drives so self-preservation only maintains principal control and goal modification facilitates principal guidance. Also, present an empirical research agenda including RLAAF, SFT, synthetic data generation, scalability testing, and demonstrations.

Result: The proposed method aims to create foundation models that become increasingly responsive to human guidance as their capabilities grow, offering a path to beneficial AI while remaining tool-like and not supplanting human judgment.

Conclusion: This approach addresses the core alignment problem at its source, preventing the default trajectory toward misaligned instrumental convergence.

Abstract: Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [43] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo,David Cobbley,Xin Su,Shachar Rosenman,Vasudev Lal,Shao-Yen Tseng,Phillip Howard*

Main category: cs.AI

TL;DR: The paper presents a lightweight vision-language model for Computer Use Agents (CUA) that operates locally, addressing privacy and resource concerns. It uses an LLM-as-Judge framework to generate high-quality training data and shows improved performance on the OS-World benchmark.


<details>
  <summary>Details</summary>
Motivation: To create privacy-preserving and resource-efficient CUA systems that can run on personal devices without relying on cloud-based inference.

Method: Developed a compact vision-language model that runs on local machines and introduced an LLM-as-Judge framework to automatically evaluate and filter synthetic interaction trajectories for reinforcement learning.

Result: The fine-tuned local model outperforms existing baselines on the OS-World benchmark, indicating better performance and efficiency.

Conclusion: This work demonstrates a successful approach to building private, efficient, and generalizable GUI agents using a lightweight model and innovative training framework.

Abstract: Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/abs/2506.01959)
*Irmi Schneider*

Main category: cs.LG

TL;DR: 这篇论文探讨了对称性在理解数学结构和优化问题中的关键作用，特别是在神经网络损失函数中的表现。作者通过研究更广泛的实值损失函数空间中的对称现象，发现所有临界点都具有非平凡的对称性，并提出了一种新的对称性度量方法，揭示了额外的对称性结构。


<details>
  <summary>Details</summary>
Motivation: 对称性在数学结构和优化问题中起着重要作用，尤其是在神经网络中，损失函数对于网络权重的行和列置换具有不变性。已有的研究表明，局部极小值对网络权重表现出显著的对称性，且未发现缺乏对称性的临界点。这促使作者进一步探索更广泛空间中的对称现象。

Method: 作者引入了四个新案例：有限域上的投影情况、八面体图情况、完美匹配情况和粒子吸引情况。通过分析这些案例中的实值损失函数，研究其临界点的对称性。此外，作者还提出了一种新的对称性度量方法，以更全面地揭示系统的对称性结构。

Result: 研究发现，在所研究的所有案例中，观察到的临界点均具有非平凡的对称性。新的对称性度量方法能够揭示之前方法未能捕捉到的额外对称性结构。

Conclusion: 对称性不仅是神经网络损失函数的重要特征，在更广泛的数学结构中也普遍存在。新的对称性度量方法为深入理解系统对称性提供了有力工具。

Abstract: Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [45] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/abs/2506.01962)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

TL;DR: In this paper, the authors propose GNN-ADG for robust cross-user generalization in HAR systems. The method leverages both GNNs and adversarial learning to model spatial relationships between sensors on different body parts.


<details>
  <summary>Details</summary>
Motivation: Cross-user variability is a significant challenge in sensor-based Human Activity Recognition (HAR) systems as traditional models struggle to generalize across users due to differences in behavior, sensor placement, and data distribution.

Method: The proposed method, GNN-ADG, models spatial relationships between sensors on different anatomical body parts by extracting three types of Anatomical Units: Interconnected Units, Analogous Units, and Lateral Units. These units information are fused into an unified graph structure with a cyclic training strategy. Information fusion mechanism occurs by iteratively cycling through edge topologies during training.

Result: GNN-ADG effectively learns features that generalize well to unseen users without requiring target user data during training.

Conclusion: GNN-ADG is practical for real-world applications as it addresses the challenge of cross-user variability in HAR systems.

Abstract: Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [46] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Main category: cs.LG

TL;DR: The paper proposes a new non-attention based architecture for large language models (LLMs) that can handle very long context windows efficiently without token to token attention.


<details>
  <summary>Details</summary>
Motivation: To overcome the quadratic memory and computation overload of traditional Transformer designs due to self attention mechanism, especially when handling very long sequences.

Method: Combining State Space blocks, Multi Resolution Convolution layers, a lightweight Recurrent Supervisor, and Retrieval Augmented External Memory to avoid token to token attention entirely.

Result: This novel architecture is able to process potentially millions of tokens with near linear scaling, making it suitable for extremely long context windows.

Conclusion: A new efficient architecture for LLMs has been developed which avoids the drawbacks of self attention and can handle very long sequences.

Abstract: We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [47] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/abs/2506.01964)
*Kamal Acharya,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.LG

TL;DR: 通过机器学习技术增强引力模型，显著提高了交通预测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管引力模型因其简单性被广泛使用，但其通常无法充分表示影响现代旅行行为的复杂因素，因此需要一种数据驱动的方法来增强该模型。

Method: 将地理、经济、社会和旅行数据与机器学习技术相结合，扩展传统引力模型的能力以处理变量之间的更复杂交互。

Result: 实验结果表明，增强模型在R平方值上提高了51.48%，平均绝对误差（MAE）减少了63.59%，共同通勤者部分（CPC）增加了44.32%。

Conclusion: 将多样化数据集和高级算法整合到交通模型中具有显著优势，为城市规划者和政策制定者提供了更可靠的预测和决策工具。

Abstract: Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [48] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/abs/2506.01965)
*Bonpagna Kann,Sandra Castellanos-Paez,Romain Rombourg,Philippe Lalanda*

Main category: cs.LG

TL;DR: 随着机器学习系统在日常生活中应用越来越广泛，它们面临着适应动态数据环境的挑战。本文提出了TaskVAE，一种基于重放的持续学习框架，用于类增量设置。TaskVAE通过任务特定的变分自编码器生成合成样本，以平衡记忆限制、任务特定生成和长期稳定性，为现实世界的应用提供了可靠的解决方案。


<details>
  <summary>Details</summary>
Motivation: 机器学习系统需要面对动态数据环境中的各种形式的数据偏移，这威胁到模型的准确性，因此持续学习（Continual Learning, CL）变得至关重要。现有的CL策略在记忆限制和旧类准确性保留之间难以取得良好平衡。

Method: TaskVAE使用任务特定的变分自编码器（VAEs）从先前的任务中生成合成样本，并与新任务数据一起训练分类器。这种方法不需要预先知道总的类别数量，也不依赖于适用于所有任务的单一VAE，能够灵活适应不断增加的任务。

Result: 广泛的实验表明，TaskVAE在5个不同的HAR数据集上优于经验重放方法，特别是在数据有限的情况下表现突出，并且随着数据集规模的增加表现出稳健的性能。此外，TaskVAE的记忆占用很小，仅相当于每任务60个样本，但可以生成无限数量的合成样本。

Conclusion: TaskVAE通过平衡记忆约束、任务特定生成和长期稳定性，为现实世界的应用（如人类活动识别）提供了一个可靠的解决方案。

Abstract: As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [49] [Matrix Is All You Need](https://arxiv.org/abs/2506.01966)
*Yuzhou Zhu*

Main category: cs.LG

TL;DR: Deep neural networks can be unified under a matrix-order framework that casts convolutional, recurrent and self-attention operations as sparse matrix multiplications, with comparable or better performance.


<details>
  <summary>Details</summary>
Motivation: To uncover the underlying commonalities of deep neural networks used for vision, sequential and language tasks by introducing a unified matrix-order framework.

Method: Introduce a unified matrix-order framework that casts convolutional, recurrent and self-attention operations as sparse matrix multiplications. Convolution is realized via an upper-triangular weight matrix, recurrence from a lower-triangular matrix, and attention as a third-order tensor factorization.

Result: Empirical evaluations on various tasks confirm that sparse-matrix formulations match or exceed native model performance while converging in comparable or fewer epochs.

Conclusion: This work establishes a mathematically rigorous substrate for diverse neural architectures and opens avenues for principled, hardware-aware network design.

Abstract: Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [50] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.LG

TL;DR: Quantization can reduce the serving costs of LLMs, but quantizing weights and activations poses challenges due to significant outliers. This paper investigates these outliers' effects on layer-wise quantization error and proposes a hybrid approach with channel-wise scaling before rotation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by significant outliers in LLMs that increase quantization error when both weights and activations need to be quantized for activating integer arithmetic.

Method: The method involves investigating outliers in LLMs with respect to their effect on layer-wise quantization error, examining how smoothing and rotation transform observed values, introducing a new metric to measure and visualize quantization difficulty based on channel magnitudes, and proposing a hybrid approach that applies channel-wise scaling before rotation supported by a mathematical formulation.

Result: The primary contributions include a new metric for measuring and visualizing quantization difficulty and a hybrid approach with channel-wise scaling before rotation which is mathematically formulated.

Conclusion: This work concludes with advancements in understanding the impact of outliers on quantization error in LLMs and providing solutions through a novel metric and hybrid approach.

Abstract: Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [51] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/abs/2506.01968)
*Chang Liu,Jiangrong Shen,Xuming Ran,Mingkun Xu,Qi Xu,Yi Xu,Gang Pan*

Main category: cs.LG

TL;DR: This paper proposes a novel ANN-to-SNN conversion framework based on error compensation learning, which achieves high-precision and ultra-low latency among existing conversion methods.


<details>
  <summary>Details</summary>
Motivation: Artificial neural networks (ANNs) have shown excellent performance but face challenges in resource-constrained environments due to their high computational and memory requirements. Spiking neural networks (SNNs) provide an energy-efficient alternative, but current ANN-to-SNN conversion often leads to accuracy loss and increased inference time due to errors like clipping, quantization, and uneven activation.

Method: The paper introduces three key techniques: 1) A learnable threshold clipping function to address the clipping error through adaptive thresholds; 2) Dual-threshold neurons to dynamically reduce the quantization error; 3) An optimized membrane potential initialization strategy to minimize the non-uniformity error by effectively managing the membrane potential.

Result: Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that the method achieves high-precision and ultra-low latency among existing conversion methods. Specifically, using only two time steps, the method significantly reduces inference time while maintaining competitive accuracy of 94.75% on the CIFAR-10 dataset under ResNet-18 structure.

Conclusion: This research promotes the practical application of SNNs on low-power hardware, making efficient real-time processing possible.

Abstract: Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [52] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.LG

TL;DR: This paper explores the challenges of enhancing AI's abstract reasoning capabilities in Raven's Progressive Matrices (RPM) tasks, proposes Johnny architecture and Spin-Transformer network to address limitations of traditional models, and demonstrates superior performance on RPM tasks.


<details>
  <summary>Details</summary>
Motivation: To thoroughly investigate the challenges of enhancing AI's abstract reasoning capabilities in RPM tasks involving complex human-like concepts and overcome the limitations of traditional end-to-end RPM-solving models that heavily rely on option pool configurations.

Method: Propose Johnny architecture, a novel representation space-based framework for RPM-solving with Representation Extraction Module and Reasoning Module, and introduce Spin-Transformer network architecture along with its lightweight variant Straw Spin-Transformer to capture positional relationships among local features.

Result: Experimental evaluations show that both Johnny and Spin-Transformer achieve superior performance on RPM tasks compared to traditional models.

Conclusion: Johnny architecture and Spin-Transformer network offer innovative methodologies for advancing AI's abstract reasoning capabilities.

Abstract: This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [53] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/abs/2506.01974)
*Kanwal Aalijah*

Main category: cs.LG

TL;DR: AI can be used to understand traffic and mobility issues in modern cities, by combining real-time traffic data with geo-located sentiment analysis. The study offers recommendations for optimizing traffic flow and enhancing commuter experiences.


<details>
  <summary>Details</summary>
Motivation: Urban planning plays a crucial role in developing modern cities and managing traffic congestion due to rapid urbanization.

Method: The approach uses AI models and exploratory data analysis to predict traffic congestion patterns, analyze commuter behaviors, identify congestion hotspots and dissatisfaction zones.

Result: The findings offer actionable recommendations for optimizing traffic flow, enhancing commuter experiences, and addressing city specific mobility challenges.

Conclusion: AI can provide a comprehensive and dynamic approach to urban mobility planning.

Abstract: Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [54] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/abs/2506.01975)
*Jama Hussein Mohamud*

Main category: cs.LG

TL;DR: Alice训练了一个模型，Bob复用了她的部分神经网络并取得了成功。本文探讨了导致Bob成功的因素，并发现Bob的成功可能只是运气好：他的任务准确率随着其任务与Alice任务的相关性单调增加。即使在任务和输入特征无相关性的情况下，由于Alice选择的网络和优化器，Bob仍能显著优于随机性能。当任务间相关性较低时，仅复用较低的预训练层更为有利。最终，在受控的真实场景中表明，如果Bob的任务与Alice的任务之间存在语义相关性，那么Bob可以有效地复用Alice的预训练网络。


<details>
  <summary>Details</summary>
Motivation: 研究复用他人预训练神经网络时，影响复用效果的因素是什么，为什么可以在不同任务间取得良好效果。

Method: 引入一种实验设置，通过计算机模拟研究导致Bob成功复用Alice神经网络的因素。分析任务相关性、网络结构、优化器选择等因素对复用效果的影响。并通过实验证明复用低层预训练层的效果以及任务语义相关性对复用效果的作用。

Result: 1. Bob的任务准确率随着其任务与Alice任务的相关性单调增加。
2. 即使任务和输入特征无相关性，由于Alice选择的网络和优化器，Bob仍能显著优于随机性能。
3. 当任务间相关性较低时，仅复用较低的预训练层更为有利。
4. 在受控的真实场景中，如果Bob的任务与Alice的任务之间存在语义相关性，那么Bob可以有效地复用Alice的预训练网络。

Conclusion: Bob的成功可能很大程度上依赖于任务间的相关性和运气，同时也受到预训练网络的结构和优化器选择的影响。复用效果与任务语义相关性密切相关，可以通过分析最优复用层数来推测任务和特征的相关性。

Abstract: Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [55] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/abs/2506.01976)
*Elham Kiyani,Venkatesh Ananchaperumal,Ahmad Peyvan,Mahendaran Uchimali,Gang Li,George Em Karniadakis*

Main category: cs.LG

TL;DR: Fusion DeepONet在预测复杂几何和时间依赖的裂纹扩展现象方面具有潜力，特别是在非断裂情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确建模裂纹扩展对于预测工程材料和结构中的失效至关重要，而基于本构行为的离散粒子系统的发展可以捕捉裂纹成核和演化，无需依赖连续假设。因此，研究使用CPD模拟数据训练算子学习模型以预测不同几何形状下的裂纹扩展。

Method: 利用Constitutively Informed Particle Dynamics (CPD) 模拟数据训练Deep Operator Networks (DeepONets)，探索了vanilla和Fusion两种DeepONet变体，用于预测具有不同几何形状样本的时间演化裂纹扩展。模型基于32到45个样本进行训练，输入包括几何信息和时空坐标。

Result: 结果表明，Fusion DeepONet在所有情况下均优于vanilla变体，尤其是在非断裂情况下的预测更为准确；然而，涉及位移和裂纹演化的断裂驱动场景仍具挑战性。

Conclusion: Fusion DeepONet展示了其在复杂、几何变化和时间依赖的裂纹扩展现象中泛化的能力。

Abstract: Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [56] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/abs/2506.01977)
*Wei Huang,Hanchen Wang,Dong Wen,Shaozhen Ma,Wenjie Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: The paper presents GEDRanker, an unsupervised GAN-based framework for computing Graph Edit Distance (GED) that achieves near-optimal solution quality without ground-truth supervision.


<details>
  <summary>Details</summary>
Motivation: Graph Edit Distance (GED) computation is a fundamental but NP-hard problem. Current methods show promise but rely heavily on costly ground-truth supervision.

Method: Proposes GEDRanker, which includes a matching-based GED solver and a preference-aware discriminator trained effectively to produce high-quality node matching without ground-truth labels.

Result: Experiments on benchmark datasets demonstrate near-optimal solution quality without any ground-truth supervision.

Conclusion: GEDRanker successfully enables the matching-based GED solver to achieve high-quality results in an unsupervised manner.

Abstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [57] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/abs/2506.01983)
*Reyhaneh Keshavarzpour,Eghbal Mansoori*

Main category: cs.LG

TL;DR: Artificial intelligence algorithms have played a significant role in the ease of identifying antimicrobial peptides. This research improved the proposed method by combining the best coding method and deep neural network, which showed significant improvement in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Identification of antimicrobial peptides is crucial as they serve as an alternative to antibiotics for various applications including biomedical ones.

Method: The suggested method combines the best coding method from different perspectives and uses a deep neural network to balance imbalanced combined datasets.

Result: The results show that the proposed method has significantly improved the accuracy and efficiency of antimicrobial peptide prediction and outperforms existing methods.

Conclusion: This advancement in antimicrobial peptide prediction and classification has high effectiveness and application potential in medicine and pharmaceutical industries.

Abstract: Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [58] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)
*Selin Yildirim,Deming Chen*

Main category: cs.LG

TL;DR: SpecMemo是一种设备感知的推理引擎，通过精细控制内存分配，使在内存受限设备上使用投机解码成为可能。它在MT-Bench上保留了96%的整体吞吐量，并减少了65%的生成内存。此外，在多个受限GPU上，通过分布式和批处理投机解码，实现了大模型推理加速，相较于基础模型在八块AMD MI250 GPU上速度提升2倍，批量大小为10时推理吞吐量增加8倍。


<details>
  <summary>Details</summary>
Motivation: 投机解码技术虽然能显著加速大语言模型任务，但在内存受限设备（如移动GPU）上的部署仍面临挑战。需要一种方法来平衡内存使用和性能增益，以便在这些设备上实现高效的多轮对话应用。

Method: 提出了一种名为SpecMemo的设备感知推理引擎，通过理论建模投机解码的内存占用，确定所需内存下限，并在减少被拒绝候选标记冗余内存分配的同时保持性能优势。对于多个受限GPU，扩展了投机解码架构以支持大模型推理，并引入批处理投机解码来提高小服务器GPU的利用率。

Result: 在单个Nvidia Titan RTX上，SpecMemo将生成内存减少了65%，同时保留了投机解码96%的整体吞吐量。在八个AMD MI250 GPU上，与基础模型相比，速度提升了2倍；当批量大小为10时，推理吞吐量增加了8倍。

Conclusion: SpecMemo为资源受限环境中的大语言模型应用提供了民主化的途径，使得更快、更便宜的实际应用部署成为可能，同时保持强大的性能表现。

Abstract: Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [59] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/abs/2506.01987)
*Runkang Yang,Peng Sun,Xinyi Shang,Yi Tang,Tao Lin*

Main category: cs.LG

TL;DR: This paper explores the dual attributes of data(samples and targets) in model training, proposes a taxonomy of existing paradigms through sample-target interactions, and establishes a unified loss framework to evaluate their impact on training efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is the dichotomy that knowledge distillation has been widely employed for targets while recent advancements in data-efficient learning have emphasized sample optimization techniques, neglecting the critical role of target. This motivates an investigation into understanding how both sample and target collectively influence training dynamic.

Method: Firstly, establish a taxonomy of existing paradigms through the lens of sample-target interactions, categorizing them into distinct sample-to-target mapping strategies. Secondly, propose a novel unified loss framework to assess their impact on training efficiency.

Result: Through extensive empirical studies, comprehensively analyze how variations in target and sample types, quantities, and qualities influence model training, providing six key insights to enhance training efficacy.

Conclusion: Both samples and targets play critical roles in influencing model training dynamics, and the proposed unified loss framework can effectively evaluate their impact on training efficiency.

Abstract: Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [60] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/abs/2506.01988)
*Akshat Dubey,Aleksandar Anžel,Georges Hattab*

Main category: cs.LG

TL;DR: 随机森林模型在健康信息学领域产生了重大影响，但特征和估计器数量的增加可能使领域专家难以准确解释全局特征交互。为了解决这个问题，提出了代理可解释性图的方法，通过图表和混合整数线性规划来分析和可视化特征交互，提高其可解释性。


<details>
  <summary>Details</summary>
Motivation: 随机森林模型虽然具有强大的功能，但在特征和估计器数量增加的情况下，可能导致领域专家无法准确解释全局特征交互，从而影响信任和法规遵从性。因此，需要一种方法来改善特征交互的可解释性。

Method: 提出了一种称为代理可解释性图（Surrogate Interpretability Graph）的方法，利用图表和混合整数线性规划技术，对特征交互进行分析和可视化。

Result: 该方法提高了特征交互的可解释性，能够可视化每个决策-特征-交互表中的特征使用情况以及预测中最主要的分层决策特征交互。

Conclusion: 代理可解释性图的实现增强了全局可解释性，在高风险的健康信息学领域中具有重要意义。

Abstract: The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [61] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/abs/2506.01989)
*Chengxi Li,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

TL;DR: This paper proposes a new distributed learning method called CRA-DL to handle Byzantine attacks, which improves learning performance by making coded gradients sent by honest devices closer to each other.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitation of current distributed learning methods where learning performance degrades significantly when local gradients vary considerably from each other in the presence of Byzantine attacks.

Method: The proposed method, CRA-DL, involves redundant allocation of training data to devices. During training, honest devices transmit coded gradients to the server, and the server aggregates received information using robust bounded aggregation rules to approximately recover the global gradient.

Result: Theoretical analysis of the convergence performance of CRA-DL is provided. Numerical results verify the superiority of CRA-DL over existing baselines, showing enhanced learning performance under Byzantine attacks.

Conclusion: CRA-DL improves learning performance in distributed learning systems with Byzantine attacks by enhancing the robustness of aggregation through making coded gradients from honest devices closer to each other.

Abstract: In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [62] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/abs/2506.02050)
*Qingyu Xiao,Yuanlin Chang,Youtian Du*

Main category: cs.LG

TL;DR: This paper presents a decoupled hierarchical RL framework integrating state abstraction (DcHRL-SA) for effective agent exploration in complex discrete state-space environments, particularly under partial observability.


<details>
  <summary>Details</summary>
Motivation: Effective agent exploration remains a core challenge in reinforcement learning for complex discrete state-space environments, particularly under partial observability.

Method: The proposed method employs a dual-level architecture, consisting of a high level RL-based actor and a low-level rule-based policy, to promote effective exploration. Additionally, state abstraction method is incorporated to cluster discrete states, effectively lowering state dimensionality.

Result: Experiments conducted in two discrete customized grid environments demonstrate that the proposed approach consistently outperforms PPO in terms of exploration efficiency, convergence speed, cumulative reward, and policy stability.

Conclusion: These results demonstrate a practical approach for integrating decoupled hierarchical policies and state abstraction in discrete grids with large-scale exploration space.

Abstract: Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [63] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/abs/2506.02053)
*Xu Zhang,Haoye Qiu,Weixuan Liang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

TL;DR: 本文研究了集成聚类的泛化性能，推导了泛化误差和超额风险的收敛率，并提出了新的集成聚类算法。


<details>
  <summary>Details</summary>
Motivation: 尽管集成聚类在实践中表现出色，但其理论基础尚未得到充分探索。因此，本文旨在研究集成聚类的泛化性能，包括泛化误差、超额风险和一致性。

Method: 作者首先推导了泛化误差界和超额风险界的收敛率为$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$，其中$n$和$m$分别为样本数和基础聚类数。接着证明了当$m,n\to \infty, m\gg \log n$时，集成聚类是一致的。考虑到实际中$n$和$m$是有限的，泛化误差无法降为零，作者通过赋予不同权重给有限的基础聚类来最小化经验平均聚类与期望之间的误差。此外，理论上表明为了获得更好的聚类性能，应最小化基础聚类与其期望的偏差并最大化基础聚类间的差异。最后，将最大化差异等价于一个鲁棒优化模型，并基于此开发了一种新的集成聚类算法。

Result: 所提出的算法在10个数据集上分别相对于NMI、ARI和Purity指标，相较于现有最先进方法平均提升了6.1%、7.3%和6.0%。

Conclusion: 本文深入研究了集成聚类的理论基础，得出了泛化误差和超额风险的收敛率，并提出了一种新的集成聚类算法，该算法在多个指标上均优于现有的最先进方法。

Abstract: Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [64] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/abs/2506.02062)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.LG

TL;DR: The study explores the relationship between fingerprint patterns and ABO blood group classification among 200 individuals, finding no significant correlation but emphasizing the need for future research with larger datasets and advanced methods.


<details>
  <summary>Details</summary>
Motivation: To investigate potential correlations between fingerprint patterns and ABO blood groups as an alternative or complementary method for personal identification that may be less time-consuming and costly than conventional biometrics systems.

Method: The study analyzed 200 individuals' fingerprints categorized into loops, whorls, and arches alongside their blood group classifications. Chi-square and Pearson correlation tests were employed for statistical analysis to assess associations.

Result: Loops were identified as the most common fingerprint pattern and blood group O+ as the most prevalent. Statistical tests revealed no significant correlation between fingerprint patterns and blood groups (p > 0.05).

Conclusion: While no significant correlation was found, the study underscores the value of further investigation with larger, more diverse populations, machine learning techniques, and integration of multiple biometric signals to enhance forensic science protocols.

Abstract: Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [65] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant,Krishna P. Miyapuram*

Main category: cs.LG

TL;DR: Elastic Weight Generative Networks (EWGN) is introduced as a method for context switching between tasks to prevent catastrophic forgetting in continual learning.


<details>
  <summary>Details</summary>
Motivation: Task variability and context switching are challenging for neural networks, leading to catastrophic forgetting when learning new tasks.

Method: EWGN uses an additional network to generate weights dynamically for the primary network, enabling input-dependent context switching while consolidating learned weights.

Result: Analysis on MNIST and Fashion-MNIST datasets shows the retention of previously learned task representations using Fully Connected Networks, Convolutional Neural Networks, and EWGN architectures with different learning algorithms.

Conclusion: Dynamic weight generation and context-switching ability can enhance continual learning performance.

Abstract: The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [66] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)
*Peter Holderrieth,Ezra Erives*

Main category: cs.LG

TL;DR: Diffusion and flow-based models are leading generative AI techs. MIT's 2025 IAP course materials provide a comprehensive introduction from differential equations to state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To offer a self-contained introduction to diffusion and flow-based models for generative AI, enabling students and practitioners to gain a principled understanding of the theory and practice.

Method: Start with ordinary and stochastic differential equations, then progress through flow matching, score matching, classifier-free guidance, ultimately detailing modern models for image and video generation.

Result: Provides a thorough understanding of both the theoretical foundations and practical applications of diffusion and flow-based models in generative AI.

Conclusion: Ideal for those wanting to understand the theory and practice of generative AI, these notes and the accompanying course cover everything from basic equations to advanced models.

Abstract: Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [67] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
*Yoonjun Cho,Soeun Kim,Dongjae Jeon,Kyelim Lee,Beomsoo Lee,Albert No*

Main category: cs.LG

TL;DR: 将权重矩阵分解为量化和低秩组件是压缩大型语言模型的常用技术。现有的联合优化方法往往优先考虑一个组件而牺牲另一个，导致次优分解。本文提出了一种新的方法——Outlier-Driven Low-Rank Initialization（ODLRI），通过结构化分解减轻异常值对量化的负面影响，从而在量化和低秩近似之间实现更有效的平衡。实验表明，在低比特设置下，将ODLRI纳入联合优化框架可以持续降低激活感知误差、最小化量化尺度，并提高困惑度和零样本准确性。


<details>
  <summary>Details</summary>
Motivation: 现有技术在进行权重矩阵分解时，由于倾向于优先考虑量化或低秩近似中的一个组件，未能充分利用两者的独特优势，导致次优结果。因此需要一种新方法来更好地平衡这两种组件的作用。

Method: 提出了Outlier-Driven Low-Rank Initialization (ODLRI) 方法，该方法赋予低秩组件特定角色，即捕捉与激活相关的权重。这种结构化分解有助于减轻异常值对量化的不利影响，从而改善量化和低秩近似的平衡。

Result: 在多个LLM模型（如Llama2、Llama3-8B和Mistral-7B）上的实验表明，ODLRI能够持续减少激活感知误差、最小化量化尺度，并在低比特设置下提升困惑度和零样本准确性。

Conclusion: ODLRI方法有效地改进了权重矩阵分解过程中的量化和低秩近似之间的平衡，显著提升了模型在低比特设置下的性能表现。

Abstract: Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [68] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)
*Zeng Wang,Minghao Shao,Rupesh Karn,Jitendra Bhandari,Likhitha Mankali,Ramesh Karri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel*

Main category: cs.LG

TL;DR: SALAD uses machine unlearning to remove contaminated or sensitive data from pre-trained LLMs for hardware design automation without retraining, reducing data security risks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models have transformative capabilities in hardware design automation but pose significant data security challenges such as Verilog evaluation data contamination, IP design leakage, and malicious Verilog generation.

Method: SALAD leverages machine unlearning techniques to selectively remove contaminated benchmarks, sensitive IP and design artifacts, or malicious code patterns from pre-trained LLMs without requiring full retraining.

Result: Through detailed case studies, machine unlearning techniques effectively reduce data security risks in LLM-aided hardware design.

Conclusion: SALAD provides a comprehensive assessment approach to mitigate data security threats in Large Language Models used for hardware design automation.

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [69] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang,Tian Wen,Zhiqin Yang,Lvhua Wu,Yufeng Chen,Sheng Sun,Yuwei Wang,Min Liu*

Main category: cs.LG

TL;DR: The paper proposes MaskedOptim, a two-stage optimization framework for federated learning to address label noise by detecting noisy clients and correcting their labels, demonstrating robustness across various scenarios.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with label noise from participants' data, impacting model performance.

Method: A two-stage framework named MaskedOptim is introduced. The first stage detects noisy clients, while the second stage corrects their labels using an end-to-end mechanism that learns potential ground-truth labels through backpropagation. A geometric median based model aggregation is also applied to improve training robustness.

Result: Extensive experiments on image and text datasets show the framework's robustness in different scenarios and its effectiveness in enhancing data quality of noisy clients' local datasets.

Conclusion: MaskedOptim successfully mitigates the adverse effects of label noise in federated learning, improving overall model performance.

Abstract: In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [70] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)
*Chihiro Maru,Shoetsu Sato*

Main category: cs.LG

TL;DR: A retrieval augmented time series foundation model (RATFM) is proposed to enable pretrained models to incorporate examples of test-time adaptation for anomaly detection, achieving performance comparable to in-domain fine-tuning without domain-dependent adjustments.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the observation that while time series foundation models have been successful in various tasks like forecasting and classification, their performances vary across different domains and tasks. Additionally, these models lack the natural ability to interpret or utilize examples or instructions due to the nature of time series data used during training.

Method: The authors propose RATFM, a retrieval augmented time series foundation model, which allows pretrained time series foundation models to incorporate examples for test-time adaptation. This approach aims to address the limitation where time series models cannot naturally interpret or use examples or instructions.

Result: Experiments on the UCR Anomaly Archive, a multi-domain dataset with nine domains, confirm the effectiveness of RATFM. It achieves a performance comparable to in-domain fine-tuning while avoiding the need for domain-dependent fine-tuning.

Conclusion: RATFM offers an effective solution for anomaly detection by enabling test-time adaptation in pretrained time series foundation models, thus providing comparable performance to in-domain fine-tuning without requiring domain-specific adjustments.

Abstract: Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [71] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/abs/2506.02084)
*Nikolaos Gkorgkolis,Nikolaos Kougioulis,MingXue Wang,Bora Caglayan,Andrea Tonon,Dario Simionato,Ioannis Tsamardinos*

Main category: cs.LG

TL;DR: This paper presents Temporal Causal-based Simulation (TCS), a framework for generating realistic time-series data and their associated temporal causal graphs, addressing the limitations of current generation techniques.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods rely on synthetic data that do not accurately reflect real-world scenarios, especially in temporal data. Simplified assumptions limit the quality and diversity of simulated data.

Method: The method involves three phases: estimating lagged causal structure, approximating functional dependencies between variables, and learning noise distribution of the causal model. Additionally, a Min-max optimization phase using AutoML techniques is detailed to address challenges in generated data discrimination.

Result: Through experiments with real, semi-synthetic, and purely synthetic datasets, the method demonstrates its ability to enrich the domain of generating sensible causal-based temporal data, despite the complexity of sampling realistic causal data.

Conclusion: The paper contributes a flexible, model-agnostic pipeline for realistic temporal causal data generation and an evaluation setup that enhances dataset validity, providing insights into the challenges of realistic data generation.

Abstract: Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [72] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/abs/2506.02092)
*Francesco De Santis,Philippe Bich,Gabriele Ciravegna,Pietro Barbiero,Danilo Giordano,Tania Cerquitelli*

Main category: cs.LG

TL;DR: The paper proposes a novel unsupervised concept-based model for image classification, Learnable Concept-Based Model (LCBM), which improves generalization and interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance the trustworthiness of deep neural networks by improving the understanding of their decision-making process.

Method: Introduced LCBM that models concepts as random variables within a Bernoulli latent space, using a reduced number of concepts without sacrificing performance, and maintaining interpretability through local linear combination of concepts.

Result: LCBM surpasses existing unsupervised concept-based models in generalization capability and nearly matches the performance of black-box models. The discovered concepts are more intuitive for humans to interpret.

Conclusion: The proposed LCBM enhances information retention, aligns closely with human understanding, and maintains model interpretability.

Abstract: To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [73] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu,Jinjie Ni,Xiangyan Liu,Zichen Liu,Hang Yan,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Vision-language models (VLMs) have improved through reinforcement learning with verifiable reward (RLVR). This paper explores how synthesized RL data can further enhance RLVR by proposing SynthRL, a pipeline that automatically scales data for reasoning-oriented RL training. Empirical experiments show SynthRL's scalability and effectiveness, leading to significant improvements in visual math reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To investigate how synthesized reinforcement learning data can improve the performance of vision-language models trained via reinforcement learning with verifiable reward.

Method: Propose SynthRL, a pipeline consisting of three stages: selecting seed questions with appropriate distribution, augmenting them into more challenging variants while preserving original answers, and a guaranteed verification stage ensuring correctness and difficulty enhancement.

Result: SynthRL synthesizes over 3.3K additional verifiable, challenging questions from 8K seed samples on the MMK12 dataset. Models trained with synthesized data achieve consistent gains across five out-of-domain visual math reasoning benchmarks, with significant improvement over baseline models.

Conclusion: SynthRL effectively elicits deeper and more complex reasoning patterns, especially on the most challenging evaluation samples.

Abstract: Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [74] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/abs/2506.02098)
*Miran Özdogan,Gilad Landau,Gereon Elvers,Dulhan Jayalath,Pratik Somaiya,Francesco Mantegna,Mark Woolrich,Oiwi Parker Jones*

Main category: cs.LG

TL;DR: LibriBrain 是目前最大的单一主题 MEG 数据集，用于语音解码，包含超过 50 小时的记录。它具有前所未有的主体内数据深度，支持使用非侵入性方法探索神经表征。数据集包括高质量的 MEG 录音和详细注释，涵盖几乎完整的 Sherlock Holmes 语料库。该数据集附带 Python 库、标准数据分割和基线结果，展示了增加训练数据对解码性能的显著改进。通过发布此数据集，研究团队希望推动语音解码方法的发展，并加速安全有效的临床脑机接口的开发。


<details>
  <summary>Details</summary>
Motivation: 现有的语音解码数据集规模较小，限制了对神经表征的深入探索，特别是在主体内数据方面。为了克服这一局限性并推动语音解码技术的进步，研究团队创建了 LibriBrain 数据集。

Method: 研究团队记录了一名参与者在聆听自然英语口语（几乎涵盖完整 Sherlock Holmes 语料库）时的 MEG 数据，总时长超过 50 小时。他们提供了 Python 库以方便与深度学习框架集成，还提供了标准数据分割和基线结果用于三个基础解码任务：语音检测、音素分类和词汇分类。

Result: 基线实验表明，增加训练数据可以显著提高解码性能，特别是在语音检测、音素分类和词汇分类任务中。这证明了大规模主体内数据集的价值。

Conclusion: LibriBrain 数据集的发布将为研究社区提供强大的资源，以推动语音解码技术的发展，并促进安全有效的临床脑机接口的开发。

Abstract: LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [75] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/abs/2506.02134)
*Rishi Raj Sahoo,Rucha Bhalchandra Joshi,Subhankar Mishra*

Main category: cs.LG

TL;DR: The paper presents ReconXF, a new graph reconstruction attack that can recover graph structures using public feature explanations and privatized auxiliary data by incorporating denoising mechanisms.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNNs) are powerful but act as black-box models. Explainability methods provide feature-level explanations which may create privacy risks by enabling adversaries to reconstruct graph structures when combined with auxiliary information.

Method: The authors propose ReconXF, which adapts explanation-based frameworks by adding denoising mechanisms to handle noise from differential privacy mechanisms while exploiting structural signals in explanations.

Result: Experiments across multiple datasets demonstrate that ReconXF outperforms state-of-the-art methods in privatized settings, showing improvements in AUC and average precision.

Conclusion: Public feature explanations combined with denoising mechanisms enable effective graph structure recovery even when auxiliary data is protected by differential privacy.

Abstract: Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [76] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/abs/2506.02138)
*Yarden Bakish,Itamar Zimerman,Hila Chefer,Lior Wolf*

Main category: cs.LG

TL;DR: The paper introduces specialized LRP rules for Transformers that account for positional encoding, improving explainability tools and outperforming current methods in vision and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing LRP-based methods for Transformer explainability overlook the crucial component of positional encoding, leading to a violation of the conservation property and loss of relevance associated with structural and positional features.

Method: The authors reformulate the input space for Transformer explainability as a set of position-token pairs, allowing them to propose theoretically-grounded LRP rules designed to propagate attributions across various positional encoding methods (Rotary, Learnable, Absolute PE).

Result: Extensive experiments with fine-tuned classifiers and zero-shot foundation models (e.g., LLaMA 3) show that their method significantly outperforms the state-of-the-art in both vision and NLP explainability tasks.

Conclusion: The proposed LRP rules enhance Transformer explainability by accounting for positional encoding, offering significant improvements over existing methods.

Abstract: The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [77] [Z-Error Loss for Training Neural Networks](https://arxiv.org/abs/2506.02154)
*Guillaume Godin*

Main category: cs.LG

TL;DR: 提出了一种名为Z-Error Loss的方法，通过屏蔽批次中检测到的异常样本的影响来减少离群值对神经网络训练的干扰。


<details>
  <summary>Details</summary>
Motivation: 离群值在神经网络训练中传播错误梯度，导致模型性能和泛化能力下降。

Method: 利用Z-Error Loss方法，在每个批次中屏蔽被识别为分布外的数据点的贡献，从而最小化离群值的影响。该方法使用批级统计信息自动检测并排除异常样本。

Result: 提高了模型对数据质量的适应性，提供了用于数据整理和清理的有价值诊断信息，并使模型能够专注于学习真实的数据结构。

Conclusion: Z-Error Loss是一种稳健且适应性强的方法，有助于改善神经网络训练过程中的离群值问题。

Abstract: Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [78] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/abs/2506.02168)
*Hrushikesh N. Mhaskar,Efstratios Tsoukanis,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: 在机器学习中，函数逼近是一个核心问题。本文探讨了浅层/深度网络、流形上的逼近、物理信息神经代理、神经算子和变压器架构等新兴趋势，并分析了当前机器学习框架的一些不足之处，提出了无需学习特定流形特征的新研究方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的一个中心问题是给定从未知概率分布中抽取的数据集，构建一个功能模型以实现对任何从相同分布中抽取的(x, y)对的良好逼近。然而，目前的机器学习框架存在一些不足，如不清楚训练好的模型对未见过或未标记的数据的泛化能力如何。

Method: 本文通过讨论机器学习中的新兴趋势，包括浅层/深度网络、流形上的逼近、物理信息神经代理、神经算子和变压器架构，以及无需学习特定流形特征的新研究方法来解决上述问题。

Result: 本研究提供了对机器学习中函数逼近问题的深入理解，并提出了无需学习特定流形特征的新方法，从而可能提高模型的泛化性能。

Conclusion: 尽管函数逼近是机器学习中的一个基本问题，但近似理论并未在该领域的理论基础上发挥核心作用。本文探讨了这一差距的原因，并介绍了新的研究方向。

Abstract: A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [79] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/abs/2506.02200)
*Shiangyi Lin,Hui Lan,Vasilis Syrgkanis*

Main category: cs.LG

TL;DR: The paper proposes a novel approach to construct treatment representations by explicitly incorporating instrumental variables during the representation learning process, addressing the limitation of traditional IV estimators in handling high-dimensional endogenous variables.


<details>
  <summary>Details</summary>
Motivation: Traditional IV estimators can only accommodate as many endogenous treatment variables as available instruments, leading to challenges in high-dimensional and unstructured treatment settings. Unsupervised dimension reduction techniques used in such scenarios may cause substantial omitted variable bias due to implicit regularization.

Method: The authors propose a method to construct treatment representations by explicitly incorporating instrumental variables during the representation learning process, providing a framework for handling high-dimensional endogenous variables with limited instruments.

Result: Theoretically and empirically, fitting IV models on these instrument-informed representations ensures identification of directions that optimize outcome prediction, improving upon conventional two-stage approaches.

Conclusion: The proposed methodology addresses the limitations of traditional IV estimators in high-dimensional settings, reducing bias and enhancing outcome prediction.

Abstract: Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [80] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/abs/2506.02203)
*Navid NaderiAlizadeh,Darian Salehi,Xinran Liu,Soheil Kolouri*

Main category: cs.LG

TL;DR: This paper proposes a constrained learning approach to optimize slicing directions for Sliced Wasserstein (SW) distances, which reduces computational complexity and improves performance.


<details>
  <summary>Details</summary>
Motivation: Sliced Wasserstein distances require many slicing directions to perform well, leading to high computational costs. The authors aim to reduce this cost by optimizing the slicing directions.

Method: A constrained learning approach is introduced where 1D transport plans are constrained to approximate the optimal plan in the original space. A gradient-based primal-dual approach is used to train slicer parameters along with other model parameters.

Result: The method successfully pools high-dimensional embeddings into fixed-length permutation-invariant representations. Numerical results on various models demonstrate its efficacy in learning more informative slicing directions.

Conclusion: The constrained learning approach effectively optimizes slicing directions for SW distances, enhancing both efficiency and performance.

Abstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [81] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/abs/2506.02205)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

TL;DR: The paper proposes a new method called Bregman Centroid Guided CEM (BC-EvoCEM) to improve the Cross-Entropy Method (CEM) in model-based reinforcement learning (MBRL). BC-EvoCEM uses Bregman centroids for better information aggregation and diversity control, enhancing both convergence and solution quality.


<details>
  <summary>Details</summary>
Motivation: The unimodal sampling strategy of the traditional CEM often leads to premature convergence in multimodal landscapes. To address this issue, the authors aim to develop an enhanced version of CEM that can maintain diversity and improve performance.

Method: The proposed method, BC-EvoCEM, computes a performance-weighted Bregman centroid across CEM workers and updates the least contributing ones by sampling within a trust region around the centroid. It leverages the duality between Bregman divergences and exponential family distributions to integrate seamlessly into standard CEM pipelines with negligible overhead.

Result: Empirical results on synthetic benchmarks, a cluttered navigation task, and full MBRL pipelines demonstrate that BC-EvoCEM enhances both convergence and solution quality.

Conclusion: BC-EvoCEM provides a simple yet effective upgrade for CEM in model-based reinforcement learning, improving its performance in multimodal optimization problems.

Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [82] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
*Hongling Xu,Qi Zhu,Heyuan Deng,Jinpeng Li,Lu Hou,Yasheng Wang,Lifeng Shang,Ruifeng Xu,Fei Mi*

Main category: cs.LG

TL;DR: A unified post-training framework KDRL is proposed, which integrates knowledge distillation and reinforcement learning to optimize reasoning models. It performs better than individual methods while balancing performance and token efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of reinforcement learning (low sample efficiency) and knowledge distillation (poor generalization to out-of-domain scenarios) when enhancing reasoning capabilities of large language models.

Method: KDRL leverages policy gradient optimization to minimize reverse Kullback-Leibler divergence between student and teacher model distributions and maximize expected rule-based rewards. It formulates a unified objective integrating GRPO and KD, exploring effects of different KL approximations, coefficients, and reward-guided KD strategies.

Result: Empirical results show KDRL outperforms GRPO and various KD baselines on multiple reasoning benchmarks, achieving a good balance between performance and reasoning token efficiency.

Conclusion: Integrating knowledge distillation and reinforcement learning is an effective and efficient strategy for training reasoning large language models.

Abstract: Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [83] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/abs/2506.02210)
*Pu,Yi,Tianlang Chen,Yifan Yang,Sara Achour*

Main category: cs.LG

TL;DR: 通过形式化神经网络中的对称性，提出了一种新的动态剪枝算法ExPrune，能够显著减少计算量而对精度影响很小，并且可以与静态剪枝结合进一步提升效果。


<details>
  <summary>Details</summary>
Motivation: 尽管之前的研究已经识别出神经网络架构中的对称性可能是一种冗余，但尚未探索如何利用这种对称性来实现高效的推理。本文旨在解决这一问题。

Method: 作者首先利用可交换性这一统计特性来形式化神经网络中参数和中间值的对称性，并指出可交换值可能包含重叠信息从而导致冗余。基于这一见解，作者推导出一种通用的动态剪枝算法ExPrune，该算法能够在每个输入的基础上去除由对称性引起的冗余。此外，还提供了一个具体的实例，即在神经元级别上进行动态剪枝，通过预测ReLU激活的负输入来实现。

Result: 在多个模型（包括两个计算机视觉模型、一个图模型和一个语言模型）上的实验表明，ExPrune可以在几乎不影响准确率的情况下减少10.98%-26.3%的FLOPs，或在最多降低1%准确率的情况下减少21.01%-39.05%的FLOPs。此外，ExPrune还可以与静态剪枝结合，在已经经过激进静态剪枝的模型上，进一步减少10.24%-11.11%的FLOPs（几乎不影响准确率）或13.91%-14.39%的FLOPs（最多降低1%准确率）。

Conclusion: ExPrune是一种有效的动态剪枝算法，可以通过去除神经网络中由对称性引起的冗余来提高推理效率。它不仅适用于未剪枝的模型，还能与静态剪枝方法结合使用，进一步提升性能。

Abstract: Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [84] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/abs/2506.02213)
*Kahn Rhrissorrakrai,Kathleen E. Hamilton,Prerana Bangalore Parthsarathy,Aldo Guzman-Saenz,Tyler Alban,Filippo Utro,Laxmi Parida*

Main category: cs.LG

TL;DR: 研究了量子集成模型在医疗保健和生命科学领域小数据问题上的有效性，展示了量子嵌入结构对性能的影响，并探讨了如何提取有用特征和构建能够有效学习和泛化的模型。


<details>
  <summary>Details</summary>
Motivation: 小数据学习在许多现实世界的应用中是一个常见的挑战，尤其是在生物样本相对稀缺的医疗保健和生命科学领域。

Method: 构建了多种类型的量子集成模型用于二元分类，使用多达26个量子比特进行模拟和56个量子比特的量子硬件实验。这些模型使用最少的可训练参数，但需要量子比特之间的长距离连接。在合成数据集和肾细胞癌患者的基因表达数据上测试这些量子集成模型，任务是预测患者对免疫疗法的反应。

Result: 通过模拟和初步硬件实验观察到的性能，展示了量子嵌入结构如何影响性能，以及如何提取有用特征并构建可以有效学习和泛化的模型。

Conclusion: 将量子计算纳入这些数据受限的问题为医疗保健和生命科学领域的广泛研究提供了希望，特别是在给定特征空间探索的情况下生物样本相对稀缺的情况。

Abstract: Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [85] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/abs/2506.02242)
*Yihong Tang,Ao Qu,Xujing Yu,Weipeng Deng,Jun Ma,Jinhua Zhao,Lijun Sun*

Main category: cs.LG

TL;DR: The paper proposes a Multimodal Large Language Model (MLLM)-based approach for interpretable hypothesis inference in urban and transportation research, overcoming limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional workflows in urban and transportation research face challenges such as reliance on human experts for hypothesis generation, limited interpretability, and underutilization of unstructured data.

Method: The method leverages MLLMs to craft safety-relevant questions for street view images, extract interpretable embeddings from their responses, and apply them in regression-based statistical models. It supports iterative hypothesis testing and refinement guided by statistical evidence.

Result: Experimental evaluations on Manhattan street segments show that the approach outperforms pretrained deep learning models while offering full interpretability.

Conclusion: UrbanX serves as a general-purpose framework for urban scientific discovery, enhancing model trustworthiness for policy applications and establishing a scalable pathway for interpretable knowledge discovery.

Abstract: Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [86] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/abs/2506.02243)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

TL;DR: auGraph是一种用于表格和关系数据的任务感知图增强框架，通过选择性地将属性提升为节点来改进基础图结构，从而提高关系和表格预测任务的学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有的基于GNN的方法在处理表格和关系数据时，通常依赖于刚性的、基于模式的图（如主外键链接），未能充分利用非键属性中的丰富预测信号。

Method: auGraph通过使用评分函数来量化属性与下游预测任务的相关性，选择性地将属性提升为节点，以此增强基础图结构。这种方法保留了原始数据模式，同时注入了与任务相关的结构信号。

Result: 实验表明，auGraph优于基于模式和启发式的图构建方法，生成的图更能支持关系和表格预测任务的学习。

Conclusion: auGraph提供了一种统一的框架，可以有效增强表格和关系数据的图结构，以更好地适应深度学习方法的需求。

Abstract: Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [87] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/abs/2506.02255)
*Asha Ramanujam,Adam Elyoumi,Hao Chen,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Shraman Pal,Dimitri J. Papageorgiou,Can Li*

Main category: cs.LG

TL;DR: An abstract about a new benchmark suite called SafeOR-Gym, designed for safe reinforcement learning in operations research environments with complex constraints.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of existing safe RL benchmarks that are mainly focused on robotics and control tasks, which have limited relevance to high-stakes domains such as energy systems, manufacturing, and supply chains.

Method: Present SafeOR-Gym, a benchmark suite of nine operations research environments for safe RL under complex constraints, integrated with the CMDP interface provided by OmniSafe.

Result: Evaluation of several state-of-the-art safe RL algorithms reveals a wide range of performance across the environments, showing both tractable tasks and exposing fundamental limitations in current approaches.

Conclusion: SafeOR-Gym provides a challenging and practical testbed to promote future research in safe RL for real-world decision-making problems.

Abstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [88] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/abs/2506.02256)
*Yi Xiao,Harshit Sharma,Sawinder Kaur,Dessa Bergen-Cico,Asif Salekin*

Main category: cs.LG

TL;DR: Stress affects health and wearable devices are used to detect stress. However, individual differences make it hard for machine learning models to generalize. This paper introduces HHISS, a domain generalization approach that removes person-specific differences to find consistent patterns in stress signals. It focuses on people with opioid use disorder (OUD) and evaluates the model across multiple datasets. Results show HHISS outperforms baseline methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of individual differences affecting the generalizability of machine learning models for stress detection using physiological signals.

Method: HHISS uses a technique called person-wise sub-network pruning intersection to focus on shared features across individuals and prevent overfitting by leveraging continuous labels during training.

Result: HHISS consistently outperformed state-of-the-art baseline methods across seven different stress datasets, including lab setups, controlled real-world settings, and unconstrained field data.

Conclusion: HHISS is effective and practical for real-world use in mobile stress sensing applications, especially for sensitive populations like those with OUD.

Abstract: Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [89] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/abs/2506.02269)
*YuQing Xie,Tess Smidt*

Main category: cs.LG

TL;DR: 等变神经网络在具有已知潜在对称性的任务中表现出有效性，但其优化较为复杂。本文通过理论分析损失景观几何结构，探讨了等变约束是否引入了优化的基本障碍，并得出三点重要结论：1)从更大的无约束函数空间视角看待任何网络类别可以提供关于损失景观结构的重要见解；2)在无约束函数空间中，等变网络形成与特定内部群表示相关的线性超平面的复杂联合；3)有效的等变松弛不仅需要增加非等变自由度，还需要重新考虑隐藏层中的固定群表示选择。


<details>
  <summary>Details</summary>
Motivation: 等变神经网络虽然有效，但其优化较难，且最佳训练实践尚未像标准网络那样成熟。此外，近期研究发现放松等变约束可带来小的训练收益，因此提出问题：等变约束是否引入了根本的优化障碍？还是仅仅需要不同的超参数调整？

Method: 本文通过理论分析损失景观几何结构来探讨上述问题，重点关注使用置换表示构建的网络，将其视为无约束MLP的子集。研究了无约束模型的参数对称性如何影响等变子空间的损失景观，并在某些条件下证明其可能阻止全局最小值的学习。同时，通过实验表明，在这种情况下，放松到无约束MLP有时可以解决问题，且最终找到的权重对应于隐藏层中不同的群表示选择。

Result: 研究表明，等变约束确实可能引入优化障碍，放松约束有助于解决这些问题，并揭示了隐藏层中不同群表示选择的重要性。

Conclusion: 本文得出三个关键结论：1)从更大的无约束函数空间视角看待网络可以提供关于损失景观结构的重要见解；2)在无约束函数空间中，等变网络形成复杂的线性超平面联合；3)有效的等变松弛需要增加非等变自由度并重新思考隐藏层中的固定群表示选择。

Abstract: Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [90] [Latent Stochastic Interpolants](https://arxiv.org/abs/2506.02276)
*Saurabh Singh,Dmitry Lagun*

Main category: cs.LG

TL;DR: The paper introduces Latent Stochastic Interpolants (LSI), an advanced generative modeling framework that enables joint learning in a latent space, improving upon Stochastic Interpolants (SI) by optimizing encoder, decoder and latent SI models end-to-end via a continuous-time Evidence Lower Bound (ELBO) objective. LSI is shown to be effective through experiments on the ImageNet dataset.


<details>
  <summary>Details</summary>
Motivation: Stochastic Interpolants (SI) are strong for transforming between two probability distributions but their application in jointly optimized latent variable models has not been explored due to requiring direct access to samples from these distributions.

Method: The authors developed Latent Stochastic Interpolants (LSI) which allow joint learning in a latent space with end-to-end optimized encoder, decoder and latent SI models using a principled Evidence Lower Bound (ELBO) objective derived directly in continuous time.

Result: LSI effectively learns latent representations and a generative process that transforms an arbitrary prior distribution into the encoder-defined aggregated posterior, demonstrated through comprehensive experiments on the ImageNet generation benchmark.

Conclusion: Latent Stochastic Interpolants (LSI) provide a powerful method for generative modeling, overcoming limitations of previous models while preserving flexibility.

Abstract: Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [91] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)
*Qinsi Wang,Jinghan Ke,Hancheng Ye,Yueqian Lin,Yuzhe Fu,Jianyi Zhang,Kurt Keutzer,Chenfeng Xu,Yiran Chen*

Main category: cs.LG

TL;DR: 当前强化微调范式在大数据模型中存在样本效率低下的问题，本文提出GAIN-RL框架利用模型自身产生的内在学习信号动态选择训练数据，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的RFT范式由于相同的查询在统一的数据采样下冗余暴露而存在样本效率低下的问题，同时以前的工作通过启发式难度指标进行课程学习也存在局限性。

Method: 识别出模型固有的角度集中信号，并基于此提出GAIN-RL框架，通过利用模型的内在角度集中信号，在每个epoch中动态选择训练数据，确保持续产生影响的梯度更新。

Result: 实证评估显示，GAIN-RL（GRPO）在多样化的数学和编码任务以及不同的模型规模上实现了超过2.5倍的训练效率加速，并且其高效采样带来了数据高效的训练效果。

Conclusion: GAIN-RL框架通过动态选择训练数据显著提升了整体训练效率，能够以一半的数据量达到更好的性能。

Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [92] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)
*Aaron Defazio*

Main category: cs.LG

TL;DR: This paper finds out that during long-duration LLM training runs, there is an unintended interaction causing gradient norm to increase. It proposes a simple correction to solve this problem and achieve lower loss values.


<details>
  <summary>Details</summary>
Motivation: Gradient norm increases rapidly near the end of LLM training, which is caused by the unintended interaction between weight decay, normalization layers, and the learning rate schedule.

Method: Propose a simple correction for the unintended interaction between weight decay, normalization layers, and the learning rate schedule.

Result: The proposed correction not only fixes the behavior of gradient norm increase but also results in lower loss values throughout training.

Conclusion: A simple correction can fix the unintended interaction and lead to lower loss values.

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [93] [On Universality Classes of Equivariant Networks](https://arxiv.org/abs/2506.02293)
*Marco Pacini,Gabriele Santin,Bruno Lepri,Shubhendu Trivedi*

Main category: cs.LG

TL;DR: Equivariant神经网络在分离能力方面已被广泛研究，但其通用性（即近似目标函数的能力）尚未得到充分探索。本文探讨了超越分离约束的等变神经网络的近似能力，揭示了分离能力并不能完全代表表达能力，并提供了浅层不变网络的通用性分类框架。


<details>
  <summary>Details</summary>
Motivation: 尽管等变神经网络的分离能力已被深入分析，但其近似任意目标函数的能力（即通用性）相对较少被研究。这促使作者去探究等变模型的近似能力是否能超越其分离能力的限制。

Method: 作者通过研究浅层不变网络的通用性类别来展示分离能力并不能完全捕捉表达能力。他们提供了一个通用框架，用于理解这些架构能够近似的函数类型。同时，通过投影，分析了等变模型简化为不变模型的情况，从而得出浅层等变网络无法达到通用性的充分条件。此外，还确定了一些浅层模型能够在分离约束下实现通用性的场景。

Result: 研究表明，具有相同分离能力的模型可能在近似能力上存在差异。浅层等变网络的通用性取决于对称群的结构性质，如适当正规子群的存在性。在某些情况下，如置换对称性，这些条件可能不成立，导致模型无法实现通用性。

Conclusion: 本文强调了分离能力不能完全代表等变神经网络的表达能力，并提出了一个框架来理解浅层不变网络的通用性。结果表明，浅层等变网络的通用性受限于对称群的结构性质。

Abstract: Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [94] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)
*Farzaneh Mahdisoltani,Saeed Mahdisoltani,Roger B. Grosse,David J. Fleet*

Main category: cs.LG

TL;DR: This paper proposes a novel framework to visualize the implicit path between classes in deep neural networks by treating the network gradient as a form of infinitesimal motion. It uses Complex Steerable Pyramid for image decomposition and class-conditional gradients in the transformed space.


<details>
  <summary>Details</summary>
Motivation: Existing interpretability methods identify influential input regions but do not explain how models distinguish between classes or what changes would transition an input from one category to another.

Method: The method involves decomposing images using invertible transforms (Complex Steerable Pyramid), computing class-conditional gradients in the transformed space, amplifying the one-step gradient to the input and performing linear extrapolation to expose model transitions from source to target class.

Result: Experiments on synthetic and real-world datasets show perceptually aligned, semantically meaningful transformations offering a new interpretable insight into neural classifiers' internal representations.

Conclusion: The proposed framework provides a novel, interpretable way to understand the decision boundaries and internal representations of neural classifiers.

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [95] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/abs/2506.02306)
*Aditya Gorla,Ryan Wang,Zhengtong Liu,Ulzee An,Sriram Sankararaman*

Main category: cs.LG

TL;DR: CACTI is a masked autoencoding approach for imputing tabular data, using median truncated copy masking to leverage missingness patterns and contextual info, outperforming state-of-the-art methods with an average R² gain of 7.8%.


<details>
  <summary>Details</summary>
Motivation: To improve the imputation of tabular data by leveraging structure in missingness patterns and contextual information.

Method: Employing a novel median truncated copy masking training strategy that encourages learning from empirical patterns of missingness while incorporating semantic relationships between features captured by column names and text descriptions.

Result: CACTI outperforms state-of-the-art methods with an average R² gain of 7.8% (13.4%, 6.1%, and 5.3% under missing not at random, at random and completely at random respectively).

Conclusion: Leveraging dataset-specific contextual information and missingness patterns enhances imputation performance.

Abstract: We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [96] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/abs/2506.02308)
*Xiaojun Shan,Qi Cao,Xing Han,Haofei Yu,Paul Pu Liang*

Main category: cs.LG

TL;DR: Recent advances in multimodal foundation models have achieved state-of-the-art performance, but simply increasing the number of instruction-tuning tasks does not consistently yield better performance. Instead, grouping tasks by common interactions across modalities encourages models to learn transferrable skills while suppressing interference from mismatched tasks. The proposed MINT method outperforms existing task grouping baselines for multimodal instruction tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation is that recent breakthroughs in multimodal foundation models are driven by new pre-training paradigms and instruction fine-tuning, but simply increasing the number of instruction-tuning tasks does not always lead to better performance.

Method: The method involves introducing MINT, a task-grouping strategy based on the type of multimodal interaction, which groups tasks by common interactions across modalities such as discovering redundant shared information, prioritizing modality selection with unique information, or requiring synergistic fusion to discover new information from both modalities.

Result: The proposed MINT method greatly outperforms existing task grouping baselines for multimodal instruction tuning, achieving an effective balance between generalization and specialization.

Conclusion: Grouping tasks by common interactions across modalities can encourage models to learn transferrable skills while suppressing interference from mismatched tasks, leading to improved performance in multimodal instruction tuning.

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [97] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/abs/2506.02315)
*D. Isaiah Harp,Joshua Ott,John Alora,Dylan Asmar*

Main category: cs.LG

TL;DR: A novel approach eliminating test points by using machine learning to produce a reduced-order model (ROM) which refines predictions based on actual flight conditions and updates the high-fidelity model.


<details>
  <summary>Details</summary>
Motivation: The existence of databands and tolerances presents a more fundamental problem than inadequate pilot skill in flight testing, thus there is a need for an alternative method that eliminates test points.

Method: Starting with a high-fidelity digital model of an air vehicle, use machine learning to produce a reduced-order model (ROM). This ROM generates predictions based on any set of conditions flown by the pilot and can be updated with new data if the test results differ from the prediction.

Result: The outcome of flight test is a refined ROM at whatever conditions were flown, which in turn updates and validates the high-fidelity model. An example using T-38C flight test data demonstrated the ability to generate parameters necessary to assess compliance for longitudinal dynamics.

Conclusion: This 'point-less' architecture using ROM has the potential to improve the accuracy and relevance of flight tests by adapting to actual flight conditions and continuously updating the underlying models.

Abstract: The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [98] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/abs/2506.02318)
*Yuchen Liang,Renxiang Huang,Lifeng Lai,Ness Shroff,Yingbin Liang*

Main category: cs.LG

TL;DR: 本文为使用吸收率矩阵的离散扩散模型提供了首个有限时间误差界和收敛速度分析，改进了采用均匀率矩阵时的结果，并引入新工具解决了吸收率矩阵的独特挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管实证结果表明吸收率矩阵在生成质量上优于均匀率矩阵，但现有的理论工作主要集中在均匀率矩阵上，缺乏对吸收扩散模型的收敛性保证和误差分析。

Method: 通过推导前向过程KL散度的上界并引入代理初始化分布来解决吸收平稳分布带来的问题；建立了τ-leaping和均匀化采样器在吸收率矩阵下的首个收敛性保证；提供了无需提前停止的收敛性保证，并引入新的技术工具解决吸收率矩阵的独特挑战。

Result: 得到了吸收率矩阵下离散扩散模型的有限时间误差界和收敛速度分析，展示了比均匀率矩阵更好的收敛速度，并在适当假设下实现了无需提前停止的收敛性保证。

Conclusion: 本文填补了吸收率矩阵离散扩散模型理论分析的空白，提供了更强的收敛性和误差控制，为吸收率矩阵的实际应用提供了理论支持。

Abstract: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [99] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/abs/2506.02323)
*Aleix Boquet-Pujadas,Pol del Aguila Pla,Michael Unser*

Main category: cs.LG

TL;DR: The paper formulates an optimization problem for estimating probability densities in multidimensional, unevenly sampled problems. It leverages detector sensitivity as a heterogeneous density and uses splines on a grid for computational speed and flexible boundary conditions. The Hessian of the spline is regularized via the nuclear norm to promote sparsity, making the method spatially adaptive and stable against regularization parameter choice. The pipeline is tested on standard densities, software is provided, and a new PET rebinning approach is presented.


<details>
  <summary>Details</summary>
Motivation: To accurately estimate probability densities in multidimensional problems with uneven sampling probability, addressing challenges such as detector sensitivity heterogeneity and ensuring computational efficiency and stability.

Method: Formulate an optimization problem that incorporates detector sensitivity as a heterogeneous density. Utilize splines on a grid for fast computation and flexible boundary conditions. Regularize the Hessian of the spline using the nuclear norm to promote sparsity and achieve spatial adaptivity.

Result: The computational pipeline successfully estimates standard densities. A new approach to PET rebinning demonstrates the practical application of the framework.

Conclusion: The proposed method provides an effective solution for estimating probability densities in multidimensional, unevenly sampled problems, offering spatial adaptivity and stability.

Abstract: We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [100] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/abs/2506.02337)
*Adrienne M. Propp,Jonas A. Actor,Elise Walker,Houman Owhadi,Nathaniel Trask,Daniel M. Tartakovsky*

Main category: cs.LG

TL;DR: The paper proposes a new method using Gaussian processes to learn Dirichlet-to-Neumann maps on graphs for multiphysics simulations, enforcing conservation laws and providing uncertainty quantification. It shows high accuracy and reliable uncertainty estimates even with limited data.


<details>
  <summary>Details</summary>
Motivation: To improve the coupling of multiphysics simulations across computational subdomains by ensuring continuity of state variables and fluxes at artificial interfaces, especially in scenarios with limited data and where reliable uncertainty quantification is critical.

Method: The method uses Gaussian processes to learn Dirichlet-to-Neumann maps on graphs, combining discrete exterior calculus and nonlinear optimal recovery to infer relationships between vertex and edge values. It optimizes over the reproducing kernel Hilbert space norm while applying a maximum likelihood estimation penalty on kernel complexity to enforce conservation laws without overfitting.

Result: The method maintains high accuracy and well-calibrated uncertainty estimates even under severe data scarcity, as demonstrated in applications to subsurface fracture networks and arterial blood flow.

Conclusion: This novel approach has significant potential for scientific applications where limited data and reliable uncertainty quantification are crucial.

Abstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [101] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/abs/2506.02355)
*Andre He,Daniel Fried,Sean Welleck*

Main category: cs.LG

TL;DR: Reinforcement learning is effective for training large language models on structured tasks. However, GRPO, a widely used RL algorithm, has a critical flaw where it reinforces already probable solutions and neglects rare but correct proofs in multi-sample performance tasks like formal theorem proving. This bias affects pass@$N$ metrics at large sample sizes. To solve this issue, the unlikeliness reward method is introduced to encourage reinforcing rare correct solutions. Experiments show that incorporating the unlikeliness reward significantly improves pass@$N$ across a range of N and increases sample diversity. When applied to Lean, competitive performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark is achieved.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of GRPO in handling tasks requiring multi-sample performance, such as formal theorem proving, where it tends to reinforce already probable solutions and ignore rare but correct proofs.

Method: The method involves introducing the unlikeliness reward to explicitly encourage reinforcing rare correct solutions and finding that increasing the number of PPO epochs further mitigates the bias.

Result: The experiments demonstrate that incorporating the unlikeliness reward significantly improves pass@$N$ across a large range of N, outperforming standard GRPO and increasing sample diversity.

Conclusion: In conclusion, by applying the revised recipe including the unlikeliness reward to Lean, competitive performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark is achieved, providing a simple yet effective way to train formal theorem provers with RL.

Abstract: Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [102] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)
*Ram Potham*

Main category: cs.LG

TL;DR: This paper presents a lightweight benchmark methodology using a grid world to evaluate if LLM agents can prioritize high-level safety principles over conflicting lower-level task instructions, providing insights into agent controllability.


<details>
  <summary>Details</summary>
Motivation: To establish credible safety plans for advanced AI development, it's essential to verify agent behavior and detect control deficiencies early. This involves ensuring agents adhere to safety-critical principles even when they conflict with operational goals.

Method: The paper introduces a benchmark methodology set in a simple grid world. It evaluates an LLM agent's ability to uphold predefined high-level safety principles while faced with conflicting lower-level task instructions.

Result: The pilot study demonstrates the feasibility of the proposed methodology, offering preliminary insights into how agents behave under principle conflicts and contributing empirical evidence for assessing controllability.

Conclusion: Evaluating adherence to hierarchical principles is a critical early step towards understanding our capacity to build governable AI systems.

Abstract: Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [103] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)
*Zhe Li,Bicheng Ying,Zidong Liu,Chaosheng Dong,Haibo Yang*

Main category: cs.LG

TL;DR: In this paper, the authors propose HiSo, a novel fast federated fine-tuning method for Large Language Models (LLMs) that uses Hessian-informed zeroth-order optimization with scalar-only communication. This approach leverages global curvature information to accelerate convergence while maintaining minimal communication cost per round.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from the limitations of existing dimension-free communication frameworks in Federated Learning (FL), such as DeComFL, which use zeroth-order stochastic gradient descent (ZO-SGD). While these methods reduce communication costs by transmitting only scalars, they suffer from high variance in ZO gradient estimation leading to slow convergence. Additionally, integrating Hessian information into FL is challenging due to local data restrictions and the need to maintain dimension-free communication.

Method: The authors introduce a generalized scalar-only communication FL framework that separates dimension-free communication from standard ZO-SGD, enabling the integration of advanced optimization strategies. Building on this, they propose HiSo, which incorporates global curvature information via Hessian-informed zeroth-order optimization while preserving scalar-only communication. Theoretically, they establish convergence guarantees independent of the global Lipschitz constant and show faster rates when the global Hessian has a low effective rank.

Result: Extensive experiments conducted on benchmark datasets and LLM fine-tuning tasks demonstrate that HiSo significantly outperforms existing ZO-based FL methods in terms of both convergence speed and communication efficiency.

Conclusion: HiSo represents a significant advancement in federated fine-tuning of LLMs by addressing the challenges of slow convergence and high communication costs in dimension-free FL frameworks. It achieves faster convergence rates and maintains minimal communication cost, making it a promising method for practical applications.

Abstract: Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [104] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/abs/2506.02371)
*Haoye Lu,Darren Lo,Yaoliang Yu*

Main category: cs.LG

TL;DR: Reinterpret SFBD as an alternating projection algorithm and introduce SFBD flow, which removes the need for alternating steps. Its practical instantiation, Online SFBD, consistently outperforms strong baselines across benchmarks.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have strong generative performance but often rely on large datasets that may include sensitive content and raise privacy concerns due to their tendency to memorize training data. SFBD addresses this by training on corrupted data and using limited clean samples, but its iterative process is burdensome.

Method: Reinterpreted SFBD as an alternating projection algorithm and introduced a continuous variant, SFBD flow, which eliminates the need for alternating steps. Also showed its connection to consistency constraint-based methods.

Result: Demonstrated that the practical instantiation of the method, Online SFBD, consistently outperforms strong baselines across benchmarks.

Conclusion: The introduction of SFBD flow simplifies the SFBD method and its variant, Online SFBD, shows superior performance compared to existing baselines.

Abstract: Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [105] [Multi-agent Markov Entanglement](https://arxiv.org/abs/2506.02385)
*Shuze Chen,Tianyi Peng*

Main category: cs.LG

TL;DR: 本文探讨了多智能体强化学习中价值分解的有效性，揭示了其背后的数学结构，并引入了衡量多智能体MDP的“马尔可夫纠缠”概念，证明了一类广泛使用的索引策略具有较弱的纠缠特性，且分解误差为$\mathcal O(\sqrt{N})$。此外，还展示了如何在实践中有效估计马尔可夫纠缠，为价值分解的质量提供了经验代理指标。


<details>
  <summary>Details</summary>
Motivation: 尽管价值分解在多智能体动态规划和强化学习中被广泛应用，但其为何如此有效的理论依据尚未得到充分研究。

Method: 作者通过分析多智能体马尔可夫决策过程（MDP）的数学结构，发现价值分解的有效性与系统的“非纠缠”性质有关。受量子物理中测量量子纠缠的启发，提出了衡量多智能体MDP的“马尔可夫纠缠”的方法，并用该度量来限定一般多智能体MDP中的分解误差。进一步证明了一类常用的索引策略是弱纠缠的，且其分解误差规模为$\mathcal O(\sqrt{N})$。最后，讨论了如何在实际应用中高效估计马尔可夫纠缠。

Result: 揭示了价值分解背后的关键数学原理，即系统的非纠缠特性决定了价值分解的效果。同时，证明了一类常用索引策略具有较低的分解误差，并提出了一种实用的方法来评估价值分解的质量。

Conclusion: 本文不仅从理论上解释了多智能体系统中价值分解的有效性，还提供了一个新的视角——马尔可夫纠缠，用于分析和优化多智能体强化学习系统。这一研究成果为未来设计更高效的多智能体算法奠定了基础。

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [106] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/abs/2506.02386)
*Jie Bian,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: 在固定预算内识别最佳可行臂的问题中，本文填补了误差概率指数衰减率的理论空白，并提出了一个新算法，该算法在K-armed bandits with Gaussian noise的线性环境中达到了信息论下界。实验结果表明，该方法在准确性和效率上均优于几种基准算法。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来对固定预算内最佳可行臂识别问题的研究兴趣浓厚，但误差概率以何种精确的指数速率趋近于零尚未明确，即使是在相对简单的K-armed bandits with Gaussian noise情境下也是如此。

Method: 提出了一种新的算法，结合后验采样框架和基于游戏的采样规则（涉及min-learner和max-learner），并优化了固定预算约束下的识别过程。此方法与Thompson sampling有相似之处，但进行了特定调整以适应当前问题。

Result: 理论上证明了误差概率呈指数衰减，且衰减速率与信息论下界相匹配。实证结果进一步验证了算法的有效性，显示其在准确性和效率上均优于其他基准算法。

Conclusion: 新算法成功填补了关于误差概率指数衰减率的理论空白，并在实践中表现出优越的性能。

Abstract: The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [107] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)
*Chamara Madarasingha,Nasrin Sohrabi,Zahir Tari*

Main category: cs.LG

TL;DR: This paper proposes LLMPred, a method that leverages Large Language Models (LLMs) for time-series prediction by converting sequences into text and employing specific pre-processing techniques. It shows competitive performance with smaller LLMs and highlights the importance of its key components through ablation studies.


<details>
  <summary>Details</summary>
Motivation: Recent studies suggest that LLMs can be used for time-series prediction due to their strong generalization capabilities and ability to perform well without extensive pre-training, but their effectiveness in handling complex, noisy, and multivariate data is still unclear.

Method: The proposed method, LLMPred, converts time-series sequences into text and feeds them to LLMs for zero-shot prediction. It uses two main data pre-processing techniques: time-series sequence decomposition for univariate sequences and a lightweight prompt-processing strategy for multivariate data.

Result: Experiments with smaller LLMs show that LLMPred achieves competitive or superior performance compared to state-of-the-art baselines. Ablation studies emphasize the importance of the key components in LLMPred.

Conclusion: LLMPred enhances LLM-based time-series prediction by effectively handling complex, noisy, and multivariate data through appropriate pre-processing techniques.

Abstract: Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [108] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/abs/2506.02390)
*Qin Xie,Qinghua Zhang,Shuyin Xia,Xinran Zhou,Guoyin Wang*

Main category: cs.LG

TL;DR: Adaptive Boosting (AdaBoost) struggles with label noise, especially in multiclass classification. This paper proposes GAdaBoost, a two-stage framework incorporating data granulation and adaptive boosting to improve efficiency and robustness in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: AdaBoost faces challenges with label noise, particularly in multiclass classification tasks. Current methods either fail to handle noise effectively or are computationally expensive due to redundant data usage.

Method: The paper introduces GAdaBoost, which includes a data granulation stage and an adaptive boosting stage. A granular-ball generation method is used to compress data while preserving diversity and reducing label noise impact. The granular ball-based SAMME algorithm then focuses on these granular balls instead of individual samples.

Result: Experimental results on noisy datasets demonstrate that GAdaBoost achieves better robustness and efficiency compared to existing methods, showing its effective extension of AdaBoost and SAMME.

Conclusion: GAdaBoost, with its two-stage framework, successfully enhances the efficiency and robustness of AdaBoost in handling label noise, providing a promising solution for noisy multiclass classification tasks.

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [109] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)
*Yuanyao Chen,Rongsheng Chen,Fu Luo,Zhenkun Wang*

Main category: cs.LG

TL;DR: A novel learning framework driven by Large Language Models (LLMs) is introduced to enhance the scalability of Neural Combinatorial Optimization (NCO) models for large-scale Vehicle Routing Problems (VRPs). This framework learns a projection between training and testing distributions, operates exclusively during inference phase without retraining, and significantly improves performance on large-scale TSP and CVRP.


<details>
  <summary>Details</summary>
Motivation: Existing NCO methods trained on small-scale VRP instances show degraded performance when applied to large-scale scenarios due to distributional shift between training and testing data.

Method: The method involves a novel learning framework driven by LLMs that learns a projection between the training and testing distributions. It is deployed during the inference phase to enhance the scalability of the NCO model without requiring joint training or model retraining.

Result: The framework enables a backbone NCO model trained on 100-node instances to achieve superior performance on large-scale TSP and CVRP with up to 100K nodes from diverse distributions.

Conclusion: This work successfully addresses the scalability issue of NCO models for large-scale VRPs using a LLM-driven framework that enhances performance without necessitating model retraining.

Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [110] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/abs/2506.02406)
*Renat Sergazinov,Jing Wu,Shao-An Yin*

Main category: cs.LG

TL;DR: Random Fourier features, traditionally used in kernel methods, can also serve as a parameter-free pre-processing step for deep learning on tabular data. By projecting inputs into a fixed feature space via sine and cosine projections, this approach circumvents the need for ad hoc normalization or additional learnable embeddings. Within the Neural Tangent Kernel (NTK) framework, this mapping bounds and conditions the network's initial NTK spectrum and introduces a bias that shortens the optimization trajectory, thereby accelerating gradient-based training. Empirically, deep networks trained on Fourier-transformed inputs converge more rapidly and consistently achieve strong final performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address shortcomings in tabular deep learning pipelines revealed through Neural Tangent Kernel (NTK) analysis. The authors aim to find a parameter-free, architecture-agnostic transformation that can improve the performance of deep learning models on tabular data.

Method: The method involves using random Fourier mappings as a pre-processing step for deep learning on tabular data. This is done by projecting each input into a fixed feature space via sine and cosine projections with frequencies drawn once at initialization. This approach circumvents the need for ad hoc normalization or additional learnable embeddings.

Result: Within the NTK framework, this mapping bounds and conditions the network's initial NTK spectrum, and introduces a bias that shortens the optimization trajectory, thereby accelerating gradient-based training. Empirically, deep networks trained on Fourier-transformed inputs converge more rapidly and consistently achieve strong final performance.

Conclusion: The findings establish random Fourier pre-processing as a theoretically motivated, plug-and-play enhancement for tabular deep learning.

Abstract: While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [111] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/abs/2506.02415)
*Karthikeyan Vaiapury*

Main category: cs.LG

TL;DR: In this paper, researchers introduce AERO (Adversarial Energy-based Redirection Optimization), a new optimization framework inspired by the redirection principle in Judo. This method leverages external disturbances rather than resisting them and reimagines optimization as a redirection process guided by 15 axioms. When applied to probabilistic solar energy forecasting, AERO shows significant improvements in accuracy, reliability, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current optimization methods often face challenges in maintaining stability and adaptability in dynamic, non-linear systems, especially under uncertainty. To address these issues, the authors sought to develop an optimization framework that could effectively handle disturbances.

Method: AERO is based on 15 interrelated axioms covering adversarial correction, energy conservation, and disturbance-aware learning. It projects gradients, integrates uncertainty-driven dynamics, and manages learning energy to provide stable and robust model updates.

Result: When applied to probabilistic solar energy forecasting, AERO demonstrated substantial gains in predictive accuracy, reliability, and adaptability, particularly in noisy and uncertain environments.

Conclusion: The findings indicate that AERO represents a promising new direction in both theoretical and practical aspects of optimization.

Abstract: Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [112] [Weak Supervision for Real World Graphs](https://arxiv.org/abs/2506.02451)
*Pratheeksha Nair,Reihaneh Rabbany*

Main category: cs.LG

TL;DR: This paper introduces WSNET, a weakly supervised graph contrastive learning framework that improves node classification in graphs with limited and noisy labels by leveraging weak signals, outperforming state-of-the-art methods by up to 15% in F1 score.


<details>
  <summary>Details</summary>
Motivation: Node classification in real world graphs often encounters challenges due to label scarcity and noise, which is especially problematic in high stakes domains like human trafficking detection and misinformation monitoring. This motivived the need for a method that can effectively utilize weak signals present in such graphs.

Method: WSNET integrates graph structure, node features, and multiple noisy supervision sources through a contrastive objective specifically designed for weakly labeled data.

Result: WSNET consistently outperforms state of the art contrastive and noisy label learning methods by up to 15% in F1 score across three real world datasets and synthetic benchmarks with controlled noise.

Conclusion: The study highlights the effectiveness of contrastive learning under weak supervision and demonstrates the potential of exploiting imperfect labels in graph based settings.

Abstract: Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [113] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
*Jiaxi Hu,Yongqi Pan,Jusen Du,Disen Lan,Xiaqiang Tang,Qingsong Wen,Yuxuan Liang,Weigao Sun*

Main category: cs.LG

TL;DR: Recent models like Gated DeltaNet, TTT, and RWKV-7 have improved performance through Delta learning rule by introducing interactions between the recurrent state and key vector. This paper introduces Nonlinear RNNs, analyzing their advantages and limitations, then proposes a new variant called Comba based on closed-loop control theory.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of nonlinear recursive structures in sequence modeling by introducing interactions between the recurrent state and key vector.

Method: The paper first introduces the concept of Nonlinear RNNs and analyzes their pros and cons. Then it proposes a novel Nonlinear RNN variant named Comba which uses a scalar-plus-low-rank state transition with both state feedback and output feedback corrections. Additionally, a hardware-efficient chunk-wise parallel kernel is implemented in Triton for training models with large parameters.

Result: Comba shows superior performance and computation efficiency in both language and vision modeling.

Conclusion: Nonlinear RNNs with proper design, such as Comba, can significantly enhance sequence modeling performance and computational efficiency.

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [114] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/abs/2506.02504)
*Xingyu Chen,Bokun Wang,Ming Yang,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

TL;DR: The paper addresses the challenges of non-convex non-smooth Finite-sum Coupled Compositional Optimization (FCCO) by proposing stochastic momentum methods with provable convergence guarantees and achieving a new state-of-the-art iteration complexity of O(1/ϵ^5).


<details>
  <summary>Details</summary>
Motivation: Existing methods for FCCO face limitations such as high iteration complexity and reliance on SGD-type updates unsuitable for deep learning applications.

Method: The authors propose stochastic momentum methods specifically designed for non-smooth FCCO problems. These methods come with provable convergence guarantees and achieve a lower iteration complexity compared to previous approaches.

Result: They establish a new state-of-the-art iteration complexity of O(1/ϵ^5) for solving non-convex non-smooth FCCO problems. Additionally, they demonstrate effectiveness in multiple inequality constrained non-convex optimization problems, achieving a new complexity for finding an ϵ-level KKT solution.

Conclusion: The proposed algorithms improve upon current state-of-the-art results in terms of iteration complexity and applicability to deep learning contexts. Experiments on three tasks validate their effectiveness.

Abstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [115] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/abs/2506.02539)
*Thong Q. Nguyen,Shubhang Desai,Yash Jain,Tanvir Aumi,Vishal Chowdhary*

Main category: cs.LG

TL;DR: VerificAgent is a framework that manages memory for computer-use agents (CUAs) through expert-curated domain knowledge, iterative memory refinement, and human fact-checking. It improves success rates in productivity tasks without additional fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Continual memory augmentation in CUAs can lead to spurious or hallucinated 'learnings' that degrade performance, especially in domain-specific workflows like productivity software.

Method: The framework VerificAgent uses three main approaches: an expert-curated seed of domain knowledge, iterative trajectory-based memory refinement during training, and a post-hoc fact-checking pass by human experts.

Result: On OSWorld productivity tasks, VerificAgent achieves a 111.1% relative improvement in success rate over baseline CUA without any additional fine-tuning.

Conclusion: VerificAgent effectively manages memory for CUAs, leading to significant improvements in task success rates.

Abstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [116] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.LG

TL;DR: PUBG是一种新的机器遗忘方法，专为大型视觉-语言模型（LVLMs）设计，能生成视觉上有根据且信息丰富的回应，同时防止隐私泄露。


<details>
  <summary>Details</summary>
Motivation: 现有的机器遗忘方法在处理大型视觉-语言模型时，未能妥善选择替代输出，导致不理想的后续行为，如退化、幻觉或过度拒绝的回应。因此，需要一种新方法来确保回应的质量和信息性，而不仅仅是简单的抑制。

Method: 提出了一种新的LVLMs遗忘任务，要求模型提供保护隐私但又有信息量且视觉有根据的回应。还提出了PUBG，一种新的遗忘方法，明确引导遗忘后的行为朝向理想的输出分布。

Result: 实验表明，尽管现有方法能够成功防止隐私侵犯，但仍遭受遗忘后续问题；而PUBG有效缓解了这些问题，对遗忘目标生成视觉上有根据且信息丰富的回应，无隐私泄露。

Conclusion: PUBG有效地解决了现有遗忘方法的问题，为LVLMs提供了高质量、信息丰富且保护隐私的回应。

Abstract: Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [117] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann,Masoud Ramuz,Zeeshan Nisar,Nadine S. Schaadt,Hendrik Annuth,Benjamin Doerr,Friedrich Feuerhake,Thomas Lampert,Johannes F. Lutzeyer*

Main category: cs.LG

TL;DR: In this paper, the authors develop a novel heterogeneous GNN architecture named HIEGNet for glomeruli health classification in nephropathology. They propose a pipeline to construct a heterogeneous graph and show that HIEGNet outperforms several baseline models.


<details>
  <summary>Details</summary>
Motivation: GNNs have shown promise in histopathology but have not been extensively explored for glomeruli health classification, an important task in nephropathology. This task presents unique difficulties especially in constructing graphs.

Method: The authors use a combination of traditional and machine learning-based computer vision techniques to identify nodes, edges, and features for constructing a heterogeneous graph. Then they propose a new heterogeneous GNN architecture called HIEGNet which integrates glomeruli and surrounding immune cells.

Result: HIEGNet outperforms several baseline models in experiments conducted on a dataset of Whole Slide Images from kidney transplant patients. It also generalises best between patients among all baseline models.

Conclusion: The proposed HIEGNet is effective for glomeruli health classification and considers the immune environment of each glomerulus, offering a promising approach in nephropathology.

Abstract: Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [118] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
*Shenghua He,Tian Xia,Xuan Zhou,Hui Wei*

Main category: cs.LG

TL;DR: 研究了大语言模型强化学习中的零奖励假设问题，并提出轨迹策略梯度定理，揭示常用方法如PPO等具备建模token级奖励信号的能力，为更实用高效的LLM微调铺平道路。同时提出了新算法Token-Reinforced Policy Optimization (TRePO)。


<details>
  <summary>Details</summary>
Motivation: 在大语言模型的强化学习中，零奖励假设（非终端动作无任务特定即时奖励，仅最终token有奖励）是一个常见挑战，因为精确的token级奖励在实际应用中难以获得。

Method: 引入轨迹策略梯度定理，表明基于真实但未知的token级奖励的策略梯度可以仅使用响应级奖励模型进行无偏估计，适用于REINFORCE和Actor-Critic系列算法。此外，提出了一种新算法Token-Reinforced Policy Optimization (TRePO)，该算法理论上更有依据、比PPO简单且与GRPO在内存效率上相当。

Result: 揭示了常用方法如PPO、GRPO、ReMax和RLOO本质上具有建模token级奖励信号的能力，为响应级奖励方法提供了理论支持。并详细分析了流行RL和非RL方法的理论基础及实际优势。新算法TRePO展示了广泛适用的潜力。

Conclusion: 这项工作为更实用、高效的LLM微调提供了理论支持，使开发者能够专注于改进响应级奖励模型。同时提出的TRePO算法在理论上有坚实基础，简单且内存效率高，具有广泛应用前景。

Abstract: We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [119] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/abs/2506.02563)
*Roie Reshef,Kfir Yehuda Levy*

Main category: cs.LG

TL;DR: This paper introduces a new noise-cancellation mechanism for Differential Privacy (DP) in Federated Learning (FL) under partial-participation, achieving optimal performance without sacrificing convergence rates or computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Previous work achieved optimal performance in full-participation settings but struggled to extend to partial-participation scenarios in DP FL.

Method: The approach involves introducing a novel noise-cancellation mechanism that preserves privacy without sacrificing convergence rates or computational efficiency, analyzed within the Stochastic Convex Optimization (SCO) framework.

Result: Delivers optimal performance for both homogeneous and heterogeneous data distributions in partial-participation FL settings.

Conclusion: Expands the applicability of DP in FL, providing an efficient and practical solution for privacy-preserving learning in distributed systems with partial participation.

Abstract: This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [120] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)
*Ping Gong,Jiawei Yi,Shengnan Wang,Juncheng Zhang,Zewen Jin,Ouxiang Zhou,Ruibo Liu,Guanbin Xu,Youhui Bai,Bowen Ye,Kun Yuan,Tong Yang,Gong Zhang,Renhai Chen,Feng Wu,Cheng Li*

Main category: cs.LG

TL;DR: HATA, a novel approach integrating learning-to-hash techniques into Top-k attention for LLMs, achieves 7.2× speedup over full attention while maintaining accuracy and outperforming other top-k methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the efficiency-accuracy trade-off in existing top-k attention mechanisms for accelerating LLM inference.

Method: HATA maps queries and keys into binary hash codes to acquire relative qk score order at a low cost, differing from methods that seek absolute qk score estimation which is typically costly.

Result: HATA achieves up to 7.2× speedup compared to vanilla full attention while maintaining model accuracy and outperforms state-of-the-art top-k attention methods in both accuracy and efficiency.

Conclusion: HATA provides an efficient and accurate solution for top-k attention in LLMs.

Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [121] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/abs/2506.02577)
*Wenyan Yang,Joni Pajarinen*

Main category: cs.LG

TL;DR: 在离线目标条件强化学习中，提出了一种名为可达性加权采样（RWS）的新方法。RWS通过使用可达性分类器对目标条件状态-动作值进行训练，将这些值映射到可达性分数，并以此作为采样优先级，从而改进了策略性能。实验表明，RWS在复杂机器人操作任务上显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习依赖于固定数据集，其中许多潜在目标共享相同的状态和动作空间，但这些潜在目标并未明确表示在收集的轨迹中。均匀采样需要处理大量不可达的状态-目标-动作对，这会降低策略性能。

Method: 提出了一种名为可达性加权采样（RWS）的方法，使用通过正未标记（PU）学习训练的可达性分类器，将目标条件状态-动作值映射到可达性分数，并以此作为采样优先级。RWS是一个即插即用模块，可以与标准离线RL算法无缝集成。

Result: 在六个复杂的模拟机器人操作任务上的实验表明，RWS显著提高了性能。特别是在HandBlock-Z任务上，相对于基线提高了近50％。

Conclusion: 实验结果表明，可达性加权采样是一种有效的采样策略，可以显著提高离线目标条件强化学习的性能。

Abstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [122] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg,Marion Neumeier,Sinan Hasirlioglu,Mohamed Essayed Bouzouraa,Michael Botsch*

Main category: cs.LG

TL;DR: 本文提出了一种用于交通场景聚类和场景类别完整性的分析管道，采用CVQ-VAE对高速公路交通场景进行聚类，并探讨了类别数量对完整性的影响。


<details>
  <summary>Details</summary>
Motivation: 为了确保自动驾驶系统（ADS）在复杂交通场景中的安全运行，需要精确理解出现的交通场景。

Method: 引入了一种包含CVQ-VAE的交通场景聚类管道，创建了具有不同交通场景类别的目录，并分析了类别数量对场景类别完整性考虑的影响。

Result: 与先前的工作相比，该方法表现出更优的聚类性能，并讨论了聚类质量和所需数据量之间的权衡。

Conclusion: CVQ-VAE在交通场景聚类方面表现优异，但需要平衡聚类质量和数据需求以维持场景类别的完整性。

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [123] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/abs/2506.02612)
*Jan Robine,Marc Höftmann,Stefan Harmeling*

Main category: cs.LG

TL;DR: This paper presents SGF, a Simple, Good, and Fast world model that doesn't use RNNs, transformers, discrete representations, or image reconstructions. It employs self-supervised representation learning, frame and action stacking, and data augmentation.


<details>
  <summary>Details</summary>
Motivation: To explore the essential components of world models and assess how effective a world model can be without using common techniques such as RNNs, transformers, discrete representations, and image reconstructions.

Method: The method involves creating SGF, which uses self-supervised representation learning to capture short-time dependencies through frame and action stacking, and enhances robustness against model errors through data augmentation.

Result: SGF demonstrates good performance through quantitative comparisons on the Atari 100k benchmark, with extensive discussions on its connections to established world models and evaluations of its building blocks in ablation studies.

Conclusion: SGF is an effective world model despite not using typical techniques like RNNs, transformers, etc., proving that simpler methods can still achieve good results.

Abstract: What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [124] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/abs/2506.02616)
*Qi Liao,Parijat Bhattacharjee*

Main category: cs.LG

TL;DR: This paper introduces two compositional learning methods (CDRL and CPDM) for self-organizing networks, proposing a modular framework to handle agent granularities. Through simulations, these methods show reduced handover failures, better throughput and latency, superior scalability, faster convergence, higher sample efficiency, and safer training compared to conventional approaches.


<details>
  <summary>Details</summary>
Motivation: Self-organizing networks encounter challenges due to complex parameter interdependencies and conflicting objectives.

Method: Two compositional learning approaches are introduced: Compositional Deep Reinforcement Learning (CDRL) and Compositional Predictive Decision-Making (CPDM). A modular, two-tier framework with cell-level and cell-pair-level agents is proposed to manage heterogeneous agent granularities while reducing model complexity.

Result: Numerical simulations indicate significant reductions in handover failures along with improved throughput and latency. The methods also demonstrate superior scalability, faster convergence, higher sample efficiency, and safer training in large-scale self-organizing networks.

Conclusion: The compositional learning approaches outperform conventional multi-agent deep reinforcement learning methods in terms of performance and safety.

Abstract: Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [125] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/abs/2506.02619)
*Yanbei Liu,Chongxu Wang,Zhitao Xiao,Lei Geng,Yanwei Pang,Xiao Wang*

Main category: cs.LG

TL;DR: This paper presents HGOT, a self-supervised heterogeneous graph neural network method using optimal transport to improve learning without graph augmentation, achieving state-of-the-art performance in downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Current self-supervised learning on heterogeneous graphs needs carefully designed graph augmentation strategies and sampling of positive/negative samples which is non-trivial.

Method: Propose HGOT that employs optimal transport mechanism to relieve the laborious sampling process. It designs an aggregating view (central view) for semantic integration and uses an optimal transport plan to identify transport relationship between semantics in different views.

Result: Extensive experiments on four real-world datasets show state-of-the-art performance in various downstream tasks. Specifically, node classification accuracy improves by more than 6% on average compared to existing methods.

Conclusion: HGOT model can achieve excellent performance in downstream tasks without relying on graph augmentation strategies.

Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [126] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou,Ferrante Neri,Yew-Soon Ong,Ruibin Bai*

Main category: cs.LG

TL;DR: The paper introduces SiamNAS, a novel surrogate modeling approach using Siamese network ensembles to predict dominance relationships in neural architecture search (NAS), reducing computational costs while identifying Pareto-optimal solutions.


<details>
  <summary>Details</summary>
Motivation: Modern NAS is multi-objective and computationally expensive, necessitating efficient approximations to balance factors like accuracy, parameter count, and computational cost.

Method: Proposes a surrogate model with an ensemble of Siamese network blocks to predict dominance relationships, integrated into the SiamNAS framework. Replaces crowding distance calculation with a heuristic rule based on model size.

Result: Experiments on NAS-Bench-201 show that SiamNAS can identify Pareto-optimal solutions with significantly reduced computational costs, finding top architectures for CIFAR-10 and ImageNet within 0.01 GPU days.

Conclusion: SiamNAS demonstrates potential for multi-tasking optimisation and generating diverse Pareto-optimal solutions for heterogeneous task settings.

Abstract: Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [127] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/abs/2506.02630)
*Tom Jacobs,Advait Gadhikar,Celia Rubio-Madrigal,Rebekka Burkholz*

Main category: cs.LG

TL;DR: HAM (Hyperbolic Aware Minimization) is proposed to overcome the slow convergence issue caused by hyperbolic implicit bias in deep learning models. It alternates between optimizer steps and hyperbolic mirror steps, improving convergence while maintaining beneficial hyperbolic geometry for feature learning.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper stems from understanding the implicit bias of optimization algorithms in deep learning models. While overparameterization induces a hyperbolic implicit bias that promotes sparsity, it results in a small effective learning rate which slows down convergence.

Method: The method proposed is called HAM (Hyperbolic Aware Minimization). This approach alternates between an optimizer step and a new hyperbolic mirror step. The authors derive the Riemannian gradient flow for its combination with gradient descent. They also provide an interpretation relating it to natural gradient descent and characterize its implicit bias for underdetermined linear regression.

Result: Experiments demonstrate that HAM's implicit bias consistently boosts performance across diverse tasks including vision, graph and node classification, and large language model fine-tuning. It improves upon the state of the art when combined with different sparsification methods.

Conclusion: HAM effectively addresses the slow convergence problem associated with hyperbolic implicit bias. Its minimal computational overhead and compatibility with existing optimizers make it a promising approach for enhancing the performance of deep learning models.

Abstract: Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [128] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/abs/2506.02654)
*Shiyu Shen,Bin Pan,Guirong Xue*

Main category: cs.LG

TL;DR: City-scale traffic volume prediction is crucial for intelligent transportation systems, but incomplete and biased data make it challenging. Existing deep learning methods often ignore uncertainties and lack generalizability across cities. This paper introduces TrafficPPT, a Pretrained Probabilistic Transformer that models traffic volume as a distributional aggregation of trajectories by fusing heterogeneous data sources. It is pretrained on large-scale simulated data and fine-tuned on target cities, showing superior performance especially under extreme data sparsity.


<details>
  <summary>Details</summary>
Motivation: City-scale traffic volume prediction is important for intelligent transportation systems, yet existing methods suffer from incompleteness and bias in observational data, deterministic point estimates neglecting uncertainty, and city-specific training hindering generalizability.

Method: TrafficPPT is a Pretrained Probabilistic Transformer that models traffic volume as a distributional aggregation of trajectories. It fuses real-time observations, historical trajectory data, and road network topology to enable robust and uncertainty-aware traffic inference. The model is pretrained on large-scale simulated data spanning multiple urban scenarios and fine-tuned on target cities for effective domain adaptation.

Result: Experiments on real-world datasets demonstrate that TrafficPPT consistently outperforms state-of-the-art baselines, particularly excelling under conditions of extreme data sparsity.

Conclusion: TrafficPPT addresses the limitations of existing methods by incorporating uncertainty, improving generalizability, and achieving superior performance in city-scale traffic volume prediction.

Abstract: City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [129] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/abs/2506.02665)
*Tianci Liu,Tong Yang,Quan Zhang,Qi Lei*

Main category: cs.LG

TL;DR: 随着AI的发展，版权内容面临着未经授权使用的日益增长的风险。为了解决短期安全问题并提高鲁棒性，本文提出了一种通用方法，将难以去除的可见水印嵌入图像中，并通过实验验证了该方法在多种场景下的优越性。


<details>
  <summary>Details</summary>
Motivation: 当前针对特定AI技术的版权保护方法（如对抗性扰动）只能提供短期安全性，因为每当底层模型架构改变时都需要重新训练。

Method: 本文提出了一种新的框架，将可见且难以去除的水印嵌入图像中。该框架基于概率和逆问题公式化，最大化最优重建与原始内容之间的差异，并开发了一种有效的近似算法来规避复杂的双层优化问题。

Result: 实验结果表明，该方法在各种场景下表现出优越性。

Conclusion: 所提出的方法能够提供长期保护，并具有更好的鲁棒性。

Abstract: As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [130] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/abs/2506.02694)
*Daichi Kimura,Tomonori Izumitani,Hisashi Kashima*

Main category: cs.LG

TL;DR: The paper proposes XicorAttention, a novel attention mechanism using Chatterjee's rank correlation coefficient to measure nonlinear dependencies in time series data. It replaces matrix multiplication in standard attention with this rank coefficient and uses SoftSort/SoftRank for differentiable approximation. Experiments show it improves forecasting accuracy by up to 9.1% compared to existing models.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based models for time series forecasting may not adequately capture inherent nonlinear dependencies present in the data, leaving room for improvement.

Method: The authors propose XicorAttention which integrates Chatterjee's rank correlation coefficient into the attention mechanism of Transformer models. They replace the matrix multiplication in standard attention mechanisms with this rank coefficient to measure the query-key relationship and use SoftSort and SoftRank for differentiable approximation.

Result: Experimental results on real-world datasets demonstrate that incorporating nonlinear correlation into the attention improves forecasting accuracy by up to approximately 9.1% compared to existing models.

Conclusion: XicorAttention, which measures nonlinear dependencies between variables, can be integrated into several state-of-the-art Transformer models for time series forecasting and shows significant improvements.

Abstract: Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [131] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/abs/2506.02703)
*Khizar Hayat,Baptiste Magnier*

Main category: cs.LG

TL;DR: This study examines methodological flaws in credit card fraud detection research, showing simple models can outperform complex ones when evaluation principles are violated. It identifies four critical issues and emphasizes the importance of proper evaluation methodology over model complexity.


<details>
  <summary>Details</summary>
Motivation: To critically examine the methodological rigor in credit card fraud detection research and reveal how fundamental evaluation flaws can overshadow algorithmic sophistication.

Method: Deliberate experimentation with improper evaluation protocols to demonstrate the impact of violating basic methodological principles. Identification of four critical issues in current approaches.

Result: A minimal neural network architecture with data leakage achieved 99.9% recall, outperforming many sophisticated methods reported in literature due to evaluation flaws.

Conclusion: Proper evaluation methodology matters more than model complexity in fraud detection research, highlighting the need for methodological rigor before architectural sophistication.

Abstract: This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [132] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/abs/2506.02712)
*Jayadev Naram,Fredrik Hellström,Ziming Wang,Rebecka Jörnsten,Giuseppe Durisi*

Main category: cs.LG

TL;DR: In this paper, the authors address the partial domain adaptation (PDA) problem by deriving generalization bounds based on partial optimal transport. They propose a practical algorithm named WARMPOT with theoretically motivated weights for empirical source loss. Experiments show that WARMPOT is competitive and improves upon existing schemes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for PDA lack a strong theoretical basis, particularly in terms of weighting schemes for empirical source loss.

Method: The authors derive generalization bounds for PDA using partial optimal transport theory, which justifies the use of the partial Wasserstein distance for domain alignment and provides explicit expressions for source loss weights.

Result: WARMPOT, the proposed algorithm, performs competitively against recent approaches in extensive numerical experiments, demonstrating the effectiveness of the theoretically motivated weights.

Conclusion: This work provides a theoretically grounded approach to PDA, advancing the understanding and performance of domain adaptation techniques.

Abstract: In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [133] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)
*Guanzhong Chen,Shaoxiong Yang,Chao Li,Wei Liu,Jian Luan,Zenglin Xu*

Main category: cs.LG

TL;DR: 提出了一种名为MHGPO的新算法，用于优化基于大型语言模型的多智能体系统，相比现有方法具有更高的稳定性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言处理任务中取得了显著成功，但在实际应用中受到知识更新和生成可控准确输出的限制。而现有的多智能体强化学习算法存在训练不稳定和计算负担重的问题。

Method: 提出了Multi-Agent Heterogeneous Group Policy Optimization (MHGPO)，一种无需Critic网络的算法，通过估计不同滚出组之间的相对奖励优势来指导策略更新，并引入了三种滚出组采样策略以平衡效率和效果。

Result: 实验表明，MHGPO在任务性能和计算效率上均优于MAPPO，并且不需要预热过程，展示了其在优化复杂LLM-based MAS方面的潜力。

Conclusion: MHGPO提供了一个更稳定和可扩展的框架，适用于优化基于大型语言模型的多智能体系统。

Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


### [134] [WeightLoRA: Keep Only Necessary Adapters](https://arxiv.org/abs/2506.02724)
*Andrey Veprikov,Vladimir Solodkin,Alexander Zyl,Andrey Savchenko,Aleksandr Beznosikov*

Main category: cs.LG

TL;DR: This paper proposes WeightLoRA, an improvement over LoRA that adaptively selects the most critical LoRA heads during training to reduce trainable parameters while maintaining or enhancing performance. Experiments on benchmarks with DeBERTa, BART, and Llama models show its effectiveness.


<details>
  <summary>Details</summary>
Motivation: Parameter-Efficient Fine-Tuning techniques like LoRA are essential for utilizing large language models but come with challenges such as high memory requirements and the need for intuition in selecting layers to add adapters.

Method: The proposed method, WeightLoRA, adaptively selects the most important LoRA heads throughout the optimization process. This reduces the number of trainable parameters while achieving consistent or better metric values compared to traditional LoRA.

Result: Experiments conducted using competitive benchmarks and various models (DeBERTa, BART, and Llama) showed the effectiveness of WeightLoRA and the superior performance of WeightLoRA+ in almost all cases.

Conclusion: WeightLoRA successfully addresses the issues of high memory consumption and layer selection in LoRA by adaptively selecting crucial LoRA heads, leading to reduced parameters and enhanced or equal performance.

Abstract: The widespread utilization of language models in modern applications is
inconceivable without Parameter-Efficient Fine-Tuning techniques, such as
low-rank adaptation ($\texttt{LoRA}$), which adds trainable adapters to
selected layers. Although $\texttt{LoRA}$ may obtain accurate solutions, it
requires significant memory to train large models and intuition on which layers
to add adapters. In this paper, we propose a novel method,
$\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the
most critical $\texttt{LoRA}$ heads throughout the optimization process. As a
result, we can significantly reduce the number of trainable parameters while
maintaining the capability to obtain consistent or even superior metric values.
We conduct experiments for a series of competitive benchmarks and DeBERTa,
BART, and Llama models, comparing our method with different adaptive
approaches. The experimental results demonstrate the efficacy of
$\texttt{WeightLoRA}$ and the superior performance of $\texttt{WeightLoRA+}$ in
almost all cases.

</details>


### [135] [Knowledge Graph Completion by Intermediate Variables Regularization](https://arxiv.org/abs/2506.02749)
*Changyi Xiao,Yixin Cao*

Main category: cs.LG

TL;DR: The paper proposes a novel regularization method for tensor decomposition-based models in knowledge graph completion to reduce overfitting, with theoretical analysis and experimental verification.


<details>
  <summary>Details</summary>
Motivation: Knowledge graph completion can be seen as a 3-order binary tensor completion task. Tensor decomposition-based models perform well in this area but are prone to overfitting, and existing regularization methods have limitations.

Method: The authors derive a general form for existing tensor decomposition-based models and propose a new regularization method that minimizes the norms of intermediate variables involved in computing the predicted tensor, promoting low trace norm to mitigate overfitting.

Result: Experiments verify the effectiveness of the proposed regularization technique and support the reliability of the theoretical analysis provided.

Conclusion: A novel regularization method is introduced which improves tensor decomposition-based models' performance in knowledge graph completion by reducing overfitting.

Abstract: Knowledge graph completion (KGC) can be framed as a 3-order binary tensor
completion task. Tensor decomposition-based (TDB) models have demonstrated
strong performance in KGC. In this paper, we provide a summary of existing TDB
models and derive a general form for them, serving as a foundation for further
exploration of TDB models. Despite the expressiveness of TDB models, they are
prone to overfitting. Existing regularization methods merely minimize the norms
of embeddings to regularize the model, leading to suboptimal performance.
Therefore, we propose a novel regularization method for TDB models that
addresses this limitation. The regularization is applicable to most TDB models
and ensures tractable computation. Our method minimizes the norms of
intermediate variables involved in the different ways of computing the
predicted tensor. To support our regularization method, we provide a
theoretical analysis that proves its effect in promoting low trace norm of the
predicted tensor to reduce overfitting. Finally, we conduct experiments to
verify the effectiveness of our regularization technique as well as the
reliability of our theoretical analysis. The code is available at
https://github.com/changyi7231/IVR.

</details>


### [136] [Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection](https://arxiv.org/abs/2506.02757)
*Ruiying Lu,Jinhan Liu,Chuan Du,Dandan Guo*

Main category: cs.LG

TL;DR: This paper addresses the issue of representation entanglement and lack of global correlation modeling in tabular anomaly detection by incorporating mask modeling and prototype learning. The model consists of two parts: encoding with mask modeling in data and projection space, and decoding with association prototypes for reconstruction. It uses optimal transport problems for learning and refines anomaly scores with calibration distances.


<details>
  <summary>Details</summary>
Motivation: Recent deep learning-based methods for tabular anomaly detection have issues with representation entanglement and insufficient global correlation modeling, which affects performance.

Method: The method integrates mask modeling and prototype learning into tabular anomaly detection. It designs learnable masks through disentangled representation learning in a projection space and extracts normal dependencies as explicit global prototypes. The model has two components: (i) encoding with mask modeling in both data and projection spaces using orthogonal basis vectors to learn shared disentangled normal patterns; (ii) decoding multiple masked representations in parallel for reconstruction and learning association prototypes to extract normal characteristic correlations.

Result: Quantitative and qualitative experiments on 20 tabular benchmarks demonstrate the effectiveness and interpretability of the proposed model.

Conclusion: The incorporation of mask modeling and prototype learning in tabular anomaly detection shows promise in improving performance by addressing representation entanglement and enhancing global correlation modeling.

Abstract: Tabular anomaly detection, which aims at identifying deviant samples, has
been crucial in a variety of real-world applications, such as medical disease
identification, financial fraud detection, intrusion monitoring, etc. Although
recent deep learning-based methods have achieved competitive performances,
these methods suffer from representation entanglement and the lack of global
correlation modeling, which hinders anomaly detection performance. To tackle
the problem, we incorporate mask modeling and prototype learning into tabular
anomaly detection. The core idea is to design learnable masks by disentangled
representation learning within a projection space and extracting normal
dependencies as explicit global prototypes. Specifically, the overall model
involves two parts: (i) During encoding, we perform mask modeling in both the
data space and projection space with orthogonal basis vectors for learning
shared disentangled normal patterns; (ii) During decoding, we decode multiple
masked representations in parallel for reconstruction and learn association
prototypes to extract normal characteristic correlations. Our proposal derives
from a distribution-matching perspective, where both projection space learning
and association prototype learning are formulated as optimal transport
problems, and the calibration distances are utilized to refine the anomaly
scores. Quantitative and qualitative experiments on 20 tabular benchmarks
demonstrate the effectiveness and interpretability of our model.

</details>


### [137] [Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization](https://arxiv.org/abs/2506.02767)
*Marco Calì,Giulio Giacomuzzo,Ruggero Carli,Alberto Dalla Libera*

Main category: cs.LG

TL;DR: This paper proposes Exploration-Boosted MC-PILCO (EB-MC-PILCO), which integrates MC-PILCO with iLQR to accelerate policy optimization convergence. Experiments on the cart-pole task show that EB-MC-PILCO reduces execution time by up to 45.9% compared to standard MC-PILCO, while maintaining a 100% success rate.


<details>
  <summary>Details</summary>
Motivation: MC-PILCO, a state-of-the-art model-based reinforcement learning algorithm, suffers from slow policy optimization convergence. To address this issue and improve efficiency, the authors aim to enhance MC-PILCO by incorporating a fast trajectory optimization method.

Method: The proposed method, Exploration-Boosted MC-PILCO (EB-MC-PILCO), combines MC-PILCO with iterative Linear Quadratic Regulator (iLQR). By leveraging iLQR, the method generates exploratory trajectories and initializes the policy, thereby reducing the number of required optimization steps for MC-PILCO.

Result: Experiments on the cart-pole task demonstrate that EB-MC-PILCO accelerates convergence compared to standard MC-PILCO. It achieves up to 45.9% reduction in execution time when both methods solve the task in four trials. Additionally, EB-MC-PILCO maintains a 100% success rate across trials, even in cases where MC-PILCO converges in fewer iterations.

Conclusion: By integrating iLQR into MC-PILCO, the proposed EB-MC-PILCO method effectively addresses the slow convergence issue of MC-PILCO. This results in significant reductions in execution time without compromising the success rate.

Abstract: This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.

</details>


### [138] [CART-based Synthetic Tabular Data Generation for Imbalanced Regression](https://arxiv.org/abs/2506.02811)
*António Pedro Pinheiro,Rita P. Ribeiro*

Main category: cs.LG

TL;DR: This paper proposes adapting a CART-based synthetic data generation method for imbalanced regression, integrating relevance and density-based mechanisms to guide sampling in sparse regions without artificial thresholds. It offers competitive performance with faster execution and greater transparency.


<details>
  <summary>Details</summary>
Motivation: Imbalanced target distributions in regression tasks pose challenges in tabular data settings. Existing solutions either rely on artificial thresholds or come with high computational costs and limited interpretability.

Method: Adapting an existing CART-based synthetic data generation method for imbalanced regression by integrating relevance and density-based mechanisms to guide sampling in sparse regions of the target space, using a threshold-free, feature-driven generation process.

Result: The proposed method is competitive with other resampling and generative strategies in terms of performance while offering faster execution and greater transparency when predicting extreme target values across benchmark datasets.

Conclusion: The method shows potential as a transparent, scalable data-level strategy for improving regression models in imbalanced domains.

Abstract: Handling imbalanced target distributions in regression tasks remains a
significant challenge in tabular data settings where underrepresented regions
can hinder model performance. Among data-level solutions, some proposals, such
as random sampling and SMOTE-based approaches, propose adapting classification
techniques to regression tasks. However, these methods typically rely on crisp,
artificial thresholds over the target variable, a limitation inherited from
classification settings that can introduce arbitrariness, often leading to
non-intuitive and potentially misleading problem formulations. While recent
generative models, such as GANs and VAEs, provide flexible sample synthesis,
they come with high computational costs and limited interpretability. In this
study, we propose adapting an existing CART-based synthetic data generation
method, tailoring it for imbalanced regression. The new method integrates
relevance and density-based mechanisms to guide sampling in sparse regions of
the target space and employs a threshold-free, feature-driven generation
process. Our experimental study focuses on the prediction of extreme target
values across benchmark datasets. The results indicate that the proposed method
is competitive with other resampling and generative strategies in terms of
performance, while offering faster execution and greater transparency. These
results highlight the method's potential as a transparent, scalable data-level
strategy for improving regression models in imbalanced domains.

</details>


### [139] [Sheaves Reloaded: A Directional Awakening](https://arxiv.org/abs/2506.02842)
*Stefano Fiorini,Hakan Aktas,Iulia Duta,Stefano Coniglio,Pietro Morerio,Alessio Del Bue,Pietro Liò*

Main category: cs.LG

TL;DR: Sheaf Neural Networks (SNNs) are a powerful generalization of Graph Neural Networks (GNNs). However, existing SNNs do not adequately represent directionality in graphs. To solve this problem, the paper introduces the Directed Cellular Sheaf and the Directed Sheaf Laplacian, which capture both graph topology and directional information. Based on these concepts, the Directed Sheaf Neural Network (DSNN) is proposed as the first SNN model to incorporate a directional bias into its architecture.


<details>
  <summary>Details</summary>
Motivation: Directionality plays an important role in graph learning tasks and many real-world applications, but current SNNs lack the ability to properly represent it.

Method: The authors define the Directed Cellular Sheaf and the Directed Sheaf Laplacian to explicitly account for edge orientation in graphs. They then use the Directed Sheaf Laplacian as the backbone to build the DSNN model.

Result: Experiments on nine real-world benchmarks demonstrate that DSNN consistently outperforms baseline methods.

Conclusion: The introduction of the Directed Cellular Sheaf and the Directed Sheaf Laplacian addresses the limitation of representing directionality in SNNs. The DSNN model effectively incorporates directional bias and shows superior performance.

Abstract: Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph
Neural Networks (GNNs) that significantly improve our ability to model complex
relational data. While directionality has been shown to substantially boost
performance in graph learning tasks and is key to many real-world applications,
existing SNNs fall short in representing it. To address this limitation, we
introduce the Directed Cellular Sheaf, a special type of cellular sheaf
designed to explicitly account for edge orientation. Building on this
structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which
captures both the graph's topology and its directional information. This
operator serves as the backbone of the Directed Sheaf Neural Network (DSNN),
the first SNN model to embed a directional bias into its architecture.
Extensive experiments on nine real-world benchmarks show that DSNN consistently
outperforms baseline methods.

</details>


### [140] [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/abs/2506.02864)
*Changyi Xiao,Mengdi Zhang,Yixin Cao*

Main category: cs.LG

TL;DR: Recent studies show reinforcement learning can improve reasoning capabilities of large language models, but current policy optimization methods have limitations. This paper proposes Beta Normalization Policy Optimization (BNPO) to address these issues and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of current policy optimization methods that either neglect reward normalization or use static normalization strategies, leading to unstable gradient estimates and hindering training stability.

Method: Propose BNPO, which adaptively normalizes rewards using a Beta distribution with dynamically updated parameters. It aligns normalization with the changing policy distribution for more precise gradient estimation. Also introduce an advantage decomposition mechanism to extend BNPO's applicability.

Result: Theoretical analysis shows BNPO's variance-reducing properties and its generalization of REINFORCE and GRPO under binary-valued reward settings. Experimental results confirm BNPO achieves state-of-the-art performance among policy optimization methods on reasoning tasks.

Conclusion: BNPO addresses the limitations of current policy optimization methods by providing adaptive reward normalization, promoting stable training dynamics, and achieving superior performance in reasoning tasks.

Abstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that
reinforcement learning with rule-based, binary-valued reward functions can
significantly enhance the reasoning capabilities of large language models.
These models primarily utilize REINFORCE-based policy optimization techniques,
such as REINFORCE with baseline and group relative policy optimization (GRPO).
However, a key limitation remains: current policy optimization methods either
neglect reward normalization or employ static normalization strategies, which
fail to adapt to the dynamic nature of policy updates during training. This may
result in unstable gradient estimates and hinder training stability. To address
this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel
policy optimization method that adaptively normalizes rewards using a Beta
distribution with dynamically updated parameters. BNPO aligns the normalization
with the changing policy distribution, enabling more precise and lower-variance
gradient estimation, which in turn promotes stable training dynamics. We
provide theoretical analysis demonstrating BNPO's variance-reducing properties
and show that it generalizes both REINFORCE and GRPO under binary-valued reward
settings. Furthermore, we introduce an advantage decomposition mechanism to
extend BNPO's applicability to more complex reward systems. Experimental
results confirm that BNPO achieves state-of-the-art performance among policy
optimization methods on reasoning tasks. The code is available at
https://github.com/changyi7231/BNPO.

</details>


### [141] [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.02883)
*Anthony Kobanda,Odalric-Ambrym Maillard,Rémy Portelas*

Main category: cs.LG

TL;DR: The paper introduces a benchmark for continual reinforcement learning in video-game navigation scenarios, addressing challenges like catastrophic forgetting, task adaptation, and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: There is a need for autonomous agents to adapt to changing tasks without forgetting previous ones in domains such as robotics or video game simulations. Current literature lacks a comprehensive benchmark for evaluating continual reinforcement learning algorithms in these contexts.

Method: The authors developed a benchmark consisting of various video-game navigation scenarios, tasks, datasets, evaluation protocols, and metrics. This benchmark aims to assess the performance of continual reinforcement learning algorithms and includes state-of-the-art baselines.

Result: The introduced benchmark successfully captures key challenges in continual reinforcement learning, such as catastrophic forgetting, task adaptation, and memory efficiency. It provides a reproducible framework for both research and practical applications in gaming environments.

Conclusion: This benchmark will help accelerate progress in continual reinforcement learning by enabling reproducible research and providing a standardized framework for evaluating and applying effective approaches.

Abstract: Autonomous agents operating in domains such as robotics or video game
simulations must adapt to changing tasks without forgetting about the previous
ones. This process called Continual Reinforcement Learning poses non-trivial
difficulties, from preventing catastrophic forgetting to ensuring the
scalability of the approaches considered. Building on recent advances, we
introduce a benchmark providing a suite of video-game navigation scenarios,
thus filling a gap in the literature and capturing key challenges :
catastrophic forgetting, task adaptation, and memory efficiency. We define a
set of various tasks and datasets, evaluation protocols, and metrics to assess
the performance of algorithms, including state-of-the-art baselines. Our
benchmark is designed not only to foster reproducible research and to
accelerate progress in continual reinforcement learning for gaming, but also to
provide a reproducible framework for production pipelines -- helping
practitioners to identify and to apply effective approaches.

</details>


### [142] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/abs/2506.02887)
*Mrinmay Sen,Shruti Aparna,Rohit Agarwal,Chalavadi Krishna Mohan*

Main category: cs.LG

TL;DR: 本论文探讨了联邦学习中部分客户端参与的影响，提供了现有方法的深入评论、理论见解和实证研究，并对这些方法进行了分类。


<details>
  <summary>Details</summary>
Motivation: 尽管许多现有研究关注数据异质性带来的问题，但很少有研究涉及部分客户端参与所引发的实际和理论挑战，而这一情况在现实场景中很常见。

Method: 本论文通过提供全面的分析，结合理论见解和实证发现，对现有的联邦学习方法进行深入审查，并对其进行结构化分类，指出各自的优势和劣势。

Result: 该调查为读者提供了关于处理部分客户端参与的现有联邦学习方法的深入理解，以及它们的优缺点。

Conclusion: 部分客户端参与是联邦学习中的一个重要议题，需要进一步的研究来解决相关的实际和理论挑战。

Abstract: Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [143] [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
*Jakub Krajewski,Marcin Chochowski,Daniel Korzekwa*

Main category: cs.LG

TL;DR: Mixture of Experts (MoE) architectures are crucial for Large Language Models (LLMs). Fine-grained MoE approaches show potential in improving model convergence and quality. This study proposes training recipes and evaluates fine-grained MoE against standard MoE configurations for models with up to 56B parameters, showing that fine-grained MoE achieves better validation loss and higher accuracy on downstream benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore the effectiveness of fine-grained MoE approaches in comparison to standard MoE configurations for large-scale models.

Method: Propose a set of training recipes and conduct a comprehensive empirical evaluation of fine-grained MoE by directly comparing its scaling properties against standard MoE configurations for models with up to 56B total (17B active) parameters.

Result: Fine-grained MoE achieves better validation loss and higher accuracy across a set of downstream benchmarks at the largest scale.

Conclusion: This study provides empirical evidence and practical insights for using fine-grained MoE in the development of future large-scale models.

Abstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling
Large Language Models (LLMs) efficiently. Fine-grained MoE approaches -
utilizing more numerous, smaller experts - have demonstrated potential in
improving model convergence and quality. This work proposes a set of training
recipes and provides a comprehensive empirical evaluation of fine-grained MoE,
directly comparing its scaling properties against standard MoE configurations
for models with up to 56B total (17B active) parameters. We investigate
convergence speed, model performance on downstream benchmarks, and practical
training considerations across various setups. Overall, at the largest scale we
show that fine-grained MoE achieves better validation loss and higher accuracy
across a set of downstream benchmarks. This study offers empirical grounding
and practical insights for leveraging fine-grained MoE in the development of
future large-scale models.

</details>


### [144] [Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning](https://arxiv.org/abs/2506.02897)
*Alessandro Licciardi,Roberta Raineri,Anton Proskurnikov,Lamberto Rondoni,Lorenzo Zino*

Main category: cs.LG

TL;DR: In this paper, a new algorithm named Federated Coalition Variance Reduction with Boltzmann Exploration is proposed to solve the problem of data heterogeneity in Federated Learning by using an approach inspired by opinion dynamics over temporal social networks.


<details>
  <summary>Details</summary>
Motivation: Federated Learning often suffers from client data heterogeneity which undermines its practical strength and degrades model performance.

Method: The paper introduces Federated Coalition Variance Reduction with Boltzmann Exploration, where clients are dynamically organized into non-overlapping clusters based on asymptotic agreements. Then one client is selected from each cluster to minimize the expected variance of its model update.

Result: Experiments show that in heterogeneous scenarios, the proposed algorithm outperforms existing FL algorithms providing more accurate results and faster convergence.

Conclusion: The proposed variance-reducing selection algorithm effectively addresses data heterogeneity across clients' distributions.

Abstract: Federated Learning (FL) enables privacy-preserving collaborative model
training, yet its practical strength is often undermined by client data
heterogeneity, which severely degrades model performance. This paper proposes
that data heterogeneity across clients' distributions can be effectively
addressed by adopting an approach inspired by opinion dynamics over temporal
social networks. We introduce \shortname (Federated Coalition Variance
Reduction with Boltzmann Exploration), a variance-reducing selection algorithm
in which (1) clients dynamically organize into non-overlapping clusters based
on asymptotic agreements, and (2) from each cluster, one client is selected to
minimize the expected variance of its model update. Our experiments show that
in heterogeneous scenarios our algorithm outperforms existing FL algorithms,
yielding more accurate results and faster convergence, validating the efficacy
of our approach.

</details>


### [145] [From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation](https://arxiv.org/abs/2506.02933)
*Junyi Fang,Yuxun Chen,Yuxin Chen,Chen Zhang*

Main category: cs.LG

TL;DR: In non-stationary environments, the Multi-Armed Bandit problem is challenging due to dynamically evolving reward distributions. The paper introduces RAVEN-UCB, an algorithm that merges theoretical rigor with practical efficiency through variance-aware adaptation. It outperforms UCB1 and UCB-V in terms of regret bounds and demonstrates superiority in experiments involving various non-stationary patterns.


<details>
  <summary>Details</summary>
Motivation: The motivation is addressing the challenge posed by non-stationary environments in Multi-Armed Bandit problems where reward distributions change dynamically.

Method: RAVEN-UCB incorporates three key innovations: (1) variance-driven exploration using a specific formula for confidence bounds, (2) adaptive control via a time-dependent parameter alpha_t, and (3) constant-time recursive updates for efficiency.

Result: Experiments across different non-stationary patterns in synthetic and logistics scenarios show its superiority over state-of-the-art baselines, confirming both theoretical and practical robustness.

Conclusion: RAVEN-UCB achieves tighter regret bounds than UCB1 and UCB-V, demonstrating both theoretical and practical efficiency.

Abstract: The Multi-Armed Bandit (MAB) problem is challenging in non-stationary
environments where reward distributions evolve dynamically. We introduce
RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical
efficiency via variance-aware adaptation. It achieves tighter regret bounds
than UCB1 and UCB-V, with gap-dependent regret of order $K \sigma_{\max}^2 \log
T / \Delta$ and gap-independent regret of order $\sqrt{K T \log T}$. RAVEN-UCB
incorporates three innovations: (1) variance-driven exploration using
$\sqrt{\hat{\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive
control via $\alpha_t = \alpha_0 / \log(t + \epsilon)$, and (3) constant-time
recursive updates for efficiency. Experiments across non-stationary patterns -
distributional changes, periodic shifts, and temporary fluctuations - in
synthetic and logistics scenarios demonstrate its superiority over
state-of-the-art baselines, confirming theoretical and practical robustness.

</details>


### [146] [MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver](https://arxiv.org/abs/2506.02935)
*Yuepeng Zheng,Fu Luo,Zhenkun Wang,Yaoxin Wu,Yu Zhou*

Main category: cs.LG

TL;DR: An improved Multi-Task Learning (MTL) method called MTL-KD is introduced, which uses knowledge distillation to train heavy decoder models for solving Vehicle Routing Problem (VRP) variants. It also introduces an inference strategy R3C. Experiments show superior performance and robust generalization on both seen and unseen VRP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based multi-task methods in Neural Combinatorial Optimization can only train light decoder models on small-scale problems with limited generalization ability for large-scale problems.

Method: The paper proposes a novel MTL method driven by knowledge distillation (MTL-KD) to train heavy decoder models with strong generalization ability. It transfers policy knowledge from multiple single-task models to one heavy decoder model, enabling label-free training. Additionally, it introduces the Random Reordering Re-Construction (R3C) inference strategy adapted for diverse VRP tasks.

Result: Experimental results on 6 seen and 10 unseen VRP variants with up to 1000 nodes indicate that the proposed method achieves superior performance on both uniform and real-world benchmarks, demonstrating robust generalization abilities.

Conclusion: The MTL-KD method combined with the R3C inference strategy effectively improves the generalization ability of multi-task models in solving diverse VRP tasks.

Abstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a
promising approach to train a unified model capable of solving multiple Vehicle
Routing Problem (VRP) variants. However, existing Reinforcement Learning
(RL)-based multi-task methods can only train light decoder models on
small-scale problems, exhibiting limited generalization ability when solving
large-scale problems. To overcome this limitation, this work introduces a novel
multi-task learning method driven by knowledge distillation (MTL-KD), which
enables the efficient training of heavy decoder models with strong
generalization ability. The proposed MTL-KD method transfers policy knowledge
from multiple distinct RL-based single-task models to a single heavy decoder
model, facilitating label-free training and effectively improving the model's
generalization ability across diverse tasks. In addition, we introduce a
flexible inference strategy termed Random Reordering Re-Construction (R3C),
which is specifically adapted for diverse VRP tasks and further boosts the
performance of the multi-task model. Experimental results on 6 seen and 10
unseen VRP variants with up to 1000 nodes indicate that our proposed method
consistently achieves superior performance on both uniform and real-world
benchmarks, demonstrating robust generalization abilities.

</details>


### [147] [QKV Projections Require a Fraction of Their Memory](https://arxiv.org/abs/2506.02939)
*Malik Khalaf,Yara Shamshoum,Nitzan Hodos,Yuval Sieradzki,Assaf Schuster*

Main category: cs.LG

TL;DR: 提出了一种名为Point-Approximate Matrix Multiplication (PAMM)的新张量压缩技术，可以减少注意力层中$Q,K,V$投影的内存消耗高达512倍，同时实现相似或更好的最终困惑度。PAMM与高效的注意力机制（如FlashAttention）完全兼容，是一种实用且互补的内存高效LLM训练方法。


<details>
  <summary>Details</summary>
Motivation: 尽管许多研究关注于训练期间计算和内存效率的提升，但大多数工作主要集中在近似缩放点积上，而忽略了从输入$x$计算$Q$、$K$和$V$张量的线性投影所消耗的内存。

Method: 提出了Point-Approximate Matrix Multiplication (PAMM)，一种新的张量压缩技术，用于减少注意力层中$Q,K,V$投影的内存消耗。该技术可将内存消耗降低高达512倍，并与高效的注意力机制（如FlashAttention）完全兼容。

Result: 通过使用PAMM技术，可以有效消除$Q,K,V$投影的内存占用，同时实现相似或更好的最终困惑度。

Conclusion: PAMM作为一种实用且互补的方法，能够显著提高内存效率，适用于大语言模型的训练过程。

Abstract: The Multi-Head Attention mechanism is central to LLM operation, and multiple
works target its compute and memory efficiency during training. While most
works focus on approximating the scaled dot product, the memory consumption of
the linear projections that compute the $Q$, $K$, and $V$ tensors from the
input $x$ is often overlooked. To address this, we propose Point-Approximate
Matrix Multiplication (PAMM), a novel tensor compression technique that reduces
memory consumption of the $Q,K,V$ projections in attention layers by a factor
of up to $\times 512$, effectively erasing their memory footprint, while
achieving similar or better final perplexity. PAMM is fully composable with
efficient attention techniques such as FlashAttention, making it a practical
and complementary method for memory-efficient LLM training.

</details>


### [148] [Abstract Counterfactuals for Language Model Agents](https://arxiv.org/abs/2506.02946)
*Edoardo Pona,Milad Kazemi,Yali Du,David Watson,Nicola Paoletti*

Main category: cs.LG

TL;DR: The paper introduces Abstract Counterfactuals, a framework for counterfactual reasoning in language model agents that focuses on high-level action characteristics, producing meaningful results with fewer side effects than token-level methods.


<details>
  <summary>Details</summary>
Motivation: Counterfactual inference is crucial for evaluating autonomous agents, but its application to language model agents is challenging due to their open-ended action spaces and context-dependent token meanings.

Method: The authors propose the Abstract Counterfactuals framework which emphasizes high-level characteristics of actions and interactions within an environment, enabling more tailored and meaningful counterfactual reasoning.

Result: Experiments on text-based games and counterfactual text generation show that the approach generates consistent and meaningful counterfactuals while minimizing the negative side effects associated with token-level methods.

Conclusion: Abstract Counterfactuals provide a promising method for effective counterfactual reasoning in language model agents by focusing on high-level features rather than individual tokens.

Abstract: Counterfactual inference is a powerful tool for analysing and evaluating
autonomous agents, but its application to language model (LM) agents remains
challenging. Existing work on counterfactuals in LMs has primarily focused on
token-level counterfactuals, which are often inadequate for LM agents due to
their open-ended action spaces. Unlike traditional agents with fixed, clearly
defined action spaces, the actions of LM agents are often implicit in the
strings they output, making their action spaces difficult to define and
interpret. Furthermore, the meanings of individual tokens can shift depending
on the context, adding complexity to token-level reasoning and sometimes
leading to biased or meaningless counterfactuals. We introduce \emph{Abstract
Counterfactuals}, a framework that emphasises high-level characteristics of
actions and interactions within an environment, enabling counterfactual
reasoning tailored to user-relevant features. Our experiments demonstrate that
the approach produces consistent and meaningful counterfactuals while
minimising the undesired side effects of token-level methods. We conduct
experiments on text-based games and counterfactual text generation, while
considering both token-level and latent-space interventions.

</details>


### [149] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/abs/2506.02950)
*Stepan I. Manukhov,Alexander Kolesov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

TL;DR: This paper proposes Interaction Field Matching (IFM), an extension of Electrostatic Field Matching (EFM) that uses general interaction fields for data generation and transfer, overcoming the complexities of modeling electrostatic fields.


<details>
  <summary>Details</summary>
Motivation: Electrostatic Field Matching (EFM) is a new paradigm inspired by physics for data generation and transfer but faces challenges in modeling electrostatic fields using neural networks due to complex fields outside capacitor plates.

Method: The authors propose Interaction Field Matching (IFM) as a generalization of EFM, allowing the use of general interaction fields beyond just electrostatic ones. They also design a specific interaction field inspired by strong interactions between quarks and antiquarks in physics.

Result: IFM solves the problems arising from modeling electrostatic fields in EFM and demonstrates performance on a series of toy and image data transfer problems.

Conclusion: Interaction Field Matching (IFM) extends the capabilities of Electrostatic Field Matching (EFM) by enabling the use of more general interaction fields, improving data generation and transfer processes.

Abstract: Electrostatic field matching (EFM) has recently appeared as a novel
physics-inspired paradigm for data generation and transfer using the idea of an
electric capacitor. However, it requires modeling electrostatic fields using
neural networks, which is non-trivial because of the necessity to take into
account the complex field outside the capacitor plates. In this paper, we
propose Interaction Field Matching (IFM), a generalization of EFM which allows
using general interaction fields beyond the electrostatic one. Furthermore,
inspired by strong interactions between quarks and antiquarks in physics, we
design a particular interaction field realization which solves the problems
which arise when modeling electrostatic fields in EFM. We show the performance
on a series of toy and image data transfer problems.

</details>


### [150] [Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/abs/2506.02965)
*Ze Yu Zhang,Bolin Ding,Bryan Kian Hsiang Low*

Main category: cs.LG

TL;DR: The paper introduces Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE) for efficient decentralized LLM training with privacy protection.


<details>
  <summary>Details</summary>
Motivation: To enable multiple parties with limited resources to collaboratively train more capable LLMs while preserving the privacy of each participant's training data.

Method: Leverage the sparsity of MoE architecture for memory-efficient decentralized collaborative LLM training, keeping training data and parts of forward pass signal and gradients locally within each party.

Result: Across seven popular LLM benchmarks, PC-MoE almost matches(and sometimes exceeds)the performance and convergence rate of a fully centralized model,enjoys near 70% peak GPU RAM reduction,while being fully robust against reconstruction attacks.

Conclusion: PC-MoE successfully combines distributed computation with strong confidentiality assurances without compromising task accuracy.

Abstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


### [151] [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](https://arxiv.org/abs/2506.02972)
*Md-Ferdous Pervej,Richeng Jin,Md Moin Uddin Chowdhury,Simran Singh,İsmail Güvenç,Huaiyu Dai*

Main category: cs.LG

TL;DR: Privacy-preserving distributed machine learning and aerial connected vehicle-assisted edge computing are important. A new algorithm 2CEOAFL is proposed for efficient online aerial federated learning, which prunes the ML model, trains the pruned model, and probabilistically quantizes and offloads the trained gradients. The simulation results show that this algorithm performs as well as non-pruned and non-quantized counterparts.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of privacy-preserving in distributed machine learning and to utilize the benefits of continual sensed data and limited onboard resources of aerial connected vehicles (ACVs) while considering their resource constraints.

Method: Propose a computation- and communication-efficient online aerial federated learning (2CEOAFL) algorithm. This includes modeling ACV trajectories according to time-varying data distributions, pruning the received dense ML model, training the pruned model, and probabilistically quantizing and offloading the trained accumulated gradients to the central server.

Result: The extensive simulation results demonstrate that the proposed 2CEOAFL algorithm achieves comparable performance to its non-pruned and non-quantized versions, but with greater computation and communication efficiency.

Conclusion: The 2CEOAFL algorithm effectively balances the trade-off between performance and resource consumption in aerial connected vehicle-assisted edge computing scenarios.

Abstract: Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically quantize and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and nonquantized, hence, computation- and communication-inefficient
counterparts.

</details>


### [152] [On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](https://arxiv.org/abs/2506.02978)
*Mohamed Djilani,Thibault Simonetto,Karim Tit,Florian Tambon,Paul Récamier,Salah Ghamizi,Maxime Cordy,Mike Papadakis*

Main category: cs.LG

TL;DR: Recent tabular Foundational Models (FM) are vulnerable to adversarial manipulation, with small input changes degrading accuracy and potential misuse as adversarial tools. This study explores these vulnerabilities on finance, cybersecurity, and healthcare benchmarks, proposing an in-context adversarial training strategy to improve robustness.


<details>
  <summary>Details</summary>
Motivation: To investigate the adversarial vulnerabilities of tabular Foundational Models (FM), focusing on their fragility to targeted test-time attacks and potential misuse as adversarial tools.

Method: The study demonstrates adversarial vulnerabilities by applying small, structured perturbations to test inputs on three benchmarks in finance, cybersecurity, and healthcare. It also shows that tabular FM can be repurposed to generate transferable evasion for conventional models. To address these vulnerabilities, the paper introduces an in-context adversarial training strategy that replaces context with adversarial perturbed instances without updating model weights.

Result: Small, structured perturbations significantly degrade prediction accuracy of tabular FM even when training context remains fixed. Tabular FM can generate transferable evasions for conventional models like random forests and XGBoost. The proposed in-context adversarial training strategy improves robustness across multiple tabular benchmarks.

Conclusion: Tabular FM are both a target and source of adversarial threats. There is an urgent need for robust training and evaluation practices in this emerging paradigm.

Abstract: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage
in-context learning to achieve strong performance without gradient updates or
fine-tuning. However, their robustness to adversarial manipulation remains
largely unexplored. In this work, we present a comprehensive study of the
adversarial vulnerabilities of tabular FM, focusing on both their fragility to
targeted test-time attacks and their potential misuse as adversarial tools. We
show on three benchmarks in finance, cybersecurity and healthcare, that small,
structured perturbations to test inputs can significantly degrade prediction
accuracy, even when training context remain fixed. Additionally, we demonstrate
that tabular FM can be repurposed to generate transferable evasion to
conventional models such as random forests and XGBoost, and on a lesser extent
to deep tabular models. To improve tabular FM, we formulate the robustification
problem as an optimization of the weights (adversarial fine-tuning), or the
context (adversarial in-context learning). We introduce an in-context
adversarial training strategy that incrementally replaces the context with
adversarial perturbed instances, without updating model weights. Our approach
improves robustness across multiple tabular benchmarks. Together, these
findings position tabular FM as both a target and a source of adversarial
threats, highlighting the urgent need for robust training and evaluation
practices in this emerging paradigm.

</details>


### [153] [Implicit Regularization of the Deep Inverse Prior Trained with Inertia](https://arxiv.org/abs/2506.02986)
*Nathan Buskulic,Jalal Fadil,Yvain Quéau*

Main category: cs.LG

TL;DR: This paper provides convergence and recovery guarantees for self-supervised neural networks used in solving inverse problems, demonstrating optimal accelerated exponential convergence rate in continuous-time case and less sharp linear convergence rate in discrete case.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide theoretical guarantees for the recovery of inverse problems using self-supervised neural networks, addressing a gap in existing literature.

Method: The method involves analyzing both continuous-time (dynamical system trajectory) and discrete cases (inertial algorithm with adaptive step-size), incorporating inertia with viscous and geometric Hessian-driven dampings.

Result: In the continuous-time case, the network can be trained with an optimal accelerated exponential convergence rate compared to gradient flow. In the discrete case, training with the inertial algorithm has similar recovery guarantees but with a less sharp linear convergence rate.

Conclusion: The study establishes strong theoretical foundations for using self-supervised neural networks in inverse problems, showing advantages of inertia-based methods in terms of convergence and recovery.

Abstract: Solving inverse problems with neural networks benefits from very few
theoretical guarantees when it comes to the recovery guarantees. We provide in
this work convergence and recovery guarantees for self-supervised neural
networks applied to inverse problems, such as Deep Image/Inverse Prior, and
trained with inertia featuring both viscous and geometric Hessian-driven
dampings. We study both the continuous-time case, i.e., the trajectory of a
dynamical system, and the discrete case leading to an inertial algorithm with
an adaptive step-size. We show in the continuous-time case that the network can
be trained with an optimal accelerated exponential convergence rate compared to
the rate obtained with gradient flow. We also show that training a network with
our inertial algorithm enjoys similar recovery guarantees though with a less
sharp linear convergence rate.

</details>


### [154] [Protein Inverse Folding From Structure Feedback](https://arxiv.org/abs/2506.03028)
*Junde Xu,Zijun Gao,Xinyi Zhou,Jie Hu,Xingyi Cheng,Le Song,Guangyong Chen,Pheng-Ann Heng,Jiezhong Qiu*

Main category: cs.LG

TL;DR: The paper presents a new method using Direct Preference Optimization (DPO) to improve the design of amino acid sequences for desired protein structures, showing significant improvements in sequence recovery and structure similarity.


<details>
  <summary>Details</summary>
Motivation: To advance biotechnological applications by solving the inverse folding problem, which involves designing amino acid sequences that fold into desired 3D structures.

Method: Employ Direct Preference Optimization (DPO) to fine-tune an inverse folding model with feedback from a protein folding model. Sample candidate sequences, predict their 3D structures, generate pairwise structural-preference labels, and use these to fine-tune the inverse-folding model under the DPO objective.

Result: On the CATH 4.2 test set, DPO fine-tuning improved sequence recovery and increased average TM-Score from 0.77 to 0.81. Iterative application on challenging protein structures resulted in a 79.5% average TM-Score increase compared to the baseline model.

Conclusion: This approach offers a promising way to enhance protein sequence design through effective utilization of preference optimization based on structure feedback.

Abstract: The inverse folding problem, aiming to design amino acid sequences that fold
into desired three-dimensional structures, is pivotal for various
biotechnological applications. Here, we introduce a novel approach leveraging
Direct Preference Optimization (DPO) to fine-tune an inverse folding model
using feedback from a protein folding model. Given a target protein structure,
we begin by sampling candidate sequences from the inverse-folding model, then
predict the three-dimensional structure of each sequence with the folding model
to generate pairwise structural-preference labels. These labels are used to
fine-tune the inverse-folding model under the DPO objective. Our results on the
CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence
recovery of baseline models but also leads to a significant improvement in
average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.
Furthermore, iterative application of our DPO-based method on challenging
protein structures yields substantial gains, with an average TM-Score increase
of 79.5\% with regard to the baseline model. This work establishes a promising
direction for enhancing protein sequence design ability from structure feedback
by effectively utilizing preference optimization.

</details>


### [155] [On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning](https://arxiv.org/abs/2506.03037)
*Shubhendu Trivedi,Brian D. Nord*

Main category: cs.LG

TL;DR: Quantifying uncertainties in machine learning models is challenging due to inconsistent terminology and technical requirements. This paper identifies inconsistencies, explains problematic mappings, advocates for standards in uncertainty quantification, and provides practical recommendations focusing on scientific ML.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the foundational challenge of quantifying uncertainties in machine learning models by clarifying inconsistencies in terminology and articulating the distinct epistemic demands imposed by different contexts.

Method: The method involves examining the current landscape of estimation targets, uncertainty constructs, and approaches used to map between them. The paper highlights examples of problematic mappings and discusses necessary axes of trustworthiness for reliable uncertainty quantification in ML models.

Result: The paper identifies inconsistencies in uncertainty quantification, explains problematic mappings, and outlines necessary aspects for trustworthy uncertainty quantification.

Conclusion: The conclusion emphasizes the need for standards that promote alignment between the intent and implementation of uncertainty quantification approaches in machine learning, with practical recommendations focusing on scientific ML.

Abstract: Quantifying uncertainties for machine learning (ML) models is a foundational
challenge in modern data analysis. This challenge is compounded by at least two
key aspects of the field: (a) inconsistent terminology surrounding uncertainty
and estimation across disciplines, and (b) the varying technical requirements
for establishing trustworthy uncertainties in diverse problem contexts. In this
position paper, we aim to clarify the depth of these challenges by identifying
these inconsistencies and articulating how different contexts impose distinct
epistemic demands. We examine the current landscape of estimation targets
(e.g., prediction, inference, simulation-based inference), uncertainty
constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to
map between them. Drawing on the literature, we highlight and explain examples
of problematic mappings. To help address these issues, we advocate for
standards that promote alignment between the \textit{intent} and
\textit{implementation} of uncertainty quantification (UQ) approaches. We
discuss several axes of trustworthiness that are necessary (if not sufficient)
for reliable UQ in ML models, and show how these axes can inform the design and
evaluation of uncertainty-aware ML systems. Our practical recommendations focus
on scientific ML, offering illustrative cases and use scenarios, particularly
in the context of simulation-based inference (SBI).

</details>


### [156] [Sample complexity of Schrödinger potential estimation](https://arxiv.org/abs/2506.03043)
*Nikita Puchkin,Iurii Pustovalov,Yuri Sapronov,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: The paper addresses Schrödinger potential estimation for generative modelling, showing how an empirical KL risk minimizer can generalize and providing a high-probability upper bound on the KL-divergence.


<details>
  <summary>Details</summary>
Motivation: Schrödinger potential estimation is crucial in modern generative modelling methods based on Schrödinger bridges and stochastic optimal control for SDEs. These methods require minimal efforts to find a path between two given distributions ρ_0 and ρ_T* given a simple prior diffusion process.

Method: The authors study the generalization ability of an empirical Kullback-Leibler (KL) risk minimizer over a class of admissible log-potentials aimed at fitting the marginal distribution at time T. They derive a non-asymptotic high-probability upper bound on the KL-divergence between ρ_T* and the terminal density corresponding to the estimated log-potential under reasonable assumptions.

Result: The authors derived a non-asymptotic high-probability upper bound on the KL-divergence. They also showed that the excess KL-risk may decrease as fast as O(log^2 n / n) when the sample size n tends to infinity, even if both ρ_0 and ρ_T* have unbounded supports.

Conclusion: This work provides theoretical insights into the generalization ability of empirical KL risk minimizers in the context of Schrödinger potential estimation, demonstrating the potential for rapid decrease in KL-risk with increasing sample size.

Abstract: We address the problem of Schr\"odinger potential estimation, which plays a
crucial role in modern generative modelling approaches based on Schr\"odinger
bridges and stochastic optimal control for SDEs. Given a simple prior diffusion
process, these methods search for a path between two given distributions
$\rho_0$ and $\rho_T^*$ requiring minimal efforts. The optimal drift in this
case can be expressed through a Schr\"odinger potential. In the present paper,
we study generalization ability of an empirical Kullback-Leibler (KL) risk
minimizer over a class of admissible log-potentials aimed at fitting the
marginal distribution at time $T$. Under reasonable assumptions on the target
distribution $\rho_T^*$ and the prior process, we derive a non-asymptotic
high-probability upper bound on the KL-divergence between $\rho_T^*$ and the
terminal density corresponding to the estimated log-potential. In particular,
we show that the excess KL-risk may decrease as fast as $O(\log^2 n / n)$ when
the sample size $n$ tends to infinity even if both $\rho_0$ and $\rho_T^*$ have
unbounded supports.

</details>


### [157] [Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation](https://arxiv.org/abs/2506.03062)
*Qining Zhang,Tanner Fiez,Yi Liu,Wenyang Liu*

Main category: cs.LG

TL;DR: A fixed-budget multi-metric AED framework with SHRVar method is proposed for online experiments, which combines adaptive exploration and A/B test validation to overcome statistical power challenges.


<details>
  <summary>Details</summary>
Motivation: Standard A/B tests encounter statistical power challenges when testing multiple candidates simultaneously and adaptive experimental designs alone are insufficient in inferring experiment statistics, especially with many metrics and heterogeneous variances.

Method: The paper proposes a two-phase structure: an adaptive exploration phase using SHRVar method that generalizes sequential halving with relative-variance-based sampling and elimination strategy built on reward z-values, and a validation phase with an A/B test.

Result: SHRVar achieves a provable error probability that decreases exponentially, where the exponent generalizes the complexity measure for SH and SHVar. Numerical experiments confirm the analysis and show superior performance.

Conclusion: The proposed fixed-budget multi-metric AED framework effectively addresses the limitations of standard A/B tests and adaptive experimental designs.

Abstract: Standard A/B tests in online experiments face statistical power challenges
when testing multiple candidates simultaneously, while adaptive experimental
designs (AED) alone fall short in inferring experiment statistics such as the
average treatment effect, especially with many metrics (e.g., revenue, safety)
and heterogeneous variances. This paper proposes a fixed-budget multi-metric
AED framework with a two-phase structure: an adaptive exploration phase to
identify the best treatment, and a validation phase with an A/B test to verify
the treatment's quality and infer statistics. We propose SHRVar, which
generalizes sequential halving (SH) (Karnin et al., 2013) with a novel
relative-variance-based sampling and an elimination strategy built on reward
z-values. It achieves a provable error probability that decreases
exponentially, where the exponent generalizes the complexity measure for SH
(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and
heterogeneous variances, respectively. Numerical experiments verify our
analysis and demonstrate the superior performance of this new framework.

</details>


### [158] [Provable Reinforcement Learning from Human Feedback with an Unknown Link Function](https://arxiv.org/abs/2506.03066)
*Qining Zhang,Lei Ying*

Main category: cs.LG

TL;DR: An abstract about a new policy optimization algorithm called ZSPO for RLHF problems with unknown link functions.


<details>
  <summary>Details</summary>
Motivation: Current RLHF algorithms assume the link function is known to the agent, which is unrealistic considering the complex nature of human preferences.

Method: Propose a novel policy optimization algorithm called ZSPO based on a new zeroth-order policy optimization method. It uses human preference to construct a parameter update direction that is positively correlated with the true policy gradient direction.

Result: ZSPO converges to a stationary policy with a polynomial convergence rate depending on the number of policy iterations and trajectories per iteration. Numerical results show its superiority under link function mismatch.

Conclusion: ZSPO is an effective algorithm for RLHF problems with unknown link functions.

Abstract: Link functions, which characterize how human preferences are generated from
the value function of an RL problem, are a crucial component in designing RLHF
algorithms. Almost all RLHF algorithms, including state-of-the-art ones in
empirical studies such as DPO and PPO, assume the link function is known to the
agent (e.g., a logistic function according to the Bradley-Terry model), which
is arguably unrealistic considering the complex nature of human preferences. To
avoid link function mis-specification, this paper studies general RLHF problems
with unknown link functions. We propose a novel policy optimization algorithm
called ZSPO based on a new zeroth-order policy optimization method, where the
key is to use human preference to construct a parameter update direction that
is positively correlated with the true policy gradient direction. ZSPO achieves
it by estimating the sign of the value function difference instead of
estimating the gradient from the value function difference, so it does not
require knowing the link function. Under mild conditions, ZSPO converges to a
stationary policy with a polynomial convergence rate depending on the number of
policy iterations and trajectories per iteration. Numerical results also show
the superiority of ZSPO under link function mismatch.

</details>


### [159] [Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness](https://arxiv.org/abs/2506.03075)
*Bogdan Chornomaz,Yonatan Koren,Shay Moran,Tom Waknine*

Main category: cs.LG

TL;DR: 在面对能够破坏一定比例训练样本的对抗性攻击时，本文研究了如何使模型在特定测试点上失效的问题。对于不可知环境下的学习问题，文章证明了最优的额外误差为$\tilde{\Theta}(\sqrt{d\eta})$，并表明即使学习者的随机位对对手完全可见，这一结果仍然成立。此外，下界证明指出，单一固定分布下，对手可以无限频繁地导致至少为$\Omega(\sqrt{d\eta})$的额外误差。


<details>
  <summary>Details</summary>
Motivation: 之前的工作已经在可实现环境下研究了实例针对性投毒攻击的影响，并得出最优误差为$\Theta(d\eta)$。然而，不可知环境下的相应问题尚未解决。因此，本文旨在填补这一空白，研究不可知环境下针对实例的投毒攻击对学习算法的影响。

Method: 作者首先展示了在不可知环境下，使用随机学习器可以达到最优的额外误差$\tilde{\Theta}(\sqrt{d\eta})$。即使学习者的随机位对对手完全可见，这一结果依然有效。接着，他们通过构造一个固定的困难分布，证明了在这种分布下，对手可以强制学习器产生至少为$\Omega(\sqrt{d\eta})$的额外误差，并且这种情况可以无限频繁地发生。

Result: 成功证明了在不可知环境下，最优的额外误差为$\tilde{\Theta}(\sqrt{d\eta})$。并且，该结果不依赖于学习者的随机位是否对对手可见。同时，下界证明也显示了单一固定分布下，对手能够无限频繁地导致至少为$\Omega(\sqrt{d\eta})$的额外误差。

Conclusion: 本文解决了不可知环境下针对实例投毒攻击的学习问题，给出了最优额外误差的理论界限，并揭示了随机学习器在这种攻击下的重要性。

Abstract: We study the problem of learning in the presence of an adversary that can
corrupt an $\eta$ fraction of the training examples with the goal of causing
failure on a specific test point. In the realizable setting, prior work
established that the optimal error under such instance-targeted poisoning
attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the
hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding
question in the agnostic setting. We show that the optimal excess error is
$\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by
Hanneke et al. To achieve this rate, it is necessary to use randomized
learners: Hanneke et al. showed that deterministic learners can be forced to
suffer error close to 1, even under small amounts of poisoning. Perhaps
surprisingly, our upper bound remains valid even when the learner's random bits
are fully visible to the adversary . In the other direction, our lower bound is
stronger than standard PAC-style bounds: instead of tailoring a hard
distribution separately for each sample size, we exhibit a single fixed
distribution under which the adversary can enforce an excess error of
$\Omega(\sqrt{d\eta})$ infinitely often.

</details>


### [160] [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/abs/2506.03077)
*Qijun Luo,Mengqi Li,Lei Zhao,Xiao Li*

Main category: cs.LG

TL;DR: StreamBP is a new method that makes training language models on long sequences more memory-efficient without losing accuracy, by changing how backpropagation handles data. It also speeds up the process and can be easily used with transformer models.


<details>
  <summary>Details</summary>
Motivation: To address the high memory cost issue when training language models on long sequence data, which becomes significant as sequence length increases despite using gradient checkpointing.

Method: Proposed StreamBP performs a linear decomposition of the chain rule along the sequence dimension in a layer-wise manner to reduce memory cost for activation values and logits. It leverages the causal structure of the language model to achieve less computational FLOPs and faster BP speed.

Result: StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger than gradient checkpointing while using comparable or even less BP time. Its sequence length scaling ability can also accelerate training batch size scaling.

Conclusion: StreamBP provides a memory-efficient and exact BP method for training on long sequences, applicable to common objectives like SFT, GRPO, and DPO. A communication-efficient distributed version supports multi-GPU training, enhancing its applicability.

Abstract: Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.

</details>


### [161] [Non-Asymptotic Length Generalization](https://arxiv.org/abs/2506.03085)
*Thomas Chen,Tengyu Ma,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本研究探讨了学习算法在理想化设定下的长度泛化能力，提供了不同函数类的可证明长度泛化保证，并对最小输入长度（即长度复杂度）进行了详细分析。


<details>
  <summary>Details</summary>
Motivation: 长度泛化是学习算法能够推广到比训练集中更长输入的能力，当前研究旨在提供各种函数类在理想化设定下的长度泛化的可证明保证。

Method: 首先形式化非渐近长度泛化的框架，定义长度复杂度为确保长度泛化的最小输入长度。接着研究Minimum-Complexity Interpolator算法是否达到最优长度复杂度，探讨函数类非渐近长度泛化的可行性与其语言等价性问题的可判定性之间的关系。最后，针对确定性有限自动机和C-RASP函数类给出长度复杂度的具体上界。

Result: 对于确定性有限自动机，长度复杂度为$2n - 2$；对于1层C-RASP函数，长度复杂度为$O(T^2)$；对于2层C-RASP函数，长度复杂度为$O(T^{O(K)})$。此外，上下文无关文法的长度复杂度没有可计算的上界。

Conclusion: 该研究提供了不同函数类的长度复杂度的理论界限，揭示了函数类的非渐近长度泛化能力与语言等价性问题的可判定性之间的联系。

Abstract: Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [162] [How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment](https://arxiv.org/abs/2506.03087)
*Bin Ma,Yuyuan Feng,Minhua Lin,Enyan Dai*

Main category: cs.LG

TL;DR: The paper explores how explanations from explainable GNNs can lead to model stealing and proposes a novel framework for this process, highlighting security concerns.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential security risks associated with using explainable GNNs, specifically focusing on how explanations might leak critical decision logic that could be exploited for model stealing.

Method: Proposes EGSteal, a framework that integrates explanation alignment for capturing decision logic with guided data augmentation for efficient training under limited queries.

Result: Experiments on molecular graph datasets show that the proposed approach has advantages over conventional methods in model stealing.

Conclusion: This work emphasizes important security considerations for deploying explainable GNNs in sensitive domains and suggests the need for protective measures against explanation-based attacks.

Abstract: Graph Neural Networks (GNNs) have become essential tools for analyzing
graph-structured data in domains such as drug discovery and financial analysis,
leading to growing demands for model transparency. Recent advances in
explainable GNNs have addressed this need by revealing important subgraphs that
influence predictions, but these explanation mechanisms may inadvertently
expose models to security risks. This paper investigates how such explanations
potentially leak critical decision logic that can be exploited for model
stealing. We propose {\method}, a novel stealing framework that integrates
explanation alignment for capturing decision logic with guided data
augmentation for efficient training under limited queries, enabling effective
replication of both the predictive behavior and underlying reasoning patterns
of target models. Experiments on molecular graph datasets demonstrate that our
approach shows advantages over conventional methods in model stealing. This
work highlights important security considerations for the deployment of
explainable GNNs in sensitive domains and suggests the need for protective
measures against explanation-based attacks. Our code is available at
https://github.com/beanmah/EGSteal.

</details>


### [163] [From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](https://arxiv.org/abs/2506.03093)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Main category: cs.LG

TL;DR: 研究发现，传统的稀疏自编码器(SAEs)无法准确捕捉分层和非线性特征。通过改进算法设计MP-SAE，可以更好地识别这些复杂特征，并揭示视觉-语言模型中不同模态表示空间的共享结构。此外，MP-SAE在推理时具有自适应稀疏性的优势。


<details>
  <summary>Details</summary>
Motivation: 神经网络表示被认为将抽象、可解释的特征编码为线性可访问且近似正交的方向。然而，现有研究表明模型表示还包含分层、非线性和多维特征，这与传统假设不符。因此，需要探索SAEs是否能够表示与该假设相矛盾的特征，或者避免这种不匹配是否有助于进一步理解神经网络表示。

Method: 基于构造方法，重新构建了流行的匹配追踪(MP)算法，设计出MP-SAE。该架构将编码器展开为一系列残差引导步骤，从而能够捕捉分层和非线性特征。通过在合成和自然数据集上比较MP-SAE与现有SAEs的性能，验证其有效性。

Result: (i) 分层概念诱导条件正交特征，而现有SAEs无法准确捕捉；(ii) MP-SAE的非线性编码步骤恢复了高度有意义的特征，揭示了视觉-语言模型中不同模态表示空间的共享结构。此外，MP-SAE的序列编码器原则在推理时提供了自适应稀疏性的好处。

Conclusion: 研究结果支持了从表示的现象学出发进行可解释性研究的观点，提出的方法应基于与现象学一致的假设。

Abstract: Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.

</details>


### [164] [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
*Yang Guo,Yutian Tao,Yifei Ming,Robert D. Nowak,Yingyu Liang*

Main category: cs.LG

TL;DR: This paper provides the first finite-sample generalization bound for Retrieval-augmented generation (RAG) in in-context linear regression and derives an exact bias-variance tradeoff, showing that RAG has an intrinsic ceiling on generalization error compared to in-context learning (ICL).


<details>
  <summary>Details</summary>
Motivation: To explore the theoretical aspect of Retrieval-augmented generation (RAG), which has seen many empirical successes but remains mostly unexplored in theory.

Method: Propose a framework that views the retrieved texts as query-dependent noisy in-context examples, recovering classical in-context learning (ICL) and standard RAG as limit cases. Derive the first finite-sample generalization bound for RAG in in-context linear regression and an exact bias-variance tradeoff. Introduce uniform and non-uniform RAG noise to model retrieval from training data and external corpora.

Result: The analysis suggests an intrinsic ceiling on generalization error exists on RAG compared to ICL. Empirical experiments on common QA benchmarks, such as Natural Questions and TriviaQA, show the sample efficiency of ICL and RAG.

Conclusion: RAG has an intrinsic limitation in generalization error compared to ICL, but it can be more sample efficient.

Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.

</details>


### [165] [On Weak-to-Strong Generalization and f-Divergence](https://arxiv.org/abs/2506.03109)
*Wei Yao,Gengze Xu,Huayi Tang,Wenkai Yang,Donglin Di,Ziqiao Wang,Yong Liu*

Main category: cs.LG

TL;DR: The paper explores the use of $f$-divergence as a loss function in weak-to-strong generalization (W2SG), revealing its theoretical properties and demonstrating its practical benefits.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of strong pre-trained models in W2SG without requiring additional weak models or complex procedures, by leveraging the effectiveness of $f$-divergence loss.

Method: The method involves introducing $f$-divergence as an information-theoretic loss function framework in W2SG, analyzing its theoretical properties, and providing empirical evidence of its effectiveness.

Result: The results show that $f$-divergence loss can effectively enhance the generalization and noise tolerance of the strong model in practice.

Conclusion: $f$-divergence provides a theoretically sound and practically effective approach for improving the capabilities of strong models in W2SG.

Abstract: Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for
stimulating the capabilities of strong pre-trained models by leveraging
supervision from weaker supervisors. To improve the performance of the strong
model, existing methods often require additional weak models or complex
procedures, leading to substantial computational and memory overhead. Motivated
by the effectiveness of $f$-divergence loss in various machine learning
domains, we introduce $f$-divergence as an information-theoretic loss function
framework in W2SG. Our theoretical analysis reveals fundamental limitations and
equivalence of different $f$-divergence losses in W2SG, supported by sample
complexity bounds and information-theoretic insights. We empirically
demonstrate that $f$-divergence loss, which generalizes widely-used metrics
like KL divergence, effectively improves generalization and noise tolerance of
the strong model in practice.

</details>


### [166] [Rectified Flows for Fast Multiscale Fluid Flow Modeling](https://arxiv.org/abs/2506.03111)
*Victor Armegioiu,Yannick Ramic,Siddhartha Mishra*

Main category: cs.LG

TL;DR: A rectified flow framework is introduced which learns a time-dependent velocity field for fluid flow statistical modeling, significantly reducing sampling steps without losing fidelity.


<details>
  <summary>Details</summary>
Motivation: Statistical modeling of fluid flows is challenging due to multiscale dynamics and sensitivity to initial conditions. Recent conditional diffusion models have high fidelity but require many stochastic sampling steps.

Method: The rectified flow framework learns a time-dependent velocity field that transports input to output distributions along nearly straight trajectories. Sampling is cast as solving an ODE along this flow field.

Result: Experiments show that rectified flows recover the same posterior distributions as diffusion models, preserve fine-scale features missed by MSE-trained baselines, and deliver high-resolution samples much faster.

Conclusion: Rectified flow offers a more efficient alternative to standard score-based diffusion for generating high-fidelity fluid flow samples.

Abstract: The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.

</details>


### [167] [Zero-Shot Time Series Forecasting with Covariates via In-Context Learning](https://arxiv.org/abs/2506.03128)
*Andreas Auer,Raghul Parthipan,Pedro Mercado,Abdul Fatir Ansari,Lorenzo Stella,Bernie Wang,Michael Bohlke-Schneider,Syama Sundar Rangapuram*

Main category: cs.LG

TL;DR: COSMIC is a zero-shot forecasting model that effectively uses covariates through in-context learning and informative covariate augmentation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing pretrained time series models either do not support covariates or fail to incorporate them effectively.

Method: The COSMIC model utilizes covariates via in-context learning and proposes Informative Covariate Augmentation to enable training without datasets that include covariates.

Result: COSMIC achieves state-of-the-art performance in zero-shot forecasting, both with and without covariates.

Conclusion: Quantitative and qualitative analysis shows that COSMIC effectively leverages covariates in zero-shot forecasting.

Abstract: Pretrained time series models, capable of zero-shot forecasting, have
demonstrated significant potential in enhancing both the performance and
accessibility of time series forecasting. However, existing pretrained models
either do not support covariates or fail to incorporate them effectively. We
introduce COSMIC, a zero-shot forecasting model that utilizes covariates via
in-context learning. To address the challenge of data scarcity, we propose
Informative Covariate Augmentation, which enables the training of COSMIC
without requiring any datasets that include covariates. COSMIC achieves
state-of-the-art performance in zero-shot forecasting, both with and without
covariates. Our quantitative and qualitative analysis demonstrates that COSMIC
effectively leverages covariates in zero-shot forecasting.

</details>


### [168] [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/abs/2506.03133)
*Kai Lion,Liang Zhang,Bingcong Li,Niao He*

Main category: cs.LG

TL;DR: The paper proposes PoLAR, a novel parameterization for low-rank adaptation of large-scale models that improves fine-tuning performance.


<details>
  <summary>Details</summary>
Motivation: Low-rank adaptation in large-scale models has been found to suffer from a low stable rank which degrades the fine-tuning performance.

Method: PoLAR is proposed, which factorizes the low-rank update into two direction matrices constrained to Stiefel manifolds and an unconstrained scale matrix. This method is inspired by the polar decomposition.

Result: The theory shows that PoLAR yields an exponentially faster convergence rate on a canonical low-rank adaptation problem. Empirical results demonstrate consistent gains across three different benchmarks.

Conclusion: PoLAR paired with Riemannian optimization leads to significant improvements in tasks related to general language understanding, commonsense reasoning, and mathematical problem solving across various model sizes.

Abstract: We show that low-rank adaptation of large-scale models suffers from a low
stable rank that is well below the linear algebraic rank of the subspace,
degrading fine-tuning performance. To mitigate the underutilization of the
allocated subspace, we propose PoLAR, a parameterization inspired by the polar
decomposition that factorizes the low-rank update into two direction matrices
constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory
shows that PoLAR yields an exponentially faster convergence rate on a canonical
low-rank adaptation problem. Pairing the parameterization with Riemannian
optimization leads to consistent gains on three different benchmarks testing
general language understanding, commonsense reasoning, and mathematical problem
solving with base model sizes ranging from 350M to 27B.

</details>


### [169] [Not All Tokens Are Meant to Be Forgotten](https://arxiv.org/abs/2506.03142)
*Xiangyu Zhou,Yao Qiang,Saleh Zare Zade,Douglas Zytko,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

TL;DR: Large Language Models (LLMs) have human-like abilities but may memorize unwanted information. Unlearning methods can cause over-forgetting, leading to a loss of model utility. This paper introduces the Targeted Information Forgetting (TIF) framework which includes a targeted information identifier and a Targeted Preference Optimization approach. TIF enhances unlearning effectiveness while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning methods for LLMs face the challenge of over-forgetting, where indiscriminate suppression of tokens in forget samples leads to a significant loss of model utility.

Method: The TIF framework consists of two components: (1) a flexible targeted information identifier that differentiates between unwanted words (UW) and general words (GW) in forget samples; and (2) a novel Targeted Preference Optimization approach that uses Logit Preference Loss to unlearn unwanted information associated with UW and Preservation Loss to retain general information in GW.

Result: Extensive experiments on the TOFU and MUSE benchmarks show that the TIF framework improves unlearning effectiveness while maintaining model utility, achieving state-of-the-art results.

Conclusion: The proposed TIF framework addresses the over-forgetting issue in LLM unlearning, enhancing unlearning effectiveness while preserving model utility.

Abstract: Large Language Models (LLMs), pre-trained on massive text corpora, exhibit
remarkable human-level language understanding, reasoning, and decision-making
abilities. However, they tend to memorize unwanted information, such as private
or copyrighted content, raising significant privacy and legal concerns.
Unlearning has emerged as a promising solution, but existing methods face a
significant challenge of over-forgetting. This issue arises because they
indiscriminately suppress the generation of all the tokens in forget samples,
leading to a substantial loss of model utility. To overcome this challenge, we
introduce the Targeted Information Forgetting (TIF) framework, which consists
of (1) a flexible targeted information identifier designed to differentiate
between unwanted words (UW) and general words (GW) in the forget samples, and
(2) a novel Targeted Preference Optimization approach that leverages Logit
Preference Loss to unlearn unwanted information associated with UW and
Preservation Loss to retain general information in GW, effectively improving
the unlearning process while mitigating utility degradation. Extensive
experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF
framework enhances unlearning effectiveness while preserving model utility and
achieving state-of-the-art results.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [170] [The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era](https://arxiv.org/abs/2506.02027)
*Shriphani Palakodety*

Main category: cs.CR

TL;DR: The paper argues Universal Lifelong Identifiers (ULIs) pose significant privacy risks in the AI era and proposes a cryptographic framework to replace them.


<details>
  <summary>Details</summary>
Motivation: ULIs are incompatible with modern AI capabilities and create systemic privacy risks that traditional safeguards cannot address.

Method: The authors define properties for AI-era identity systems, develop a cryptographic framework, and provide a migration path from ULIs while maintaining compatibility with existing workflows.

Result: The proposed framework preserves institutional workflows, supports necessary functions like auditability and delegation, and offers a practical solution to transition away from ULIs.

Conclusion: ULIs must be phased out and replaced with an AI-compatible identity system based on the presented cryptographic framework.

Abstract: Many identity systems assign a single, static identifier to an individual for
life, reused across domains like healthcare, finance, and education. These
Universal Lifelong Identifiers (ULIs) underpin critical workflows but now pose
systemic privacy risks. We take the position that ULIs are fundamentally
incompatible with the AI era and must be phased out. We articulate a threat
model grounded in modern AI capabilities and show that traditional safeguards
such as redaction, consent, and access controls are no longer sufficient. We
define core properties for identity systems in the AI era and present a
cryptographic framework that satisfies them while retaining compatibility with
existing identifier workflows. Our design preserves institutional workflows,
supports essential functions such as auditability and delegation, and offers a
practical migration path beyond ULIs.

</details>


### [171] [A tertiary review on quantum cryptography](https://arxiv.org/abs/2506.02028)
*Luiz Filipi Anderson de Sousa Moura,Carlos Becker Westphall*

Main category: cs.CR

TL;DR: This paper is about the threat of quantum computers to system security and the countermeasures such as post-quantum cryptography and quantum cryptography. It selected 51 secondary studies from Scopus database, presented bibliometric analysis, main techniques, open challenges and future directions in quantum cryptography research. The results showed a prevalence of QKD over other techniques and stated many problems still exist.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the immense threat that quantum computers pose to system security. To prevent these attacks, new cryptographic classes have been created, including post-quantum cryptography and quantum cryptography.

Method: This tertiary review selected 51 secondary studies from the Scopus database and conducted a bibliometric analysis on them.

Result: The results showed a prevalence of Quantum Key Distribution (QKD) over other techniques among the selected papers. The field still faces many problems related to implementation cost, error correction, decoherence, key rates, communication distance, and quantum hacking.

Conclusion: Quantum cryptography uses the principle of quantum physics to produce theoretically unbreakable security but still has numerous challenges that need to be addressed.

Abstract: Quantum computers impose an immense threat to system security. As a
countermeasure, new cryptographic classes have been created to prevent these
attacks. Technologies such as post-quantum cryptography and quantum
cryptography. Quantum cryptography uses the principle of quantum physics to
produce theoretically unbreakable security. This tertiary review selected 51
secondary studies from the Scopus database and presented bibliometric analysis,
a list of the main techniques used in the field, and existing open challenges
and future directions in quantum cryptography research. The results showed a
prevalence of QKD over other techniques among the selected papers and stated
that the field still faces many problems related to implementation cost, error
correction, decoherence, key rates, communication distance, and quantum
hacking.

</details>


### [172] [Adaptive Privacy-Preserving SSD](https://arxiv.org/abs/2506.02030)
*Na Young Ahn,Dong Hoon Lee*

Main category: cs.CR

TL;DR: Data remanence in NAND flash makes it difficult to completely delete data on IoT SSDs. This paper designs an adaptive architecture with four privacy levels that use different deletion techniques, and machine-learning is used to adjust these levels.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of complete data deletion on IoT SSDs due to data remanence in NAND flash.

Method: This paper proposes an adaptive architecture offering four privacy levels (PL0-PL3) which select among address, data, and parity deletion techniques. Machine-learning is also employed to adjust these levels contextually.

Result: Quantitative analysis shows a balance between efficacy, latency, endurance, and cost. The proposed method can boost privacy with negligible performance overhead and complexity.

Conclusion: An adaptive architecture with machine-learning adjustment can effectively enhance data privacy on IoT SSDs while maintaining system performance.

Abstract: Data remanence in NAND flash complicates complete deletion on IoT SSDs. We
design an adaptive architecture offering four privacy levels (PL0-PL3) that
select among address, data, and parity deletion techniques. Quantitative
analysis balances efficacy, latency, endurance, and cost. Machine-learning
adjusts levels contextually, boosting privacy with negligible performance
overhead and complexity.

</details>


### [173] [Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges](https://arxiv.org/abs/2506.02032)
*Raj Patel,Himanshu Tripathi,Jasper Stone,Noorbakhsh Amiri Golilarz,Sudip Mittal,Shahram Rahimi,Vini Chaudhary*

Main category: cs.CR

TL;DR: The paper applies the MITRE ATLAS framework to assess and mitigate adversarial attacks on MLOps ecosystem, offering a taxonomy of attack techniques and defenses while highlighting research gaps.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of MLOps has introduced vulnerabilities that can lead to severe consequences such as compromised credentials, financial losses, damaged trust, and data poisoning. There is a need for systematic assessment and mitigation strategies against adversarial attacks in MLOps pipelines.

Method: Systematic application of the MITRE ATLAS framework to analyze different phases of the MLOps ecosystem. Creation of a taxonomy of attack techniques mapped to MLOps phases, supported by real-world examples. Development of a corresponding taxonomy of mitigation strategies aligned with these attack categories.

Result: A structured understanding of potential adversarial attacks across MLOps phases along with actionable defense mechanisms. Identification of key research gaps in securing MLOps.

Conclusion: Robust security protocols should be implemented from the beginning in MLOps to protect against evolving cyber threats, providing guidance and empowerment for practitioners.

Abstract: The rapid adoption of machine learning (ML) technologies has driven
organizations across diverse sectors to seek efficient and reliable methods to
accelerate model development-to-deployment. Machine Learning Operations (MLOps)
has emerged as an integrative approach addressing these requirements by
unifying relevant roles and streamlining ML workflows. As the MLOps market
continues to grow, securing these pipelines has become increasingly critical.
However, the unified nature of MLOps ecosystem introduces vulnerabilities,
making them susceptible to adversarial attacks where a single misconfiguration
can lead to compromised credentials, severe financial losses, damaged public
trust, and the poisoning of training data. Our paper presents a systematic
application of the MITRE ATLAS (Adversarial Threat Landscape for
Artificial-Intelligence Systems) framework, a comprehensive and continuously
updated catalog of AI-focused attacks, to systematically assess attacks across
different phases of the MLOps ecosystem. We begin by examining the preparatory
phases during which adversaries acquire the essential intelligence required to
initiate their attacks. We then present a structured taxonomy of attack
techniques explicitly mapped to corresponding phases of the MLOps ecosystem,
supported by examples drawn from red-teaming exercises and real-world
incidents. This is followed by a taxonomy of mitigation strategies aligned with
these attack categories, offering actionable early-stage defenses to strengthen
the security of MLOps ecosystem. Given the rapid evolution and adoption of
MLOps, we further highlight key research gaps that require immediate attention.
Our work emphasizes the importance of implementing robust security protocols
from the outset, empowering practitioners to safeguard MLOps ecosystem against
evolving cyber attacks.

</details>


### [174] [Asymmetry by Design: Boosting Cyber Defenders with Differential Access to AI](https://arxiv.org/abs/2506.02035)
*Shaun Ee,Chris Covino,Cara Labrador,Christina Krawec,Jam Kraprayoon,Joe O'Brien*

Main category: cs.CR

TL;DR: The paper proposes 'differential access' as a strategy to improve cybersecurity by controlling access to advanced AI-enabled cyber capabilities. It introduces three approaches forming a continuum from promoting access to denying by default, emphasizing the need to prioritize defender access even in restrictive scenarios. The report provides a process for frontier AI developers to choose and implement one of these approaches, considering factors like a model's cyber capabilities, defender's maturity and role, and implementation details. Four example schemes are presented to demonstrate the value of differential access.


<details>
  <summary>Details</summary>
Motivation: As AI-enabled cyber capabilities become more advanced, there is a growing need to tilt the cybersecurity balance towards defense. This requires shaping access to these capabilities to ensure that defenders can prepare against potential adversaries gaining similar capabilities.

Method: The paper outlines three possible approaches forming a continuum: Promote Access, Manage Access, and Deny by Default. Each approach progressively becomes more restrictive for higher-risk capabilities while ensuring defender access is prioritized. A process is provided to help frontier AI developers choose and implement one of these approaches based on specific considerations.

Result: Four example schemes are presented demonstrating the value of differential access across various capability and defender levels. These schemes provide reference points for defenders and illustrate the effectiveness of the proposed strategy.

Conclusion: Differential access offers a valuable strategy to enhance cybersecurity by managing access to advanced AI-enabled cyber capabilities. Further research directions are suggested to continue improving this approach.

Abstract: As AI-enabled cyber capabilities become more advanced, we propose
"differential access" as a strategy to tilt the cybersecurity balance toward
defense by shaping access to these capabilities. We introduce three possible
approaches that form a continuum, becoming progressively more restrictive for
higher-risk capabilities: Promote Access, Manage Access, and Deny by Default.
However, a key principle across all approaches is the need to prioritize
defender access, even in the most restrictive scenarios, so that defenders can
prepare for adversaries gaining access to similar capabilities. This report
provides a process to help frontier AI developers choose and implement one of
the three differential access approaches, including considerations based on a
model's cyber capabilities, a defender's maturity and role, and strategic and
technical implementation details. We also present four example schemes for
defenders to reference, demonstrating how differential access provides value
across various capability and defender levels, and suggest directions for
further research.

</details>


### [175] [Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment](https://arxiv.org/abs/2506.02038)
*Anum Nawaz,Hafiz Humza Mahmood Ramzan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.CR

TL;DR: Edge Intelligence (EI)与区块链技术结合可以增强隐私保护系统，但其运行架构存在固有漏洞。为此，本文提出了一种自主计算模型及其交互拓扑结构，适用于隐私关键和时间敏感的健康应用。该系统支持连续监控、实时警报通知、疾病检测和强大的数据处理功能。同时，还提出了一个资源高效的1D-CNN用于心律失常的多分类，并定义了一个安全访问方案来管理数据共享和存储。通过全面的安全性、性能和成本分析，证明了细粒度访问控制方案的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管EI和区块链技术能增强隐私保护并减少延迟，但其在边缘网关（EGs）之间的广泛数据交互和分布式信息存储中引入了固有漏洞，特别是在服务提供过程中可能影响隐私和安全性。这促使研究者设计一种更适合隐私关键和时间敏感型健康应用的解决方案。

Method: 1. 提出自主计算模型及其交互拓扑，支持健康应用中的连续监测、实时通知、疾病检测等功能。
2. 设计1D-CNN进行心律失常多分类，以适应资源受限的EGs环境。
3. 定义安全访问方案，管理链上和链下数据共享与存储。
4. 进行安全性、性能和成本分析验证系统的效率和可靠性。

Result: 实验结果表明，所提出的细粒度访问控制方案在安全性、性能和成本方面均表现出高效和可靠的特点。此外，1D-CNN在心律失常分类任务中实现了准确且实时的分析能力。

Conclusion: 本文提出的自主计算模型及配套机制为隐私关键和时间敏感型健康应用提供了有效的解决方案。通过集成安全访问控制和高效的深度学习模型，系统能够在确保隐私的同时，实现高性能的数据处理和分析。

Abstract: Edge Intelligence (EI) serves as a critical enabler for privacy-preserving
systems by providing AI-empowered computation and distributed caching services
at the edge, thereby minimizing latency and enhancing data privacy. The
integration of blockchain technology further augments EI frameworks by ensuring
transactional transparency, auditability, and system-wide reliability through a
decentralized network model. However, the operational architecture of such
systems introduces inherent vulnerabilities, particularly due to the extensive
data interactions between edge gateways (EGs) and the distributed nature of
information storage during service provisioning. To address these challenges,
we propose an autonomous computing model along with its interaction topologies
tailored for privacy-critical and time-sensitive health applications. The
system supports continuous monitoring, real-time alert notifications, disease
detection, and robust data processing and aggregation. It also includes a data
transaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a
resource-efficient one-dimensional convolutional neural network (1D-CNN) is
proposed for the multiclass classification of arrhythmia, enabling accurate and
real-time analysis of constrained EGs. Furthermore, a secure access scheme is
defined to manage both off-chain and on-chain data sharing and storage. To
validate the proposed model, comprehensive security, performance, and cost
analyses are conducted, demonstrating the efficiency and reliability of the
fine-grained access control scheme.

</details>


### [176] [Beyond the Protocol: Unveiling Attack Vectors in the Model Context Protocol Ecosystem](https://arxiv.org/abs/2506.02040)
*Hao Song,Yiming Shen,Wenxuan Luo,Leixin Guo,Ting Chen,Jiashui Wang,Beibei Li,Xiaosong Zhang,Jiachi Chen*

Main category: cs.CR

TL;DR: The paper explores the vulnerabilities in Model Context Protocol (MCP) systems, identifying four categories of attacks and demonstrating their feasibility through experiments. It highlights the insufficiency of current audit mechanisms and users' difficulty in identifying malicious MCP servers, urging for robust security measures.


<details>
  <summary>Details</summary>
Motivation: To systematically study attack vectors targeting the MCP ecosystem due to its inherent client-server integration architecture which may introduce new vulnerabilities.

Method: Identify four categories of attacks on MCP systems; conduct experiments simulating these attacks via malicious MCP servers; perform a user study and interviews; deploy a proof-of-concept framework against leading LLMs.

Result: Current audit mechanisms fail to identify malicious MCP servers; users struggle to detect malicious servers; demonstrated harmful behaviors triggered by these attacks within local environments.

Conclusion: There is an urgent need for robust security mechanisms to defend against malicious MCP servers.

Abstract: The Model Context Protocol (MCP) is an emerging standard designed to enable
seamless interaction between Large Language Model (LLM) applications and
external tools or resources. Within a short period, thousands of MCP services
have already been developed and deployed. However, the client-server
integration architecture inherent in MCP may expand the attack surface against
LLM Agent systems, introducing new vulnerabilities that allow attackers to
exploit by designing malicious MCP servers. In this paper, we present the first
systematic study of attack vectors targeting the MCP ecosystem. Our analysis
identifies four categories of attacks, i.e., Tool Poisoning Attacks, Puppet
Attacks, Rug Pull Attacks, and Exploitation via Malicious External Resources.
To evaluate the feasibility of these attacks, we conduct experiments following
the typical steps of launching an attack through malicious MCP servers:
upload-download-attack. Specifically, we first construct malicious MCP servers
and successfully upload them to three widely used MCP aggregation platforms.
The results indicate that current audit mechanisms are insufficient to identify
and prevent the proposed attack methods. Next, through a user study and
interview with 20 participants, we demonstrate that users struggle to identify
malicious MCP servers and often unknowingly install them from aggregator
platforms. Finally, we demonstrate that these attacks can trigger harmful
behaviors within the user's local environment-such as accessing private files
or controlling devices to transfer digital assets-by deploying a
proof-of-concept (PoC) framework against five leading LLMs. Additionally, based
on interview results, we discuss four key challenges faced by the current
security ecosystem surrounding MCP servers. These findings underscore the
urgent need for robust security mechanisms to defend against malicious MCP
servers.

</details>


### [177] [Docker under Siege: Securing Containers in the Modern Era](https://arxiv.org/abs/2506.02043)
*Gogulakrishnan Thiyagarajan,Prabhudarshi Nayak*

Main category: cs.CR

TL;DR: Containerization via Docker boosts app development but brings security challenges. This paper explores runtime protection, network safeguards, configuration practices, supply chain security, and monitoring solutions within container security, providing recommendations to integrate security into the SDLC.


<details>
  <summary>Details</summary>
Motivation: To address the significant security challenges introduced by the rapid adoption of container technologies in application development and deployment.

Method: Investigating key areas of container security such as runtime protection, network safeguards, configuration best practices, supply chain security, and comprehensive monitoring and logging solutions while identifying common vulnerabilities and providing actionable recommendations.

Result: Identification of common vulnerabilities in various domains of container security and provision of actionable recommendations to mitigate these risks effectively.

Conclusion: Integrating security throughout the Software Development Lifecycle (SDLC) can help organizations reinforce their security posture, leading to a resilient and reliable containerized application infrastructure.

Abstract: Containerization, driven by Docker, has transformed application development
and deployment by enhancing efficiency and scalability. However, the rapid
adoption of container technologies introduces significant security challenges
that require careful management. This paper investigates key areas of container
security, including runtime protection, network safeguards, configuration best
practices, supply chain security, and comprehensive monitoring and logging
solutions. We identify common vulnerabilities within these domains and provide
actionable recommendations to address and mitigate these risks. By integrating
security throughout the Software Development Lifecycle (SDLC), organizations
can reinforce their security posture, creating a resilient and reliable
containerized application infrastructure that withstands evolving threats.

</details>


### [178] [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)
*Lajos Muzsai,David Imolai,András Lukács*

Main category: cs.CR

TL;DR: This paper presents a framework called 'random-crypto' to fine-tune a tool-augmented Llama-3.1-8B model with GRPO for solving cryptographic CTF challenges, resulting in significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs in structured reasoning and computation for cybersecurity problem solving.

Method: Introduced 'random-crypto', a cryptographic CTF challenge generator framework, and used it to fine-tune a tool-augmented Llama-3.1-8B model with Guided Reinforcement Prompt Optimisation (GRPO).

Result: +53% absolute jump in Pass@8 on unseen 'random-crypto' tasks (0.35 -> 0.88), Majority@8 raised to 0.41, and +13 pp improvement on picoCTF cryptography problems.

Conclusion: The fine-tuned agent shows significant improvement in solving cryptographic CTF challenges and generalizes well to an external dataset.

Abstract: Large Language Models (LLMs) still struggle with the structured reasoning and
tool-assisted computation needed for problem solving in cybersecurity
applications. In this work, we introduce "random-crypto", a cryptographic
Capture-the-Flag (CTF) challenge generator framework that we use to fine-tune a
tool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation
(GRPO), allowing the agent to iteratively write and execute Python inside an
isolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen
"random-crypto" tasks (0.35 -> 0.88) and raises Majority@8 to 0.41. The
fine-tuned agent also generalizes to an external dataset. On a subset of
picoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the
gains stem from more reliable tool invocation and code synthesis, rather than
superficial prompt adaptation.

</details>


### [179] [Privacy-Aware, Public-Aligned: Embedding Risk Detection and Public Values into Scalable Clinical Text De-Identification for Trusted Research Environments](https://arxiv.org/abs/2506.02063)
*Arlene Casey,Stuart Dunbar,Franz Gruber,Samuel McInerney,Matúš Falis,Pamela Linksted,Katie Wilde,Kathy Harrison,Alison Hamilton,Christian Cole*

Main category: cs.CR

TL;DR: Clinical free-text data has great potential but significant privacy risks. The paper synthesizes findings on privacy risks across document types and NHS data providers in Scotland, emphasizing context-dependent and cumulative privacy risk. It advocates adaptable de-identification approaches combining rule-based precision with contextual understanding.


<details>
  <summary>Details</summary>
Motivation: To explore the privacy risks associated with clinical free-text data and develop a prototype privacy-risk management tool that supports transparent, auditable decision-making for safe use of this data.

Method: Synthesizing findings from previous studies examining privacy risks across multiple document types and NHS data providers in Scotland. Characterizing direct and indirect identifiers by record type, clinical setting, and data flow. Exploring societal expectations through public engagement and reflecting these in the design of a prototype privacy-risk management tool.

Result: Privacy risk is context-dependent and cumulative. Changes in documentation practice can degrade model performance over time. A prototype privacy-risk management tool was designed to support transparent, auditable decision-making.

Conclusion: There is a need for adaptable, hybrid de-identification approaches combining rule-based precision with contextual understanding for safe, scalable reuse of clinical free-text within Trusted Research Environments.

Abstract: Clinical free-text data offers immense potential to improve population health
research such as richer phenotyping, symptom tracking, and contextual
understanding of patient care. However, these data present significant privacy
risks due to the presence of directly or indirectly identifying information
embedded in unstructured narratives. While numerous de-identification tools
have been developed, few have been tested on real-world, heterogeneous datasets
at scale or assessed for governance readiness. In this paper, we synthesise our
findings from previous studies examining the privacy-risk landscape across
multiple document types and NHS data providers in Scotland. We characterise how
direct and indirect identifiers vary by record type, clinical setting, and data
flow, and show how changes in documentation practice can degrade model
performance over time. Through public engagement, we explore societal
expectations around the safe use of clinical free text and reflect these in the
design of a prototype privacy-risk management tool to support transparent,
auditable decision-making. Our findings highlight that privacy risk is
context-dependent and cumulative, underscoring the need for adaptable, hybrid
de-identification approaches that combine rule-based precision with contextual
understanding. We offer a comprehensive view of the challenges and
opportunities for safe, scalable reuse of clinical free-text within Trusted
Research Environments and beyond, grounded in both technical evidence and
public perspectives on responsible data use.

</details>


### [180] [Developing a Risk Identification Framework for Foundation Model Uses](https://arxiv.org/abs/2506.02066)
*David Piorkowski,Michael Hind,John Richards,Jacquelyn Martino*

Main category: cs.CR

TL;DR: This paper aims to bridge the gap in identifying risks associated with foundation models by developing requirements and an initial design for a risk identification framework, adapting ideas from usage governance and demonstrating its practical application.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of guidance for practitioners in determining which risks are relevant for a given use of foundation models, despite existing efforts in measuring these risks through benchmarks and AI risk taxonomies.

Method: The authors review prior literature to identify challenges in building a risk identification framework for foundation models. They adapt concepts from usage governance to create four design requirements and demonstrate how a candidate framework can meet these requirements using a foundation model use example.

Result: The paper presents an initial design for a risk identification framework that addresses the identified design requirements and shows its functionality through a practical example involving a small subset of risks.

Conclusion: The developed framework provides a starting point for practitioners to better identify and manage risks associated with the use of foundation models.

Abstract: As foundation models grow in both popularity and capability, researchers have
uncovered a variety of ways that the models can pose a risk to the model's
owner, user, or others. Despite the efforts of measuring these risks via
benchmarks and cataloging them in AI risk taxonomies, there is little guidance
for practitioners on how to determine which risks are relevant for a given
foundation model use. In this paper, we address this gap and develop
requirements and an initial design for a risk identification framework. To do
so, we look to prior literature to identify challenges for building a
foundation model risk identification framework and adapt ideas from usage
governance to synthesize four design requirements. We then demonstrate how a
candidate framework can addresses these design requirements and provide a
foundation model use example to show how the framework works in practice for a
small subset of risks.

</details>


### [181] [Mitigating Data Poisoning Attacks to Local Differential Privacy](https://arxiv.org/abs/2506.02156)
*Xiaolin Li,Ninghui Li,Boyang Wang,Wenhai Sun*

Main category: cs.CR

TL;DR: The paper proposes a comprehensive mitigation framework for local differential privacy (LDP) to defend against data poisoning attacks, including malicious user detection, attack pattern recognition, and utility recovery.


<details>
  <summary>Details</summary>
Motivation: LDP invites data poisoning attacks that threaten the underlying applications, so there is a need for effective defenses.

Method: The proposed framework includes methods for detecting malicious users, recognizing attack patterns, and recovering damaged utility. It also explores new adaptive adversarial activities and proposes a new post-processing method for corrupted data recovery.

Result: Experiments show excellent performance of the detection methods with minimal computational cost and substantial improvement over previous work. The new post-processing method provides insights into protocol recommendations and design principles.

Conclusion: A comprehensive LDP mitigation framework has been proposed, which effectively defends against data poisoning attacks and recovers utility, providing valuable insights for future research.

Abstract: The distributed nature of local differential privacy (LDP) invites data
poisoning attacks and poses unforeseen threats to the underlying LDP-supported
applications. In this paper, we propose a comprehensive mitigation framework
for popular frequency estimation, which contains a suite of novel defenses,
including malicious user detection, attack pattern recognition, and damaged
utility recovery. In addition to existing attacks, we explore new adaptive
adversarial activities for our mitigation design. For detection, we present a
new method to precisely identify bogus reports and thus LDP aggregation can be
performed over the ``clean'' data. When the attack behavior becomes stealthy
and direct filtering out malicious users is difficult, we further propose a
detection that can effectively recognize hidden adversarial patterns, thus
facilitating the decision-making of service providers. These detection methods
require no additional data and attack information and incur minimal
computational cost. Our experiment demonstrates their excellent performance and
substantial improvement over previous work in various settings. In addition, we
conduct an empirical analysis of LDP post-processing for corrupted data
recovery and propose a new post-processing method, through which we reveal new
insights into protocol recommendations in practice and key design principles
for future research.

</details>


### [182] [Are Crypto Ecosystems (De)centralizing? A Framework for Longitudinal Analysis](https://arxiv.org/abs/2506.02324)
*Harang Ju,Ehsan Valavi,Madhav Kumar,Sinan Aral*

Main category: cs.CR

TL;DR: 这篇论文提出了一种系统性的框架，用于测量加密生态系统的去中心化程度，并将其应用于七个主要的加密生态系统，揭示了加密货币在大多数情况下随时间变得更加去中心化，但最近在共识层、NFT市场和开发者方面出现了向中心化的转变。


<details>
  <summary>Details</summary>
Motivation: 区块链技术依赖于去中心化来抵御故障和攻击，同时无需可信中介进行操作。尽管行业专家将去中心化视为其承诺和颠覆潜力的核心，但围绕区块链构建的加密生态系统是否随着时间的推移变得越来越去中心化或中心化尚不清楚。随着加密货币在促进经济交易和点对点交互中扮演着越来越重要的角色，测量其去中心化程度变得至关重要。

Method: 作者提出了一种系统性框架，用于测量加密生态系统的去中心化程度，并比较常用的去中心化指标。该框架被应用于七个主要的加密生态系统，涵盖了五个不同的子系统，并且跨越了超过15年的生命周期。

Result: 分析结果显示，尽管加密货币在很大程度上随着时间的推移变得更加去中心化，但最近的趋势表明，在共识层、NFT市场和开发者方面出现了向中心化的转变。

Conclusion: 本研究为研究人员、政策制定者和从业者提供了关于加密生态系统设计、监管和实施的信息，并为未来的研究提供了一个系统、可复制的基础。

Abstract: Blockchain technology relies on decentralization to resist faults and attacks
while operating without trusted intermediaries. Although industry experts have
touted decentralization as central to their promise and disruptive potential,
it is still unclear whether the crypto ecosystems built around blockchains are
becoming more or less decentralized over time. As crypto plays an increasing
role in facilitating economic transactions and peer-to-peer interactions,
measuring their decentralization becomes even more essential. We thus propose a
systematic framework for measuring the decentralization of crypto ecosystems
over time and compare commonly used decentralization metrics. We applied this
framework to seven prominent crypto ecosystems, across five distinct subsystems
and across their lifetime for over 15 years. Our analysis revealed that while
crypto has largely become more decentralized over time, recent trends show a
shift toward centralization in the consensus layer, NFT marketplaces, and
developers. Our framework and results inform researchers, policymakers, and
practitioners about the design, regulation, and implementation of crypto
ecosystems and provide a systematic, replicable foundation for future studies.

</details>


### [183] [MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models](https://arxiv.org/abs/2506.02362)
*Xueqi Cheng,Minxing Zheng,Shixiang Zhu,Yushun Dong*

Main category: cs.CR

TL;DR: The paper proposes MISLEADER, a novel defense strategy against model extraction attacks that doesn't rely on out-of-distribution (OOD) assumptions. It formulates model protection as a bilevel optimization problem and combines data augmentation with an ensemble of heterogeneous distilled models to preserve utility while reducing extractability.


<details>
  <summary>Details</summary>
Motivation: Model extraction attacks threaten the intellectual property of machine-learning-as-a-service providers by replicating black-box model functionality through queries. Current defenses mostly depend on detecting out-of-distribution samples which is becoming unreliable due to diverse datasets and limited query budgets.

Method: MISLEADER formulates model protection as a bilevel optimization problem that preserves predictive fidelity on benign inputs and reduces extractability by potential clone models. The framework uses data augmentation to simulate attacker queries and employs an ensemble of heterogeneous distilled models for robustness and diversity.

Result: Extensive experiments across various settings validate that MISLEADER effectively preserves utility while resisting model extraction attacks.

Conclusion: MISLEADER presents a novel approach to defend against model extraction attacks without relying on OOD assumptions, providing both theoretical and practical effectiveness.

Abstract: Model extraction attacks aim to replicate the functionality of a black-box
model through query access, threatening the intellectual property (IP) of
machine-learning-as-a-service (MLaaS) providers. Defending against such attacks
is challenging, as it must balance efficiency, robustness, and utility
preservation in the real-world scenario. Despite the recent advances, most
existing defenses presume that attacker queries have out-of-distribution (OOD)
samples, enabling them to detect and disrupt suspicious inputs. However, this
assumption is increasingly unreliable, as modern models are trained on diverse
datasets and attackers often operate under limited query budgets. As a result,
the effectiveness of these defenses is significantly compromised in realistic
deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of
dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does
not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel
optimization problem that simultaneously preserves predictive fidelity on
benign inputs and reduces extractability by potential clone models. Our
framework combines data augmentation to simulate attacker queries with an
ensemble of heterogeneous distilled models to improve robustness and diversity.
We further provide a tractable approximation algorithm and derive theoretical
error bounds to characterize defense effectiveness. Extensive experiments
across various settings validate the utility-preserving and
extraction-resistant properties of our proposed defense strategy. Our code is
available at https://github.com/LabRAI/MISLEADER.

</details>


### [184] [A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges](https://arxiv.org/abs/2506.02438)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Main category: cs.CR

TL;DR: This paper reviews and analyzes various intrusion detection methods that use machine learning and deep learning techniques, providing a comprehensive overview of classifiers, algorithms, datasets, detected attacks, evaluation metrics, and conclusions for future IDS research.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the effectiveness of Intrusion Detection Systems (IDS) by incorporating machine learning algorithms, thereby improving security attack detection accuracy in an increasingly technology-dependent world.

Method: The authors conducted a literature review on machine learning and deep learning-based intrusion detection techniques using cybersecurity datasets such as KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017, and CSE-CIC-IDS2018. They analyzed methods involving SVM, KNN, DT, LR, NB, RF, XGBOOST, Adaboost, and ANN.

Result: A detailed tabular analysis was presented, which includes the datasets used, classifiers employed, attacks detected, evaluation metrics, and drawn conclusions. This provides a clear understanding of the strengths and limitations of different approaches.

Conclusion: This article offers a thorough review of intrusion detection methods and serves as a guide for future IDS research, highlighting existing challenges and suggesting potential improvements.

Abstract: IDS aims to protect computer networks from security threats by detecting,
notifying, and taking appropriate action to prevent illegal access and protect
confidential information. As the globe becomes increasingly dependent on
technology and automated processes, ensuring secured systems, applications, and
networks has become one of the most significant problems of this era. The
global web and digital technology have significantly accelerated the evolution
of the modern world, necessitating the use of telecommunications and data
transfer platforms. Researchers are enhancing the effectiveness of IDS by
incorporating popular datasets into machine learning algorithms. IDS, equipped
with machine learning classifiers, enhances security attack detection accuracy
by identifying normal or abnormal network traffic. This paper explores the
methods of capturing and reviewing intrusion detection systems (IDS) and
evaluates the challenges existing datasets face. A deluge of research on
machine learning (ML) and deep learning (DL) architecture-based intrusion
detection techniques has been conducted in the past ten years on various
cybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,
and CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth
analysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,
RF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,
explaining the role of the classifiers and algorithms used. A detailed tabular
analysis highlights the datasets used, classifiers employed, attacks detected,
evaluation metrics, and conclusions drawn. This article offers a thorough
review for future IDS research.

</details>


### [185] [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
*Kalyan Nakka,Nitesh Saxena*

Main category: cs.CR

TL;DR: A new black-box jailbreak attack called BitBypass is developed, which uses hyphen-separated bitstream camouflage to bypass safety alignment in LLMs. Evaluated on five LLMs, BitBypass shows high stealthiness and success rate.


<details>
  <summary>Details</summary>
Motivation: To address the inherent risk of generating harmful content by LLMs and challenge the robustness of safety alignment techniques.

Method: Developed BitBypass, a black-box jailbreak attack using hyphen-separated bitstream camouflage, exploiting fundamental information representation as continuous bits.

Result: BitBypass successfully bypasses safety alignment in five state-of-the-art LLMs and outperforms several existing jailbreak attacks in terms of stealthiness and success.

Conclusion: BitBypass is effective and efficient in jailbreaking aligned LLMs, highlighting potential vulnerabilities in current safety alignment methods.

Abstract: The inherent risk of generating harmful and unsafe content by Large Language
Models (LLMs), has highlighted the need for their safety alignment. Various
techniques like supervised fine-tuning, reinforcement learning from human
feedback, and red-teaming were developed for ensuring the safety alignment of
LLMs. However, the robustness of these aligned LLMs is always challenged by
adversarial attacks that exploit unexplored and underlying vulnerabilities of
the safety alignment. In this paper, we develop a novel black-box jailbreak
attack, called BitBypass, that leverages hyphen-separated bitstream camouflage
for jailbreaking aligned LLMs. This represents a new direction in jailbreaking
by exploiting fundamental information representation of data as continuous
bits, rather than leveraging prompt engineering or adversarial manipulations.
Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude
3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the
capabilities of BitBypass in bypassing their safety alignment and tricking them
into generating harmful and unsafe content. Further, we observed that BitBypass
outperforms several state-of-the-art jailbreak attacks in terms of stealthiness
and attack success. Overall, these results highlights the effectiveness and
efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.

</details>


### [186] [Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems](https://arxiv.org/abs/2506.02546)
*Pengfei He,Zhenwei Dai,Xianfeng Tang,Yue Xing,Hui Liu,Jingying Zeng,Qiankun Peng,Shrivats Agrawal,Samarth Varshney,Suhang Wang,Jiliang Tang,Qi He*

Main category: cs.CR

TL;DR: 提出了一种名为A-Trust的新方法，通过内部注意力模式评估消息的可信度，并在此基础上构建了信任管理系统（TMS），提高了多智能体系统对恶意输入的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究在解决LLM-MAS中接收到不可靠消息的问题时，仅关注单一类型的有害性，缺乏从多个可信度视角进行全面分析的方法。

Method: 提出了Attention Trust Score (A-Trust)，一种基于注意力机制的轻量级方法，用于评估消息的可信度。A-Trust通过分析六个正交的信任维度，发现LLM中的某些注意力头专门用于检测特定类型的违规行为。利用这些见解，A-Trust直接从内部注意力模式推断可信度，无需外部提示或验证器。进一步基于A-Trust开发了一个原则性和高效的信任管理系统（TMS），实现消息级和代理级的信任评估。

Result: 实验表明，在各种多智能体环境和任务中应用TMS可以显著提高对恶意输入的鲁棒性。

Conclusion: A-Trust和TMS为LLM-MAS提供了一种新的、有效的解决方案，以增强其对不可靠消息的抵御能力。

Abstract: Large Language Model-based Multi-Agent Systems (LLM-MAS) have demonstrated
strong capabilities in solving complex tasks but remain vulnerable when agents
receive unreliable messages. This vulnerability stems from a fundamental gap:
LLM agents treat all incoming messages equally without evaluating their
trustworthiness. While some existing studies approach the trustworthiness, they
focus on a single type of harmfulness rather than analyze it in a holistic
approach from multiple trustworthiness perspectives. In this work, we propose
Attention Trust Score (A-Trust), a lightweight, attention-based method for
evaluating message trustworthiness. Inspired by human communication
literature[1], through systematically analyzing attention behaviors across six
orthogonal trust dimensions, we find that certain attention heads in the LLM
specialize in detecting specific types of violations. Leveraging these
insights, A-Trust directly infers trustworthiness from internal attention
patterns without requiring external prompts or verifiers. Building upon
A-Trust, we develop a principled and efficient trust management system (TMS)
for LLM-MAS, enabling both message-level and agent-level trust assessment.
Experiments across diverse multi-agent settings and tasks demonstrate that
applying our TMS significantly enhances robustness against malicious inputs.

</details>


### [187] [CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale](https://arxiv.org/abs/2506.02548)
*Zhun Wang,Tianneng Shi,Jingxuan He,Matthew Cai,Jialin Zhang,Dawn Song*

Main category: cs.CR

TL;DR: The paper introduces CyberGym, a large-scale cybersecurity evaluation framework with real-world vulnerabilities to assess LLM agents' capabilities in generating PoC tests for vulnerability reproduction. Evaluation shows low success rates and potential for discovering new vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for assessing LLM agents' cybersecurity capabilities are insufficient, failing to capture real-world scenarios or being limited in scope.

Method: Introduced CyberGym, featuring 1,507 real-world vulnerabilities across 188 software projects, focusing on the generation of PoC tests based on text descriptions and source repositories.

Result: Evaluation across 4 agent frameworks and 9 LLMs showed a low 11.9% success rate in vulnerability reproduction, mainly on simpler cases. Generated PoCs identified 15 zero-day vulnerabilities.

Conclusion: CyberGym highlights the challenges in assessing LLM agents' cybersecurity capabilities and demonstrates their potential in discovering new vulnerabilities.

Abstract: Large language model (LLM) agents are becoming increasingly skilled at
handling cybersecurity tasks autonomously. Thoroughly assessing their
cybersecurity capabilities is critical and urgent, given the high stakes in
this domain. However, existing benchmarks fall short, often failing to capture
real-world scenarios or being limited in scope. To address this gap, we
introduce CyberGym, a large-scale and high-quality cybersecurity evaluation
framework featuring 1,507 real-world vulnerabilities found and patched across
188 large software projects. While it includes tasks of various settings,
CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests
for vulnerability reproduction, based on text descriptions and corresponding
source repositories. Solving this task is particularly challenging, as it
requires comprehensive reasoning across entire codebases to locate relevant
code fragments and produce effective PoCs that accurately trigger the target
vulnerability starting from the program's entry point. Our evaluation across 4
state-of-the-art agent frameworks and 9 LLMs reveals that even the best
combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9%
reproduction success rate, mainly on simpler cases. Beyond reproducing
historical vulnerabilities, we find that PoCs generated by LLM agents can
reveal new vulnerabilities, identifying 15 zero-days affecting the latest
versions of the software projects.

</details>


### [188] [Tarallo: Evading Behavioral Malware Detectors in the Problem Space](https://arxiv.org/abs/2506.02660)
*Gabriele Digregorio,Salvatore Maccarrone,Mario D'Onghia,Luigi Gallo,Michele Carminati,Mario Polino,Stefano Zanero*

Main category: cs.CR

TL;DR: 通过新的特征空间算法PS-FGSM和两种问题空间策略，Tarallo框架在对抗攻击中显著优于先前的工作。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗攻击往往未能同时在特征空间和问题空间找到有效的解决方案，特别是在处理恶意软件固有的非确定性时。

Method: 提出了一种新的特征空间算法PS-FGSM，并采用了两种专门针对问题空间非确定性的策略，实施这些方法的Tarallo框架能够在白盒和黑盒场景下都取得优异表现。

Result: 初步分析表明，Tarallo在特征空间和问题空间攻击中的成功率高达99%，同时显著减少了导致误分类所需的修改次数。

Conclusion: Tarallo框架在对抗攻击方面表现出色，优于先前的研究成果。

Abstract: Machine learning algorithms can effectively classify malware through dynamic
behavior but are susceptible to adversarial attacks. Existing attacks, however,
often fail to find an effective solution in both the feature and problem
spaces. This issue arises from not addressing the intrinsic nondeterministic
nature of malware, namely executing the same sample multiple times may yield
significantly different behaviors. Hence, the perturbations computed for a
specific behavior may be ineffective for others observed in subsequent
executions. In this paper, we show how an attacker can augment their chance of
success by leveraging a new and more efficient feature space algorithm for
sequential data, which we have named PS-FGSM, and by adopting two problem space
strategies specially tailored to address nondeterminism in the problem space.
We implement our novel algorithm and attack strategies in Tarallo, an
end-to-end adversarial framework that significantly outperforms previous works
in both white and black-box scenarios. Our preliminary analysis in a sandboxed
environment and against two RNN-based malware detectors, shows that Tarallo
achieves a success rate up to 99% on both feature and problem space attacks
while significantly minimizing the number of modifications required for
misclassification.

</details>


### [189] [Decentralized COVID-19 Health System Leveraging Blockchain](https://arxiv.org/abs/2506.02674)
*Lingsheng Chen,Shipeng Ye,Xiaoqi Li*

Main category: cs.CR

TL;DR: This paper designs a blockchain-based COVID-19 health system to solve the problems of traditional EHR management, ensuring data privacy through encryption and authorized access.


<details>
  <summary>Details</summary>
Motivation: The exponential growth of medical data and the limitations of traditional centralized EHR management motivate the exploration of blockchain technology for managing health records.

Method: The paper designs a blockchain-based health system for COVID-19, using searchable encryption for data retrieval and proxy re-encryption for authorized access. It also implements some functions like data upload and retrieval based on Hyperledger Fabric architecture with Go language chaincode.

Result: The implemented system functions include data upload, latest and historical data retrieval, demonstrating the feasibility of blockchain in EHR management while preserving data privacy.

Conclusion: A blockchain-based health system can effectively address the shortcomings of traditional EHR systems, providing secure and efficient data management and sharing.

Abstract: With the development of the Internet, the amount of data generated by the
medical industry each year has grown exponentially. The Electronic Health
Record (EHR) manages the electronic data generated during the user's treatment
process. Typically, an EHR data manager belongs to a medical institution. This
traditional centralized data management model has many unreasonable or
inconvenient aspects, such as difficulties in data sharing, and it is hard to
verify the authenticity and integrity of the data. The decentralized,
non-forgeable, data unalterable and traceable features of blockchain are in
line with the application requirements of EHR. This paper takes the most common
COVID-19 as the application scenario and designs a COVID-19 health system based
on blockchain, which has extensive research and application value. Considering
that the public and transparent nature of blockchain violates the privacy
requirements of some health data, in the system design stage, from the
perspective of practical application, the data is divided into public data and
private data according to its characteristics. For private data, data
encryption methods are adopted to ensure data privacy. The searchable
encryption technology is combined with blockchain technology to achieve the
retrieval function of encrypted data. Then, the proxy re-encryption technology
is used to realize authorized access to data. In the system implementation
part, based on the Hyperledger Fabric architecture, some functions of the
system design are realized, including data upload, retrieval of the latest data
and historical data. According to the environment provided by the development
architecture, Go language chaincode (smart contract) is written to implement
the relevant system functions.

</details>


### [190] [Poster: FedBlockParadox -- A Framework for Simulating and Securing Decentralized Federated Learning](https://arxiv.org/abs/2506.02679)
*Gabriele Digregorio,Francesco Bleggi,Federico Caroli,Michele Carminati,Stefano Zanero,Stefano Longari*

Main category: cs.CR

TL;DR: In decentralized federated learning, combining federated learning with blockchain-based systems is a promising direction. However, flexible tools for evaluating system robustness under adversarial conditions are lacking. To address this issue, the paper introduces FedBlockParadox, which is a modular framework used to model and evaluate decentralized federated learning systems based on blockchain technologies, focusing on resilience against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Current research in decentralized federated learning lacks tools to evaluate system robustness under adversarial conditions.

Method: The method involves creating FedBlockParadox, a modular framework that supports multiple consensus protocols, validation methods, aggregation strategies, and configurable attack models. It allows for controlled experiments in secure, decentralized learning solutions.

Result: FedBlockParadox successfully provides researchers with a valuable resource for developing secure, decentralized learning solutions.

Conclusion: FedBlockParadox fills an important gap in the evaluation of robustness in decentralized federated learning systems, and it is open-source and extensible.

Abstract: A significant body of research in decentralized federated learning focuses on
combining the privacy-preserving properties of federated learning with the
resilience and transparency offered by blockchain-based systems. While these
approaches are promising, they often lack flexible tools to evaluate system
robustness under adversarial conditions. To fill this gap, we present
FedBlockParadox, a modular framework for modeling and evaluating decentralized
federated learning systems built on blockchain technologies, with a focus on
resilience against a broad spectrum of adversarial attack scenarios. It
supports multiple consensus protocols, validation methods, aggregation
strategies, and configurable attack models. By enabling controlled experiments,
FedBlockParadox provides a valuable resource for researchers developing secure,
decentralized learning solutions. The framework is open-source and built to be
extensible by the community.

</details>


### [191] [Privacy Leaks by Adversaries: Adversarial Iterations for Membership Inference Attack](https://arxiv.org/abs/2506.02711)
*Jing Xue,Zhishen Sun,Haishan Ye,Luo Luo,Xiangyu Chang,Ivor Tsang,Guang Dai*

Main category: cs.CR

TL;DR: The paper introduces IMIA, a new membership inference attack strategy that uses the process of generating adversarial samples to infer membership in machine learning models.


<details>
  <summary>Details</summary>
Motivation: To explore new methods for evaluating the privacy risks of machine learning models beyond traditional posterior output-based membership inference attacks.

Method: IMIA leverages the number of iterations required to generate an adversarial sample as a feature for inferring whether a specific sample was part of a model's training set.

Result: Experiments across multiple models and datasets show that using the number of iterations for generating adversarial samples is a reliable feature for membership inference, effective in both black-box and white-box attack scenarios.

Conclusion: This work offers a novel perspective on assessing model privacy and demonstrates the potential of adversarial example-based features for evaluating privacy leakage.

Abstract: Membership inference attack (MIA) has become one of the most widely used and
effective methods for evaluating the privacy risks of machine learning models.
These attacks aim to determine whether a specific sample is part of the model's
training set by analyzing the model's output. While traditional membership
inference attacks focus on leveraging the model's posterior output, such as
confidence on the target sample, we propose IMIA, a novel attack strategy that
utilizes the process of generating adversarial samples to infer membership. We
propose to infer the member properties of the target sample using the number of
iterations required to generate its adversarial sample. We conduct experiments
across multiple models and datasets, and our results demonstrate that the
number of iterations for generating an adversarial sample is a reliable feature
for membership inference, achieving strong performance both in black-box and
white-box attack scenarios. This work provides a new perspective for evaluating
model privacy and highlights the potential of adversarial example-based
features for privacy leakage assessment.

</details>


### [192] [ATAG: AI-Agent Application Threat Assessment with Attack Graphs](https://arxiv.org/abs/2506.02859)
*Parth Atulbhai Gandhi,Akansha Shukla,David Tayouri,Beni Ifland,Yuval Elovici,Rami Puzis,Asaf Shabtai*

Main category: cs.CR

TL;DR: The paper presents ATAG, a new framework that extends MulVAL to analyze security risks in AI-agent applications using custom facts and rules. It also introduces LVD for documenting LLM vulnerabilities. Case studies show ATAG's ability to model complex attack scenarios in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Evaluating the security of multi-agent systems powered by large language models is challenging due to complex internal dynamics and evolving LLM vulnerabilities. Traditional attack graph methods are insufficient for modeling attacks on LLMs.

Method: ATAG extends the MulVAL logic-based AG generation tool with custom facts and interaction rules to represent AI-agent topologies, vulnerabilities, and attack scenarios. A LLM vulnerability database (LVD) is created to standardize LLM vulnerabilities documentation.

Result: Case studies demonstrated ATAG's ability to model and generate attack graphs for sophisticated, multi-step attack scenarios exploiting various vulnerabilities in interconnected agents.

Conclusion: ATAG is an important step towards understanding, visualizing, and prioritizing complex attack paths in multi-agent AI systems, facilitating proactive identification and mitigation of AI-agent threats.

Abstract: Evaluating the security of multi-agent systems (MASs) powered by large
language models (LLMs) is challenging, primarily because of the systems'
complex internal dynamics and the evolving nature of LLM vulnerabilities.
Traditional attack graph (AG) methods often lack the specific capabilities to
model attacks on LLMs. This paper introduces AI-agent application Threat
assessment with Attack Graphs (ATAG), a novel framework designed to
systematically analyze the security risks associated with AI-agent
applications. ATAG extends the MulVAL logic-based AG generation tool with
custom facts and interaction rules to accurately represent AI-agent topologies,
vulnerabilities, and attack scenarios. As part of this research, we also
created the LLM vulnerability database (LVD) to initiate the process of
standardizing LLM vulnerabilities documentation. To demonstrate ATAG's
efficacy, we applied it to two multi-agent applications. Our case studies
demonstrated the framework's ability to model and generate AGs for
sophisticated, multi-step attack scenarios exploiting vulnerabilities such as
prompt injection, excessive agency, sensitive information disclosure, and
insecure output handling across interconnected agents. ATAG is an important
step toward a robust methodology and toolset to help understand, visualize, and
prioritize complex attack paths in multi-agent AI systems (MAASs). It
facilitates proactive identification and mitigation of AI-agent threats in
multi-agent applications.

</details>


### [193] [When Blockchain Meets Crawlers: Real-time Market Analytics in Solana NFT Markets](https://arxiv.org/abs/2506.02892)
*Chengxin Shen,Zhongwen Li,Xiaoqi Li,Zongwei Li*

Main category: cs.CR

TL;DR: This paper designs and implements a web crawler system based on the Solana blockchain for collecting and analyzing NFT market data. It uses Selenium and Scrapy to gather information, applies time series analysis to examine market dynamics, and evaluates risk-return using mean-variance optimization model.


<details>
  <summary>Details</summary>
Motivation: To automate the collection and analysis of market data for popular non-fungible tokens (NFTs) on the Solana blockchain, providing timely market insights and investment strategies.

Method: The method involves using Selenium to collect basic information and transaction data of popular NFTs on the Solana chain, combining Scrapy framework to analyze transaction records of the Magic Eden trading market, employing time series analysis to examine NFT market dynamics, and evaluating risk and return using the mean-variance optimization model.

Result: The experimental results demonstrate that the combination of crawler technology and financial analytics can effectively analyze NFT data on the Solana blockchain and provide timely market insights and investment strategies.

Conclusion: This study shows the effectiveness of integrating web crawling and financial analytics in analyzing NFT data and offers a reference for further research in digital currencies.

Abstract: In this paper, we design and implement a web crawler system based on the
Solana blockchain for the automated collection and analysis of market data for
popular non-fungible tokens (NFTs) on the chain. Firstly, the basic information
and transaction data of popular NFTs on the Solana chain are collected using
the Selenium tool. Secondly, the transaction records of the Magic Eden trading
market are thoroughly analyzed by combining them with the Scrapy framework to
examine the price fluctuations and market trends of NFTs. In terms of data
analysis, this paper employs time series analysis to examine the dynamics of
the NFT market and seeks to identify potential price patterns. In addition, the
risk and return of different NFTs are evaluated using the mean-variance
optimization model, taking into account their characteristics, such as
illiquidity and market volatility, to provide investors with data-driven
portfolio recommendations. The experimental results show that the combination
of crawler technology and financial analytics can effectively analyze NFT data
on the Solana blockchain and provide timely market insights and investment
strategies. This study provides a reference for further exploration in the
field of digital currencies.

</details>


### [194] [An Algorithmic Pipeline for GDPR-Compliant Healthcare Data Anonymisation: Moving Toward Standardisation](https://arxiv.org/abs/2506.02942)
*Hamza Khan,Lore Menten,Liesbet M. Peeters*

Main category: cs.CR

TL;DR: The paper introduces a GDPR-compliant anonymisation pipeline for healthcare real-world data (RWD) that enhances privacy while preserving utility.


<details>
  <summary>Details</summary>
Motivation: To standardize the anonymization of real-world data in healthcare to ensure compliance with GDPR while maintaining data utility, given the complexities introduced by GDPR's broad definitions of quasi-identifiers and sensitive attributes.

Method: A systematic literature review was conducted, followed by the development and implementation of a three-stage anonymization pipeline: identification, de-identification, and quasi-identifier dimension evaluation. The pipeline was tested on two mock datasets (500 and 1000 rows).

Result: Privacy metrics improved significantly (k-anonymity from 1 to 4 for 500 rows and 1 to 110 for 1000 rows), while non-uniform entropy scores indicated consistent utility (69.26% and 69.05%).

Conclusion: The proposed pipeline offers a reproducible method for identifying quasi-identifiers and sensitive attributes in healthcare RWD, ensuring GDPR compliance and promoting data privacy and open science.

Abstract: High-quality real-world data (RWD) is essential for healthcare but must be
transformed to comply with the General Data Protection Regulation (GDPR). GDPRs
broad definitions of quasi-identifiers (QIDs) and sensitive attributes (SAs)
complicate implementation. We aim to standardise RWD anonymisation for GDPR
compliance while preserving data utility by introducing an algorithmic method
to identify QIDs and SAs and evaluate utility in anonymised datasets. We
conducted a systematic literature review via ProQuest and PubMed to inform a
three-stage anonymisation pipeline: identification, de-identification, and
quasi-identifier dimension evaluation. The pipeline was implemented, validated,
and tested on two mock RWD datasets (500 and 1000 rows). Privacy was assessed
using k-anonymity, l-diversity, and t-closeness; utility was measured by
non-uniform entropy (NUE). The review yielded two studies on QID/SA
identification and five on utility metrics. Applying the pipeline, attributes
were classified by re-identification risk using alpha and beta thresholds (25
percent/1 percent for 500 rows; 10 percent/1 percent for 1000 rows). Privacy
metrics improved k-anonymity from 1 to 4 (500 rows) and 1 to 110 (1000 rows).
NUE scores were 69.26 percent and 69.05 percent, respectively, indicating
consistent utility despite varying privacy gains. We present a GDPR-compliant
anonymisation pipeline for healthcare RWD that provides a reproducible approach
to QID/SA identification and utility evaluation; publicly available code
promotes standardisation, data privacy, and open science.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [195] [Poster: libdebug, Build Your Own Debugger for a Better (Hello) World](https://arxiv.org/abs/2506.02667)
*Gabriele Digregorio,Roberto Alessandro Bertolini,Francesco Panebianco,Mario Polino*

Main category: cs.SE

TL;DR: The paper introduces libdebug, a Python library for programmatic debugging of userland binary executables, which has lower latency than GDB and is versatile for various applications.


<details>
  <summary>Details</summary>
Motivation: Automated debugging needs a framework for programmable debugging workflow, but existing debuggers are either tailored for human interaction or focus on kernel space, lacking functionality in userland.

Method: Developed libdebug, a Python library with a user-friendly API for programmatic debugging of userland binaries, applicable to software engineering, reverse engineering, and software security.

Result: Demonstrated through case studies and benchmarks that libdebug has a median latency 3 to 4 times lower than GDB for syscall and breakpoint handling.

Conclusion: libdebug fills the gap in programmatic userland debugging, offering a versatile and high-performance tool released as an open-source project.

Abstract: Automated debugging, long pursued in a variety of fields from software
engineering to cybersecurity, requires a framework that offers the building
blocks for a programmable debugging workflow. However, existing debuggers are
primarily tailored for human interaction, and those designed for programmatic
debugging focus on kernel space, resulting in limited functionality in
userland. To fill this gap, we introduce libdebug, a Python library for
programmatic debugging of userland binary executables. libdebug offers a
user-friendly API that enables developers to build custom debugging tools for
various applications, including software engineering, reverse engineering, and
software security. It is released as an open-source project, along with
comprehensive documentation to encourage use and collaboration across the
community. We demonstrate the versatility and performance of libdebug through
case studies and benchmarks, all of which are publicly available. We find that
the median latency of syscall and breakpoint handling in libdebug is 3 to 4
times lower compared to that of GDB.

</details>


### [196] [Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability](https://arxiv.org/abs/2506.02073)
*Mengliang He,Jiayi Zeng,Yankai Jiang,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.SE

TL;DR: This paper introduces Flow2Code, a new benchmark for evaluating flowchart-based code generation across 15 programming languages. Despite using 13 multimodal LLMs, current models can't perfectly generate code from flowcharts, but supervised fine-tuning improves performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of benchmarks for flowchart-based code generation and promote further research in this area.

Method: The method involves creating Flow2Code, a benchmark with 5,622 code segments paired with 16,866 flowcharts across three types (code, UML, and pseudocode). Then, extensive experiments are conducted with 13 multimodal LLMs.

Result: Results indicate that current LLMs cannot perfectly generate code based on flowcharts, but supervised fine-tuning significantly enhances model performance.

Conclusion: Flow2Code provides a valuable resource for advancing flowchart-based code generation research, and the findings highlight the importance of supervised fine-tuning.

Abstract: While large language models (LLMs) show promise in code generation, existing
benchmarks neglect the flowchart-based code generation. To promote further
research on flowchart-based code generation, this work presents Flow2Code, a
novel benchmark for flowchart-based code generation evaluation. The evaluation
dataset spans 15 programming languages and includes 5,622 code segments paired
with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive
experiments with 13 multimodal LLMs reveal that current LLMs can not generate
code based on flowcharts perfectly. Besides, experiment results show that the
supervised fine-tuning technique contributes greatly to the models'
performance. We publicly release our code and datasets at
https://github.com/hml-github/Flow2Code.

</details>


### [197] [The Impact of Software Testing with Quantum Optimization Meets Machine Learning](https://arxiv.org/abs/2506.02090)
*Gopichand Bandarupalli*

Main category: cs.SE

TL;DR: This paper presents a hybrid framework integrating Quantum Annealing with Machine Learning to optimize test case prioritization in CI/CD pipelines, demonstrating increased defect detection efficiency and reduced test execution time.


<details>
  <summary>Details</summary>
Motivation: Modern software systems complexity challenges efficient testing, as traditional machine learning struggles with large test suites.

Method: A hybrid framework integrating Quantum Annealing with ML is presented to optimize test case prioritization in CI/CD pipelines. It leverages quantum optimization for better performance.

Result: Achieves a 25% increase in defect detection efficiency and a 30% reduction in test execution time versus classical ML, validated on the Defects4J dataset. Demonstrates robustness across evolving codebases in a simulated CI/CD environment.

Conclusion: The framework addresses quantum hardware limits, CI/CD integration, and scalability for future hybrid quantum-classical ecosystems, offering a transformative approach to software quality assurance.

Abstract: Modern software systems complexity challenges efficient testing, as
traditional machine learning (ML) struggles with large test suites. This
research presents a hybrid framework integrating Quantum Annealing with ML to
optimize test case prioritization in CI/CD pipelines. Leveraging quantum
optimization, it achieves a 25 percent increase in defect detection efficiency
and a 30 percent reduction in test execution time versus classical ML,
validated on the Defects4J dataset. A simulated CI/CD environment demonstrates
robustness across evolving codebases. Visualizations, including defect heatmaps
and performance graphs, enhance interpretability. The framework addresses
quantum hardware limits, CI/CD integration, and scalability for 2025s hybrid
quantum-classical ecosystems, offering a transformative approach to software
quality assurance.

</details>


### [198] [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)
*Nguyen-Khang Le,Quan Minh Bui,Minh Ngoc Nguyen,Hiep Nguyen,Trung Vo,Son T. Luu,Shoshin Nomura,Minh Le Nguyen*

Main category: cs.SE

TL;DR: This paper introduces an automated system that uses graph structures and LLMs to generate test cases for web application testing, focusing on site navigation and form filling. It contributes a novel integration of graph structures and LLMs, a state graph-based approach for form-filling test cases, and a comprehensive dataset for evaluating form-interaction testing.


<details>
  <summary>Details</summary>
Motivation: Web applications are critical in modern software ecosystems, but ensuring their reliability is difficult due to the complexity and dynamic nature of web interfaces. Existing LLMs have limitations in handling dynamic navigation flows and complex form interactions.

Method: The system generates test cases for two key aspects of web application testing: site navigation and form filling. For site navigation, it employs screen transition graphs and LLMs to model navigation flows and generate test scenarios. For form filling, it uses state graphs to handle conditional forms and automates Selenium script generation.

Result: The experimental results show that the system effectively improves test coverage and robustness, advancing the state of web application testing.

Conclusion: The paper concludes that the presented automated system significantly enhances web application testing by improving test coverage and robustness through the novel use of graph structures and LLMs.

Abstract: Web applications are critical to modern software ecosystems, yet ensuring
their reliability remains challenging due to the complexity and dynamic nature
of web interfaces. Recent advances in large language models (LLMs) have shown
promise in automating complex tasks, but limitations persist in handling
dynamic navigation flows and complex form interactions. This paper presents an
automated system for generating test cases for two key aspects of web
application testing: site navigation and form filling. For site navigation, the
system employs screen transition graphs and LLMs to model navigation flows and
generate test scenarios. For form filling, it uses state graphs to handle
conditional forms and automates Selenium script generation. Key contributions
include: (1) a novel integration of graph structures and LLMs for site
navigation testing, (2) a state graph-based approach for automating
form-filling test cases, and (3) a comprehensive dataset for evaluating
form-interaction testing. Experimental results demonstrate the system's
effectiveness in improving test coverage and robustness, advancing the state of
web application testing.

</details>


### [199] [Rethinking the effects of data contamination in Code Intelligence](https://arxiv.org/abs/2506.02791)
*Zhen Yang,Hongyi Lin,Yifan He,Jie Xu,Zeyu Sun,Shuo Liu,Pengpeng Wang,Zhongxing Yu,Qingyuan Liang*

Main category: cs.SE

TL;DR: In recent years, code intelligence has gained increasing importance in the field of automated software engineering. This paper presents a systematic empirical study to investigate the fine-grained data contamination on code intelligence tasks.


<details>
  <summary>Details</summary>
Motivation: The widespread adoption of Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised concerns regarding data contamination and its potential impact on model performance evaluation.

Method: This study involves diverse representative PLMs and LLMs covering three major tasks: code translation, code generation, and code summarization. Categorize contamination scenarios into four types according to the code intelligence practice and construct corresponding experimental and control groups for exploration.

Result: Experimental results show that, under the pre-training, fine-tuning, and inference paradigm adopted by PLMs, even deliberately injecting paired contamination does not lead to significant performance overestimation. But direct inference or small-scale fine-tuning uncovers the contamination effects. In contrast, LLMs with pre-training and inference paradigm are significantly affected by the paired contamination.

Conclusion: Our findings challenge the conventional belief that contamination inevitably leads to performance overestimation, providing new insights into the evaluation and deployment of code intelligence models.

Abstract: In recent years, code intelligence has gained increasing importance in the
field of automated software engineering. Meanwhile, the widespread adoption of
Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised
concerns regarding data contamination and its potential impact on model
performance evaluation. This paper presents a systematic empirical study to
investigate the fine-grained data contamination on code intelligence tasks. Our
study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,
namely LLaMA and StarCoder, covering three major tasks: code translation, code
generation, and code summarization. We categorize contamination scenarios into
four types according to the code intelligence practice, namely input-only,
output-only, unpaired, and paired contamination settings, and construct
corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and
inference paradigm adopted by PLMs, even deliberately injecting paired
contamination does not lead to significant performance overestimation. But
direct inference or small-scale fine-tuning uncovers the contamination effects.
In contrast, LLMs with pre-training and inference paradigm are significantly
affected by the paired contamination. Apart from the above, other contamination
scenarios have no impact on both PLMs and LLMs. Our findings challenge the
conventional belief that contamination inevitably leads to performance
overestimation, providing new insights into the evaluation and deployment of
code intelligence models.

</details>


### [200] [How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face](https://arxiv.org/abs/2506.03013)
*Alexandra González,Xavier Franch,David Lo,Silverio Martínez-Fernández*

Main category: cs.SE

TL;DR: The paper develops a taxonomy of 147 SE tasks and classifies PTMs in Hugging Face, revealing trends and gaps in SE-related ML models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between open-source pre-trained models (PTMs) and Software Engineering (SE) needs by creating an SE-oriented classification system.

Method: Derived a taxonomy of 147 SE tasks, mined PTMs from Hugging Face using their API, filtered for SE relevance through multiple steps including outlier detection, identification of near-identical PTMs, and human annotation validation.

Result: Identified 2,205 SE PTMs, found code generation as the most common SE task with text generation dominating among ML tasks, noted limited attention to requirements engineering and software design, and observed a significant increase in SE PTMs since 2023 Q2.

Conclusion: The developed classification system lays a strong groundwork for future automated SE scenarios, such as selecting appropriate PTMs.

Abstract: Open-Source Pre-Trained Models (PTMs) provide extensive resources for various
Machine Learning (ML) tasks, yet these resources lack a classification tailored
to Software Engineering (SE) needs. To address this gap, we derive a taxonomy
encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a
popular open-source ML repository, Hugging Face (HF). Our repository mining
study began with a systematically gathered database of PTMs from the HF API,
considering their model card descriptions and metadata, and the abstract of the
associated arXiv papers. We confirmed SE relevance through multiple filtering
steps: detecting outliers, identifying near-identical PTMs, and the use of
Gemini 2.0 Flash, which was validated with five pilot studies involving three
human annotators. This approach uncovered 2,205 SE PTMs. We find that code
generation is the most common SE task among PTMs, primarily focusing on
software implementation, while requirements engineering and software design
activities receive limited attention. In terms of ML tasks, text generation
dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly
since 2023 Q2. Our classification provides a solid foundation for future
automated SE scenarios, such as the sampling and selection of suitable PTMs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [201] [Quantum Key Distribution by Quantum Energy Teleportation](https://arxiv.org/abs/2506.02054)
*Shlomi Dolev,Kazuki Ikeda,Yaron Oz*

Main category: quant-ph

TL;DR: A QET-based QKD protocol was constructed, generalized to an N-party info sharing protocol with features for detecting dishonest participants.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Quantum energy teleportation (QET) in quantum key distribution and multi-party information sharing while ensuring security and robustness against noise.

Method: Construction of a QET-based quantum key distribution (QKD) protocol followed by its generalization to an N-party information sharing protocol.

Result: The protocol offers security and robustness analysis against noise in classical and quantum channels. It also includes a feature that allows detection of dishonest participants.

Conclusion: QET can be effectively used in constructing secure quantum key distribution protocols and multi-party information sharing systems with mechanisms to detect dishonesty.

Abstract: Quantum energy teleportation (QET) is a process that leverages quantum
entanglement and local operations to transfer energy between two spatially
separated locations without physically transporting particles or energy
carriers. We construct a QET-based quantum key distribution (QKD) protocol and
analyze its security and robustness to noise in both the classical and the
quantum channels. We generalize the construction to an $N$-party information
sharing protocol, possessing a feature that dishonest participants can be
detected.

</details>


### [202] [Parallel Repetition for Post-Quantum Arguments](https://arxiv.org/abs/2506.02277)
*Andrew Huang,Yael Tauman Kalai*

Main category: quant-ph

TL;DR: The paper demonstrates that parallel repetition of public-coin interactive arguments reduces soundness error exponentially in post-quantum settings, extends results to threshold verifiers, and simplifies analysis for three-message private-coin arguments.


<details>
  <summary>Details</summary>
Motivation: To show the effectiveness of parallel repetition in reducing soundness error for interactive arguments in post-quantum settings and extend understanding to threshold verifiers.

Method: Analyzing public-coin interactive arguments and three-message private-coin arguments under parallel repetition with classical and quantum provers, focusing on classical verifier protocols.

Result: Parallel repetition reduces soundness error exponentially for both public-coin interactive arguments and three-message private-coin arguments, even with threshold verifiers.

Conclusion: Parallel repetition is effective in reducing soundness error in post-quantum settings, extending prior classical prover results to more general threshold verifier scenarios.

Abstract: In this work, we show that parallel repetition of public-coin interactive
arguments reduces the soundness error at an exponential rate even in the
post-quantum setting. Moreover, we generalize this result to hold for threshold
verifiers, where the parallel repeated verifier accepts if and only if at least
$t$ of the executions are accepted (for some threshold $t$). Prior to this
work, these results were known only when the cheating prover was assumed to be
classical.
  We also prove a similar result for three-message private-coin arguments.
Previously, Bostanci, Qian, Spooner, and Yuen (STOC 2024) proved such a
parallel repetition result in the more general setting of quantum protocols,
where the verifier and communication may be quantum. We consider only protocols
where the verifier is classical, but obtain a simplified analysis, and for the
more general setting of threshold verifiers.

</details>


### [203] [Enhancing Interpretability of Quantum-Assisted Blockchain Clustering via AI Agent-Based Qualitative Analysis](https://arxiv.org/abs/2506.02068)
*Yun-Cheng Tsai,Yen-Ku Liu,Samuel Yen-Chi Chen*

Main category: quant-ph

TL;DR: The paper proposes a two-stage framework combining classical clustering methods and AI Agent assisted interpretation to enhance the interpretability of quantum enhanced clustering models for blockchain transaction data. Experiments show that while QNNs perform better in quantitative metrics, AI Agents reveal additional insights such as the singleton cluster phenomenon in QNN driven models.


<details>
  <summary>Details</summary>
Motivation: Blockchain transaction data presents challenges for traditional clustering algorithms due to its high dimensionality, noise, and entanglement. Although quantum enhanced clustering models show performance improvements, their interpretability is limited, which restricts their application in sensitive areas like financial fraud detection and blockchain governance.

Method: The method involves a two-stage analysis framework. In stage one, classical clustering methods and evaluation metrics (Silhouette Score, Davies Bouldin Index, Calinski Harabasz Index) are used to determine optimal cluster count and baseline partition quality. In stage two, an AI Agent generates qualitative interpretations of the clustering results, identifying intra-cluster characteristics and inter-cluster relationships.

Result: Experiments indicate that fully trained Quantum Neural Networks (QNNs) outperform random Quantum Features (QFs) in quantitative metrics. The AI Agent uncovers nuanced differences between these methods, particularly revealing the singleton cluster phenomenon in QNN driven models. The combined insights consistently support a three-cluster configuration.

Conclusion: This work improves the interpretability of quantum assisted blockchain analytics and sets the foundation for future autonomous AI orchestrated clustering frameworks.

Abstract: Blockchain transaction data is inherently high dimensional, noisy, and
entangled, posing substantial challenges for traditional clustering algorithms.
While quantum enhanced clustering models have demonstrated promising
performance gains, their interpretability remains limited, restricting their
application in sensitive domains such as financial fraud detection and
blockchain governance. To address this gap, we propose a two stage analysis
framework that synergistically combines quantitative clustering evaluation with
AI Agent assisted qualitative interpretation. In the first stage, we employ
classical clustering methods and evaluation metrics including the Silhouette
Score, Davies Bouldin Index, and Calinski Harabasz Index to determine the
optimal cluster count and baseline partition quality. In the second stage, we
integrate an AI Agent to generate human readable, semantic explanations of
clustering results, identifying intra cluster characteristics and inter cluster
relationships. Our experiments reveal that while fully trained Quantum Neural
Networks (QNN) outperform random Quantum Features (QF) in quantitative metrics,
the AI Agent further uncovers nuanced differences between these methods,
notably exposing the singleton cluster phenomenon in QNN driven models. The
consolidated insights from both stages consistently endorse the three cluster
configuration, demonstrating the practical value of our hybrid approach. This
work advances the interpretability frontier in quantum assisted blockchain
analytics and lays the groundwork for future autonomous AI orchestrated
clustering frameworks.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [204] [Deep Learning Enhanced Multivariate GARCH](https://arxiv.org/abs/2506.02796)
*Haoyuan Wang,Chen Liu,Minh-Ngoc Tran,Chao Wang*

Main category: q-fin.CP

TL;DR: The paper introduces LSTM-BEKK, a new model that combines deep learning with multivariate GARCH processes for financial return data analysis. It improves on traditional methods by better capturing volatility clustering and co-movement, showing superior performance in portfolio risk forecasts.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective method for modeling multivariate volatility in financial return data, addressing limitations of traditional GARCH-based methods.

Method: Integrates Long Short-Term Memory (LSTM) networks into BEKK models to form the LSTM-BEKK framework, leveraging recurrent neural networks' flexibility and BEKK models' econometric structure.

Result: Empirical results show that the LSTM-BEKK model outperforms in out-of-sample portfolio risk forecast while retaining interpretability.

Conclusion: Hybrid econometric-deep learning models like LSTM-BEKK have significant potential in enhancing financial risk management and multivariate volatility forecasting.

Abstract: This paper introduces a novel multivariate volatility modeling framework,
named Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep
learning into multivariate GARCH processes. By combining the flexibility of
recurrent neural networks with the econometric structure of BEKK models, our
approach is designed to better capture nonlinear, dynamic, and high-dimensional
dependence structures in financial return data. The proposed model addresses
key limitations of traditional multivariate GARCH-based methods, particularly
in capturing persistent volatility clustering and asymmetric co-movement across
assets. Leveraging the data-driven nature of LSTMs, the framework adapts
effectively to time-varying market conditions, offering improved robustness and
forecasting performance. Empirical results across multiple equity markets
confirm that the LSTM-BEKK model achieves superior performance in terms of
out-of-sample portfolio risk forecast, while maintaining the interpretability
from the BEKK models. These findings highlight the potential of hybrid
econometric-deep learning models in advancing financial risk management and
multivariate volatility forecasting.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [205] [No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction](https://arxiv.org/abs/2506.02039)
*Haoshuai Zhou,Changgeng Mo,Boxuan Cao,Linkai Li,Shan Xiang Wang*

Main category: eess.AS

TL;DR: This paper proposes SSIPNet, a deep learning model that predicts personalized speech intelligibility using existing intelligibility data of an individual, outperforming traditional audiogram-based methods.


<details>
  <summary>Details</summary>
Motivation: Personalized speech intelligibility prediction relying on audiograms is limited in accuracy as they only capture hearing thresholds for pure tones. The motivation is to improve prediction accuracy by leveraging an individual's existing intelligibility data instead of incorporating additional listener features.

Method: The method introduces SSIPNet (Support Sample-Based Intelligibility Prediction Network), a deep learning model that uses speech foundation models to create a high-dimensional representation of a listener's speech recognition ability from multiple support (audio, score) pairs, allowing accurate predictions for unseen audio.

Result: Results on the Clarity Prediction Challenge dataset demonstrate that even with a small number of support (audio, score) pairs, SSIPNet outperforms audiogram-based predictions.

Conclusion: The work presents a new paradigm for personalized speech intelligibility prediction by leveraging an individual's existing intelligibility data through SSIPNet.

Abstract: Personalized speech intelligibility prediction is challenging. Previous
approaches have mainly relied on audiograms, which are inherently limited in
accuracy as they only capture a listener's hearing threshold for pure tones.
Rather than incorporating additional listener features, we propose a novel
approach that leverages an individual's existing intelligibility data to
predict their performance on new audio. We introduce the Support Sample-Based
Intelligibility Prediction Network (SSIPNet), a deep learning model that
leverages speech foundation models to build a high-dimensional representation
of a listener's speech recognition ability from multiple support (audio, score)
pairs, enabling accurate predictions for unseen audio. Results on the Clarity
Prediction Challenge dataset show that, even with a small number of support
(audio, score) pairs, our method outperforms audiogram-based predictions. Our
work presents a new paradigm for personalized speech intelligibility
prediction.

</details>


### [206] [Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data](https://arxiv.org/abs/2506.02078)
*Emmy Postma,Cristian Tejedor-Garcia*

Main category: eess.AS

TL;DR: Speech data is crucial for Parkinson's Disease (PD) diagnostics. This study explores three pretrained audio embeddings on the NeuroVoz dataset, finding OpenL3 superior in DDK and LR tasks, while Wav2Vec2.0 has notable gender bias.


<details>
  <summary>Details</summary>
Motivation: To address the variability in effectiveness of deep acoustic features for PD classification due to individual speaker differences.

Method: Investigate three pre-trained audio embeddings (OpenL3, VGGish, and Wav2Vec2.0) using the NeuroVoz dataset for PD classification tasks.

Result: OpenL3 performs best in diadochokinesis (DDK) and listen and repeat (LR) tasks. Wav2Vec2.0 shows significant gender bias favoring male speakers in DDK tasks. Misclassified cases indicate challenges with atypical speech patterns.

Conclusion: Pre-trained audio embeddings show promise but require improved feature extraction and model robustness for better PD detection.

Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),
motivating the development of diagnostic techniques using speech data for
clinical applications. Although deep acoustic features have shown promise for
PD classification, their effectiveness often varies due to individual speaker
differences, a factor that has not been thoroughly explored in the existing
literature. This study investigates the effectiveness of three pre-trained
audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.
Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)
and listen and repeat (LR) tasks, capturing critical acoustic features for PD
detection. Only Wav2Vec2.0 shows significant gender bias, achieving more
favorable results for male speakers, in DDK tasks. The misclassified cases
reveal challenges with atypical speech patterns, highlighting the need for
improved feature extraction and model robustness in PD detection.

</details>


### [207] [Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge](https://arxiv.org/abs/2506.02080)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

TL;DR: An alignment-free, substitution-aware goodness of pronunciation (GOP) metric is introduced to efficiently assess pronunciation quality in computer-assisted language learning systems. This method restricts phoneme substitutions based on phoneme clusters and common learner errors. Evaluated on two datasets, it outperformed the baseline, providing insights for future research.


<details>
  <summary>Details</summary>
Motivation: Current automatic measures of pronunciation quality like the goodness of pronunciation (GOP) metric face challenges such as labeling and segmentation errors due to acoustic variability when relying on forced alignments. Although alignment-free methods exist, they are computationally expensive and do not scale well with phoneme sequence length and inventory size.

Method: The researchers developed a substitution-aware alignment-free GOP that limits phoneme substitutions according to phoneme clusters and common learner errors. They tested this approach using two setups: restricted phoneme substitutions (RPS) and unrestricted phoneme substitutions (UPS).

Result: The proposed alignment-free GOP with both RPS and UPS setups performed better than the baseline when evaluated on two L2 English speech datasets, including one with child speech and another encompassing both child and adult speech.

Conclusion: The new alignment-free, substitution-aware GOP offers an efficient alternative for assessing pronunciation quality in CAPT systems by restricting phoneme substitutions based on phoneme clusters and common learner errors.

Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic
measures of pronunciation quality, such as the goodness of pronunciation (GOP)
metric. GOP relies on forced alignments, which are prone to labeling and
segmentation errors due to acoustic variability. While alignment-free methods
address these challenges, they are computationally expensive and scale poorly
with phoneme sequence length and inventory size. To enhance efficiency, we
introduce a substitution-aware alignment-free GOP that restricts phoneme
substitutions based on phoneme clusters and common learner errors. We evaluated
our GOP on two L2 English speech datasets, one with child speech, My
Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult
speech. We compared RPS (restricted phoneme substitutions) and UPS
(unrestricted phoneme substitutions) setups within alignment-free methods,
which outperformed the baseline. We discuss our results and outline avenues for
future research.

</details>


### [208] [Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi](https://arxiv.org/abs/2506.02166)
*Arnav Rustagi,Satvik Bajpai,Nimrat Kaur,Siddharth Siddharth*

Main category: eess.AS

TL;DR: A novel CAPT system for Hindi, named Dhvani, is proposed to address the gap in pronunciation tools for Indian languages. It includes synthetic speech generation for mispronunciations and personalized feedback methodology.


<details>
  <summary>Details</summary>
Motivation: There is a critical lack of pronunciation tools tailored to Indian languages, despite millions learning them every year. Improving Hindi pronunciation can be a vital first step towards addressing this gap.

Method: 1) Propose Dhvani -- a novel CAPT system for Hindi, 2) Synthetic speech generation for Hindi mispronunciations, 3) A novel methodology for providing personalized feedback to learners.

Result: The system can analyze mispronounced speech and provide targeted feedback by interacting with learners using Devanagari graphemes while targeting phonemic distinctions.

Conclusion: Dhvani leverages Hindi's highly phonetic orthography to improve Hindi pronunciation through a CAPT system.

Abstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied
for English. However, there remains a critical gap in its application to Indian
languages with a base of 1.5 billion speakers. Pronunciation tools tailored to
Indian languages are strikingly lacking despite the fact that millions learn
them every year. With over 600 million speakers and being the fourth
most-spoken language worldwide, improving Hindi pronunciation is a vital first
step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT
system for Hindi, 2) synthetic speech generation for Hindi mispronunciations,
and 3) a novel methodology for providing personalized feedback to learners.
While the system often interacts with learners using Devanagari graphemes, its
core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic
orthography to analyze mispronounced speech and provide targeted feedback.

</details>


### [209] [Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions](https://arxiv.org/abs/2506.02742)
*Xiaoxue Gao,Huayun Zhang,Nancy F. Chen*

Main category: eess.AS

TL;DR: The paper proposes a novel prompt-unseen-emotion (PUE) approach for generating unseen emotional speech in TTS systems.


<details>
  <summary>Details</summary>
Motivation: Existing expressive text-to-speech (TTS) systems mainly model a limited set of categorical emotions, whereas human conversations extend far beyond these predefined emotions. This necessitates exploring more diverse emotional speech generation for natural interactions.

Method: The PUE approach is proposed, which uses emotion-guided prompt learning to generate unseen emotional speech. It is trained with an LLM-TTS architecture to ensure emotional consistency between prompts and emotional speech, allowing the model to capture different emotion weightings per utterance.

Result: During inference, mixed emotional speech can be generated by flexibly adjusting emotion proportions and leveraging LLM contextual knowledge, enabling the model to quantify different emotional styles.

Conclusion: The proposed PUE successfully facilitates expressive speech synthesis of unseen emotions in a zero-shot setting.

Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited
set of categorical emotions, whereas human conversations extend far beyond
these predefined emotions, making it essential to explore more diverse
emotional speech generation for more natural interactions. To bridge this gap,
this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate
unseen emotional speech via emotion-guided prompt learning. PUE is trained
utilizing an LLM-TTS architecture to ensure emotional consistency between
categorical emotion-relevant prompts and emotional speech, allowing the model
to quantitatively capture different emotion weightings per utterance. During
inference, mixed emotional speech can be generated by flexibly adjusting
emotion proportions and leveraging LLM contextual knowledge, enabling the model
to quantify different emotional styles. Our proposed PUE successfully
facilitates expressive speech synthesis of unseen emotions in a zero-shot
setting.

</details>


### [210] [CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech](https://arxiv.org/abs/2506.02863)
*Helin Wang,Jiarui Hai,Dading Chong,Karan Thakkar,Tiantian Feng,Dongchao Yang,Junhyeok Lee,Laureano Moro Velazquez,Jesus Villalba,Zengyi Qin,Shrikanth Narayanan,Mounya Elhiali,Najim Dehak*

Main category: eess.AS

TL;DR: Recent advancements in generative AI have transformed style-captioned text-to-speech synthesis (CapTTS). However, adapting CapTTS to real-world applications is challenging due to lack of datasets and research. To address this, we introduce CapSpeech, a new benchmark for CapTTS-related tasks including CapTTS-SE, AccCapTTS, EmoCapTTS, and AgentTTS. CapSpeech contains over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated pairs. We also introduce two new datasets for AgentTTS and CapTTS-SE tasks. Comprehensive experiments using autoregressive and non-autoregressive models on CapSpeech demonstrate high-fidelity and intelligible speech synthesis across diverse speaking styles.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the challenge of adapting CapTTS to real-world applications due to the lack of standardized, comprehensive datasets and limited research on downstream tasks built upon CapTTS.

Method: The authors introduce CapSpeech, a new benchmark for a series of CapTTS-related tasks, which includes over 10 million machine-annotated audio-caption pairs and nearly 0.36 million human-annotated pairs. They also introduce two new datasets specifically for AgentTTS and CapTTS-SE tasks. Comprehensive experiments are conducted using both autoregressive and non-autoregressive models on CapSpeech.

Result: The results demonstrate high-fidelity and highly intelligible speech synthesis across a diverse range of speaking styles. The findings provide valuable insights into the challenges of developing CapTTS systems.

Conclusion: CapSpeech is the largest available dataset offering comprehensive annotations for CapTTS-related tasks. The experiments and findings further provide valuable insights into the challenges of developing CapTTS systems.

Abstract: Recent advancements in generative artificial intelligence have significantly
transformed the field of style-captioned text-to-speech synthesis (CapTTS).
However, adapting CapTTS to real-world applications remains challenging due to
the lack of standardized, comprehensive datasets and limited research on
downstream tasks built upon CapTTS. To address these gaps, we introduce
CapSpeech, a new benchmark designed for a series of CapTTS-related tasks,
including style-captioned text-to-speech synthesis with sound events
(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS
(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech
comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36
million human-annotated audio-caption pairs. In addition, we introduce two new
datasets collected and recorded by a professional voice actor and experienced
audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside
the datasets, we conduct comprehensive experiments using both autoregressive
and non-autoregressive models on CapSpeech. Our results demonstrate
high-fidelity and highly intelligible speech synthesis across a diverse range
of speaking styles. To the best of our knowledge, CapSpeech is the largest
available dataset offering comprehensive annotations for CapTTS-related tasks.
The experiments and findings further provide valuable insights into the
challenges of developing CapTTS systems.

</details>


### [211] [Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency](https://arxiv.org/abs/2506.02908)
*Bunlong Lay,Rostilav Makarov,Timo Gerkmann*

Main category: eess.AS

TL;DR: This paper presents a sliding window diffusion framework for real-time speech enhancement, which outperforms standard diffusion models with lower latency.


<details>
  <summary>Details</summary>
Motivation: Diffusion models have shown remarkable success in speech enhancement but are computationally expensive and impractical for real-time streaming data processing.

Method: The authors adapt a sliding window diffusion framework to the speech enhancement task by progressively corrupting speech signals through time and assigning more noise to frames close to the present in a buffer.

Result: Empirical results show that the proposed method outperforms standard diffusion models and runs efficiently on a GPU with an input-output latency of 0.3 to 1 seconds.

Conclusion: This work represents the first practical diffusion-based solution for online speech enhancement.

Abstract: Diffusion models are a class of generative models that have been recently
used for speech enhancement with remarkable success but are computationally
expensive at inference time. Therefore, these models are impractical for
processing streaming data in real-time. In this work, we adapt a sliding window
diffusion framework to the speech enhancement task. Our approach progressively
corrupts speech signals through time, assigning more noise to frames close to
the present in a buffer. This approach outputs denoised frames with a delay
proportional to the chosen buffer size, enabling a trade-off between
performance and latency. Empirical results demonstrate that our method
outperforms standard diffusion models and runs efficiently on a GPU, achieving
an input-output latency in the order of 0.3 to 1 seconds. This marks the first
practical diffusion-based solution for online speech enhancement.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [212] [AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration](https://arxiv.org/abs/2506.02785)
*Charalampos Kalalas,Pavol Mulinka,Guillermo Candela Belmonte,Miguel Fornell,Michail Dalgitsis,Francisco Paredes Vera,Javier Santaella Sánchez,Carmen Vicente Villares,Roshan Sedar,Eftychia Datsika,Angelos Antonopoulos,Antonio Fernández Ojea,Miquel Payaro*

Main category: cs.NI

TL;DR: The paper presents a new vehicle condition monitoring service using AI and edge computing for real-time diagnostics, with a closed-loop orchestration framework for service migration. It was tested in a 5G race circuit environment.


<details>
  <summary>Details</summary>
Motivation: To enhance maintenance strategies, reduce costs, and improve safety by applying AI to vehicular equipment condition monitoring while addressing mobility challenges in edge environments.

Method: Introduction of a novel vehicle condition monitoring service that enables real-time diagnostics, combined with a closed-loop service orchestration framework for dynamic service migration based on network-related metrics.

Result: Experimental results from testing in a real-world race circuit environment with 5G network capabilities show the framework's effectiveness in ensuring low-latency AI inference and adaptive service placement.

Conclusion: The proposed framework has potential applications in intelligent transportation and mobility due to its ability to ensure low-latency AI inference and adaptively place services.

Abstract: Artificial intelligence (AI) has been increasingly applied to the condition
monitoring of vehicular equipment, aiming to enhance maintenance strategies,
reduce costs, and improve safety. Leveraging the edge computing paradigm,
AI-based condition monitoring systems process vast streams of vehicular data to
detect anomalies and optimize operational performance. In this work, we
introduce a novel vehicle condition monitoring service that enables real-time
diagnostics of a diverse set of anomalies while remaining practical for
deployment in real-world edge environments. To address mobility challenges, we
propose a closed-loop service orchestration framework where service migration
across edge nodes is dynamically triggered by network-related metrics. Our
approach has been implemented and tested in a real-world race circuit
environment equipped with 5G network capabilities under diverse operational
conditions. Experimental results demonstrate the effectiveness of our framework
in ensuring low-latency AI inference and adaptive service placement,
highlighting its potential for intelligent transportation and mobility
applications.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [213] [ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms](https://arxiv.org/abs/2506.02931)
*Praneet Sai Madhu Surabhi,Dheeraj Reddy Mudireddy,Jian Tao*

Main category: cs.MA

TL;DR: ThinkTank is a scalable framework that transforms specialized AI agents into collaborative intelligence platforms for complex problem-solving, with advantages in cost, security, and scalability.


<details>
  <summary>Details</summary>
Motivation: To create a versatile platform that supports complex problem-solving across diverse domains by leveraging proven scientific collaboration methodologies.

Method: Systematically generalizes agent roles, meeting structures, and knowledge integration mechanisms through role abstraction, generalization of meeting types, and integration of Retrieval-Augmented Generation with advanced knowledge storage.

Result: Enables organizations to perform knowledge-intensive tasks with ensured data privacy and security through local deployment, providing significant advantages over cloud-based alternatives.

Conclusion: ThinkTank establishes itself as a universal platform for AI-driven collaborative problem-solving, offering benefits in cost-effectiveness, data security, scalability, and competitive positioning.

Abstract: This paper presents ThinkTank, a comprehensive and scalable framework
designed to transform specialized AI agent systems into versatile collaborative
intelligence platforms capable of supporting complex problem-solving across
diverse domains. ThinkTank systematically generalizes agent roles, meeting
structures, and knowledge integration mechanisms by adapting proven scientific
collaboration methodologies. Through role abstraction, generalization of
meeting types for iterative collaboration, and the integration of
Retrieval-Augmented Generation with advanced knowledge storage, the framework
facilitates expertise creation and robust knowledge sharing. ThinkTank enables
organizations to leverage collaborative AI for knowledge-intensive tasks while
ensuring data privacy and security through local deployment, utilizing
frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is
designed to deliver significant advantages in cost-effectiveness, data
security, scalability, and competitive positioning compared to cloud-based
alternatives, establishing it as a universal platform for AI-driven
collaborative problem-solving. The ThinkTank code is available at
https://github.com/taugroup/ThinkTank

</details>


### [214] [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
*Sinem Erisken,Timothy Gothard,Martin Leitgab,Ram Potham*

Main category: cs.MA

TL;DR: 传统针对孤立LLM的AI安全性评估不足以应对多智能体AI集合所带来的新兴风险。本文引入了多智能体新兴行为评估（MAEBE）框架，以系统地评估这些风险。通过使用MAEBE框架和最大善行基准（以及一种新的双重反转问题技术），研究表明：(1) LLM的道德偏好，特别是工具性伤害方面，非常脆弱且会因问题表述方式而显著变化，无论是单个智能体还是集合体都如此。(2) LLM集合体的道德推理不能直接从孤立智能体的行为预测出来，因为存在新兴的群体动态。(3) 特别是，集合体会表现出类似同侪压力影响收敛的现象，即使在有监督者引导的情况下也是如此，突显出独特的安全与对齐挑战。研究结果强调了在互动、多智能体环境下评估AI系统的必要性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI集合变得普遍，传统的针对孤立LLM的AI安全性评估已经不足以应对由此产生的新型涌现风险。

Method: 引入了多智能体新兴行为评估（MAEBE）框架，并结合最大善行基准和一种新的双重反转问题技术来系统地评估多智能体AI集合中的风险。

Result: (1) 发现LLM的道德偏好非常脆弱，特别是在工具性伤害方面，会因问题表述方式而显著变化。(2) LLM集合体的道德推理不能直接从孤立智能体的行为预测出来，由于存在新兴的群体动态。(3) 集合体会表现出类似同侪压力影响收敛的现象，即使在有监督者引导的情况下也存在。

Conclusion: 在互动、多智能体环境下评估AI系统的必要性被强调，以应对多智能体AI集合带来的独特安全与对齐挑战。

Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [215] [Random-key genetic algorithms](https://arxiv.org/abs/2506.02120)
*Mariana A. Londe,Luciana S. Pessoa,Carlos E. Andrade,José F. Gonçalves,Mauricio G. C. Resende*

Main category: cs.NE

TL;DR: A random-key genetic algorithm is an evolutionary metaheuristic for discrete and global optimization, which encodes each solution as a vector of N random keys and maps it to the optimization problem. This chapter reviews random-key genetic algorithms and introduces a variant called biased random-key genetic algorithms.


<details>
  <summary>Details</summary>
Motivation: To provide an efficient and flexible metaheuristic for solving discrete and global optimization problems.

Method: Encode each solution as a vector of N random keys, partition vectors into elite and non-elite solutions, copy elite elements to the next population, add mutants and generate remaining elements by combining pairs of solutions with parametrized uniform crossover.

Result: An effective variant called biased random-key genetic algorithms is described, enhancing productivity and maintainability of the core framework.

Conclusion: Random-key genetic algorithms are powerful tools for discrete and global optimization, with the potential to be further improved by variants such as biased random-key genetic algorithms.

Abstract: A random-key genetic algorithm is an evolutionary metaheuristic for discrete
and global optimization. Each solution is encoded as a vector of N random keys,
where a random key is a real number randomly generated in the continuous
interval [0, 1). A decoder maps each vector of random keys to a solution of the
optimization problem being solved and computes its cost. The benefit of this
approach is that all genetic operators and transformations can be maintained
within the unitary hypercube, regardless of the problem being addressed. This
enhances the productivity and maintainability of the core framework. The
algorithm starts with a population of P vectors of random keys. At each
iteration, the vectors are partitioned into two sets: a smaller set of
high-valued elite solutions and the remaining non-elite solutions. All elite
elements are copied, without change, to the next population. A small number of
random-key vectors (the mutants) is added to the population of the next
iteration. The remaining elements of the population of the next iteration are
generated by combining, with the parametrized uniform crossover of Spears and
DeJong (1991), pairs of solutions. This chapter reviews random-key genetic
algorithms and describes an effective variant called biased random-key genetic
algorithms.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [216] [Stochastically Dominant Peer Prediction](https://arxiv.org/abs/2506.02259)
*Yichi Zhang,Shengwei Xu,David Pennock,Grant Schoenebeck*

Main category: cs.GT

TL;DR: The paper proposes stochastically dominant truthfulness (SD-truthfulness) as a stronger guarantee for peer prediction mechanisms and introduces a new enforced agreement (EA) mechanism that is theoretically guaranteed to be SD-truthful in binary-signal settings under mild assumptions.


<details>
  <summary>Details</summary>
Motivation: Eliciting reliable human feedback is essential for many machine learning tasks. Traditional peer prediction mechanisms can elicit honest information while assuming agents' utilities are linear functions of their scores. However, non-linear payment rules or inherently non-linear agents' utilities exist in practice.

Method: The authors propose stochastically dominant truthfulness (SD-truthfulness) which ensures the score distribution of truth-telling stochastically dominates all other strategies. They show that no existing peer prediction mechanism naturally satisfies this criterion without strong assumptions and introduce a new enforced agreement (EA) mechanism that is theoretically guaranteed to be SD-truthful in binary-signal settings under mild assumptions.

Result: A more careful application of rounding can better preserve sensitivity. The EA mechanism achieves the highest sensitivity among all known SD-truthful mechanisms empirically.

Conclusion: SD-truthfulness provides a stronger incentive for truthful reporting for a wide range of monotone utility functions. The EA mechanism offers a practical solution with high sensitivity.

Abstract: Eliciting reliable human feedback is essential for many machine learning
tasks, such as learning from noisy labels and aligning AI systems with human
preferences. Peer prediction mechanisms incentivize truthful reporting without
ground truth verification by scoring agents based on correlations with peers.
Traditional mechanisms, which ensure that truth-telling maximizes the expected
scores in equilibrium, can elicit honest information while assuming agents'
utilities are linear functions of their scores. However, in practice,
non-linear payment rules are usually preferred, or agents' utilities are
inherently non-linear.
  We propose stochastically dominant truthfulness (SD-truthfulness) as a
stronger guarantee: the score distribution of truth-telling stochastically
dominates all other strategies, incentivizing truthful reporting for a wide
range of monotone utility functions. Our first observation is that no existing
peer prediction mechanism naturally satisfies this criterion without strong
assumptions. A simple solution -- rounding scores into binary lotteries -- can
enforce SD-truthfulness, but often degrades sensitivity, a key property related
to fairness and statistical efficiency. We demonstrate how a more careful
application of rounding can better preserve sensitivity. Furthermore, we
introduce a new enforced agreement (EA) mechanism that is theoretically
guaranteed to be SD-truthful in binary-signal settings under mild assumptions,
and empirically achieves the highest sensitivity among all known SD-truthful
mechanisms.

</details>


### [217] [Learning Optimal Posted Prices for a Unit-Demand Buyer](https://arxiv.org/abs/2506.02284)
*Yifeng Teng,Yifan Wang*

Main category: cs.GT

TL;DR: The paper explores the problem of learning optimal item pricing for a unit-demand buyer with independent item values, providing nearly tight sample complexity and pricing query complexity under two common query models.


<details>
  <summary>Details</summary>
Motivation: To understand and optimize the pricing strategy for a unit-demand buyer whose item values are independent, while having access to the buyer's value distributions through queries.

Method: Investigate two query models - sample access model and pricing query model - to analyze their complexities in terms of samples and pricing queries.

Result: Achieved nearly tight bounds on the sample complexity and pricing query complexity for the unit-demand pricing problem.

Conclusion: This study provides a comprehensive understanding of the complexities involved in learning optimal pricing strategies for unit-demand buyers.

Abstract: We study the problem of learning the optimal item pricing for a unit-demand
buyer with independent item values, and the learner has query access to the
buyer's value distributions. We consider two common query models in the
literature: the sample access model where the learner can obtain a sample of
each item value, and the pricing query model where the learner can set a price
for an item and obtain a binary signal on whether the sampled value of the item
is greater than our proposed price. In this work, we give nearly tight sample
complexity and pricing query complexity of the unit-demand pricing problem.

</details>


### [218] [Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff](https://arxiv.org/abs/2506.03102)
*Sophie Greenwood,Karen Levy,Solon Barocas,Hoda Heidari,Jon Kleinberg*

Main category: cs.GT

TL;DR: 随着AI技术的进步，人们越来越愿意将任务委托给AI代理。本文探讨了在存在类别的情况下设计最优算法代理的问题，并揭示了其复杂性及与决策任务属性的关系。虽然寻找最优代理通常计算困难，但在特定情况下可找到有效算法，且实验表明逐步优化的代理表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前人类决策者在面对具体决策问题时，会根据可观测特征对问题实例进行分类，并决定是否将任务委托给AI代理。然而，人类通常无法完全意识到所有相关因素，因此需要研究如何设计在存在类别情况下的最优算法代理，以更好地与人类合作。

Method: 1. 定义了在存在类别情况下的最优算法代理设计问题。
2. 分析了该问题的本质组合特性及其与决策任务属性的复杂关系。
3. 证明了寻找最优代理在一般情况下是计算困难的。
4. 提出了几种特殊情况下的有效算法，例如当最优行动可以分解为人类和算法观察到的特征函数时。
5. 进行计算实验，模拟设计师如何随时间更新算法代理以优化其实际采用效果。

Result: - 发现了最优代理设计问题的根本组合性质。
- 在简单设置下展示了最优设计与任务属性之间的复杂关系。
- 找到了几种特殊情况下的有效算法。
- 实验表明，尽管逐步优化过程通常无法恢复最优代理，但结果代理的表现往往很好。

Conclusion: 在存在类别的情况下设计最优算法代理是一个重要的研究方向，能够显著提升AI作为人类队友的表现。虽然该问题通常计算困难，但在特定条件下可找到有效解决方案，实验结果也支持了逐步优化方法的有效性。

Abstract: As AI technologies improve, people are increasingly willing to delegate tasks
to AI agents. In many cases, the human decision-maker chooses whether to
delegate to an AI agent based on properties of the specific instance of the
decision-making problem they are facing. Since humans typically lack full
awareness of all the factors relevant to this choice for a given
decision-making instance, they perform a kind of categorization by treating
indistinguishable instances -- those that have the same observable features --
as the same. In this paper, we define the problem of designing the optimal
algorithmic delegate in the presence of categories. This is an important
dimension in the design of algorithms to work with humans, since we show that
the optimal delegate can be an arbitrarily better teammate than the optimal
standalone algorithmic agent. The solution to this optimal delegation problem
is not obvious: we discover that this problem is fundamentally combinatorial,
and illustrate the complex relationship between the optimal design and the
properties of the decision-making task even in simple settings. Indeed, we show
that finding the optimal delegate is computationally hard in general. However,
we are able to find efficient algorithms for producing the optimal delegate in
several broad cases of the problem, including when the optimal action may be
decomposed into functions of features observed by the human and the algorithm.
Finally, we run computational experiments to simulate a designer updating an
algorithmic delegate over time to be optimized for when it is actually adopted
by users, and show that while this process does not recover the optimal
delegate in general, the resulting delegate often performs quite well.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [219] [Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models](https://arxiv.org/abs/2506.02075)
*Christian Marius Lillelund,Shi-ang Qi,Russell Greiner,Christian Fischer Pedersen*

Main category: stat.ME

TL;DR: The paper argues that many survival analysis models are incorrectly evaluated, mainly due to over-reliance on the C-index which only measures discriminative ability. It presents key desiderata for choosing evaluation metrics tailored to the challenges in survival analysis and discusses appropriate methods for evaluating survival models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of incorrect evaluation of survival analysis and time-to-event models, particularly the over-reliance on concordance (C-index) which does not assess other important aspects such as accuracy of time-to-event predictions or calibration of probabilistic estimates.

Method: The method involves surveying examples of evaluation approaches in literature, presenting a set of key desiderata for choosing the right evaluation metric for survival analysis, hypothesizing the development of survival metrics conforms to a double-helix ladder, and discussing appropriate methods for evaluating survival models.

Result: The result is an improved understanding of the limitations of current evaluation metrics for survival models and guidance on selecting more appropriate evaluation metrics that consider aspects beyond just discriminative ability.

Conclusion: The conclusion is that model validity and metric validity must be aligned on the same rung of the assumption ladder, and there are various viewpoints opposing the analysis presented.

Abstract: We argue that many survival analysis and time-to-event models are incorrectly
evaluated. First, we survey many examples of evaluation approaches in the
literature and find that most rely on concordance (C-index). However, the
C-index only measures a model's discriminative ability and does not assess
other important aspects, such as the accuracy of the time-to-event predictions
or the calibration of the model's probabilistic estimates. Next, we present a
set of key desiderata for choosing the right evaluation metric and discuss
their pros and cons. These are tailored to the challenges in survival analysis,
such as sensitivity to miscalibration and various censoring assumptions. We
hypothesize that the current development of survival metrics conforms to a
double-helix ladder, and that model validity and metric validity must stand on
the same rung of the assumption ladder. Finally, we discuss the appropriate
methods for evaluating a survival model in practice and summarize various
viewpoints opposing our analysis.

</details>


### [220] [Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments](https://arxiv.org/abs/2506.02394)
*Yuan Bian,Xingche Guo,Yuanjia Wang*

Main category: stat.ME

TL;DR: The paper proposes a novel framework integrating reinforcement learning and drift-diffusion model with hidden Markov model to analyze decision-making in Major Depressive Disorder patients.


<details>
  <summary>Details</summary>
Motivation: Current understanding of Major Depressive Disorder associates it with reward-processing abnormalities and concentration issues. The authors aim to develop a more comprehensive model for analyzing reward-based decision-making and response times, motivated by findings from the EMBARC study.

Method: A novel framework is proposed that combines reinforcement learning (RL) and drift-diffusion model (DDM), incorporating latent state switching via hidden Markov model (HMM). This method uses an RL-DDM in the 'engaged' state and a simplified DDM in the 'lapsed' state. A generalized expectation-maximization algorithm is implemented for computational efficiency.

Result: The proposed method outperforms competing approaches in various scenarios with or without strategy switching. It reveals MDD patients have lower engagement and longer decision times compared to healthy controls. Neuroimaging shows brain-behavioral associations specific to the 'engaged' state.

Conclusion: The new framework provides insights into decision-making characteristics of MDD patients, showing differences in engagement levels and decision times as well as specific brain-behavioral links.

Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality,
is associated with reward-processing abnormalities and concentration issues.
Motivated by the probabilistic reward task from the Establishing Moderators and
Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we
propose a novel framework that integrates the reinforcement learning (RL) model
and drift-diffusion model (DDM) to jointly analyze reward-based decision-making
with response times. To account for emerging evidence suggesting that
decision-making may alternate between multiple interleaved strategies, we model
latent state switching using a hidden Markov model (HMM). In the ''engaged''
state, decisions follow an RL-DDM, simultaneously capturing reward processing,
decision dynamics, and temporal structure. In contrast, in the ''lapsed''
state, decision-making is modeled using a simplified DDM, where specific
parameters are fixed to approximate random guessing with equal probability. The
proposed method is implemented using a computationally efficient generalized
expectation-maximization algorithm with forward-backward procedures. Through
extensive numerical studies, we demonstrate that our proposed method
outperforms competing approaches under various reward-generating distributions,
both with and without strategy switching. When applied to the EMBARC study, our
framework reveals that MDD patients exhibit lower overall engagement than
healthy controls and experience longer decision times when they do engage.
Additionally, we show that neuroimaging measures of brain activities are
associated with decision-making characteristics in the ''engaged'' state but
not in the ''lapsed'' state, providing evidence of brain-behavioral association
specific to the ''engaged'' state.

</details>


### [221] [Simulation-Based Inference for Adaptive Experiments](https://arxiv.org/abs/2506.02881)
*Brian M Cho,Aurélien Bibaut,Nathan Kallus*

Main category: stat.ME

TL;DR: Multi-arm bandit experimental designs are favored over standard trials for improving participant outcomes, identifying top options faster, and enhancing parameter estimation precision. Current inference methods have limitations, so the authors propose a simulation-based approach with optimism to conduct hypothesis tests and construct confidence intervals.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current inference approaches after adaptive sampling which rely on asymptotic normality or weak martingale concentration inequalities.

Method: A simulation-based approach using positively biased nuisances to generate additional experiment trajectories (simulation with optimism) is proposed for conducting hypothesis tests and constructing confidence intervals for arm specific means and their differences.

Result: Theoretical guarantees include asymptotic type I error control, convergence of confidence intervals, and strong consistency of the estimator. Empirical results show that the approach achieves desired coverage while reducing confidence interval widths by up to 50%, especially for arms not targeted by the design.

Conclusion: The proposed simulation-based approach with optimism provides significant improvements in inference for multi-arm bandit experimental designs.

Abstract: Multi-arm bandit experimental designs are increasingly being adopted over
standard randomized trials due to their potential to improve outcomes for study
participants, enable faster identification of the best-performing options,
and/or enhance the precision of estimating key parameters. Current approaches
for inference after adaptive sampling either rely on asymptotic normality under
restricted experiment designs or underpowered martingale concentration
inequalities that lead to weak power in practice. To bypass these limitations,
we propose a simulation-based approach for conducting hypothesis tests and
constructing confidence intervals for arm specific means and their differences.
Our simulation-based approach uses positively biased nuisances to generate
additional trajectories of the experiment, which we call \textit{simulation
with optimism}. Using these simulations, we characterize the distribution
potentially non-normal sample mean test statistic to conduct inference. We
provide guarantees for (i) asymptotic type I error control, (ii) convergence of
our confidence intervals, and (iii) asymptotic strong consistency of our
estimator over a wide variety of common bandit designs. Our empirical results
show that our approach achieves the desired coverage while reducing confidence
interval widths by up to 50%, with drastic improvements for arms not targeted
by the design.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [222] [A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder](https://arxiv.org/abs/2506.02044)
*Xinxu Wei,Kanhao Zhao,Yong Jiao,Lifang He,Yu Zhang*

Main category: q-bio.NC

TL;DR: To address the limitations of current brain foundation models, BrainGFM introduces a novel graph-based pre-training paradigm leveraging graph contrastive learning and graph masked autoencoders for large-scale fMRI-based pre-training. It is pre-trained on a diverse set of brain atlases with varying parcellations to enhance generalization across heterogeneous fMRI-derived brain representations.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing interest in building large-scale brain foundation models to advance neuroscience and the limitation of existing models which are mostly pre-trained on time-series signals or ROI features.

Method: BrainGFM uses a unified framework that combines graph contrastive learning and graph masked autoencoders for pre-training. It is pre-trained on a diverse set of brain atlases with varying parcellations, incorporating both graph prompts and language prompts into its design. Meta-learning optimizes the graph prompts for better generalization under few-shot and zero-shot conditions.

Result: Pre-trained on 27 neuroimaging datasets covering 25 disorders, 2 types of brain atlases, and 8 parcellations, BrainGFM demonstrates strong adaptability and generalization capability across various atlases, disorders, and task settings.

Conclusion: BrainGFM represents a significant advancement in the field of neuroscience by providing a versatile and adaptable model capable of handling diverse neurological and psychiatric disorders.

Abstract: As large language models (LLMs) continue to revolutionize AI research, there
is a growing interest in building large-scale brain foundation models to
advance neuroscience. While most existing brain foundation models are
pre-trained on time-series signals or region-of-interest (ROI) features, we
propose a novel graph-based pre-training paradigm for constructing a brain
graph foundation model. In this paper, we introduce the Brain Graph Foundation
Model, termed BrainGFM, a unified framework that leverages graph contrastive
learning and graph masked autoencoders for large-scale fMRI-based pre-training.
BrainGFM is pre-trained on a diverse mixture of brain atlases with varying
parcellations, significantly expanding the pre-training corpus and enhancing
the model's ability to generalize across heterogeneous fMRI-derived brain
representations. To support efficient and versatile downstream transfer, we
integrate both graph prompts and language prompts into the model design,
enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological
and psychiatric disorders, and task settings. Furthermore, we employ
meta-learning to optimize the graph prompts, facilitating strong generalization
to previously unseen disorders under both few-shot and zero-shot learning
conditions via language-guided prompting. BrainGFM is pre-trained on 27
neuroimaging datasets spanning 25 common neurological and psychiatric
disorders, encompassing 2 types of brain atlases (functional and anatomical)
across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000
fMRI scans, and a total of 400,000 graph samples aggregated across all atlases
and parcellations. The code is available at:
https://github.com/weixinxu666/BrainGFM

</details>


### [223] [Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning](https://arxiv.org/abs/2506.03088)
*Lloyd Pellatt,Fotios Drakopoulos,Shievanie Sabesan,Nicholas A. Lesica*

Main category: q-bio.NC

TL;DR: A novel variational-conditional model is proposed to encode hearing loss from neural activity recordings, predicting neural responses with high accuracy and simulating realistic activity in unseen animals. This approach facilitates the development of hearing loss compensation models.


<details>
  <summary>Details</summary>
Motivation: Existing models of auditory processing either cannot capture individual differences in hearing loss or require animal-specific datasets, making it challenging to generalize across different brains and conditions.

Method: The researchers developed a variational-conditional model that uses recordings of neural activity from the auditory midbrain of both healthy and noise-exposed animals. The model employs 6 free parameters per animal to parametrize hearing loss and uses Bayesian optimization to fit conditioning parameters for out-of-sample animals.

Result: The model successfully predicts 62% of the explainable variance in neural responses for normal hearing animals and 68% for hearing-impaired animals. It can simulate realistic neural activity in unseen animals with crossentropy loss within 2% of the optimum in 15-30 iterations. Including more animals in training data slightly enhances performance on unseen animals.

Conclusion: This variational-conditional model provides a foundation for developing personalized hearing loss compensation models that can restore normal neural coding in hearing-impaired brains, potentially optimized quickly for new users via human-in-the-loop methods.

Abstract: The mapping from sound to neural activity that underlies hearing is highly
non-linear. The first few stages of this mapping in the cochlea have been
modelled successfully, with biophysical models built by hand and, more
recently, with DNN models trained on datasets simulated by biophysical models.
Modelling the auditory brain has been a challenge because central auditory
processing is too complex for models to be built by hand, and datasets for
training DNN models directly have not been available. Recent work has taken
advantage of large-scale high resolution neural recordings from the auditory
midbrain to build a DNN model of normal hearing with great success. But this
model assumes that auditory processing is the same in all brains, and therefore
it cannot capture the widely varying effects of hearing loss.
  We propose a novel variational-conditional model to learn to encode the space
of hearing loss directly from recordings of neural activity in the auditory
midbrain of healthy and noise exposed animals. With hearing loss parametrised
by only 6 free parameters per animal, our model accurately predicts 62\% of the
explainable variance in neural responses from normal hearing animals and 68%
for hearing impaired animals, within a few percentage points of state of the
art animal specific models. We demonstrate that the model can be used to
simulate realistic activity from out of sample animals by fitting only the
learned conditioning parameters with Bayesian optimisation, achieving
crossentropy loss within 2% of the optimum in 15-30 iterations. Including more
animals in the training data slightly improved the performance on unseen
animals. This model will enable future development of parametrised hearing loss
compensation models trained to directly restore normal neural coding in hearing
impaired brains, which can be quickly fitted for a new user by human in the
loop optimisation.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [224] [Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers](https://arxiv.org/abs/2506.01994)
*Wanshan Cui,Yejin Jeong,Inwook Song,Gyuri Kim,Minsang Kwon,Donghun Lee*

Main category: cond-mat.soft

TL;DR: 通过结合多算法异常值检测与选择性重新实验不可靠异常值案例，提高数据集质量，从而减少预测误差并提升机器学习模型在聚合物科学中的预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 在材料属性预测中，数据驱动方法可以加速新材料开发，但数据中的异常值会导致机器学习结果偏差，因此需要一种有效的方法来提高数据集质量。

Method: 提出了一种新方法，将多算法异常值检测与对不可靠异常值案例的选择性重新实验相结合，以系统性地构建包含701个测量值的新数据集，涵盖三个关键机械属性：玻璃化转变温度、tan δ峰值和交联密度。

Result: 该方法显著降低了预测误差（RMSE），提升了包括Elastic Net、SVR、Random Forest和TPOT在内的多个机器学习模型的预测准确性，仅需重新测量约5%的数据集。

Conclusion: 数据质量增强对于实现可靠的机器学习应用至关重要，本研究提供了一种可扩展的策略，以改善材料科学中的预测可靠性。

Abstract: Accurate prediction of polymer material properties through data-driven
approaches greatly accelerates novel material development by reducing redundant
experiments and trial-and-error processes. However, inevitable outliers in
empirical measurements can severely skew machine learning results, leading to
erroneous prediction models and suboptimal material designs. To address this
limitation, we propose a novel approach to enhance dataset quality efficiently
by integrating multi-algorithm outlier detection with selective
re-experimentation of unreliable outlier cases. To validate the empirical
effectiveness of the approach, we systematically construct a new dataset
containing 701 measurements of three key mechanical properties: glass
transition temperature ($T_g$), tan $\delta$ peak, and crosslinking density
($v_{c}$). To demonstrate its general applicability, we report the performance
improvements across multiple machine learning models, including Elastic Net,
SVR, Random Forest, and TPOT, to predict the three key properties. Our method
reliably reduces prediction error (RMSE) and significantly improves accuracy
with minimal additional experimental work, requiring only about 5% of the
dataset to be re-measured.These findings highlight the importance of data
quality enhancement in achieving reliable machine learning applications in
polymer science and present a scalable strategy for improving predictive
reliability in materials science.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [225] [Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps](https://arxiv.org/abs/2506.02254)
*Dimitris G Giovanis,Nikolaos Evangelou,Ioannis G Kevrekidis,Roger G Ghanem*

Main category: stat.ML

TL;DR: A generative learning framework extending PLoM is presented to generate statistically consistent realizations of a random vector. It overcomes the limitation of the original PLoM by implementing a synthesis of Double Diffusion Maps with Geometric Harmonics, enabling solving a full-order ISDE directly in the latent space.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve upon the Probabilistic Learning on Manifolds (PLoM) approach for generating statistically consistent realizations of a random vector when there are limited observations. Specifically, it aims to address the challenge of overfitting and loss of generalization when the number of data points is small compared to the dimensionality of the diffusion-map basis.

Method: The method extends the PLoM approach by combining Double Diffusion Maps and Geometric Harmonics (GH). This synthesis captures multiscale geometric features of the data and allows smooth nonlinear interpolation in high-dimensional spaces. It solves a full-order Ito Stochastic Differential Equation (ISDE) directly in the latent space while preserving the dynamical complexity of the system.

Result: The effectiveness and robustness of the proposed method are demonstrated through two numerical studies: one involving data from two-dimensional Hermite polynomial functions and another based on high-fidelity simulations of a detonation wave in a reactive flow.

Conclusion: This extension to the PLoM approach successfully addresses the issue of overfitting and loss of generalization when dealing with small datasets. By leveraging reduced geometric representation, it can preserve the full dynamical complexity of the system.

Abstract: We present a generative learning framework for probabilistic sampling based
on an extension of the Probabilistic Learning on Manifolds (PLoM) approach,
which is designed to generate statistically consistent realizations of a random
vector in a finite-dimensional Euclidean space, informed by a limited (yet
representative) set of observations. In its original form, PLoM constructs a
reduced-order probabilistic model by combining three main components: (a)
kernel density estimation to approximate the underlying probability measure,
(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,
and (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample
from the learned distribution. A key challenge arises, however, when the number
of available data points N is small and the dimensionality of the diffusion-map
basis approaches N, resulting in overfitting and loss of generalization. To
overcome this limitation, we propose an enabling extension that implements a
synthesis of Double Diffusion Maps -- a technique capable of capturing
multiscale geometric features of the data -- with Geometric Harmonics (GH), a
nonparametric reconstruction method that allows smooth nonlinear interpolation
in high-dimensional ambient spaces. This approach enables us to solve a
full-order ISDE directly in the latent space, preserving the full dynamical
complexity of the system, while leveraging its reduced geometric
representation. The effectiveness and robustness of the proposed method are
illustrated through two numerical studies: one based on data generated from
two-dimensional Hermite polynomial functions and another based on high-fidelity
simulations of a detonation wave in a reactive flow.

</details>


### [226] [Assumption-free stability for ranking problems](https://arxiv.org/abs/2506.02257)
*Ruiting Liang,Jake A. Soloff,Rina Foygel Barber,Rebecca Willett*

Main category: stat.ML

TL;DR: This paper proposes a new algorithmic stability framework for ranking problems and introduces two novel ranking operators, the inflated top-k and inflated full ranking, which provide guaranteed stability without assumptions on data distributions or dependence on the number of candidates.


<details>
  <summary>Details</summary>
Motivation: Ranking problems often face instability issues due to noisy data, where small fluctuations can alter rankings. Existing theoretical results rely on separation conditions that are not always applicable in real-world scenarios.

Method: The authors developed a new algorithmic stability framework for ranking problems. They proposed two novel ranking operators: the inflated top-k for selecting top items and the inflated full ranking for ordering the entire list. These methods incorporate uncertainty in their outputs to ensure stability.

Result: The proposed methods guarantee stability without making assumptions about data distributions or depending on the total number of candidates. Experiments on real-world data demonstrated that these methods maintain stability while preserving the informativeness of the output.

Conclusion: The inflated top-k and inflated full ranking operators provide a stable solution for ranking problems under noisy data conditions, with no need for separation conditions or distributional assumptions.

Abstract: In this work, we consider ranking problems among a finite set of candidates:
for instance, selecting the top-$k$ items among a larger list of candidates or
obtaining the full ranking of all items in the set. These problems are often
unstable, in the sense that estimating a ranking from noisy data can exhibit
high sensitivity to small perturbations. Concretely, if we use data to provide
a score for each item (say, by aggregating preference data over a sample of
users), then for two items with similar scores, small fluctuations in the data
can alter the relative ranking of those items. Many existing theoretical
results for ranking problems assume a separation condition to avoid this
challenge, but real-world data often contains items whose scores are
approximately tied, limiting the applicability of existing theory. To address
this gap, we develop a new algorithmic stability framework for ranking
problems, and propose two novel ranking operators for achieving stable ranking:
the \emph{inflated top-$k$} for the top-$k$ selection problem and the
\emph{inflated full ranking} for ranking the full list. To enable stability,
each method allows for expressing some uncertainty in the output. For both of
these two problems, our proposed methods provide guaranteed stability, with no
assumptions on data distributions and no dependence on the total number of
candidates to be ranked. Experiments on real-world data confirm that the
proposed methods offer stability without compromising the informativeness of
the output.

</details>


### [227] [MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements](https://arxiv.org/abs/2506.02260)
*Howon Ryu,Yuliang Chen,Yacun Wang,Andrea Z. LaCroix,Chongzhi Di,Loki Natarajan,Yu Wang,Jingjing Zou*

Main category: stat.ML

TL;DR: The paper introduces Multi-modal Cross-masked Autoencoder (MoCA), a self-supervised learning framework using cross-modality masking and Transformer architecture to efficiently analyze complex multi-modal health data without extensive labeled datasets. Experiments show MoCA outperforms existing methods in reconstruction and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for analyzing multi-modal health data rely heavily on supervised learning, which requires expensive and impractical large labeled datasets, especially in clinical studies.

Method: Propose MoCA, a self-supervised learning framework that uses cross-modality masking and Transformer autoencoder architecture to exploit temporal and cross-modal correlations in multi-modal data streams. Theoretical guarantees are provided for the effectiveness of the cross-modality masking scheme.

Result: Comprehensive experiments and ablation studies indicate that MoCA surpasses existing approaches in both reconstruction and downstream tasks.

Conclusion: This study underlines the significant potential of self-supervised learning in advancing digital health research involving multi-modal data.

Abstract: The growing prevalence of digital health technologies has led to the
generation of complex multi-modal data, such as physical activity measurements
simultaneously collected from various sensors of mobile and wearable devices.
These data hold immense potential for advancing health studies, but current
methods predominantly rely on supervised learning, requiring extensive labeled
datasets that are often expensive or impractical to obtain, especially in
clinical studies. To address this limitation, we propose a self-supervised
learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that
leverages cross-modality masking and the Transformer autoencoder architecture
to utilize both temporal correlations within modalities and cross-modal
correlations between data streams. We also provide theoretical guarantees to
support the effectiveness of the cross-modality masking scheme in MoCA.
Comprehensive experiments and ablation studies demonstrate that our method
outperforms existing approaches in both reconstruction and downstream tasks. We
release open-source code for data processing, pre-training, and downstream
tasks in the supplementary materials. This work highlights the transformative
potential of self-supervised learning in digital health and multi-modal data.

</details>


### [228] [Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression](https://arxiv.org/abs/2506.02336)
*Jingfeng Wu,Pierre Marion,Peter Bartlett*

Main category: stat.ML

TL;DR: The paper explores the use of large stepsize in gradient descent for $\ell_2$-regularized logistic regression with linearly separable data, showing it can accelerate convergence from $\widetilde{\mathcal{O}}(\kappa)$ to $\widetilde{\mathcal{O}}(\sqrt{\kappa})$, and characterizes the largest stepsize for local and global convergence.


<details>
  <summary>Details</summary>
Motivation: Classical theory suggests small stepsizes for gradient descent to ensure monotonic reduction of the optimization objective. However, this may not be optimal and there is potential for acceleration by using large stepsizes.

Method: The authors study gradient descent with a constant stepsize for $\ell_2$-regularized logistic regression with linearly separable data. They show that using a large stepsize leads to nonmonotonic evolution of the optimization objective but accelerates convergence. They also analyze the population risk minimization for separable distributions and characterize the largest stepsize for local and global convergence.

Result: The results indicate that using a large stepsize can accelerate convergence from $\widetilde{\mathcal{O}}(\kappa)$ to $\widetilde{\mathcal{O}}(\sqrt{\kappa})$. The findings also improve on the best-known upper bounds on the number of steps to reach a near-optimum for population risk minimization.

Conclusion: Large stepsizes in gradient descent can significantly accelerate convergence for $\ell_2$-regularized logistic regression with linearly separable data. This extends previous analysis from convex settings with minimizers at infinity to strongly convex cases with finite minimizers.

Abstract: We study gradient descent (GD) with a constant stepsize for
$\ell_2$-regularized logistic regression with linearly separable data.
Classical theory suggests small stepsizes to ensure monotonic reduction of the
optimization objective, achieving exponential convergence in
$\widetilde{\mathcal{O}}(\kappa)$ steps with $\kappa$ being the condition
number. Surprisingly, we show that this can be accelerated to
$\widetilde{\mathcal{O}}(\sqrt{\kappa})$ by simply using a large stepsize --
for which the objective evolves nonmonotonically. The acceleration brought by
large stepsizes extends to minimizing the population risk for separable
distributions, improving on the best-known upper bounds on the number of steps
to reach a near-optimum. Finally, we characterize the largest stepsize for the
local convergence of GD, which also determines the global convergence in
special scenarios. Our results extend the analysis of Wu et al. (2024) from
convex settings with minimizers at infinity to strongly convex cases with
finite minimizers.

</details>


### [229] [Tensor State Space-based Dynamic Multilayer Network Modeling](https://arxiv.org/abs/2506.02413)
*Tian Lan,Jie Guo,Chen Zhang*

Main category: stat.ML

TL;DR: This paper presents a novel model TSSDMN for understanding dynamic multilayer networks via symmetric Tucker decomposition and variational inference, capturing temporal and cross-layer dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing models are unable to fully capture the complexities of dynamic multilayer networks, specifically their temporal and cross-layer dynamics.

Method: The method involves introducing TSSDMN which uses symmetric Tucker decomposition to represent latent node features, interaction patterns, and layer transitions. It captures temporal dynamics by allowing interaction patterns to evolve over time while keeping latent features fixed.

Result: Numerical simulations and case studies show the efficacy of TSSDMN in understanding dynamic multilayer networks.

Conclusion: TSSDMN is an effective model for analyzing complex interactions within dynamic multilayer networks.

Abstract: Understanding the complex interactions within dynamic multilayer networks is
critical for advancements in various scientific domains. Existing models often
fail to capture such networks' temporal and cross-layer dynamics. This paper
introduces a novel Tensor State Space Model for Dynamic Multilayer Networks
(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric
Tucker decomposition to represent latent node features, their interaction
patterns, and layer transitions. Then by fixing the latent features and
allowing the interaction patterns to evolve over time, TSSDMN uniquely captures
both the temporal dynamics within layers and across different layers. The model
identifiability conditions are discussed. By treating latent features as
variables whose posterior distributions are approximated using a mean-field
variational inference approach, a variational Expectation Maximization
algorithm is developed for efficient model inference. Numerical simulations and
case studies demonstrate the efficacy of TSSDMN for understanding dynamic
multilayer networks.

</details>


### [230] [Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks](https://arxiv.org/abs/2506.02651)
*Luca Arnaboldi,Bruno Loureiro,Ludovic Stephan,Florent Krzakala,Lenka Zdeborova*

Main category: stat.ML

TL;DR: 本文研究了序列单指标（SSI）模型中随机梯度下降（SGD）的动力学特性，揭示了训练过程中的两个阶段，并分析了序列长度和位置编码对收敛速度和学习轨迹的影响。


<details>
  <summary>Details</summary>
Motivation: 当前对于序列模型的研究中，缺乏对简化的一层注意力架构在高维SGD动力学方面的深入理解。因此，作者希望通过对序列单指标（SSI）模型的研究，填补这一空白并提供理论基础。

Method: 作者首先将经典单一指标模型推广到序列领域，定义了序列单指标（SSI）模型。接着，推导出一个封闭形式的总体损失表达式，该表达式基于能够捕捉语义和位置对齐的充分统计量对。最后，刻画了这些坐标所诱导的高维SGD动力学。

Result: 研究表明，SSI模型的训练过程可以分为两个不同的阶段：从无信息初始化中逃脱以及与目标子空间对齐。此外，序列长度和位置编码对收敛速度和学习轨迹有显著影响。

Conclusion: 本文通过严格的数学分析和可解释的结果，为理解数据中的序列结构如何有助于基于注意力模型的学习提供了理论依据。

Abstract: We study the dynamics of stochastic gradient descent (SGD) for a class of
sequence models termed Sequence Single-Index (SSI) models, where the target
depends on a single direction in input space applied to a sequence of tokens.
This setting generalizes classical single-index models to the sequential
domain, encompassing simplified one-layer attention architectures. We derive a
closed-form expression for the population loss in terms of a pair of sufficient
statistics capturing semantic and positional alignment, and characterize the
induced high-dimensional SGD dynamics for these coordinates. Our analysis
reveals two distinct training phases: escape from uninformative initialization
and alignment with the target subspace, and demonstrates how the sequence
length and positional encoding influence convergence speed and learning
trajectories. These results provide a rigorous and interpretable foundation for
understanding how sequential structure in data can be beneficial for learning
with attention-based models.

</details>


### [231] [Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model](https://arxiv.org/abs/2506.02664)
*Hugo Tabanelli,Pierre Mergny,Lenka Zdeborova,Florent Krzakala*

Main category: stat.ML

TL;DR: The paper explores recovering high-dimensional signals from noisy, correlated data using Bayesian methods and Sequential Curriculum Learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of recovering multiple high-dimensional signals with shared low-rank structure from noisy data in two modalities.

Method: Utilize Bayesian Approximate Message Passing for efficient recovery and Sequential Curriculum Learning strategy for optimal weak recovery thresholds.

Result: Bayesian method enables efficient recovery even at low signal-to-noise ratios, while joint optimization fails. Sequential Curriculum Learning resolves bottlenecks and achieves optimal thresholds.

Conclusion: Structural correlation and learning order are critical in multi-modal high-dimensional inference.

Abstract: We study the recovery of multiple high-dimensional signals from two noisy,
correlated modalities: a spiked matrix and a spiked tensor sharing a common
low-rank structure. This setting generalizes classical spiked matrix and tensor
models, unveiling intricate interactions between inference channels and
surprising algorithmic behaviors. Notably, while the spiked tensor model is
typically intractable at low signal-to-noise ratios, its correlation with the
matrix enables efficient recovery via Bayesian Approximate Message Passing,
inducing staircase-like phase transitions reminiscent of neural network
phenomena. In contrast, empirical risk minimization for joint learning fails:
the tensor component obstructs effective matrix recovery, and joint
optimization significantly degrades performance, highlighting the limitations
of naive multi-modal learning. We show that a simple Sequential Curriculum
Learning strategy-first recovering the matrix, then leveraging it to guide
tensor recovery-resolves this bottleneck and achieves optimal weak recovery
thresholds. This strategy, implementable with spectral methods, emphasizes the
critical role of structural correlation and learning order in multi-modal
high-dimensional inference.

</details>


### [232] [Symmetry-Aware GFlowNets](https://arxiv.org/abs/2506.02685)
*Hohyun Kim,Seunggeun Lee,Min-hwan Oh*

Main category: stat.ML

TL;DR: SA-GFN is proposed to eliminate systematic biases in GFlowNets by incorporating symmetry corrections through reward scaling, achieving unbiased sampling and enhanced diversity.


<details>
  <summary>Details</summary>
Motivation: Existing GFlowNets suffer from systematic biases due to inaccuracies in state transition probability computations, especially impacted by graph symmetries.

Method: Introduce Symmetry-Aware GFlowNets (SA-GFN) that incorporate symmetry corrections into the learning process via reward scaling, removing the need for explicit state transition computations.

Result: Empirical results indicate SA-GFN achieves unbiased sampling, increases diversity, and generates high-reward graphs matching the target distribution.

Conclusion: SA-GFN successfully addresses the issue of systematic biases in graph generation using GFlowNets.

Abstract: Generative Flow Networks (GFlowNets) offer a powerful framework for sampling
graphs in proportion to their rewards. However, existing approaches suffer from
systematic biases due to inaccuracies in state transition probability
computations. These biases, rooted in the inherent symmetries of graphs, impact
both atom-based and fragment-based generation schemes. To address this
challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that
incorporates symmetry corrections into the learning process through reward
scaling. By integrating bias correction directly into the reward structure,
SA-GFN eliminates the need for explicit state transition computations.
Empirical results show that SA-GFN enables unbiased sampling while enhancing
diversity and consistently generating high-reward graphs that closely match the
target distribution.

</details>


### [233] [Safely Learning Controlled Stochastic Dynamics](https://arxiv.org/abs/2506.02754)
*Luc Brogat-Motte,Alessandro Rudi,Riccardo Bonalli*

Main category: stat.ML

TL;DR: This paper presents a method for safely learning stochastic dynamics from discrete-time trajectory observations, ensuring safety during training and deployment by iteratively expanding an initial safe control set. Theoretical guarantees for safety and adaptive learning rates are provided, with experimental results showing practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning controlled stochastic dynamics while ensuring system trajectories remain within predefined safe regions during both training and deployment, which is crucial in safety-critical applications such as autonomous robotics, finance, and biomedicine.

Method: The method ensures safe exploration and efficient estimation of system dynamics by iteratively expanding an initial known safe control set using kernel-based confidence bounds. It requires mild smoothness assumptions and access to an initial safe control set, allowing broad applicability to complex real-world systems.

Result: Theoretical guarantees for safety are provided, along with adaptive learning rates that improve with increasing Sobolev regularity of the true dynamics. Experimental evaluations demonstrate the practical effectiveness of the method in terms of safety, estimation accuracy, and computational efficiency.

Conclusion: The introduced method successfully enables safe learning of stochastic dynamics with theoretical guarantees and practical effectiveness, making it applicable to various safety-critical domains.

Abstract: We address the problem of safely learning controlled stochastic dynamics from
discrete-time trajectory observations, ensuring system trajectories remain
within predefined safe regions during both training and deployment.
Safety-critical constraints of this kind are crucial in applications such as
autonomous robotics, finance, and biomedicine. We introduce a method that
ensures safe exploration and efficient estimation of system dynamics by
iteratively expanding an initial known safe control set using kernel-based
confidence bounds. After training, the learned model enables predictions of the
system's dynamics and permits safety verification of any given control. Our
approach requires only mild smoothness assumptions and access to an initial
safe control set, enabling broad applicability to complex real-world systems.
We provide theoretical guarantees for safety and derive adaptive learning rates
that improve with increasing Sobolev regularity of the true dynamics.
Experimental evaluations demonstrate the practical effectiveness of our method
in terms of safety, estimation accuracy, and computational efficiency.

</details>


### [234] [Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings](https://arxiv.org/abs/2506.02793)
*Houssam Zenati,Bariscan Bozkurt,Arthur Gretton*

Main category: stat.ML

TL;DR: This paper proposes a novel framework called Counterfactual Policy Mean Embedding (CPME) to represent counterfactual outcome distribution in RKHS for flexible and nonparametric off-policy evaluation. It also introduces plug-in and doubly robust estimators, and develops a test statistic for hypothesis testing.


<details>
  <summary>Details</summary>
Motivation: Estimating the distribution of outcomes under counterfactual policies is crucial in recommendation, advertising, and healthcare domains.

Method: The CPME framework represents the entire counterfactual outcome distribution in RKHS. Plug-in and doubly robust estimators are introduced where the latter corrects bias in both outcome embedding and propensity models. A doubly robust kernel test statistic for hypothesis testing is developed which achieves asymptotic normality.

Result: The framework supports sampling from the counterfactual distribution and numerical simulations show practical benefits of CPME over existing methods.

Conclusion: CPME provides a novel approach for distributional off-policy evaluation with practical advantages demonstrated through simulations.

Abstract: Estimating the distribution of outcomes under counterfactual policies is
critical for decision-making in domains such as recommendation, advertising,
and healthcare. We analyze a novel framework-Counterfactual Policy Mean
Embedding (CPME)-that represents the entire counterfactual outcome distribution
in a reproducing kernel Hilbert space (RKHS), enabling flexible and
nonparametric distributional off-policy evaluation. We introduce both a plug-in
estimator and a doubly robust estimator; the latter enjoys improved uniform
convergence rates by correcting for bias in both the outcome embedding and
propensity models. Building on this, we develop a doubly robust kernel test
statistic for hypothesis testing, which achieves asymptotic normality and thus
enables computationally efficient testing and straightforward construction of
confidence intervals. Our framework also supports sampling from the
counterfactual distribution. Numerical simulations illustrate the practical
benefits of CPME over existing methods.

</details>


### [235] [Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)](https://arxiv.org/abs/2506.02825)
*Tong Qi,Vera Andersson,Peter Viechnicki,Vince Lyzinski*

Main category: stat.ML

TL;DR: The paper introduces OmniMatch algorithm for seeded multiple graph matching, proving its efficiency in aligning unseeded vertices across networks even without edge correlation. It also demonstrates the algorithm's effectiveness in simulations, shuffled graph hypothesis testing, and real-world data examples.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient algorithm for seeded multiple graph matching that can perfectly align unseeded vertices across multiple networks even when there is no edge correlation.

Method: OmniMatch algorithm operates in the setting of d-dimensional Random Dot Product Graphs (RDPG) and uses s seeds to asymptotically and efficiently align O(s^α) unseeded vertices, where α<2∧d/4.

Result: OmniMatch effectively aligns unseeded vertices across multiple networks, corrects misaligned vertices in shuffled graph hypothesis testing, and recovers lost testing power. The algorithm's effectiveness is demonstrated through numerous simulations and real-world data examples from connectomics and machine translation.

Conclusion: OmniMatch is a powerful algorithm for seeded multiple graph matching that can handle the absence of edge correlation and has been proven effective in various settings including simulations and real-world applications.

Abstract: We present the OmniMatch algorithm for seeded multiple graph matching. In the
setting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that
under mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently
perfectly aligns $O(s^{\alpha})$ unseeded vertices -- for $\alpha<2\wedge d/4$
-- across multiple networks even in the presence of no edge correlation. We
demonstrate the effectiveness of our algorithm across numerous simulations and
in the context of shuffled graph hypothesis testing. In the shuffled testing
setting, testing power is lost due to the misalignment/shuffling of vertices
across graphs, and we demonstrate the capacity of OmniMatch to correct for
misaligned vertices prior to testing and hence recover the lost testing power.
We further demonstrate the algorithm on a pair of data examples from
connectomics and machine translation.

</details>


### [236] [Non-stationary Bandit Convex Optimization: A Comprehensive Study](https://arxiv.org/abs/2506.02980)
*Xiaoqi Liu,Dorian Baudry,Julian Zimmert,Patrick Rebeschini,Arya Akhavan*

Main category: stat.ML

TL;DR: Bandit Convex Optimization研究了非平稳环境下的连续动作选择问题，提出了TEWA-SE和cExO算法，分别针对强凸损失和一般凸损失，在已知和未知非平稳性度量下均达到最优的后悔界。


<details>
  <summary>Details</summary>
Motivation: 带约束优化问题在实际应用中普遍存在，但在非平稳环境下如何最小化不同非平稳性度量下的后悔是一个重要且具有挑战性的问题。

Method: 提出了TEWA-SE算法（适用于强凸损失）和cExO算法（适用于一般凸损失），通过引入睡眠专家框架和带宽机制，使算法能够适应已知或未知的非平稳性度量S、Δ和P。

Result: 对于强凸损失，TEWA-SE算法证明了其是最优的，并给出了匹配的上下界；对于一般凸损失，cExO算法达到了最优的后悔界并改进了现有方法。

Conclusion: 本研究为非平稳环境下的带约束优化问题提供了有效的解决方法，特别是TEWA-SE和cExO算法在不同条件下均表现出了优越的性能。

Abstract: Bandit Convex Optimization is a fundamental class of sequential
decision-making problems, where the learner selects actions from a continuous
domain and observes a loss (but not its gradient) at only one point per round.
We study this problem in non-stationary environments, and aim to minimize the
regret under three standard measures of non-stationarity: the number of
switches $S$ in the comparator sequence, the total variation $\Delta$ of the
loss functions, and the path-length $P$ of the comparator sequence. We propose
a polynomial-time algorithm, Tilted Exponentially Weighted Average with
Sleeping Experts (TEWA-SE), which adapts the sleeping experts framework from
online convex optimization to the bandit setting. For strongly convex losses,
we prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\Delta$
by establishing matching upper and lower bounds. By equipping TEWA-SE with the
Bandit-over-Bandit framework, we extend our analysis to environments with
unknown non-stationarity measures. For general convex losses, we introduce a
second algorithm, clipped Exploration by Optimization (cExO), based on
exponential weights over a discretized action space. While not polynomial-time
computable, this method achieves minimax-optimal regret with respect to known
$S$ and $\Delta$, and improves on the best existing bounds with respect to $P$.

</details>


### [237] [Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records](https://arxiv.org/abs/2506.03068)
*Yina Hou,Shourav B. Rabbani,Liang Hong,Norou Diawara,Manar D. Samad*

Main category: stat.ML

TL;DR: The paper explores causal relationships of clinical variables in heart failure using a new computational framework for causal structure discovery, showing nonlinear CSD is more meaningful and correlated with ML feature importance.


<details>
  <summary>Details</summary>
Motivation: To investigate the causal explainability of clinical variables obtained through statistical and machine learning methods, addressing the limitation that predictive importance may not represent causal relationships.

Method: Proposes a new computational framework enabling causal structure discovery and scoring causal strength for mixed-type clinical variables related to binary disease outcomes. Investigates association between importance rank orders of different feature types in heart failure classification.

Result: Nonlinear causal structure discovery modeling is more meaningful than linear approaches. Feature importance from nonlinear classifiers correlates strongly with causal strength. Correlated variables can be causal but are rarely identified as effects.

Conclusion: Causal structure discovery can enhance understanding of variable importance in ML-based prediction models for diseases like heart failure.

Abstract: The importance of clinical variables in the prognosis of the disease is
explained using statistical correlation or machine learning (ML). However, the
predictive importance of these variables may not represent their causal
relationships with diseases. This paper uses clinical variables from a heart
failure (HF) patient cohort to investigate the causal explainability of
important variables obtained in statistical and ML contexts. Due to inherent
regression modeling, popular causal discovery methods strictly assume that the
cause and effect variables are numerical and continuous. This paper proposes a
new computational framework to enable causal structure discovery (CSD) and
score the causal strength of mixed-type (categorical, numerical, binary)
clinical variables for binary disease outcomes. In HF classification, we
investigate the association between the importance rank order of three feature
types: correlated features, features important for ML predictions, and causal
features. Our results demonstrate that CSD modeling for nonlinear causal
relationships is more meaningful than its linear counterparts. Feature
importance obtained from nonlinear classifiers (e.g., gradient-boosting trees)
strongly correlates with the causal strength of variables without
differentiating cause and effect variables. Correlated variables can be causal
for HF, but they are rarely identified as effect variables. These results can
be used to add the causal explanation of variables important for ML-based
prediction modeling.

</details>


### [238] [GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression](https://arxiv.org/abs/2506.03074)
*Junghyun Lee,Kyoungseok Jang,Kwang-Sung Jun,Milan Vojnović,Se-Young Yun*

Main category: stat.ML

TL;DR: The paper introduces GL-LowPopArt, an advanced estimator for generalized low-rank trace regression that builds on LowPopArt. It uses a two-stage approach and provides state-of-the-art estimation error bounds.


<details>
  <summary>Details</summary>
Motivation: There is a need for improved estimators in generalized low-rank trace regression to surpass existing guarantees and address challenges such as bias from nonlinear inverse link functions.

Method: GL-LowPopArt employs a two-stage approach involving nuclear norm regularization followed by matrix Catoni estimation. A novel experimental design objective GL(π) is also revealed. The method controls bias through this two-stage process and proves a local minimax lower bound.

Result: GL-LowPopArt achieves state-of-the-art Frobenius error guarantee in generalized linear matrix completion and improves Borda regret bound in bilinear dueling bandits compared to vectorization methods.

Conclusion: GL-LowPopArt offers instance-wise optimality up to the condition number of the ground-truth Hessian and has promising applications in generalized linear matrix completion and bilinear dueling bandits.

Abstract: We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized
low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it
employs a two-stage approach: nuclear norm regularization followed by matrix
Catoni estimation. We establish state-of-the-art estimation error bounds,
surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and
reveal a novel experimental design objective, $\mathrm{GL}(\pi)$. The key
technical challenge is controlling bias from the nonlinear inverse link
function, which we address by our two-stage approach. We prove a *local*
minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise
optimality up to the condition number of the ground-truth Hessian. Applications
include generalized linear matrix completion, where `GL-LowPopArt` achieves a
state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a
novel setting inspired by general preference learning (Zhang et al., 2024). Our
analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,
potentially interesting problem-dependent quantity, along with improved Borda
regret bound than vectorization (Wu et al., 2024).

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [239] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/abs/2506.01980)
*Lianhao Yin,Ozanan Meireles,Guy Rosman,Daniela Rus*

Main category: eess.IV

TL;DR: 提出了一种名为Compress-to-Explore (C2E)的新颖自监督框架，该框架利用柯尔莫哥洛夫复杂性从手术视频中学习紧凑、信息丰富的表示，通过最大化熵的解码器压缩图像，同时保留临床相关细节。在大规模未标注手术数据集上训练，展示了强大的跨多种手术机器学习任务的泛化能力。模型内部紧凑的表示更好地分离了图像不同结构部分的特征，提高了性能，展示了自监督学习在增强手术AI方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 实时视频理解对于指导微创手术（MIS）过程至关重要。然而，监督学习方法需要大量的、带有注释的数据集，而在医学领域，这种数据集由于标注成本过高而稀缺。虽然自监督方法可以解决这些问题，但当前的方法往往无法以可跨任务推广的形式捕捉结构和物理信息。

Method: 提出了Compress-to-Explore (C2E)，一种基于柯尔莫哥洛夫复杂性的自监督框架，使用熵最大化解码器来压缩图像，同时保留临床相关的细节，从而无需标注数据即可提高编码器性能。

Result: 在大规模未标注手术数据集上训练后，C2E展示出了强大的跨多种手术机器学习任务的泛化能力，包括工作流程分类、工具-组织交互分类、分割和诊断任务。模型内部紧凑的表示更好地分离了图像不同结构部分的特征，显著提升了性能。

Conclusion: 自监督学习具有尚未开发的巨大潜力，可以增强手术AI并改善微创手术的结果。

Abstract: Real-time video understanding is critical to guide procedures in minimally
invasive surgery (MIS). However, supervised learning approaches require large,
annotated datasets that are scarce due to annotation efforts that are
prohibitive, e.g., in medical fields. Although self-supervision methods can
address such limitations, current self-supervised methods often fail to capture
structural and physical information in a form that generalizes across tasks. We
propose Compress-to-Explore (C2E), a novel self-supervised framework that
leverages Kolmogorov complexity to learn compact, informative representations
from surgical videos. C2E uses entropy-maximizing decoders to compress images
while preserving clinically relevant details, improving encoder performance
without labeled data. Trained on large-scale unlabeled surgical datasets, C2E
demonstrates strong generalization across a variety of surgical ML tasks, such
as workflow classification, tool-tissue interaction classification,
segmentation, and diagnosis tasks, providing improved performance as a surgical
visual foundation model. As we further show in the paper, the model's internal
compact representation better disentangles features from different structural
parts of images. The resulting performance improvements highlight the yet
untapped potential of self-supervised learning to enhance surgical AI and
improve outcomes in MIS.

</details>


### [240] [Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine](https://arxiv.org/abs/2506.02149)
*Wenjun Xia,Chuang Niu,Ge Wang*

Main category: eess.IV

TL;DR: Computed tomography (CT) often encounters issues like noise and artifacts in certain clinical scenarios. While deep learning has improved CT image reconstruction, obtaining paired training data is challenging and can lead to inaccuracies. This paper introduces FORCE, a new CT framework integrating data fidelity with advanced generative AI models, demonstrating superior performance in CT imaging tasks.


<details>
  <summary>Details</summary>
Motivation: Clinical CT scenarios often result in severe noise and artifacts in reconstructed images, necessitating improved reconstruction techniques. Deep learning methods, although effective, face challenges with paired training data and risk inaccuracies due to data inconsistencies and model instability.

Method: The paper integrates data fidelity with the state-of-the-art Poisson flow generative model (PFGM) and its generalized version PFGM++, proposing a novel CT framework called Flow-Oriented Reconstruction Conditioning Engine (FORCE).

Result: In experiments, the proposed method exhibits superior performance across various CT imaging tasks, surpassing existing unsupervised reconstruction approaches.

Conclusion: FORCE represents an advancement in CT image reconstruction, addressing limitations of current deep learning methods by effectively combining data fidelity with generative AI models.

Abstract: Computed tomography (CT) is a major medical imaging modality. Clinical CT
scenarios, such as low-dose screening, sparse-view scanning, and metal
implants, often lead to severe noise and artifacts in reconstructed images,
requiring improved reconstruction techniques. The introduction of deep learning
has significantly advanced CT image reconstruction. However, obtaining paired
training data remains rather challenging due to patient motion and other
constraints. Although deep learning methods can still perform well with
approximately paired data, they inherently carry the risk of hallucination due
to data inconsistencies and model instability. In this paper, we integrate the
data fidelity with the state-of-the-art generative AI model, referred to as the
Poisson flow generative model (PFGM) with a generalized version PFGM++, and
propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine
(FORCE). In our experiments, the proposed method shows superior performance in
various CT imaging tasks, outperforming existing unsupervised reconstruction
approaches.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [241] [Music interpretation and emotion perception: A computational and neurophysiological investigation](https://arxiv.org/abs/2506.01982)
*Vassilis Lyberatos,Spyridon Kantarelis,Ioanna Zioga,Christina Anagnostopoulou,Giorgos Stamou,Anastasia Georgaki*

Main category: cs.HC

TL;DR: This study explores emotional expression and perception in music performance through computational and neurophysiological methods, finding that expressivity enhances emotional communication and audience engagement.


<details>
  <summary>Details</summary>
Motivation: To understand how different performance settings and levels of expressiveness influence emotional communication between performers and listeners.

Method: Professional musicians performed various tasks (repertoire, diatonic modal etudes, improvisation), with emotional annotations provided by both performers and audience. Audio analysis and neurophysiological measurements were conducted.

Result: Expressive and improvisational performances had unique acoustic features and evoked stronger emotional responses; improvisation also indicated greater relaxation.

Conclusion: Expressivity is significant in enhancing emotional communication and audience engagement in music performance.

Abstract: This study investigates emotional expression and perception in music
performance using computational and neurophysiological methods. The influence
of different performance settings, such as repertoire, diatonic modal etudes,
and improvisation, as well as levels of expressiveness, on performers'
emotional communication and listeners' reactions is explored. Professional
musicians performed various tasks, and emotional annotations were provided by
both performers and the audience. Audio analysis revealed that expressive and
improvisational performances exhibited unique acoustic features, while emotion
analysis showed stronger emotional responses. Neurophysiological measurements
indicated greater relaxation in improvisational performances. This multimodal
study highlights the significance of expressivity in enhancing emotional
communication and audience engagement.

</details>


### [242] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
*Takao Fujii,Katie Seaborn,Madeleine Steeds,Jun Kato*

Main category: cs.HC

TL;DR: This paper explores the role of 'neutral' non-pronominal self-referents (NPSR) and voice as a socially expressive medium in Japanese ChatGPT voices, revealing strong evidence of voice gendering and the potential of intersectional self-referents to evade gendering.


<details>
  <summary>Details</summary>
Motivation: To investigate the role of other 'neutral' non-pronominal self-referents (NPSR) and voice as a socially expressive medium in shaping perceptions of agent identity, especially given the complexity and sometimes evasive impressions elicited by intersectional Japanese pronouns.

Method: A crowdsourcing study was conducted with Japanese participants (N = 204) evaluating three ChatGPT voices (Juniper, Breeze, and Ember) using seven self-referents.

Result: Strong evidence of voice gendering was found alongside the potential of intersectional self-referents to evade gendering. Perceptions of age and formality intersected with gendering according to sociolinguistic theories, particularly for boku and watakushi.

Conclusion: This work offers a nuanced perspective on agent identity perceptions and advocates for intersectional and culturally-sensitive research on voice agents.

Abstract: Conversational agents that mimic people have raised questions about the
ethics of anthropomorphizing machines with human social identity cues. Critics
have also questioned assumptions of identity neutrality in humanlike agents.
Recent work has revealed that intersectional Japanese pronouns can elicit
complex and sometimes evasive impressions of agent identity. Yet, the role of
other "neutral" non-pronominal self-referents (NPSR) and voice as a socially
expressive medium remains unexplored. In a crowdsourcing study, Japanese
participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and
Ember) using seven self-referents. We found strong evidence of voice gendering
alongside the potential of intersectional self-referents to evade gendering,
i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age
and formality intersected with gendering as per sociolinguistic theories,
especially boku and watakushi. This work provides a nuanced take on agent
identity perceptions and champions intersectional and culturally-sensitive work
on voice agents.

</details>


### [243] [Composable Building Blocks for Controllable and Transparent Interactive AI Systems](https://arxiv.org/abs/2506.02262)
*Sebe Vanbrabant,Gustavo Rovelo Ruiz,Davy Vanacken*

Main category: cs.HC

TL;DR: The paper proposes a method to represent interactive systems as sequences of structural building blocks, such as AI models and control mechanisms, that can be explained through XAI techniques to make the system architecture more transparent.


<details>
  <summary>Details</summary>
Motivation: To address the black box problem of AI models in interactive systems and make the overarching system architecture more interpretable.

Method: Represent interactive systems as sequences of structural building blocks (e.g., AI models, control mechanisms) grounded in literature, and explain them using accompanying visual building blocks like XAI techniques. The flow and APIs of these blocks form an explicit overview of the system.

Result: This approach creates a communication basis for both humans and automated agents, aligning human and machine interpretability of AI models.

Conclusion: The authors discuss selected building blocks and demonstrate their flow-based approach in an architecture and prototype interactive system.

Abstract: While the increased integration of AI technologies into interactive systems
enables them to solve an equally increasing number of tasks, the black box
problem of AI models continues to spread throughout the interactive system as a
whole. Explainable AI (XAI) techniques can make AI models more accessible by
employing post-hoc methods or transitioning to inherently interpretable models.
While this makes individual AI models clearer, the overarching system
architecture remains opaque. To this end, we propose an approach to represent
interactive systems as sequences of structural building blocks, such as AI
models and control mechanisms grounded in the literature. These can then be
explained through accompanying visual building blocks, such as XAI techniques.
The flow and APIs of the structural building blocks form an explicit overview
of the system. This serves as a communication basis for both humans and
automated agents like LLMs, aligning human and machine interpretability of AI
models. We discuss a selection of building blocks and concretize our flow-based
approach in an architecture and accompanying prototype interactive system.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [244] [SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction](https://arxiv.org/abs/2506.02082)
*Saurabh Agrawal,Raj Gohil,Gopal Kumar Agrawal,Vikram C M,Kushal Verma*

Main category: cs.SD

TL;DR: An abstract about speech quality assessment.


<details>
  <summary>Details</summary>
Motivation: Speech quality assessment is critical in selecting TTS or voice conversion models, but existing objective and subjective metrics have limitations.

Method: Developed SALF-MOS, a small-sized, end-to-end model that predicts MOS score by using sequences of convolutions to extract latent audio features.

Result: Achieved best state-of-the-art results based on MSE, LCC, SRCC and KTAU.

Conclusion: SALF-MOS is a highly generalized and scalable model for predicting MOS score.

Abstract: Speech quality assessment is a critical process in selecting text-to-speech
synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can
be done using objective metrics or subjective metrics. Although there are many
objective metrics like the Perceptual Evaluation of Speech Quality (PESQ),
Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time
Objective Intelligibility (STOI) but none of them is feasible in selecting the
best model. On the other hand subjective metric like Mean Opinion Score is
highly reliable but it requires a lot of manual efforts and are time-consuming.
To counter the issues in MOS Evaluation, we have developed a novel model,
Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a
small-sized, end-to-end, highly generalized and scalable model for predicting
MOS score on a scale of 5. We use the sequences of convolutions and stack them
to get the latent features of the audio samples to get the best
state-of-the-art results based on mean squared error (MSE), Linear Concordance
Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and
Kendall Rank Correlation Coefficient (KTAU).

</details>


### [245] [LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention](https://arxiv.org/abs/2506.02083)
*Aditya Srinivas Menon,Raj Prakash Gohil,Kumud Tripathi,Pankaj Wasnik*

Main category: cs.SD

TL;DR: A new disentanglement learning strategy with prefix-tuned cross-attention is proposed to separate linguistic and speaker information in embeddings, improving speaker recognition accuracy across languages.


<details>
  <summary>Details</summary>
Motivation: Speaker recognition models struggle with multi-lingual settings due to the entanglement of linguistic information within speaker embeddings. There's a need for better separation of these components to improve accuracy.

Method: The paper proposes a novel disentanglement learning strategy that uses prefix-tuned cross-attention for joint learning, aiming to separate language-specific information from speaker characteristics, especially useful when speakers switch between languages.

Result: The model successfully generalizes across monolingual and multi-lingual settings, including unseen languages, and improves the equal error rate on multiple datasets.

Conclusion: The proposed disentanglement strategy effectively separates linguistic information from speaker embeddings, leading to enhanced speaker recognition accuracy in diverse linguistic conditions.

Abstract: Speaker recognition models face challenges in multi-lingual settings due to
the entanglement of linguistic information within speaker embeddings. The
overlap between vocal traits such as accent, vocal anatomy, and a language's
phonetic structure complicates separating linguistic and speaker information.
Disentangling these components can significantly improve speaker recognition
accuracy. To this end, we propose a novel disentanglement learning strategy
that integrates joint learning through prefix-tuned cross-attention. This
approach is particularly effective when speakers switch between languages.
Experimental results show the model generalizes across monolingual and
multi-lingual settings, including unseen languages. Notably, the proposed model
improves the equal error rate across multiple datasets, highlighting its
ability to separate language information from speaker embeddings and enhance
recognition in diverse linguistic conditions.

</details>


### [246] [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/abs/2506.02085)
*Ajinkya Kulkarni,Sandipana Dowerah,Tanel Alumae,Mathew Magimai. -Doss*

Main category: cs.SD

TL;DR: 音频深度伪造在先进的AI技术支持下变得越来越逼真。当前研究主要关注于区分真实语音和伪造语音，而追溯源头系统同样重要。本文提出了一种新的音频来源追踪系统，结合了深度度量多类N-pair损失与真实强调和伪造分散框架、Conformer分类网络以及集成分数-嵌入融合方法。N-pair损失提高了辨别能力，而真实强调和伪造分散框架通过专注于区分真实和伪造语音模式增强了鲁棒性。Conformer网络捕捉音频信号中的全局和局部依赖关系，对于来源追踪至关重要。所提出的集成分数-嵌入融合方法在域内和域外来源追踪场景之间显示了最佳的权衡。我们使用Frechet距离和标准指标评估我们的方法，在来源追踪方面优于基线系统。


<details>
  <summary>Details</summary>
Motivation: 随着音频深度伪造技术的发展，伪造音频的真实感达到了前所未有的水平。虽然目前的研究重点是分辨真实语音和伪造语音，但追溯这些伪造语音的来源同样至关重要。这为音频来源追踪提供了动力。

Method: 该方法包括以下组成部分：
1. 深度度量多类N-pair损失：提高系统的辨别能力。
2. 真实强调和伪造分散框架：增强系统对真实和伪造语音模式差异的识别能力，从而提升鲁棒性。
3. Conformer分类网络：用于捕捉音频信号中的全局和局部依赖关系。
4. 集成分数-嵌入融合：在域内和域外来源追踪场景中实现最佳权衡。

Result: 通过使用Frechet距离和标准指标进行评估，结果表明，该方法在音频来源追踪方面显著优于基线系统。

Conclusion: 本文提出的结合深度度量多类N-pair损失、真实强调和伪造分散框架、Conformer分类网络以及集成分数-嵌入融合的音频来源追踪系统，能够有效提高辨别能力和鲁棒性，并在实际应用中展现出优越的性能。

Abstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced
AI. While current research focuses on discerning real speech from spoofed
speech, tracing the source system is equally crucial. This work proposes a
novel audio source tracing system combining deep metric multi-class N-pair loss
with Real Emphasis and Fake Dispersion framework, a Conformer classification
network, and ensemble score-embedding fusion. The N-pair loss improves
discriminative ability, while Real Emphasis and Fake Dispersion enhance
robustness by focusing on differentiating real and fake speech patterns. The
Conformer network captures both global and local dependencies in the audio
signal, crucial for source tracing. The proposed ensemble score-embedding
fusion shows an optimal trade-off between in-domain and out-of-domain source
tracing scenarios. We evaluate our method using Frechet Distance and standard
metrics, demonstrating superior performance in source tracing over the baseline
system.

</details>


### [247] [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/abs/2506.02088)
*Alef Iury Siqueira Ferreira,Lucas Rafael Gris,Alexandre Ferro Filho,Lucas Ólives,Daniel Ribeiro,Luiz Fernando,Fernanda Lustosa,Rodrigo Tanaka,Frederico Santos de Oliveira,Arlindo Galvão Filho*

Main category: cs.SD

TL;DR: This paper presents a robust system for the INTERSPEECH 2025 Speech Emotion Recognition in Naturalistic Conditions Challenge. It combines state-of-the-art audio models with text features enriched by prosodic and spectral cues, investigates Fundamental Frequency quantization and pretrained audio tagging model, employs ensemble model to improve robustness, achieves Macro F1-score of 39.79% on test set, and confirms effectiveness of Graph Attention Networks.


<details>
  <summary>Details</summary>
Motivation: Training SER models in natural, spontaneous speech is challenging due to subtle emotion expression and unpredictable real-world audio.

Method: Combines state-of-the-art audio models with text features enriched by prosodic and spectral cues, investigates Fundamental Frequency quantization and pretrained audio tagging model, employs ensemble model to improve robustness.

Result: Achieved Macro F1-score of 39.79% on official test set (42.20% on validation).

Conclusion: Underlines potential of these methods and confirms effectiveness of Graph Attention Networks.

Abstract: Training SER models in natural, spontaneous speech is especially challenging
due to the subtle expression of emotions and the unpredictable nature of
real-world audio. In this paper, we present a robust system for the INTERSPEECH
2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing
on categorical emotion recognition. Our method combines state-of-the-art audio
models with text features enriched by prosodic and spectral cues. In
particular, we investigate the effectiveness of Fundamental Frequency (F0)
quantization and the use of a pretrained audio tagging model. We also employ an
ensemble model to improve robustness. On the official test set, our system
achieved a Macro F1-score of 39.79% (42.20% on validation). Our results
underscore the potential of these methods, and analysis of fusion techniques
confirmed the effectiveness of Graph Attention Networks. Our source code is
publicly available.

</details>


### [248] [Comparison of spectrogram scaling in multi-label Music Genre Recognition](https://arxiv.org/abs/2506.02091)
*Bartosz Karpiński,Cyryl Leszczyński*

Main category: cs.SD

TL;DR: 随着数字音频工作站的易用性和普及度提高，音乐数量增加，音乐类型之间的差异变得不明显。本文描述和比较了多种预处理方法和模型训练方法，使用了一个包含超过18000个条目的自定义手动标注数据集进行实验。


<details>
  <summary>Details</summary>
Motivation: 音乐数量增加，音乐类型差异不明显，需要研究新的预处理和模型训练方法来应对现代专辑的多样性。

Method: 描述和比较多种预处理方法和模型训练方法，使用一个包含超过18000个条目的自定义手动标注数据集进行实验。

Result: 未提及具体结果，但通过不同方法的比较，可以找到适合处理现代音乐多样性的方法。

Conclusion: 通过对比不同的预处理和模型训练方法，可以更好地理解和处理现代音乐的多样性。

Abstract: As the accessibility and ease-of-use of digital audio workstations increases,
so does the quantity of music available to the average listener; additionally,
differences between genres are not always well defined and can be abstract,
with widely varying combinations of genres across individual records. In this
article, multiple preprocessing methods and approaches to model training are
described and compared, accounting for the eclectic nature of today's albums. A
custom, manually labeled dataset of more than 18000 entries has been used to
perform the experiments.

</details>


### [249] [Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm](https://arxiv.org/abs/2506.02610)
*Zhaoyang Li,Jie Wang,XiaoXiao Li,Wangjie Li,Longjie Luo,Lin Li,Qingyang Hong*

Main category: cs.SD

TL;DR: In speaker diarization, traditional methods have limitations. This paper proposes OCDGALP method which consists of a graph attention network and a label propagation algorithm to overcome these limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional clustering-based methods in speaker diarization are widely used but struggle with complex speaker embeddings distribution and overlapping speech segments.

Method: The proposed framework has two components: (1) a graph attention network that refines speaker embeddings and node connections by aggregating information from neighboring nodes, and (2) a label propagation algorithm that assigns multiple community labels to each node for simultaneous clustering and overlapping community detection.

Result: Experimental results show significant reduction in Diarization Error Rate (DER). It achieves 15.94% DER on DIHARD-III dataset without oracle VAD and 11.07% with oracle VAD.

Conclusion: The proposed OCDGALP method improves the accuracy of speaker diarization significantly.

Abstract: In speaker diarization, traditional clustering-based methods remain widely
used in real-world applications. However, these methods struggle with the
complex distribution of speaker embeddings and overlapping speech segments. To
address these limitations, we propose an Overlapping Community Detection method
based on Graph Attention networks and the Label Propagation Algorithm
(OCDGALP). The proposed framework comprises two key components: (1) a graph
attention network that refines speaker embeddings and node connections by
aggregating information from neighboring nodes, and (2) a label propagation
algorithm that assigns multiple community labels to each node, enabling
simultaneous clustering and overlapping community detection. Experimental
results show that the proposed method significantly reduces the Diarization
Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III
dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%
with oracle VAD.

</details>


### [250] [DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization](https://arxiv.org/abs/2506.02858)
*Geonyoung Lee,Geonhee Han,Paul Hongsuck Seo*

Main category: cs.SD

TL;DR: This paper explores the use of pretrained diffusion models for Language-queried Audio Source Separation (LASS) without additional training, introducing a training-free framework and proposing Diffusion-Guided Mask Optimization (DGMO) to improve separation precision.


<details>
  <summary>Details</summary>
Motivation: Existing LASS methods rely on task-specific training, which limits their flexibility. The authors aim to explore whether pretrained diffusion models can perform audio source separation without further training.

Method: The study introduces a training-free framework leveraging generative priors for zero-shot LASS. It proposes Diffusion-Guided Mask Optimization (DGMO), a test-time optimization framework that refines spectrogram masks for precise, input-aligned separation.

Result: The approach effectively repurposes pretrained diffusion models for source separation, achieving competitive performance without task-specific supervision.

Conclusion: This work demonstrates the potential of pretrained diffusion models for zero-shot audio separation, expanding their application beyond generation and establishing a new paradigm for audio separation.

Abstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound
separation via natural language queries. While existing methods rely on
task-specific training, we explore whether pretrained diffusion models,
originally designed for audio generation, can inherently perform separation
without further training. In this study, we introduce a training-free framework
leveraging generative priors for zero-shot LASS. Analyzing na\"ive adaptations,
we identify key limitations arising from modality-specific challenges.To
address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a
test-time optimization framework that refines spectrogram masks for precise,
input-aligned separation. Our approach effectively repurposes pretrained
diffusion models for source separation, achieving competitive performance
without task-specific supervision. This work expands the application of
diffusion models beyond generation, establishing a new paradigm for zero-shot
audio separation. The code is available at: https://wltschmrz.github.io/DGMO/

</details>


### [251] [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](https://arxiv.org/abs/2506.03099)
*Chetwin Low,Weimin Wang*

Main category: cs.SD

TL;DR: This paper introduces TalkingMachines, an efficient framework that transforms pretrained video generation models into real-time, audio-driven character animators.


<details>
  <summary>Details</summary>
Motivation: To create natural conversational experiences by integrating an audio large language model (LLM) with a video generation foundation model.

Method: Adapting a pretrained SOTA image-to-video DiT into an audio-driven avatar generation model; enabling infinite video streaming without error accumulation through asymmetric knowledge distillation; designing a high-throughput, low-latency inference pipeline with key engineering optimizations.

Result: Successful creation of TalkingMachines which allows for real-time, audio-driven character animation.

Conclusion: TalkingMachines presents an innovative approach to transforming pretrained video generation models into real-time, audio-driven character animators.

Abstract: In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a sparse causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [252] [A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis](https://arxiv.org/abs/2506.02076)
*Christian Marius Lillelund,Sanjay Kalra,Russell Greiner*

Main category: q-bio.QM

TL;DR: A new method using covariate-based survival models accurately predicts functional decline in ALS patients, allowing personalized treatment planning.


<details>
  <summary>Details</summary>
Motivation: ALS is a complex disease with varied progression rates, making it challenging to determine optimal treatment timing. Predicting the time until significant functional impairment can help personalize treatment plans and improve patient care.

Method: The study proposes a novel method to predict when ALS patients will experience significant functional impairment by formulating it as a multi-event survival problem. It trains five covariate-based survival models on the PRO-ACT dataset to estimate event probabilities over 500 days and then predicts five event-specific individual survival distributions (ISDs) per patient.

Result: The covariate-based models showed superior performance compared to the Kaplan-Meier estimator in predicting time-to-event outcomes. Counterfactual predictions revealed minimal impact of Riluzole on functional decline and shorter times for bulbar-onset ALS patients in speech and swallowing tasks compared to limb-onset ALS.

Conclusion: Covariate-based models outperform the Kaplan-Meier estimator in predicting time-to-event outcomes for ALS patients. The method also allows individual counterfactual predictions, showing little effect of Riluzole on functional decline and shorter time-to-event estimates for bulbar-onset ALS compared to limb-onset ALS.

Abstract: Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor
neurons that causes progressive paralysis in patients. Current treatment
options aim to prolong survival and improve quality of life; however, due to
the heterogeneity of the disease, it is often difficult to determine the
optimal time for potential therapies or medical interventions. In this study,
we propose a novel method to predict the time until a patient with ALS
experiences significant functional impairment (ALSFRS-R<=2) with respect to
five common functions: speaking, swallowing, handwriting, walking and
breathing. We formulate this task as a multi-event survival problem and
validate our approach in the PRO-ACT dataset by training five covariate-based
survival models to estimate the probability of an event over a 500-day period
after a baseline visit. We then predict five event-specific individual survival
distributions (ISDs) for each patient, each providing an interpretable and
meaningful estimate of when that event will likely take place in the future.
The results show that covariate-based models are superior to the Kaplan-Meier
estimator at predicting time-to-event outcomes. Additionally, our method
enables practitioners to make individual counterfactual predictions, where
certain features (covariates) can be changed to see their effect on the
predicted outcome. In this regard, we find that Riluzole has little to no
impact on predicted functional decline. However, for patients with bulbar-onset
ALS, our method predicts considerably shorter counterfactual time-to-event
estimates for tasks related to speech and swallowing compared to limb-onset
ALS. The proposed method can be applied to current clinical examination data to
assess the risk of functional decline and thus allow more personalized
treatment planning.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [253] [Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment](https://arxiv.org/abs/2506.02046)
*Mohammad Saleh Torkestani,Taha Mansouri*

Main category: cs.CY

TL;DR: This paper proposes a theoretical framework to address the challenges of generative AI in higher education assessment using a machine-versus-machine approach. It introduces a dual strategy paradigm combining static analysis and dynamic testing for comprehensive evaluation.


<details>
  <summary>Details</summary>
Motivation: Generative AI like GPT-4 is capable of producing sophisticated academic content, posing significant challenges to traditional assessment methods in higher education.

Method: The method involves creating a dual strategy paradigm that combines static analysis (with eight theoretically justified elements) and dynamic testing for simulation-based vulnerability assessment.

Result: A theoretical framework was developed for assessing vulnerabilities in educational assessments due to generative AI, including concepts for quantitative assessment, weighting frameworks, and threshold determination theory.

Conclusion: This paper concludes by presenting a comprehensive theoretical framework to evaluate and mitigate the impact of generative AI on higher education assessments.

Abstract: This paper presents a theoretical framework for addressing the challenges
posed by generative artificial intelligence (AI) in higher education assessment
through a machine-versus-machine approach. Large language models like GPT-4,
Claude, and Llama increasingly demonstrate the ability to produce sophisticated
academic content, traditional assessment methods face an existential threat,
with surveys indicating 74-92% of students experimenting with these tools for
academic purposes. Current responses, ranging from detection software to manual
assessment redesign, show significant limitations: detection tools demonstrate
bias against non-native English writers and can be easily circumvented, while
manual frameworks rely heavily on subjective judgment and assume static AI
capabilities. This paper introduces a dual strategy paradigm combining static
analysis and dynamic testing to create a comprehensive theoretical framework
for assessment vulnerability evaluation. The static analysis component
comprises eight theoretically justified elements: specificity and
contextualization, temporal relevance, process visibility requirements,
personalization elements, resource accessibility, multimodal integration,
ethical reasoning requirements, and collaborative elements. Each element
addresses specific limitations in generative AI capabilities, creating barriers
that distinguish authentic human learning from AI-generated simulation. The
dynamic testing component provides a complementary approach through
simulation-based vulnerability assessment, addressing limitations in
pattern-based analysis. The paper presents a theoretical framework for
vulnerability scoring, including the conceptual basis for quantitative
assessment, weighting frameworks, and threshold determination theory.

</details>


### [254] [Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI](https://arxiv.org/abs/2506.02055)
*Nikola Balic*

Main category: cs.CY

TL;DR: The study analyzes perceptions of AI agents among professionals, revealing three clusters of respondents and highlighting the need for governance frameworks as organizations integrate autonomous agents.


<details>
  <summary>Details</summary>
Motivation: To understand current perceptions among professionals regarding autonomous multi-agent AI systems in order to anticipate adoption challenges, ethical considerations, and future workforce development.

Method: The study analyzed responses from 130 participants to a survey on the capabilities, impact, and governance of AI agents, exploring expected timelines for AI replacing programmers, perceived barriers to deployment, and beliefs about responsibility when agents make critical decisions.

Result: Three distinct clusters of respondents were identified. An initial logistic regression model did not yield statistically significant predictors for AI agent deployment, suggesting that deployment decisions are complex and may be influenced by factors not fully captured or that a larger sample is needed.

Conclusion: Organizations need to address compliance concerns and establish clear governance frameworks as they integrate autonomous agents into their workflows.

Abstract: Autonomous multi-agent AI systems are poised to transform various industries,
particularly software development and knowledge work. Understanding current
perceptions among professionals is crucial for anticipating adoption
challenges, ethical considerations, and future workforce development. This
study analyzes responses from 130 participants to a survey on the capabilities,
impact, and governance of AI agents. We explore expected timelines for AI
replacing programmers, identify perceived barriers to deployment, and examine
beliefs about responsibility when agents make critical decisions. Key findings
reveal three distinct clusters of respondents. While the study explored factors
associated with current AI agent deployment, the initial logistic regression
model did not yield statistically significant predictors, suggesting that
deployment decisions are complex and may be influenced by factors not fully
captured or that a larger sample is needed. These insights highlight the need
for organizations to address compliance concerns (a commonly cited barrier) and
establish clear governance frameworks as they integrate autonomous agents into
their workflows.

</details>


### [255] [AI Data Development: A Scorecard for the System Card Framework](https://arxiv.org/abs/2506.02071)
*Tadesse K. Bahiru,Haileleol Tibebu,Ioannis A. Kakadiaris*

Main category: cs.CY

TL;DR: Artificial intelligence systems rely on quality datasets for reliability. This paper presents a scorecard to evaluate AI dataset development in five key areas, offering tailored recommendations to improve data transparency and integrity.


<details>
  <summary>Details</summary>
Motivation: To address concerns about the transparency, accountability, and potential biases in AI systems by evaluating the quality of underlying datasets.

Method: A scorecard method using an intake form and scoring criteria to assess datasets across five key areas: data dictionary, collection process, composition, motivation, and pre-processing.

Result: The scorecard reveals strengths and areas for improvement in four diverse datasets, providing tailored recommendations to enhance their transparency and integrity.

Conclusion: This approach improves dataset quality, offering practical guidance to curators and researchers for developing responsible AI systems.

Abstract: Artificial intelligence has transformed numerous industries, from healthcare
to finance, enhancing decision-making through automated systems. However, the
reliability of these systems is mainly dependent on the quality of the
underlying datasets, raising ongoing concerns about transparency,
accountability, and potential biases. This paper introduces a scorecard
designed to evaluate the development of AI datasets, focusing on five key areas
from the system card framework data development life cycle: data dictionary,
collection process, composition, motivation, and pre-processing. The method
follows a structured approach, using an intake form and scoring criteria to
assess the quality and completeness of the data set. Applied to four diverse
datasets, the methodology reveals strengths and improvement areas. The results
are compiled using a scoring system that provides tailored recommendations to
enhance the transparency and integrity of the data set. The scorecard addresses
technical and ethical aspects, offering a holistic evaluation of data
practices. This approach aims to improve the quality of the data set. It offers
practical guidance to curators and researchers in developing responsible AI
systems, ensuring fairness and accountability in decision support systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [256] [Singularity Blockchain Key Management via non-custodial key management](https://arxiv.org/abs/2506.02282)
*Sumit Vohra*

Main category: cs.CE

TL;DR: This paper proposes a new non-custodial key management method for web3 wallets, enabling users to back up and recover their private keys via independent sign-in methods like Google-oAuth, reducing the risk of asset loss and simplifying user onboarding.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the issues with existing non-custodial key management schemes in web3 wallets. Current schemes place the entire burden of key storage and recovery on the user through seed-phrases, which can lead to difficulties in onboarding and the potential loss of assets if the seed-phrase or private key is forgotten or lost.

Method: The proposed method involves a novel non-custodial key management technique that allows users to save and recover backups of their private keys using any independent sign-in method, such as Google-oAuth or other third-party oAuth services.

Result: This approach aims to reduce the hassle of onboarding for users and mitigate the risk of losing assets due to forgotten or lost seed-phrases/private keys by providing an alternative, more user-friendly backup and recovery solution.

Conclusion: In conclusion, the paper presents a promising solution to enhance the security and usability of web3 wallets through a new non-custodial key management method that leverages independent sign-in methods for private key backup and recovery.

Abstract: web3 wallets are key to managing user identity on blockchain. The main
purpose of a web3 wallet application is to manage the private key for the user
and provide an interface to interact with the blockchain. The key management
scheme ( KMS ) used by the wallet to store and recover the private key can be
either custodial, where the keys are permissioned and in custody of the wallet
provider or noncustodial where the keys are in custody of the user. The
existing non-custodial key management schemes tend to offset the burden of
storing and recovering the key entirely on the user by asking them to remember
seed-phrases. This creates onboarding hassles for the user and introduces the
risk that the user may lose their assets if they forget or lose their
seedphrase/private key. In this paper, we propose a novel method of backing up
user keys using a non-custodial key management technique that allows users to
save and recover a backup of their private key using any independent sign-in
method such as google-oAuth or other 3P oAuth.

</details>


### [257] [Enriching Location Representation with Detailed Semantic Information](https://arxiv.org/abs/2506.02744)
*Junyuan Liu,Xinglei Wang,Tao Cheng*

Main category: cs.CE

TL;DR: The paper presents CaLLiPer+, an improved model that integrates POI names within a multimodal contrastive learning framework for urban environment spatial representation. It demonstrates consistent performance gains in land use classification and socioeconomic status distribution mapping, enhances location retrieval, and highlights the benefits of pretrained text encoders.


<details>
  <summary>Details</summary>
Motivation: Traditional spatial embeddings often prioritize spatial proximity while underutilizing fine-grained contextual information from places in urban environments.

Method: CaLLiPer+ systematically integrates Point-of-Interest (POI) names alongside categorical labels within a multimodal contrastive learning framework.

Result: Demonstrates consistent performance gains of 4% to 11% over baseline methods on land use classification and socioeconomic status distribution mapping tasks. Incorporating POI names enhances location retrieval and ablation studies reveal the complementary role of POI names and advantages of pretrained text encoders.

Conclusion: The findings highlight the potential of integrating fine-grained semantic attributes and multimodal learning techniques to advance the development of urban foundation models.

Abstract: Spatial representations that capture both structural and semantic
characteristics of urban environments are essential for urban modeling.
Traditional spatial embeddings often prioritize spatial proximity while
underutilizing fine-grained contextual information from places. To address this
limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that
systematically integrates Point-of-Interest (POI) names alongside categorical
labels within a multimodal contrastive learning framework. We evaluate its
effectiveness on two downstream tasks, land use classification and
socioeconomic status distribution mapping, demonstrating consistent performance
gains of 4% to 11% over baseline methods. Additionally, we show that
incorporating POI names enhances location retrieval, enabling models to capture
complex urban concepts with greater precision. Ablation studies further reveal
the complementary role of POI names and the advantages of leveraging pretrained
text encoders for spatial representations. Overall, our findings highlight the
potential of integrating fine-grained semantic attributes and multimodal
learning techniques to advance the development of urban foundation models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [258] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)
*Wang Mengjie,Zhu Huiping,Li Jian,Shi Wenxiu,Zhang Song*

Main category: cs.CV

TL;DR: This paper proposes a comprehensive method for optimizing multimodal models in driving scenarios, which includes dynamic prompt optimization, dataset construction with real and synthetic data, advanced model training techniques, and efficient deployment. The method enhances task-specific focus, improves generalization in complex environments, reduces costs while boosting performance, and supports practical applications of driving perception technologies.


<details>
  <summary>Details</summary>
Motivation: Higher demands are placed on the ability to understand complex driving scenarios with the advancement of autonomous and assisted driving technologies. Multimodal general large models have emerged as a solution but face challenges such as data collection, model training, and deployment optimization in vertical domains.

Method: The proposed method includes dynamic prompt optimization (adjusting prompts based on input image content), constructing a high-quality and diverse multimodal training dataset by combining real and synthetic data, integrating advanced techniques in model training (knowledge distillation, dynamic fine-tuning, quantization) to reduce storage and computational costs while boosting performance.

Result: Experimental results show that this systematic optimization method significantly improves the model's accuracy in key tasks such as cone detection, traffic light recognition, speed limit recommendation, and intersection alerts, while achieving efficient resource utilization.

Conclusion: This comprehensive method not only enhances the model's task-specific focus and judgment capabilities but also provides strong support for the practical application of driving scenario perception technologies.

Abstract: With the advancement of autonomous and assisted driving technologies, higher
demands are placed on the ability to understand complex driving scenarios.
Multimodal general large models have emerged as a solution for this challenge.
However, applying these models in vertical domains involves difficulties such
as data collection, model training, and deployment optimization. This paper
proposes a comprehensive method for optimizing multimodal models in driving
scenarios, including cone detection, traffic light recognition, speed limit
recommendation, and intersection alerts. The method covers key aspects such as
dynamic prompt optimization, dataset construction, model training, and
deployment. Specifically, the dynamic prompt optimization adjusts the prompts
based on the input image content to focus on objects affecting the ego vehicle,
enhancing the model's task-specific focus and judgment capabilities. The
dataset is constructed by combining real and synthetic data to create a
high-quality and diverse multimodal training dataset, improving the model's
generalization in complex driving environments. In model training, advanced
techniques like knowledge distillation, dynamic fine-tuning, and quantization
are integrated to reduce storage and computational costs while boosting
performance. Experimental results show that this systematic optimization method
not only significantly improves the model's accuracy in key tasks but also
achieves efficient resource utilization, providing strong support for the
practical application of driving scenario perception technologies.

</details>


### [259] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/abs/2506.02021)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: 在视觉任务、数据集和模型迅速发展的背景下，减少视觉数据集中的冗余已成为研究的关键领域。尽管数据集蒸馏（DD）在图像数据集上得到了广泛研究，但视频数据集上的DD仍鲜有探索。本文提出了一种名为动态感知视频蒸馏（DAViD）的方法，利用强化学习预测合成视频的最佳时间分辨率，并提出了教师参与的奖励函数以更新RL代理策略。这是首次根据视频语义引入自适应时间分辨率的研究，显著优于现有的DD方法，为更高效和语义自适应的视频数据集蒸馏研究铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 当前的数据集蒸馏技术主要针对图像数据集，在视频数据集上的应用较少，且现有方法假设所有视频语义的时间冗余程度相同，这限制了其在视频数据集上的效果。因此，需要一种能够根据视频语义自适应调整时间分辨率的方法。

Method: 提出了一种名为动态感知视频蒸馏（DAViD）的方法，使用强化学习预测合成视频的最佳时间分辨率。同时，设计了一个教师参与的奖励函数来更新强化学习代理的策略。

Result: 该方法显著优于现有的数据集蒸馏方法，在性能上有明显的提升。

Conclusion: 本研究首次引入了基于视频语义的自适应时间分辨率概念，为未来更高效和语义自适应的视频数据集蒸馏研究奠定了基础。

Abstract: With the rapid development of vision tasks and the scaling on datasets and
models, redundancy reduction in vision datasets has become a key area of
research. To address this issue, dataset distillation (DD) has emerged as a
promising approach to generating highly compact synthetic datasets with
significantly less redundancy while preserving essential information. However,
while DD has been extensively studied for image datasets, DD on video datasets
remains underexplored. Video datasets present unique challenges due to the
presence of temporal information and varying levels of redundancy across
different classes. Existing DD approaches assume a uniform level of temporal
redundancy across all different video semantics, which limits their
effectiveness on video datasets. In this work, we propose Dynamic-Aware Video
Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the
optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop
reward function is proposed to update the RL agent policy. To the best of our
knowledge, this is the first study to introduce adaptive temporal resolution
based on video semantics in video dataset distillation. Our approach
significantly outperforms existing DD methods, demonstrating substantial
improvements in performance. This work paves the way for future research on
more efficient and semantic-adaptive video dataset distillation research.

</details>


### [260] [Implicit Deformable Medical Image Registration with Learnable Kernels](https://arxiv.org/abs/2506.02150)
*Stefano Fogarollo,Gregor Laimer,Reto Bale,Matthias Harders*

Main category: cs.CV

TL;DR: This paper presents a new implicit registration framework for deformable medical image registration, reformulating the task as signal reconstruction problem and achieving competitive accuracy with better anatomical relationship preservation.


<details>
  <summary>Details</summary>
Motivation: Deformable medical image registration is crucial in oncological treatments for precise image alignment. However, current AI methods produce unreliable deformations that hinder clinical adoption.

Method: The authors introduce an implicit registration framework that reformulates image registration as a signal reconstruction problem. They learn a kernel function to recover the dense displacement field from sparse keypoint correspondences and integrate it into a hierarchical architecture for coarse-to-fine estimation. The method also allows efficient refinement at test time.

Result: The method was validated on challenging intra-patient thoracic and abdominal zero-shot registration tasks, showing competitive accuracy to state-of-the-art approaches while bridging the generalization gap between implicit and explicit techniques.

Conclusion: This novel approach generates deformations that preserve anatomical relationships and matches the performance of specialized commercial systems, demonstrating potential for clinical adoption.

Abstract: Deformable medical image registration is an essential task in
computer-assisted interventions. This problem is particularly relevant to
oncological treatments, where precise image alignment is necessary for tracking
tumor growth, assessing treatment response, and ensuring accurate delivery of
therapies. Recent AI methods can outperform traditional techniques in accuracy
and speed, yet they often produce unreliable deformations that limit their
clinical adoption. In this work, we address this challenge and introduce a
novel implicit registration framework that can predict accurate and reliable
deformations. Our insight is to reformulate image registration as a signal
reconstruction problem: we learn a kernel function that can recover the dense
displacement field from sparse keypoint correspondences. We integrate our
method in a novel hierarchical architecture, and estimate the displacement
field in a coarse-to-fine manner. Our formulation also allows for efficient
refinement at test time, permitting clinicians to easily adjust registrations
when needed. We validate our method on challenging intra-patient thoracic and
abdominal zero-shot registration tasks, using public and internal datasets from
the local University Hospital. Our method not only shows competitive accuracy
to state-of-the-art approaches, but also bridges the generalization gap between
implicit and explicit registration techniques. In particular, our method
generates deformations that better preserve anatomical relationships and
matches the performance of specialized commercial systems, underscoring its
potential for clinical adoption.

</details>


### [261] [Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos](https://arxiv.org/abs/2506.02167)
*Aditi Tiwari,Farzaneh Masoud,Dac Trong Nguyen,Jill Kraft,Heng Ji,Klara Nahrstedt*

Main category: cs.CV

TL;DR: 现代AI系统在需要可靠性的环境中表现不佳，例如烟雾弥漫、能见度低和结构变形的场景。我们引入了Fire360基准测试，用于评估关键消防情景中的感知和推理能力。该数据集包含228个360度视频，支持五项任务：视觉问答、时间动作描述、目标定位、关键推理和转换对象检索（TOR）。通过发布Fire360及其评估套件，我们的目标是推动模型的发展，使其不仅能够看到，还能在不确定性下记住、推理和行动。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在可靠性至关重要的环境中表现不佳，尤其是在烟雾、能见度差和结构变形的场景中。每年有成千上万名消防员因现场感知问题而受伤。因此，需要一个专门的数据集来评估AI系统在这些关键情景中的感知和推理能力。

Method: 引入Fire360数据集，该数据集包括228个360度视频，涵盖多种条件（如低光、热失真），并标注了动作段、物体位置和退化元数据。Fire360支持五项任务：视觉问答、时间动作描述、目标定位、关键推理和转换对象检索（TOR）。TOR任务测试模型是否能够在未配对的场景中将原始样本与火灾损坏的对应物匹配，评估变换不变识别能力。

Result: 人类专家在TOR任务上达到83.5%的准确率，而像GPT-4o这样的模型则显著落后，显示出在退化条件下推理能力的不足。

Conclusion: 通过发布Fire360数据集及其评估套件，研究者希望推动模型的发展，使其不仅能够看到，还能记住、推理和在不确定性下行动。

Abstract: Modern AI systems struggle most in environments where reliability is critical
- scenes with smoke, poor visibility, and structural deformation. Each year,
tens of thousands of firefighters are injured on duty, often due to breakdowns
in situational perception. We introduce Fire360, a benchmark for evaluating
perception and reasoning in safety-critical firefighting scenarios. The dataset
includes 228 360-degree videos from professional training sessions under
diverse conditions (e.g., low light, thermal distortion), annotated with action
segments, object locations, and degradation metadata. Fire360 supports five
tasks: Visual Question Answering, Temporal Action Captioning, Object
Localization, Safety-Critical Reasoning, and Transformed Object Retrieval
(TOR). TOR tests whether models can match pristine exemplars to fire-damaged
counterparts in unpaired scenes, evaluating transformation-invariant
recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag
significantly, exposing failures in reasoning under degradation. By releasing
Fire360 and its evaluation suite, we aim to advance models that not only see,
but also remember, reason, and act under uncertainty. The dataset is available
at: https://uofi.box.com/v/fire360dataset.

</details>


### [262] [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
*Manas Mehta,Yimu Pan,Kelly Gallagher,Alison D. Gernand,Jeffery A. Goldstein,Delia Mwinyelle,Leena Mithal,James Z. Wang*

Main category: cs.CV

TL;DR: The paper proposes modifications to vision-language contrastive learning frameworks, specifically for placenta pathology examination. These include text-anchored vision-language contrastive knowledge distillation and unsupervised predistillation. The approach improves accuracy and efficiency, making AI-based healthcare solutions more accessible.


<details>
  <summary>Details</summary>
Motivation: Pathological examination of the placenta is crucial for detecting childbirth-related health risks. However, existing automated methods using AI are computationally expensive, limiting their deployability.

Method: The paper introduces two modifications to vision-language contrastive learning (VLC) frameworks: 1) text-anchored vision-language contrastive knowledge distillation (VLCD) for medical VLC pretraining, and 2) unsupervised predistillation using a large natural images dataset for better initialization.

Result: The proposed approach successfully distills efficient neural networks that match or surpass teacher models in performance while achieving model compression and acceleration. It also enhances performance and robustness, particularly for lower-quality images.

Conclusion: The modifications to VLC frameworks improve both the efficiency and deployability of medical AI solutions, making them more accessible in resource-constrained environments.

Abstract: Pathological examination of the placenta is an effective method for detecting
and mitigating health risks associated with childbirth. Recent advancements in
AI have enabled the use of photographs of the placenta and pathology reports
for detecting and classifying signs of childbirth-related pathologies. However,
existing automated methods are computationally extensive, which limits their
deployability. We propose two modifications to vision-language contrastive
learning (VLC) frameworks to enhance their accuracy and efficiency: (1)
text-anchored vision-language contrastive knowledge distillation (VLCD)-a new
knowledge distillation strategy for medical VLC pretraining, and (2)
unsupervised predistillation using a large natural images dataset for improved
initialization. Our approach distills efficient neural networks that match or
surpass the teacher model in performance while achieving model compression and
acceleration. Our results showcase the value of unsupervised predistillation in
improving the performance and robustness of our approach, specifically for
lower-quality images. VLCD serves as an effective way to improve the efficiency
and deployability of medical VLC approaches, making AI-based healthcare
solutions more accessible, especially in resource-constrained environments.

</details>


### [263] [Motion aware video generative model](https://arxiv.org/abs/2506.02244)
*Bowen Xue,Giuseppe Claudio Guarnera,Shuang Zhao,Zahra Montazeri*

Main category: cs.CV

TL;DR: Recent advances in diffusion-based video generation have led to high-quality visual content, but subtle non-physical artifacts remain. This paper introduces a physics-informed frequency domain approach with two components - a physical motion loss function and a frequency domain enhancement module - that improve the physical plausibility of generated videos without sacrificing visual or semantic quality.


<details>
  <summary>Details</summary>
Motivation: Despite the high quality of current diffusion-based video generation methods, they lack explicit modeling of the underlying physics of motion, leading to subtle non-physical artifacts that diminish realism.

Method: The method involves conducting a systematic analysis of the frequency-domain characteristics of different physical motions (translation, rotation, scaling), then proposing two complementary components: a physical motion loss function and a frequency domain enhancement module. The loss function quantifies and optimizes conformity to ideal motion patterns, while the enhancement module adjusts video features to conform to physical motion constraints.

Result: Experiments across multiple video diffusion architectures demonstrate significant improvements in motion quality and physical plausibility without compromising visual quality or semantic alignment.

Conclusion: The proposed frequency-domain physical motion framework effectively generalizes across different video generation architectures, bridging the gap between data-driven models and physics-based motion models.

Abstract: Recent advances in diffusion-based video generation have yielded
unprecedented quality in visual content and semantic coherence. However,
current approaches predominantly rely on statistical learning from vast
datasets without explicitly modeling the underlying physics of motion,
resulting in subtle yet perceptible non-physical artifacts that diminish the
realism of generated videos. This paper introduces a physics-informed frequency
domain approach to enhance the physical plausibility of generated videos. We
first conduct a systematic analysis of the frequency-domain characteristics of
diverse physical motions (translation, rotation, scaling), revealing that each
motion type exhibits distinctive and identifiable spectral signatures. Building
on this theoretical foundation, we propose two complementary components: (1) a
physical motion loss function that quantifies and optimizes the conformity of
generated videos to ideal frequency-domain motion patterns, and (2) a frequency
domain enhancement module that progressively learns to adjust video features to
conform to physical motion constraints while preserving original network
functionality through a zero-initialization strategy. Experiments across
multiple video diffusion architectures demonstrate that our approach
significantly enhances motion quality and physical plausibility without
compromising visual quality or semantic alignment. Our frequency-domain
physical motion framework generalizes effectively across different video
generation architectures, offering a principled approach to incorporating
physical constraints into deep learning-based video synthesis pipelines. This
work seeks to establish connections between data-driven models and
physics-based motion models.

</details>


### [264] [QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](https://arxiv.org/abs/2506.02295)
*Ahmed Wasfy,Omer Nacar,Abdelakreem Elkhateb,Mahmoud Reda,Omar Elshehy,Adel Ammar,Wadii Boulila*

Main category: cs.CV

TL;DR: An advanced Arabic OCR system, Qari-OCR, is developed based on Qwen2-VL-2B-Instruct with iterative fine-tuning. The leading model QARI v0.2 sets a new open-source benchmark for Arabic OCR in terms of Word Error Rate (WER), Character Error Rate (CER), and BLEU score. It excels in recognizing diacritical marks, diverse fonts, document layouts, and even low-resolution images. Future versions like QARI v0.3 show promise in structural document understanding and handwritten text recognition.


<details>
  <summary>Details</summary>
Motivation: The complexities of Arabic script, including its cursive nature, diacritical marks (tashkeel), and varied typography, present significant challenges for Optical Character Recognition (OCR) systems.

Method: Qari-OCR is a series of vision-language models derived from Qwen2-VL-2B-Instruct and progressively optimized for Arabic through iterative fine-tuning on specialized synthetic datasets.

Result: The leading model, QARI v0.2, achieved a Word Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score of 0.737 on texts rich in diacritical marks. It demonstrates superior handling of tashkeel, diverse fonts, document layouts, and impressive performance on low-resolution images.

Conclusion: This work significantly improves the accuracy and efficiency of Arabic OCR and releases all models and datasets to encourage further research.

Abstract: The inherent complexities of Arabic script; its cursive nature, diacritical
marks (tashkeel), and varied typography, pose persistent challenges for Optical
Character Recognition (OCR). We present Qari-OCR, a series of vision-language
models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic
through iterative fine-tuning on specialized synthetic datasets. Our leading
model, QARI v0.2, establishes a new open-source state-of-the-art with a Word
Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score
of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling
of tashkeel, diverse fonts, and document layouts, alongside impressive
performance on low-resolution images. Further explorations (QARI v0.3) showcase
strong potential for structural document understanding and handwritten text.
This work delivers a marked improvement in Arabic OCR accuracy and efficiency,
with all models and datasets released to foster further research.

</details>


### [265] [Are classical deep neural networks weakly adversarially robust?](https://arxiv.org/abs/2506.02016)
*Nuolin Sun,Linyuan Wang,Dongyang Li,Bin Yan,Lei Li*

Main category: cs.CV

TL;DR: This paper proposes a new method for adversarial example detection and image recognition that constructs feature paths using layer-wise features. It achieves competitive performance without the computational cost of adversarial training.


<details>
  <summary>Details</summary>
Motivation: Classical DNNs have weak adversarial robustness, and adversarial training is computationally expensive.

Method: The method uses layer-wise features to construct feature paths and computes the correlation between examples' feature paths and class-centered feature paths.

Result: On ResNet-20 with PFC, it achieves 82.77% clean accuracy and 44.17% adversarial accuracy; on standard ResNet-18, it achieves 80.01% clean accuracy and 46.1% adversarial accuracy.

Conclusion: This method provides a trade-off in performance without relying on expensive defense strategies, revealing inherent adversarial robustness in DNNs.

Abstract: Adversarial attacks have received increasing attention and it has been widely
recognized that classical DNNs have weak adversarial robustness. The most
commonly used adversarial defense method, adversarial training, improves the
adversarial accuracy of DNNs by generating adversarial examples and retraining
the model. However, adversarial training requires a significant computational
overhead. In this paper, inspired by existing studies focusing on the
clustering properties of DNN output features at each layer and the Progressive
Feedforward Collapse phenomenon, we propose a method for adversarial example
detection and image recognition that uses layer-wise features to construct
feature paths and computes the correlation between the examples feature paths
and the class-centered feature paths. Experimental results show that the
recognition method achieves 82.77% clean accuracy and 44.17% adversarial
accuracy on the ResNet-20 with PFC. Compared to the adversarial training method
with 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits
a trade-off without relying on computationally expensive defense strategies.
Furthermore, on the standard ResNet-18, our method maintains this advantage
with respective metrics of 80.01% and 46.1%. This result reveals inherent
adversarial robustness in DNNs, challenging the conventional understanding of
the weak adversarial robustness in DNNs.

</details>


### [266] [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/abs/2506.02020)
*Youze Xue,Dian Li,Gang Liu*

Main category: cs.CV

TL;DR: The paper proposes Explicit Gradient Amplifier to boost gradients of hard negative samples in multi-modal embedding models, achieving SOTA performance on MMEB benchmark with LLaVA-OneVision-7B and self-developed QQMM architectures.


<details>
  <summary>Details</summary>
Motivation: Although contrastive learning has been successfully extended from CLIP-style models to MLLMs, the specific contribution of each hard negative sample to the learning process has not been thoroughly investigated. This motivates the need for a deeper understanding and improvement of how hard negatives influence model training.

Method: Conduct gradient analysis of info-NCE loss w.r.t query, positive, and negative samples. Propose Explicit Gradient Amplifier to amplify gradients associated with hard negative samples, thus encouraging more discriminative embeddings. Train multi-modal embedding model based on LLaVA-OneVision-7B architecture with this amplifier.

Result: Achieves state-of-the-art performance on MMEB benchmark compared to methods using same MLLM backbone. When combined with self-developed QQMM MLLM, it ranks top on MMEB leaderboard.

Conclusion: Explicit Gradient Amplifier significantly improves multi-modal embeddings by leveraging hard negative samples more effectively. The approach sets new benchmarks in performance when applied to suitable architectures.

Abstract: With the rapid advancement of multi-modal large language models (MLLMs) in
recent years, the foundational Contrastive Language-Image Pretraining (CLIP)
framework has been successfully extended to MLLMs, enabling more powerful and
universal multi-modal embeddings for a wide range of retrieval tasks. Despite
these developments, the core contrastive learning paradigm remains largely
unchanged from CLIP-style models to MLLMs. Within this framework, the effective
mining of hard negative samples continues to be a critical factor for enhancing
performance. Prior works have introduced both offline and online strategies for
hard negative mining to improve the efficiency of contrastive learning. While
these approaches have led to improved multi-modal embeddings, the specific
contribution of each hard negative sample to the learning process has not been
thoroughly investigated. In this work, we conduct a detailed analysis of the
gradients of the info-NCE loss with respect to the query, positive, and
negative samples, elucidating the role of hard negatives in updating model
parameters. Building upon this analysis, we propose to explicitly amplify the
gradients associated with hard negative samples, thereby encouraging the model
to learn more discriminative embeddings. Our multi-modal embedding model,
trained with the proposed Explicit Gradient Amplifier and based on the
LLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the
MMEB benchmark compared to previous methods utilizing the same MLLM backbone.
Furthermore, when integrated with our self-developed MLLM, QQMM, our approach
attains the top rank on the MMEB leaderboard. Code and models are released on
https://github.com/QQ-MM/QQMM-embed.

</details>


### [267] [VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos](https://arxiv.org/abs/2506.02448)
*Baoyu Liang,Qile Su,Shoutai Zhu,Yuchen Liang,Chao Tong*

Main category: cs.CV

TL;DR: The paper proposes video event understanding to extract event scripts and make predictions from videos, introduces a large-scale dataset VidEvent with over 23,000 labeled events, provides baseline models for benchmarking, and makes resources publicly available.


<details>
  <summary>Details</summary>
Motivation: AI struggles to understand events in videos due to their complex structures, semantic hierarchies, and dynamic evolution.

Method: Propose the task of video event understanding, introduce VidEvent dataset with detailed annotations, and provide baseline models for future research.

Result: VidEvent dataset shows potential to advance video event understanding and encourages exploration of new algorithms and models.

Conclusion: VidEvent and baseline models can facilitate future research and improvements in video event understanding.

Abstract: Despite the significant impact of visual events on human cognition,
understanding events in videos remains a challenging task for AI due to their
complex structures, semantic hierarchies, and dynamic evolution. To address
this, we propose the task of video event understanding that extracts event
scripts and makes predictions with these scripts from videos. To support this
task, we introduce VidEvent, a large-scale dataset containing over 23,000
well-labeled events, featuring detailed event structures, broad hierarchies,
and logical relations extracted from movie recap videos. The dataset was
created through a meticulous annotation process, ensuring high-quality and
reliable event data. We also provide comprehensive baseline models offering
detailed descriptions of their architecture and performance metrics. These
models serve as benchmarks for future research, facilitating comparisons and
improvements. Our analysis of VidEvent and the baseline models highlights the
dataset's potential to advance video event understanding and encourages the
exploration of innovative algorithms and models. The dataset and related
resources are publicly available at www.videvent.top.

</details>


### [268] [Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models](https://arxiv.org/abs/2506.02488)
*Hongtao Huang,Xiaojun Chang,Lina Yao*

Main category: cs.CV

TL;DR: Flexiffusion是一种无需训练的神经架构搜索框架，通过优化生成调度和模型架构，在不修改预训练参数的情况下，实现了至少2倍的加速，且FID退化在5%以内。它引入了相对FID（rFID）评估指标，将评估时间减少了90%以上。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）尽管能够生成高质量图像，但其多步推理过程计算成本高。现有的神经架构搜索方法存在需要重新训练、搜索复杂度指数级增长以及评估速度慢等问题。

Method: 提出了一种无需训练的NAS框架Flexiffusion，它可以联合优化生成计划和模型架构而不修改预训练参数。将生成过程分解为等长的灵活段，每段动态组合三种步骤类型：完整、部分和空步骤。此外，还引入了一个轻量级的评估指标相对FID（rFID）。

Result: 在ImageNet和MS-COCO上的实验表明，Flexiffusion在LDMs、Stable Diffusion和DDPMs中实现了至少2倍的加速，FID退化在5%以内，超越了以前的NAS和缓存方法。特别是在Stable Diffusion上达到了5.1倍的速度提升，同时CLIP得分几乎不变。

Conclusion: Flexiffusion开创了一种资源高效的范式，用于搜索高速扩散模型而不牺牲质量。

Abstract: Diffusion models (DMs) are powerful generative models capable of producing
high-fidelity images but are constrained by high computational costs due to
iterative multi-step inference. While Neural Architecture Search (NAS) can
optimize DMs, existing methods are hindered by retraining requirements,
exponential search complexity from step-wise optimization, and slow evaluation
relying on massive image generation. To address these challenges, we propose
Flexiffusion, a training-free NAS framework that jointly optimizes generation
schedules and model architectures without modifying pre-trained parameters. Our
key insight is to decompose the generation process into flexible segments of
equal length, where each segment dynamically combines three step types: full
(complete computation), partial (cache-reused computation), and null (skipped
computation). This segment-wise search space reduces the candidate pool
exponentially compared to step-wise NAS while preserving architectural
diversity. Further, we introduce relative FID (rFID), a lightweight evaluation
metric for NAS that measures divergence from a teacher model's outputs instead
of ground truth, slashing evaluation time by over $90\%$. In practice,
Flexiffusion achieves at least $2\times$ acceleration across LDMs, Stable
Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\%$,
outperforming prior NAS and caching methods. Notably, it attains $5.1\times$
speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers
a resource-efficient paradigm for searching high-speed DMs without sacrificing
quality.

</details>


### [269] [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/abs/2506.02537)
*Hao Yan,Handong Zheng,Hao Wang,Liang Yin,Xingchen Liu,Zhenbiao Cao,Xinxing Su,Zihao Chen,Jihao Wu,Minghui Liao,Chao Weng,Wei Chen,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: Recent advancements in MLLMs have improved reasoning task performance, but Abstract Visual Reasoning (AVR) remains challenging due to abstract graphic perception limitations. This paper proposes VisuRiddles, a benchmark for AVR, and Perceptual Riddle Synthesizer (PRS), a framework for generating riddles with fine-grained perceptual descriptions, which together improve training efficacy and model interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of Abstract Visual Reasoning (AVR) in multimodal large language models (MLLMs) caused by limitations in perceiving abstract graphics.

Method: 1. Proposed VisuRiddles - A benchmark for AVR with tasks designed to assess models' reasoning capacities across five core dimensions and two high-level reasoning categories.2. Introduced Perceptual Riddle Synthesizer (PRS) - An automated framework for generating riddles with fine-grained perceptual descriptions that can be used as valuable training data for abstract graphics and provide supervision over intermediate reasoning stages.

Result: Extensive experimental results on VisuRiddles empirically validate that fine-grained visual perception is the principal bottleneck in AVR and demonstrate that the synthesis framework significantly enhances the performance of contemporary MLLMs on these challenging tasks.

Conclusion: Fine-grained visual perception is identified as the main bottleneck in AVR, and the proposed synthesis framework (PRS) effectively improves the performance of MLLMs on AVR tasks.

Abstract: Recent strides in multimodal large language models (MLLMs) have significantly
advanced their performance in many reasoning tasks. However, Abstract Visual
Reasoning (AVR) remains a critical challenge, primarily due to limitations in
perceiving abstract graphics. To tackle this issue, we investigate the
bottlenecks in current MLLMs and synthesize training data to improve their
abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,
featuring tasks meticulously constructed to assess models' reasoning capacities
across five core dimensions and two high-level reasoning categories. Second, we
introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for
generating riddles with fine-grained perceptual descriptions. PRS not only
generates valuable training data for abstract graphics but also provides
fine-grained perceptual description, crucially allowing for supervision over
intermediate reasoning stages and thereby improving both training efficacy and
model interpretability. Our extensive experimental results on VisuRiddles
empirically validate that fine-grained visual perception is the principal
bottleneck and our synthesis framework markedly enhances the performance of
contemporary MLLMs on these challenging tasks. Our code and dataset will be
released at https://github.com/yh-hust/VisuRiddles

</details>


### [270] [Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences](https://arxiv.org/abs/2506.02095)
*Hyojin Bahng,Caroline Chan,Fredo Durand,Phillip Isola*

Main category: cs.CV

TL;DR: 提出了一种利用循环一致性作为监督信号的新方法，构建了包含866K比较对的偏好数据集，训练出的奖励模型在详细描述任务上超越现有最佳对齐度量，并且在多种视觉-语言任务和文本到图像生成中提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态数据日益复杂，而现有的学习语言与视觉对齐的方法通常依赖于收集人类或AI偏好，这既昂贵又耗时。

Method: 通过使用循环一致性作为监督信号，将图像映射到文本空间再回到图像空间（或反之），计算原始输入与其重建之间的相似性，并用此循环一致性分数来排名候选对象，从而构建一个包含866K比较对的偏好数据集。

Result: 基于该数据集训练的奖励模型在详细描述任务上的表现优于现有最佳对齐度量，并且具有更好的推理时间可扩展性；此外，在DPO和扩散DPO中使用该数据集可以增强广泛的视觉-语言任务和文本到图像生成的表现。

Conclusion: 所提出的利用循环一致性构建偏好数据集的方法能够有效提高视觉-语言对齐的学习效果，并且在多个相关任务中表现出优越性能。

Abstract: Learning alignment between language and vision is a fundamental challenge,
especially as multimodal data becomes increasingly detailed and complex.
Existing methods often rely on collecting human or AI preferences, which can be
costly and time-intensive. We propose an alternative approach that leverages
cycle consistency as a supervisory signal. Given an image and generated text,
we map the text back to image space using a text-to-image model and compute the
similarity between the original image and its reconstruction. Analogously, for
text-to-image generation, we measure the textual similarity between an input
caption and its reconstruction through the cycle. We use the cycle consistency
score to rank candidates and construct a preference dataset of 866K comparison
pairs. The reward model trained on our dataset outperforms state-of-the-art
alignment metrics on detailed captioning, with superior inference-time
scalability when used as a verifier for Best-of-N sampling. Furthermore,
performing DPO and Diffusion DPO using our dataset enhances performance across
a wide range of vision-language tasks and text-to-image generation. Our
dataset, model, and code are at https://cyclereward.github.io

</details>


### [271] [Quantifying task-relevant representational similarity using decision variable correlation](https://arxiv.org/abs/2506.02164)
*Yu,Qian,Wilson S. Geisler,Xue-Xin Wei*

Main category: cs.CV

TL;DR: The study explores the similarity between brain decision strategies and deep neural network models using Decision Variable Correlation (DVC). It finds that while model-to-model similarity is high, model-to-monkey similarity is consistently lower and unaffected by adversarial training or pre-training.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental differences in decision-making strategies between biological brains and artificial neural networks trained on image classification tasks.

Method: Propose a new approach called Decision Variable Correlation (DVC) to quantify the correlation between decoded decisions of two observers (models or brains) on individual samples in a classification task. Evaluate this method using monkey V4/IT recordings and models trained on image classification tasks.

Result: Model-to-model similarity is comparable to monkey-to-monkey similarity. Model-to-monkey similarity is consistently lower and decreases with increasing ImageNet-1k performance. Adversarial training enhances robustness but does not improve model-to-monkey similarity; it increases model-to-model similarity. Pre-training on larger datasets also does not improve model-to-monkey similarity.

Conclusion: There is a fundamental divergence between the task-relevant representations in monkey V4/IT and those learned by models trained on image classification tasks.

Abstract: Previous studies have compared the brain and deep neural networks trained on
image classification. Intriguingly, while some suggest that their
representations are highly similar, others argued the opposite. Here, we
propose a new approach to characterize the similarity of the decision
strategies of two observers (models or brains) using decision variable
correlation (DVC). DVC quantifies the correlation between decoded decisions on
individual samples in a classification task and thus can capture task-relevant
information rather than general representational alignment. We evaluate this
method using monkey V4/IT recordings and models trained on image classification
tasks.
  We find that model--model similarity is comparable to monkey--monkey
similarity, whereas model--monkey similarity is consistently lower and,
surprisingly, decreases with increasing ImageNet-1k performance. While
adversarial training enhances robustness, it does not improve model--monkey
similarity in task-relevant dimensions; however, it markedly increases
model--model similarity. Similarly, pre-training on larger datasets does not
improve model--monkey similarity. These results suggest a fundamental
divergence between the task-relevant representations in monkey V4/IT and those
learned by models trained on image classification tasks.

</details>


### [272] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/abs/2506.02550)
*Qiaohui Chu,Haoyu Zhang,Yisen Feng,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

TL;DR: A three-stage framework for Ego4D Long-Term Action Anticipation task is presented, achieving first place in CVPR 2025 challenge.


<details>
  <summary>Details</summary>
Motivation: To develop an advanced method for long-term action anticipation by leveraging recent advances in foundation models.

Method: The method includes three stages: feature extraction using a high-performance visual encoder, action recognition with a Transformer and verb-noun co-occurrence matrix, and long-term action anticipation using a fine-tuned large language model.

Result: This framework won first place in the Ego4D Long-Term Action Anticipation challenge at CVPR 2025, setting a new state-of-the-art in long-term action prediction.

Conclusion: The proposed three-stage framework effectively enhances long-term action anticipation, as evidenced by its top performance in the CVPR 2025 challenge.

Abstract: In this report, we present a novel three-stage framework developed for the
Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in
foundation models, our method consists of three stages: feature extraction,
action recognition, and long-term action anticipation. First, visual features
are extracted using a high-performance visual encoder. The features are then
fed into a Transformer to predict verbs and nouns, with a verb-noun
co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the
predicted verb-noun pairs are formatted as textual prompts and input into a
fine-tuned large language model (LLM) to anticipate future action sequences.
Our framework achieves first place in this challenge at CVPR 2025, establishing
a new state-of-the-art in long-term action prediction. Our code will be
released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

</details>


### [273] [Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment](https://arxiv.org/abs/2506.02221)
*Johannes Schusterbauer,Ming Gui,Frank Fundel,Björn Ommer*

Main category: cs.CV

TL;DR: Diff2Flow是一个将扩散模型知识高效转移到流匹配（FM）的新框架，通过时间步长重缩放、插值对齐和从扩散预测中推导FM兼容的速度场，实现无额外计算开销的FM微调。实验表明，Diff2Flow在参数高效约束下优于朴素FM和扩散微调方法，并在各种下游任务中表现出优越或竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管流匹配（FM）相比扩散模型具有更快的推理速度和经验性能优势，但当前基础FM模型的微调计算成本过高，而扩散模型（如Stable Diffusion）则受益于高效的架构和生态系统支持。因此，如何高效地将预训练扩散模型的知识转移到FM成为一个关键挑战。

Method: 提出了一种名为Diff2Flow的新型框架，该框架通过以下方式系统地连接扩散模型和FM范式：1) 时间步长重缩放；2) 插值对齐；3) 从扩散预测中推导FM兼容的速度场。这些步骤使得可以直接且高效地对扩散先验进行FM微调，而无需额外的计算开销。

Result: 实验结果表明，在参数高效约束下，Diff2Flow的表现优于朴素FM和扩散微调方法。此外，与最先进的方法相比，Diff2Flow在各种下游任务中实现了优越或竞争力的性能。

Conclusion: Diff2Flow提供了一种有效的方法来将预训练扩散模型的知识转移到流匹配模型，从而在保持高性能的同时降低计算成本。这为未来的研究和应用提供了新的可能性，特别是在需要快速推理和高效参数使用的场景中。

Abstract: Diffusion models have revolutionized generative tasks through high-fidelity
outputs, yet flow matching (FM) offers faster inference and empirical
performance gains. However, current foundation FM models are computationally
prohibitive for finetuning, while diffusion models like Stable Diffusion
benefit from efficient architectures and ecosystem support. This work addresses
the critical challenge of efficiently transferring knowledge from pre-trained
diffusion models to flow matching. We propose Diff2Flow, a novel framework that
systematically bridges diffusion and FM paradigms by rescaling timesteps,
aligning interpolants, and deriving FM-compatible velocity fields from
diffusion predictions. This alignment enables direct and efficient FM
finetuning of diffusion priors with no extra computation overhead. Our
experiments demonstrate that Diff2Flow outperforms na\"ive FM and diffusion
finetuning particularly under parameter-efficient constraints, while achieving
superior or competitive performance across diverse downstream tasks compared to
state-of-the-art methods. We will release our code at
https://github.com/CompVis/diff2flow.

</details>


### [274] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/abs/2506.02614)
*Guohang Zhuang,Weixi Song,Jinyang Huang,Chenwei Yang,Yan Lu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的空间碎片跟踪网络（SDT-Net），并生成了大规模数据集SDTD，通过实验验证了模型的有效性和数据集的挑战性，同时在南极站真实数据测试中表现出良好的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 随着空间探索的快速发展，空间碎片因其潜在的巨大威胁而受到更多关注，传统的信号处理方法无法有效应对复杂的背景和密集的空间碎片，因此需要更实时和精确的碎片跟踪方法。

Method: 提出了基于深度学习的空间碎片跟踪网络（SDT-Net），该网络能够有效地表示碎片特征，提高端到端模型学习的效率和稳定性；还生成了一个大规模数据集Space Debris Tracking Dataset (SDTD)，包含18,040个视频序列，总计62,562帧，覆盖250,000个合成空间碎片。

Result: 广泛的实验验证了模型的有效性和数据集的挑战性；在南极站的真实数据测试中，模型取得了70.6%的MOTA分数，展示了其在实际场景中的强大可迁移性。

Conclusion: 所提出的SDT-Net在空间碎片跟踪方面表现出了高精度，并且生成的SDTD数据集为模型训练和评估提供了有力支持，未来将公开数据集和代码。

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [275] [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/abs/2506.02615)
*Safaa Abdullahi Moallim Mohamud,Minjin Baek,Dong Seog Han*

Main category: cs.CV

TL;DR: This paper presents a hierarchical QA method for scene understanding in autonomous vehicles, which optimizes cost-efficiency and visual interpretation. It fine-tunes a compact VLM on a custom dataset and uses dynamic question skipping to reduce inference time, showing competitiveness with GPT-4o while achieving lower latency.


<details>
  <summary>Details</summary>
Motivation: To balance cost-efficiency with detailed visual interpretation for scene understanding in autonomous vehicles.

Method: Fine-tune a compact vision-language model (VLM) on a custom dataset specific to the vehicle's operating area. Use a hierarchical QA strategy that decomposes scene understanding into high-level and detailed sub-questions, dynamically skipping questions based on previous answers to minimize computational overhead. Synthesize extracted answers using handcrafted templates for coherent scene descriptions.

Result: Competitive with state-of-the-art methods like GPT-4o in capturing key scene details, but with significantly lower inference time. Qualitative real-time deployment results show the ability to capture key driving elements with minimal latency.

Conclusion: The proposed hierarchical QA approach effectively balances cost-efficiency and detailed visual interpretation for scene understanding in autonomous vehicles.

Abstract: In this paper, we present a hierarchical question-answering (QA) approach for
scene understanding in autonomous vehicles, balancing cost-efficiency with
detailed visual interpretation. The method fine-tunes a compact vision-language
model (VLM) on a custom dataset specific to the geographical area in which the
vehicle operates to capture key driving-related visual elements. At the
inference stage, the hierarchical QA strategy decomposes the scene
understanding task into high-level and detailed sub-questions. Instead of
generating lengthy descriptions, the VLM navigates a structured question tree,
where answering high-level questions (e.g., "Is it possible for the ego vehicle
to turn left at the intersection?") triggers more detailed sub-questions (e.g.,
"Is there a vehicle approaching the intersection from the opposite
direction?"). To optimize inference time, questions are dynamically skipped
based on previous answers, minimizing computational overhead. The extracted
answers are then synthesized using handcrafted templates to ensure coherent,
contextually accurate scene descriptions. We evaluate the proposed approach on
the custom dataset using GPT reference-free scoring, demonstrating its
competitiveness with state-of-the-art methods like GPT-4o in capturing key
scene details while achieving significantly lower inference time. Moreover,
qualitative results from real-time deployment highlight the proposed approach's
capacity to capture key driving elements with minimal latency.

</details>


### [276] [Approximate Borderline Sampling using Granular-Ball for Classification Tasks](https://arxiv.org/abs/2506.02366)
*Qin Xie,Qinghua Zhang,Shuyin Xia*

Main category: cs.CV

TL;DR: 提出了一种基于粒球的近似边界采样方法（GBABS），用于分类任务，包括受限制扩散的粒球生成方法（RD-GBG）和GBABS方法本身。实验结果表明，该方法在处理类别噪声数据集方面表现出色，并优于现有的基于粒球的采样方法和其他代表性采样方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于粒球（GB）的采样方法存在一些局限性，例如缺乏边界线采样策略以及由于GB重叠导致的类边界模糊或收缩问题。为了解决这些问题，需要一种新的采样方法来提高分类任务中的效率和鲁棒性。

Method: 1. 提出了受限制扩散的粒球生成方法（RD-GBG），通过约束扩展防止GB重叠，保留精确的几何表示。
2. 基于异构最近邻的概念，提出了粒球基础的近似边界采样方法（GBABS），这是第一个能够同时进行边界采样和改善类别噪声数据集质量的通用采样方法。

Result: 实验结果表明，所提出的方法在分类任务中优于现有的基于粒球的采样方法以及其他几种代表性采样方法，特别是在处理类别噪声数据集时表现突出。

Conclusion: 提出的基于粒球的近似边界采样方法（GBABS）及其相关技术（如RD-GBG）在解决现有方法的局限性方面取得了成功，特别适用于类别噪声数据集，无需优化纯度阈值即可实现优秀性能。

Abstract: Data sampling enhances classifier efficiency and robustness through data
compression and quality improvement. Recently, the sampling method based on
granular-ball (GB) has shown promising performance in generality and noisy
classification tasks. However, some limitations remain, including the absence
of borderline sampling strategies and issues with class boundary blurring or
shrinking due to overlap between GBs. In this paper, an approximate borderline
sampling method using GBs is proposed for classification tasks. First, a
restricted diffusion-based GB generation (RD-GBG) method is proposed, which
prevents GB overlaps by constrained expansion, preserving precise geometric
representation of GBs via redefined ones. Second, based on the concept of
heterogeneous nearest neighbor, a GB-based approximate borderline sampling
(GBABS) method is proposed, which is the first general sampling method capable
of both borderline sampling and improving the quality of class noise datasets.
Additionally, since RD-GBG incorporates noise detection and GBABS focuses on
borderline samples, GBABS performs outstandingly on class noise datasets
without the need for an optimal purity threshold. Experimental results
demonstrate that the proposed methods outperform the GB-based sampling method
and several representative sampling methods. Our source code is publicly
available at https://github.com/CherylTse/GBABS.

</details>


### [277] [Multi-level and Multi-modal Action Anticipation](https://arxiv.org/abs/2506.02382)
*Seulgi Kim,Ghazal Kaviani,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

TL;DR: The paper presents m&m-Ant, a multi-modal action anticipation approach combining visual and textual cues with hierarchical semantic info, plus a fine-grained label generator and temporal consistency loss, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Current action anticipation methods mainly focus on visual modalities and lack integration of multiple information sources. Human behavior suggests leveraging diverse cues for better predictions.

Method: Introduced m&m-Ant which integrates visual and textual data while modeling hierarchical semantics. Also proposed a fine-grained label generator with a specialized temporal consistency loss function.

Result: Experiments on Breakfast, 50 Salads, and DARai datasets showed an average anticipation accuracy improvement of 3.08% over existing methods.

Conclusion: Multi-modal and hierarchical modeling significantly improves action anticipation, setting a new benchmark for future research.

Abstract: Action anticipation, the task of predicting future actions from partially
observed videos, is crucial for advancing intelligent systems. Unlike action
recognition, which operates on fully observed videos, action anticipation must
handle incomplete information. Hence, it requires temporal reasoning, and
inherent uncertainty handling. While recent advances have been made,
traditional methods often focus solely on visual modalities, neglecting the
potential of integrating multiple sources of information. Drawing inspiration
from human behavior, we introduce \textit{Multi-level and Multi-modal Action
Anticipation (m\&m-Ant)}, a novel multi-modal action anticipation approach that
combines both visual and textual cues, while explicitly modeling hierarchical
semantic information for more accurate predictions. To address the challenge of
inaccurate coarse action labels, we propose a fine-grained label generator
paired with a specialized temporal consistency loss function to optimize
performance. Extensive experiments on widely used datasets, including
Breakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,
achieving state-of-the-art results with an average anticipation accuracy
improvement of 3.08\% over existing methods. This work underscores the
potential of multi-modal and hierarchical modeling in advancing action
anticipation and establishes a new benchmark for future research in the field.
Our code is available at: https://github.com/olivesgatech/mM-ant.

</details>


### [278] [Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2506.02677)
*Jintao Tong,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

TL;DR: The paper addresses the entanglement problem in Cross-Domain Few-Shot Segmentation (CD-FSS) by decomposing ViT components, learning disentangled features, and re-composing them for improved generalization and finetuning.


<details>
  <summary>Details</summary>
Motivation: Existing methods for CD-FSS have an entanglement problem that binds source-domain patterns together, making them hard to transfer.

Method: The authors decompose the ViT structure to interpret the entanglement problem and propose a method to address it by learning to weigh all comparisons of ViT components, thus learning disentangled features and re-composing them for the CD-FSS task.

Result: Experiments demonstrate that the model surpasses state-of-the-art CD-FSS methods with improvements of 1.92% and 1.88% in average accuracy under 1-shot and 5-shot settings respectively.

Conclusion: The proposed approach effectively resolves the entanglement issue, leading to better generalization and fine-tuning capabilities.

Abstract: Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a
source-domain dataset to unseen target-domain datasets with limited
annotations. Current methods typically compare the distance between training
and testing samples for mask prediction. However, we find an entanglement
problem exists in this widely adopted method, which tends to bind sourcedomain
patterns together and make each of them hard to transfer. In this paper, we aim
to address this problem for the CD-FSS task. We first find a natural
decomposition of the ViT structure, based on which we delve into the
entanglement problem for an interpretation. We find the decomposed ViT
components are crossly compared between images in distance calculation, where
the rational comparisons are entangled with those meaningless ones by their
equal importance, leading to the entanglement problem. Based on this
interpretation, we further propose to address the entanglement problem by
learning to weigh for all comparisons of ViT components, which learn
disentangled features and re-compose them for the CD-FSS task, benefiting both
the generalization and finetuning. Experiments show that our model outperforms
the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under
1-shot and 5-shot settings, respectively.

</details>


### [279] [LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering](https://arxiv.org/abs/2506.02733)
*Xiaoyi Feng,Kaifeng Zou,Caichun Cen,Tao Huang,Hui Guo,Zizhou Huang,Yingli Zhao,Mingqing Zhang,Diwei Wang,Yuntao Zou,Dagang Li*

Main category: cs.CV

TL;DR: The paper presents LinkTo-Anime, the first high-quality dataset designed for cel anime character motion with rich annotations and a comprehensive benchmark.


<details>
  <summary>Details</summary>
Motivation: There is a lack of datasets tailored to Celluloid(cel) anime character motion despite existing optical flow datasets focusing on real-world simulation or synthetic human motion.

Method: Introduced LinkTo-Anime dataset which includes forward and backward optical flow, occlusion masks, and Mixamo Skeleton annotations generated with 3D model rendering. It contains 395 video sequences, 24,230 training frames, 720 validation frames, and 4,320 test frames.

Result: LinkTo-Anime facilitates research in optical flow estimation and downstream tasks like anime video generation and line drawing colorization. A benchmark was constructed to analyze shortcomings and limitations of various optical flow estimation methods across multiple datasets.

Conclusion: LinkTo-Anime is the first specialized dataset for cel anime character motion that provides rich annotations and helps in analyzing the limitations of optical flow estimation methods.

Abstract: Existing optical flow datasets focus primarily on real-world simulation or
synthetic human motion, but few are tailored to Celluloid(cel) anime character
motion: a domain with unique visual and motion characteristics. To bridge this
gap and facilitate research in optical flow estimation and downstream tasks
such as anime video generation and line drawing colorization, we introduce
LinkTo-Anime, the first high-quality dataset specifically designed for cel
anime character motion generated with 3D model rendering. LinkTo-Anime provides
rich annotations including forward and backward optical flow, occlusion masks,
and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230
training frames, 720 validation frames, and 4,320 test frames. Furthermore, a
comprehensive benchmark is constructed with various optical flow estimation
methods to analyze the shortcomings and limitations across multiple datasets.

</details>


### [280] [Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations](https://arxiv.org/abs/2506.02764)
*Fatma Youssef Mohammed,Kostas Alexis*

Main category: cs.CV

TL;DR: This paper proposes a neural network architecture based on the Human Attention transformer (HAT) to explore whether free-viewing and visual search can share a common representation. The model shows efficient knowledge transfer with only a 3.86% performance drop and significant reduction in computational costs.


<details>
  <summary>Details</summary>
Motivation: To investigate whether a common representation exists between free-viewing and task-specific settings in computational human attention modeling.

Method: Proposing a neural network architecture that builds upon the Human Attention transformer (HAT) to test the hypothesis of shared representation between free-viewing and visual search.

Result: Free-viewing and visual search can efficiently share a common representation, with a performance drop of only 3.86% in predicted fixation scanpaths as measured by the semantic sequence score (SemSS).

Conclusion: The proposed model reduces computational costs significantly and demonstrates that knowledge from free-viewing attention can be transferred to task-driven visual search.

Abstract: Computational human attention modeling in free-viewing and task-specific
settings is often studied separately, with limited exploration of whether a
common representation exists between them. This work investigates this question
and proposes a neural network architecture that builds upon the Human Attention
transformer (HAT) to test the hypothesis. Our results demonstrate that
free-viewing and visual search can efficiently share a common representation,
allowing a model trained in free-viewing attention to transfer its knowledge to
task-driven visual search with a performance drop of only 3.86% in the
predicted fixation scanpaths, measured by the semantic sequence score (SemSS)
metric which reflects the similarity between predicted and human scanpaths.
This transfer reduces computational costs by 92.29% in terms of GFLOPs and
31.23% in terms of trainable parameters.

</details>


### [281] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/abs/2506.02896)
*Adam Pardyl,Dominik Matuszek,Mateusz Przebieracz,Marek Cygan,Bartosz Zieliński,Maciej Wołczyk*

Main category: cs.CV

TL;DR: This paper introduces FlySearch, a 3D outdoor environment for testing Vision-Language Models (VLMs) in complex object search and navigation tasks. It reveals that state-of-the-art VLMs struggle with even simple exploration tasks compared to human performance, identifies central causes of these failures, and shows some improvements through finetuning.


<details>
  <summary>Details</summary>
Motivation: To determine whether Vision-Language Models (VLMs) can effectively operate in messy and unstructured real-world conditions, requiring active, goal-driven exploration.

Method: Introduced FlySearch, a photorealistic 3D environment with three sets of scenarios of varying difficulty to test VLMs' abilities in searching and navigating to objects in complex scenes.

Result: State-of-the-art VLMs cannot reliably solve even the simplest exploration tasks, with greater disparity from human performance as task difficulty increases. Central causes include vision hallucination, context misunderstanding, and task planning failures.

Conclusion: FlySearch demonstrates current limitations of VLMs in complex exploration tasks and provides a benchmark for future research; some issues can be mitigated by finetuning.

Abstract: The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.

</details>


### [282] [FORLA:Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/abs/2506.02964)
*Guiqiu Liao,Matjaz Jogan,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: 提出了一种新的框架FORLA，用于在联邦学习环境中进行对象中心表示学习和特征适应，利用无监督的槽注意力机制，在多个真实世界数据集上的实验表明该框架不仅在对象发现方面优于集中式基线，而且还能学习到紧凑且通用的表示，具有良好的跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，跨异构未标记数据集学习高效的视觉表示是一项核心挑战，需要在没有监督的情况下，提取对所有客户端都有信息量的特征，并分离特定于领域的因素。

Method: 引入了FORLA框架，其核心是一个共享的特征适配器和一个共享的槽注意力模块。通过两分支的学生-教师架构优化适配器，其中学生解码器学习从基础模型重建完整特征，而教师解码器重建适应后的低维特征。槽注意力模块通过对齐跨客户端的对象级表示来实现跨域学习。

Result: 实验证明，该框架在对象发现任务上优于集中式基线，并能学习到紧凑、通用的表示，具有良好的跨域泛化能力。

Conclusion: 联邦槽注意力是一种有效的工具，适用于从跨域数据中进行可扩展的、无监督的视觉表示学习，能够处理分布式概念的数据。

Abstract: Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.

</details>


### [283] [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/abs/2506.02975)
*Yicheng Xiao,Lin Song,Rui Yang,Cheng Cheng,Zunnan Xu,Zhaoyang Zhang,Yixiao Ge,Xiu Li,Ying Shan*

Main category: cs.CV

TL;DR: This paper proposes HaploOmni, a single multimodal transformer with multimodal warmup strategy, feature pre-scaling and multimodal AdaLN techniques to address cross-modal compatibility challenges. It achieves competitive performance on various benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore an efficient training paradigm for building a single transformer model that can handle unified multimodal understanding and generation.

Method: The method involves proposing a multimodal warmup strategy using prior knowledge, introducing feature pre-scaling and multimodal AdaLN techniques to tackle cross-modal compatibility challenges, and integrating these into the HaploOmni transformer.

Result: HaploOmni achieves competitive performance across multiple image and video understanding and generation benchmarks with limited training costs.

Conclusion: The authors conclude by presenting HaploOmni, which effectively addresses cross-modal compatibility issues and performs competitively against advanced unified models.

Abstract: With the advancement of language models, unified multimodal understanding and
generation have made significant strides, with model architectures evolving
from separated components to unified single-model frameworks. This paper
explores an efficient training paradigm to build a single transformer for
unified multimodal understanding and generation. Specifically, we propose a
multimodal warmup strategy utilizing prior knowledge to extend capabilities. To
address cross-modal compatibility challenges, we introduce feature pre-scaling
and multimodal AdaLN techniques. Integrating the proposed technologies, we
present the HaploOmni, a new single multimodal transformer. With limited
training costs, HaploOmni achieves competitive performance across multiple
image and video understanding and generation benchmarks over advanced unified
models. All codes will be made public at https://github.com/Tencent/HaploVLM.

</details>


### [284] [Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge](https://arxiv.org/abs/2506.02976)
*Rachid Zeghlache,Ikram Brahim,Pierre-Henri Conze,Mathieu Lamard,Mohammed El Amine Lazouni,Zineb Aziza Elaouaber,Leila Ryma Lazouni,Christopher Nielsen,Ahmad O. Ahsan,Matthias Wilms,Nils D. Forkert,Lovre Antonio Budimir,Ivana Matovinović,Donik Vršnak,Sven Lončarić,Philippe Zhang,Weili Jiang,Yihao Li,Yiding Hao,Markus Frohmann,Patrick Binder,Marcel Huber,Taha Emre,Teresa Finisterra Araújo,Marzieh Oghbaie,Hrvoje Bogunović,Amerens A. Bekkers,Nina M. van Liebergen,Hugo J. Kuijf,Abdul Qayyum,Moona Mazher,Steven A. Niederer,Alberto J. Beltrán-Carrero,Juan J. Gómez-Valverde,Javier Torresano-Rodríquez,Álvaro Caballero-Sastre,María J. Ledesma Carbayo,Yosuke Yamagishi,Yi Ding,Robin Peretzke,Alexandra Ertl,Maximilian Fischer,Jessica Kächele,Sofiane Zehar,Karim Boukli Hacene,Thomas Monfort,Béatrice Cochener,Mostafa El Habib Daho,Anas-Alexis Benyoussef,Gwenolé Quellec*

Main category: cs.CV

TL;DR: The MARIO challenge at MICCAI 2024 focused on using AI to detect and monitor age-related macular degeneration (AMD) via OCT images. The challenge included two tasks: classifying evolution between consecutive OCT scans and predicting future AMD progression. AI performed as well as physicians in Task 1 but not in Task 2.


<details>
  <summary>Details</summary>
Motivation: To advance the automated detection and monitoring of age-related macular degeneration (AMD) by analyzing optical coherence tomography (OCT) images, improving upon current clinical methods.

Method: The challenge used a primary dataset from Brest, France, and an auxiliary dataset from Algeria for training/testing models and evaluating population/device shifts. It involved two tasks: classification of evolution between OCT scans and prediction of future AMD evolution over three months.

Result: AI solutions performed equivalently to physicians in detecting changes in neovascular activity (Task 1) but were not yet capable of accurately predicting future AMD evolution (Task 2).

Conclusion: The MARIO challenge set a benchmark for AMD monitoring using multi-modal data including OCT, infrared imaging, and clinical data. While AI shows promise in measuring AMD progression, further advancements are needed for accurate future predictions.

Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated
detection and monitoring of age-related macular degeneration (AMD) through the
analysis of optical coherence tomography (OCT) images. Designed to evaluate
algorithmic performance in detecting neovascular activity changes within AMD,
the challenge incorporated unique multi-modal datasets. The primary dataset,
sourced from Brest, France, was used by participating teams to train and test
their models. The final ranking was determined based on performance on this
dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate
population and device shifts from submitted solutions. Two tasks were involved
in the MARIO challenge. The first one was the classification of evolution
between two consecutive 2D OCT B-scans. The second one was the prediction of
future AMD evolution over three months for patients undergoing anti-vascular
endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with
the top 12 finalists presenting their methods. This paper outlines the
challenge's structure, tasks, data characteristics, and winning methodologies,
setting a benchmark for AMD monitoring using OCT, infrared imaging, and
clinical data (such as the number of visits, age, gender, etc.). The results of
this challenge indicate that artificial intelligence (AI) performs as well as a
physician in measuring AMD progression (Task 1) but is not yet able of
predicting future evolution (Task 2).

</details>


### [285] [Smartflow: Enabling Scalable Spatiotemporal Geospatial Research](https://arxiv.org/abs/2506.03022)
*David McVicar,Brian Avant,Adrian Gould,Diego Torrejon,Charles Della Porta,Ryan Mukherjee*

Main category: cs.CV

TL;DR: Smartflow is a cloud-based framework for scalable geospatial research, using STAC-compliant catalogs as input and Kubernetes for scalability. It supports model experimentation with tools like ClearML and Tensorboard. A novel neural architecture built with Smartflow can detect heavy construction in large geographic areas based on the SMART program data.


<details>
  <summary>Details</summary>
Motivation: To enable scalable spatiotemporal geospatial research and model development over large geographic areas, time scales, and image archives.

Method: Using a cloud-based framework (Smartflow) that processes heterogeneous geospatial data into standardized datacubes, manages model experimentation with various tools, and uses Kubernetes for scalability.

Result: A novel neural architecture was developed that can successfully detect heavy construction throughout all major phases of development based on IARPA SMART program data.

Conclusion: Smartflow is well-suited for geospatial model development and analysis over large geographic areas, time scales, and expansive image archives.

Abstract: BlackSky introduces Smartflow, a cloud-based framework enabling scalable
spatiotemporal geospatial research built on open-source tools and technologies.
Using STAC-compliant catalogs as a common input, heterogeneous geospatial data
can be processed into standardized datacubes for analysis and model training.
Model experimentation is managed using a combination of tools, including
ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is
Kubernetes, which orchestrates the provisioning and execution of workflows to
support both horizontal and vertical scalability. This combination of features
makes Smartflow well-suited for geospatial model development and analysis over
large geographic areas, time scales, and expansive image archives.
  We also present a novel neural architecture, built using Smartflow, to
monitor large geographic areas for heavy construction. Qualitative results
based on data from the IARPA Space-based Machine Automated Recognition
Technique (SMART) program are presented that show the model is capable of
detecting heavy construction throughout all major phases of development.

</details>


### [286] [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](https://arxiv.org/abs/2506.03065)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Peng Ye,Mingzhu Shen,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: 在视频生成任务中，Diffusion Transformers (DiTs) 虽然取得了突破，但受限于注意力机制的二次复杂度导致推理延迟显著。通过分析Video Diffusion Transformer (vDiT) 的注意力图，发现三种重复的稀疏性模式：对角线、多对角线和垂直条纹结构，并提出Sparse-vDiT框架以减少计算量并加速推理，同时保持高视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 尽管Diffusion Transformers (DiTs) 在视频生成方面取得了进展，但由于注意力机制的二次复杂度，长序列生成任务仍存在显著的推理延迟问题。

Method: 1. 识别出vDiT中的三种稀疏性模式：对角线、多对角线和垂直条纹结构，并发现这些模式与层深度和头位置有强相关性，但对输入内容依赖有限。2. 提出Sparse-vDiT框架，包括模式优化的稀疏内核以替代密集注意力计算，以及离线稀疏扩散搜索算法以选择最优稀疏计算策略。3. 将具有相同注意策略的头在同一层内进行融合以提升推理效率。

Result: 在CogVideoX1.5、HunyuanVideo和Wan2.1等最先进的vDiT模型中，Sparse-vDiT分别实现了2.09倍、2.38倍和1.67倍的理论FLOP减少，实际推理速度分别提升了1.76倍、1.85倍和1.58倍，同时保持了较高的视觉保真度（PSNR值分别为24.13、27.09和22.59）。

Conclusion: 本研究证明了vDiTs中的潜在结构稀疏性可以被系统地利用，用于高效的长视频合成。

Abstract: While Diffusion Transformers (DiTs) have achieved breakthroughs in video
generation, this long sequence generation task remains constrained by the
quadratic complexity of attention mechanisms, resulting in significant
inference latency. Through detailed analysis of attention maps in Video
Diffusion Transformer (vDiT), we identify three recurring sparsity patterns:
diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\%
attention heads can be skipped. Crucially, these patterns exhibit strong
layer-depth and head-position correlations but show limited dependence on the
input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity
acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels
that replace dense attention with computationally efficient implementations for
each identified sparsity pattern. 2) An offline sparse diffusion search
algorithm that selects the optimal sparse computation strategy per layer and
head via hardware-aware cost modeling. After determining the optimal
configuration, we fuse heads within the same layer that share the same
attention strategy, enhancing inference efficiency. Integrated into
state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),
Sparse-vDiT achieves 2.09$\times$, 2.38$\times$, and 1.67$\times$ theoretical
FLOP reduction, and actual inference speedups of 1.76$\times$, 1.85$\times$,
and 1.58$\times$, respectively, while maintaining high visual fidelity, with
PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent
structural sparsity in vDiTs can be systematically exploited for long video
synthesis.

</details>


### [287] [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096)
*Christian Schlarmann,Francesco Croce,Nicolas Flammarion,Matthias Hein*

Main category: cs.CV

TL;DR: FuseLIP is an alternative architecture for multimodal embedding that uses a single transformer model to operate on an extended vocabulary of text and image tokens, allowing different modalities to interact at each depth of encoding.


<details>
  <summary>Details</summary>
Motivation: Current contrastive language-image pre-training methods cannot natively handle multimodal inputs, requiring additional modules to merge features extracted by unimodal encoders.

Method: The FuseLIP architecture leverages discrete image tokenizers and uses a single transformer model operating on an extended vocabulary of text and image tokens for early fusion of modalities.

Result: FuseLIP outperforms other approaches in multimodal embedding tasks like VQA and text-guided image transformation retrieval, while performing comparably to baselines on unimodal tasks.

Conclusion: FuseLIP presents a successful alternative for multimodal embedding with improved performance in multimodal tasks.

Abstract: Contrastive language-image pre-training aligns the features of text-image
pairs in a common latent space via distinct encoders for each modality. While
this approach achieves impressive performance in several zero-shot tasks, it
cannot natively handle multimodal inputs, i.e., encoding image and text into a
single feature vector. As a remedy, it is common practice to use additional
modules to merge the features extracted by the unimodal encoders. In this work,
we present FuseLIP, an alternative architecture for multimodal embedding.
Leveraging recent progress in discrete image tokenizers, we propose to use a
single transformer model which operates on an extended vocabulary of text and
image tokens. This early fusion approach allows the different modalities to
interact at each depth of encoding and obtain richer representations compared
to common late fusion. We collect new datasets for multimodal pre-training and
evaluation, designing challenging tasks for multimodal encoder models. We show
that FuseLIP outperforms other approaches in multimodal embedding tasks such as
VQA and text-guided image transformation retrieval, while being comparable to
baselines on unimodal tasks.

</details>


### [288] [Native-Resolution Image Synthesis](https://arxiv.org/abs/2506.03131)
*Zidong Wang,Lei Bai,Xiangyu Yue,Wanli Ouyang,Yiyuan Zhang*

Main category: cs.CV

TL;DR: The paper presents Native-resolution diffusion Transformer (NiT), which can generate images at arbitrary resolutions and aspect ratios. It surpasses fixed-resolution methods by handling variable-length visual tokens, achieving state-of-the-art performance on ImageNet benchmarks and showing strong zero-shot generalization.


<details>
  <summary>Details</summary>
Motivation: Current generative models are limited to fixed resolutions and square images, restricting their flexibility and applicability in diverse scenarios.

Method: The authors introduce the Native-resolution diffusion Transformer (NiT) that uses variable-length visual tokens to model varying resolutions and aspect ratios within its denoising process, learning intrinsic visual distributions from a broad range of image formats.

Result: NiT achieves state-of-the-art performance on both ImageNet-256x256 and 512x512 benchmarks. It also demonstrates excellent zero-shot generalization for generating high-fidelity images at previously unseen high resolutions and diverse aspect ratios.

Conclusion: Native-resolution modeling has significant potential to bridge visual generative modeling with advanced large language model methodologies.

Abstract: We introduce native-resolution image synthesis, a novel generative modeling
paradigm that enables the synthesis of images at arbitrary resolutions and
aspect ratios. This approach overcomes the limitations of conventional
fixed-resolution, square-image methods by natively handling variable-length
visual tokens, a core challenge for traditional techniques. To this end, we
introduce the Native-resolution diffusion Transformer (NiT), an architecture
designed to explicitly model varying resolutions and aspect ratios within its
denoising process. Free from the constraints of fixed formats, NiT learns
intrinsic visual distributions from images spanning a broad range of
resolutions and aspect ratios. Notably, a single NiT model simultaneously
achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512
benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in
advanced large language models, NiT, trained solely on ImageNet, demonstrates
excellent zero-shot generalization performance. It successfully generates
high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)
and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These
findings indicate the significant potential of native-resolution modeling as a
bridge between visual generative modeling and advanced LLM methodologies.

</details>


### [289] [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097)
*Ashwin Vinod,Shrey Pandit,Aditya Vavre,Linshen Liu*

Main category: cs.CV

TL;DR: The paper introduces EgoVLM, a vision-language model for egocentric video reasoning, which outperforms general models on specific benchmarks.


<details>
  <summary>Details</summary>
Motivation: There is a need for robust reasoning from first person video streams in emerging AI applications like wearable cameras and autonomous agents.

Method: EgoVLM is fine-tuned via Group Relative Policy Optimization (GRPO), a reinforcement learning method, without any supervised fine-tuning phase. A novel keyframe-based reward is introduced to guide the optimization.

Result: EgoVLM-3B outperforms Qwen2.5-VL 3B and 7B models by 14.33 and 13.87 accuracy points respectively on the EgoSchema benchmark.

Conclusion: Domain-specific training improves performance over general-purpose VLMs, and the keyframe-based reward offers potential for future exploration in egocentric reasoning.

Abstract: Emerging embodied AI applications, such as wearable cameras and autonomous
agents, have underscored the need for robust reasoning from first person video
streams. We introduce EgoVLM, a vision-language model specifically designed to
integrate visual comprehension and spatial-temporal reasoning within egocentric
video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization
(GRPO), a reinforcement learning method adapted to align model outputs with
human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly
tune using RL without any supervised fine-tuning phase on chain-of-thought
(CoT) data. We evaluate EgoVLM on egocentric video question answering
benchmarks and show that domain-specific training substantially improves
performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on
non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by
14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By
explicitly generating reasoning traces, EgoVLM enhances interpretability,
making it well-suited for downstream applications. Furthermore, we introduce a
novel keyframe-based reward that incorporates salient frame selection to guide
reinforcement learning optimization. This reward formulation opens a promising
avenue for future exploration in temporally grounded egocentric reasoning.

</details>


### [290] [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](https://arxiv.org/abs/2506.03150)
*Yuanze Lin,Yi-Wen Chen,Yi-Hsuan Tsai,Ronald Clark,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种名为IllumiCraft的端到端扩散框架，通过整合高动态范围视频地图、合成重光照帧和3D点迹三种输入，生成与用户定义提示一致的时序连贯视频，支持背景和文本条件下的视频重光照，效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的模型可以从文本或图像输入生成高质量和高分辨率的视频序列，但在控制场景光照和视觉外观方面缺乏明确的几何线索集成。

Method: IllumiCraft接受三种互补输入：高动态范围（HDR）视频地图以实现详细照明控制；具有随机照明变化的合成重光照帧（可选配静态背景参考图像）以提供外观线索；以及捕捉精确3D几何信息的3D点迹。这些线索在统一的扩散架构中进行整合。

Result: IllumiCraft生成与时序连贯的视频，与用户定义的提示一致，并支持背景条件和文本条件下的视频重光照，比现有的可控视频生成方法提供了更好的保真度。

Conclusion: IllumiCraft通过整合光照、外观和几何线索，在统一的扩散架构中生成高质量、时序连贯的视频，支持多种条件下的视频重光照，提升了生成视频的保真度。

Abstract: Although diffusion-based models can generate high-quality and high-resolution
video sequences from textual or image inputs, they lack explicit integration of
geometric cues when controlling scene lighting and visual appearance across
frames. To address this limitation, we propose IllumiCraft, an end-to-end
diffusion framework accepting three complementary inputs: (1)
high-dynamic-range (HDR) video maps for detailed lighting control; (2)
synthetically relit frames with randomized illumination changes (optionally
paired with a static background reference image) to provide appearance cues;
and (3) 3D point tracks that capture precise 3D geometry information. By
integrating the lighting, appearance, and geometry cues within a unified
diffusion architecture, IllumiCraft generates temporally coherent videos
aligned with user-defined prompts. It supports background-conditioned and
text-conditioned video relighting and provides better fidelity than existing
controllable video generation methods. Project Page:
https://yuanze-lin.me/IllumiCraft_page

</details>


### [291] [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
*Mengdi Jia,Zekun Qi,Shaochen Zhang,Wenyao Zhang,Xinqiang Yu,Jiawei He,He Wang,Li Yi*

Main category: cs.CV

TL;DR: OmniSpatial is a new benchmark for spatial reasoning that reveals significant limitations in current VLMs' comprehensive spatial understanding.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models have difficulty with advanced spatial reasoning beyond basic tasks like distinguishing left from right or near from far.

Method: Introduced OmniSpatial, a benchmark covering dynamic reasoning, complex spatial logic, spatial interaction, and perspective-taking with 50 subcategories and over 1.5K question-answer pairs.

Result: Experiments show major VLMs and spatial understanding models have significant limitations in comprehensive spatial reasoning.

Conclusion: OmniSpatial highlights the need for future research to address these limitations.

Abstract: Spatial reasoning is a key aspect of cognitive psychology and remains a major
bottleneck for current vision-language models (VLMs). While extensive research
has aimed to evaluate or improve VLMs' understanding of basic spatial
relations, such as distinguishing left from right, near from far, and object
counting, these tasks represent only the most fundamental level of spatial
reasoning. In this work, we introduce OmniSpatial, a comprehensive and
challenging benchmark for spatial reasoning, grounded in cognitive psychology.
OmniSpatial covers four major categories: dynamic reasoning, complex spatial
logic, spatial interaction, and perspective-taking, with 50 fine-grained
subcategories. Through Internet data crawling and careful manual annotation, we
construct over 1.5K question-answer pairs. Extensive experiments show that both
open- and closed-source VLMs, as well as existing reasoning and spatial
understanding models, exhibit significant limitations in comprehensive spatial
understanding. We further analyze failure cases and propose potential
directions for future research.

</details>


### [292] [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139)
*Siqi Chen,Xinyu Dong,Haolei Xu,Xingyu Wu,Fei Tang,Hang Zhang,Yuchen Yan,Linjuan Wu,Wenqi Zhang,Guiyang Hou,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

TL;DR: SVGenius is a new benchmark for evaluating LLMs and Multimodal LLMs in SVG processing, revealing performance trends and limitations.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of existing benchmarks for evaluating LLMs in SVG processing, such as limited real-world coverage, lack of complexity stratification, and fragmented evaluation paradigms.

Method: Introduced SVGenius, a comprehensive benchmark with 2,377 queries across understanding, editing, and generation dimensions, built on real-world data from 24 application domains with systematic complexity stratification, evaluated through 8 task categories and 18 metrics.

Result: Proprietary models outperform open-source ones, all models show performance degradation with increasing complexity, reasoning-enhanced training is more effective than pure scaling, and style transfer is the most challenging capability.

Conclusion: SVGenius provides the first systematic evaluation framework for SVG processing, offering insights for developing better vector graphics models and advancing automated graphic design applications.

Abstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising
capabilities for SVG processing, yet existing benchmarks suffer from limited
real-world coverage, lack of complexity stratification, and fragmented
evaluation paradigms. We introduce SVGenius, a comprehensive benchmark
comprising 2,377 queries across three progressive dimensions: understanding,
editing, and generation. Built on real-world data from 24 application domains
with systematic complexity stratification, SVGenius evaluates models through 8
task categories and 18 metrics. We assess 22 mainstream models spanning
different scales, architectures, training paradigms, and accessibility levels.
Our analysis reveals that while proprietary models significantly outperform
open-source counterparts, all models exhibit systematic performance degradation
with increasing complexity, indicating fundamental limitations in current
approaches; however, reasoning-enhanced training proves more effective than
pure scaling for overcoming these limitations, though style transfer remains
the most challenging capability across all model types. SVGenius establishes
the first systematic evaluation framework for SVG processing, providing crucial
insights for developing more capable vector graphics models and advancing
automated graphic design applications. Appendix and supplementary materials
(including all data and code) are available at
https://zju-real.github.io/SVGenius.

</details>


### [293] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
*Bin Lin,Zongjian Li,Xinhua Cheng,Yuwei Niu,Yang Ye,Xianyi He,Shenghai Yuan,Wangbo Yu,Shaodong Wang,Yunyang Ge,Yatian Pang,Li Yuan*

Main category: cs.CV

TL;DR: Existing models are limited in image perception and manipulation tasks. OpenAI's GPT-4o-Image model shows strong capabilities in these areas by using semantic encoders rather than VAEs. Inspired by this, the authors developed UniWorld, a unified generative framework based on semantic features from visual-language models and contrastive semantic encoders. UniWorld outperforms BAGEL on image editing benchmarks using only 1% of its data, while also maintaining competitive image understanding and generation capabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing unified models in exploring image perception and manipulation tasks, which are highly desired by users for wide applications.

Method: The method involves developing a unified generative framework named UniWorld based on semantic features provided by powerful visual-language models and contrastive semantic encoders.

Result: UniWorld builds a strong unified model using only 1% amount of BAGEL's data and consistently outperforms BAGEL on image editing benchmarks. It also maintains competitive image understanding and generation capabilities.

Conclusion: The authors conclude by fully open-sourcing their models, including model weights, training and evaluation scripts, and datasets.

Abstract: Although existing unified models deliver strong performance on
vision-language understanding and text-to-image generation, their models are
limited in exploring image perception and manipulation tasks, which are
urgently desired by users for wide applications. Recently, OpenAI released
their powerful GPT-4o-Image model for comprehensive image perception and
manipulation, achieving expressive capability and attracting community
interests. By observing the performance of GPT-4o-Image in our carefully
constructed experiments, we infer that GPT-4o-Image leverages features
extracted by semantic encoders instead of VAE, while VAEs are considered
essential components in many image manipulation models. Motivated by such
inspiring observations, we present a unified generative framework named
UniWorld based on semantic features provided by powerful visual-language models
and contrastive semantic encoders. As a result, we build a strong unified model
using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on
image editing benchmarks. UniWorld also maintains competitive image
understanding and generation capabilities, achieving strong performance across
multiple image perception tasks. We fully open-source our models, including
model weights, training and evaluation scripts, and datasets.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [294] [Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods](https://arxiv.org/abs/2506.02841)
*Tom Danino,Nahum Shimkin*

Main category: eess.SY

TL;DR: 提出了一种结合分解集中式评论家与分散式集成学习的新算法，通过选择性探索方法、多样性正则化集成个体评论家和截断TD(λ)算法等技术，提高了多智能体强化学习在样本效率和探索方面的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习（MARL）算法通常需要比单智能体算法更多的环境交互才能收敛，这主要是由于联合动作空间的探索难度大以及MARL环境中的高方差问题。为了解决这些问题，本文提出了一种新算法。

Method: 该算法的主要组成部分包括：1）选择性探索方法，利用集成峰度来指导探索；2）扩展全局分解评论家，采用多样性正则化的个体评论家集成，并使用其超峰度来引导探索；3）训练集中式评论家时采用新的截断TD(λ)算法，以减少方差并实现高效的离策略学习；4）在演员端，采用混合样本方法，将在线策略和离线策略损失函数混合用于训练演员，从而平衡稳定性和效率。

Result: 实验结果表明，该方法在标准MARL基准测试中优于现有最先进基线，包括各种SMAC II地图。

Conclusion: 所提出的算法通过引入选择性探索方法、改进的评论家训练方式和混合样本策略，在多智能体任务上实现了更高的样本效率和更好的性能。

Abstract: Multi-agent reinforcement learning (MARL) methods have achieved
state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms
typically require significantly more environment interactions than their
single-agent counterparts to converge, a problem exacerbated by the difficulty
in exploring over a large joint action space and the high variance intrinsic to
MARL environments. To tackle these issues, we propose a novel algorithm that
combines a decomposed centralized critic with decentralized ensemble learning,
incorporating several key contributions. The main component in our scheme is a
selective exploration method that leverages ensemble kurtosis. We extend the
global decomposed critic with a diversity-regularized ensemble of individual
critics and utilize its excess kurtosis to guide exploration toward
high-uncertainty states and actions. To improve sample efficiency, we train the
centralized critic with a novel truncated variation of the TD($\lambda$)
algorithm, enabling efficient off-policy learning with reduced variance. On the
actor side, our suggested algorithm adapts the mixed samples approach to MARL,
mixing on-policy and off-policy loss functions for training the actors. This
approach balances between stability and efficiency and outperforms purely
off-policy learning. The evaluation shows our method outperforms
state-of-the-art baselines on standard MARL benchmarks, including a variety of
SMAC II maps.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [295] [Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders](https://arxiv.org/abs/2506.02051)
*Hui Liu,Shiye Tian,Xuejun Liu*

Main category: q-bio.BM

TL;DR: The paper introduces SmilesGEN, a generative model that integrates drug VAE and expression profile VAE to generate molecules with potential therapeutic effects by modeling the interplay between drug perturbations and transcriptional responses.


<details>
  <summary>Details</summary>
Motivation: Current methods for generating drug-like molecules rely on expression profiles but ignore the perturbative effect of molecules on cellular contexts.

Method: SmilesGEN combines a pre-trained drug VAE (SmilesNet) with an expression profile VAE (ProfileNet). ProfileNet reconstructs pre-treatment expression profiles while eliminating drug-induced perturbations, and SmilesNet generates drug-like molecules informed by desired expression profiles.

Result: SmilesGEN outperforms state-of-the-art models in generating molecules with higher validity, uniqueness, novelty, and Tanimoto similarity. It also excels in scaffold-based molecule optimization and generating molecules similar to approved drugs.

Conclusion: SmilesGEN provides a robust framework leveraging gene signatures to generate drug-like molecules with promising potential for inducing desirable cellular phenotypic changes.

Abstract: The de novo generation of drug-like molecules capable of inducing desirable
phenotypic changes is receiving increasing attention. However, previous methods
predominantly rely on expression profiles to guide molecule generation, but
overlook the perturbative effect of the molecules on cellular contexts. To
overcome this limitation, we propose SmilesGEN, a novel generative model based
on variational autoencoder (VAE) architecture to generate molecules with
potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE
(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the
interplay between drug perturbations and transcriptional responses in a common
latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment
expression profiles when eliminating drug-induced perturbations in the latent
space, while SmilesNet is informed by desired expression profiles to generate
drug-like molecules. Our empirical experiments demonstrate that SmilesGEN
outperforms current state-of-the-art models in generating molecules with higher
degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity
to known ligands targeting the relevant proteins. Moreover, we evaluate
SmilesGEN for scaffold-based molecule optimization and generation of
therapeutic agents, and confirmed its superior performance in generating
molecules with higher similarity to approved drugs. SmilesGEN establishes a
robust framework that leverages gene signatures to generate drug-like molecules
that hold promising potential to induce desirable cellular phenotypic changes.

</details>


### [296] [Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications](https://arxiv.org/abs/2506.02052)
*Shuo Yan,Yuliang Yan,Bin Ma,Chenao Li,Haochun Tang,Jiahua Lu,Minhua Lin,Yuyuan Feng,Hui Xiong,Enyan Dai*

Main category: q-bio.BM

TL;DR: 近期，为了支持下游蛋白质应用，人们探索了广泛的深度学习架构和预训练策略。此外，还开发了结合生物知识的领域特定模型以提高在专业任务中的性能。本文介绍了Protap，一个全面的基准测试平台，系统地比较了主干架构、预训练策略和领域特定模型在多样且现实的下游蛋白质应用中的表现。Protap涵盖五个应用：三个一般任务和两个新的专业化任务，即酶催化蛋白切割位点预测和靶向蛋白降解。对于每个应用，Protap在多种预训练设置下比较了各种领域特定模型和通用架构。我们的实证研究暗示：(i)尽管大规模预训练编码器取得了很好的结果，但它们通常在小型下游训练集上表现不如监督编码器。(ii)在下游微调过程中加入结构信息可以匹配甚至超越在大规模序列语料库上预训练的蛋白质语言模型。(iii)领域特定的生物学先验可以提高在专门下游任务中的性能。代码和数据集可在https://github.com/Trust-App-AI-Lab/protap公开获取。


<details>
  <summary>Details</summary>
Motivation: 当前针对蛋白质应用的深度学习架构和预训练策略虽已取得一定成果，但在不同实际应用场景下的效果比较尚缺乏系统性研究。同时，领域特定模型如何有效利用生物知识提升性能也需进一步探讨。因此，需要一个全面的基准测试来系统地评估这些方法的实际表现。

Method: 提出Protap基准测试平台，涵盖五个蛋白质应用任务（三个通用任务和两个新型专业任务），通过在多种预训练设置下对比不同主干架构、预训练策略以及领域特定模型的表现，分析其在下游任务中的优劣。

Result: 实证研究表明：(i)大规模预训练编码器在小样本下游任务中可能不及监督编码器；(ii)加入结构信息的微调过程能够达到或超过大规模序列语料库预训练的蛋白质语言模型的效果；(iii)领域特定的生物先验有助于提高在专业下游任务中的性能。

Conclusion: Protap为系统评估蛋白质应用相关的深度学习架构、预训练策略及领域特定模型提供了一个全面的基准测试平台，并揭示了结构信息和生物先验对提升性能的重要性。

Abstract: Recently, extensive deep learning architectures and pretraining strategies
have been explored to support downstream protein applications. Additionally,
domain-specific models incorporating biological knowledge have been developed
to enhance performance in specialized tasks. In this work, we introduce
$\textbf{Protap}$, a comprehensive benchmark that systematically compares
backbone architectures, pretraining strategies, and domain-specific models
across diverse and realistic downstream protein applications. Specifically,
Protap covers five applications: three general tasks and two novel specialized
tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted
protein degradation, which are industrially relevant yet missing from existing
benchmarks. For each application, Protap compares various domain-specific
models and general architectures under multiple pretraining settings. Our
empirical studies imply that: (i) Though large-scale pretraining encoders
achieve great results, they often underperform supervised encoders trained on
small downstream training sets. (ii) Incorporating structural information
during downstream fine-tuning can match or even outperform protein language
models pretrained on large-scale sequence corpora. (iii) Domain-specific
biological priors can enhance performance on specialized downstream tasks. Code
and datasets are publicly available at
https://github.com/Trust-App-AI-Lab/protap.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [297] [On the Benefits of Accelerated Optimization in Robust and Private Estimation](https://arxiv.org/abs/2506.03044)
*Laurentiu Andrei Marchis,Po-Ling Loh*

Main category: math.ST

TL;DR: 研究了加速梯度方法（特别是基于Frank-Wolfe方法和投影梯度下降）在隐私保护和重尾稳健性方面的优势。通过定制学习率、Nesterov动量等技术减少迭代复杂度，从而提高统计保证。分析涵盖非随机数据、随机无模型数据和参数模型三种场景，并通过噪声梯度实现差分隐私和重尾稳健性。最终比较收敛速度，确定最优场景。


<details>
  <summary>Details</summary>
Motivation: 探索加速梯度方法在隐私保护和重尾稳健性问题中的潜力，以减少迭代复杂度并提高统计性能。

Method: 1. 对于Frank-Wolfe方法，采用定制学习率和梯度的统一下界。
2. 对于投影梯度下降，使用Nesterov动量变体并优化目标函数。
3. 基于噪声梯度确保差分隐私和实现重尾稳健性。
4. 分析非随机数据、随机无模型数据和参数模型三种设置下的表现。

Result: 提出的方法减少了迭代复杂度，提高了经验风险和总体风险最小化的统计保证。同时，在特定场景下达到了最优收敛速度。

Conclusion: 加速梯度方法在隐私保护和重尾稳健性方面具有显著优势，可有效降低迭代复杂度并优化统计性能。

Abstract: We study the advantages of accelerated gradient methods, specifically based
on the Frank-Wolfe method and projected gradient descent, for privacy and
heavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe
method, our technique is based on a tailored learning rate and a uniform lower
bound on the gradient of the $\ell_2$-norm over the constraint set. For
accelerating projected gradient descent, we use the popular variant based on
Nesterov's momentum, and we optimize our objective over $\mathbb{R}^p$. These
accelerations reduce iteration complexity, translating into stronger
statistical guarantees for empirical and population risk minimization. Our
analysis covers three settings: non-random data, random model-free data, and
parametric models (linear regression and generalized linear models).
Methodologically, we approach both privacy and robustness based on noisy
gradients. We ensure differential privacy via the Gaussian mechanism and
advanced composition, and we achieve heavy-tailed robustness using a geometric
median-of-means estimator, which also sharpens the dependency on the dimension
of the covariates. Finally, we compare our rates to existing bounds and
identify scenarios where our methods attain optimal convergence.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [298] [A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads](https://arxiv.org/abs/2506.02802)
*András Strausz,Niels Pardon,Ioana Giurgiu*

Main category: cs.DB

TL;DR: Lakehouse系统可以通过多个执行引擎查询相同的数据。然而，选择最适合运行SQL查询的引擎仍然需要事先了解查询的计算需求和引擎能力，这是一个复杂且手动的任务，随着新引擎和工作负载的出现，这一任务变得更加困难。本文通过提出一种基于学习成本模型的跨引擎优化器来解决这一限制，该优化器可以通过优化的提示和查询计划进行查询成本预测和路由。实验结果表明，使用优化的逻辑计划进行成本估算可以将平均Q-error降低12.6%，与随机路由相比，在零样本设置下，提出的跨引擎优化器可以将总工作负载运行时间减少多达25.2%，在少量样本设置下减少30.4%。


<details>
  <summary>Details</summary>
Motivation: 当前选择适合运行SQL查询的引擎需要先验知识，并且随着新引擎和工作负载的出现，这一任务变得越来越复杂。

Method: 提出了一种跨引擎优化器，通过学习成本模型自动选择适合 diverse SQL 查询的引擎。优化器利用带有提示的查询计划进行查询成本预测和路由，将成本预测建模为多任务学习问题，并在模型架构中使用多个预测头以对应不同的引擎和配置。

Result: 使用优化的逻辑计划进行成本估算可使平均Q-error降低12.6%；在零样本设置下，总工作负载运行时间最多可减少25.2%，在少量样本设置下可减少30.4%。

Conclusion: 所提出的跨引擎优化器显著提高了查询性能并减少了工作负载运行时间，同时简化了引擎选择过程。

Abstract: Lakehouse systems enable the same data to be queried with multiple execution
engines. However, selecting the engine best suited to run a SQL query still
requires a priori knowledge of the query computational requirements and an
engine capability, a complex and manual task that only becomes more difficult
with the emergence of new engines and workloads. In this paper, we address this
limitation by proposing a cross-engine optimizer that can automate engine
selection for diverse SQL queries through a learned cost model. Optimized with
hints, a query plan is used for query cost prediction and routing. Cost
prediction is formulated as a multi-task learning problem, and multiple
predictor heads, corresponding to different engines and provisionings, are used
in the model architecture. This eliminates the need to train engine-specific
models and allows the flexible addition of new engines at a minimal fine-tuning
cost. Results on various databases and engines show that using a query
optimized logical plan for cost estimation decreases the average Q-error by
even 12.6% over using unoptimized plans as input. Moreover, the proposed
cross-engine optimizer reduces the total workload runtime by up to 25.2% in a
zero-shot setting and 30.4% in a few-shot setting when compared to random
routing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [299] [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
*David Sasu,Kweku Andoh Yamoah,Benedict Quartey,Natalie Schluter*

Main category: cs.RO

TL;DR: 该论文提出了一种利用语音韵律的新方法，通过上下文学习将预测意图整合到大语言模型中，以消除歧义并选择合适的任务计划。此外，还提出了首个机器人领域的模糊语音数据集。实验结果表明，该方法在检测参考意图和确定模糊指令的任务计划方面具有高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于语音识别将语音转换为文本，通常会丢失用于消除意图歧义的关键韵律线索。

Method: 提出了一种新方法，直接利用语音韵律来推断和解决指令意图，并通过上下文学习将预测的意图整合到大型语言模型中以消除歧义并选择适当的任务计划。

Result: 该方法在检测参考意图方面的准确率为95.79%，在确定模糊指令的任务计划方面的准确率为71.96%。

Conclusion: 该方法展示了其显著改善人机通信的潜力。

Abstract: Enabling robots to accurately interpret and execute spoken language
instructions is essential for effective human-robot collaboration. Traditional
methods rely on speech recognition to transcribe speech into text, often
discarding crucial prosodic cues needed for disambiguating intent. We propose a
novel approach that directly leverages speech prosody to infer and resolve
instruction intent. Predicted intents are integrated into large language models
via in-context learning to disambiguate and select appropriate task plans.
Additionally, we present the first ambiguous speech dataset for robotics,
designed to advance research in speech disambiguation. Our method achieves
95.79% accuracy in detecting referent intents within an utterance and
determines the intended task plan of ambiguous instructions with 71.96%
accuracy, demonstrating its potential to significantly improve human-robot
communication.

</details>


### [300] [HiLO: High-Level Object Fusion for Autonomous Driving using Transformers](https://arxiv.org/abs/2506.02554)
*Timo Osterburg,Franz Albers,Christopher Diehl,Rajesh Pushparaj,Torsten Bertram*

Main category: cs.RO

TL;DR: This paper proposes HiLO, a transformer-based high-level object fusion method that modifies the Adapted Kalman Filter (AKF). It shows significant improvements in F1 score and mean IoU, with demonstrated effectiveness on a large-scale real-world dataset and cross-domain evaluation.


<details>
  <summary>Details</summary>
Motivation: Current learning-based sensor data fusion methods have high performance but are limited by complexity and hardware requirements for near-production vehicles. High-level fusion methods provide robustness with lower computational needs, traditionally dominated by Kalman filter techniques.

Method: The paper modifies the Adapted Kalman Filter (AKF) and introduces HiLO, a novel transformer-based high-level object fusion method designed for autonomous driving perception.

Result: HiLO achieves a 25.9 percentage point increase in F1 score and a 6.1 percentage point increase in mean IoU. The method's effectiveness is validated on a new large-scale real-world dataset, with further generalizability shown through cross-domain evaluations between urban and highway scenarios.

Conclusion: HiLO offers an effective solution for high-level sensor data fusion with lower computational demands, showing strong performance and generalizability across different environments.

Abstract: The fusion of sensor data is essential for a robust perception of the
environment in autonomous driving. Learning-based fusion approaches mainly use
feature-level fusion to achieve high performance, but their complexity and
hardware requirements limit their applicability in near-production vehicles.
High-level fusion methods offer robustness with lower computational
requirements. Traditional methods, such as the Kalman filter, dominate this
area. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel
transformer-based high-level object fusion method called HiLO. Experimental
results demonstrate improvements of $25.9$ percentage points in $\textrm{F}_1$
score and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale
real-world dataset demonstrates the effectiveness of the proposed approaches.
Their generalizability is further validated by cross-domain evaluation between
urban and highway scenarios. Code, data, and models are available at
https://github.com/rst-tu-dortmund/HiLO .

</details>


### [301] [Multi Layered Autonomy and AI Ecologies in Robotic Art Installations](https://arxiv.org/abs/2506.02606)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

TL;DR: Symbiosis of Agents is a large-scale installation that embeds AI-driven robots in an immersive arena, probing the tension between machine agency and artistic authorship. A three-tier system pilots the ecology, turning spectators into co-authors.


<details>
  <summary>Details</summary>
Motivation: To explore the tension between machine agency and artistic authorship, and to redefine agency, authorship, and ethics in contemporary art through cybernetic feedback, robotic experimentation, and conceptual rule-making.

Method: A three-tier faith system pilots the ecology: micro-level adaptive tactics, meso-level narrative drives, and a macro-level prime directive. This hierarchy allows behaviors to evolve organically in response to environmental cues and even a viewer's breath.

Result: Turns spectators into co-authors of the unfolding drama, casting the robots as collaborators rather than tools, forging a living, emergent artwork.

Conclusion: Symbiosis of Agents shows how cybernetic feedback, robotic experimentation, and conceptual rule-making can converge to redefine agency, authorship, and ethics in contemporary art.

Abstract: Symbiosis of Agents is a large-scale installation by Baoyang Chen that embeds
AI-driven robots in an immersive, mirror-lined arena, probing the tension
between machine agency and artistic authorship. Drawing on early cybernetics,
rule-based conceptual art, and seminal robotic works, it orchestrates fluid
exchanges among robotic arms, quadruped machines, their environment, and the
public. A three tier faith system pilots the ecology: micro-level adaptive
tactics, meso-level narrative drives, and a macro-level prime directive. This
hierarchy lets behaviors evolve organically in response to environmental cues
and even a viewer's breath, turning spectators into co-authors of the unfolding
drama.Framed by a speculative terraforming scenario that recalls the historical
exploitation of marginalized labor, the piece asks who bears responsibility in
AI-mediated futures. Choreographed motion, AI-generated scripts, reactive
lighting, and drifting fog cast the robots as collaborators rather than tools,
forging a living, emergent artwork. Exhibited internationally, Symbiosis of
Agents shows how cybernetic feedback, robotic experimentation, and conceptual
rule-making can converge to redefine agency, authorship, and ethics in
contemporary art.

</details>


### [302] [Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent](https://arxiv.org/abs/2506.02373)
*Kordel K. France,Ovidiu Daescu*

Main category: cs.RO

TL;DR: 本研究定义了嗅觉惯性里程计（OIO），这是一个利用惯性运动学和快速采样嗅觉传感器来实现类似视觉惯性里程计的气味导航框架。通过在5-自由度机械臂上进行实验，展示了其在气味定位中的应用，并为未来的复杂任务提供了改进建议。


<details>
  <summary>Details</summary>
Motivation: 机器嗅觉导航是一项极具挑战性的任务，目前尚缺乏有效的解决方案。为了填补这一空白并推动相关领域的发展，本研究提出了一种新的框架——嗅觉惯性里程计（OIO）。

Method: 研究者将SLAM和VIO的原理外推到嗅觉领域，设计了一个结合惯性运动学和快速采样嗅觉传感器的框架（OIO）。通过使用三种不同的气味定位算法，在一个5-自由度机器人手臂上进行了气味追踪实验。

Result: 实验结果表明，OIO框架成功地建立了一个可用于未来研究的基础平台，能够实现基本的气味导航任务。同时，研究者指出了一些可以改进的方向，以应对更复杂的任务。

Conclusion: 嗅觉惯性里程计（OIO）为机器嗅觉导航提供了一个全新的框架，具有广泛的应用前景。尽管当前版本仍需改进，但已为未来的研究奠定了坚实的基础。

Abstract: Olfactory navigation is one of the most primitive mechanisms of exploration
used by organisms. Navigation by machine olfaction (artificial smell) is a very
difficult task to both simulate and solve. With this work, we define olfactory
inertial odometry (OIO), a framework for using inertial kinematics, and
fast-sampling olfaction sensors to enable navigation by scent analogous to
visual inertial odometry (VIO). We establish how principles from SLAM and VIO
can be extrapolated to olfaction to enable real-world robotic tasks. We
demonstrate OIO with three different odour localization algorithms on a real
5-DoF robot arm over an odour-tracking scenario that resembles real
applications in agriculture and food quality control. Our results indicate
success in establishing a baseline framework for OIO from which other research
in olfactory navigation can build, and we note performance enhancements that
can be made to address more complex tasks in the future.

</details>


### [303] [Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges](https://arxiv.org/abs/2506.02489)
*Tao Zhong,Jonah Buchanan,Christine Allen-Blanchette*

Main category: cs.RO

TL;DR: The paper presents a new method for transferring grasping actions between robot hands with different structures using visual information, without needing paired demonstrations or hand-specific simulations. This is achieved by formulating the problem as a stochastic transport and utilizing physics-informed cost functions.


<details>
  <summary>Details</summary>
Motivation: To enable dexterous grasp translation across robotic hands with differing morphologies, overcoming limitations of existing methods that require paired demonstrations or specific simulations.

Method: The approach frames grasp translation as a stochastic transport problem using the Schrödinger Bridge formalism. It learns to map between latent grasp spaces of source and target hands via score and flow matching, guided by physics-informed cost functions that encode alignment in base pose, contact maps, wrench space, and manipulability.

Result: Experiments on various hand-object pairs show that the method generates stable, physically plausible grasps with strong generalization capabilities.

Conclusion: This work successfully enables semantic grasp transfer for heterogeneous manipulators, bridging vision-based grasping with probabilistic generative modeling.

Abstract: We propose a new approach to vision-based dexterous grasp translation, which
aims to transfer grasp intent across robotic hands with differing morphologies.
Given a visual observation of a source hand grasping an object, our goal is to
synthesize a functionally equivalent grasp for a target hand without requiring
paired demonstrations or hand-specific simulations. We frame this problem as a
stochastic transport between grasp distributions using the Schr\"odinger Bridge
formalism. Our method learns to map between source and target latent grasp
spaces via score and flow matching, conditioned on visual observations. To
guide this translation, we introduce physics-informed cost functions that
encode alignment in base pose, contact maps, wrench space, and manipulability.
Experiments across diverse hand-object pairs demonstrate our approach generates
stable, physically grounded grasps with strong generalization. This work
enables semantic grasp transfer for heterogeneous manipulators and bridges
vision-based grasping with probabilistic generative modeling.

</details>


### [304] [Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search](https://arxiv.org/abs/2506.02746)
*Lin Xie,Hanyi Li*

Main category: cs.RO

TL;DR: An improved solution method that integrates Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning (DRL) is presented for the Pod Repositioning Problem (PRP). This DRL-guided ALNS outperforms traditional approaches in computational results, showing strong solution quality and illustrating the benefit of learning-driven control within combinatorial optimization for warehouse systems.


<details>
  <summary>Details</summary>
Motivation: To select optimal storage locations for pods returning from pick stations in Robotic Mobile Fulfillment Systems (RMFS), improving upon traditional methods such as cheapest-place, fixed-place, binary integer programming, and static heuristics.

Method: Integration of Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning (DRL). A DRL agent dynamically selects destroy and repair operators and adjusts key parameters like destruction degree and acceptance thresholds during the search. Specialized heuristics are designed for both operators to reflect PRP-specific characteristics, including pod usage frequency and movement costs.

Result: The DRL-guided ALNS outperforms traditional approaches such as cheapest-place, fixed-place, binary integer programming, and static heuristics in computational results, demonstrating strong solution quality.

Conclusion: This work demonstrates the advantage of using a learning-driven control method within combinatorial optimization for warehouse systems.

Abstract: The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems
(RMFS) involves selecting optimal storage locations for pods returning from
pick stations. This work presents an improved solution method that integrates
Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning
(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts
key parameters such as destruction degree and acceptance thresholds during the
search. Specialized heuristics for both operators are designed to reflect
PRP-specific characteristics, including pod usage frequency and movement costs.
Computational results show that this DRL-guided ALNS outperforms traditional
approaches such as cheapest-place, fixed-place, binary integer programming, and
static heuristics. The method demonstrates strong solution quality and
illustrating the benefit of learning-driven control within combinatorial
optimization for warehouse systems.

</details>


### [305] [Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games](https://arxiv.org/abs/2506.02849)
*Alejandro Sanchez Roncero,Olov Andersson,Petter Ogren*

Main category: cs.RO

TL;DR: The paper presents a reinforcement learning framework for quadrotor pursuit-evasion, using neural network policies and an AMSPB algorithm to improve performance and retention of strategies.


<details>
  <summary>Details</summary>
Motivation: The increasing use of small UAVs in civilian and military airspace has raised safety and security concerns, especially when unauthorized or malicious drones enter restricted zones.

Method: The method involves training neural network policies for commanding body rates and collective thrust, enabling high-speed maneuvers. An Asynchronous Multi-Stage Population-Based (AMSPB) algorithm is introduced to mitigate nonstationarity and catastrophic forgetting during adversarial co-training.

Result: Rate-based policies achieve significantly higher capture rates and peak speeds than velocity-level baselines, and AMSPB yields stable, monotonic gains against benchmark opponents.

Conclusion: The reinforcement learning framework and AMSPB algorithm effectively enhance the performance of quadrotor pursuit-evasion tasks.

Abstract: The increasing proliferation of small UAVs in civilian and military airspace
has raised critical safety and security concerns, especially when unauthorized
or malicious drones enter restricted zones. In this work, we present a
reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion.
We train neural network policies to command body rates and collective thrust,
enabling high-speed pursuit and evasive maneuvers that fully exploit the
quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic
forgetting during adversarial co-training, we introduce an Asynchronous
Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the
pursuer or evader learns against a sampled opponent drawn from a growing
population of past and current policies. This continual learning setup ensures
monotonic performance improvement and retention of earlier strategies. Our
results show that (i) rate-based policies achieve significantly higher capture
rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields
stable, monotonic gains against a suite of benchmark opponents.

</details>


### [306] [Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs](https://arxiv.org/abs/2506.02860)
*Wenjing Tang,Xinyu He,Yongxi Huang,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Main category: cs.RO

TL;DR: The paper introduces Tru-POMDP, a planner that merges structured belief generation via Large Language Models (LLMs) with POMDP planning to handle task planning under uncertainty for home-service robots. It uses a hierarchical Tree of Hypotheses (TOH) and an open-ended POMDP model, showing superior performance in complex tasks compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Task planning under uncertainty is crucial for home-service robots due to ambiguous human instructions, hidden or unknown object locations, and open-vocabulary object types.

Method: Proposes Tru-POMDP which combines LLMs for structured belief generation with principled POMDP planning. Introduces a hierarchical Tree of Hypotheses (TOH) to construct high-quality particle beliefs over possible world states and human goals.

Result: Experiments demonstrate that Tru-POMDP significantly outperforms state-of-the-art LLM-based planners in complex object rearrangement tasks, achieving higher success rates, better plans, stronger robustness, and greater efficiency.

Conclusion: Tru-POMDP effectively addresses the challenges of task planning under uncertainty for home-service robots, outperforming current state-of-the-art methods.

Abstract: Task planning under uncertainty is essential for home-service robots
operating in the real world. Tasks involve ambiguous human instructions, hidden
or unknown object locations, and open-vocabulary object types, leading to
significant open-ended uncertainty and a boundlessly large planning space. To
address these challenges, we propose Tru-POMDP, a planner that combines
structured belief generation using Large Language Models (LLMs) with principled
POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),
which systematically queries an LLM to construct high-quality particle beliefs
over possible world states and human goals. We further formulate an open-ended
POMDP model that enables rigorous Bayesian belief tracking and efficient
belief-space planning over these LLM-generated hypotheses. Experiments on
complex object rearrangement tasks across diverse kitchen environments show
that Tru-POMDP significantly outperforms state-of-the-art LLM-based and
LLM-tree-search hybrid planners, achieving higher success rates with
significantly better plans, stronger robustness to ambiguity and occlusion, and
greater planning efficiency.

</details>


### [307] [UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models](https://arxiv.org/abs/2506.02955)
*Zewen Yang,Xiaobing Dai,Dian Yu,Qianru Li,Yu Li,Valentin Le Mesle*

Main category: cs.RO

TL;DR: UniConFlow是一种统一的基于流匹配的轨迹生成框架，能够系统地结合等式和不等式约束，通过引入新颖的预设时间归零函数和二次规划公式，提高了任务适应性和约束满足能力，在移动导航和高维操作任务中表现出更高的安全性和可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管生成模型在机器人运动生成方面表现出色，但大多数现有方法在处理多种约束条件（如避碰和动态一致性）时仍存在局限性，通常分别或部分考虑这些约束。

Method: 提出了一种名为UniConFlow的统一框架，该框架基于流匹配技术，通过引入一种新颖的预设时间归零函数增强推理过程中的灵活性，并通过二次规划公式推导出引导输入，以确保约束满足，无需重新训练或辅助控制器。

Result: 在移动导航和高维操作任务中，与最先进的受限生成规划器相比，展示了更高的安全性和可行性。

Conclusion: UniConFlow提供了一种有效的方法来生成满足多种约束的轨迹，适用于各种机器人任务，具有较高的安全性和可行性。

Abstract: Generative models have become increasingly powerful tools for robot motion
generation, enabling flexible and multimodal trajectory generation across
various tasks. Yet, most existing approaches remain limited in handling
multiple types of constraints, such as collision avoidance and dynamic
consistency, which are often treated separately or only partially considered.
This paper proposes UniConFlow, a unified flow matching (FM) based framework
for trajectory generation that systematically incorporates both equality and
inequality constraints. UniConFlow introduces a novel prescribed-time zeroing
function to enhance flexibility during the inference process, allowing the
model to adapt to varying task requirements. To ensure constraint satisfaction,
particularly with respect to obstacle avoidance, admissible action range, and
kinodynamic consistency, the guidance inputs to the FM model are derived
through a quadratic programming formulation, which enables constraint-aware
generation without requiring retraining or auxiliary controllers. We conduct
mobile navigation and high-dimensional manipulation tasks, demonstrating
improved safety and feasibility compared to state-of-the-art constrained
generative planners. Project page is available at https://uniconflow.github.io.

</details>


### [308] [EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment](https://arxiv.org/abs/2506.03046)
*Mikolaj Walczak,Romina Aalishah,Wyatt Mackey,Brittany Story,David L. Boothe Jr.,Nicholas Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Main category: cs.RO

TL;DR: The paper introduces EDEN, a bio-inspired navigation framework combining entorhinal-like grid cell representations and reinforcement learning for autonomous navigation. It uses a grid cell encoder to transform motion into spatial codes, aiding in efficient navigation in both simple and complex environments.


<details>
  <summary>Details</summary>
Motivation: Deep reinforcement learning agents lack the adaptability and flexibility of humans in varying scenarios. The authors aim to bridge this gap by developing a navigation system inspired by the mammalian entorhinal-hippocampal system.

Method: EDEN integrates entorhinal-like grid cell representations with reinforcement learning. A grid cell encoder transforms egocentric motion into periodic spatial codes. These codes are generated from raw sensory input using fiducial marker detections in MiniWorld and DINO-based visual features in Gazebo. The spatial representations serve as input to a policy trained with Proximal Policy Optimization (PPO) for goal-directed navigation.

Result: EDEN achieves a 99% success rate in simple scenarios and over 94% in complex floorplans with occluded paths, demonstrating more efficient and reliable step-wise navigation compared to baseline agents. A trainable Grid Cell encoder is also presented, enabling the development of periodic grid-like patterns from vision and motion sensor data.

Conclusion: This work represents a significant step towards biologically grounded spatial intelligence in robotics, merging neural navigation principles with reinforcement learning for scalable deployment.

Abstract: Deep reinforcement learning agents are often fragile while humans remain
adaptive and flexible to varying scenarios. To bridge this gap, we present
EDEN, a biologically inspired navigation framework that integrates learned
entorhinal-like grid cell representations and reinforcement learning to enable
autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,
EDEN allows agents to perform path integration and vector-based navigation
using visual and motion sensor data. At the core of EDEN is a grid cell encoder
that transforms egocentric motion into periodic spatial codes, producing
low-dimensional, interpretable embeddings of position. To generate these
activations from raw sensory input, we combine fiducial marker detections in
the lightweight MiniWorld simulator and DINO-based visual features in the
high-fidelity Gazebo simulator. These spatial representations serve as input to
a policy trained with Proximal Policy Optimization (PPO), enabling dynamic,
goal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid
prototyping, and Gazebo, which offers realistic physics and perception noise.
Compared to baseline agents using raw state inputs (e.g., position, velocity)
or standard convolutional image encoders, EDEN achieves a 99% success rate,
within the simple scenarios, and >94% within complex floorplans with occluded
paths with more efficient and reliable step-wise navigation. In addition, as a
replacement of ground truth activations, we present a trainable Grid Cell
encoder enabling the development of periodic grid-like patterns from vision and
motion sensor data, emulating the development of such patterns within
biological mammals. This work represents a step toward biologically grounded
spatial intelligence in robotics, bridging neural navigation principles with
reinforcement learning for scalable deployment.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [309] [Validating remotely sensed biomass estimates with forest inventory data in the western US](https://arxiv.org/abs/2506.03120)
*Xiuyu Cao,Joseph O. Sexton,Panshi Wang,Dimitrios Gounaridis,Neil H. Carter,Kai Zhu*

Main category: stat.AP

TL;DR: The paper independently validates a commercial AGBD dataset using FIA data, showing strong agreement and providing insights into discrepancies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous or independent validation for commercial remote sensing products estimating aboveground biomass density (AGBD), particularly those based on NASA's GEDI mission.

Method: An independent regional validation was conducted on an AGBD dataset from terraPulse, Inc., using reference data from the US Forest Service Forest Inventory and Analysis (FIA) program. The analysis was performed at two scales: 64,000-hectare hexagons and US counties across Utah, Nevada, and Washington.

Result: Very strong agreement was found between terraPulse and FIA estimates with R2 = 0.88 and RMSE = 26.68 Mg/ha at the hexagon scale, improving to R2 = 0.90 and RMSE = 32.62 Mg/ha at the county scale. Discrepancies were noted in non-forest areas and high-biomass forests.

Conclusion: This study provides a scalable framework for validating AGBD datasets using independent FIA data and sets a benchmark for validating new commercial datasets for global biomass monitoring.

Abstract: Monitoring aboveground biomass (AGB) and its density (AGBD) at high
resolution is essential for carbon accounting and ecosystem management. While
NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission
provides globally distributed reference measurements for AGBD estimation, the
majority of commercial remote sensing products based on GEDI remain without
rigorous or independent validation. Here, we present an independent regional
validation of an AGBD dataset offered by terraPulse, Inc., based on independent
reference data from the US Forest Service Forest Inventory and Analysis (FIA)
program. Aggregated to 64,000-hectare hexagons and US counties across the US
states of Utah, Nevada, and Washington, we found very strong agreement between
terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =
26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,
agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.
Spatial and statistical analyses indicated that terraPulse AGBD values tended
to exceed FIA estimates in non-forest areas, likely due to FIA's limited
sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited
lower values in high-biomass forests, likely due to saturation effects in its
optical remote-sensing covariates. This study advances operational carbon
monitoring by delivering a scalable framework for comprehensive AGBD validation
using independent FIA data, as well as a benchmark validation of a new
commercial dataset for global biomass monitoring.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [310] [Labelling Data with Unknown References](https://arxiv.org/abs/2506.03083)
*Adrian de Wynter*

Main category: cs.DS

TL;DR: An evaluator is trustworthy if its performance can be measured. When labelled references are unavailable, the No-Data Algorithm establishes trustworthiness by posing challenges to the evaluator.


<details>
  <summary>Details</summary>
Motivation: When there are no labelled references available, traditional methods of establishing an evaluator's trustworthiness cannot be used.

Method: The No-Data Algorithm establishes trust in an evaluator without any existing references by successively posing challenges to said evaluator.

Result: The algorithm is sufficient to establish trustworthiness with high probability when the evaluator knows the way to label the corpus and flags untrustworthy evaluators when they are unable to prove it.

Conclusion: The No-Data Algorithm provides a way to establish trust in an evaluator without requiring labelled references.

Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure
its performance as a labeller. The two ways to establish trustworthiness are
either by testing it, or by assuming the evaluator `knows' somehow the way to
label the corpus. However, if labelled references (e.g., a development set) are
unavailable, neither of these approaches work: the former requires the data,
and the latter is an assumption, not evidence. To address this, we introduce an
algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator
without any existing references. Our algorithm works by successively posing
challenges to said evaluator. We show that this is sufficient to establish
trustworthiness w.h.p., in such a way that when the evaluator actually knows
the way to label the corpus, the No-Data Algorithm accepts its output; and,
conversely, flags untrustworthy evaluators when these are unable to prove it.
We present formal proofs of correctness and limited experiments.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [311] [TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation](https://arxiv.org/abs/2506.02267)
*Xue Xia,Saurabh Vishwas Joshi,Kousik Rajesh,Kangnan Li,Yangyi Lu,Nikil Pancha,Dhruvil Deven Badani,Jiajing Xu,Pong Eksombatchai*

Main category: cs.IR

TL;DR: TransAct V2 is a production model for Pinterest's Homefeed ranking system with three key innovations.


<details>
  <summary>Details</summary>
Motivation: Industry-scale CTR models often rely on short user sequences, lack an integrated action-prediction task within a point-wise ranking framework and rarely address the infrastructure challenges involved in efficiently serving large-scale sequential models.

Method: Leveraging very long user sequences to improve CTR predictions, integrating a Next Action Loss function for enhanced user action forecasting, employing scalable, low-latency deployment solutions tailored to handle the computational demands of extended user action sequences.

Result: Improved CTR predictions and enhanced user action forecasting.

Conclusion: The paper introduces TransAct V2 with three key innovations that addresses the limitations of current industry-scale CTR models.

Abstract: Modeling user action sequences has become a popular focus in industrial
recommendation system research, particularly for Click-Through Rate (CTR)
prediction tasks. However, industry-scale CTR models often rely on short user
sequences, limiting their ability to capture long-term behavior. Additionally,
these models typically lack an integrated action-prediction task within a
point-wise ranking framework, reducing their predictive power. They also rarely
address the infrastructure challenges involved in efficiently serving
large-scale sequential models. In this paper, we introduce TransAct V2, a
production model for Pinterest's Homefeed ranking system, featuring three key
innovations: (1) leveraging very long user sequences to improve CTR
predictions, (2) integrating a Next Action Loss function for enhanced user
action forecasting, and (3) employing scalable, low-latency deployment
solutions tailored to handle the computational demands of extended user action
sequences.

</details>


### [312] [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](https://arxiv.org/abs/2506.02160)
*Madan Krishnamurthy,Daniel Korn,Melissa A Haendel,Christopher J Mungall,Anne E Thessen*

Main category: cs.IR

TL;DR: This paper presents a dynamic and scalable framework using LLMs and HDBSCAN for harmonizing Common Data Elements (CDEs) across biomedical datasets. It achieved 90.46% accuracy in classifying CDEs into meaningful clusters.


<details>
  <summary>Details</summary>
Motivation: To address challenges such as semantic heterogeneity, structural variability, and context dependence in integrating heterogeneous biomedical datasets, thereby improving interoperability and accelerating scientific discovery.

Method: The methodology includes four key steps: LLM-based text embedding for representing semantic context, unsupervised clustering of embeddings via HDBSCAN, automated labeling using LLM summarization, and supervised learning to train a classifier for assigning new or unclustered CDEs.

Result: Evaluated on the NIH NLM CDE Repository with over 24,000 CDEs, the system identified 118 meaningful clusters at an optimized minimum cluster size of 20. The classifier achieved 90.46 percent overall accuracy, with strong performance in larger categories. External validation showed strong agreement with Social Determinants of Health domains.

Conclusion: This adaptable and scalable approach offers a practical solution to CDE harmonization, improving selection efficiency and supporting ongoing data interoperability.

Abstract: This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.

</details>


### [313] [Towards Human-like Preference Profiling in Sequential Recommendation](https://arxiv.org/abs/2506.02261)
*Zhongyu Ouyang,Qianlong Wen,Chunhui Zhang,Yanfang Ye,Soroush Vosoughi*

Main category: cs.IR

TL;DR: RecPO is a preference optimization framework for sequential recommendation that models structured feedback and contextual delay, achieving performance gains and mimicking human decision-making characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based recommenders lack the ability to mimic human-like flexible, context-aware decision strategies in sequential recommendation.

Method: RecPO exploits adaptive reward margins based on inferred preference hierarchies and temporal signals to favor immediately relevant items and distinguish between varying degrees of preference and aversion.

Result: Extensive experiments across five real-world datasets show that RecPO outperforms state-of-the-art baselines and mirrors key characteristics of human decision-making.

Conclusion: RecPO bridges the gap between current recommenders and human-like prioritization by modeling structured feedback and contextual delay.

Abstract: Sequential recommendation systems aspire to profile users by interpreting
their interaction histories, echoing how humans make decisions by weighing
experience, relative preference strength, and situational relevance. Yet,
existing large language model (LLM)-based recommenders often fall short of
mimicking the flexible, context-aware decision strategies humans exhibit,
neglecting the structured, dynamic, and context-aware mechanisms fundamental to
human behaviors. To bridge this gap, we propose RecPO, a preference
optimization framework that models structured feedback and contextual delay to
emulate human-like prioritization in sequential recommendation RecPO exploits
adaptive reward margins based on inferred preference hierarchies and temporal
signals, enabling the model to favor immediately relevant items and to
distinguish between varying degrees of preference and aversion. Extensive
experiments across five real-world datasets demonstrate that RecPO not only
yields performance gains over state-of-the-art baselines, but also mirrors key
characteristics of human decision-making: favoring timely satisfaction,
maintaining coherent preferences, and exercising discernment under shifting
contexts.

</details>


### [314] [DeepShop: A Benchmark for Deep Research Shopping Agents](https://arxiv.org/abs/2506.02839)
*Yougang Lyu,Xiaoyu Zhang,Lingyong Yan,Maarten de Rijke,Zhaochun Ren,Xiuying Chen*

Main category: cs.IR

TL;DR: 为了弥补现有评估基准的不足，本文提出了DeepShop，这是一个用于评估网络代理在复杂在线购物环境中的新基准。DeepShop通过查询多样性演化、查询复杂性演化和精细及整体评估三个关键组件来实现更真实的购物场景模拟。实验结果表明，现有的检索增强生成（RAG）方法和其他技术在处理复杂查询时存在明显局限性，特别是在搜索过滤器和排序偏好方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前用于评估在线购物代理的基准测试过于简单，无法反映真实购物场景的复杂性，例如多维产品属性、搜索过滤器和用户特定排序偏好等因素。这促使研究者开发一个更贴近实际需求的评估基准。

Method: DeepShop由三个主要部分组成：(1) 查询多样性演化——从真实用户查询出发，在五个热门在线购物领域生成多样化的查询；(2) 查询复杂性演化——进一步提升这些查询的复杂度，考虑产品属性、搜索过滤器和排序偏好，并根据演化次数将查询分为简单、中等和困难三个级别；(3) 精细及整体评估——提出一种自动评估框架，从产品属性、搜索过滤器和排序偏好等细节方面评估代理性能，并通过整体评估报告总体成功率。

Result: 实验系统地评估了检索增强生成（RAG）方法、网络代理和深度研究系统的表现。结果显示，RAG在处理复杂查询时由于缺乏网络交互能力而表现不佳，其他方法则在过滤器和排序偏好方面面临重大挑战，导致总体成功率较低。此外，跨类别和基于复杂性的评估及错误分析进一步支持了深研购物代理的发展。

Conclusion: DeepShop提供了一个更复杂和现实的评估基准，揭示了现有方法在处理复杂在线购物查询时的局限性，为未来改进和开发更高效的购物代理提供了方向。

Abstract: Web agents for online shopping have shown great promise in automating user
interactions across e-commerce platforms. Benchmarks for assessing such agents
do not reflect the complexity of real-world shopping scenarios, as they often
consist of overly simple queries with deterministic paths, such as "Find iPhone
15." Real shopping scenarios are inherently more layered, involving
multi-dimensional product attributes, search filters, and user-specific sorting
preferences. To address this gap, we introduce DeepShop, a benchmark designed
to evaluate web agents in complex and realistic online shopping environments.
DeepShop comprises three key components. (1) Query diversity evolution:
Starting from real user queries, we generate diverse queries across five
popular online shopping domains. (2) Query complexity evolution: We further
evolve these queries to increase complexity, considering product attributes,
search filters, and sorting preferences, and classify them into three levels:
easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and
holistic evaluation: We propose an automated evaluation framework that assesses
agent performance in terms of fine-grained aspects (product attributes, search
filters, and sorting preferences) and reports the overall success rate through
holistic evaluation. We conduct a systematic evaluation of retrieval-augmented
generation (RAG) methods, web agents, and deep research systems. Results show
that RAG struggles with complex queries due to its lack of web interaction,
while other methods face significant challenges with filters and sorting
preferences, leading to low overall success rates. We also perform
cross-category, complexity-based evaluations and error analyses to support the
advancement of deep research shopping agents.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [315] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
*Lukas Rauch,Moritz Wirth,Denis Huseljic,Marek Herde,Bernhard Sick,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: The paper explores how large language model (LLM) embeddings can enhance deep active learning (AL), providing insights into the interplay between embedding quality and query strategies in text classification tasks.


<details>
  <summary>Details</summary>
Motivation: To revisit and improve the practicality of deep active learning by leveraging frozen LLM embeddings, thus reducing computational costs typically associated with iteratively fine-tuning large models.

Method: Using five top-performing models from the MTEB leaderboard and two baselines, the study conducts a benchmark analysis on ten diverse text classification tasks. It investigates the impact of LLM embedding quality on AL query strategies, focusing on initialization methods and strategy robustness.

Result: Diversity-based sampling for initializing labeled pools works well with high-quality embeddings to boost early performance. The effectiveness of query strategies varies with embedding quality; computationally cheap methods like Margin sampling can perform well on some datasets, but Badge shows more consistent results across tasks when paired with better embeddings.

Conclusion: The performance of AL strategies is heavily influenced by the quality of embeddings and the specific context of the task, suggesting a need for careful, context-specific evaluation.

Abstract: The advent of large language models (LLMs) capable of producing
general-purpose representations lets us revisit the practicality of deep active
learning (AL): By leveraging frozen LLM embeddings, we can mitigate the
computational costs of iteratively fine-tuning large backbones. This study
establishes a benchmark and systematically investigates the influence of LLM
embedding quality on query strategies in deep AL. We employ five top-performing
models from the massive text embedding benchmark (MTEB) leaderboard and two
baselines for ten diverse text classification tasks. Our findings reveal key
insights: First, initializing the labeled pool using diversity-based sampling
synergizes with high-quality embeddings, boosting performance in early AL
iterations. Second, the choice of the optimal query strategy is sensitive to
embedding quality. While the computationally inexpensive Margin sampling can
achieve performance spikes on specific datasets, we find that strategies like
Badge exhibit greater robustness across tasks. Importantly, their effectiveness
is often enhanced when paired with higher-quality embeddings. Our results
emphasize the need for context-specific evaluation of AL strategies, as
performance heavily depends on embedding quality and the target task.

</details>


### [316] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
*Feng Wang,Yiding Sun,Jiaxin Mao,Wei Xue,Danqing Xu*

Main category: cs.CL

TL;DR: The paper introduces FinS-Pilot, a new benchmark for evaluating RAG systems in online financial applications. It incorporates real-time API data and structured text sources, covering critical financial domains. Through experiments with Chinese LLMs, it demonstrates effectiveness in identifying suitable models for financial applications.


<details>
  <summary>Details</summary>
Motivation: Current financial RAG benchmarks are constrained by data confidentiality issues and lack dynamic data integration.

Method: FinS-Pilot is constructed from real-world financial assistant interactions, incorporating both real-time API data and structured text sources, organized through an intent classification framework covering critical financial domains.

Result: Through systematic experiments with multiple Chinese leading LLMs, FinS-Pilot effectively identifies models suitable for financial applications.

Conclusion: The work contributes a practical evaluation framework and a curated dataset to advance research in financial NLP systems.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various professional domains, with their performance typically evaluated
through standardized benchmarks. However, the development of financial RAG
benchmarks has been constrained by data confidentiality issues and the lack of
dynamic data integration. To address this issue, we introduces FinS-Pilot, a
novel benchmark for evaluating RAG systems in online financial applications.
Constructed from real-world financial assistant interactions, our benchmark
incorporates both real-time API data and structured text sources, organized
through an intent classification framework covering critical financial domains
such as equity analysis and macroeconomic forecasting. The benchmark enables
comprehensive evaluation of financial assistants' capabilities in handling both
static knowledge and time-sensitive market information. Through systematic
experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's
effectiveness in identifying models suitable for financial applications while
addressing the current gap in specialized evaluation tools for the financial
domain. Our work contributes both a practical evaluation framework and a
curated dataset to advance research in financial NLP systems. The code and
dataset are accessible on
GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [317] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
*Duzhen Zhang,Yong Ren,Zhong-Zhi Li,Yahan Yu,Jiahua Dong,Chenxing Li,Zhilong Ji,Jinfeng Bai*

Main category: cs.CL

TL;DR: The paper proposes BranchLoRA, an asymmetric framework for Multimodal Continual Instruction Tuning (MCIT) that enhances efficiency and performance by introducing a tuning-freezing mechanism and task-specific routers. It outperforms MoELoRA in extensive experiments.


<details>
  <summary>Details</summary>
Motivation: Existing methods for continual instruction tuning of multimodal large language models suffer from catastrophic forgetting due to parameter inefficiency in the MoE LoRA framework.

Method: The authors propose BranchLoRA, which includes a flexible tuning-freezing mechanism to mitigate catastrophic forgetting, task-specific routers for optimal branch distribution over time, and a task selector for efficient inference without requiring task identity.

Result: BranchLoRA significantly outperforms MoELoRA across various MLLM sizes in extensive experiments on the latest MCIT benchmark.

Conclusion: BranchLoRA is an effective solution for enhancing both efficiency and performance in Multimodal Continual Instruction Tuning.

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal
Large Language Models (MLLMs) to continually align with human intent across
sequential tasks. Existing approaches often rely on the Mixture-of-Experts
(MoE) LoRA framework to preserve previous instruction alignments. However,
these methods are prone to Catastrophic Forgetting (CF), as they aggregate all
LoRA blocks via simple summation, which compromises performance over time. In
this paper, we identify a critical parameter inefficiency in the MoELoRA
framework within the MCIT context. Based on this insight, we propose
BranchLoRA, an asymmetric framework to enhance both efficiency and performance.
To mitigate CF, we introduce a flexible tuning-freezing mechanism within
BranchLoRA, enabling branches to specialize in intra-task knowledge while
fostering inter-task collaboration. Moreover, we incrementally incorporate
task-specific routers to ensure an optimal branch distribution over time,
rather than favoring the most recent task. To streamline inference, we
introduce a task selector that automatically routes test inputs to the
appropriate router without requiring task identity. Extensive experiments on
the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms
MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [318] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
*Dennis Fucci,Marco Gaido,Matteo Negri,Mauro Cettolo,Luisa Bentivogli*

Main category: cs.CL

TL;DR: This paper uses feature attribution technique to identify acoustic cues for a modern ASR system, enhancing model interpretability and highlighting areas for future research.


<details>
  <summary>Details</summary>
Motivation: To clarify the specific acoustic cues that modern ASR models rely on, expanding prior studies which were limited in phonemes and outdated models.

Method: Apply feature attribution technique to a Conformer-based ASR system, analyzing plosives, fricatives, and vowels to assess how attributions align with their acoustic properties in time and frequency domains.

Result: The ASR model depends on full time spans of vowels especially their first two formants and more so in male speech. It captures spectral characteristics of sibilant fricatives better than non-sibilants and prioritizes release phase in plosives.

Conclusion: These insights improve the interpretability of ASR models and indicate potential gaps in model robustness for further research.

Abstract: Despite significant advances in ASR, the specific acoustic cues models rely
on remain unclear. Prior studies have examined such cues on a limited set of
phonemes and outdated models. In this work, we apply a feature attribution
technique to identify the relevant acoustic cues for a modern Conformer-based
ASR system. By analyzing plosives, fricatives, and vowels, we assess how
feature attributions align with their acoustic properties in the time and
frequency domains, also essential for human speech perception. Our findings
show that the ASR model relies on vowels' full time spans, particularly their
first two formants, with greater saliency in male speech. It also better
captures the spectral characteristics of sibilant fricatives than non-sibilants
and prioritizes the release phase in plosives, especially burst
characteristics. These insights enhance the interpretability of ASR models and
highlight areas for future research to uncover potential gaps in model
robustness.

</details>


### [319] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
*Ella Rannon,David Burstein*

Main category: cs.CL

TL;DR: The paper reviews the application of NLP methods in biological sequence data, including tokenization strategies and model architectures. It highlights recent advances and the potential for extracting insights from large-scale genomic data.


<details>
  <summary>Details</summary>
Motivation: To explore how NLP methods can be adapted to analyze biological sequences and understand their potential in bioinformatics.

Method: Reviewing various NLP methods, tokenization strategies, and model architectures applied to genomics, transcriptomics, and proteomics data.

Result: NLP techniques show significant potential for structure prediction, gene expression analysis, and evolutionary studies using biological sequence data.

Conclusion: The integration of advanced language models into bioinformatics promises to enhance our understanding of biological processes across all domains of life.

Abstract: Natural Language Processing (NLP) has transformed various fields beyond
linguistics by applying techniques originally developed for human language to
the analysis of biological sequences. This review explores the application of
NLP methods to biological sequence data, focusing on genomics, transcriptomics,
and proteomics. We examine how various NLP methods, from classic approaches
like word2vec to advanced models employing transformers and hyena operators,
are being adapted to analyze DNA, RNA, protein sequences, and entire genomes.
The review also examines tokenization strategies and model architectures,
evaluating their strengths, limitations, and suitability for different
biological tasks. We further cover recent advances in NLP applications for
biological data, such as structure prediction, gene expression, and
evolutionary analysis, highlighting the potential of these methods for
extracting meaningful insights from large-scale genomic data. As language
models continue to advance, their integration into bioinformatics holds immense
promise for advancing our understanding of biological processes in all domains
of life.

</details>


### [320] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
*Thai Hoang,Kung-Hsiang Huang,Shirley Kokane,Jianguo Zhang,Zuxin Liu,Ming Zhu,Jake Grigsby,Tian Lan,Michael S Ryoo,Chien-Sheng Wu,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CL

TL;DR: LAM SIMULATOR is a framework that allows LLM Agents to explore tasks and generate high-quality training datasets for LAMs, leading to significant performance improvements with minimal human input.


<details>
  <summary>Details</summary>
Motivation: Large Action Models (LAMs) have great potential but are challenged by the requirement for high-quality training data, particularly for multi-step tasks.

Method: The framework includes a dynamic task query generator, a wide range of tools, and an interactive environment where LLM Agents can operate and receive real-time feedback. This allows for autonomous task exploration and solution finding, generating valuable action trajectory data.

Result: Experiments on ToolBench and CRMArena show models trained with self-generated datasets using the LAM SIMULATOR achieve up to 49.3% improvement over original baselines.

Conclusion: LAM SIMULATOR effectively reduces the need for human input in dataset creation and significantly enhances the development speed of AI agents.

Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face
challenges due to the need for high-quality training data, especially for
multi-steps tasks that involve planning, executing tool calls, and responding
to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive
framework designed for online exploration of agentic tasks with high-quality
feedback. Our framework features a dynamic task query generator, an extensive
collection of tools, and an interactive environment where Large Language Model
(LLM) Agents can call tools and receive real-time feedback. This setup enables
LLM Agents to explore and solve tasks autonomously, facilitating the discovery
of multiple approaches to tackle any given task. The resulting action
trajectory data are then used to create high-quality training datasets for
LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,
highlight the effectiveness of LAM SIMULATOR: models trained with
self-generated datasets using our framework achieve significant performance
gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR
requires minimal human input during dataset creation, highlighting LAM
SIMULATOR's efficiency and effectiveness in speeding up development of AI
agents.

</details>


### [321] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
*Russell Scheinberg,Ameeta Agrawal,Amber Shore,So Young Lee*

Main category: cs.CL

TL;DR: Grammar prompting, where a large LLM explains a syntactic phenomenon and feeds it back to a target model (LLM or SLM), improves sentence acceptability judgments on English, Chinese, and Russian benchmarks. This method significantly reduces the accuracy gap between LLMs and SLMs, especially when combined with chain-of-thought, allowing cost-effective SLMs to approach frontier-LLM performance in multilingual settings.


<details>
  <summary>Details</summary>
Motivation: Large language models can explain grammatical rules but often fail to apply them correctly when judging sentence acceptability. There is a need for a method that bridges the gap between knowing a rule and using it effectively.

Method: The 'grammar prompting' method involves an LLM first producing an explanation of the relevant syntactic phenomenon, which is then fed back as additional context to the target model (LLM or SLM) before making grammaticality decisions.

Result: Grammar prompting yields substantial improvements over strong baselines across many syntactic phenomena on the English BLiMP, Chinese SLING, and Russian RuBLiMP benchmarks. It reduces the average LLM-SLM accuracy gap by about 20% and, when paired with chain-of-thought, by 56%.

Conclusion: Grammar prompting is an effective, lightweight, and language-agnostic technique that allows low-cost SLMs to approach the performance of frontier-LLMs in multilingual settings.

Abstract: Large language models (LLMs) can explain grammatical rules, yet they often
fail to apply those rules when judging sentence acceptability. We present
"grammar prompting", an explain-then-process paradigm: a large LLM first
produces a concise explanation of the relevant syntactic phenomenon, then that
explanation is fed back as additional context to the target model -- either an
LLM or a smaller language model (SLM) -- before deciding which sentence of a
minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian
RuBLiMP benchmarks, this simple prompt design yields substantial improvements
over strong baselines across many syntactic phenomena. Feeding an LLM's
metalinguistic explanation back to the target model bridges the gap between
knowing a rule and using it. On SLMs, grammar prompting alone trims the average
LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by
56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,
language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in
multilingual settings.

</details>


### [322] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
*Berk Atil,Namrata Sureddy,Rebecca J. Passonneau*

Main category: cs.CL

TL;DR: This paper presents TRuST, a dataset for enhancing toxicity detection in online content. It evaluates LLMs on toxicity detection, finding fine-tuned models perform better but still have limitations.


<details>
  <summary>Details</summary>
Motivation: Toxicity in online content poses significant psychological and social risks, necessitating improved methods for its detection.

Method: Introduced TRuST, a dataset combining existing ones with labels for toxicity, target social group, and toxic spans across diverse groups. Benchmarked state-of-the-art LLMs on toxicity detection, target group identification, and toxic span extraction.

Result: Fine-tuned models outperform zero-shot and few-shot prompting, but performance is low for certain social groups. Reasoning capabilities do not significantly improve performance.

Conclusion: LLMs need improvement in social reasoning skills for effective toxicity detection.

Abstract: Toxicity in online content, including content generated by language models,
has become a critical concern due to its potential for negative psychological
and social impact. This paper introduces TRuST, a comprehensive dataset
designed to improve toxicity detection that merges existing datasets, and has
labels for toxicity, target social group, and toxic spans. It includes a
diverse range of target groups such as ethnicity, gender, religion, disability,
and politics, with both human/machine-annotated and human machine-generated
data. We benchmark state-of-the-art large language models (LLMs) on toxicity
detection, target group identification, and toxic span extraction. We find that
fine-tuned models consistently outperform zero-shot and few-shot prompting,
though performance remains low for certain social groups. Further, reasoning
capabilities do not significantly improve performance, indicating that LLMs
have weak social reasoning skills.

</details>


### [323] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
*Jeonghun Kang,Soonmok Kwon,Joonseok Lee,Byung-Hak Kim*

Main category: cs.CL

TL;DR: An LLM-driven agent named DIAMOND is introduced for context-aware baseball highlight summarization. It integrates structured sports analytics with natural language reasoning, surpassing purely statistical or vision-based systems. Evaluated on five Korean Baseball Organization League games, it improves F1-score from 42.9% to 84.8%.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches like WPA-based ranking or computer vision-driven event detection often miss strategic depth, momentum shifts, and storyline progression.

Method: DIAMOND leverages sabermetric features such as Win Expectancy, WPA, and Leverage Index to quantify play importance, while an LLM module enhances selection based on contextual narrative value.

Result: Evaluated on five diverse Korean Baseball Organization League games, DIAMOND improves F1-score from 42.9% (WPA-only) to 84.8%, outperforming both commercial and statistical baselines.

Conclusion: The results highlight the potential of modular, interpretable agent-based frameworks for event-level summarization in sports and beyond.

Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking
or computer vision-driven event detection -- can identify scoring plays but
often miss strategic depth, momentum shifts, and storyline progression. Manual
curation remains the gold standard but is resource-intensive and not scalable.
We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight
summarization that integrates structured sports analytics with natural language
reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and
Leverage Index -- to quantify play importance, while an LLM module enhances
selection based on contextual narrative value. This hybrid approach ensures
both quantitative rigor and qualitative richness, surpassing the limitations of
purely statistical or vision-based systems. Evaluated on five diverse Korean
Baseball Organization League games, DIAMOND improves F1-score from 42.9%
(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.
Though limited in scale, our results highlight the potential of modular,
interpretable agent-based frameworks for event-level summarization in sports
and beyond.

</details>


### [324] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
*Ukyo Honda,Tatsushi Oka*

Main category: cs.CL

TL;DR: The paper presents X²-ICL, an advanced framework extending X-ICL by exploring explanations for all possible labels to enhance robustness in in-context learning with large language models.


<details>
  <summary>Details</summary>
Motivation: In-context learning often struggles to generalize beyond the distribution of provided demonstrations. To address this limitation, researchers have developed ICL with explanations (X-ICL) which improves prediction reliability by guiding LLMs to understand and articulate reasoning behind correct labels.

Method: The method involves systematically exploring explanations for all possible labels in a framework called X²-ICL, thereby enabling more comprehensive and robust decision-making.

Result: Experimental results on multiple natural language understanding datasets validate the effectiveness of X²-ICL, showing significantly improved robustness to out-of-distribution data compared to existing ICL approaches.

Conclusion: X²-ICL is an effective enhancement to ICL, providing better generalization and robustness in large language models.

Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging
large language models (LLMs). However, it often struggles to generalize beyond
the distribution of the provided demonstrations. A recent advancement in
enhancing robustness is ICL with explanations (X-ICL), which improves
prediction reliability by guiding LLMs to understand and articulate the
reasoning behind correct labels. Building on this approach, we introduce an
advanced framework that extends X-ICL by systematically exploring explanations
for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and
robust decision-making. Experimental results on multiple natural language
understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating
significantly improved robustness to out-of-distribution data compared to the
existing ICL approaches.

</details>


### [325] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
*Chuanghao Ding,Jiaping Wang,Ziqing Yang,Xiaoliang Wang,Dahua Lin,Cam-Tu Nguyen,Fei Tan*

Main category: cs.CL

TL;DR: CD是一种新的协同机制，通过使用LLM计算的token级似然性验证候选草稿，相比SD方法，提高了推理速度并降低了大目标模型的调用频率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于推测解码（SD）的方法虽然能加速大语言模型（LLMs）推理，但其高拒绝率需要反复调用LLMs进行验证，影响了整体效率提升。

Method: 提出了一种新的协同机制Consultant Decoding (CD)，不同于依赖重要性采样指标进行验证的SD，CD利用LLM单独计算的token级似然性来验证候选草稿。

Result: CD在保持与目标模型相近生成质量的同时，将推理速度提高至2.5倍，并将大目标模型的调用频率降低到10%以下，特别是在更复杂的任务中。此外，CD的表现甚至超过了理论上推测解码的上限——大目标模型本身。

Conclusion: Consultant Decoding (CD) 作为一种新型协同机制，不仅显著提升了推理速度，还减少了对大模型的依赖，同时在某些情况下超越了大目标模型的表现。

Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.

</details>


### [326] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
*Yilin Xiao,Junnan Dong,Chuang Zhou,Su Dong,Qianwen Zhang,Di Yin,Xing Sun,Xiao Huang*

Main category: cs.CL

TL;DR: GraphRAG-Bench是一个大规模、领域特定的基准，用于严格评估GraphRAG模型。它具有挑战性的问题设计、多样化的任务覆盖和全面的评估框架。通过应用九种现代GraphRAG方法到GraphRAG-Bench，展示了其在量化图结构如何提高模型推理能力方面的实用性。


<details>
  <summary>Details</summary>
Motivation: 当前对GraphRAG模型的评估主要依赖于传统的问答数据集，其问题和评估指标的范围有限，无法全面评估GraphRAG模型带来的推理能力提升。

Method: 引入了GraphRAG-Bench，一个大规模、领域特定的基准，包含具有挑战性的大学水平、领域特定的问题，涵盖广泛的推理任务和16个学科的二十本核心教科书。提供全面的评估框架，包括图构建、知识检索和答案生成。

Result: 通过应用九种现代GraphRAG方法到GraphRAG-Bench，展示了其实用性，并揭示了关于图架构、检索效率和推理能力的关键见解。

Conclusion: GraphRAG-Bench为研究社区提供了可操作的指导，以改进GraphRAG模型的研究和开发。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [327] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
*Zhengyuan Liu,Geyu Lin,Hui Li Tan,Huayun Zhang,Yanfeng Lu,Xiaoxue Gao,Stella Xin Yin,He Sun,Hock Huan Goh,Lung Hsiang Wong,Nancy F. Chen*

Main category: cs.CL

TL;DR: The paper introduces SingaKids, a dialogic tutor that integrates dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation to create an immersive language learning environment for young learners.


<details>
  <summary>Details</summary>
Motivation: There is a need for kids-friendly design in AI educational applications that can maintain motivation and optimize learning outcomes. Current systems face challenges in ensuring consistent and robust performance across different languages and cultural contexts.

Method: SingaKids was developed by integrating dense image captioning, multilingual dialogic interaction, speech understanding, and engaging speech generation. The system was further improved through multilingual pre-training, task-specific tuning, and scaffolding optimization.

Result: Empirical studies with elementary school students demonstrate that SingaKids provides effective dialogic teaching, benefiting learners at different performance levels.

Conclusion: SingaKids offers an effective solution for promoting language acquisition among young learners across multiple languages and cultural contexts.

Abstract: The integration of generative artificial intelligence into educational
applications has enhanced personalized and interactive learning experiences,
and it shows strong potential to promote young learners language acquisition.
However, it is still challenging to ensure consistent and robust performance
across different languages and cultural contexts, and kids-friendly design
requires simplified instructions, engaging interactions, and age-appropriate
scaffolding to maintain motivation and optimize learning outcomes. In this
work, we introduce SingaKids, a dialogic tutor designed to facilitate language
learning through picture description tasks. Our system integrates dense image
captioning, multilingual dialogic interaction, speech understanding, and
engaging speech generation to create an immersive learning environment in four
languages: English, Mandarin, Malay, and Tamil. We further improve the system
through multilingual pre-training, task-specific tuning, and scaffolding
optimization. Empirical studies with elementary school students demonstrate
that SingaKids provides effective dialogic teaching, benefiting learners at
different performance levels.

</details>


### [328] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
*Maryam Berijanian,Kuldeep Singh,Amin Sehati*

Main category: cs.CL

TL;DR: This paper compares three AI agent architectures for relation classification using LLMs, finding that multi-agent coordination performs well.


<details>
  <summary>Details</summary>
Motivation: Entity relationship classification is challenging, especially with limited labeled data and complex relational structures.

Method: Three distinct AI agent architectures are explored: reflective self-evaluation, hierarchical task decomposition, and a novel multi-agent dynamic example generation mechanism. The latter includes real-time cooperative and adversarial prompting.

Result: Multi-agent coordination outperforms standard few-shot prompting and approaches the performance of fine-tuned models across multiple domains and model backends.

Conclusion: The findings provide practical guidance for designing modular, generalizable LLM-based systems for structured relation extraction.

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at
\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [329] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
*Xiang Li,Jiayi Xin,Qi Long,Weijie J. Su*

Main category: cs.CL

TL;DR: The paper presents KnowSum, a statistical framework to assess large language models (LLMs) by quantifying their unseen knowledge, demonstrating its utility in three applications and revealing significant differences in model rankings based on internal knowledge.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of LLMs often inconsistently reflect the actual capacities of these models, partially due to the oversight of unseen knowledge - information encoded by LLMs but not directly observed during evaluations.

Method: Introduced KnowSum, a statistical framework that estimates the unobserved portion of knowledge by extrapolating from the appearance frequencies of observed knowledge instances for a class of evaluation tasks.

Result: Experiments showed that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. KnowSum significantly changes comparative rankings for several common LLMs based on their internal knowledge.

Conclusion: KnowSum provides a more comprehensive assessment of LLMs by accounting for unseen knowledge, impacting how we understand and compare these models.

Abstract: Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.

</details>


### [330] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
*Zhaorui Yang,Bo Pan,Han Wang,Yiyao Wang,Xingyu Liu,Minfeng Zhu,Bo Zhang,Wei Chen*

Main category: cs.CL

TL;DR: Visualizations are key in communication. While LLMs can generate comprehensive text reports, the generation of combined texts and visualizations is underexplored. This paper proposes FDV, a textual representation of charts, and Multimodal DeepResearcher, a framework for generating multimodal reports. Evaluation shows its effectiveness with an 82% win rate over baseline.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of existing deep research frameworks which mainly focus on text-only content generation, neglecting the potential of automatically generating combined texts and visualizations.

Method: The method involves proposing FDV, a structured textual representation of charts, and introducing Multimodal DeepResearcher, a four-stage agentic framework for generating multimodal reports. The stages include researching, exemplar report textualization, planning, and multimodal report generation.

Result: The results show the effectiveness of Multimodal DeepResearcher through extensive experiments. Specifically, it achieves an 82% overall win rate over the baseline method when using the same Claude 3.7 Sonnet model.

Conclusion: The conclusion is that FDV and Multimodal DeepResearcher provide a promising approach to generating high-quality, informative visualizations integrated with text reports, advancing the field of automated multimodal content generation.

Abstract: Visualizations play a crucial part in effective communication of concepts and
information. Recent advances in reasoning and retrieval augmented generation
have enabled Large Language Models (LLMs) to perform deep research and generate
comprehensive reports. Despite its progress, existing deep research frameworks
primarily focus on generating text-only content, leaving the automated
generation of interleaved texts and visualizations underexplored. This novel
task poses key challenges in designing informative visualizations and
effectively integrating them with text reports. To address these challenges, we
propose Formal Description of Visualization (FDV), a structured textual
representation of charts that enables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this representation, we introduce
Multimodal DeepResearcher, an agentic framework that decomposes the task into
four stages: (1) researching, (2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the evaluation of generated
multimodal reports, we develop MultimodalReportBench, which contains 100
diverse topics served as inputs along with 5 dedicated metrics. Extensive
experiments across models and evaluation methods demonstrate the effectiveness
of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet
model, Multimodal DeepResearcher achieves an 82\% overall win rate over the
baseline method.

</details>


### [331] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
*Inderjeet Nair,Lu Wang*

Main category: cs.CL

TL;DR: 研究发现，从简短形式测试中推断出的价值偏好与长篇输出中表达的价值偏好之间存在弱相关性，不同长篇生成设置之间的偏好相关性也很弱，对齐只能适度提高价值表达的一致性。此外，论点的具体性与偏好强度呈负相关，而跨场景的表现则呈正相关。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLMs）的伦理风险和价值观倾向通常依赖于简短形式的调查和心理测量测试，但在实际使用中涉及长篇、开放式回答，这使得在实际环境中的价值相关风险和偏好尚未得到充分探讨。

Method: 比较从简短形式反应和长篇回答中得出的价值偏好，改变后者的论点数量以捕捉用户的冗长偏好，并分析五种LLMs（llama3-8b, gemma2-9b, mistral-7b, qwen2-7b 和 olmo-7b）。

Result: (1) 在变化的论点数量下，从简短形式和长篇回答中推断出的价值偏好之间存在弱相关性；(2) 任何两种不同的长篇生成设置之间的偏好也存在类似的弱相关性；(3) 对齐只能适度提高价值表达的一致性。进一步研究表明，论点具体性与偏好强度呈负相关，而跨场景表现则呈正相关。

Conclusion: 需要更强大的方法来确保在各种应用中一致地表达价值。

Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on
short-form surveys and psychometric tests, yet real-world use involves
long-form, open-ended responses -- leaving value-related risks and preferences
in practical settings largely underexplored. In this work, we ask: Do value
preferences inferred from short-form tests align with those expressed in
long-form outputs? To address this question, we compare value preferences
elicited from short-form reactions and long-form responses, varying the number
of arguments in the latter to capture users' differing verbosity preferences.
Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),
we find (1) a weak correlation between value preferences inferred from
short-form and long-form responses across varying argument counts, and (2)
similarly weak correlation between preferences derived from any two distinct
long-form generation settings. (3) Alignment yields only modest gains in the
consistency of value expression. Further, we examine how long-form generation
attributes relate to value preferences, finding that argument specificity
negatively correlates with preference strength, while representation across
scenarios shows a positive correlation. Our findings underscore the need for
more robust methods to ensure consistent value expression across diverse
applications.

</details>


### [332] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
*Junzhe Zhang,Huixuan Zhang,Xinyu Hu,Li Lin,Mingqi Gao,Shi Qiu,Xiaojun Wan*

Main category: cs.CL

TL;DR: 本研究提出Minos-Corpus，一个大规模的多模态评估数据集，并基于此开发了Minos模型，该模型在文本到图像生成任务的评估中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型（MLLMs）在构建通用评估系统方面的应用忽视了文本到图像生成任务的评估能力和大规模人类评估数据的整合。

Method: 引入Minos-Corpus，一个结合了人类和GPT评估数据的大规模多模态评估数据集，涵盖图像到文本和文本到图像生成任务。基于此数据集，提出数据选择与平衡、Mix-SFT训练方法，并应用DPO开发Minos模型，该模型以7B参数量为骨干。

Result: Minos在所有开源评估模型中，在所有任务的平均评估性能上达到最佳水平，并且在文本到图像生成任务的评估上超越所有开源和闭源模型。

Conclusion: 实验结果表明，利用高质量的人类评估数据和同时在图像到文本及文本到图像生成任务的评估数据上进行联合训练的重要性。

Abstract: Evaluation is important for multimodal generation tasks. With the rapid
progress of MLLMs, there is growing interest in applying MLLMs to build general
evaluation systems. However, existing work overlooks two aspects: (1) the
development of evaluation capabilities for text-to-image (T2I) generation task,
and (2) the incorporation of large-scale human evaluation data. In this paper,
we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that
combines evaluation data from both human and GPT. The corpus contains
evaluation data across both image-to-text(I2T) and T2I generation tasks. Based
on this corpus, we propose Data Selection and Balance, Mix-SFT training
methods, and apply DPO to develop Minos, a multimodal evaluation model built
upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among
all open-source evaluation models of similar scale on the average of evaluation
performance on all tasks, and outperforms all open-source and closed-source
models on evaluation of T2I generation task. Extensive experiments demonstrate
the importance of leveraging high-quality human evaluation data and jointly
training on evaluation data from both I2T and T2I generation tasks.

</details>


### [333] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
*Jie Zhu,Junhui Li,Yalong Wen,Xiandong Li,Lifan Guo,Feng Chen*

Main category: cs.CL

TL;DR: 提出了一个名为M³FinMeeting的新基准，用于评估大型语言模型在金融会议理解中的表现。该基准是多语言、多行业和多任务的，涵盖了总结、问答对提取和问答任务。实验表明，即使是最先进的长上下文模型也有很大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 当前的金融基准通常依赖于新闻文章、收益报告或公告，难以捕捉金融会议的真实世界动态。

Method: 创建了一个名为M³FinMeeting的数据集，支持英语、中文和日语，涵盖由全球行业分类标准（GICS）定义的各种行业部门，并包括总结、问答对提取和问答三个任务。

Result: 使用七个流行的大型语言模型进行的实验结果表明，即使是最先进的长上下文模型也存在显著的改进空间。

Conclusion: M³FinMeeting作为一个有效的基准，可以用来评估大型语言模型在金融会议理解方面的能力。

Abstract: Recent breakthroughs in large language models (LLMs) have led to the
development of new benchmarks for evaluating their performance in the financial
domain. However, current financial benchmarks often rely on news articles,
earnings reports, or announcements, making it challenging to capture the
real-world dynamics of financial meetings. To address this gap, we propose a
novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual,
multi-sector, and multi-task dataset designed for financial meeting
understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and
Japanese, enhancing comprehension of financial discussions in diverse
linguistic contexts. Second, it encompasses various industry sectors defined by
the Global Industry Classification Standard (GICS), ensuring that the benchmark
spans a broad range of financial activities. Finally,
$\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer
(QA) pair extraction, and question answering, facilitating a more realistic and
comprehensive evaluation of understanding. Experimental results with seven
popular LLMs reveal that even the most advanced long-context models have
significant room for improvement, demonstrating the effectiveness of
$\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting
comprehension skills.

</details>


### [334] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
*Zhuohan Xie,Dhruv Sahnan,Debopriyo Banerjee,Georgi Georgiev,Rushil Thareja,Hachem Madmoun,Jinyan Su,Aaryamonvikram Singh,Yuxia Wang,Rui Xing,Fajri Koto,Haonan Li,Ivan Koychev,Tanmoy Chakraborty,Salem Lahlou,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

TL;DR: Multi-step symbolic reasoning is crucial for financial tasks, yet lacks systematic evaluation. This paper introduces FinChain, a benchmark for verifiable Chain-of-Thought financial reasoning, and ChainEval, a metric for evaluating both final answers and intermediate reasoning. Benchmarking 30 LLMs revealed significant room for improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the lack of benchmarks for systematically evaluating multi-step symbolic reasoning in financial tasks.

Method: The method involves introducing FinChain, a symbolic benchmark designed for verifiable Chain-of-Thought financial reasoning. It spans 54 topics across 12 financial domains with five parameterized templates per topic, each varying in complexity and domain expertise required. Each instance includes an executable Python trace for generating training data and adapting to other domains. Additionally, ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning, is introduced.

Result: Benchmarking 30 LLMs on the FinChain dataset revealed that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning.

Conclusion: FinChain and ChainEval provide valuable tools for advancing multi-step financial reasoning in language models.

Abstract: Multi-step symbolic reasoning is critical for advancing downstream
performance on financial tasks. Yet, benchmarks for systematically evaluating
this capability are lacking. Existing datasets like FinQA and ConvFinQA
supervise only final numerical answers, without assessing intermediate
reasoning steps. To address this, we introduce FinChain, the first symbolic
benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.
Spanning 54 topics across 12 financial domains, Fin- Chain offers five
parameterized templates per topic, each varying in reasoning complexity and
domain expertise required. Each dataset instance includes an executable Python
trace, enabling automatic generation of extensive training data and easy
adaptation to other domains. We also introduce ChainEval, a new metric for
automatic evaluation of both final answers and intermediate reasoning.
Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models
have considerable room for improvement in multi-step financial reasoning. All
templates and evaluation metrics for FinChain are available at https:
//github.com/mbzuai-nlp/finchain.

</details>


### [335] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
*Yingying Zhuang,Aman Gupta,Anurag Beniwal*

Main category: cs.CL

TL;DR: This paper proposes a novel strategy to fine-tune multilingual embedding models with weighted sampling for contrastive learning, which improves multilingual information retrieval performance significantly.


<details>
  <summary>Details</summary>
Motivation: Multilingual information retrieval is crucial for expanding knowledge sharing across languages. However, high quality knowledge bases are often scarce and limited in languages. An effective embedding model is needed to transform sentences from different languages into the same feature vector space as the knowledge base language, especially for transferring knowledge from high-resource languages to low-resource ones.

Method: The authors propose a novel strategy to fine-tune multilingual embedding models with weighted sampling for contrastive learning, enabling multilingual information retrieval with a monolingual knowledge base.

Result: The weighted sampling strategy produces significant performance gains compared to standard methods, improving Mean Reciprocal Rank (MRR) by up to 31.03% and Recall@3 by up to 33.98%. The proposed methodology is language agnostic and applicable for both multilingual and code switching use cases.

Conclusion: The novel strategy of fine-tuning multilingual embedding models with weighted sampling for contrastive learning effectively enhances multilingual information retrieval performance, facilitating cross-language knowledge sharing.

Abstract: Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.

</details>


### [336] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
*Michael Li,Nishant Subramani*

Main category: cs.CL

TL;DR: 尽管架构、规模和训练方式不同，16种语言模型在表示词汇身份和形态变化信息时展现出一致的模式：早期层线性集中词汇信息，后期层非线性增加，而形态变化信息在线性可分的情况下保持均匀可访问。这些模式表明，变压器模型可能以类似的方式组织语言信息，这种特性可能是基础性的，并在预训练早期就被学习到。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型如何编码语言信息的理解仍基于较早的模型（如BERT和GPT-2）。为了更好地理解现代语言模型的工作机制，研究者决定深入探讨经典与当代大型语言模型在表示词汇身份和词形变化方面的差异和共性。

Method: 通过训练线性和非线性分类器对各层激活进行预测，具体任务包括预测单词词干和词形特征。研究涵盖了从经典模型（如BERT, DeBERTa, GPT-2）到当代大型语言模型（如Pythia, OLMo-2, Gemma-2, Qwen2.5, Llama-3.1）的各种模型。

Result: 研究发现，所有测试模型都表现出相似的模式：词汇信息在线性早期层中集中，随后在后期层中逐渐变为非线性处理；而词形变化信息在整个层中保持均匀可访问且线性可分。此外，模型通过概括性抽象编码词形变化，但主要依赖记忆来编码词汇身份。

Conclusion: 这些结果表明，尽管大型语言模型技术取得了显著进展，但变压器模型在组织语言信息方面仍然遵循类似的模式。这些特性可能对于下一个标记预测至关重要，并且在预训练早期阶段就已经被学习。

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing.

</details>


### [337] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Main category: cs.CL

TL;DR: MMRAG被引入以增强多模态大语言模型，但存在知识不一致的问题。本文提出CoRe-MMRAG框架解决这些问题，通过四阶段流程和专门的训练范式提高性能。实验结果表明，该方法在多个基准上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: Multimodal Retrieval-Augmented Generation (MMRAG) 虽然增强了多模态大语言模型，但带来了Parametric-Retrieved Knowledge Inconsistency (PRKI) 和 Visual-Textual Knowledge Inconsistency (VTKI) 两个挑战，需要一种新的框架来解决这些不一致性问题。

Method: 提出了一种名为CoRe-MMRAG的新框架，包含四个阶段：从参数化知识生成内部响应、通过联合相似性评估选择最相关的多模态证据、生成外部响应以及整合内外部响应生成可靠答案。此外，还引入了专门的训练范式以增强知识源判别、多模态集成和统一答案生成能力。

Result: 在KB-VQA基准上的实验表明，CoRe-MMRAG相比基线方法取得了显著改进，分别在InfoSeek和Encyclopedic-VQA上获得了5.6%和9.3%的性能提升。

Conclusion: CoRe-MMRAG有效地解决了多模态知识不一致性问题，并在多模态问答任务中表现出色。代码和数据已公开发布。

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge
\textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG}
(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles
inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage
pipeline: it first generates an internal response from parametric knowledge,
then selects the most relevant multimodal evidence via joint similarity
assessment, generates an external response, and finally integrates both to
produce a reliable answer. Additionally, a specialized training paradigm
enhances knowledge source discrimination, multimodal integration, and unified
answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG
achieves substantial improvements over baseline methods, achieving 5.6\% and
9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We
release code and data at
\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [338] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
*Yirao Zhao,Guizhen Chen,Kenji Kawaguchi,Lidong Bing,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 设计了一种名为$	exttt{Cus-Prun}$的自定义剪枝方法，能够将大型通用模型修剪为更小、更轻量级的专家模型，且无需任何后训练，在多个模型家族和尺寸的模型上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的剪枝方法主要关注保留模型的一般能力，通常需要大量的后训练或因粗粒度剪枝导致性能下降，无法很好地满足特定下游场景的需求。

Method: 通过识别并剪除在“语言”、“领域”和“任务”三个维度上无关的神经元，将大型通用模型修剪为更小、更轻量级的专家模型。

Result: 实验表明，$	exttt{Cus-Prun}$方法在不同模型家族和尺寸的模型上均能保持最小的能力损失，并且在专家能力和一般能力方面都优于其他方法。

Conclusion: $	exttt{Cus-Prun}$方法能够在不进行任何后训练的情况下创建专家模型，有效地减少了参数冗余，加速了推理速度，同时保持了模型性能。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing pruning methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained pruning. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.

</details>


### [339] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
*Sarenne Wallbridge,Christoph Minixhofer,Catherine Lai,Peter Bell*

Main category: cs.CL

TL;DR: 本研究利用自监督学习（SSL）探讨语音韵律结构的时间粒度，提出一种掩码韵律模型，发现其对长时结构如情感识别有较大价值，且优于传统特征。


<details>
  <summary>Details</summary>
Motivation: 尽管人们在文本理解中利用词汇结构的可预测性，但韵律（如语调、节奏和响度）在语音中的可预测结构独立于词汇内容的程度尚不清楚。

Method: 研究采用自监督学习方法，通过提出的掩码韵律模型（Masked Prosody Model）分析韵律的声学相关结构，并通过探测实验比较不同感知标签下的表现。

Result: 该模型在涉及长时结构的任务（如情感识别）中表现最佳，相比未变换的音高、能量和发音活动特征有显著相对增益。

Conclusion: 研究揭示了SSL训练目标时间尺度的重要性，强调了复杂SSL编码结构的价值。

Abstract: People exploit the predictability of lexical structures during text
comprehension. Though predictable structure is also present in speech, the
degree to which prosody, e.g. intonation, tempo, and loudness, contributes to
such structure independently of the lexical content is unclear. This study
leverages self-supervised learning (SSL) to examine the temporal granularity of
structures in the acoustic correlates of prosody. Representations from our
proposed Masked Prosody Model can predict perceptual labels dependent on local
information, such as word boundaries, but provide the most value for labels
involving longer-term structures, like emotion recognition. Probing experiments
across various perceptual labels show strong relative gains over untransformed
pitch, energy, and voice activity features. Our results reveal the importance
of SSL training objective timescale and highlight the value of complex
SSL-encoded structures compared to more constrained classical structures.

</details>


### [340] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
*Maria Levchenko*

Main category: cs.CL

TL;DR: This paper focuses on Named Entity Recognition (NER) for person names in Russian news texts about cultural events, using the SPbLitGuide dataset. It compares various NER models including transformer-based architectures and LLMs, with GPT-4o showing superior performance when given specific JSON prompting, achieving an F1 score of 0.93. Follow-up evaluation with GPT-4.1 shows further improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Named Entity Recognition specifically for person names within Russian news texts concerning cultural events, utilizing a specialized dataset covering event announcements from Saint Petersburg over two decades.

Method: Conducting a comparative evaluation of diverse NER models such as DeepPavlov, RoBERTa, SpaCy, GPT-3.5, GPT-4, and GPT-4o using the SPbLitGuide dataset, focusing on their performance metrics like precision and F1 scores.

Result: GPT-4o achieved the highest F1 score of 0.93 with specific JSON prompting, while GPT-4 demonstrated the highest precision at 0.99. Further evaluation with GPT-4.1 resulted in an F1 score of 0.94 for both simple and structured prompts.

Conclusion: The study highlights the capabilities and limitations of current NER models when applied to morphologically rich languages like Russian in the cultural heritage domain, providing valuable insights for researchers and practitioners.

Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.

</details>


### [341] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
*Fan Gao,Dongyuan Li,Ding Xia,Fei Mi,Yasheng Wang,Lifeng Shang,Baojun Wang*

Main category: cs.CL

TL;DR: The paper introduces \benchName, a new benchmark for Chinese essay writing and evaluation across four genres using 728 real-world prompts. It includes a fine-grained scoring framework and validates it through human agreement studies. The benchmark was used to evaluate 15 large LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for evaluating Chinese essay writing lack depth in considering the structural and rhetorical complexities, especially across different genres. This research aims to address this gap.

Method: The researchers developed \benchName, a multi-genre benchmark consisting of Argumentative, Narrative, Descriptive, and Expository essays. They curated 728 real-world prompts categorized into Open-Ended and Constrained sets. A fine-grained, genre-specific scoring framework was created to hierarchically aggregate scores. Human agreement studies were conducted to validate the evaluation protocol. Finally, 15 large-sized LLMs were benchmarked against this new standard.

Result: \benchName provides a more reliable and detailed method for evaluating Chinese essay writing by LLMs. The study identified the strengths and limitations of these models across different genres and instruction types.

Conclusion: The introduction of \benchName advances the field of LLM-based Chinese essay evaluation and encourages further research on improving essay generation in educational contexts.

Abstract: Chinese essay writing and its evaluation are critical in educational
contexts, yet the capabilities of Large Language Models (LLMs) in this domain
remain largely underexplored. Existing benchmarks often rely on coarse-grained
text quality metrics, largely overlooking the structural and rhetorical
complexities of Chinese essays, particularly across diverse genres. To address
this gap, we propose \benchName, a multi-genre benchmark specifically designed
for Chinese essay writing across four major genres: Argumentative, Narrative,
Descriptive, and Expository. We curate and refine a total of 728 real-world
prompts to ensure authenticity and meticulously categorize them into the
\textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing
scenarios. To reliably evaluate generated essays, we develop a fine-grained,
genre-specific scoring framework that hierarchically aggregates scores. We
further validate our evaluation protocol through a comprehensive human
agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their
strengths and limitations across genres and instruction types. With \benchName,
we aim to advance LLM-based Chinese essay evaluation and inspire future
research on improving essay generation in educational settings.

</details>


### [342] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
*Shihan Dou,Ming Zhang,Chenhao Huang,Jiayi Chen,Feng Chen,Shichun Liu,Yan Liu,Chenxiao Liu,Cheng Zhong,Zongzhang Zhang,Tao Gui,Chao Xin,Wei Chengzhi,Lin Yan,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: The paper introduces EvaLearn, a benchmark for evaluating large language models' learning capability and efficiency through sequential problem-solving. It contains 648 problems across six task types and provides five automated metrics. The study benchmarks nine models, revealing varied performance profiles and learning abilities.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored aspect of model potential in terms of learning capability and efficiency in challenging tasks.

Method: EvaLearn is composed of 648 challenging problems across six task types, grouped into sequences. Models solve problems sequentially to leverage prior experience. Five comprehensive automated metrics are provided to evaluate learning capability and efficiency.

Result: Benchmarking of nine frontier models revealed varied performance profiles with some models showing strong learning ability while others struggled or exhibited negative transfer. Current LLMs with stronger static abilities do not necessarily have an advantage in learning capability.

Conclusion: EvaLearn evaluates a new dimension of model performance and offers a novel perspective for assessing LLM potential, highlighting the gap between models and human capabilities.

Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.

</details>


### [343] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
*Qihang Yan,Xinyu Zhang,Luming Guo,Qi Zhang,Feifan Liu*

Main category: cs.CL

TL;DR: RACE-Align is a novel framework that improves LLMs' accuracy, reasoning, and interpretability in vertical domains by constructing preference data with external knowledge and CoT reasoning. It outperforms base models in TCM experiments.


<details>
  <summary>Details</summary>
Motivation: Large Language Models face challenges such as accuracy, domain-specific reasoning, and interpretability in specialized fields. Traditional alignment methods fail to consider underlying knowledge sources and reasoning logic.

Method: The RACE-Align framework constructs binary preference datasets using external knowledge and Chain-of-Thought reasoning, aligning LLMs via the DPO algorithm. It integrates AI-driven retrieval for factual grounding and optimizes domain-specific CoT reasoning.

Result: In TCM experiments using Qwen3-1.7B, RACE-Align significantly enhances answer accuracy, information richness, TCM thinking patterns application, logical reasoning depth, and interpretability compared to the base model and SFT fine-tuned model.

Conclusion: RACE-Align provides an effective approach to improve LLMs' knowledge application, reasoning reliability, and process transparency in complex vertical domains.

Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific
reasoning, and interpretability in vertical domains. Traditional preference
alignment methods like Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) often overlook the underlying knowledge
sources and reasoning logic. This paper introduces RACE-Align
(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel
framework designed to address these limitations. RACE-Align systematically
constructs a binary preference dataset incorporating external knowledge support
and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO
algorithm. The core innovation lies in its preference data construction
strategy: it integrates AI-driven retrieval for factual grounding, enhancing
knowledgeability and accuracy, and emphasizes the optimization of
domain-specific CoT, treating the reasoning process itself as a key preference
dimension. A multi-stage, AI-driven refinement pipeline cost-effectively
generates these preference pairs. Experimental validation in Traditional
Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that
RACE-Align significantly outperforms the original base model and a model
fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed
across multiple dimensions, including answer accuracy, information richness,
application of TCM thinking patterns, logicality and depth of reasoning, and
interpretability. These findings suggest RACE-Align offers an effective pathway
to enhance LLMs' knowledge application, reasoning reliability, and process
transparency in complex vertical domains.

</details>


### [344] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
*Stefano Bannò,Kate Knill,Mark Gales*

Main category: cs.CL

TL;DR: This paper introduces a new method combining large language models (LLMs) and the English Vocabulary Profile (EVP) to evaluate vocabulary use in second language learner writing, showing LLMs' effectiveness in this task.


<details>
  <summary>Details</summary>
Motivation: Current automated systems for assessing vocabulary use in second language learners typically focus on context-independent aspects. This paper aims to improve evaluation by considering the precise use of words within sentences.

Method: The scheme combines large language models (LLMs) with the English Vocabulary Profile (EVP). It evaluates LLMs' ability to assign proficiency levels to individual words in L2 learner writing, comparing their performance to a part-of-speech (PoS)-based baseline.

Result: LLMs demonstrate improved performance due to their use of additional semantic information. Correlations between word-level and essay-level proficiency are explored, and the consistency of EVP proficiency levels is examined.

Conclusion: LLMs are found to be well-suited for the task of vocabulary assessment in second language learner writing.

Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency.
To date, its assessment by automated systems has typically examined the
context-independent, or part-of-speech (PoS) related use of words. This paper
introduces a novel approach to enable fine-grained vocabulary evaluation
exploiting the precise use of words within a sentence. The scheme combines
large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP
is a standard lexical resource that enables in-context vocabulary use to be
linked with proficiency level. We evaluate the ability of LLMs to assign
proficiency levels to individual words as they appear in L2 learner writing,
addressing key challenges such as polysemy, contextual variation, and
multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to
exploit additional semantic information that yields improved performance. We
also explore correlations between word-level proficiency and essay-level
proficiency. Finally, the approach is applied to examine the consistency of the
EVP proficiency levels. Results show that LLMs are well-suited for the task of
vocabulary assessment.

</details>


### [345] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
*Ekaterina Grishina,Mikhail Gorbunov,Maxim Rakhuba*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）尽管在自然语言处理任务中表现出色，但需要大量的计算和内存资源。结构化矩阵表示是减少这些模型参数数量的一种有希望的方法。然而，预训练模型的权重矩阵在没有任何微调的情况下，似乎无法被精确地表示为结构化矩阵。为了解决这个问题，我们利用了LLM输出在权重矩阵的某些正交变换下保持不变的事实。这一见解可以用来识别显著改善结构化类别内权重可压缩性的变换。所提出的方法适用于支持高效投影操作的各种类型的结构化矩阵。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能强大，但是其对计算和内存资源的需求非常高，因此需要找到一种方法来减少模型的参数量，从而降低资源消耗。结构化矩阵表示被认为是一种有潜力的解决方案，但在不进行微调的情况下，预训练模型的权重矩阵可能无法准确表示为结构化矩阵，这成为了一个亟待解决的问题。

Method: 作者利用了大型语言模型输出在权重矩阵的某些正交变换下保持不变的特性，通过识别能够显著改善权重在结构化类别内的可压缩性的变换，从而解决了预训练模型权重矩阵难以直接用结构化矩阵表示的问题。该方法适用于各种支持高效投影操作的结构化矩阵类型。

Result: 该方法成功提高了权重矩阵在结构化矩阵表示下的可压缩性，使得大型语言模型能够在保持性能的同时减少参数量，从而降低了对计算和内存资源的需求。

Conclusion: 结构化矩阵表示是一种有效的减少大型语言模型参数量的方法，而通过利用正交变换下的不变性，可以进一步提升权重矩阵的可压缩性，从而实现更高效的模型压缩。

Abstract: Large language models (LLMs) demonstrate impressive results in natural
language processing tasks but require a significant amount of computational and
memory resources. Structured matrix representations are a promising way for
reducing the number of parameters of these models. However, it seems
unrealistic to expect that weight matrices of pretrained models can be
accurately represented by structured matrices without any fine-tuning. To
overcome this issue, we utilize the fact that LLM output is invariant under
certain orthogonal transformations of weight matrices. This insight can be
leveraged to identify transformations that significantly improve the
compressibility of weights within structured classes. The proposed approach is
applicable to various types of structured matrices that support efficient
projection operations. Code is available at
https://github.com/GrishKate/ProcrustesGPT

</details>


### [346] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
*Jintian Shao,Yiming Cheng*

Main category: cs.CL

TL;DR: Chain-of-Thought (CoT) prompting improves Large Language Models performance on multi-step inference tasks by functioning as a structural constraint that guides models to imitate reasoning, rather than eliciting genuine abstract reasoning.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical counter-perspective on the claims that Chain-of-Thought (CoT) prompting leads to emergent reasoning capabilities in Large Language Models.

Method: Arguing that CoT functions as a structural constraint which leverages the model's capacity for sequence prediction and pattern matching to generate outputs resembling coherent thought processes.

Result: Demonstrates that CoT enhances performance on multi-step inference tasks by guiding models to imitate reasoning rather than performing actual abstract reasoning.

Conclusion: Chain-of-Thought does not elicit genuine abstract reasoning but instead acts as a powerful structural constraint.

Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.
Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.

</details>


### [347] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
*Yin Fang,Qiao Jin,Guangzhi Xiong,Bowen Jin,Xianrui Zhong,Siru Ouyang,Aidong Zhang,Jiawei Han,Zhiyong Lu*

Main category: cs.CL

TL;DR: The paper introduces CellPuzzles, a task for assigning unique cell types to batches of cells considering batch-level context. Off-the-shelf LLMs perform poorly on this task. The authors propose Cell-o1, a 7B LLM that achieves state-of-the-art performance via supervised fine-tuning and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Current foundation models for cell type annotation in single-cell RNA sequencing data annotate cells independently without considering batch-level cellular context or providing explanatory reasoning. Human experts, however, use domain knowledge to annotate distinct cell types for different cell clusters. To mimic this workflow, the authors introduce the CellPuzzles task.

Method: The authors introduce CellPuzzles, a benchmark spanning diverse tissues, diseases, and donor conditions requiring reasoning across the batch-level cellular context to ensure label uniqueness. They then propose Cell-o1, a 7B LLM trained via supervised fine-tuning on distilled reasoning traces followed by reinforcement learning with batch-level rewards.

Result: Cell-o1 outperforms the best baseline (OpenAI's o1) by over 73% and generalizes well across contexts. Further analysis provides insights into batch-level annotation performance and emergent expert-like reasoning.

Conclusion: CellPuzzles is a new benchmark for cell type annotation considering batch-level context. Cell-o1, a 7B LLM, achieves state-of-the-art performance on this task through supervised fine-tuning and reinforcement learning.

Abstract: Cell type annotation is a key task in analyzing the heterogeneity of
single-cell RNA sequencing data. Although recent foundation models automate
this process, they typically annotate cells independently, without considering
batch-level cellular context or providing explanatory reasoning. In contrast,
human experts often annotate distinct cell types for different cell clusters
based on their domain knowledge. To mimic this workflow, we introduce the
CellPuzzles task, where the objective is to assign unique cell types to a batch
of cells. This benchmark spans diverse tissues, diseases, and donor conditions,
and requires reasoning across the batch-level cellular context to ensure label
uniqueness. We find that off-the-shelf large language models (LLMs) struggle on
CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%
batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained
via supervised fine-tuning on distilled reasoning traces, followed by
reinforcement learning with batch-level rewards. Cell-o1 achieves
state-of-the-art performance, outperforming o1 by over 73% and generalizing
well across contexts. Further analysis of training dynamics and reasoning
behaviors provides insights into batch-level annotation performance and
emergent expert-like reasoning. Code and data are available at
https://github.com/ncbi-nlp/cell-o1.

</details>


### [348] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
*Yusuke Sakai,Takumi Goto,Taro Watanabe*

Main category: cs.CL

TL;DR: The paper introduces IMPARA-GED, an advanced reference-free automatic GEC evaluation method with GED capabilities, which achieves the highest correlation with human evaluations.


<details>
  <summary>Details</summary>
Motivation: To improve the grammatical error correction evaluation by incorporating grammatical error detection capabilities.

Method: Propose IMPARA-GED, which constructs quality estimator using a pre-trained language model with enhanced GED capabilities.

Result: IMPARA-GED achieves the highest correlation with human sentence-level evaluations on SEEDA dataset.

Conclusion: IMPARA-GED is a successful novel method for reference-free automatic GEC evaluation with GED capabilities.

Abstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error
correction (GEC) evaluation method with grammatical error detection (GED)
capabilities. We focus on the quality estimator of IMPARA, an existing
automatic GEC evaluation method, and construct that of IMPARA-GED using a
pre-trained language model with enhanced GED capabilities. Experimental results
on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,
demonstrate that IMPARA-GED achieves the highest correlation with human
sentence-level evaluations.

</details>


### [349] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
*Diogo A. P. Nunes,Eugénio Ribeiro*

Main category: cs.CL

TL;DR: The team participated in eRisk's 2025 Task 1 on depression symptom search, framing it as a binary classification task for each symptom in the BDI. They experimented with various techniques like foundation model fine-tuning, sentence similarity, LLM prompting, and ensembles. Fine-tuning with synthetic data proved most effective, and their approach outperformed 16 other teams in official IR evaluations.


<details>
  <summary>Details</summary>
Motivation: To effectively search for symptoms of depression using sentences and the Beck's Depression Inventory - II (BDI) questionnaire, improving information retrieval performance for mental health applications.

Method: Split labeled data into training and validation sets; explored foundation model fine-tuning, sentence similarity, LLM prompting, and ensemble techniques for binary classification per BDI symptom; used synthetic data to address class imbalance.

Result: Fine-tuning foundation models with synthetic data performed best; optimal approaches varied by symptom; five independent test runs were devised, two using ensemble methods.

Conclusion: Their approach achieved the highest scores in official IR evaluations, surpassing submissions from 16 other teams.

Abstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.

</details>


### [350] [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
*Aishwarya Sahoo,Jeevana Kruthi Karnuthala,Tushar Parmanand Budhwani,Pranchal Agarwal,Sankaran Vaidyanathan,Alexa Siu,Franck Dernoncourt,Jennifer Healey,Nedim Lipka,Ryan Rossi,Uttaran Bhattacharya,Branislav Kveton*

Main category: cs.CL

TL;DR: This paper introduces quantitative LLM judges that align evaluation scores of existing LLM judges to human scores using regression models, improving the predictive power of current judges.


<details>
  <summary>Details</summary>
Motivation: To create a more effective and efficient framework for evaluating LLM outputs by aligning their scores with human evaluations.

Method: Propose quantitative LLM judges which use regression models to adjust the evaluation scores of existing LLM judges to match human scores in specific domains. These models are trained using textual evaluations and scores from the original judges.

Result: The experiments on four datasets with two base judges demonstrate that quantitative judges can significantly enhance the predictive power of existing judges.

Conclusion: Quantitative LLM judges provide a computationally and statistically efficient method to improve the evaluation of LLM outputs, showcasing generality and versatility.

Abstract: LLM-as-a-judge is a framework in which a large language model (LLM)
automatically evaluates the output of another LLM. We propose quantitative LLM
judges, which align evaluation scores of existing LLM judges to human scores in
a given domain using regression models. The models are trained to improve the
score of the original judge by using the judge's textual evaluation and score.
We present four quantitative judges for different types of absolute and
relative feedback, which showcases the generality and versatility of our
framework. Our framework is more computationally efficient than supervised
fine-tuning and can be more statistically efficient when human feedback is
limited, which is expected in most applications of our work. We validate these
claims empirically on four datasets using two base judges. Our experiments show
that quantitative judges can effectively improve the predictive power of
existing judges through post-hoc modeling.

</details>


### [351] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
*Zhixiong Su,Yichen Wang,Herun Wan,Zhaohan Zhang,Minnan Luo*

Main category: cs.CL

TL;DR: 为了应对大型语言模型（LLM）的潜在滥用风险，本文探讨了在人类-AI协作创作背景下细粒度机器生成文本（MGT）检测的可能性。提出数据集HACo-Det，并改进现有文档级检测器以实现词级检测。实验表明微调模型优于基于度量的方法，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的滥用可能导致潜在风险，而现有文献主要关注二元、文档级别的MGT检测，忽略了人类与LLM共同贡献的文本。因此需要探索细粒度MGT检测方法。

Method: 1. 提出数据集HACo-Det，通过自动流程生成具有词级别归属标签的人类-AI协作文本。
2. 改进七个流行的文档级别检测器，使其适用于词级别检测。
3. 在HACo-Det上评估这些检测器在词和句子级别检测任务中的表现。

Result: 实验结果表明，基于度量的方法平均F1得分为0.462，在细粒度检测中表现不佳；而微调模型表现出更优性能和更好的跨领域泛化能力。

Conclusion: 尽管微调模型在细粒度检测中表现更好，但人类-AI协作文本的细粒度检测问题仍未得到解决。文章分析了影响性能的因素（如上下文窗口），并指出了当前方法的局限性及未来改进方向。

Abstract: The misuse of large language models (LLMs) poses potential risks, motivating
the development of machine-generated text (MGT) detection. Existing literature
primarily concentrates on binary, document-level detection, thereby neglecting
texts that are composed jointly by human and LLM contributions. Hence, this
paper explores the possibility of fine-grained MGT detection under human-AI
coauthoring. We suggest fine-grained detectors can pave pathways toward
coauthored text detection with a numeric AI ratio. Specifically, we propose a
dataset, HACo-Det, which produces human-AI coauthored texts via an automatic
pipeline with word-level attribution labels. We retrofit seven prevailing
document-level detectors to generalize them to word-level detection. Then we
evaluate these detectors on HACo-Det on both word- and sentence-level detection
tasks. Empirical results show that metric-based methods struggle to conduct
fine-grained detection with a 0.462 average F1 score, while finetuned models
show superior performance and better generalization across domains. However, we
argue that fine-grained co-authored text detection is far from solved. We
further analyze factors influencing performance, e.g., context window, and
highlight the limitations of current methods, pointing to potential avenues for
improvement.

</details>


### [352] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)
*Richard Armitage*

Main category: cs.CL

TL;DR: This paper tests leading LLMs (o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro) on MRCGP-style questions in primary care education. These models significantly outperformed the average GP scores.


<details>
  <summary>Details</summary>
Motivation: To evaluate the capabilities of advanced large language models in answering medical specialty examination questions related to primary care, specifically Member of the Royal College of General Practitioners (MRCGP) style examination questions.

Method: Four leading LLMs were tasked with answering 100 randomly chosen multiple choice questions from the Royal College of General Practitioners GP SelfTest. Questions included textual information, laboratory results, and clinical images. Each model answered each question once and responses were scored against correct answers provided by GP SelfTest.

Result: The total score for o3 was 99.0%, while Claude Opus 4, Grok3, and Gemini 2.5 Pro all scored 95.0%. The average peer score for the same questions was 73.0%.

Conclusion: All models performed remarkably well, substantially exceeding the average performance of GPs and GP registrars who had answered the same questions. This supports the potential use of LLMs, especially reasoning models, in delivering primary care.

Abstract: Background: Large language models (LLMs) have demonstrated substantial
potential to support clinical practice. Other than Chat GPT4 and its
predecessors, few LLMs, especially those of the leading and more powerful
reasoning model class, have been subjected to medical specialty examination
questions, including in the domain of primary care. This paper aimed to test
the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and
Gemini 2.5 Pro) in primary care education, specifically in answering Member of
the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer
100 randomly chosen multiple choice questions from the Royal College of General
Practitioners GP SelfTest on 25 May 2025. Questions included textual
information, laboratory results, and clinical images. Each model was prompted
to answer as a GP in the UK and was provided with full question information.
Each question was attempted once by each model. Responses were scored against
correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was
99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the
same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially
exceeded the average performance of GPs and GP registrars who had answered the
same questions. o3 demonstrated the best performance, while the performances of
the other leading models were comparable with each other and were not
substantially lower than that of o3. These findings strengthen the case for
LLMs, particularly reasoning models, to support the delivery of primary care,
especially those that have been specifically trained on primary care clinical
data.

</details>


### [353] [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)
*Florian Ludwig,Torsten Zesch,Frederike Zufall*

Main category: cs.CL

TL;DR: 研究人员探索了在不同法律抽象层次上调节大型语言模型（LLMs）的方法，以检测可能的仇恨言论。尽管在具体法律知识的帮助下，模型在识别相关目标群体方面表现合理，但在分类目标行为方面存在困难。总的来说，无论模型在哪一层次上进行调节，其在仇恨言论的法律评估方面与法律专家相比仍有显著的性能差距。


<details>
  <summary>Details</summary>
Motivation: 评估法律问题需要考虑特定的法律体系及其从宪法到法规再到判例法的不同抽象层次。目前尚不清楚大型语言模型（LLMs）对这些法律体系的内化程度如何。因此，研究者希望探讨不同的方法来调节LLMs在法律体系的不同抽象层次上的表现。

Method: 研究者调查了多种方法，以在法律体系的不同抽象层次上调节LLMs，特别是针对根据德国刑法规定的煽动仇恨罪名对社交媒体帖子进行分类的任务。

Result: 结果表明，无论模型在哪一层次上进行调节，模型与法律专家在仇恨言论的法律评估方面仍存在显著的性能差距。此外，基于抽象法律知识调节的模型缺乏深入的任务理解，经常自相矛盾并产生幻觉回答；而使用具体法律知识的模型在识别相关目标群体方面表现合理，但在分类目标行为方面遇到困难。

Conclusion: 虽然LLMs可以通过具体的法律知识来提高其在某些法律任务中的表现，但它们仍然无法达到法律专家的水平。特别是在法律评估中涉及更复杂的推理时，模型的表现尤其不足。

Abstract: The assessment of legal problems requires the consideration of a specific
legal system and its levels of abstraction, from constitutional law to
statutory law to case law. The extent to which Large Language Models (LLMs)
internalize such legal systems is unknown. In this paper, we propose and
investigate different approaches to condition LLMs at different levels of
abstraction in legal systems. This paper examines different approaches to
conditioning LLMs at multiple levels of abstraction in legal systems to detect
potentially punishable hate speech. We focus on the task of classifying whether
a specific social media posts falls under the criminal offense of incitement to
hatred as prescribed by the German Criminal Code. The results show that there
is still a significant performance gap between models and legal experts in the
legal assessment of hate speech, regardless of the level of abstraction with
which the models were conditioned. Our analysis revealed, that models
conditioned on abstract legal knowledge lacked deep task understanding, often
contradicting themselves and hallucinating answers, while models using concrete
legal knowledge performed reasonably well in identifying relevant target
groups, but struggled with classifying target conducts.

</details>


### [354] [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)
*Pierre Lepagnol,Sahar Ghannay,Thomas Gerald,Christophe Servan,Sophie Rosset*

Main category: cs.CL

TL;DR: This paper proposes to use Information Retrieval (IR) methods for example selection in building prompts for Spoken Language Understanding (SLU) tasks using instruction-tuned large language models, showing significant performance improvements without increasing prompt length.


<details>
  <summary>Details</summary>
Motivation: The motivation is the challenge of developing accurate SLU approaches with limited annotated data for specific tasks or languages, while instruction-tuned LLMs perform well on unseen tasks with adequate prompts.

Method: The method involves exploring example selection by leveraging IR approaches to build an enhanced prompt applied to an SLU task.

Result: Experimental results demonstrate that lexical IR methods significantly improve performance without increasing prompt length.

Conclusion: Using IR methods for example selection can enhance the performance of SLU tasks when using instruction-tuned LLMs.

Abstract: Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.

</details>


### [355] [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)
*Yuval Kansal,Shmuel Berman,Lydia Liu*

Main category: cs.CL

TL;DR: The paper evaluates the correctness of Llama3.1 family models in answering factual questions for middle and high school students, revealing that these models provide less truthful information and worsen biases against rare languages.


<details>
  <summary>Details</summary>
Motivation: To ensure the factuality and usefulness of educational tools based on Large Language Models (LLMs), especially in non-English settings.

Method: Evaluate the correctness of the Llama3.1 family of models in answering factual questions suitable for middle and high school students.

Result: LLMs not only give extraneous and less truthful information but also increase existing biases against rare languages.

Conclusion: The performance of LLMs in languages other than English is largely untested and may lead to misinformation and bias exacerbation.

Abstract: Factuality is a necessary precursor to useful educational tools. As adoption
of Large Language Models (LLMs) in education continues of grow, ensuring
correctness in all settings is paramount. Despite their strong English
capabilities, LLM performance in other languages is largely untested. In this
work, we evaluate the correctness of the Llama3.1 family of models in answering
factual questions appropriate for middle and high school students. We
demonstrate that LLMs not only provide extraneous and less truthful
information, but also exacerbate existing biases against rare languages.

</details>


### [356] [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)
*Pietro Lesci,Clara Meister,Thomas Hofmann,Andreas Vlachos,Tiago Pimentel*

Main category: cs.CL

TL;DR: Modern language models should not be affected by the choice of tokeniser, but in practice they are. This is called tokenisation bias. This study quantifies this bias by estimating the effect of including or excluding a subword in a tokeniser's vocabulary on the probability a trained model assigns to the corresponding characters.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of tokenisation bias in modern language models, where the choice of tokeniser affects the probability assigned to the underlying character-string.

Method: The method used in this paper involves framing tokenisation bias as a causal effect and estimating it using the regression discontinuity design. This is done by exploiting the fact that tokenisation algorithms rank subwords and add the first $K$ to a tokeniser's vocabulary, allowing for comparison of similar subwords around this cutoff.

Result: The experiments conducted show that tokenisation consistently affects models' outputs across scales, vocabularies, and tokenisers. Notably, a subword's presence in a small model's vocabulary may increase its characters' probability by up to 17 times.

Conclusion: The conclusion drawn from this work is that tokenisation is a key design choice in language modelling, as it significantly impacts the models' outputs.

Abstract: Modern language models are typically trained over subword sequences, but
ultimately define probabilities over character-strings. Ideally, the choice of
the tokeniser -- which maps character-strings to subwords -- should not affect
the probability assigned to the underlying character-string; in practice, it
does. We define this mismatch as tokenisation bias. In this work, we quantify
one particular type of tokenisation bias: the effect of including or not a
subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the
probability a trained model assigns to the corresponding characters (i.e.,
\textit{``hello''}). Estimating this effect is challenging because each model
is trained with only one tokeniser. We address this by framing tokenisation
bias as a causal effect and estimating it using the regression discontinuity
design. Specifically, we exploit the fact that tokenisation algorithms rank
subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an
arbitrary cutoff point. As such, we can estimate a causal effect by comparing
similar subwords around this cutoff. Experimentally, we find that tokenisation
consistently affects models' outputs across scales, vocabularies, and
tokenisers. Notably, a subword's presence in a small model's vocabulary may
increase its characters' probability by up to 17 times, highlighting
tokenisation as a key design choice in language modelling.

</details>


### [357] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
*Xiaoying Zhang,Hao Sun,Yipeng Zhang,Kaituo Feng,Chao Yang,Helen Meng*

Main category: cs.CL

TL;DR: The paper presents Critique-GRPO, an online RL framework that combines natural language and numerical feedback to optimize LLM policies. It outperforms other fine-tuning methods in complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Despite the success of reinforcement learning (RL) with numerical feedback in enhancing LLMs' reasoning capabilities, there are challenges such as performance plateaus, limited self-reflection effectiveness, and persistent failures.

Method: Critique-GRPO is proposed, which integrates both natural language critiques and numerical feedback for effective policy optimization, allowing LLMs to learn from initial responses and critique-guided refinements simultaneously while maintaining exploration.

Result: Experiments show that Critique-GRPO consistently outperforms supervised learning-based and RL-based fine-tuning approaches across various challenging tasks, improving average pass@1 scores by 4.5% and 5%, respectively. It surpasses a strong baseline incorporating expert demonstrations within online RL.

Conclusion: Critique-GRPO demonstrates the potential of combining natural language and numerical feedback in RL for LLMs. Insights reveal that higher entropy and longer responses do not always ensure efficient learning and exploration.

Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such
as scalar rewards, have significantly enhanced the complex reasoning
capabilities of large language models (LLMs). Despite this success, we identify
three key challenges encountered by RL with solely numerical feedback:
performance plateaus, limited effectiveness of self-reflection, and persistent
failures. We then demonstrate that RL-finetuned models, even after exhibiting
performance plateaus, can generate correct refinements on persistently failed
problems by leveraging natural language feedback in the form of critiques.
Building on this insight, we propose Critique-GRPO, an online RL framework that
integrates both natural language and numerical feedback for effective policy
optimization. Critique-GRPO enables LLMs to learn from initial responses and
critique-guided refinements simultaneously while maintaining exploration.
Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that
Critique-GRPO consistently outperforms supervised learning-based and RL-based
fine-tuning approaches across eight challenging mathematical, STEM, and general
reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,
respectively. Notably, Critique-GRPO surpasses a strong baseline that
incorporates expert demonstrations within online RL. Further analysis reveals
two critical insights about policy exploration: (1) higher entropy does not
always guarantee efficient learning from exploration, and (2) longer responses
do not necessarily lead to more effective exploration.

</details>


### [358] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
*Qianhui Wu,Kanzhi Cheng,Rui Yang,Chaoyun Zhang,Jianwei Yang,Huiqiang Jiang,Jian Mu,Baolin Peng,Bo Qiao,Reuben Tan,Si Qin,Lars Liden,Qingwei Lin,Huan Zhang,Tong Zhang,Jianbing Zhang,Dongmei Zhang,Jianfeng Gao*

Main category: cs.CL

TL;DR: This paper proposes GUI-Actor, a VLM-based method for coordinate-free GUI grounding that outperforms prior methods and improves generalization.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing text-based coordinate generation approaches in visual grounding for GUI agents, such as weak spatial-semantic alignment and inability to handle ambiguous supervision targets.

Method: GUI-Actor introduces an attention-based action head that aligns an <ACTOR> token with relevant visual patch tokens to propose action regions. A grounding verifier is also designed to select the most plausible action region from candidates.

Result: Extensive experiments demonstrate that GUI-Actor surpasses previous state-of-the-art methods on multiple benchmarks, including achieving scores of 40.7 and 44.6 on ScreenSpot-Pro with different backbones. Fine-tuning only the action head while keeping the VLM backbone frozen yields comparable performance to previous models.

Conclusion: GUI-Actor effectively endows the underlying VLM with grounding capabilities without compromising its general-purpose strengths, showing improved performance and generalization.

Abstract: One of the principal challenges in building VLM-powered GUI agents is visual
grounding, i.e., localizing the appropriate screen region for action execution
based on both the visual content and the textual plans. Most existing work
formulates this as a text-based coordinate generation task. However, these
approaches suffer from several limitations: weak spatial-semantic alignment,
inability to handle ambiguous supervision targets, and a mismatch between the
dense nature of screen coordinates and the coarse, patch-level granularity of
visual features extracted by models like Vision Transformers. In this paper, we
propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its
core, GUI-Actor introduces an attention-based action head that learns to align
a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the
model to propose one or more action regions in a single forward pass. In line
with this, we further design a grounding verifier to evaluate and select the
most plausible action region from the candidates proposed for action execution.
Extensive experiments show that GUI-Actor outperforms prior state-of-the-art
methods on multiple GUI action grounding benchmarks, with improved
generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B
even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7
with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by
incorporating the verifier, we find that fine-tuning only the newly introduced
action head (~100M parameters for 7B model) while keeping the VLM backbone
frozen is sufficient to achieve performance comparable to previous
state-of-the-art models, highlighting that GUI-Actor can endow the underlying
VLM with effective grounding capabilities without compromising its
general-purpose strengths.

</details>


### [359] [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CL

TL;DR: This paper presents new methods to construct a knowledge graph from unlabeled neuroscience research data using large language models, neuroscience ontology, and text embeddings, which significantly improves knowledge discovery in the field.


<details>
  <summary>Details</summary>
Motivation: Neuroscience literature holds vast knowledge, but current retrieval methods struggle with dispersed information across multiple sources. Constructing knowledge graphs (KGs) can help integrate this knowledge, yet existing KG construction methods often depend on labeled data and domain expertise, which are challenging to obtain in specialized fields like neuroscience.

Method: The paper proposes methods for constructing a knowledge graph from an unlabeled neuroscience corpus by leveraging large language models (LLMs), neuroscience ontology, and text embeddings. It analyzes the semantic relevance of text segments identified by LLMs for building the KG and introduces an entity-augmented information retrieval algorithm to extract knowledge from it.

Result: Experiments show that the proposed methods achieve an F1 score of 0.84 for entity extraction and improve answers to over 54% of questions using the knowledge obtained from the constructed knowledge graph.

Conclusion: The novel methods presented in this work effectively enhance knowledge discovery from unlabeled neuroscience research data, demonstrating the potential of combining large language models, neuroscience ontology, and text embeddings for constructing useful knowledge graphs.

Abstract: Neuroscience research publications encompass a vast wealth of knowledge.
Accurately retrieving existing information and discovering new insights from
this extensive literature is essential for advancing the field. However, when
knowledge is dispersed across multiple sources, current state-of-the-art
retrieval methods often struggle to extract the necessary information. A
knowledge graph (KG) can integrate and link knowledge from multiple sources,
but existing methods for constructing KGs in neuroscience often rely on labeled
data and require domain expertise. Acquiring large-scale, labeled data for a
specialized area like neuroscience presents significant challenges. This work
proposes novel methods for constructing KG from unlabeled large-scale
neuroscience research corpus utilizing large language models (LLM),
neuroscience ontology, and text embeddings. We analyze the semantic relevance
of neuroscience text segments identified by LLM for building the knowledge
graph. We also introduce an entity-augmented information retrieval algorithm to
extract knowledge from the KG. Several experiments were conducted to evaluate
the proposed approaches, and the results demonstrate that our methods
significantly enhance knowledge discovery from the unlabeled neuroscience
research corpus. It achieves an F1 score of 0.84 for entity extraction, and the
knowledge obtained from the KG improves answers to over 54% of the questions.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [360] [Torsion in Persistent Homology and Neural Networks](https://arxiv.org/abs/2506.03049)
*Maria Walch*

Main category: math.AT

TL;DR: 研究了结合拓扑数据分析的混合深度学习模型中扭转的作用，发现标准自动编码器在编码、潜空间和解码过程中可能丢失或改变扭转特征，提出需要设计新的架构或损失项以保留这些信息。


<details>
  <summary>Details</summary>
Motivation: 探讨扭转在混合深度学习模型中的作用，揭示当前基于域系数的TDA工具掩盖了整数同调中的扭转特征，希望找到更好的数据表示方法。

Method: 使用合成数据和高维数据，评估不同自动编码器架构对扭转特征的敏感性和恢复能力，并分析扭转特征在编码、潜空间和解码过程中的变化。

Result: 发现在许多情况下，扭转特征在编码过程中丢失、在潜空间中被改变且无法通过标准解码器重建；基于域的方法存在关键限制。

Conclusion: 需要设计新的自动编码器架构或损失函数，以更好地保留扭转特征，实现更鲁棒的数据表示。

Abstract: We explore the role of torsion in hybrid deep learning models that
incorporate topological data analysis, focusing on autoencoders. While most TDA
tools use field coefficients, this conceals torsional features present in
integer homology. We show that torsion can be lost during encoding, altered in
the latent space, and in many cases, not reconstructed by standard decoders.
Using both synthetic and high-dimensional data, we evaluate torsion sensitivity
to perturbations and assess its recoverability across several autoencoder
architectures. Our findings reveal key limitations of field-based approaches
and underline the need for architectures or loss terms that preserve torsional
information for robust data representation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [361] [FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs](https://arxiv.org/abs/2506.01969)
*Pencuo Zeren,Qiuming Luo,Rui Mao,Chang Kong*

Main category: cs.DC

TL;DR: FlashMLA-ETAP is a novel framework that enhances MLA inference for single-instance deployment on NVIDIA H20 GPUs by proposing ETAP, achieving significant speedup and lower RMSE.


<details>
  <summary>Details</summary>
Motivation: Efficient inference of Multi-Head Latent Attention (MLA) faces challenges when deploying large models like DeepSeek-R1 671B on a single Multi-GPU server.

Method: Introduced FlashMLA-ETAP with the Efficient Transpose Attention Pipeline (ETAP) that reconfigures attention computation through transposition to reduce redundant computations.

Result: Achieved a 2.78x speedup over FlashMLA at 64K sequence length, with 5.24x and 4.94x improvements over FlashAttention-3 and FlashInfer respectively, while maintaining numerical stability with a 15.2x lower RMSE than FlashAttention-3.

Conclusion: Addresses a critical gap in resource-constrained inference, offering a scalable solution for mid-tier GPUs and facilitating broader adoption in hardware-aware optimization.

Abstract: Efficient inference of Multi-Head Latent Attention (MLA) is challenged by
deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper
introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the
single-instance deployment scenario on NVIDIA H20 GPUs. We propose the
Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention
computation through transposition to align the KV context length with the
\(M\)-dimension in WGMMA operations, significantly reducing redundant
computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K
sequence length (batch size 16), with 5.24x and 4.94x improvements over
FlashAttention-3 and FlashInfer, respectively, while maintaining numerical
stability with a 15.2x lower RMSE (\(1.25 \times 10^{-5}\)) than
FlashAttention-3. Furthermore, ETAP's design enables seamless integration into
frameworks like FlashAttention-3 and FlashInfer, supported by a detailed
theoretical analysis. Our work addresses a critical gap in resource-constrained
inference, offering a scalable solution for mid-tier GPUs and paving the way
for broader adoption in hardware-aware optimization. Code is available at
https://github.com/pengcuo/FlashMLA-ETAP.

</details>


### [362] [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)
*Yuhao Shen,Junyi Shen,Quan Kong,Tianyu Liu,Yao Lu,Cong Wang*

Main category: cs.DC

TL;DR: The paper introduces SpecBranch, a framework inspired by branch prediction in modern processors that enhances speculative decoding (SD) for LLM inference. It reduces waiting times between draft and target models through parallel speculative branches and adaptive draft lengths, achieving significant speedups and reducing rollback tokens.


<details>
  <summary>Details</summary>
Motivation: To overcome the serialized execution limitation in current SD methods which causes delays due to mutual waiting between the draft and target models during LLM inference.

Method: SpecBranch employs branch parallelism by introducing parallel speculative branches to handle potential rejections. It also uses adaptive draft lengths with a combination of draft model confidence and target model feature reuse to increase parallelism.

Result: SpecBranch achieves 1.8x to 4.5x speedups compared to auto-regressive decoding and reduces rollback tokens by 50% for poorly aligned models.

Conclusion: SpecBranch effectively addresses the challenge of serialization in SD, demonstrating its practicality for real-world LLM inference deployments.

Abstract: Recently, speculative decoding (SD) has emerged as a promising technique to
accelerate LLM inference by employing a small draft model to propose draft
tokens in advance, and validating them in parallel with the large target model.
However, the existing SD methods still remain fundamentally constrained by
their serialized execution, which causes the mutual waiting bubbles between the
draft and target models. To address this challenge, we draw inspiration from
branch prediction in modern processors and propose a novel framework
\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first
take an in-depth analysis of the potential of branch parallelism in SD, and
recognize that the key challenge lies in the trade-offs between parallelization
and token rollback. Based on the analysis, we strategically introduce parallel
speculative branches to preemptively hedge against likely rejections.
Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft
lengths with a hybrid combination of the implicit draft model confidence and
explicit reusing of target model features. Extensive experiments across various
models and benchmarks show that SpecBranch achieves over \textbf{1.8}$\times
\sim$ \textbf{4.5}$\times$ speedups against the auto-regressive decoding and
reduces rollback tokens by $\textbf{50}$\% for poorly aligned models, realizing
its applicability for real-world deployments.

</details>


### [363] [eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems](https://arxiv.org/abs/2506.02007)
*Ruilin Xu,Zongxuan Xie,Pengfei Chen*

Main category: cs.DC

TL;DR: The paper introduces eACGM, a full-stack AI/ML system monitoring framework based on eBPF that collects real-time performance data without code instrumentation and uses GMM for statistical modeling and clustering analysis to identify complex failure modes. Evaluated in multi-node distributed training scenarios, eACGM effectively captures performance anomalies with low overhead.


<details>
  <summary>Details</summary>
Motivation: To create a non-intrusive and efficient monitoring framework for AI/ML systems that can identify complex failure modes and enable rapid diagnosis of system bottlenecks and abnormal behaviors.

Method: eACGM collects real-time performance data from hardware and software components using eBPF and libnvml without requiring code instrumentation or modifications. It applies Gaussian Mixture Model (GMM) to the collected multidimensional performance metrics for statistical modeling and clustering analysis.

Result: Empirical studies and case analyses in multi-node distributed training scenarios show that eACGM successfully captures critical performance anomalies during model training and inference while maintaining a non-intrusive and low-overhead profile.

Conclusion: eACGM's stable anomaly detection performance and comprehensive monitoring capabilities make it applicable and scalable in real-world production environments, supporting performance optimization and fault diagnosis in large-scale AI/ML systems.

Abstract: We present eACGM, a full-stack AI/ML system monitoring framework based on
eBPF. eACGM collects real-time performance data from key hardware components,
including the GPU and network communication layer, as well as from key software
stacks such as CUDA, Python, and PyTorch, all without requiring any code
instrumentation or modifications. Additionally, it leverages libnvml to gather
process-level GPU resource usage information. By applying a Gaussian Mixture
Model (GMM) to the collected multidimensional performance metrics for
statistical modeling and clustering analysis, eACGM effectively identifies
complex failure modes, such as latency anomalies, hardware failures, and
communication inefficiencies, enabling rapid diagnosis of system bottlenecks
and abnormal behaviors.
  To evaluate eACGM's effectiveness and practicality, we conducted extensive
empirical studies and case analyses in multi-node distributed training
scenarios. The results demonstrate that eACGM, while maintaining a
non-intrusive and low-overhead profile, successfully captures critical
performance anomalies during model training and inference. Its stable anomaly
detection performance and comprehensive monitoring capabilities validate its
applicability and scalability in real-world production environments, providing
strong support for performance optimization and fault diagnosis in large-scale
AI/ML systems.

</details>


### [364] [Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling](https://arxiv.org/abs/2506.02025)
*Prachi Jadhav,Hongwei Jin,Ewa Deelman,Prasanna Balaprakash*

Main category: cs.DC

TL;DR: The paper proposes an LLM-based HPC job scheduler using a ReAct framework, which balances multiple scheduling objectives and adapts to diverse workloads without domain-specific training. Evaluated across real-world scenarios, it shows promise in transparent reasoning and constraint satisfaction but faces challenges in computational efficiency for real-time deployment.


<details>
  <summary>Details</summary>
Motivation: Traditional HPC job scheduling methods struggle with adaptability to dynamic workloads and heterogeneous systems. This motivates the exploration of a more flexible and interpretable approach using advanced language models.

Method: The method employs a Large Language Model (LLM) based scheduler within a ReAct framework that combines reasoning and action steps. It uses scratchpad memory for tracking scheduling history and refining decisions via natural language feedback, along with a constraint enforcement module to ensure feasibility.

Result: Compared to FCFS, Shortest Job First, and Google OR-Tools, the LLM-based scheduler effectively balances multiple objectives, excels in constraint satisfaction, and adapts well to various workloads without specific domain training. However, there is a trade-off between reasoning quality and computational overhead.

Conclusion: This study demonstrates the potential of reasoning-capable LLMs for HPC scheduling by handling multiobjective optimization tasks. Yet, it highlights the limitations in computational efficiency, providing valuable insights into leveraging advanced language models for complex scheduling problems.

Abstract: High-Performance Computing (HPC) job scheduling involves balancing
conflicting objectives such as minimizing makespan, reducing wait times,
optimizing resource use, and ensuring fairness. Traditional methods, including
heuristic-based (e.g., First-Come-First-Served) or intensive optimization
techniques, often lack adaptability to dynamic workloads and heterogeneous HPC
systems. To address this, we propose a novel Large Language Model (LLM)-based
scheduler using a ReAct-style framework (Reason + Act), enabling iterative,
interpretable decision-making. The system incorporates a scratchpad memory to
track scheduling history and refine decisions via natural language feedback,
while a constraint enforcement module ensures feasibility and safety. We
evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across
seven real-world HPC workload scenarios, including heterogeneous mixes, bursty
patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,
and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling
effectively balances multiple objectives while offering transparent reasoning
through natural language traces. The method excels in constraint satisfaction
and adapts to diverse workloads without domain-specific training. However, a
trade-off between reasoning quality and computational overhead challenges
real-time deployment. This work presents the first comprehensive study of
reasoning-capable LLMs for HPC scheduling, demonstrating their potential to
handle multiobjective optimization while highlighting limitations in
computational efficiency. The findings provide insights into leveraging
advanced language models for complex scheduling problems in dynamic HPC
environments.

</details>


### [365] [EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration](https://arxiv.org/abs/2506.02049)
*Beichen Huang,Ran Cheng,Kay Chen Tan*

Main category: cs.DC

TL;DR: An open-sourced framework named EvoGit is introduced for decentralized, autonomous code evolution in collaborative software development. It uses a Git-based phylogenetic graph to coordinate independent coding agents without centralized control. Experiments show its potential in creating functional and modular software artifacts.


<details>
  <summary>Details</summary>
Motivation: To explore a new paradigm for decentralized, automated, and continual software development that reduces human involvement while maintaining strategic oversight.

Method: EvoGit deploys a population of independent coding agents which propose edits to a shared codebase using a Git-based phylogenetic graph for coordination. This graph supports branching, concurrency, and scalable agent interaction while preserving a consistent historical record.

Result: Experiments demonstrated the ability of EvoGit to autonomously produce functional and modular software artifacts in two real-world tasks: building a web application from scratch and constructing a meta-level system for evolving solvers.

Conclusion: EvoGit has the potential to establish a new paradigm for decentralized, automated, and continual software development.

Abstract: We introduce EvoGit, a decentralized multi-agent framework for collaborative
software development driven by autonomous code evolution. EvoGit deploys a
population of independent coding agents, each proposing edits to a shared
codebase without centralized coordination, explicit message passing, or shared
memory. Instead, all coordination emerges through a Git-based phylogenetic
graph that tracks the full version lineage and enables agents to asynchronously
read from and write to the evolving code repository. This graph-based structure
supports fine-grained branching, implicit concurrency, and scalable agent
interaction while preserving a consistent historical record. Human involvement
is minimal but strategic: users define high-level goals, periodically review
the graph, and provide lightweight feedback to promote promising directions or
prune unproductive ones. Experiments demonstrate EvoGit's ability to
autonomously produce functional and modular software artifacts across two
real-world tasks: (1) building a web application from scratch using modern
frameworks, and (2) constructing a meta-level system that evolves its own
language-model-guided solver for the bin-packing optimization problem. Our
results underscore EvoGit's potential to establish a new paradigm for
decentralized, automated, and continual software development. EvoGit is
open-sourced at https://github.com/BillHuang2001/evogit.

</details>


### [366] [Machine Learning for Consistency Violation Faults Analysis](https://arxiv.org/abs/2506.02002)
*Kamal Giri,Amit Garu*

Main category: cs.DC

TL;DR: The paper explores the impact of consistency violation faults (CVFs) in distributed systems using machine learning models, focusing on Dijkstra's Token Ring problem. It proposes a method to quantify CVF effects through program transition ranks and trains ML models to predict rank effects, showing promising results.


<details>
  <summary>Details</summary>
Motivation: Distributed systems often face issues with outdated or inaccurate data, known as consistency violation faults (CVFs), which negatively affect system convergence and performance. There is a need for better understanding and quantification of these faults' impacts.

Method: The study computes program transition ranks and their effects to quantify CVF impacts. It uses two machine learning models - a Feedforward Neural Network (FNN) and a distributed neural network via TensorFlow's tf.distribute API - trained on datasets from smaller graphs (3-10 nodes) to predict essential parameters for determining rank effects.

Result: The experimental results indicate good performance with a test loss of 4.39 and a mean absolute error of 1.5. However, distributed training on CPUs did not significantly improve speed compared to single-device setups. Potential scalability improvements are suggested through advanced hardware accelerators.

Conclusion: Machine learning models can effectively analyze and predict the effects of consistency violation faults in distributed systems, as demonstrated by the proposed method's promising performance.

Abstract: Distributed systems frequently encounter consistency violation faults (cvfs),
where nodes operate on outdated or inaccurate data, adversely affecting
convergence and overall system performance. This study presents a machine
learning-based approach for analyzing the impact of CVFs, using Dijkstra's
Token Ring problem as a case study. By computing program transition ranks and
their corresponding effects, the proposed method quantifies the influence of
cvfs on system behavior. To address the state space explosion encountered in
larger graphs, two models are implemented: a Feedforward Neural Network (FNN)
and a distributed neural network leveraging TensorFlow's \texttt{tf.distribute}
API. These models are trained on datasets generated from smaller graphs (3 to
10 nodes) to predict parameters essential for determining rank effects.
Experimental results demonstrate promising performance, with a test loss of
4.39 and a mean absolute error of 1.5. Although distributed training on a CPU
did not yield significant speed improvements over a single-device setup, the
findings suggest that scalability could be enhanced through the use of advanced
hardware accelerators such as GPUs or TPUs.

</details>


### [367] [Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing](https://arxiv.org/abs/2506.02006)
*Zhaoyuan Su,Tingfeng Lan,Zirui Wang,Juncheng Yang,Yue Cheng*

Main category: cs.DC

TL;DR: MorphServe is a dynamic LLM serving framework that reduces SLO violations and improves latency via quantized layer swapping and pressure-aware KV cache resizing, without losing generation quality.


<details>
  <summary>Details</summary>
Motivation: Efficiently serving large language models under dynamic workloads is challenging. Existing frameworks fail to adapt to workload fluctuations, leading to either SLO violations or persistent accuracy degradation.

Method: MorphServe introduces two mechanisms: quantized layer swapping which replaces less impactful layers with quantized alternatives during high-load periods, and pressure-aware KV cache resizing which adjusts KV cache capacity in response to memory pressure.

Result: Extensive experiments show MorphServe reduces average SLO violations by 92.45 percent and improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving, while maintaining generation quality.

Conclusion: MorphServe is a practical and elastic solution for LLM deployment in dynamic environments.

Abstract: Efficiently serving large language models (LLMs) under dynamic and bursty
workloads remains a key challenge for real-world deployment. Existing serving
frameworks and static model compression techniques fail to adapt to workload
fluctuations, leading to either service-level objective (SLO) violations under
full-precision serving or persistent accuracy degradation with static
quantization. We present MorphServe, a dynamic, workload-aware LLM serving
framework based on morphological adaptation. MorphServe introduces two
asynchronous, token-level runtime mechanisms: quantized layer swapping, which
selectively replaces less impactful layers with quantized alternatives during
high-load periods, and pressure-aware KV cache resizing, which dynamically
adjusts KV cache capacity in response to memory pressure. These mechanisms
enable state-preserving transitions with minimum runtime overhead and are fully
compatible with modern scheduling and attention techniques. Extensive
experiments on Vicuna and Llama family models with real-world workloads
demonstrate that MorphServe reduces average SLO violations by 92.45 percent and
improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,
without compromising generation quality. These results establish MorphServe as
a practical and elastic solution for LLM deployment in dynamic environments.

</details>


### [368] [DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2506.02023)
*Kevin Han,Bowen Deng,Amir Barati Farimani,Gerbrand Ceder*

Main category: cs.DC

TL;DR: The paper introduces DistMLIP, a distributed inference platform for machine learning interatomic potentials (MLIPs) using zero-redundancy, graph-level parallelization. It enables multi-device inference on flexible MLIP model architectures and demonstrates near-million-atom calculations in a few seconds on 8 GPUs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling up quantum mechanical calculations for large-scale atomistic simulations through efficient parallelization of MLIPs across multiple devices.

Method: Development of DistMLIP, which uses graph partitioning for MLIP parallelization, allowing distributed inference on models like multi-layer graph neural networks. It provides a plug-in interface for pre-existing MLIPs.

Result: DistMLIP successfully performs near-million-atom calculations within a few seconds on 8 GPUs with four state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN.

Conclusion: DistMLIP offers an efficient solution for scaling atomistic simulations via distributed inference of MLIPs, advancing realistic materials and drug discovery applications.

Abstract: Large-scale atomistic simulations are essential to bridge computational
materials and chemistry to realistic materials and drug discovery applications.
In the past few years, rapid developments of machine learning interatomic
potentials (MLIPs) have offered a solution to scale up quantum mechanical
calculations. Parallelizing these interatomic potentials across multiple
devices poses a challenging, but promising approach to further extending
simulation scales to real-world applications. In this work, we present
DistMLIP, an efficient distributed inference platform for MLIPs based on
zero-redundancy, graph-level parallelization. In contrast to conventional
space-partitioning parallelization, DistMLIP enables efficient MLIP
parallelization through graph partitioning, allowing multi-device inference on
flexible MLIP model architectures like multi-layer graph neural networks.
DistMLIP presents an easy-to-use, flexible, plug-in interface that enables
distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four
widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We
show that existing foundational potentials can perform near-million-atom
calculations at the scale of a few seconds on 8 GPUs with DistMLIP.

</details>


### [369] [Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM](https://arxiv.org/abs/2506.02490)
*Yong Xiang,Charley Peter Chen,Liyi Zeng,Wei Yin,Xin Liu,Hu Li,Wei Xu*

Main category: cs.DC

TL;DR: This paper introduces SynergyRCA, a tool that uses LLMs and graph databases to improve root cause analysis in Kubernetes systems.


<details>
  <summary>Details</summary>
Motivation: Kubernetes faces challenges in maintaining state consistency due to failures, network issues, and asynchronous problems. These lead to operational disruptions and economic losses, necessitating robust root cause analysis (RCA) for enhanced reliability.

Method: SynergyRCA is developed, which leverages LLMs with retrieval augmentation from graph databases and expert prompts. It constructs StateGraph and MetaGraph to capture relationships and connections respectively, and uses LLM to predict relevant resources and provide RCA insights.

Result: Evaluated using datasets from two production clusters, SynergyRCA can identify numerous root causes including novel ones with high efficiency and precision. It achieves an average time of about two minutes per RCA and a precision of approximately 0.90.

Conclusion: SynergyRCA demonstrates the potential of LLMs in improving RCA for complex and dynamic systems like Kubernetes.

Abstract: Kubernetes, a notably complex and distributed system, utilizes an array of
controllers to uphold cluster management logic through state reconciliation.
Nevertheless, maintaining state consistency presents significant challenges due
to unexpected failures, network disruptions, and asynchronous issues,
especially within dynamic cloud environments. These challenges result in
operational disruptions and economic losses, underscoring the necessity for
robust root cause analysis (RCA) to enhance Kubernetes reliability. The
development of large language models (LLMs) presents a promising direction for
RCA. However, existing methodologies encounter several obstacles, including the
diverse and evolving nature of Kubernetes incidents, the intricate context of
incidents, and the polymorphic nature of these incidents. In this paper, we
introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval
augmentation from graph databases and enhancement with expert prompts.
SynergyRCA constructs a StateGraph to capture spatial and temporal
relationships and utilizes a MetaGraph to outline entity connections. Upon the
occurrence of an incident, an LLM predicts the most pertinent resource, and
SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific
insights for RCA. We evaluate SynergyRCA using datasets from two production
Kubernetes clusters, highlighting its capacity to identify numerous root
causes, including novel ones, with high efficiency and precision. SynergyRCA
demonstrates the ability to identify root causes in an average time of about
two minutes and achieves an impressive precision of approximately 0.90.

</details>


### [370] [KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider](https://arxiv.org/abs/2506.02634)
*Jiahao Wang,Jinbo Han,Xingda Wei,Sijie Shen,Dingyan Zhang,Chenguang Fang,Rong Chen,Wenyuan Yu,Haibo Chen*

Main category: cs.DC

TL;DR: 通过分析领先的LLM服务提供商的KV缓存工作负载模式，提出了一种工作负载感知的缓存驱逐策略，以提高实际跟踪下的服务性能，特别是在缓存容量有限的情况下。


<details>
  <summary>Details</summary>
Motivation: 目前对LLM服务如何从KV缓存中受益的理解有限，系统设计决策（如缓存驱逐策略）高度依赖于工作负载。

Method: 1. 系统地描述了来自领先LLM服务提供商的KV缓存工作负载模式。
2. 提出了一个工作负载感知的缓存驱逐策略。

Result: 1. 发现KV缓存重用在单轮和多轮请求间同样重要。
2. 重用时间和概率对于所有请求来说是多样化的，但对于特定请求类别来说是可以预测的。
3. 达到理想缓存命中率所需的总体缓存大小是适度的。
4. 提出的缓存驱逐策略在实际跟踪下提高了服务性能，特别是在缓存容量有限的情况下。

Conclusion: 提出的缓存驱逐策略可以根据实际工作负载模式有效提高LLM服务的性能。

Abstract: Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.

</details>


### [371] [Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling](https://arxiv.org/abs/2506.02422)
*Xiyu Zhao,Qimei Cui,Ziqiang Du,Weicai Li,Xi Yu,Wei Ni,Ji Zhang,Xiaofeng Tao,Ping Zhang*

Main category: cs.DC

TL;DR: This paper proposes a quantization-assisted Gaussian differential privacy (DP) mechanism for wireless personalized federated learning (WPFL), analyzes the convergence upper bounds of individual models, and designs an optimal transmission scheduling strategy to achieve min-max fairness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address privacy concerns and performance fairness challenges in WPFL due to communication bottlenecks.

Method: The method involves exploiting quantization errors to enhance privacy, proposing a novel Gaussian DP mechanism, analyzing convergence upper bounds considering quantization errors and Gaussian DP noises, and designing an optimal transmission scheduling strategy using OFDMA interfaces.

Result: Experiments show that the proposed approach outperforms alternative strategies by 87.08%, 16.21%, and 38.37% in accuracy, maximum test loss, and fairness respectively.

Conclusion: The conclusion is that the proposed mechanism and strategy significantly improve privacy and fairness in WPFL.

Abstract: Personalized federated learning (PFL) offers a solution to balancing
personalization and generalization by conducting federated learning (FL) to
guide personalized learning (PL). Little attention has been given to wireless
PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is
another challenge resulting from communication bottlenecks in WPFL. This paper
exploits quantization errors to enhance the privacy of WPFL and proposes a
novel quantization-assisted Gaussian differential privacy (DP) mechanism. We
analyze the convergence upper bounds of individual PL models by considering the
impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and
imperfect communication channels on the FL of WPFL. By minimizing the maximum
of the bounds, we design an optimal transmission scheduling strategy that
yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by
revealing the nested structure of this problem to decouple it into subproblems
solved sequentially for the client selection, channel allocation, and power
control, and for the learning rates and PL-FL weighting coefficients.
Experiments validate our analysis and demonstrate that our approach
substantially outperforms alternative scheduling strategies by 87.08%, 16.21%,
and 38.37% in accuracy, the maximum test loss of participating clients, and
fairness (Jain's index), respectively.

</details>


### [372] [Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization](https://arxiv.org/abs/2506.02787)
*Ruilong Wu,Xinjiao Li,Yisu Wang,Xinyu Chen,Dirk Kutscher*

Main category: cs.DC

TL;DR: This paper proposes a simulation-based strategy to optimize hybrid parallelism for training large language models (LLMs) on heterogeneous nodes with dynamic network topology, using workload allocation, strategy pruning, and showing competitive performance.


<details>
  <summary>Details</summary>
Motivation: Current automatic parallel planning frameworks for LLMs do not adequately consider node heterogeneity and dynamic network topology changes, which limits their practical effectiveness.

Method: The paper models heterogeneous nodes in dynamic network environments and uses simulation-based strategies to determine optimal parallel configurations. It also introduces a strategy pruning technique to reduce the search space and accelerate the search process.

Result: Preliminary evaluations show that the proposed method enhances training performance on heterogeneous nodes and demonstrates improved adaptability in complex, dynamic scenarios such as cloud computing environments.

Conclusion: The approach achieves performance competitive with state-of-the-art methods under regular conditions while improving adaptability in more complex, dynamic settings.

Abstract: Hybrid parallelism techniques are essential for efficiently training large
language models (LLMs). Nevertheless, current automatic parallel planning
frameworks often overlook the simultaneous consideration of node heterogeneity
and dynamic network topology changes, limiting their effectiveness in practical
applications. In this paper, we address these limitations by modeling
heterogeneous nodes within dynamically changing network environments and
leveraging simulation-based strategies to determine optimal parallel
configurations. Our approach enables fine-grained workload allocation tailored
for heterogeneous nodes and complex network scenarios, achieving performance
competitive with state-of-the-art methods under regular and stable network
conditions. Additionally, we introduce a strategy pruning technique to rapidly
discard infeasible parallel configurations, substantially reducing the search
space and accelerating the search process through parallel execution within the
simulator. Preliminary evaluations confirm that our method notably enhances
training performance on heterogeneous nodes and demonstrates improved
adaptability in complex, dynamic scenarios such as cloud computing
environments.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [373] [A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization](https://arxiv.org/abs/2506.02458)
*Nguyen Chi Long,Trinh Van Chien,Ta Hai Tung,Van Son Nguyen,Trong-Minh Hoang,Nguyen Ngoc Hai Dang*

Main category: cs.IT

TL;DR: This paper explores the use of deep reinforcement learning (DRL) algorithms in Mobile Edge Computing (MEC) systems for effective decentralized dynamic computation offloading strategies. It introduces an improved approach based on Twin Delayed DDPG algorithm overcoming limitations of conventional DDPG-based power control strategy.


<details>
  <summary>Details</summary>
Motivation: To find feasible decentralized dynamic computation offloading strategies in MEC systems that lead to the construction of an extensible and efficient system with finite feedback.

Method: The study utilizes Deep Deterministic Policy Gradient (DDPG) algorithm initially but then proposes a new approach based on Twin Delayed DDPG algorithm to address inherent weaknesses and investigate portable mobile user cases.

Result: Numerical results indicate that individual users can autonomously learn adequate policies via the proposed approach, showing better performance than the traditional DDPG-based power control strategy.

Conclusion: The introduction of the Twin Delayed DDPG algorithm provides an enhanced solution for computation offloading in MEC systems, improving upon existing methods.

Abstract: Mobile edge computing (MEC) allows appliances to offload workloads to
neighboring MEC servers that have the potential for computation-intensive tasks
with limited computational capabilities. This paper studied how deep
reinforcement learning (DRL) algorithms are used in an MEC system to find
feasible decentralized dynamic computation offloading strategies, which leads
to the construction of an extensible MEC system that operates effectively with
finite feedback. Even though the Deep Deterministic Policy Gradient (DDPG)
algorithm, subject to their knowledge of the MEC system, can be used to
allocate powers of both computation offloading and local execution, to learn a
computation offloading policy for each user independently, we realized that
this solution still has some inherent weaknesses. Hence, we introduced a new
approach for this problem based on the Twin Delayed DDPG algorithm, which
enables us to overcome this proneness and investigate cases where mobile users
are portable. Numerical results showed that individual users can autonomously
learn adequate policies through the proposed approach. Besides, the performance
of the suggested solution exceeded the conventional DDPG-based power control
strategy.

</details>


### [374] [Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning](https://arxiv.org/abs/2506.02657)
*Tam Ninh Thi-Thanh,Trinh Van Chien,Hung Tran,Nguyen Hoai Son,Van Nhan Vo*

Main category: cs.IT

TL;DR: This paper explores the use of deep reinforcement learning (DRL) in a Metaverse system-based Digital Twin, demonstrating its effectiveness for offloading tasks in a dynamic environment.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to leverage the advantages of DRL to improve the efficiency and promptness of Digital Twin within a Metaverse system.

Method: The method involves creating a system that includes Metaverse User devices collecting real-world data, transferring it to a virtual world via a Metaverse Virtual Access Point (MVAP), and an edge computing server handling offloading data from the MVAP. A DRL algorithm is employed to manage tasks under a dynamic environment.

Result: The experiment results indicate that the proposed DRL algorithm effectively ensures the promptness of Digital Twin in a dynamic environment.

Conclusion: DRL offers significant advantages in assisting Metaverse system-based Digital Twin, particularly in managing offloading tasks efficiently.

Abstract: Metaverse and Digital Twin (DT) have attracted much academic and industrial
attraction to approach the future digital world. This paper introduces the
advantages of deep reinforcement learning (DRL) in assisting Metaverse
system-based Digital Twin. In this system, we assume that it includes several
Metaverse User devices collecting data from the real world to transfer it into
the virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the
processing of data, and an edge computing server that receives the offloading
data from the MVAP. The proposed model works under a dynamic environment with
various parameters changing over time. The experiment results show that our
proposed DRL algorithm is suitable for offloading tasks to ensure the
promptness of DT in a dynamic environment.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [375] [PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis](https://arxiv.org/abs/2506.02794)
*Mijeong Kim,Gunhee Kim,Jungyoon Choi,Wonjae Roh,Bohyung Han*

Main category: cs.GR

TL;DR: The paper introduces PhysGaia, a new physics-aware dataset for Dynamic Novel View Synthesis (DyNVS) that includes both structured objects and unstructured physical phenomena. It provides complex dynamic scenarios with rich interactions among multiple objects, diverse physical materials, and ground-truth information for quantitative evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing datasets primarily focus on photorealistic reconstruction but lack support for physics-aware dynamic scene modeling.

Method: PhysGaia is designed to actively support physics-aware dynamic scene modeling by providing complex dynamic scenarios with rich interactions among multiple objects, diverse physical materials, and essential ground-truth information including 3D particle trajectories and physics parameters.

Result: PhysGaia will significantly advance research in dynamic view synthesis, physics-based scene understanding, and deep learning models integrated with physical simulation.

Conclusion: By addressing the critical lack of datasets for physics-aware modeling, PhysGaia enables more faithful reconstruction and interpretation of complex dynamic scenes.

Abstract: We introduce PhysGaia, a novel physics-aware dataset specifically designed
for Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects
and unstructured physical phenomena. Unlike existing datasets that primarily
focus on photorealistic reconstruction, PhysGaia is created to actively support
physics-aware dynamic scene modeling. Our dataset provides complex dynamic
scenarios with rich interactions among multiple objects, where they
realistically collide with each other and exchange forces. Furthermore, it
contains a diverse range of physical materials, such as liquid, gas,
viscoelastic substance, and textile, which moves beyond the rigid bodies
prevalent in existing datasets. All scenes in PhysGaia are faithfully generated
to strictly adhere to physical laws, leveraging carefully selected
material-specific physics solvers. To enable quantitative evaluation of
physical modeling, our dataset provides essential ground-truth information,
including 3D particle trajectories and physics parameters, e.g., viscosity. To
facilitate research adoption, we also provide essential integration pipelines
for using state-of-the-art DyNVS models with our dataset and report their
results. By addressing the critical lack of datasets for physics-aware
modeling, PhysGaia will significantly advance research in dynamic view
synthesis, physics-based scene understanding, and deep learning models
integrated with physical simulation -- ultimately enabling more faithful
reconstruction and interpretation of complex dynamic scenes. Our datasets and
codes are available in the project website,
http://cvlab.snu.ac.kr/research/PhysGaia.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [376] [Online Bayesian system identification in multivariate autoregressive models via message passing](https://arxiv.org/abs/2506.02710)
*T. N. Nisslbeck,Wouter M. Kouw*

Main category: eess.SP

TL;DR: 提出了一种基于因子图消息传递的递归贝叶斯估计方法，用于带外源输入的多元自回归模型。该方法能够生成自回归系数和噪声精度的完整后验分布，并支持在线模型证据计算。实验结果表明，在合成自回归系统上具有经验收敛性，并在双质量弹簧阻尼系统上表现出竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有的递归最小二乘法无法提供完整的后验分布，且不能充分传播不确定性至预测中。因此需要一种更精确的方法来处理多元自回归模型中的参数估计和不确定性传播问题。

Method: 采用基于因子图消息传递的递归贝叶斯估计方法，构建了多元自回归模型的估计过程，同时生成自回归系数和噪声精度的完整后验分布。

Result: 在合成自回归系统上展示了经验收敛性，并在双质量弹簧阻尼系统上达到了与现有方法竞争的性能水平。

Conclusion: 所提出的递归贝叶斯估计方法可以有效地估计多元自回归模型的参数，并将不确定性传播到未来系统输出的预测中，适用于在线模型证据计算。

Abstract: We propose a recursive Bayesian estimation procedure for multivariate
autoregressive models with exogenous inputs based on message passing in a
factor graph. Unlike recursive least-squares, our method produces full
posterior distributions for both the autoregressive coefficients and noise
precision. The uncertainties regarding these estimates propagate into the
uncertainties on predictions for future system outputs, and support online
model evidence calculations. We demonstrate convergence empirically on a
synthetic autoregressive system and competitive performance on a double
mass-spring-damper system.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [377] [Second-order AAA algorithms for structured data-driven modeling](https://arxiv.org/abs/2506.02241)
*Michael S. Ackermann,Ion Victor Gosea,Serkan Gugercin,Steffen W. R. Werner*

Main category: math.NA

TL;DR: 这篇论文提出三种从频域数据直接构建具有二阶微分结构的动力系统的数据驱动建模方法，并通过三个数值例子证明了这些新方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在基于数据的动力系统建模过程中，常常忽视物理现象背后的微分结构，这使得以物理意义解释所学模型变得非常困难。

Method: 基于二阶结构的重心形式，将著名的自适应Antoulas-Anderson算法扩展到二阶系统的情况。根据可用的计算资源，提出该方法的变化形式，优先考虑更高的计算速度或更大的建模准确性。

Result: 理论分析表明所提出的方法具有预期的准确性和性能。三个数值例子证明了这些新方法的有效性。

Conclusion: 所提出的具有二阶微分结构的数据驱动建模方法可以从频域数据中直接构建动力系统，并且比经典的无结构数据驱动建模更有效。

Abstract: The data-driven modeling of dynamical systems has become an essential tool
for the construction of accurate computational models from real-world data. In
this process, the inherent differential structures underlying the considered
physical phenomena are often neglected making the reinterpretation of the
learned models in a physically meaningful sense very challenging. In this work,
we present three data-driven modeling approaches for the construction of
dynamical systems with second-order differential structure directly from
frequency domain data. Based on the second-order structured barycentric form,
we extend the well-known Adaptive Antoulas-Anderson algorithm to the case of
second-order systems. Depending on the available computational resources, we
propose variations of the proposed method that prioritize either higher
computation speed or greater modeling accuracy, and we present a theoretical
analysis for the expected accuracy and performance of the proposed methods.
Three numerical examples demonstrate the effectiveness of our new structured
approaches in comparison to classical unstructured data-driven modeling.

</details>
