<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 34]
- [cs.LG](#cs.LG) [Total: 123]
- [cs.CR](#cs.CR) [Total: 18]
- [cs.SD](#cs.SD) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [stat.ME](#stat.ME) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 28]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.DB](#cs.DB) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [math.OC](#math.OC) [Total: 5]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [quant-ph](#quant-ph) [Total: 5]
- [cs.SE](#cs.SE) [Total: 7]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.CL](#cs.CL) [Total: 40]
- [stat.ML](#stat.ML) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TIP-Search: Time-Predictable Inference Scheduling for Market Prediction under Uncertain Load](https://arxiv.org/abs/2506.08026)
*Xibai Wang*

Main category: cs.AI

TL;DR: This paper proposes TIP-Search, a framework for real-time market prediction which can improve accuracy by 8.5% and ensure 100% deadline satisfaction.


<details>
  <summary>Details</summary>
Motivation: Strict latency demands in high-frequency financial systems motivate the need for a time-predictable inference scheduling framework.

Method: TIP-Search dynamically selects deep learning models from a heterogeneous pool based on offline profiling of latency and generalization performance, with online task-aware selection not needing explicit input domain labels.

Result: TIP-Search outperforms static baselines with up to 8.5% improvement in accuracy and achieves 100% deadline satisfaction when evaluated on three real-world datasets.

Conclusion: TIP-Search is effective for robust low-latency financial inference under uncertain workloads.

Abstract: This paper proposes TIP-Search, a time-predictable inference scheduling
framework for real-time market prediction under uncertain workloads. Motivated
by the strict latency demands in high-frequency financial systems, TIP-Search
dynamically selects a deep learning model from a heterogeneous pool, aiming to
maximize predictive accuracy while satisfying per-task deadline constraints.
Our approach profiles latency and generalization performance offline, then
performs online task-aware selection without relying on explicit input domain
labels. We evaluate TIP-Search on three real-world limit order book datasets
(FI-2010, Binance BTC/USDT, LOBSTER AAPL) and demonstrate that it outperforms
static baselines with up to 8.5% improvement in accuracy and 100% deadline
satisfaction. Our results highlight the effectiveness of TIP-Search in robust
low-latency financial inference under uncertainty.

</details>


### [2] [Cognitive Weave: Synthesizing Abstracted Knowledge with a Spatio-Temporal Resonance Graph](https://arxiv.org/abs/2506.08098)
*Akash Vishwakarma,Hojin Lee,Mohith Suresh,Priyam Shankar Sharma,Rahul Vishwakarma,Sparsh Gupta,Yuvraj Anupam Chauhan*

Main category: cs.AI

TL;DR: This paper introduces Cognitive Weave, a novel memory framework based on a multi-layered spatio-temporal resonance graph (STRG) that manages information as insight particles interconnected through typed relational strands. It includes a cognitive refinement process for synthesizing higher-level knowledge structures and shows significant improvements in long-horizon planning tasks, question-answering scenarios, and dialogue coherence.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of current memory systems in terms of structural flexibility, temporal awareness, and the ability to synthesize higher-level insights from raw interaction data for capable large language model (LLM) based agents.

Method: The method involves introducing Cognitive Weave, which utilizes a multi-layered spatio-temporal resonance graph (STRG) to manage information as semantically rich insight particles (IPs). These IPs are dynamically enriched via a semantic oracle interface (SOI) and interconnected through typed relational strands. The system also features a cognitive refinement process for synthesizing insight aggregates (IAs).

Result: Experimental results show a 34% average improvement in task completion rates and a 42% reduction in mean query latency compared to state-of-the-art baselines in long-horizon planning tasks, evolving question-answering scenarios, and multi-session dialogue coherence.

Conclusion: Cognitive Weave demonstrates marked enhancement over existing approaches in various tasks and raises important ethical considerations for advanced memory systems in LLMs, while outlining future research trajectories.

Abstract: The emergence of capable large language model (LLM) based agents necessitates
memory architectures that transcend mere data storage, enabling continuous
learning, nuanced reasoning, and dynamic adaptation. Current memory systems
often grapple with fundamental limitations in structural flexibility, temporal
awareness, and the ability to synthesize higher-level insights from raw
interaction data. This paper introduces Cognitive Weave, a novel memory
framework centered around a multi-layered spatio-temporal resonance graph
(STRG). This graph manages information as semantically rich insight particles
(IPs), which are dynamically enriched with resonance keys, signifiers, and
situational imprints via a dedicated semantic oracle interface (SOI). These IPs
are interconnected through typed relational strands, forming an evolving
knowledge tapestry. A key component of Cognitive Weave is the cognitive
refinement process, an autonomous mechanism that includes the synthesis of
insight aggregates (IAs) condensed, higher-level knowledge structures derived
from identified clusters of related IPs. We present comprehensive experimental
results demonstrating Cognitive Weave's marked enhancement over existing
approaches in long-horizon planning tasks, evolving question-answering
scenarios, and multi-session dialogue coherence. The system achieves a notable
34% average improvement in task completion rates and a 42% reduction in mean
query latency when compared to state-of-the-art baselines. Furthermore, this
paper explores the ethical considerations inherent in such advanced memory
systems, discusses the implications for long-term memory in LLMs, and outlines
promising future research trajectories.

</details>


### [3] [SOP-Bench: Complex Industrial SOPs for Evaluating LLM Agents](https://arxiv.org/abs/2506.08119)
*Subhrangshu Nandi,Arghya Datta,Nikhil Vichare,Indranil Bhattacharya,Huzefa Raja,Jing Xu,Shayan Ray,Giuseppe Carenini,Abhi Srivastava,Aaron Chan,Man Ho Woo,Amar Kandola,Brandon Theresa,Francesco Carbone*

Main category: cs.AI

TL;DR: 大型语言模型在执行复杂的、需要严格遵循标准操作程序（SOP）的工作流方面存在不足。本文提出了三个贡献：1) 合成数据生成框架以创建现实的行业级SOP；2) SOP-Bench基准测试，包含超过1,800个任务；3) 对两种代理架构进行评估，发现当前LLM的代理能力与实际需求之间存在显著差距。性能因任务和领域而异，强调了领域特定基准测试和架构选择的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型展示了强大的通用推理和问题解决能力，但在需要严格遵守标准操作程序（SOP）的复杂、长期工作流程中表现不佳。此外，缺乏反映SOP复杂性、结构和领域特定细微差别的公开基准。

Method: 1. 提出合成数据生成框架以创建现实的行业级SOP；2. 使用该框架开发SOP-Bench基准测试，包含超过1,800个跨10个工业领域的任务，每个任务都配有API、工具接口和人工验证的测试用例；3. 在SOP-Bench上评估Function-Calling和ReAct Agents两种代理架构的性能。

Result: Function-Calling和ReAct Agents的平均成功率分别为27%和48%。当工具注册表远大于必要时，代理几乎100%调用错误工具。这表明当前LLM的代理能力与自动化真实世界SOP的需求之间存在显著差距。性能因任务和领域而异。

Conclusion: 研究结果强调了在部署前进行领域特定基准测试和架构选择的必要性。SOP-Bench及其数据生成框架的提示已公开发布，邀请社区扩展SOP-Bench以涵盖更多工业领域的SOP。

Abstract: Large Language Models (LLMs) demonstrate impressive general-purpose reasoning
and problem-solving abilities. However, they struggle with executing complex,
long-horizon workflows that demand strict adherence to Standard Operating
Procedures (SOPs), a critical requirement for real-world industrial automation.
Despite this need, there is a lack of public benchmarks that reflect the
complexity, structure, and domain-specific nuances of SOPs. To address this, we
present three main contributions. First, we introduce a synthetic data
generation framework to create realistic, industry-grade SOPs that rigorously
test the planning, reasoning, and tool-use capabilities of LLM-based agents.
Second, using this framework, we develop SOP-Bench, a benchmark of over 1,800
tasks across 10 industrial domains, each with APIs, tool interfaces, and
human-validated test cases. Third, we evaluate two prominent agent
architectures: Function-Calling and ReAct Agents, on SOP-Bench, observing
average success rates of only 27% and 48%, respectively. Remarkably, when the
tool registry is much larger than necessary, agents invoke incorrect tools
nearly 100% of the time. These findings underscore a substantial gap between
current agentic capabilities of LLMs and the demands of automating real-world
SOPs. Performance varies significantly by task and domain, highlighting the
need for domain-specific benchmarking and architectural choices before
deployment. SOP-Bench is publicly available at
http://sop-bench.s3-website-us-west-2.amazonaws.com/. We also release the
prompts underpinning the data generation framework to support new
domain-specific SOP benchmarks. We invite the community to extend SOP-Bench
with SOPs from their industrial domains.

</details>


### [4] [The AI Imperative: Scaling High-Quality Peer Review in Machine Learning](https://arxiv.org/abs/2506.08134)
*Qiyao Wei,Samuel Holt,Jing Yang,Markus Wulfmeier,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: Peer review in machine learning is facing challenges due to the increasing number of submissions and limited reviewer capacity. This paper argues for AI-assisted peer review as a solution, proposing a comprehensive ecosystem leveraging Large Language Models to assist authors, reviewers, and Area Chairs. The development of such systems requires access to structured peer review data and faces technical and ethical challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the crisis of scale in peer review within machine learning, with exponential growth in manuscript submissions outpacing the capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue.

Method: The method involves advocating for an AI-augmented ecosystem that uses Large Language Models as collaborators rather than replacements for human judgment. Specific roles for AI are proposed in factual verification, guiding reviewer performance, assisting authors, and supporting decision-making for Area Chairs.

Result: The result would be a more scalable and consistent peer review process, ensuring the integrity and high standards of scientific validation through AI assistance.

Conclusion: The conclusion calls upon the ML community to proactively build an AI-assisted future for peer review, addressing technical and ethical challenges while maintaining high standards of peer review.

Abstract: Peer review, the bedrock of scientific advancement in machine learning (ML),
is strained by a crisis of scale. Exponential growth in manuscript submissions
to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite
capacity of qualified reviewers, leading to concerns about review quality,
consistency, and reviewer fatigue. This position paper argues that AI-assisted
peer review must become an urgent research and infrastructure priority. We
advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language
Models (LLMs) not as replacements for human judgment, but as sophisticated
collaborators for authors, reviewers, and Area Chairs (ACs). We propose
specific roles for AI in enhancing factual verification, guiding reviewer
performance, assisting authors in quality improvement, and supporting ACs in
decision-making. Crucially, we contend that the development of such systems
hinges on access to more granular, structured, and ethically-sourced peer
review process data. We outline a research agenda, including illustrative
experiments, to develop and validate these AI assistants, and discuss
significant technical and ethical challenges. We call upon the ML community to
proactively build this AI-assisted future, ensuring the continued integrity and
scalability of scientific validation, while maintaining high standards of peer
review.

</details>


### [5] [Compiling Metric Temporal Answer Set Programming](https://arxiv.org/abs/2506.08150)
*Arvid Becker,Pedro Cabalar,Martin Diéguez,Javier Romero,Susana Hahn,Torsten Schaub*

Main category: cs.AI

TL;DR: 开发了一种计算方法来解决度量ASP问题，允许表达定量的时间约束，并通过差分约束扩展ASP以保持可扩展性，从而解决了时间粒度对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 需要一种方法来处理带有定量时间约束的逻辑编程问题，同时避免因细粒度时间约束导致的性能下降。

Method: 采用ASP的差分约束扩展，将与时间相关的问题外部化处理，从而解耦度量ASP和时间粒度。

Result: 提出的方法不受时间精度影响，成功解决了时间粒度带来的可扩展性问题。

Conclusion: 此方法为处理带有时间约束的逻辑编程提供了一个新的解决方案，能够有效应对细粒度时间约束带来的挑战。

Abstract: We develop a computational approach to Metric Answer Set Programming (ASP) to
allow for expressing quantitative temporal constrains, like durations and
deadlines. A central challenge is to maintain scalability when dealing with
fine-grained timing constraints, which can significantly exacerbate ASP's
grounding bottleneck. To address this issue, we leverage extensions of ASP with
difference constraints, a simplified form of linear constraints, to handle
time-related aspects externally. Our approach effectively decouples metric ASP
from the granularity of time, resulting in a solution that is unaffected by
time precision.

</details>


### [6] [AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data](https://arxiv.org/abs/2506.08306)
*Tuan Truong,Rithwik Sudharsan,Yibo Yang,Peter Xiangyuan Ma,Ruihan Yang,Stephan Mandt,Joshua S. Bloom*

Main category: cs.AI

TL;DR: 本研究通过引入AstroCompress挑战，提供四个新数据集和一个传统数据集，以及对七种无损压缩方法（包括三种神经网络方法）的基准测试，展示了神经网络无损压缩技术在天文观测数据收集中的潜力，并为科学应用中采用神经压缩提供了指导。


<details>
  <summary>Details</summary>
Motivation: 天文观测数据传输受限于空间和地面观测站的冷暗环境条件，这限制了数据采集量。改进无损数据压缩技术可以在不增加仪器成本的情况下，显著提升可观测的科学数据量。

Method: 研究提出了AstroCompress挑战，包含四个新数据集和一个传统数据集，涵盖不同模式的16位无符号整数成像数据。同时，研究对七种无损压缩方法进行了基准测试，其中包括三种神经网络方法和四种非神经网络方法（所有当前实际最先进的算法）。

Result: 实验结果表明，无损神经压缩技术能够提高天文观测站的数据采集能力，优于经典压缩方法，并为科学应用中采用神经压缩提供了具体指导。此外，研究还探讨了未来探索有损压缩方法的潜力。

Conclusion: 神经网络无损压缩技术在天文数据压缩领域具有显著优势，可以有效提升观测数据的采集效率。尽管本文仅限于无损压缩，但有损压缩方法在未来研究中也值得进一步探索。

Abstract: The site conditions that make astronomical observatories in space and on the
ground so desirable -- cold and dark -- demand a physical remoteness that leads
to limited data transmission capabilities. Such transmission limitations
directly bottleneck the amount of data acquired and in an era of costly modern
observatories, any improvements in lossless data compression has the potential
scale to billions of dollars worth of additional science that can be
accomplished on the same instrument. Traditional lossless methods for
compressing astrophysical data are manually designed. Neural data compression,
on the other hand, holds the promise of learning compression algorithms
end-to-end from data and outperforming classical techniques by leveraging the
unique spatial, temporal, and wavelength structures of astronomical images.
This paper introduces AstroCompress: a neural compression challenge for
astrophysics data, featuring four new datasets (and one legacy dataset) with
16-bit unsigned integer imaging data in various modes: space-based,
ground-based, multi-wavelength, and time-series imaging. We provide code to
easily access the data and benchmark seven lossless compression methods (three
neural and four non-neural, including all practical state-of-the-art
algorithms). Our results on lossless compression indicate that lossless neural
compression techniques can enhance data collection at observatories, and
provide guidance on the adoption of neural compression in scientific
applications. Though the scope of this paper is restricted to lossless
compression, we also comment on the potential exploration of lossy compression
methods in future studies.

</details>


### [7] [LeanTutor: A Formally-Verified AI Tutor for Mathematical Proofs](https://arxiv.org/abs/2506.08321)
*Manooshree Patel,Rayna Bhattacharyya,Thomas Lu,Arnav Mehta,Niels Voss,Narges Norouzi,Gireeja Ranade*

Main category: cs.AI

TL;DR: The paper introduces LeanTutor, an LLM-based tutoring system for math proofs that interacts with students in natural language, verifies proofs, generates next steps, and provides instructional guidance. It consists of three modules: autoformalizer/proof-checker, next-step generator, and feedback generator. Evaluated using PeanoBench dataset, the Autoformalizer correctly formalizes 57% of tactics in correct proofs and identifies incorrect steps in 30% of incorrect proofs.


<details>
  <summary>Details</summary>
Motivation: To create a tutoring system that can help students learn math proofs by interacting in natural language, verifying proofs, generating correct next steps, and providing appropriate instructional guidance.

Method: LeanTutor is composed of three modules: (i) an autoformalizer/proof-checker that autoformalizes student proofs into Lean and verifies proof accuracy; (ii) a next-step generator that outputs a valid next Lean tactic for incorrect proofs via LLM-based candidate generation and proof search; and (iii) a natural language feedback generator that produces pedagogically-motivated hints for the student user.

Result: The Autoformalizer correctly formalizes 57% of tactics in correct proofs and accurately identifies the incorrect step in 30% of incorrect proofs. LeanTutor outperforms a simple baseline on accuracy and relevance metrics when generating natural language hints for erroneous proofs.

Conclusion: LeanTutor represents a significant advancement in AI-based educational tools for teaching mathematical proofs, effectively combining natural language interaction, formal verification, and pedagogical feedback.

Abstract: We present LeanTutor, a Large Language Model (LLM)-based tutoring system for
math proofs. LeanTutor interacts with the student in natural language, formally
verifies student-written math proofs in Lean, generates correct next steps, and
provides the appropriate instructional guidance. LeanTutor is composed of three
modules: (i) an autoformalizer/proof-checker, (ii) a next-step generator, and
(iii) a natural language feedback generator. The first module faithfully
autoformalizes student proofs into Lean and verifies proof accuracy via
successful code compilation. If the proof has an error, the incorrect step is
identified. The next-step generator module outputs a valid next Lean tactic for
incorrect proofs via LLM-based candidate generation and proof search. The
feedback generator module leverages Lean data to produce a
pedagogically-motivated natural language hint for the student user. To evaluate
our system, we introduce PeanoBench, a human-written dataset derived from the
Natural Numbers Game, consisting of 371 Peano Arithmetic proofs, where each
natural language proof step is paired with the corresponding logically
equivalent tactic in Lean. The Autoformalizer correctly formalizes 57% of
tactics in correct proofs and accurately identifies the incorrect step in 30%
of incorrect proofs. In generating natural language hints for erroneous proofs,
LeanTutor outperforms a simple baseline on accuracy and relevance metrics.

</details>


### [8] [ORFS-agent: Tool-Using Agents for Chip Design Optimization](https://arxiv.org/abs/2506.08332)
*Amur Ghose,Andrew B. Kahng,Sayak Kundu,Zhiang Wang*

Main category: cs.AI

TL;DR: The paper introduces ORFS-agent, an LLM-based optimization agent that automates parameter tuning in hardware design flows, showing improvements over Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing complex engineering workflows with thousands of parameters in integrated circuit design.

Method: ORFS-agent adaptively explores parameter configurations using Large Language Models for iterative optimization.

Result: Empirical evaluations show ORFS-agent improves routed wirelength and effective clock period by over 13%, with 40% fewer optimization iterations.

Conclusion: ORFS-agent presents a flexible, interpretable framework for multi-objective optimization and is modular and model-agnostic.

Abstract: Machine learning has been widely used to optimize complex engineering
workflows across numerous domains. In the context of integrated circuit design,
modern flows (e.g., going from a register-transfer level netlist to physical
layouts) involve extensive configuration via thousands of parameters, and small
changes to these parameters can have large downstream impacts on desired
outcomes - namely design performance, power, and area. Recent advances in Large
Language Models (LLMs) offer new opportunities for learning and reasoning
within such high-dimensional optimization tasks. In this work, we introduce
ORFS-agent, an LLM-based iterative optimization agent that automates parameter
tuning in an open-source hardware design flow. ORFS-agent adaptively explores
parameter configurations, demonstrating clear improvements over standard
Bayesian optimization approaches in terms of resource efficiency and final
design metrics. Our empirical evaluations on two different technology nodes and
a range of circuit benchmarks indicate that ORFS-agent can improve both routed
wirelength and effective clock period by over 13%, all while using 40% fewer
optimization iterations. Moreover, by following natural language objectives to
trade off certain metrics for others, ORFS-agent demonstrates a flexible and
interpretable framework for multi-objective optimization. Crucially, RFS-agent
is modular and model-agnostic, and can be plugged in to any frontier LLM
without any further fine-tuning.

</details>


### [9] [FloorplanMAE:A self-supervised framework for complete floorplan generation from partial inputs](https://arxiv.org/abs/2506.08363)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Miao Zhang,Ran Luo,Shuai Lu*

Main category: cs.AI

TL;DR: FloorplanMAE is a self-supervised learning framework that can predict and generate complete floor plans from partial ones, enhancing architectural design efficiency.


<details>
  <summary>Details</summary>
Motivation: In architectural design, creating floorplans is an iterative process requiring repeated modifications. Predicting a complete floorplan from a partial one could significantly improve design efficiency and reduce workload.

Method: The study proposes FloorplanMAE which uses Masked Autoencoders (MAE) to reconstruct missing parts of floor plans. It involves developing a specific dataset, FloorplanNet, and employing a lightweight Vision Transformer (ViT) for training.

Result: Experimental results indicate that FloorplanMAE can successfully generate high-quality complete floor plans from incomplete ones, showing promise for scalable floor plan generation.

Conclusion: FloorplanMAE presents a valuable tool in architectural design, offering a scalable solution with significant potential applications in improving design processes.

Abstract: In the architectural design process, floorplan design is often a dynamic and
iterative process. Architects progressively draw various parts of the floorplan
according to their ideas and requirements, continuously adjusting and refining
throughout the design process. Therefore, the ability to predict a complete
floorplan from a partial one holds significant value in the design process.
Such prediction can help architects quickly generate preliminary designs,
improve design efficiency, and reduce the workload associated with repeated
modifications. To address this need, we propose FloorplanMAE, a self-supervised
learning framework for restoring incomplete floor plans into complete ones.
First, we developed a floor plan reconstruction dataset, FloorplanNet,
specifically trained on architectural floor plans. Secondly, we propose a floor
plan reconstruction method based on Masked Autoencoders (MAE), which
reconstructs missing parts by masking sections of the floor plan and training a
lightweight Vision Transformer (ViT). We evaluated the reconstruction accuracy
of FloorplanMAE and compared it with state-of-the-art benchmarks. Additionally,
we validated the model using real sketches from the early stages of
architectural design. Experimental results show that the FloorplanMAE model can
generate high-quality complete floor plans from incomplete partial plans. This
framework provides a scalable solution for floor plan generation, with broad
application prospects.

</details>


### [10] [On Reasoning Strength Planning in Large Reasoning Models](https://arxiv.org/abs/2506.08390)
*Leheng Sheng,An Zhang,Zijian Wu,Weixiang Zhao,Changshuo Shen,Yi Zhang,Xiang Wang,Tat-Seng Chua*

Main category: cs.AI

TL;DR: 近期研究表明，大型推理模型（LRMs）可以自动为更难的问题分配更多的推理能力（即推理令牌的数量），展现出对任务性能更好的难度感知。然而，这种自动推理强度分配现象的内在机制尚未被充分研究。本文从模型激活的角度解释了这一现象，发现LRMs在生成前就已经在其激活中预规划了推理强度，并且该强度由一个预分配的方向向量的大小因果控制。通过线性探针仅根据问题激活就可以预测推理令牌的数量，表明LRMs提前估计了所需的推理强度。此外，LRMs通过嵌入在模型激活中的方向向量编码此推理强度，向量的大小调节推理强度。减去这个向量会导致推理令牌数量和性能的降低，而增加这个向量则会增加推理令牌数量甚至提高性能。最后，本文展示了两个潜在应用：过度思考行为检测和简单问题的有效推理。


<details>
  <summary>Details</summary>
Motivation: 尽管已经广泛观察到大型推理模型（LRMs）具有自动分配推理强度的现象，但其背后的机制尚未被深入探讨。因此，需要从模型激活的角度来揭示这种现象的成因，以更好地理解LRMs的工作原理并提供控制其推理行为的工具。

Method: 作者首先使用线性探针分析了LRMs在生成前的激活情况，证明了推理令牌数量可以根据问题激活进行预测。接着，研究发现LRMs通过一个预分配的方向向量在激活中编码推理强度，该向量的大小直接影响推理强度。实验通过增减这个向量验证了其对推理令牌数量和性能的影响。最后，作者揭示了该方向向量如何修改推理结束标记</think>的logits以影响推理长度，并提出了两种实际应用。

Result: 实验结果表明，通过线性探针可以成功预测推理令牌数量，证实LRMs确实提前规划了推理强度。同时，增减方向向量能够显著改变推理令牌数量和性能，进一步验证了该向量对推理强度的调控作用。

Conclusion: 本文从模型激活的角度揭示了LRMs自动分配推理强度的机制，发现了预分配方向向量对推理强度的因果控制作用，并展示了其在过度思考行为检测和简单问题高效推理中的潜在应用。这些发现为理解和控制LRMs的推理行为提供了新的视角和工具。

Abstract: Recent studies empirically reveal that large reasoning models (LRMs) can
automatically allocate more reasoning strengths (i.e., the number of reasoning
tokens) for harder problems, exhibiting difficulty-awareness for better task
performance. While this automatic reasoning strength allocation phenomenon has
been widely observed, its underlying mechanism remains largely unexplored. To
this end, we provide explanations for this phenomenon from the perspective of
model activations. We find evidence that LRMs pre-plan the reasoning strengths
in their activations even before generation, with this reasoning strength
causally controlled by the magnitude of a pre-allocated directional vector.
Specifically, we show that the number of reasoning tokens is predictable solely
based on the question activations using linear probes, indicating that LRMs
estimate the required reasoning strength in advance. We then uncover that LRMs
encode this reasoning strength through a pre-allocated directional vector
embedded in the activations of the model, where the vector's magnitude
modulates the reasoning strength. Subtracting this vector can lead to reduced
reasoning token number and performance, while adding this vector can lead to
increased reasoning token number and even improved performance. We further
reveal that this direction vector consistently yields positive reasoning length
prediction, and it modifies the logits of end-of-reasoning token </think> to
affect the reasoning length. Finally, we demonstrate two potential applications
of our findings: overthinking behavior detection and enabling efficient
reasoning on simple problems. Our work provides new insights into the internal
mechanisms of reasoning in LRMs and offers practical tools for controlling
their reasoning behaviors. Our code is available at
https://github.com/AlphaLab-USTC/LRM-plans-CoT.

</details>


### [11] [SafeCoT: Improving VLM Safety with Minimal Reasoning](https://arxiv.org/abs/2506.08399)
*Jiachen Ma,Zhanhui Zhou,Chao Yang,Chaochao Lu*

Main category: cs.AI

TL;DR: SafeCoT is a lightweight framework that improves refusal behavior in vision-language models by using rule-based chain-of-thought supervision, significantly reducing overrefusal and enhancing generalization.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to ensure safe and appropriate responses from vision-language models especially in high-risk or ambiguous scenarios. Current methods rely on large-scale safety annotations or complex modeling which can be resource-intensive.

Method: The SafeCoT framework uses minimal, rule-based chain-of-thought (CoT) supervision to help models reason about safety risks and make context-aware refusals without needing large-scale safety annotations or complex modeling.

Result: Experiments across multiple benchmarks indicate that SafeCoT significantly reduces overrefusal and enhances generalization even with limited training data.

Conclusion: SafeCoT offers a scalable solution for aligning vision-language models with safety-critical objectives.

Abstract: Ensuring safe and appropriate responses from vision-language models (VLMs)
remains a critical challenge, particularly in high-risk or ambiguous scenarios.
We introduce SafeCoT, a lightweight, interpretable framework that leverages
rule-based chain-of-thought (CoT) supervision to improve refusal behavior in
VLMs. Unlike prior methods that rely on large-scale safety annotations or
complex modeling, SafeCoT uses minimal supervision to help models reason about
safety risks and make context-aware refusals. Experiments across multiple
benchmarks show that SafeCoT significantly reduces overrefusal and enhances
generalization, even with limited training data. Our approach offers a scalable
solution for aligning VLMs with safety-critical objectives.

</details>


### [12] [Single-Node Trigger Backdoor Attacks in Graph-Based Recommendation Systems](https://arxiv.org/abs/2506.08401)
*Runze Li,Di Jin,Xiaobao Wang,Dongxiao He,Bingdao Feng,Zhen Wang*

Main category: cs.AI

TL;DR: This paper proposes a novel covert graph backdoor attack method for recommendation systems that enhances target item exposure to target users with minimal impact on system performance.


<details>
  <summary>Details</summary>
Motivation: Graph recommendation systems are vulnerable to attacks, especially shilling attacks which inject fake nodes and edges. However, existing methods suffer from low stealth and high destructiveness.

Method: The paper designs a single-node trigger generator to expose multiple target items to the target user by inserting one fake user node. Constraint conditions are introduced between target and irrelevant nodes to reduce impact on the recommendation system's overall performance.

Result: In 99% of target users, the exposure of target items reached at least 50%, while the impact on the recommendation system's performance was controlled within approximately 5%.

Conclusion: The proposed graph backdoor attack method effectively increases target item exposure in a covert manner with limited influence on unrelated nodes and overall system performance.

Abstract: Graph recommendation systems have been widely studied due to their ability to
effectively capture the complex interactions between users and items. However,
these systems also exhibit certain vulnerabilities when faced with attacks. The
prevailing shilling attack methods typically manipulate recommendation results
by injecting a large number of fake nodes and edges. However, such attack
strategies face two primary challenges: low stealth and high destructiveness.
To address these challenges, this paper proposes a novel graph backdoor attack
method that aims to enhance the exposure of target items to the target user in
a covert manner, without affecting other unrelated nodes. Specifically, we
design a single-node trigger generator, which can effectively expose multiple
target items to the target user by inserting only one fake user node.
Additionally, we introduce constraint conditions between the target nodes and
irrelevant nodes to mitigate the impact of fake nodes on the recommendation
system's performance. Experimental results show that the exposure of the target
items reaches no less than 50% in 99% of the target users, while the impact on
the recommendation system's performance is controlled within approximately 5%.

</details>


### [13] [Transforming Expert Knowledge into Scalable Ontology via Large Language Models](https://arxiv.org/abs/2506.08422)
*Ikkei Itoku,David Theil,Evelyn Eichelsdoerfer Uehara,Sreyoshi Bhaduri,Junnosuke Kuroda,Toshi Yumoto,Alex Gil,Natalie Perez,Rajesh Cherukuri,Naumaan Nayyar*

Main category: cs.AI

TL;DR: The paper presents a framework combining LLMs with expert calibration and prompt optimization for automating taxonomy alignment, achieving an F1-score of 0.97.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of manual and existing automated methods in handling nuanced semantic relationships and maintaining consistency in large-scale taxonomy alignment.

Method: The method integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in generating both taxonomy linkages and supporting rationales.

Result: Evaluated on a domain-specific mapping task, the framework achieved an F1-score of 0.97, surpassing the human benchmark of 0.68.

Conclusion: The approach effectively scales taxonomy alignment while maintaining high-quality mappings and preserving expert oversight for ambiguous cases.

Abstract: Having a unified, coherent taxonomy is essential for effective knowledge
representation in domain-specific applications as diverse terminologies need to
be mapped to underlying concepts. Traditional manual approaches to taxonomy
alignment rely on expert review of concept pairs, but this becomes
prohibitively expensive and time-consuming at scale, while subjective
interpretations often lead to expert disagreements. Existing automated methods
for taxonomy alignment have shown promise but face limitations in handling
nuanced semantic relationships and maintaining consistency across different
domains. These approaches often struggle with context-dependent concept
mappings and lack transparent reasoning processes. We propose a novel framework
that combines large language models (LLMs) with expert calibration and
iterative prompt optimization to automate taxonomy alignment. Our method
integrates expert-labeled examples, multi-stage prompt engineering, and human
validation to guide LLMs in generating both taxonomy linkages and supporting
rationales. In evaluating our framework on a domain-specific mapping task of
concept essentiality, we achieved an F1-score of 0.97, substantially exceeding
the human benchmark of 0.68. These results demonstrate the effectiveness of our
approach in scaling taxonomy alignment while maintaining high-quality mappings
and preserving expert oversight for ambiguous cases.

</details>


### [14] [SHIELD: Multi-task Multi-distribution Vehicle Routing Solver with Sparsity and Hierarchy](https://arxiv.org/abs/2506.08424)
*Yong Liang Goh,Zhiguang Cao,Yining Ma,Jianan Zhou,Mohammad Haroon Dupty,Wee Sun Lee*

Main category: cs.AI

TL;DR: This paper introduces SHIELD, a novel model for Multi-Task Multi-Distribution VRP (MTMDVRP), which leverages sparsity and hierarchy principles to improve efficiency, generalization, and performance on real-world maps.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation of current foundation models for routing problems that overlook complex real-world customer distributions by advancing to a more realistic MTMDVRP setting.

Method: The method involves incorporating Mixture-of-Depths (MoD) technique for sparsity in a deeper decoder architecture and developing a context-based clustering layer for exploiting hierarchical structures, both contributing to better local representations and generalization.

Result: Empirical results show superiority over existing methods on 9 real-world maps with 16 VRP variants each.

Conclusion: SHIELD significantly improves generalization on unseen tasks and distributions in VRP problems.

Abstract: Recent advances toward foundation models for routing problems have shown
great potential of a unified deep model for various VRP variants. However, they
overlook the complex real-world customer distributions. In this work, we
advance the Multi-Task VRP (MTVRP) setting to the more realistic yet
challenging Multi-Task Multi-Distribution VRP (MTMDVRP) setting, and introduce
SHIELD, a novel model that leverages both sparsity and hierarchy principles.
Building on a deeper decoder architecture, we first incorporate the
Mixture-of-Depths (MoD) technique to enforce sparsity. This improves both
efficiency and generalization by allowing the model to dynamically select nodes
to use or skip each decoder layer, providing the needed capacity to adaptively
allocate computation for learning the task/distribution specific and shared
representations. We also develop a context-based clustering layer that exploits
the presence of hierarchical structures in the problems to produce better local
representations. These two designs inductively bias the network to identify key
features that are common across tasks and distributions, leading to
significantly improved generalization on unseen ones. Our empirical results
demonstrate the superiority of our approach over existing methods on 9
real-world maps with 16 VRP variants each.

</details>


### [15] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: 数学推理是AI研究的重要领域，近年来大型语言模型在该领域取得了显著进展。本文回顾了LLM在数学推理能力方面的进展，包括理解与答案生成两个阶段，并讨论了增强数学推理能力的方法以及面临的挑战和未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型的数学推理能力对于推动人工智能的发展至关重要，同时这一领域的研究也面临诸多挑战。

Method: 通过回顾两种高级认知阶段（理解与答案生成）的发展，以及从无训练提示到微调方法（如监督微调和强化学习）等增强数学推理能力的方法。

Result: 尽管取得了一定的进展，但在容量、效率和泛化方面仍存在根本性挑战。

Conclusion: 提出了有前景的研究方向，如先进的预训练和知识增强技术、形式推理框架及通过原则性学习范式实现的元泛化，为有兴趣提高LLM推理能力的研究人员提供了见解。

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [16] [Hybrid Reasoning for Perception, Explanation, and Autonomous Action in Manufacturing](https://arxiv.org/abs/2506.08462)
*Christos Margadji,Sebastian W. Pattinson*

Main category: cs.AI

TL;DR: CIPHER is a VLA model framework that integrates process expert and regression model to enable autonomous systems with strong generalization for industrial control.


<details>
  <summary>Details</summary>
Motivation: Industrial processes need robustness and adaptability, but AI-based control systems are limited by the need for extensive labelled datasets and foundation models lack the quantitative precision for engineering applications.

Method: Integrates a process expert and a regression model within a vision-language-action (VLA) model framework, uses retrieval-augmented generation to access external expert knowledge and supports physics-informed reasoning.

Result: Exhibits strong generalization to out-of-distribution tasks, interprets visual or textual inputs, explains decisions, and autonomously generates precise machine instructions without explicit annotations.

Conclusion: Lays the foundations for autonomous systems that act with precision, reason with context, and communicate decisions transparently, supporting safe deployment in industrial settings.

Abstract: Industrial processes must be robust and adaptable, as environments and tasks
are often unpredictable, while operational errors remain costly and difficult
to detect. AI-based control systems offer a path forward, yet typically depend
on supervised learning with extensive labelled datasets, which limits their
ability to generalize across variable and data-scarce industrial settings.
Foundation models could enable broader reasoning and knowledge integration, but
rarely deliver the quantitative precision demanded by engineering applications.
Here, we introduceControl and Interpretation of Production via Hybrid Expertise
and Reasoning (CIPHER): a vision-language-action (VLA) model framework aiming
to replicate human-like reasoning for industrial control, instantiated in a
commercial-grade 3D printer. It integrates a process expert, a regression model
enabling quantitative characterization of system states required for
engineering tasks. CIPHER also incorporates retrieval-augmented generation to
access external expert knowledge and support physics-informed, chain-of-thought
reasoning. This hybrid architecture exhibits strong generalization to
out-of-distribution tasks. It interprets visual or textual inputs from process
monitoring, explains its decisions, and autonomously generates precise machine
instructions, without requiring explicit annotations. CIPHER thus lays the
foundations for autonomous systems that act with precision, reason with
context, and communicate decisions transparently, supporting safe and trusted
deployment in industrial settings.

</details>


### [17] [RHealthTwin: Towards Responsible and Multimodal Digital Twins for Personalized Well-being](https://arxiv.org/abs/2506.08486)
*Rahatara Ferdousi,M Anwar Hossain*

Main category: cs.AI

TL;DR: The paper introduces RHealthTwin, a framework using LLMs for healthcare digital twins addressing issues like hallucination and bias through the Responsible Prompt Engine (RPE). It shows state-of-the-art results in health domains while maintaining high ethical compliance.


<details>
  <summary>Details</summary>
Motivation: To create a responsible AI-powered digital twin system for healthcare that addresses concerns such as hallucination, bias, lack of transparency, and ethical misuse in consumer health contexts.

Method: Proposes RHealthTwin with a core component called Responsible Prompt Engine (RPE) that processes multimodal inputs to produce safe, relevant, and explainable responses by structuring both user inputs and system instructions dynamically. Also includes a feedback loop for adaptation over time.

Result: RPE achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, BERTScore = 0.89 on benchmark datasets across four consumer health domains. Over 90% in ethical compliance and instruction-following metrics using LLM-as-judge evaluation.

Conclusion: RHealthTwin serves as a forward-looking foundation for responsible LLM-based applications in health and well-being assistance.

Abstract: The rise of large language models (LLMs) has created new possibilities for
digital twins in healthcare. However, the deployment of such systems in
consumer health contexts raises significant concerns related to hallucination,
bias, lack of transparency, and ethical misuse. In response to recommendations
from health authorities such as the World Health Organization (WHO), we propose
Responsible Health Twin (RHealthTwin), a principled framework for building and
governing AI-powered digital twins for well-being assistance. RHealthTwin
processes multimodal inputs that guide a health-focused LLM to produce safe,
relevant, and explainable responses. At the core of RHealthTwin is the
Responsible Prompt Engine (RPE), which addresses the limitations of traditional
LLM configuration. Conventionally, users input unstructured prompt and the
system instruction to configure the LLM, which increases the risk of
hallucination. In contrast, RPE extracts predefined slots dynamically to
structure both inputs. This guides the language model to generate responses
that are context aware, personalized, fair, reliable, and explainable for
well-being assistance. The framework further adapts over time through a
feedback loop that updates the prompt structure based on user satisfaction. We
evaluate RHealthTwin across four consumer health domains including mental
support, symptom triage, nutrition planning, and activity coaching. RPE
achieves state-of-the-art results with BLEU = 0.41, ROUGE-L = 0.63, and
BERTScore = 0.89 on benchmark datasets. Also, we achieve over 90% in ethical
compliance and instruction-following metrics using LLM-as-judge evaluation,
outperforming baseline strategies. We envision RHealthTwin as a forward-looking
foundation for responsible LLM-based applications in health and well-being.

</details>


### [18] [FEDTAIL: Federated Long-Tailed Domain Generalization with Sharpness-Guided Gradient Matching](https://arxiv.org/abs/2506.08518)
*Sunny Gupta,Nikita Jangid,Shounak Das,Amit Sethi*

Main category: cs.AI

TL;DR: FedTAIL is a federated domain generalization framework that addresses challenges in long-tailed class distributions and conflicting optimization objectives through sharpness-guided, gradient-aligned optimization. It incorporates a gradient coherence regularizer, performs class-wise sharpness minimization, and enhances conditional distribution alignment.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Domain Generalization (DG) falter under long-tailed class distributions and conflicting optimization objectives. There is a need for a method that can handle these challenges effectively.

Method: FedTAIL introduces sharpness-guided, gradient-aligned optimization. It includes a gradient coherence regularizer to mitigate conflicts between classification and adversarial objectives. For class imbalance, it performs class-wise sharpness minimization and proposes a curvature-aware dynamic weighting scheme. Conditional distribution alignment is enhanced by integrating sharpness-aware perturbations into entropy regularization.

Result: FedTAIL achieves state-of-the-art performance across standard domain generalization benchmarks, particularly excelling in the presence of domain shifts and label imbalance.

Conclusion: FedTAIL unifies optimization harmonization, class-aware regularization, and conditional alignment into a scalable, federated-compatible framework, proving effective in both centralized and federated settings.

Abstract: Domain Generalization (DG) seeks to train models that perform reliably on
unseen target domains without access to target data during training. While
recent progress in smoothing the loss landscape has improved generalization,
existing methods often falter under long-tailed class distributions and
conflicting optimization objectives. We introduce FedTAIL, a federated domain
generalization framework that explicitly addresses these challenges through
sharpness-guided, gradient-aligned optimization. Our method incorporates a
gradient coherence regularizer to mitigate conflicts between classification and
adversarial objectives, leading to more stable convergence. To combat class
imbalance, we perform class-wise sharpness minimization and propose a
curvature-aware dynamic weighting scheme that adaptively emphasizes
underrepresented tail classes. Furthermore, we enhance conditional distribution
alignment by integrating sharpness-aware perturbations into entropy
regularization, improving robustness under domain shift. FedTAIL unifies
optimization harmonization, class-aware regularization, and conditional
alignment into a scalable, federated-compatible framework. Extensive
evaluations across standard domain generalization benchmarks demonstrate that
FedTAIL achieves state-of-the-art performance, particularly in the presence of
domain shifts and label imbalance, validating its effectiveness in both
centralized and federated settings. Code: https://github.com/sunnyinAI/FedTail

</details>


### [19] [Safe and Economical UAV Trajectory Planning in Low-Altitude Airspace: A Hybrid DRL-LLM Approach with Compliance Awareness](https://arxiv.org/abs/2506.08532)
*Yanwei Gong,Xiaolin Chang*

Main category: cs.AI

TL;DR: The paper proposes a new UAV trajectory planning framework combining DRL with LLM reasoning, addressing urban airspace constraints and economic efficiency while showing superior performance in various metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges of UAV trajectory planning in complex urban environments within the context of the growing low-altitude economy, particularly focusing on urban airspace constraints and economic efficiency which are often neglected in existing studies.

Method: The method involves proposing a novel UAV trajectory planning framework that integrates deep reinforcement learning (DRL) with large language model (LLM) reasoning. This combination aims to improve learning efficiency and enable safe, compliant, and economically viable path planning for UAVs.

Result: The experimental results indicate that the proposed method surpasses existing baselines in multiple aspects such as data collection rate, collision avoidance, successful landing, regulatory compliance, and energy efficiency.

Conclusion: The conclusion drawn from this study is that the integration of DRL with LLM reasoning provides an effective solution to the key challenges of UAV trajectory planning under the constraints of the low-altitude economy networking.

Abstract: The rapid growth of the low-altitude economy has driven the widespread
adoption of unmanned aerial vehicles (UAVs). This growing deployment presents
new challenges for UAV trajectory planning in complex urban environments.
However, existing studies often overlook key factors, such as urban airspace
constraints and economic efficiency, which are essential in low-altitude
economy contexts. Deep reinforcement learning (DRL) is regarded as a promising
solution to these issues, while its practical adoption remains limited by low
learning efficiency. To overcome this limitation, we propose a novel UAV
trajectory planning framework that combines DRL with large language model (LLM)
reasoning to enable safe, compliant, and economically viable path planning.
Experimental results demonstrate that our method significantly outperforms
existing baselines across multiple metrics, including data collection rate,
collision avoidance, successful landing, regulatory compliance, and energy
efficiency. These results validate the effectiveness of our approach in
addressing UAV trajectory planning key challenges under constraints of the
low-altitude economy networking.

</details>


### [20] [HGFormer: A Hierarchical Graph Transformer Framework for Two-Stage Colonel Blotto Games via Reinforcement Learning](https://arxiv.org/abs/2506.08580)
*Yang Lv,Jinlong Lei,Peng Yi*

Main category: cs.AI

TL;DR: In a two-stage Colonel Blotto game, traditional methods struggle to find globally optimal strategies due to sequential dependencies and complex constraints. The paper proposes HGformer, a hierarchical graph Transformer framework with an enhanced encoder and two-agent decision model for efficient policy generation. A feedback reinforcement learning algorithm bridges the coordination gap between decision stages. Experiments show HGformer improves resource allocation efficiency and adversarial payoff.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges posed by sequential dependencies and complex constraints in a two-stage Colonel Blotto game, where traditional approaches fail to achieve globally optimal strategies.

Method: The method involves proposing a hierarchical graph Transformer framework named HGformer. It incorporates an enhanced graph Transformer encoder with structural biases and a two-agent hierarchical decision model for policy generation. Additionally, a layer-by-layer feedback reinforcement learning algorithm is designed to optimize higher-level strategy using long-term returns from lower-level decisions.

Result: HGformer significantly enhances resource allocation efficiency and adversarial payoff compared to existing hierarchical decision-making or graph neural network methods. It demonstrates superior overall performance in complex dynamic game scenarios.

Conclusion: HGformer successfully addresses the challenges of the two-stage Colonel Blotto game by efficiently generating policies and bridging the coordination gap between decision stages through its innovative architecture and reinforcement learning approach.

Abstract: Two-stage Colonel Blotto game represents a typical adversarial resource
allocation problem, in which two opposing agents sequentially allocate
resources in a network topology across two phases: an initial resource
deployment followed by multiple rounds of dynamic reallocation adjustments. The
sequential dependency between game stages and the complex constraints imposed
by the graph topology make it difficult for traditional approaches to attain a
globally optimal strategy. To address these challenges, we propose a
hierarchical graph Transformer framework called HGformer. By incorporating an
enhanced graph Transformer encoder with structural biases and a two-agent
hierarchical decision model, our approach enables efficient policy generation
in large-scale adversarial environments. Moreover, we design a layer-by-layer
feedback reinforcement learning algorithm that feeds the long-term returns from
lower-level decisions back into the optimization of the higher-level strategy,
thus bridging the coordination gap between the two decision-making stages.
Experimental results demonstrate that, compared to existing hierarchical
decision-making or graph neural network methods, HGformer significantly
improves resource allocation efficiency and adversarial payoff, achieving
superior overall performance in complex dynamic game scenarios.

</details>


### [21] [FoldA: Computing Partial-Order Alignments Using Directed Net Unfoldings](https://arxiv.org/abs/2506.08627)
*Douwe Geurtjens,Xixi Lu*

Main category: cs.AI

TL;DR: 提出了一种新的技术FoldA，用于使用有向Petri网展开计算部分顺序对齐。尽管计算时间更长，但通常减少了排队状态的数量，并提供了对并发性的更准确表示。


<details>
  <summary>Details</summary>
Motivation: 一致性检查是过程挖掘的基本任务，现有的方法在计算对齐时容易导致状态空间爆炸，并且无法充分表示现实世界过程中的并发行为。

Method: 提出了基于有向Petri网展开的部分顺序对齐（FoldA）技术，该技术可以在运行时动态计算对齐。

Result: 在485个合成模型日志对和19个现实生活及基准模型日志对上的评估表明，尽管计算时间较长，但减少了排队状态数量并更准确地表示了并发性。

Conclusion: FoldA技术为解决高选择性和并发性模型的一致性检查问题提供了一个有效的替代方案。

Abstract: Conformance checking is a fundamental task of process mining, which
quantifies the extent to which the observed process executions match a
normative process model. The state-of-the-art approaches compute alignments by
exploring the state space formed by the synchronous product of the process
model and the trace. This often leads to state space explosion, particularly
when the model exhibits a high degree of choice and concurrency. Moreover, as
alignments inherently impose a sequential structure, they fail to fully
represent the concurrent behavior present in many real-world processes. To
address these limitations, this paper proposes a new technique for computing
partial-order alignments {on the fly using directed Petri net unfoldings, named
FoldA. We evaluate our technique on 485 synthetic model-log pairs and compare
it against Astar- and Dijkstra-alignments on 13 real-life model-log pairs and 6
benchmark pairs. The results show that our unfolding alignment, although it
requires more computation time, generally reduces the number of queued states
and provides a more accurate representation of concurrency.

</details>


### [22] [Modular Recurrence in Contextual MDPs for Universal Morphology Control](https://arxiv.org/abs/2506.08630)
*Laurens Engwegen,Daan Brinks,Wendelin Böhmer*

Main category: cs.AI

TL;DR: This paper explores the use of a modular recurrent architecture in deep reinforcement learning to improve generalization for controlling robots with unseen dynamics, kinematics, and topologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a universal controller that can generalize well to new, unseen robot morphologies, thus improving computational and data efficiency.

Method: The method involves implementing a modular recurrent architecture that uses contextual information inferred through interactions, to enhance generalization performance.

Result: The results indicate a significant improvement in performance on robots with unseen dynamics, kinematics, and topologies across four different environments.

Conclusion: A modular recurrent architecture can effectively improve the generalization of robot control to previously unseen robot morphologies.

Abstract: A universal controller for any robot morphology would greatly improve
computational and data efficiency. By utilizing contextual information about
the properties of individual robots and exploiting their modular structure in
the architecture of deep reinforcement learning agents, steps have been made
towards multi-robot control. Generalization to new, unseen robots, however,
remains a challenge. In this paper we hypothesize that the relevant contextual
information is partially observable, but that it can be inferred through
interactions for better generalization to contexts that are not seen during
training. To this extent, we implement a modular recurrent architecture and
evaluate its generalization performance on a large set of MuJoCo robots. The
results show a substantial improved performance on robots with unseen dynamics,
kinematics, and topologies, in four different environments.

</details>


### [23] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: CoVo is a new self-rewarding reinforcement learning framework for Large Language Models (LLMs) that uses consistency and volatility of reasoning trajectories to improve performance without external supervision.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of relying on external supervision in reinforcement learning, which hinders broader applicability, this work aims to enhance LLM reasoning through a novel self-rewarding RL framework.

Method: The proposed method, CoVo, leverages the consistency of intermediate reasoning states across different reasoning trajectories. It introduces an intrinsic reward mechanism integrating Consistency and Volatility using a robust vector-space aggregation strategy, along with a curiosity bonus to promote exploration.

Result: Experiments on diverse reasoning benchmarks demonstrate that CoVo achieves performance comparable to or even surpassing supervised RL.

Conclusion: CoVo provides a scalable way for LLMs to learn reasoning without external supervision, showcasing its potential in enhancing LLMs' reasoning capabilities.

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [24] [A Sample Efficient Conditional Independence Test in the Presence of Discretization](https://arxiv.org/abs/2506.08747)
*Boyang Sun,Yu Yao,Xinshuai Dong,Zongfang Liu,Tongliang Liu,Yumou Qiu,Kun Zhang*

Main category: cs.AI

TL;DR: 这篇论文提出了一种样本高效的条件独立性（CI）检验方法，无需对数据进行二值化处理，通过广义矩估计（GMM）和节点回归推导出适当的检验统计量，理论和实证结果均证明了其优越性和有效性。


<details>
  <summary>Details</summary>
Motivation: 在许多实际场景中，由于测量限制，感兴趣的变量通常以离散值表示。直接将条件独立性（CI）检验应用于这种离散化数据可能导致错误结论。现有的通过二值化观测数据来推断潜在变量的CI关系的方法会导致信息丢失，从而降低检验性能。因此，需要一种不依赖于二值化过程的CI检验方法。

Method: 论文提出了一种样本高效的CI检验方法，通过使用广义矩估计（GMM）解决过度识别约束问题，建立潜在连续变量的独立性关系。基于此见解，利用节点回归推导出适当的检验统计量，并建立了其渐近分布，正确反映条件独立性。

Result: 理论分析和多个数据集上的实证结果表明，所提出的CI检验方法具有优越性和有效性。

Conclusion: 论文提出了一种新的CI检验方法，该方法无需二值化过程，能够更有效地推断潜在连续变量的独立性关系，且理论和实证研究均验证了其优越性和有效性。

Abstract: In many real-world scenarios, interested variables are often represented as
discretized values due to measurement limitations. Applying Conditional
Independence (CI) tests directly to such discretized data, however, can lead to
incorrect conclusions. To address this, recent advancements have sought to
infer the correct CI relationship between the latent variables through
binarizing observed data. However, this process inevitably results in a loss of
information, which degrades the test's performance. Motivated by this, this
paper introduces a sample-efficient CI test that does not rely on the
binarization process. We find that the independence relationships of latent
continuous variables can be established by addressing an over-identifying
restriction problem with Generalized Method of Moments (GMM). Based on this
insight, we derive an appropriate test statistic and establish its asymptotic
distribution correctly reflecting CI by leveraging nodewise regression.
Theoretical findings and Empirical results across various datasets demonstrate
that the superiority and effectiveness of our proposed test. Our code
implementation is provided in https://github.com/boyangaaaaa/DCT

</details>


### [25] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: The paper proposes a new method integrating Knowledge Graphs (KGs) with Large Language Models (LLMs) for knowledge-based causal discovery. This approach uses metapath-based subgraphs refined by Learning-to-Rank models, incorporated into zero-shot prompts to enhance LLMs' causal inference ability. Experiments show significant improvement over baselines.


<details>
  <summary>Details</summary>
Motivation: Existing methods using LLMs for causal discovery often produce unstable and inconsistent results, which affects their reliability.

Method: The method integrates KGs with LLMs by identifying informative metapath-based subgraphs within KGs, refining the selection of these subgraphs using Learning-to-Rank-based models, and incorporating the top-ranked subgraphs into zero-shot prompts.

Result: Extensive experiments on biomedical and open-domain datasets demonstrate that the proposed method outperforms most baselines by up to 44.4 points in F1 scores, evaluated across diverse LLMs and KGs.

Conclusion: This novel approach significantly enhances the effectiveness of LLMs in inferring causal relationships and provides a reliable alternative for knowledge-based causal discovery.

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [26] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: The paper surveys the evaluation of LLM assistants and agents for data science, finding a focus on goal-oriented activities, pure assistance or fully autonomous agents, and human substitution while neglecting other important aspects.


<details>
  <summary>Details</summary>
Motivation: To assess how LLM assistants and agents are currently evaluated in data science and identify gaps or imbalances in their application and evaluation.

Method: Surveying the evaluations of LLM assistants and agents in data science to analyze their focus areas, levels of human-AI collaboration, and automation possibilities.

Result: (1) Most evaluations concentrate on goal-oriented activities, with less attention to data management and exploratory tasks; (2) There is an emphasis on either pure assistance or fully autonomous agents, ignoring intermediate levels of human-AI collaboration; (3) Evaluations primarily focus on human substitution rather than exploring higher levels of automation through task transformation.

Conclusion: There is a need to broaden the scope of evaluating LLM assistants and agents in data science to include data management, exploratory activities, intermediate levels of human-AI collaboration, and the potential for task transformation leading to higher levels of automation.

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


### [27] [Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task](https://arxiv.org/abs/2506.08872)
*Nataliya Kosmyna,Eugene Hauptmann,Ye Tong Yuan,Jessica Situ,Xian-Hao Liao,Ashly Vivian Beresnitzky,Iris Braunstein,Pattie Maes*

Main category: cs.AI

TL;DR: This study explores the neural and behavioral consequences of LLM-assisted essay writing, finding that while LLMs offer convenience, there are potential cognitive costs and long-term educational implications.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of LLMs on cognitive load, linguistic patterns, and self-reported ownership in essay writing.

Method: Participants were divided into three groups and completed essay writing sessions under different conditions. EEG was used to assess cognitive load, NLP was used to analyze essays, and essays were scored by human teachers and an AI judge.

Result: LLM users showed weaker brain connectivity and underperformed at neural, linguistic, and behavioral levels. Self-reported ownership was lowest in the LLM group. Cognitive activity scaled down with external tool use.

Conclusion: The findings highlight potential cognitive costs of LLM use and raise concerns about the long-term educational implications of relying on LLMs.

Abstract: This study explores the neural and behavioral consequences of LLM-assisted
essay writing. Participants were divided into three groups: LLM, Search Engine,
and Brain-only (no tools). Each completed three sessions under the same
condition. In a fourth session, LLM users were reassigned to Brain-only group
(LLM-to-Brain), and Brain-only users were reassigned to LLM condition
(Brain-to-LLM). A total of 54 participants took part in Sessions 1-3, with 18
completing session 4. We used electroencephalography (EEG) to assess cognitive
load during essay writing, and analyzed essays using NLP, as well as scoring
essays with the help from human teachers and an AI judge. Across groups, NERs,
n-gram patterns, and topic ontology showed within-group homogeneity. EEG
revealed significant differences in brain connectivity: Brain-only participants
exhibited the strongest, most distributed networks; Search Engine users showed
moderate engagement; and LLM users displayed the weakest connectivity.
Cognitive activity scaled down in relation to external tool use. In session 4,
LLM-to-Brain participants showed reduced alpha and beta connectivity,
indicating under-engagement. Brain-to-LLM users exhibited higher memory recall
and activation of occipito-parietal and prefrontal areas, similar to Search
Engine users. Self-reported ownership of essays was the lowest in the LLM group
and the highest in the Brain-only group. LLM users also struggled to accurately
quote their own work. While LLMs offer immediate convenience, our findings
highlight potential cognitive costs. Over four months, LLM users consistently
underperformed at neural, linguistic, and behavioral levels. These results
raise concerns about the long-term educational implications of LLM reliance and
underscore the need for deeper inquiry into AI's role in learning.

</details>


### [28] [Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation](https://arxiv.org/abs/2506.08898)
*Mingfeng Fan,Jianan Zhou,Yifeng Zhang,Yaoxin Wu,Jinbiao Chen,Guillaume Adrien Sartoretti*

Main category: cs.AI

TL;DR: Recent deep reinforcement learning methods for multi-objective combinatorial optimization problems (MOCOPs) have limitations in exploration and performance. This paper proposes POCCO, a framework that enables adaptive model structure selection for subproblems and preference-driven optimization. Experimental results show its superiority and generalization.


<details>
  <summary>Details</summary>
Motivation: Current deep reinforcement learning methods for MOCOPs treat all subproblems equally and use a single model, which limits effective exploration of the solution space and leads to suboptimal performance.

Method: The paper proposes POCCO, a plug-and-play framework that allows adaptive selection of model structures for subproblems. It includes a conditional computation block that routes subproblems to specialized neural architectures and a preference-driven optimization algorithm that learns pairwise preferences between solutions.

Result: POCCO was applied to two state-of-the-art neural methods for MOCOPs and tested across four classic MOCOP benchmarks. The results demonstrate its significant superiority and strong generalization.

Conclusion: POCCO is an effective framework for solving MOCOPs by enabling adaptive model structure selection and preference-driven optimization, leading to superior performance and strong generalization.

Abstract: Recent deep reinforcement learning methods have achieved remarkable success
in solving multi-objective combinatorial optimization problems (MOCOPs) by
decomposing them into multiple subproblems, each associated with a specific
weight vector. However, these methods typically treat all subproblems equally
and solve them using a single model, hindering the effective exploration of the
solution space and thus leading to suboptimal performance. To overcome the
limitation, we propose POCCO, a novel plug-and-play framework that enables
adaptive selection of model structures for subproblems, which are subsequently
optimized based on preference signals rather than explicit reward values.
Specifically, we design a conditional computation block that routes subproblems
to specialized neural architectures. Moreover, we propose a preference-driven
optimization algorithm that learns pairwise preferences between winning and
losing solutions. We evaluate the efficacy and versatility of POCCO by applying
it to two state-of-the-art neural methods for MOCOPs. Experimental results
across four classic MOCOP benchmarks demonstrate its significant superiority
and strong generalization.

</details>


### [29] [IntTrajSim: Trajectory Prediction for Simulating Multi-Vehicle driving at Signalized Intersections](https://arxiv.org/abs/2506.08957)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: The study proposes a data-driven simulator and multi-headed self-attention-based trajectory prediction model for mimicking driving behavior at traffic intersections, incorporating signal information to improve performance on traffic engineering-related metrics.


<details>
  <summary>Details</summary>
Motivation: Traffic simulators with rule-based approach have limitations in mimicking real-world driving behavior, especially at intersections which are crucial for safety and road efficiency.

Method: Proposing traffic engineering-related metrics to evaluate generative trajectory prediction models and providing a simulation-in-the-loop pipeline. Developing a multi-headed self-attention-based trajectory prediction model that includes signal information.

Result: The new model outperforms previous models on the proposed evaluation metrics.

Conclusion: A data-driven simulator and improved trajectory prediction model can better mimic driving behavior at intersections.

Abstract: Traffic simulators are widely used to study the operational efficiency of
road infrastructure, but their rule-based approach limits their ability to
mimic real-world driving behavior. Traffic intersections are critical
components of the road infrastructure, both in terms of safety risk (nearly 28%
of fatal crashes and 58% of nonfatal crashes happen at intersections) as well
as the operational efficiency of a road corridor. This raises an important
question: can we create a data-driven simulator that can mimic the macro- and
micro-statistics of the driving behavior at a traffic intersection? Deep
Generative Modeling-based trajectory prediction models provide a good starting
point to model the complex dynamics of vehicles at an intersection. But they
are not tested in a "live" micro-simulation scenario and are not evaluated on
traffic engineering-related metrics. In this study, we propose traffic
engineering-related metrics to evaluate generative trajectory prediction models
and provide a simulation-in-the-loop pipeline to do so. We also provide a
multi-headed self-attention-based trajectory prediction model that incorporates
the signal information, which outperforms our previous models on the evaluation
metrics.

</details>


### [30] [Evaluating Generative Vehicle Trajectory Models for Traffic Intersection Dynamics](https://arxiv.org/abs/2506.08963)
*Yash Ranjan,Rahul Sengupta,Anand Rangarajan,Sanjay Ranka*

Main category: cs.AI

TL;DR: 尽管使用理想轨迹作为输入并获得较低的轨迹重建误差，生成的轨迹仍表现出违反交通规则的行为。我们引入新指标来评估这些不良行为并展示结果。


<details>
  <summary>Details</summary>
Motivation: 交叉路口是城市路网的关键部分，但容易发生事故。当前对信号交叉口的交通动态生成模型主要依赖于计算轨迹重建误差的指标，未充分考虑如闯红灯等交通工程特定问题。

Method: 我们提供了一个全面的分析工具，用于训练、运行和评估模型，这些模型使用更符合交通工程观点的指标。我们在大型数据集上训练最先进的多车辆轨迹预测模型，并在微仿真器中评估其在未知交通条件下的性能。

Result: 结果显示，即使输入理想轨迹且重建误差低，生成的轨迹仍存在违反交通规则的行为。我们引入了新的评估指标来衡量这些不良行为。

Conclusion: 需要更好的指标来评估交通模型性能，特别是在交通工程方面的问题。

Abstract: Traffic Intersections are vital to urban road networks as they regulate the
movement of people and goods. However, they are regions of conflicting
trajectories and are prone to accidents. Deep Generative models of traffic
dynamics at signalized intersections can greatly help traffic authorities
better understand the efficiency and safety aspects. At present, models are
evaluated on computational metrics that primarily look at trajectory
reconstruction errors. They are not evaluated online in a `live'
microsimulation scenario. Further, these metrics do not adequately consider
traffic engineering-specific concerns such as red-light violations, unallowed
stoppage, etc. In this work, we provide a comprehensive analytics tool to
train, run, and evaluate models with metrics that give better insights into
model performance from a traffic engineering point of view. We train a
state-of-the-art multi-vehicle trajectory forecasting model on a large dataset
collected by running a calibrated scenario of a real-world urban intersection.
We then evaluate the performance of the prediction models, online in a
microsimulator, under unseen traffic conditions. We show that despite using
ideally-behaved trajectories as input, and achieving low trajectory
reconstruction errors, the generated trajectories show behaviors that break
traffic rules. We introduce new metrics to evaluate such undesired behaviors
and present our results.

</details>


### [31] [A Survey of Link Prediction in N-ary Knowledge Graphs](https://arxiv.org/abs/2506.08970)
*Jiyao Wei,Saiping Guan,Da Li,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.AI

TL;DR: N-ary Knowledge Graphs (NKGs) are designed for complex facts representation. Link prediction in NKGs predicts missing elements within these facts. This paper surveys link prediction methods in NKGs, categorizes them, and suggests future research directions.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the link prediction task in N-ary Knowledge Graphs and guide future research.

Method: Systematic categorization and analysis of existing link prediction methods in NKGs.

Result: Overview of the field, classification of methods, performance and application analysis, and identification of promising research directions.

Conclusion: The first comprehensive survey on link prediction in NKGs has been presented, offering insights into current methods and suggesting future research opportunities.

Abstract: N-ary Knowledge Graphs (NKGs) are a specialized type of knowledge graph
designed to efficiently represent complex real-world facts. Unlike traditional
knowledge graphs, where a fact typically involves two entities, NKGs can
capture n-ary facts containing more than two entities. Link prediction in NKGs
aims to predict missing elements within these n-ary facts, which is essential
for completing NKGs and improving the performance of downstream applications.
This task has recently gained significant attention. In this paper, we present
the first comprehensive survey of link prediction in NKGs, providing an
overview of the field, systematically categorizing existing methods, and
analyzing their performance and application scenarios. We also outline
promising directions for future research.

</details>


### [32] [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/abs/2506.09038)
*Polina Kirichenko,Mark Ibrahim,Kamalika Chaudhuri,Samuel J. Bell*

Main category: cs.AI

TL;DR: 大型语言模型（LLMs）需要知道何时不给出答案，这与正确回答问题同样重要。然而，拒绝回答的能力尚未得到充分研究，缺乏针对现代LLM的系统评估框架。本文介绍了AbstentionBench，这是一个大规模基准测试，用于全面评估20个不同数据集上的拒绝回答能力，涵盖未知答案、未明确指定、错误前提等问题。对20个前沿LLM的评估表明，增大模型规模对提升拒绝回答能力帮助不大，且最近的推理微调反而降低了该能力（平均降低24%）。虽然精心设计的系统提示可以实际提高拒绝回答能力，但并不能解决模型在不确定性推理方面的根本缺陷。我们发布了AbstentionBench以推动LLM可靠性的研究。


<details>
  <summary>Details</summary>
Motivation: 目前对于大型语言模型（LLMs）何时不应提供答案这一方面缺乏深入研究和系统性评估框架。现实世界中的用户查询可能未明确指定、表述不当或根本无法回答，因此需要LLMs具备不确定性推理能力和选择性拒绝回答的能力。

Method: 作者提出了一个名为AbstentionBench的大规模基准测试，涵盖了20个不同的数据集，包括未知答案的问题、未明确指定的问题、错误前提的问题等。通过这个基准测试，作者评估了20个前沿LLM的拒绝回答能力，并分析了模型规模、推理微调和系统提示等因素对拒绝回答能力的影响。

Result: 评估结果显示，增大模型规模对提升拒绝回答能力帮助不大，且最近的推理微调反而降低了该能力（平均降低24%）。虽然精心设计的系统提示可以实际提高拒绝回答能力，但并不能解决模型在不确定性推理方面的根本缺陷。

Conclusion: 作者认为拒绝回答是一个尚未解决的问题，并发布了AbstentionBench以推动LLM可靠性的研究。

Abstract: For Large Language Models (LLMs) to be reliably deployed in both everyday and
high-stakes domains, knowing when not to answer is equally critical as
answering correctly. Real-world user queries, which can be underspecified,
ill-posed, or fundamentally unanswerable, require LLMs to reason about
uncertainty and selectively abstain -- i.e., refuse to answer definitively.
However, abstention remains understudied, without a systematic evaluation
framework for modern LLMs. In this work, we introduce AbstentionBench, a
large-scale benchmark for holistically evaluating abstention across 20 diverse
datasets, including questions with unknown answers, underspecification, false
premises, subjective interpretations, and outdated information. Evaluating 20
frontier LLMs reveals abstention is an unsolved problem, and one where scaling
models is of little use. While recent reasoning LLMs have shown impressive
results in complex problem solving, surprisingly, we find that reasoning
fine-tuning degrades abstention (by $24\%$ on average), even for math and
science domains on which reasoning models are explicitly trained. We find that
while a carefully crafted system prompt can boost abstention in practice, it
does not resolve models' fundamental inability to reason about uncertainty. We
release AbstentionBench to foster research into advancing LLM reliability.

</details>


### [33] [VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning](https://arxiv.org/abs/2506.09049)
*Li Kang,Xiufeng Song,Heng Zhou,Yiran Qin,Jie Yang,Xiaohong Liu,Philip Torr,Lei Bai,Zhenfei Yin*

Main category: cs.AI

TL;DR: 该研究提出了VIKI-Bench，一个专为多智能体协作设计的分层基准测试平台，并提出了VIKI-R框架，通过微调预训练视觉-语言模型和强化学习，显著提升了多任务级别的表现。


<details>
  <summary>Details</summary>
Motivation: 目前在动态环境中协调多个具身智能体仍是一个核心挑战，现有的基于视觉-语言模型的方法在支持多样化的具身类型方面存在局限性。

Method: 引入了VIKI-Bench，一个包含三个结构化层次（智能体激活、任务规划和轨迹感知）的分层基准测试平台，以及VIKI-R，一个两阶段框架，利用思维链标注演示微调预训练视觉-语言模型，然后进行多层次奖励信号下的强化学习。

Result: 实验表明，VIKI-R在所有任务级别上均显著优于基线方法，并且强化学习能够促使异构智能体之间出现组合协作模式。

Conclusion: VIKI-Bench和VIKI-R共同提供了一个统一的测试平台和方法，以推动具身AI系统中多智能体、视觉驱动协作的发展。

Abstract: Coordinating multiple embodied agents in dynamic environments remains a core
challenge in artificial intelligence, requiring both perception-driven
reasoning and scalable cooperation strategies. While recent works have
leveraged large language models (LLMs) for multi-agent planning, a few have
begun to explore vision-language models (VLMs) for visual reasoning. However,
these VLM-based approaches remain limited in their support for diverse
embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical
benchmark tailored for embodied multi-agent cooperation, featuring three
structured levels: agent activation, task planning, and trajectory perception.
VIKI-Bench includes diverse robot embodiments, multi-view visual observations,
and structured supervision signals to evaluate reasoning grounded in visual
inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a
two-stage framework that fine-tunes a pretrained vision-language model (VLM)
using Chain-of-Thought annotated demonstrations, followed by reinforcement
learning under multi-level reward signals. Our extensive experiments show that
VIKI-R significantly outperforms baselines method across all task levels.
Furthermore, we show that reinforcement learning enables the emergence of
compositional cooperation patterns among heterogeneous agents. Together,
VIKI-Bench and VIKI-R offer a unified testbed and method for advancing
multi-agent, visual-driven cooperation in embodied AI systems.

</details>


### [34] [ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm Engineering](https://arxiv.org/abs/2506.09050)
*Yuki Imajuku,Kohki Horie,Yoichi Iwata,Kensho Aoki,Naohiro Takahashi,Takuya Akiba*

Main category: cs.AI

TL;DR: The paper introduces ALE-Bench, a benchmark for evaluating AI systems in solving hard optimization problems. Unlike traditional benchmarks, ALE-Bench encourages iterative solution refinement over long time horizons and supports interactive agent architectures. The evaluation of frontier LLMs showed high performance on specific problems but inconsistency across problems and lacking long-horizon problem-solving capabilities compared to humans.


<details>
  <summary>Details</summary>
Motivation: To evaluate how well AI systems perform in algorithm engineering for hard optimization problems in various domains, and to address the limitations of existing short-duration, pass/fail coding benchmarks.

Method: Introduced ALE-Bench, a new benchmark drawing on real tasks from AtCoder Heuristic Contests, presenting computationally hard optimization problems without known exact solutions. The software framework supports interactive agent architectures leveraging test-run feedback and visualizations.

Result: Frontier LLMs demonstrated high performance on specific problems but showed inconsistency across problems and lacked long-horizon problem-solving capabilities compared to humans.

Conclusion: There is a notable gap between AI systems and humans in terms of consistency and long-horizon problem-solving capabilities, highlighting the need for ALE-Bench to foster future AI advancements.

Abstract: How well do AI systems perform in algorithm engineering for hard optimization
problems in domains such as package-delivery routing, crew scheduling, factory
production planning, and power-grid balancing? We introduce ALE-Bench, a new
benchmark for evaluating AI systems on score-based algorithmic programming
contests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench
presents optimization problems that are computationally hard and admit no known
exact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench
encourages iterative solution refinement over long time horizons. Our software
framework supports interactive agent architectures that leverage test-run
feedback and visualizations. Our evaluation of frontier LLMs revealed that
while they demonstrate high performance on specific problems, a notable gap
remains compared to humans in terms of consistency across problems and
long-horizon problem-solving capabilities. This highlights the need for this
benchmark to foster future AI advancements.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [35] [KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache](https://arxiv.org/abs/2506.08018)
*Fei Li,Song Liu,Weiguo Wu,Shiqiang Nie,Jinyu Wang*

Main category: cs.LG

TL;DR: In this paper, KVmix is proposed to address the memory demands of the Key-Value (KV) Cache during LLM inference via a novel mixed-precision quantization method. It achieves near-lossless inference performance with significant memory compression and speedup.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to alleviate the high memory demands of the Key-Value (KV) Cache during the inference of Large Language Models (LLMs), particularly in resource-constrained platforms. Existing methods either use static precision allocation or fail to dynamically prioritize critical KV in long-context tasks, leading to suboptimal tradeoffs between memory, accuracy, and throughput.

Method: The proposed method, KVmix, is a novel mixed-precision quantization approach for KV Cache. It uses gradient-based importance analysis to evaluate the impact of individual Key and Value projection matrices on model loss, allowing layer-specific bit-width allocation. Additionally, KVmix introduces a dynamic long-context optimization strategy that keeps full-precision KV pairs for recent pivotal tokens and compresses older ones. Efficient low-bit quantization and CUDA kernels are also provided to reduce computational overhead.

Result: On LLMs such as Llama and Mistral, KVmix achieves near-lossless inference performance with an extremely low quantization configuration (Key 2.19bit, Value 2.38bit). It delivers a remarkable 4.9x memory compression and a 5.3x speedup in inference throughput.

Conclusion: KVmix successfully addresses the memory challenges associated with KV Cache during LLM inference by providing a tunable balance between accuracy and efficiency. Its dynamic prioritization and efficient quantization techniques make it suitable for deployment in resource-constrained environments.

Abstract: The high memory demands of the Key-Value (KV) Cache during the inference of
Large Language Models (LLMs) severely restrict their deployment in
resource-constrained platforms. Quantization can effectively alleviate the
memory pressure caused by KV Cache. However, existing methods either rely on
static one-size-fits-all precision allocation or fail to dynamically prioritize
critical KV in long-context tasks, forcing memory-accuracy-throughput
tradeoffs. In this work, we propose a novel mixed-precision quantization method
for KV Cache named KVmix. KVmix leverages gradient-based importance analysis to
evaluate how individual Key and Value projection matrices affect the model
loss, enabling layer-specific bit-width allocation for mix-precision
quantization. It dynamically prioritizes higher precision for important layers
while aggressively quantizing less influential ones, achieving a tunable
balance between accuracy and efficiency. KVmix also introduces a dynamic
long-context optimization strategy that adaptively keeps full-precision KV
pairs for recent pivotal tokens and compresses older ones, achieving
high-quality sequence generation with low memory usage. Additionally, KVmix
provides efficient low-bit quantization and CUDA kernels to optimize
computational overhead. On LLMs such as Llama and Mistral, KVmix achieves
near-lossless inference performance with extremely low quantization
configuration (Key 2.19bit Value 2.38bit), while delivering a remarkable 4.9x
memory compression and a 5.3x speedup in inference throughput.

</details>


### [36] [Gridding Forced Displacement using Semi-Supervised Learning](https://arxiv.org/abs/2506.08019)
*Andrew Wells,Geraldine Henningsen,Brice Bolane Tchinde Kengne*

Main category: cs.LG

TL;DR: This paper proposes a semi-supervised method to disaggregate refugee statistics from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan African countries, achieving 92.9% average accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to provide high granularity and spatially explicit refugee statistics by disaggregating the data from administrative boundaries, which can help identify localized displacement patterns previously obscured in broader regional and national statistics.

Method: The method involves integrating UNHCR's ProGres registration data with satellite-derived building footprints from Google Open Buildings and location coordinates from OpenStreetMap Populated Places. A label spreading algorithm is then used to create spatially explicit refugee statistics at high granularity.

Result: This methodology achieves 92.9% average accuracy in placing over 10 million refugee observations into appropriate grid cells.

Conclusion: The resulting high-resolution dataset provides a foundation for a deeper understanding of displacement drivers.

Abstract: We present a semi-supervised approach that disaggregates refugee statistics
from administrative boundaries to 0.5-degree grid cells across 25 Sub-Saharan
African countries. By integrating UNHCR's ProGres registration data with
satellite-derived building footprints from Google Open Buildings and location
coordinates from OpenStreetMap Populated Places, our label spreading algorithm
creates spatially explicit refugee statistics at high granularity.This
methodology achieves 92.9% average accuracy in placing over 10 million refugee
observations into appropriate grid cells, enabling the identification of
localized displacement patterns previously obscured in broader regional and
national statistics. The resulting high-resolution dataset provides a
foundation for a deeper understanding of displacement drivers.

</details>


### [37] [Bi-level Unbalanced Optimal Transport for Partial Domain Adaptation](https://arxiv.org/abs/2506.08020)
*Zi-Ying Chen,Chuan-Xian Ren,Hong Yan*

Main category: cs.LG

TL;DR: This paper proposes a Bi-level Unbalanced Optimal Transport (BUOT) model for Partial Domain Adaptation (PDA), which simultaneously characterizes sample-wise and class-wise relations in a unified transport framework.


<details>
  <summary>Details</summary>
Motivation: The current weighting frameworks used in PDA have limitations in modeling the sample-wise relations, insufficient exploration of cluster structures, and sensitivity to inaccurate prediction causing confusion on outlier classes.

Method: The BUOT model introduces a cooperation mechanism between sample-level and class-level transport. Sample-level transport provides structure information for class-level knowledge transfer while class-level transport supplies discriminative information for outlier identification. The model incorporates label-aware transport cost ensuring local transport structure and deriving fast computation formulation.

Result: Extensive experiments on benchmark datasets validate the competitiveness of BUOT.

Conclusion: The proposed BUOT model addresses the limitations of current methods by simultaneously characterizing sample-wise and class-wise relations, providing guidance for the alignment process, and improving efficiency.

Abstract: Partial domain adaptation (PDA) problem requires aligning cross-domain
samples while distinguishing the outlier classes for accurate knowledge
transfer. The widely used weighting framework tries to address the outlier
classes by introducing the reweighed source domain with a similar label
distribution to the target domain. However, the empirical modeling of weights
can only characterize the sample-wise relations, which leads to insufficient
exploration of cluster structures, and the weights could be sensitive to the
inaccurate prediction and cause confusion on the outlier classes. To tackle
these issues, we propose a Bi-level Unbalanced Optimal Transport (BUOT) model
to simultaneously characterize the sample-wise and class-wise relations in a
unified transport framework. Specifically, a cooperation mechanism between
sample-level and class-level transport is introduced, where the sample-level
transport provides essential structure information for the class-level
knowledge transfer, while the class-level transport supplies discriminative
information for the outlier identification. The bi-level transport plan
provides guidance for the alignment process. By incorporating the label-aware
transport cost, the local transport structure is ensured and a fast computation
formulation is derived to improve the efficiency. Extensive experiments on
benchmark datasets validate the competitiveness of BUOT.

</details>


### [38] [FlowBERT: Prompt-tuned BERT for variable flow field prediction](https://arxiv.org/abs/2506.08021)
*Weihao Zou,Weibing Feng,Pin Wu*

Main category: cs.LG

TL;DR: This study proposes a flow field prediction framework based on knowledge transfer from large language models (LLMs), integrating Proper Orthogonal Decomposition (POD) and fine-tuning strategies for pretrained LLMs. Fluid dynamics-oriented text templates are used to improve performance. The framework outperforms conventional models in few-shot learning scenarios, generalizes well across various inflow conditions and geometries, and reduces prediction time significantly while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high computational costs of traditional CFD methods and the limited cross-condition transfer capability of existing deep learning models.

Method: The framework integrates Proper Orthogonal Decomposition (POD) dimensionality reduction with fine-tuning strategies for pretrained LLMs. Fluid dynamics-oriented text templates are specifically designed to enrich contextual semantic information.

Result: The framework outperforms conventional Transformer models in few-shot learning scenarios, exhibits exceptional generalization across various inflow conditions and airfoil geometries, and reduces prediction time to seconds while maintaining over 90% accuracy.

Conclusion: The developed knowledge transfer paradigm establishes a new direction for rapid fluid dynamics prediction with potential applications in aerodynamic optimization, flow control, and other engineering domains.

Abstract: This study proposes a universal flow field prediction framework based on
knowledge transfer
  from large language model (LLM), addressing the high computational costs of
traditional
  computational fluid dynamics (CFD) methods and the limited cross-condition
transfer capability
  of existing deep learning models. The framework innovatively integrates
Proper Orthogonal
  Decomposition (POD) dimensionality reduction with fine-tuning strategies for
pretrained LLM,
  where POD facilitates compressed representation of flow field features while
the fine-tuned model
  learns to encode system dynamics in state space. To enhance the model's
adaptability to flow field
  data, we specifically designed fluid dynamics-oriented text templates that
improve predictive
  performance through enriched contextual semantic information. Experimental
results demonstrate
  that our framework outperforms conventional Transformer models in few-shot
learning scenarios while
  exhibiting exceptional generalization across various inflow conditions and
airfoil geometries.
  Ablation studies reveal the contributions of key components in the FlowBERT
architecture. Compared
  to traditional Navier-Stokes equation solvers requiring hours of computation,
our approach reduces
  prediction time to seconds while maintaining over 90% accuracy. The developed
knowledge transfer
  paradigm establishes a new direction for rapid fluid dynamics prediction,
with potential
  applications extending to aerodynamic optimization, flow control, and other
engineering domains.

</details>


### [39] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: The paper proposes Modality-Balancing Preference Optimization (MBPO), a new framework to address modality imbalance in Large Multimodal Models (LMMs). MBPO constructs an improved offline preference dataset with hard negatives and uses online responses with verified rewards, employing GRPO for training. Experiments show MBPO enhances LMM performance and reduces hallucinations.


<details>
  <summary>Details</summary>
Motivation: Current methods for aligning LMMs do not sufficiently address the issue of modality imbalance, where language biases outweigh visual inputs. Additionally, existing approaches rely on offline data and lack adaptability to dynamic changes during training.

Method: MBPO generates hard negatives by adversarially perturbing input images to create rejected responses due to LLM biases. It also uses close-ended tasks to generate online responses with verified rewards, combining these with the offline data for training via GRPO.

Result: Experiments indicate that MBPO improves LMM performance on complex vision-language tasks and effectively decreases hallucinations.

Conclusion: MBPO is an effective solution to modality imbalance in LMMs, enhancing their generalization capabilities and reducing inaccuracies.

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [40] [Recipes for Pre-training LLMs with MXFP8](https://arxiv.org/abs/2506.08027)
*Asit Mishra,Dusan Stosic,Simon Layton*

Main category: cs.LG

TL;DR: MX格式在NVIDIA最新的Blackwell GPU中对于精度缩放至关重要，但需谨慎使用以确保LLM在大规模数据集上的收敛。本文提出了一种改进的舍入模式，使用round-to-infinity计算缩放因子，成功实现了8B模型在15T标记上的MXFP8预训练。


<details>
  <summary>Details</summary>
Motivation: 尽管MX格式相较于其他低精度表示法提供了更好的数值稳定性，但在实际应用中需要谨慎使用，以确保LLM在多万亿标记数据集上能够成功收敛。

Method: 研究了OCP规范中建议的舍入模式可能导致LLM预训练发散的问题，并提出了一种改进的舍入模式——使用round-to-infinity计算缩放因子。

Result: 改进的舍入模式使得8B模型能够在15T标记的数据集上成功进行MXFP8预训练。

Conclusion: 改进的舍入模式解决了OCP规范中原始舍入模式可能引发的发散问题，为在MXFP8中成功预训练大型模型提供了支持。

Abstract: Precision scaling - using fewer bits to represent model parameters and
related tensors during pre-training - has emerged as a compelling technique for
improving GPU efficiency without sacrificing accuracy. Microscaling (MX)
formats in NVIDIA's latest Blackwell GPUs represent a major leap in enabling
this precision scaling aspect. These formats combine narrow floating-point data
types with per-block scaling factors, offering a fine-grained approach to
quantizing tensors.
  Although MX-formats offer the promise of improved numeric stability compared
to other reduced-precision representations, in practice they must be used
carefully in order to successfully converge an LLM on a multi-trillion token
dataset. In this paper, we show that the rounding mode suggested in OCP
specification can lead to divergence when pre-training an LLM. We show an
improved rounding mode, which uses round-to-infinity to compute scaling
factors, enables successful pre-training in MXFP8 for an 8B model on 15T
tokens.

</details>


### [41] [ST-GraphNet: A Spatio-Temporal Graph Neural Network for Understanding and Predicting Automated Vehicle Crash Severity](https://arxiv.org/abs/2506.08051)
*Mahmuda Sultana Mimi,Md Monzurul Islam,Anannya Ghosh Tusti,Shriyank Somvanshi,Subasish Das*

Main category: cs.LG

TL;DR: The paper presents ST-GraphNet, a spatio-temporal graph neural network framework that models and predicts automated vehicle (AV) crash severity using both fine-grained and region-aggregated spatial graphs. It achieves 97.74% test accuracy.


<details>
  <summary>Details</summary>
Motivation: To advance urban mobility safety and infrastructure planning by understanding the spatial and temporal dynamics of AV crash severity.

Method: Introduced ST-GraphNet which uses DSTGCN backbone on coarse-grained H3 graph with multimodal data including semantic, spatial, and temporal attributes.

Result: Achieved 97.74% test accuracy, outperforming the best fine-grained model with 64.7% test accuracy.

Conclusion: Spatial aggregation, dynamic message passing, and multi-modal feature integration are effective in capturing complex spatio-temporal patterns underlying AV crash severity.

Abstract: Understanding the spatial and temporal dynamics of automated vehicle (AV)
crash severity is critical for advancing urban mobility safety and
infrastructure planning. In this work, we introduce ST-GraphNet, a
spatio-temporal graph neural network framework designed to model and predict AV
crash severity by using both fine-grained and region-aggregated spatial graphs.
Using a balanced dataset of 2,352 real-world AV-related crash reports from
Texas (2024), including geospatial coordinates, crash timestamps, SAE
automation levels, and narrative descriptions, we construct two complementary
graph representations: (1) a fine-grained graph with individual crash events as
nodes, where edges are defined via spatio-temporal proximity; and (2) a
coarse-grained graph where crashes are aggregated into Hexagonal Hierarchical
Spatial Indexing (H3)-based spatial cells, connected through hexagonal
adjacency. Each node in the graph is enriched with multimodal data, including
semantic, spatial, and temporal attributes, including textual embeddings from
crash narratives using a pretrained Sentence-BERT model. We evaluate various
graph neural network (GNN) architectures, such as Graph Convolutional Networks
(GCN), Graph Attention Networks (GAT), and Dynamic Spatio-Temporal GCN
(DSTGCN), to classify crash severity and predict high-risk regions. Our
proposed ST-GraphNet, which utilizes a DSTGCN backbone on the coarse-grained H3
graph, achieves a test accuracy of 97.74\%, substantially outperforming the
best fine-grained model (64.7\% test accuracy). These findings highlight the
effectiveness of spatial aggregation, dynamic message passing, and multi-modal
feature integration in capturing the complex spatio-temporal patterns
underlying AV crash severity.

</details>


### [42] [STAMImputer: Spatio-Temporal Attention MoE for Traffic Data Imputation](https://arxiv.org/abs/2506.08054)
*Yiming Wang,Hao Peng,Senzhang Wang,Haohua Du,Chunyang Liu,Jia Wu,Guanlin Wu*

Main category: cs.LG

TL;DR: The paper proposes STAMImputer, a SpatioTemporal Attention Mixture of experts network for traffic data imputation that significantly outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing time-to-space sequential methods fail to effectively extract features in block-wise missing data scenarios and static graph structure constrains the models flexibility in handling nonstationary traffic data.

Method: STAMImputer uses a Mixture of Experts (MoE) framework to capture latent spatio-temporal features and their influence weights, and introduces a Low-rank guided Sampling Graph ATtention (LrSGAT) mechanism to dynamically balance local and global correlations across road networks.

Result: STAMImputer achieves significantly better performance compared with existing state-of-the-art approaches when evaluated on four traffic datasets.

Conclusion: STAMImputer is an effective method for traffic data imputation that addresses the limitations of existing methods.

Abstract: Traffic data imputation is fundamentally important to support various
applications in intelligent transportation systems such as traffic flow
prediction. However, existing time-to-space sequential methods often fail to
effectively extract features in block-wise missing data scenarios. Meanwhile,
the static graph structure for spatial feature propagation significantly
constrains the models flexibility in handling the distribution shift issue for
the nonstationary traffic data. To address these issues, this paper proposes a
SpatioTemporal Attention Mixture of experts network named STAMImputer for
traffic data imputation. Specifically, we introduce a Mixture of Experts (MoE)
framework to capture latent spatio-temporal features and their influence
weights, effectively imputing block missing. A novel Low-rank guided Sampling
Graph ATtention (LrSGAT) mechanism is designed to dynamically balance the local
and global correlations across road networks. The sampled attention vectors are
utilized to generate dynamic graphs that capture real-time spatial
correlations. Extensive experiments are conducted on four traffic datasets for
evaluation. The result shows STAMImputer achieves significantly performance
improvement compared with existing SOTA approaches. Our codes are available at
https://github.com/RingBDStack/STAMImupter.

</details>


### [43] [Eliciting Fine-Tuned Transformer Capabilities via Inference-Time Techniques](https://arxiv.org/abs/2506.08060)
*Asankhaya Sharma*

Main category: cs.LG

TL;DR: 大型语言模型的能力可以通过推理时的技术（如情境学习）来近似实现，而无需改变模型参数。本文提供了理论基础和资源高效部署的方法。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）对于大型语言模型来说计算成本高，因此研究如何通过其他方式获得类似的能力是必要的。

Method: 在理想假设下（无限计算资源和访问微调数据集），证明了通过情境学习可以近似获得SFT的能力，并扩展到实际场景中有限的上下文长度和部分数据集访问情况。

Result: 对于文本生成任务和线性分类任务，给出了所需数据集大小的理论结果，这些结果为资源高效的模型部署提供了理论支持。

Conclusion: 基于Transformer的图灵完备性，本文为大型语言模型的资源高效部署提供了理论基础，结合实际技术如检索增强生成，可将理论应用于现实世界。

Abstract: Large language models have transformed natural language processing, yet
supervised fine-tuning (SFT) remains computationally intensive. This paper
formally proves that capabilities acquired through SFT can be approximated by a
base transformer model using inference-time techniques, specifically in-context
learning (ICL), without altering model parameters, under idealized assumptions
including unbounded computational resources and access to the fine-tuning
dataset. We extend these results to practical scenarios with finite context
lengths and partial dataset access. For text generation tasks with fixed output
length $l$, datasets of size $\mathrm{O}\left( \frac{m V}{\varepsilon^2} \log
\frac{m}{\delta} \right)$ or, with bounded context, $\mathrm{O}\left( \frac{l
\log V}{\varepsilon^2} \log \frac{1}{\delta} \right)$ suffice to approximate
fine-tuned behavior across $m$ contexts within error $\varepsilon$, where $V$
is the vocabulary size and $\delta$ is the failure probability. For linear
classification, datasets of size $\mathrm{O}\left( \frac{d}{\varepsilon}
\right)$ or, with fixed context, $\mathrm{O}\left( \frac{1}{\varepsilon^2} \log
\frac{1}{\delta} \right)$ are sufficient, where $d$ is the input dimension.
Grounded in the Turing completeness of transformers, these results provide a
theoretical foundation for resource-efficient deployment of large language
models, with practical techniques like retrieval-augmented generation bridging
theory to real-world applications.

</details>


### [44] [FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2506.08062)
*Woosung Kim,Jinho Lee,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: FairDICE is the first offline MORL framework that directly optimizes nonlinear welfare objective, showing strong fairness-aware performance in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for optimizing policies in conflicting objectives often use linear scalarization which cannot capture fairness-oriented goals. While online algorithms have been proposed for specific fairness objectives, a unified approach for optimizing nonlinear welfare criteria in the offline setting remains unexplored.

Method: FairDICE leverages distribution correction estimation to jointly account for welfare maximization and distributional regularization, enabling stable and sample-efficient learning without requiring explicit preference weights or exhaustive weight search.

Result: Across multiple offline benchmarks, FairDICE demonstrates strong fairness-aware performance compared to existing baselines.

Conclusion: FairDICE presents a novel approach to optimize nonlinear welfare criteria in the offline setting, providing strong fairness-aware performance.

Abstract: Multi-objective reinforcement learning (MORL) aims to optimize policies in
the presence of conflicting objectives, where linear scalarization is commonly
used to reduce vector-valued returns into scalar signals. While effective for
certain preferences, this approach cannot capture fairness-oriented goals such
as Nash social welfare or max-min fairness, which require nonlinear and
non-additive trade-offs. Although several online algorithms have been proposed
for specific fairness objectives, a unified approach for optimizing nonlinear
welfare criteria in the offline setting-where learning must proceed from a
fixed dataset-remains unexplored. In this work, we present FairDICE, the first
offline MORL framework that directly optimizes nonlinear welfare objective.
FairDICE leverages distribution correction estimation to jointly account for
welfare maximization and distributional regularization, enabling stable and
sample-efficient learning without requiring explicit preference weights or
exhaustive weight search. Across multiple offline benchmarks, FairDICE
demonstrates strong fairness-aware performance compared to existing baselines.

</details>


### [45] [Lite-RVFL: A Lightweight Random Vector Functional-Link Neural Network for Learning Under Concept Drift](https://arxiv.org/abs/2506.08063)
*Songqiao Hu,Zeyi Liu,Xiao He*

Main category: cs.LG

TL;DR: The paper proposes Lite-RVFL, a network that adapts to concept drift without detection or retraining by emphasizing recent data through an exponential weighting function. Experiments show its efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing methods for handling concept drift in online learning require model retraining or drift detection, which are computationally expensive and not suitable for real-time applications.

Method: Lite-RVFL introduces an objective function that assigns exponentially increasing weights to new samples, allowing the model to adapt to concept drift without needing drift detection or retraining.

Result: Experimental results on a real-world safety assessment task demonstrate the efficiency and effectiveness of Lite-RVFL in adapting to drift, as well as its potential to capture temporal patterns.

Conclusion: Lite-RVFL is a lightweight, fast, and efficient solution for adapting to concept drift in online learning scenarios without requiring drift detection or retraining.

Abstract: The change in data distribution over time, also known as concept drift, poses
a significant challenge to the reliability of online learning methods. Existing
methods typically require model retraining or drift detection, both of which
demand high computational costs and are often unsuitable for real-time
applications. To address these limitations, a lightweight, fast and efficient
random vector functional-link network termed Lite-RVFL is proposed, capable of
adapting to concept drift without drift detection and retraining. Lite-RVFL
introduces a novel objective function that assigns weights exponentially
increasing to new samples, thereby emphasizing recent data and enabling timely
adaptation. Theoretical analysis confirms the feasibility of this objective
function for drift adaptation, and an efficient incremental update rule is
derived. Experimental results on a real-world safety assessment task validate
the efficiency, effectiveness in adapting to drift, and potential to capture
temporal patterns of Lite-RVFL. The source code is available at
https://github.com/songqiaohu/Lite-RVFL.

</details>


### [46] [Info-Coevolution: An Efficient Framework for Data Model Coevolution](https://arxiv.org/abs/2506.08070)
*Ziheng Qin,Hailun Xu,Wei Chee Yew,Qi Jia,Yang Luo,Kanchan Sarkar,Danhui Guan,Kai Wang,Yang You*

Main category: cs.LG

TL;DR: The paper introduces Info-Coevolution, a framework that allows models and data to coevolve efficiently by selectively annotating data without introducing bias. It reduces annotation and training costs by 32% on ImageNet-1K without performance loss.


<details>
  <summary>Details</summary>
Motivation: The continuous growth of real-world data challenges efficient dataset construction and training in machine learning. The question arises whether all new data needs annotation or learning, as conventional approaches retaining all data lead to inefficiencies. Active learning, while reducing redundancy, increases complexity and introduces bias.

Method: Info-Coevolution leverages task-specific and open-source models to enable online selective annotation of data, allowing models and datasets to coevolve efficiently without bias. It integrates online and web data to enhance datasets and can automatically determine the saving ratio without manual tuning.

Result: On ImageNet-1K, Info-Coevolution reduces annotation and training costs by 32% without any performance loss. With semi-supervised learning, it can further reduce the annotation ratio to 50%. Additionally, retrieval-based dataset enhancement using unlabeled open-source data is explored.

Conclusion: Info-Coevolution presents an effective solution for efficient dataset construction and training by enabling unbiased selective annotation and coevolution of models and data.

Abstract: Machine learning relies heavily on data, yet the continuous growth of
real-world data poses challenges for efficient dataset construction and
training. A fundamental yet unsolved question is: given our current model and
data, does a new data (sample/batch) need annotation/learning? Conventional
approaches retain all available data, leading to non-optimal data and training
efficiency. Active learning aims to reduce data redundancy by selecting a
subset of samples to annotate, while it increases pipeline complexity and
introduces bias. In this work, we propose Info-Coevolution, a novel framework
that efficiently enables models and data to coevolve through online selective
annotation with no bias. Leveraging task-specific models (and open-source
models), it selectively annotates and integrates online and web data to improve
datasets efficiently. For real-world datasets like ImageNet-1K,
Info-Coevolution reduces annotation and training costs by 32\% without
performance loss. It is able to automatically give the saving ratio without
tuning the ratio. It can further reduce the annotation ratio to 50\% with
semi-supervised learning. We also explore retrieval-based dataset enhancement
using unlabeled open-source data. Code is available at
https://github.com/NUS-HPC-AI-Lab/Info-Coevolution/.

</details>


### [47] [Benchmarking Pre-Trained Time Series Models for Electricity Price Forecasting](https://arxiv.org/abs/2506.08113)
*Timothée Hornek Amir Sartipi,Igor Tchappi,Gilbert Fridgen*

Main category: cs.LG

TL;DR: Electricity price forecasting is vital in power trading. This study benchmarks several advanced models like Chronos-Bolt, Time-MoE, and biseasonal MSTL against traditional methods using day-ahead auction data from five European countries. Results indicate that while some TSFMs perform well, the biseasonal MSTL model consistently outperforms.


<details>
  <summary>Details</summary>
Motivation: Accurate electricity price forecasting (EPF) plays a critical role in effective decision-making in power trading on the spot market. The rise of generative artificial intelligence (GenAI) and pre-trained large language models (LLMs) has led to the development of time series foundation models (TSFMs). However, their effectiveness in EPF remains unclear, prompting the need for a benchmarking study.

Method: The research evaluates several state-of-the-art pretrained models including Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and TimeGPT against established statistical and machine learning (ML) methods for EPF. The evaluation uses 2024 day-ahead auction (DAA) electricity prices from Germany, France, the Netherlands, Austria, and Belgium, generating daily forecasts with a one-day horizon.

Result: Chronos-Bolt and Time-MoE emerge as the strongest among the TSFMs, performing similarly to traditional models. However, the biseasonal MSTL model stands out due to its consistent performance across different countries and evaluation metrics, with no TSFM statistically surpassing it.

Conclusion: While some TSFMs show promise in electricity price forecasting, the biseasonal MSTL model demonstrates superior and consistent performance compared to these advanced models.

Abstract: Accurate electricity price forecasting (EPF) is crucial for effective
decision-making in power trading on the spot market. While recent advances in
generative artificial intelligence (GenAI) and pre-trained large language
models (LLMs) have inspired the development of numerous time series foundation
models (TSFMs) for time series forecasting, their effectiveness in EPF remains
uncertain. To address this gap, we benchmark several state-of-the-art
pretrained models--Chronos-Bolt, Chronos-T5, TimesFM, Moirai, Time-MoE, and
TimeGPT--against established statistical and machine learning (ML) methods for
EPF. Using 2024 day-ahead auction (DAA) electricity prices from Germany,
France, the Netherlands, Austria, and Belgium, we generate daily forecasts with
a one-day horizon. Chronos-Bolt and Time-MoE emerge as the strongest among the
TSFMs, performing on par with traditional models. However, the biseasonal MSTL
model, which captures daily and weekly seasonality, stands out for its
consistent performance across countries and evaluation metrics, with no TSFM
statistically outperforming it.

</details>


### [48] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Large language models (LLMs) often produce verbose outputs. This paper introduces Bingo, an RL framework that enhances reasoning efficiency in LLMs without sacrificing accuracy. It uses significance-aware and dynamic length rewards to achieve a balance between concise and accurate outputs.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in LLMs caused by verbose or redundant outputs while maintaining or improving accuracy.

Method: Proposes Bingo, an RL framework with two key mechanisms: significance-aware length reward and dynamic length reward. The former reduces insignificant tokens gradually, and the latter adjusts reasoning elaboration based on question difficulty over time.

Result: Bingo improves both accuracy and efficiency across multiple reasoning benchmarks, outperforming vanilla rewards and other length-based reward baselines in RL.

Conclusion: The results highlight the potential of training LLMs explicitly for efficient reasoning.

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [49] [Nearness of Neighbors Attention for Regression in Supervised Finetuning](https://arxiv.org/abs/2506.08139)
*Aviad Susman,Mayte Suárez-Fariñas,Joseph T Colonel*

Main category: cs.LG

TL;DR: The paper introduces Nearness of Neighbors Attention (NONA) regression layer, a differentiable proxy for k-NN regression algorithm, using neural network attention and a novel learned attention-masking scheme. Results show improved performance over dense layer prediction and k-NN on SFT embeddings in regression tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional predictors often deliver increased performance over the SFT model itself when combined with neural networks in supervised machine learning. This suggests that directly incorporating traditional algorithms into SFT as prediction layers may further improve performance.

Method: The authors introduce NONA, which uses the mechanics of neural network attention and a novel learned attention-masking scheme to yield a differentiable proxy of the k-NN regression algorithm.

Result: Results on multiple unstructured datasets show improved performance over both dense layer prediction and k-NN on SFT embeddings for regression.

Conclusion: The Nearness of Neighbors Attention (NONA) regression layer offers a way to incorporate traditional algorithms like k-NN into neural networks, improving performance in regression tasks.

Abstract: It is common in supervised machine learning to combine the feature extraction
capabilities of neural networks with the predictive power of traditional
algorithms, such as k-nearest neighbors (k-NN) or support vector machines. This
procedure involves performing supervised fine-tuning (SFT) on a
domain-appropriate feature extractor, followed by training a traditional
predictor on the resulting SFT embeddings. When used in this manner,
traditional predictors often deliver increased performance over the SFT model
itself, despite the fine-tuned feature extractor yielding embeddings
specifically optimized for prediction by the neural network's final dense
layer. This suggests that directly incorporating traditional algorithms into
SFT as prediction layers may further improve performance. However, many
traditional algorithms have not been implemented as neural network layers due
to their non-differentiable nature and their unique optimization requirements.
As a step towards solving this problem, we introduce the Nearness of Neighbors
Attention (NONA) regression layer. NONA uses the mechanics of neural network
attention and a novel learned attention-masking scheme to yield a
differentiable proxy of the k-NN regression algorithm. Results on multiple
unstructured datasets show improved performance over both dense layer
prediction and k-NN on SFT embeddings for regression.

</details>


### [50] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: The paper introduces AutoSDT, an automatic pipeline to collect high-quality coding tasks for data-driven scientific discovery workflows. It creates the AutoSDT-5K dataset, which is used to train the Qwen2.5-Coder-Instruct LLM series (AutoSDT-Coder). This leads to significant improvements on benchmarks like ScienceAgentBench and DiscoveryBench.


<details>
  <summary>Details</summary>
Motivation: There is a lack of high-quality data for training and evaluating AI models in the field of scientific discovery. The authors aim to address this issue by creating a system that automatically gathers relevant coding tasks.

Method: The AutoSDT pipeline leverages the capabilities of LLMs to search for diverse sources, select valid tasks, and synthesize accurate task instructions and code solutions. Using this pipeline, they construct the AutoSDT-5K dataset consisting of 5,404 coding tasks covering four scientific disciplines and 756 unique Python packages.

Result: Expert feedback indicates that 93% of the collected tasks are ecologically valid and 92.2% of the synthesized programs are functionally correct. Training on AutoSDT-5K significantly improves performance on challenging benchmarks, with AutoSDT-Coder-32B doubling the performance of its base model on ScienceAgentBench and achieving a 17.4% relative improvement on DiscoveryBench.

Conclusion: AutoSDT addresses the data scarcity issue in AI for scientific discovery by providing a large, high-quality dataset. The resulting AutoSDT-Coder LLM series demonstrates substantial improvements in performance on data-driven discovery benchmarks.

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [51] [Private Evolution Converges](https://arxiv.org/abs/2506.08312)
*Tomás González,Giulia Fanti,Aaditya Ramdas*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Private Evolution (PE) is a promising training-free method for differentially
private (DP) synthetic data generation. While it achieves strong performance in
some domains (e.g., images and text), its behavior in others (e.g., tabular
data) is less consistent. To date, the only theoretical analysis of the
convergence of PE depends on unrealistic assumptions about both the algorithm's
behavior and the structure of the sensitive dataset. In this work, we develop a
new theoretical framework to explain PE's practical behavior and identify
sufficient conditions for its convergence. For $d$-dimensional sensitive
datasets with $n$ data points from a bounded domain, we prove that PE produces
an $(\epsilon, \delta)$-DP synthetic dataset with expected 1-Wasserstein
distance of order $\tilde{O}(d(n\epsilon)^{-1/d})$ from the original,
establishing worst-case convergence of the algorithm as $n \to \infty$. Our
analysis extends to general Banach spaces as well. We also connect PE to the
Private Signed Measure Mechanism, a method for DP synthetic data generation
that has thus far not seen much practical adoption. We demonstrate the
practical relevance of our theoretical findings in simulations.

</details>


### [52] [Accelerating Spectral Clustering under Fairness Constraints](https://arxiv.org/abs/2506.08143)
*Francesco Tonin,Alex Lambert,Johan A. K. Suykens,Volkan Cevher*

Main category: cs.LG

TL;DR: 本文提出了一种新的有效方法来进行公平谱聚类（Fair SC），通过引入新颖的变量增强策略和适应于DC问题的乘子交替方向法算法，解决了先前工作中计算昂贵的特征分解问题，实验表明该方法在合成和真实世界基准上均具有显著的计算加速效果。


<details>
  <summary>Details</summary>
Motivation: 决策算法的公平性日益重要，特别是在谱聚类中实现群体公平约束的问题，即每个族群在每个簇中的代表性应与总体人口成比例。

Method: 作者将公平谱聚类问题置于凸函数差（DC）框架内，并引入了一种新颖的变量增强策略以及适应于DC问题的乘子交替方向法算法。这种方法避免了先前工作中需要的昂贵的特征分解计算。

Result: 数值实验表明，该方法在合成和真实世界数据集上均表现出显著的计算加速效果，尤其是在问题规模增大时更为明显。

Conclusion: 这项工作为实际应用中的公平聚类提供了一个重要的进展。

Abstract: Fairness of decision-making algorithms is an increasingly important issue. In
this paper, we focus on spectral clustering with group fairness constraints,
where every demographic group is represented in each cluster proportionally as
in the general population. We present a new efficient method for fair spectral
clustering (Fair SC) by casting the Fair SC problem within the difference of
convex functions (DC) framework. To this end, we introduce a novel variable
augmentation strategy and employ an alternating direction method of multipliers
type of algorithm adapted to DC problems. We show that each associated
subproblem can be solved efficiently, resulting in higher computational
efficiency compared to prior work, which required a computationally expensive
eigendecomposition. Numerical experiments demonstrate the effectiveness of our
approach on both synthetic and real-world benchmarks, showing significant
speedups in computation time over prior art, especially as the problem size
grows. This work thus represents a considerable step forward towards the
adoption of fair clustering in real-world applications.

</details>


### [53] [Differentially Private Relational Learning with Entity-level Privacy Guarantees](https://arxiv.org/abs/2506.08347)
*Yinan Huang,Haoteng Ying,Eli Chien,Rongzhe Wei,Pan Li*

Main category: cs.LG

TL;DR: 该论文提出了一种针对关系数据的差分隐私学习框架，解决了DP-SGD在关系学习中的应用难题，并通过实验验证了其在隐私保护与模型效用之间的良好权衡。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在处理关系和网络结构化数据时，需要保护个体实体的隐私。然而，直接将DP-SGD应用于关系学习存在高敏感性和复杂的采样过程等问题，亟需一种专门针对关系数据的差分隐私解决方案。

Method: 1. 提出了一种针对关系学习的形式化实体级差分隐私框架。
2. 进行严格的敏感性分析并引入自适应梯度裁剪方案，依据实体出现频率调整裁剪阈值。
3. 扩展隐私放大结果至一类依赖样本大小的耦合采样子集。
4. 设计了一种适用于关系数据的改进版DP-SGD算法，提供可证明的隐私保障。

Result: 实验表明，在细调文本编码器时，该方法能够在关系数据上实现强大的效用-隐私权衡。代码已公开。

Conclusion: 本文提出的框架为关系数据的差分隐私学习提供了有效解决方案，能够在保护隐私的同时保持较高的模型效用。

Abstract: Learning with relational and network-structured data is increasingly vital in
sensitive domains where protecting the privacy of individual entities is
paramount. Differential Privacy (DP) offers a principled approach for
quantifying privacy risks, with DP-SGD emerging as a standard mechanism for
private model training. However, directly applying DP-SGD to relational
learning is challenging due to two key factors: (i) entities often participate
in multiple relations, resulting in high and difficult-to-control sensitivity;
and (ii) relational learning typically involves multi-stage, potentially
coupled (interdependent) sampling procedures that make standard privacy
amplification analyses inapplicable. This work presents a principled framework
for relational learning with formal entity-level DP guarantees. We provide a
rigorous sensitivity analysis and introduce an adaptive gradient clipping
scheme that modulates clipping thresholds based on entity occurrence frequency.
We also extend the privacy amplification results to a tractable subclass of
coupled sampling, where the dependence arises only through sample sizes. These
contributions lead to a tailored DP-SGD variant for relational data with
provable privacy guarantees. Experiments on fine-tuning text encoders over
text-attributed network-structured relational data demonstrate the strong
utility-privacy trade-offs of our approach. Our code is available at
https://github.com/Graph-COM/Node_DP.

</details>


### [54] [Fully data-driven inverse hyperelasticity with hyper-network neural ODE fields](https://arxiv.org/abs/2506.08146)
*Vahidullah Taç,Amirhossein Amiri-Hezaveh,Manuel K. Rausch,Grace N. Bechtel,Francisco Sahli Costabal,Adrian Buganza Tepole*

Main category: cs.LG

TL;DR: A new framework using neural networks with Fourier features and NODEs to identify mechanical properties of heterogeneous materials from displacement field data, showcasing robustness and generality in several numerical examples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying mechanical properties of heterogeneous materials without a closed-form constitutive equation, leveraging full-field measurements like those from digital image correlation (DIC).

Method: Propose a framework that uses a neural network incorporating Fourier features for strain field approximation and NODEs for discovering constitutive equations. A hyper-network accounts for heterogeneity by taking material coordinate system as input and outputting NODE-based constitutive equations. Parameters are optimized through a multi-objective loss function including penalties for equilibrium and boundary condition violations.

Result: The framework successfully identifies mechanical properties in various scenarios, including material parameter variations, transitions from isotropy to anisotropy, noise presence, and experimental data application, demonstrating robustness and generality.

Conclusion: This approach offers a suitable alternative to classical inverse methods, requiring very few assumptions to effectively identify the mechanical properties of heterogeneous materials.

Abstract: We propose a new framework for identifying mechanical properties of
heterogeneous materials without a closed-form constitutive equation. Given a
full-field measurement of the displacement field, for instance as obtained from
digital image correlation (DIC), a continuous approximation of the strain field
is obtained by training a neural network that incorporates Fourier features to
effectively capture sharp gradients in the data. A physics-based data-driven
method built upon ordinary neural differential equations (NODEs) is employed to
discover constitutive equations. The NODE framework can represent arbitrary
materials while satisfying constraints in the theory of constitutive equations
by default. To account for heterogeneity, a hyper-network is defined, where the
input is the material coordinate system, and the output is the NODE-based
constitutive equation. The parameters of the hyper-network are optimized by
minimizing a multi-objective loss function that includes penalty terms for
violations of the strong form of the equilibrium equations of elasticity and
the associated Neumann boundary conditions. We showcase the framework with
several numerical examples, including heterogeneity arising from variations in
material parameters, spatial transitions from isotropy to anisotropy, material
identification in the presence of noise, and, ultimately, application to
experimental data. As the numerical results suggest, the proposed approach is
robust and general in identifying the mechanical properties of heterogeneous
materials with very few assumptions, making it a suitable alternative to
classical inverse methods.

</details>


### [55] [Network Threat Detection: Addressing Class Imbalanced Data with Deep Forest](https://arxiv.org/abs/2506.08383)
*Jiaqi Chen,Rongbin Ye*

Main category: cs.LG

TL;DR: The paper conducts an empirical analysis of machine learning techniques for detecting malicious IoT traffic using the IoT-23 dataset, showing gcForest with imbalance treatment achieves superior performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting malicious traffic in real-time within expanding IoT networks, which is crucial for cybersecurity.

Method: Empirical analysis of machine learning techniques on the IoT-23 dataset with three resampling strategies to handle class imbalance and comparison of various methods including ensemble approaches like gcForest.

Result: gcForest combined with appropriate imbalance treatment techniques outperforms traditional methods in detection performance.

Conclusion: This research advances the creation of intelligent and efficient automated threat detection systems for securing IoT environments.

Abstract: With the rapid expansion of Internet of Things (IoT) networks, detecting
malicious traffic in real-time has become a critical cybersecurity challenge.
This research addresses the detection challenges by presenting a comprehensive
empirical analysis of machine learning techniques for malware detection using
the IoT-23 dataset provided by the Stratosphere Laboratory. We address the
significant class imbalance within the dataset through three resampling
strategies. We implement and compare a few machine learning techniques. Our
findings demonstrate that the combination of appropriate imbalance treatment
techniques with ensemble methods, particularly gcForest, achieves better
detection performance compared to traditional approaches. This work contributes
significantly to the development of more intelligent and efficient automated
threat detection systems for IoT environments, helping to secure critical
infrastructure against sophisticated cyber attacks while optimizing
computational resource usage.

</details>


### [56] [BLUR: A Bi-Level Optimization Approach for LLM Unlearning](https://arxiv.org/abs/2506.08164)
*Hadi Reisizadeh,Jinghan Jia,Zhiqi Bu,Bhanukiran Vinzamuri,Anil Ramakrishna,Kai-Wei Chang,Volkan Cevher,Sijia Liu,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了一种新的双层优化公式化遗忘问题的方法，并基于此提出了名为BLUR的新算法，该算法在各种遗忘任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前最流行的遗忘问题解决方案通常会导致性能下降，因为它们未能很好地处理遗忘和保留损失之间的固有折衷。因此，需要一种新的方法来更好地解决这一问题。

Method: 作者提出将遗忘问题建模为具有层级结构的双层优化问题，其中下层目标专注于最小化遗忘损失，而上层目标则旨在保持模型的实用性。基于这种新公式化，他们提出了名为BLUR的新算法。

Result: 广泛的实验表明，BLUR算法在各种遗忘任务、模型和度量标准上始终优于所有最先进的算法。

Conclusion: BLUR不仅具备强大的理论保证，而且在实践中也表现出了优越的性能。

Abstract: Enabling large language models (LLMs) to unlearn knowledge and capabilities
acquired during training has proven vital for ensuring compliance with data
regulations and promoting ethical practices in generative AI. Although there
are growing interests in developing various unlearning algorithms, it remains
unclear how to best formulate the unlearning problem. The most popular
formulation uses a weighted sum of forget and retain loss, but it often leads
to performance degradation due to the inherent trade-off between forget and
retain losses. In this work, we argue that it is important to model the
hierarchical structure of the unlearning problem, where the forget problem
(which \textit{unlearns} certain knowledge and/or capabilities) takes priority
over the retain problem (which preserves model utility). This hierarchical
structure naturally leads to a bi-level optimization formulation where the
lower-level objective focuses on minimizing the forget loss, while the
upper-level objective aims to maintain the model's utility. Based on this new
formulation, we propose a novel algorithm, termed Bi-Level UnleaRning
(\texttt{BLUR}), which not only possesses strong theoretical guarantees but
more importantly, delivers superior performance. In particular, our extensive
experiments demonstrate that \texttt{BLUR} consistently outperforms all the
state-of-the-art algorithms across various unlearning tasks, models, and
metrics. Codes are available at
https://github.com/OptimAI-Lab/BLURLLMUnlearning.

</details>


### [57] [Boosting Gradient Leakage Attacks: Data Reconstruction in Realistic FL Settings](https://arxiv.org/abs/2506.08435)
*Mingyuan Fan,Fuyi Wang,Cen Chen,Jianying Zhou*

Main category: cs.LG

TL;DR: Federated learning (FL) allows multiple clients to collaboratively train models without exposing raw data, and its privacy protection ability has been debated. Some studies introduce gradient leakage attacks (GLAs) that can reconstruct client data through shared gradients, but others argue that GLAs are not effective in practical FL environments. This paper demonstrates that client data can still be effectively reconstructed within realistic FL environments by revisiting GLAs and developing FedLeak, which introduces partial gradient matching and gradient regularization techniques. FedLeak achieves high-fidelity data reconstruction under a practical evaluation protocol, highlighting the vulnerability in FL systems.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in the debate about whether federated learning (FL) poses significant privacy risks in practical environments, as previous studies have shown both the potential for gradient leakage attacks (GLAs) and their ineffectiveness under certain conditions.

Method: The authors revisit gradient leakage attacks (GLAs) and identify performance failures due to the inability to handle the gradient matching problem. They then develop FedLeak, incorporating two novel techniques: partial gradient matching and gradient regularization. Additionally, they create a practical evaluation protocol based on a thorough review of FL literature and industry practices to assess FedLeak's performance in real-world FL environments.

Result: FedLeak is able to achieve high-fidelity data reconstruction under the practical evaluation protocol formulated by the authors, demonstrating that client data can be effectively reconstructed even in realistic FL environments.

Conclusion: This study underscores the significant vulnerability present in current FL systems and emphasizes the urgent need for more effective defense methods against data reconstruction attacks.

Abstract: Federated learning (FL) enables collaborative model training among multiple
clients without the need to expose raw data. Its ability to safeguard privacy,
at the heart of FL, has recently been a hot-button debate topic. To elaborate,
several studies have introduced a type of attacks known as gradient leakage
attacks (GLAs), which exploit the gradients shared during training to
reconstruct clients' raw data. On the flip side, some literature, however,
contends no substantial privacy risk in practical FL environments due to the
effectiveness of such GLAs being limited to overly relaxed conditions, such as
small batch sizes and knowledge of clients' data distributions.
  This paper bridges this critical gap by empirically demonstrating that
clients' data can still be effectively reconstructed, even within realistic FL
environments. Upon revisiting GLAs, we recognize that their performance
failures stem from their inability to handle the gradient matching problem. To
alleviate the performance bottlenecks identified above, we develop FedLeak,
which introduces two novel techniques, partial gradient matching and gradient
regularization. Moreover, to evaluate the performance of FedLeak in real-world
FL environments, we formulate a practical evaluation protocol grounded in a
thorough review of extensive FL literature and industry practices. Under this
protocol, FedLeak can still achieve high-fidelity data reconstruction, thereby
underscoring the significant vulnerability in FL systems and the urgent need
for more effective defense methods.

</details>


### [58] [UniVarFL: Uniformity and Variance Regularized Federated Learning for Heterogeneous Data](https://arxiv.org/abs/2506.08167)
*Sunny Gupta,Nikita Jangid,Amit Sethi*

Main category: cs.LG

TL;DR: UniVarFL is a new Federated Learning framework that addresses non-IID data challenges through two regularization strategies, improving accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Federated Learning experiences performance issues with non-IID data due to local classifier bias. Current solutions either have high computational costs or adapt poorly to feature shifts.

Method: Proposes UniVarFL with Classifier Variance Regularization to align class-wise probability distributions for mitigating bias, and Hyperspherical Uniformity Regularization to encourage uniform feature representation distribution enhancing generalization.

Result: Experiments on benchmark datasets show UniVarFL outperforms existing methods in accuracy, making it scalable and efficient for real-world applications, especially in resource-constrained settings.

Conclusion: UniVarFL presents an effective solution to the non-IID data problem in Federated Learning, offering improved accuracy and scalability.

Abstract: Federated Learning (FL) often suffers from severe performance degradation
when faced with non-IID data, largely due to local classifier bias. Traditional
remedies such as global model regularization or layer freezing either incur
high computational costs or struggle to adapt to feature shifts. In this work,
we propose UniVarFL, a novel FL framework that emulates IID-like training
dynamics directly at the client level, eliminating the need for global model
dependency. UniVarFL leverages two complementary regularization strategies
during local training: Classifier Variance Regularization, which aligns
class-wise probability distributions with those expected under IID conditions,
effectively mitigating local classifier bias; and Hyperspherical Uniformity
Regularization, which encourages a uniform distribution of feature
representations across the hypersphere, thereby enhancing the model's ability
to generalize under diverse data distributions. Extensive experiments on
multiple benchmark datasets demonstrate that UniVarFL outperforms existing
methods in accuracy, highlighting its potential as a highly scalable and
efficient solution for real-world FL deployments, especially in
resource-constrained settings. Code: https://github.com/sunnyinAI/UniVarFL

</details>


### [59] [Federated Learning on Stochastic Neural Networks](https://arxiv.org/abs/2506.08169)
*Jingqiao Tang,Ryan Bausback,Feng Bao,Richard Archibald*

Main category: cs.LG

TL;DR: In this paper, the authors tackle the issue of latent noise in federated learning by proposing Federated stochastic neural networks, which incorporate stochastic neural networks as local models. They demonstrate its performance through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to latent noise due to inaccuracies in client data from factors like limited measurement capabilities or human errors.

Method: The use of a stochastic neural network as the local model within the federated learning framework to estimate true underlying states of data and quantify latent noise.

Result: Numerical experiments show the effectiveness of the proposed method in handling non-independent and identically distributed data.

Conclusion: Federated stochastic neural networks provide an effective approach to dealing with latent noise in federated learning.

Abstract: Federated learning is a machine learning paradigm that leverages edge
computing on client devices to optimize models while maintaining user privacy
by ensuring that local data remains on the device. However, since all data is
collected by clients, federated learning is susceptible to latent noise in
local datasets. Factors such as limited measurement capabilities or human
errors may introduce inaccuracies in client data. To address this challenge, we
propose the use of a stochastic neural network as the local model within the
federated learning framework. Stochastic neural networks not only facilitate
the estimation of the true underlying states of the data but also enable the
quantification of latent noise. We refer to our federated learning approach,
which incorporates stochastic neural networks as local models, as Federated
stochastic neural networks. We will present numerical experiments demonstrating
the performance and effectiveness of our method, particularly in handling
non-independent and identically distributed data.

</details>


### [60] [FedGA-Tree: Federated Decision Tree using Genetic Algorithm](https://arxiv.org/abs/2506.08176)
*Anh V Nguyen,Diego Klabjan*

Main category: cs.LG

TL;DR: The paper explores an alternative approach using Genetic Algorithm to build personalized decision trees in Federated Learning, which accommodates categorical and numerical data for both classification and regression. Experiments show it outperforms local-only training and a benchmark algorithm.


<details>
  <summary>Details</summary>
Motivation: Federated Learning has become prominent due to data privacy concerns, but nonparametric models like decision trees are relatively understudied compared to parametric gradient-based models. Existing methods for adapting decision trees to Federated Learning have limitations with differential privacy.

Method: The paper proposes utilizing Genetic Algorithm to facilitate the construction of personalized decision trees that can accommodate both categorical and numerical data, allowing for both classification and regression trees.

Result: Comprehensive experiments demonstrate that the proposed method surpasses decision trees trained solely on local data and also outperforms a benchmark algorithm.

Conclusion: The use of Genetic Algorithm for constructing personalized decision trees in Federated Learning is effective for both classification and regression tasks, and performs better than existing methods.

Abstract: In recent years, with rising concerns for data privacy, Federated Learning
has gained prominence, as it enables collaborative training without the
aggregation of raw data from participating clients. However, much of the
current focus has been on parametric gradient-based models, while nonparametric
counterparts such as decision tree are relatively understudied. Existing
methods for adapting decision trees to Federated Learning generally combine a
greedy tree-building algorithm with differential privacy to produce a global
model for all clients. These methods are limited to classification trees and
categorical data due to the constraints of differential privacy. In this paper,
we explore an alternative approach that utilizes Genetic Algorithm to
facilitate the construction of personalized decision trees and accommodate
categorical and numerical data, thus allowing for both classification and
regression trees. Comprehensive experiments demonstrate that our method
surpasses decision trees trained solely on local data and a benchmark
algorithm.

</details>


### [61] [Correlated Noise Mechanisms for Differentially Private Learning](https://arxiv.org/abs/2506.08201)
*Krishna Pillutla,Jalaj Upadhyay,Christopher A. Choquette-Choo,Krishnamurthy Dvijotham,Arun Ganesh,Monika Henzinger,Jonathan Katz,Ryan McKenna,H. Brendan McMahan,Keith Rush,Thomas Steinke,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: This monograph explores the design and analysis of correlated noise mechanisms for differential privacy (DP) in private training of AI models.


<details>
  <summary>Details</summary>
Motivation: To improve privacy-utility trade-offs by introducing (anti-)correlations in the noise injected into each step of a stochastic gradient learning algorithm.

Method: Correlated noise mechanisms such as matrix mechanisms, factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) are applied to cancel out some of the noise added on earlier steps in subsequent steps.

Result: These mechanisms have been influential in practice, with industrial deployment at a global scale.

Conclusion: The exploration focuses on their application to private training of AI and machine learning models via the core primitive of estimation of weighted prefix sums.

Abstract: This monograph explores the design and analysis of correlated noise
mechanisms for differential privacy (DP), focusing on their application to
private training of AI and machine learning models via the core primitive of
estimation of weighted prefix sums. While typical DP mechanisms inject
independent noise into each step of a stochastic gradient (SGD) learning
algorithm in order to protect the privacy of the training data, a growing body
of recent research demonstrates that introducing (anti-)correlations in the
noise can significantly improve privacy-utility trade-offs by carefully
canceling out some of the noise added on earlier steps in subsequent steps.
Such correlated noise mechanisms, known variously as matrix mechanisms,
factorization mechanisms, and DP-Follow-the-Regularized-Leader (DP-FTRL) when
applied to learning algorithms, have also been influential in practice, with
industrial deployment at a global scale.

</details>


### [62] [A Machine Learning Approach to Generate Residual Stress Distributions using Sparse Characterization Data in Friction-Stir Processed Parts](https://arxiv.org/abs/2506.08205)
*Shadab Anwar Shaikh,Kranthi Balusu,Ayoub Soulami*

Main category: cs.LG

TL;DR: The paper proposes a machine learning-based Residual Stress Generator (RSG) using U-Net architecture to predict full-field residual stress distributions from limited measurements, significantly reducing experimental efforts.


<details>
  <summary>Details</summary>
Motivation: Residual stresses within components can negatively impact performance, and accurately determining their distribution is crucial for structural integrity and longevity. However, full-field characterization through experiments is impractical.

Method: An extensive dataset was created via process simulations with diverse parameters. A U-Net based ML model was trained with hyperparameter tuning to learn the latent structure of residual stress distribution. The model was evaluated on simulated data and validated on experimental data.

Result: The model achieved excellent predictive accuracy on simulated stress prediction and showed significant generalization ability, indicating successful learning of the latent structure. It also performed well on experimentally characterized data.

Conclusion: The proposed RSG approach is feasible for predicting full-field residual stress distributions from limited measurements, greatly reducing the need for extensive experimental efforts.

Abstract: Residual stresses, which remain within a component after processing, can
deteriorate performance. Accurately determining their full-field distributions
is essential for optimizing the structural integrity and longevity. However,
the experimental effort required for full-field characterization is
impractical. Given these challenges, this work proposes a machine learning (ML)
based Residual Stress Generator (RSG) to infer full-field stresses from limited
measurements. An extensive dataset was initially constructed by performing
numerous process simulations with a diverse parameter set. A ML model based on
U-Net architecture was then trained to learn the underlying structure through
systematic hyperparameter tuning. Then, the model's ability to generate
simulated stresses was evaluated, and it was ultimately tested on actual
characterization data to validate its effectiveness. The model's prediction of
simulated stresses shows that it achieved excellent predictive accuracy and
exhibited a significant degree of generalization, indicating that it
successfully learnt the latent structure of residual stress distribution. The
RSG's performance in predicting experimentally characterized data highlights
the feasibility of the proposed approach in providing a comprehensive
understanding of residual stress distributions from limited measurements,
thereby significantly reducing experimental efforts.

</details>


### [63] [What makes an Ensemble (Un) Interpretable?](https://arxiv.org/abs/2506.08216)
*Shahaf Bassan,Guy Amir,Meirav Zehavi,Guy Katz*

Main category: cs.LG

TL;DR: The paper explores why ensemble models are hard to interpret by using computational complexity theory, revealing that factors like the number and type of base models significantly affect interpretability.


<details>
  <summary>Details</summary>
Motivation: Ensemble models, despite being powerful, are often treated as black-boxes due to limited interpretability. There is a lack of rigorous understanding of what makes ensembles (un)-interpretable based on fundamental factors such as the number, size, and type of base models.

Method: The authors apply concepts from computational complexity theory to analyze the challenges of generating explanations for different ensemble configurations, considering various factors including the number, size, and type of base models.

Result: The analysis reveals nuanced complexity patterns showing that interpreting ensembles remains intractable even when base models are of constant size under standard complexity assumptions like P≠NP. However, small ensembles of decision trees are efficiently interpretable while ensembles with even a few linear models remain intractable.

Conclusion: The findings provide a more robust foundation for understanding ensemble interpretability, highlighting the importance of examining it through the lens of computational complexity.

Abstract: Ensemble models are widely recognized in the ML community for their limited
interpretability. For instance, while a single decision tree is considered
interpretable, ensembles of trees (e.g., boosted trees) are often treated as
black-boxes. Despite this folklore recognition, there remains a lack of
rigorous mathematical understanding of what particularly makes an ensemble
(un)-interpretable, including how fundamental factors like the (1) *number*,
(2) *size*, and (3) *type* of base models influence its interpretability. In
this work, we seek to bridge this gap by applying concepts from computational
complexity theory to study the challenges of generating explanations for
various ensemble configurations. Our analysis uncovers nuanced complexity
patterns influenced by various factors. For example, we demonstrate that under
standard complexity assumptions like P$\neq$NP, interpreting ensembles remains
intractable even when base models are of constant size. Surprisingly, the
complexity changes drastically with the number of base models: small ensembles
of decision trees are efficiently interpretable, whereas interpreting ensembles
with even a constant number of linear models remains intractable. We believe
that our findings provide a more robust foundation for understanding the
interpretability of ensembles, emphasizing the benefits of examining it through
a computational complexity lens.

</details>


### [64] [Mondrian: Transformer Operators via Domain Decomposition](https://arxiv.org/abs/2506.08226)
*Arthur Feeney,Kuei-Hsiang Huang,Aparna Chandramowlishwaran*

Main category: cs.LG

TL;DR: Mondrian is a transformer-based operator model that decomposes domains into subdomains for efficient attention computation, using neural operators within each subdomain and softmax-based inner products across subdomains. It shows strong performance on PDEs like Allen-Cahn and Navier-Stokes without retraining when resolution scales.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling transformer-based operator models to high-resolution, multiscale domains due to the quadratic cost of attention and its coupling to discretization.

Method: The method involves decomposing a domain into non-overlapping subdomains and applying attention over sequences of subdomain-restricted functions. Within each subdomain, standard layers are replaced with expressive neural operators, and attention across subdomains is computed via softmax-based inner products over functions.

Result: Mondrian achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating resolution scaling without retraining.

Conclusion: These results highlight the promise of domain-decomposed attention for scalable and general-purpose neural operators.

Abstract: Operator learning enables data-driven modeling of partial differential
equations (PDEs) by learning mappings between function spaces. However, scaling
transformer-based operator models to high-resolution, multiscale domains
remains a challenge due to the quadratic cost of attention and its coupling to
discretization. We introduce \textbf{Mondrian}, transformer operators that
decompose a domain into non-overlapping subdomains and apply attention over
sequences of subdomain-restricted functions. Leveraging principles from domain
decomposition, Mondrian decouples attention from discretization. Within each
subdomain, it replaces standard layers with expressive neural operators, and
attention across subdomains is computed via softmax-based inner products over
functions. The formulation naturally extends to hierarchical windowed and
neighborhood attention, supporting both local and global interactions. Mondrian
achieves strong performance on Allen-Cahn and Navier-Stokes PDEs, demonstrating
resolution scaling without retraining. These results highlight the promise of
domain-decomposed attention for scalable and general-purpose neural operators.

</details>


### [65] [Scaling Laws of Motion Forecasting and Planning -- A Technical Report](https://arxiv.org/abs/2506.08228)
*Mustafa Baniodeh,Kratarth Goel,Scott Ettinger,Carlos Fuertes,Ari Seff,Tim Shen,Cole Gulino,Chenjie Yang,Ghassen Jerfel,Dokook Choe,Rui Wang,Vinutha Kallem,Sergio Casas,Rami Al-Rfou,Benjamin Sapp,Dragomir Anguelov*

Main category: cs.LG

TL;DR: 研究了自动驾驶领域中编码器-解码器自回归Transformer模型的经验缩放定律，展示了模型性能与计算预算的幂律关系，并探讨了模型参数、训练数据和推理时间计算的最佳缩放策略。


<details>
  <summary>Details</summary>
Motivation: 探索在自动驾驶任务中，模型性能如何随计算预算、参数数量和训练数据规模的变化而变化，以及这些变化对开环和闭环度量的影响。

Method: 使用50万小时驾驶数据集分析Transformer模型的缩放规律，研究模型参数、训练数据规模和推理计算的最佳缩放策略，并比较小模型采样与大模型的效果。

Result: 模型性能随计算预算呈幂律增长，训练损失与评估指标高度相关；闭环指标也随缩放改善；模型大小应以1.5倍于数据集大小的速度增长；小模型通过采样和聚类可与大模型竞争至交叉点。

Conclusion: 优化模型训练和推理时的缩放特性是提升自动驾驶模型性能的关键，同时利用其他代理人的驾驶数据有助于缓解机器人数据稀缺问题。

Abstract: We study the empirical scaling laws of a family of encoder-decoder
autoregressive transformer models on the task of joint motion forecasting and
planning in the autonomous driving domain. Using a 500 thousand hours driving
dataset, we demonstrate that, similar to language modeling, model performance
improves as a power-law function of the total compute budget, and we observe a
strong correlation between model training loss and model evaluation metrics.
Most interestingly, closed-loop metrics also improve with scaling, which has
important implications for the suitability of open-loop metrics for model
development and hill climbing. We also study the optimal scaling of the number
of transformer parameters and the training data size for a training
compute-optimal model. We find that as the training compute budget grows,
optimal scaling requires increasing the model size 1.5x as fast as the dataset
size. We also study inference-time compute scaling, where we observe that
sampling and clustering the output of smaller models makes them competitive
with larger models, up to a crossover point beyond which a larger models
becomes more inference-compute efficient. Overall, our experimental results
demonstrate that optimizing the training and inference-time scaling properties
of motion forecasting and planning models is a key lever for improving their
performance to address a wide variety of driving scenarios. Finally, we briefly
study the utility of training on general logged driving data of other agents to
improve the performance of the ego-agent, an important research area to address
the scarcity of robotics data for large capacity models training.

</details>


### [66] [Ensuring Reliability of Curated EHR-Derived Data: The Validation of Accuracy for LLM/ML-Extracted Information and Data (VALID) Framework](https://arxiv.org/abs/2506.08231)
*Melissa Estevez,Nisha Singh,Lauren Dyson,Blythe Adamson,Qianyu Yuan,Megan W. Hildner,Erin Fidyk,Olive Mbah,Farhad Khan,Kathi Seidl-Rathkopf,Aaron B. Cohen*

Main category: cs.LG

TL;DR: 提出了一种评估大型语言模型从电子健康记录中提取临床数据质量的全面框架，该框架通过多维度方法确保数据可靠性、准确性和公平性，支持在肿瘤学研究和实践中可信赖地使用AI生成的证据。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）提高了真实世界数据（RWD）在肿瘤学中的整理效率，但其引入的数据可靠性、准确性和公平性问题尚未被现有质量保证框架充分解决。

Method: 提出了一个综合框架，包括：变量级别的性能基准测试（与人类专家对比）、自动化验证检查（内部一致性和合理性）、复制分析（与人类抽象数据集或外部标准比较），以及偏差评估（跨人口统计子群）。

Result: 能够识别最需改进的变量、系统检测潜在错误，并确认数据集在实际研究中的适用性，同时支持偏差评估。

Conclusion: 该框架提供了一种严格且透明的方法来评估LLM提取的真实世界数据，推动行业标准发展并促进AI在肿瘤学研究和实践中的可信应用。

Abstract: Large language models (LLMs) are increasingly used to extract clinical data
from electronic health records (EHRs), offering significant improvements in
scalability and efficiency for real-world data (RWD) curation in oncology.
However, the adoption of LLMs introduces new challenges in ensuring the
reliability, accuracy, and fairness of extracted data, which are essential for
research, regulatory, and clinical applications. Existing quality assurance
frameworks for RWD and artificial intelligence do not fully address the unique
error modes and complexities associated with LLM-extracted data. In this paper,
we propose a comprehensive framework for evaluating the quality of clinical
data extracted by LLMs. The framework integrates variable-level performance
benchmarking against expert human abstraction, automated verification checks
for internal consistency and plausibility, and replication analyses comparing
LLM-extracted data to human-abstracted datasets or external standards. This
multidimensional approach enables the identification of variables most in need
of improvement, systematic detection of latent errors, and confirmation of
dataset fitness-for-purpose in real-world research. Additionally, the framework
supports bias assessment by stratifying metrics across demographic subgroups.
By providing a rigorous and transparent method for assessing LLM-extracted RWD,
this framework advances industry standards and supports the trustworthy use of
AI-powered evidence generation in oncology research and practice.

</details>


### [67] [Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations](https://arxiv.org/abs/2506.08240)
*Dongkyu Cho,Rumi Chunara*

Main category: cs.LG

TL;DR: 通过改进随机增强方法以减少特征扭曲和遗忘问题，从而提高泛化性能。


<details>
  <summary>Details</summary>
Motivation: 随机数据增强虽然成本低但效果有限，然而其潜在的改进空间尚未被充分挖掘。

Method: 重新审视随机增强方法，提出一种简单解决方案，通过解决随机增强中因特征碰撞导致的遗忘问题来提升泛化效果。

Result: 所提方法在多个单源域泛化基准测试中表现出强大的泛化性能。

Conclusion: 改进后的随机增强方法能够显著提升泛化能力，并且在不同单源域泛化任务中效果显著。

Abstract: Data augmentation is a promising tool for enhancing out-of-distribution
generalization, where the key is to produce diverse, challenging variations of
the source domain via costly targeted augmentations that maximize its
generalization effect. Conversely, random augmentation is inexpensive but is
deemed suboptimal due to its limited effect. In this paper, we revisit random
augmentation and explore methods to address its shortcomings. We show that the
stochastic nature of random augmentation can produce a set of colliding
augmentations that distorts the learned features, similar to catastrophic
forgetting. We propose a simple solution that improves the generalization
effect of random augmentation by addressing forgetting, which displays strong
generalization performance across various single source domain generalization
(sDG) benchmarks.

</details>


### [68] [Temporalizing Confidence: Evaluation of Chain-of-Thought Reasoning with Signal Temporal Logic](https://arxiv.org/abs/2506.08243)
*Zhenjiang Mao,Artem Bisliouk,Rohith Reddy Nama,Ivan Ruchkin*

Main category: cs.LG

TL;DR: 在数学推理任务中，大型语言模型（LLMs）虽然表现出色，但有时会产生高度自信却错误的输出，这在教育等领域存在风险。本文提出了一种结构化框架，通过信号时序逻辑（STL）对逐步置信度进行建模和评估，同时引入不确定性重塑策略以确保推理过程的一致性。实验表明，该方法提高了校准指标并提供了更可靠的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 尽管链式思维提示使大型语言模型在数学推理任务中表现出色，但其可能生成高度自信且错误的输出，这在用户缺乏专业知识来评估推理步骤的领域（如教育）中带来了显著风险。

Method: 提出了一种结构化框架，将逐步置信度作为时间信号进行建模，并使用信号时序逻辑（STL）对其进行评估。定义了基于STL的形式约束以捕获理想的时序属性，并计算稳健性得分作为结构化、可解释的置信度估计。还引入了一系列不确定性重塑策略，以在整个推理轨迹中强制执行平滑性、单调性和因果一致性。

Result: 实验结果表明，该方法一致地提高了校准指标，并提供了比传统置信度聚合和事后校准更为可靠的不确定性估计。

Conclusion: 所提出的结构化框架能够有效改进LLMs在数学推理任务中的置信度估计，提供更可靠、更可解释的不确定性评估，从而降低潜在风险。

Abstract: Large Language Models (LLMs) have shown impressive performance in
mathematical reasoning tasks when guided by Chain-of-Thought (CoT) prompting.
However, they tend to produce highly confident yet incorrect outputs, which
poses significant risks in domains like education, where users may lack the
expertise to assess reasoning steps. To address this, we propose a structured
framework that models stepwise confidence as a temporal signal and evaluates it
using Signal Temporal Logic (STL). In particular, we define formal STL-based
constraints to capture desirable temporal properties and compute robustness
scores that serve as structured, interpretable confidence estimates. Our
approach also introduces a set of uncertainty reshaping strategies to enforce
smoothness, monotonicity, and causal consistency across the reasoning
trajectory. Experiments show that our approach consistently improves
calibration metrics and provides more reliable uncertainty estimates than
conventional confidence aggregation and post-hoc calibration.

</details>


### [69] [Parameter-free approximate equivariance for tasks with finite group symmetry](https://arxiv.org/abs/2506.08244)
*Riccardo Ali,Pietro Liò,Jamie Vicary*

Main category: cs.LG

TL;DR: 提出了一种无参数的方法，通过在损失函数中添加项来在潜在表示中施加有限群的近似等变性。实验表明，网络倾向于学习规则表示。该方法在三个数据集上进行了测试，与现有方法相比，在使用较少参数的情况下实现了相似或更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的等变方法计算成本高、参数多且通常与特定架构绑定。为了简化等变神经网络的设计并降低计算复杂度，需要一种新的方法来实现近似等变性。

Method: 提出了一种零参数的方法，在潜在表示中通过在损失函数中添加额外项来施加有限群的近似等变性。允许网络学习潜在空间上的群表示，并观察其是否倾向于学习规则表示。

Result: 在三个数据集上进行测试，结果表明该方法在许多情况下以更少的参数实现了与现有方法相似或更好的性能。

Conclusion: 所提出的方法是一种简单有效的手段，可以通过损失函数中的惩罚项来施加近似等变性，从而减少参数数量并提高性能。

Abstract: Equivariant neural networks incorporate symmetries through group actions,
embedding them as an inductive bias to improve performance on a wide variety of
tasks. However, existing equivariant methods can be computationally intensive,
with high parameter counts, and are often tied to a specific architecture. We
propose a simple zero-parameter approach that imposes approximate equivariance
for a finite group in the latent representation, as an additional term in the
loss function. We conduct experiments which allow the network to learn a group
representation on the latent space, and show in every case it prefers to learn
the regular representation. Fixing this action on the latent space, this yields
a simple method to impose approximate equivariance as an additional loss
penalty. We benchmark our approach on three datasets and compare it against
several existing equivariant methods, showing that in many cases it achieves
similar or better performance for a fraction of the parameters.

</details>


### [70] [SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense](https://arxiv.org/abs/2506.08255)
*Patryk Krukowski,Łukasz Gorczyca,Piotr Helm,Kamil Książek,Przemysław Spurek*

Main category: cs.LG

TL;DR: The paper proposes SHIELD, a method that combines hypernetwork-based continual learning with interval arithmetic to address catastrophic forgetting and vulnerability to adversarial attacks simultaneously.


<details>
  <summary>Details</summary>
Motivation: Traditional deep neural networks face issues like catastrophic forgetting when adapting to new datasets and vulnerability to adversarial attacks. No existing model can solve both problems at the same time.

Method: SHIELD integrates a hypernetwork-based continual learning approach with interval arithmetic. It uses hypernetworks to transfer task embedding vectors into target model weights for specific data, enabling dynamic generation of separate networks per subtask. The target model processes data samples in an interval range, creating a hypercube to provide guarantees against attacks within this range.

Result: This approach enhances security while maintaining network adaptability, successfully addressing safety in continual learning.

Conclusion: SHIELD presents a novel solution for overcoming catastrophic forgetting and adversarial attack vulnerabilities concurrently without compromising on adaptability.

Abstract: Traditional deep neural networks suffer from several limitations, including
catastrophic forgetting. When models are adapted to new datasets, they tend to
quickly forget previously learned knowledge. Another significant issue is the
lack of robustness to even small perturbations in the input data. In practice,
we can often easily perform adversarial attacks and change the network's
predictions, adding minimal noise to the input. Dedicated architectures and
training procedures can solve each of the above problems separately.
Unfortunately, currently, no model can simultaneously address both catastrophic
forgetting and vulnerability to adversarial attacks. We introduce SHIELD
(Secure Hypernetworks for Incremental Expansion and Learning Defense), a novel
approach that integrates a hypernetwork-based continual learning approach with
interval arithmetic. SHIELD use the hypernetwork to transfer trainable task
embedding vectors into the weights of a target model dedicated to specific
data. This paradigm allows for the dynamic generation of separate networks for
each subtask, while the hypernetwork aggregates and analyzes information across
all tasks. The target model takes in the input a data sample with a defined
interval range, and by creating a hypercube, produces a prediction for the
given range. Therefore, such target models provide strict guarantees against
all possible attacks for data samples within the interval range. Our approach
enhances security without sacrificing network adaptability, addressing the
overlooked challenge of safety in continual learning.

</details>


### [71] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: The paper introduces HC-RLHF, a method ensuring safe and helpful language model alignment through high-confidence safety guarantees. It separates human preferences into helpfulness and harmlessness, optimizes under pessimistic cost constraints, verifies safety with upper-confidence bounds, and demonstrates effectiveness on three models.


<details>
  <summary>Details</summary>
Motivation: Current methods for aligning language models often compromise between safety and helpfulness, potentially leading to unacceptable responses in sensitive areas. There is a need for a method that can ensure safety while maintaining helpfulness.

Method: HC-RLHF decouples human preferences into helpfulness (reward model) and harmlessness (cost model). It optimizes the reward function under a pessimistic cost constraint and then tests the model's safety within an upper-confidence bound of the actual cost constraint.

Result: HC-RLHF successfully produces safe models with high probability when applied to Qwen2-1.5B, Qwen2.5-3B, and LLaMa3.2-3B. It improves both harmlessness and helpfulness compared to previous methods.

Conclusion: HC-RLHF provides a reliable approach to align language models with human preferences, ensuring safety without significantly sacrificing helpfulness.

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [72] [Sparse Interpretable Deep Learning with LIES Networks for Symbolic Regression](https://arxiv.org/abs/2506.08267)
*Mansooreh Montazerin,Majd Al Aawar,Antonio Ortega,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: The paper introduces LIES, a new neural network architecture for symbolic regression that generates interpretable and compact mathematical expressions. It consistently outperforms baselines in producing sparse and accurate symbolic formulae.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression (SR) seeks to discover closed-form mathematical expressions describing data with interpretability. Current SR methods face challenges in scalability and symbolic consistency.

Method: LIES is a fixed neural network architecture with interpretable primitive activations optimized for modeling symbolic expressions. A framework is developed to extract compact formulae from LIES networks using an oversampling strategy and tailored loss function. Additional pruning strategies simplify learned expressions into compact formulae.

Result: Experiments on SR benchmarks show that the LIES framework consistently produces sparse and accurate symbolic formulae surpassing all baselines. Ablation studies demonstrate the importance of each design component.

Conclusion: LIES offers a scalable and effective approach for symbolic regression, generating interpretable and compact mathematical expressions.

Abstract: Symbolic regression (SR) aims to discover closed-form mathematical
expressions that accurately describe data, offering interpretability and
analytical insight beyond standard black-box models. Existing SR methods often
rely on population-based search or autoregressive modeling, which struggle with
scalability and symbolic consistency. We introduce LIES (Logarithm, Identity,
Exponential, Sine), a fixed neural network architecture with interpretable
primitive activations that are optimized to model symbolic expressions. We
develop a framework to extract compact formulae from LIES networks by training
with an appropriate oversampling strategy and a tailored loss function to
promote sparsity and to prevent gradient instability. After training, it
applies additional pruning strategies to further simplify the learned
expressions into compact formulae. Our experiments on SR benchmarks show that
the LIES framework consistently produces sparse and accurate symbolic formulae
outperforming all baselines. We also demonstrate the importance of each design
component through ablation studies.

</details>


### [73] [SWAT-NN: Simultaneous Weights and Architecture Training for Neural Networks in a Latent Space](https://arxiv.org/abs/2506.08270)
*Zitong Huang,Mansooreh Montazerin,Ajitesh Srivastava*

Main category: cs.LG

TL;DR: 本研究提出了一种同时优化神经网络结构和权重的新方法，通过多尺度自动编码器嵌入架构与参数信息至连续潜在空间，并利用稀疏性和紧凑性惩罚来促进高效模型。实验表明该方法能有效发现具有强大性能的稀疏紧凑型神经网络。


<details>
  <summary>Details</summary>
Motivation: 目前设计神经网络依赖于手动试错或神经架构搜索（NAS）加上权重训练，前者耗时且劳动密集，后者常将架构搜索与权重优化分离。

Method: 首先训练一个通用的多尺度自动编码器，将架构和参数信息嵌入到连续潜在空间中，功能相似的神经网络在该空间中映射得更接近。然后在给定数据集上，随机初始化潜在空间中的点，并通过梯度下降更新该点以获得最优神经网络，同时优化其结构和权重。优化过程中加入稀疏性和紧凑性惩罚以促进高效模型。

Result: 在合成回归任务上的实验表明，该方法能够有效地发现稀疏和紧凑的神经网络，这些网络具有强大的性能表现。

Conclusion: 所提出的框架可以同时优化神经网络的结构和权重，生成高效的稀疏和紧凑型神经网络，在合成回归任务中表现出色。

Abstract: Designing neural networks typically relies on manual trial and error or a
neural architecture search (NAS) followed by weight training. The former is
time-consuming and labor-intensive, while the latter often discretizes
architecture search and weight optimization. In this paper, we propose a
fundamentally different approach that simultaneously optimizes both the
architecture and the weights of a neural network. Our framework first trains a
universal multi-scale autoencoder that embeds both architectural and parametric
information into a continuous latent space, where functionally similar neural
networks are mapped closer together. Given a dataset, we then randomly
initialize a point in the embedding space and update it via gradient descent to
obtain the optimal neural network, jointly optimizing its structure and
weights. The optimization process incorporates sparsity and compactness
penalties to promote efficient models. Experiments on synthetic regression
tasks demonstrate that our method effectively discovers sparse and compact
neural networks with strong performance.

</details>


### [74] [Universal Differential Equations for Scientific Machine Learning of Node-Wise Battery Dynamics in Smart Grids](https://arxiv.org/abs/2506.08272)
*Tarushri N. S.*

Main category: cs.LG

TL;DR: This paper proposes a UDE-based approach to learn node-specific battery evolution by embedding a neural residual into a physically inspired battery ODE in smart grid systems.


<details>
  <summary>Details</summary>
Motivation: Modeling node-wise battery dynamics remains a challenge due to the stochasticity of solar input and variability in household load profiles. Traditional approaches often struggle with generalization and fail to capture unmodeled residual dynamics.

Method: The work proposes a Universal Differential Equations (UDEs) based approach which blends neural networks with physical differential equations for scientific machine learning (SciML). A neural residual is embedded into a physically inspired battery ODE to model unobserved or stochastic corrections arising from heterogeneity in node demand and environmental conditions.

Result: Comprehensive experiments reveal that the trained UDE aligns closely with ground truth battery trajectories, exhibits smooth convergence behavior, and maintains stability in long-term forecasts.

Conclusion: The findings affirm the viability of UDE-based SciML approaches for battery modeling in decentralized energy networks and suggest broader implications for real-time control and optimization in renewable-integrated smart grids.

Abstract: Universal Differential Equations (UDEs), which blend neural networks with
physical differential equations, have emerged as a powerful framework for
scientific machine learning (SciML), enabling data-efficient, interpretable,
and physically consistent modeling. In the context of smart grid systems,
modeling node-wise battery dynamics remains a challenge due to the
stochasticity of solar input and variability in household load profiles.
Traditional approaches often struggle with generalization and fail to capture
unmodeled residual dynamics. This work proposes a UDE-based approach to learn
node-specific battery evolution by embedding a neural residual into a
physically inspired battery ODE. Synthetic yet realistic solar generation and
load demand data are used to simulate battery dynamics over time. The neural
component learns to model unobserved or stochastic corrections arising from
heterogeneity in node demand and environmental conditions. Comprehensive
experiments reveal that the trained UDE aligns closely with ground truth
battery trajectories, exhibits smooth convergence behavior, and maintains
stability in long-term forecasts. These findings affirm the viability of
UDE-based SciML approaches for battery modeling in decentralized energy
networks and suggest broader implications for real-time control and
optimization in renewable-integrated smart grids.

</details>


### [75] [The Impact of Feature Scaling In Machine Learning: Effects on Regression and Classification Tasks](https://arxiv.org/abs/2506.08274)
*João Manoel Herrera Pinheiro,Suzana Vilas Boas de Oliveira,Thiago Henrique Segreto Silva,Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Leonardo André Ambrosio,Marcelo Becker*

Main category: cs.LG

TL;DR: This research systematically evaluates 12 feature scaling techniques across 14 ML algorithms and 16 datasets, providing model-specific guidance on optimal scaling methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive studies on feature scaling in machine learning models and provide practitioners with crucial guidance for optimal selection of scaling techniques.

Method: Systematically evaluate 12 scaling techniques across 14 different ML algorithms and 16 datasets for classification and regression tasks. Analyze impacts on predictive performance (accuracy, MAE, MSE, $R^2$) and computational costs (training time, inference time, memory usage).

Result: Ensemble methods like Random Forest and gradient boosting models show robust performance largely independent of scaling. Other models such as Logistic Regression, SVMs, TabNet, and MLPs demonstrate significant performance variations highly dependent on the chosen scaler.

Conclusion: The study offers model-specific guidance to practitioners regarding the necessity of selecting optimal feature scaling techniques.

Abstract: This research addresses the critical lack of comprehensive studies on feature
scaling by systematically evaluating 12 scaling techniques - including several
less common transformations - across 14 different Machine Learning algorithms
and 16 datasets for classification and regression tasks. We meticulously
analyzed impacts on predictive performance (using metrics such as accuracy,
MAE, MSE, and $R^2$) and computational costs (training time, inference time,
and memory usage). Key findings reveal that while ensemble methods (such as
Random Forest and gradient boosting models like XGBoost, CatBoost and LightGBM)
demonstrate robust performance largely independent of scaling, other widely
used models such as Logistic Regression, SVMs, TabNet, and MLPs show
significant performance variations highly dependent on the chosen scaler. This
extensive empirical analysis, with all source code, experimental results, and
model parameters made publicly available to ensure complete transparency and
reproducibility, offers model-specific crucial guidance to practitioners on the
need for an optimal selection of feature scaling techniques.

</details>


### [76] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: The paper introduces ECON, a hierarchical reinforcement-learning paradigm that treats multi-LLM coordination as an incomplete-information game to find a Bayesian Nash equilibrium. It reduces computational costs and improves performance compared to other methods.


<details>
  <summary>Details</summary>
Motivation: Multi-agent frameworks for large language models have significant reasoning power but suffer from high computational costs and lack convergence guarantees.

Method: ECON recasts multi-LLM coordination as an incomplete-information game seeking a Bayesian Nash equilibrium. Each LLM independently selects responses maximizing its expected reward based on beliefs about co-agents without needing costly inter-agent exchanges.

Result: ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks and demonstrates scalability by incorporating additional models.

Conclusion: ECON is a promising approach for enhancing multi-LLM coordination with reduced computational costs and improved performance.

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [77] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: 现有的基准测试主要评估大语言模型的被动推理能力，而主动推理（需与外部系统交互以获取缺失证据或数据）却缺乏系统关注。为弥补这一不足，本文提出了AR-Bench，一个专门评估大语言模型主动推理技能的新基准。它包括侦探案例、情境谜题和猜数字三个任务家族，涵盖常识、逻辑和符号推理挑战。实验表明，当代大语言模型在主动推理方面存在显著困难，即使采用先进的策略也只能获得 modest 改善。研究强调了推进主动推理方法论的重要性，例如结合互动学习、实时反馈回路和环境感知目标进行训练。


<details>
  <summary>Details</summary>
Motivation: 尽管现有基准测试涵盖了大语言模型在不同领域的推理能力，但它们主要集中在被动推理上，即提供所有解决问题所需的信息。然而，主动推理——需要大语言模型与外部系统交互以获取缺失证据或数据——尚未得到系统性研究。因此，需要一种专门评估主动推理能力的基准来揭示模型的局限性，并推动相关技术的发展。

Method: 提出 AR-Bench，一个专门用于评估大语言模型主动推理技能的基准。该基准包含三个任务家族：侦探案例、情境谜题和猜数字，这些任务模拟现实世界场景，并衡量模型在常识、逻辑和符号推理方面的表现。通过让模型与外部系统交互以获取必要信息，评估其主动推理能力。此外，还进行了消融研究，探讨树搜索和后训练等策略对性能的影响。

Result: 实验结果表明，当代大语言模型在主动推理任务上表现不佳，常常无法获取或利用解决任务所需的必要信息。即使采用高级策略（如基于树的搜索或后训练方法），也仅能带来有限的改进，仍远未达到实际应用的要求。这表明模型在主动推理方面存在显著差距。

Conclusion: 当前大语言模型在主动推理方面的能力明显不足，与被动推理能力形成鲜明对比。为了提升主动推理能力，未来的研究应着重于改进方法论，例如引入互动学习、实时反馈机制和环境感知目标。AR-Bench 的发布为这一领域提供了重要的评估工具，公开地址为 https://github.com/tmlr-group/AR-Bench。

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [78] [H$^2$GFM: Towards unifying Homogeneity and Heterogeneity on Text-Attributed Graphs](https://arxiv.org/abs/2506.08298)
*Trung-Kien Nguyen,Heng Ping,Shixuan Li,Peiyu Zhang,Nikos Kanakaris,Nicholas Kotov,Paul Bogdan*

Main category: cs.LG

TL;DR: H$^2$GFM is a novel framework that generalizes across both homogeneous and heterogeneous text-attributed graphs by employing context encoding, context-adaptive graph transformer (CGT), and a mixture of CGT experts.


<details>
  <summary>Details</summary>
Motivation: Existing research on graph foundation model (GFM) mainly focuses on homogeneous text-attributed graphs (HoTAGs), while the potential of handling heterogeneous TAGs (HeTAGs) remains underexplored.

Method: The H$^2$GFM framework projects diverse meta-relations among graphs into a unified textual space and uses context encoding to capture spatial and higher-order semantic relationships. A context-adaptive graph transformer (CGT) is proposed to capture robust node representations from context neighbors and their relationships. Additionally, a mixture of CGT experts is employed to capture structural pattern heterogeneity among graph types.

Result: Comprehensive experiments on various HoTAGs and HeTAGs as well as learning scenarios show the effectiveness of the H$^2$GFM model.

Conclusion: H$^2$GFM successfully generalizes across both homogeneous and heterogeneous text-attributed graphs, providing enhanced capabilities and applications for graph foundation models.

Abstract: The growing interests and applications of graph learning in diverse domains
have propelled the development of a unified model generalizing well across
different graphs and tasks, known as the Graph Foundation Model (GFM). Existing
research has leveraged text-attributed graphs (TAGs) to tackle the
heterogeneity in node features among graphs. However, they primarily focus on
homogeneous TAGs (HoTAGs), leaving heterogeneous TAGs (HeTAGs), where multiple
types of nodes/edges reside, underexplored. To enhance the capabilities and
applications of GFM, we introduce H$^2$GFM, a novel framework designed to
generalize across both HoTAGs and HeTAGs. Our model projects diverse
meta-relations among graphs under a unified textual space, and employs a
context encoding to capture spatial and higher-order semantic relationships. To
achieve robust node representations, we propose a novel context-adaptive graph
transformer (CGT), effectively capturing information from both context
neighbors and their relationships. Furthermore, we employ a mixture of CGT
experts to capture the heterogeneity in structural patterns among graph types.
Comprehensive experiments on a wide range of HoTAGs and HeTAGs as well as
learning scenarios demonstrate the effectiveness of our model.

</details>


### [79] [Learnable Spatial-Temporal Positional Encoding for Link Prediction](https://arxiv.org/abs/2506.08309)
*Katherine Tieu,Dongqi Fu,Zihao Li,Ross Maciejewski,Jingrui He*

Main category: cs.LG

TL;DR: 提出了一种可学习的空间-时间位置编码方法（L-STEP），解决了现有位置编码在复杂图数据上的不足，同时证明了该方法的有效性和高效性。


<details>
  <summary>Details</summary>
Motivation: 当前的位置编码方法存在三个主要问题：(1) 大多数方法使用预定义和固定的函数，难以适应复杂的属性图；(2) 少数可学习的位置编码仅限于结构信息，未考虑时变的拓扑和特征信息；(3) 在大规模结构化数据上，密集或关系注意力机制通常不可行。因此，需要一种有效且高效的位置编码方法来解决这些问题。

Method: 开发了Learnable Spatial-Temporal Positional Encoding (L-STEP)，通过以下方式实现：(1) 从空间-时间谱角度证明所提出的编码方案可以保留图的性质；(2) 验证MLP能够充分利用表达能力，并达到与Transformer相当的性能；(3) 测试不同初始位置编码输入以展示鲁棒性；(4) 分析理论复杂度并获得比SOTA更少的经验运行时间；(5) 在13个经典数据集、10种算法以及不同的采样策略下验证其时序链接预测性能。

Result: L-STEP在13个经典数据集上表现出优于10种算法的时序链接预测性能，在转导和归纳设置中均表现良好。此外，在最新的大规模TGB基准测试中，L-STEP也取得了领先性能。

Conclusion: L-STEP是一种简单而有效的模型，能够在大规模结构化数据上进行高效的时序链接预测，同时保留图的性质并具有较低的运行时间复杂度。

Abstract: Accurate predictions rely on the expressiveness power of graph deep learning
frameworks like graph neural networks and graph transformers, where a
positional encoding mechanism has become much more indispensable in recent
state-of-the-art works to record the canonical position information. However,
the current positional encoding is limited in three aspects: (1) most
positional encoding methods use pre-defined, and fixed functions, which are
inadequate to adapt to the complex attributed graphs; (2) a few pioneering
works proposed the learnable positional encoding but are still limited to the
structural information, not considering the real-world time-evolving
topological and feature information; (3) most positional encoding methods are
equipped with transformers' attention mechanism to fully leverage their
capabilities, where the dense or relational attention is often unaffordable on
large-scale structured data. Hence, we aim to develop Learnable
Spatial-Temporal Positional Encoding in an effective and efficient manner and
propose a simple temporal link prediction model named L-STEP. Briefly, for
L-STEP, we (1) prove the proposed positional learning scheme can preserve the
graph property from the spatial-temporal spectral viewpoint, (2) verify that
MLPs can fully exploit the expressiveness and reach transformers' performance
on that encoding, (3) change different initial positional encoding inputs to
show robustness, (4) analyze the theoretical complexity and obtain less
empirical running time than SOTA, and (5) demonstrate its temporal link
prediction out-performance on 13 classic datasets and with 10 algorithms in
both transductive and inductive settings using 3 different sampling strategies.
Also, \name\ obtains the leading performance in the newest large-scale TGB
benchmark. Our code is available at https://github.com/kthrn22/L-STEP.

</details>


### [80] [Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion](https://arxiv.org/abs/2506.08316)
*Alan N. Amin,Nate Gruver,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: 在离散扩散模型中，masking diffusion表现优异的原因在于它利用了离散马尔可夫过程的本质特性。基于此，研究提出了通用的SCUD模型，超越了传统的masking diffusion，在图像、文本和蛋白质数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管逐步生成理论上具有许多概念优势，但在离散扩散模型中，表现最佳的是并不逐步去噪的masking diffusion。需要解释其优越性能并探索改进方法。

Method: 通过分析masking diffusion利用离散马尔可夫过程的间断跳跃特性，提出将已知跳跃时间分布融入任意离散扩散模型的方法，形成通用的SCUD模型。

Result: SCUD模型不仅概括了经典的离散扩散和masking diffusion，还在结合图像、文本和蛋白质数据的归纳偏差的去噪过程中表现出优于masking diffusion的性能。

Conclusion: SCUD模型充分利用了离散马尔可夫过程的特性，并在多种数据类型上展现了更优的性能，为离散扩散模型的研究提供了新的方向。

Abstract: Discrete diffusion models, like continuous diffusion models, generate
high-quality samples by gradually undoing noise applied to datapoints with a
Markov process. Gradual generation in theory comes with many conceptual
benefits; for example, inductive biases can be incorporated into the noising
Markov process, and access to improved sampling algorithms. In practice,
however, the consistently best performing discrete diffusion model is,
surprisingly, masking diffusion, which does not denoise gradually. Here we
explain the superior performance of masking diffusion by noting that it makes
use of a fundamental difference between continuous and discrete Markov
processes: discrete Markov processes evolve by discontinuous jumps at a fixed
rate and, unlike other discrete diffusion models, masking diffusion builds in
the known distribution of jump times and only learns where to jump to. We show
that we can similarly bake in the known distribution of jump times into any
discrete diffusion model. The resulting models - schedule-conditioned discrete
diffusion (SCUD) - generalize classical discrete diffusion and masking
diffusion. By applying SCUD to models with noising processes that incorporate
inductive biases on images, text, and protein data, we build models that
outperform masking.

</details>


### [81] [Graph Prompting for Graph Learning Models: Recent Advances and Future Directions](https://arxiv.org/abs/2506.08326)
*Xingbo Fu,Zehong Wang,Zihan Chen,Jiazheng Li,Yaochen Zhu,Zhenyu Lei,Cong Shen,Yanfang Ye,Chuxu Zhang,Jundong Li*

Main category: cs.LG

TL;DR: This paper systematically reviews recent advancements in graph prompting, introduces representative graph pre-training methods, reviews mainstream techniques in graph prompting, summarizes real-world applications from different domains, and discusses open challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: Graph prompting has emerged as a promising approach for adapting pre-trained graph learning models to specific downstream tasks without changing the original model.

Method: The paper first introduces representative graph pre-training methods. Then it reviews mainstream techniques in graph prompting and how they design learnable prompts. It also summarizes real-world applications of graph prompting from different domains.

Result: A systematic review of graph prompting is provided, including its foundation, techniques, applications, and challenges.

Conclusion: The authors discuss several open challenges in existing studies and provide promising future directions in the field of graph prompting.

Abstract: Graph learning models have demonstrated great prowess in learning expressive
representations from large-scale graph data in a wide variety of real-world
scenarios. As a prevalent strategy for training powerful graph learning models,
the "pre-training, adaptation" scheme first pre-trains graph learning models on
unlabeled graph data in a self-supervised manner and then adapts them to
specific downstream tasks. During the adaptation phase, graph prompting emerges
as a promising approach that learns trainable prompts while keeping the
pre-trained graph learning models unchanged. In this paper, we present a
systematic review of recent advancements in graph prompting. First, we
introduce representative graph pre-training methods that serve as the
foundation step of graph prompting. Next, we review mainstream techniques in
graph prompting and elaborate on how they design learnable prompts for graph
prompting. Furthermore, we summarize the real-world applications of graph
prompting from different domains. Finally, we discuss several open challenges
in existing studies with promising future directions in this field.

</details>


### [82] [A Simple Analysis of Discretization Error in Diffusion Models](https://arxiv.org/abs/2506.08337)
*Juhyeok Choi,Chenglin Fan*

Main category: cs.LG

TL;DR: Diffusion models based on stochastic differential equations (SDEs) are powerful in generative tasks, but their error analyses can be complex. This paper simplifies the analysis of discretization errors for variance-preserving SDEs in DDPMs, achieving a convergence rate of O(1/T^1/2). It also shows that Gaussian noise can be replaced with discrete random variables without affecting performance, thus enhancing sampling efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to simplify the theoretical framework for analyzing the Euler--Maruyama discretization error in variance-preserving SDEs used in DDPMs, making it more accessible and less reliant on advanced probabilistic tools. Additionally, exploring the potential substitution of Gaussian noise with discrete random variables could lead to more efficient sampling methods.

Method: The authors employ Grönwall's inequality under Lipschitz assumptions to derive the convergence rate of O(1/T^1/2) for the Euler--Maruyama discretization of VP-SDEs in DDPMs. They also investigate the effects of replacing Gaussian noise with discrete random variables such as Rademacher or uniform noise, validating their findings through experiments.

Result: The results confirm the predicted scaling of the error with respect to the number of denoising steps T. Moreover, the use of discrete noise achieves similar sample quality compared to Gaussian noise, while incorrect noise scaling leads to performance degradation. These findings support both the theoretical analysis and practical implications of the study.

Conclusion: This research provides a simplified theoretical analysis for the discretization of VP-SDEs in diffusion models, contributing to a better understanding of their convergence properties. The ability to substitute Gaussian noise with discrete noise opens up possibilities for enhancing the efficiency of sampling processes in generative modeling.

Abstract: Diffusion models, formulated as discretizations of stochastic differential
equations (SDEs), achieve state-of-the-art generative performance. However,
existing analyses of their discretization error often rely on complex
probabilistic tools. In this work, we present a simplified theoretical
framework for analyzing the Euler--Maruyama discretization of
variance-preserving SDEs (VP-SDEs) in Denoising Diffusion Probabilistic Models
(DDPMs), where $ T $ denotes the number of denoising steps in the diffusion
process. Our approach leverages Gr\"onwall's inequality to derive a convergence
rate of $ \mathcal{O}(1/T^{1/2}) $ under Lipschitz assumptions, significantly
streamlining prior proofs. Furthermore, we demonstrate that the Gaussian noise
in the discretization can be replaced by a discrete random variable (e.g.,
Rademacher or uniform noise) without sacrificing convergence guarantees-an
insight with practical implications for efficient sampling. Experiments
validate our theory, showing that (1) the error scales as predicted, (2)
discrete noise achieves comparable sample quality to Gaussian noise, and (3)
incorrect noise scaling degrades performance. By unifying simplified analysis
and discrete noise substitution, our work bridges theoretical rigor with
practical efficiency in diffusion-based generative modeling.

</details>


### [83] [Dynamical System Optimization](https://arxiv.org/abs/2506.08340)
*Emo Todorov*

Main category: cs.LG

TL;DR: An optimization framework is developed to optimize policy parameters as an autonomous dynamical system, simplifying algorithms and unifying various tasks like behavioral cloning and generative AI model tuning.


<details>
  <summary>Details</summary>
Motivation: To create a simpler optimization framework for policy parameters without relying on traditional methods of Dynamic Programming and Reinforcement Learning.

Method: Developing an optimization framework centered around the concept of transferring control authority to a specified (parametric) policy, resulting in an autonomous dynamical system.

Result: The derived algorithms compute the same quantities as policy gradients and other related methods while providing analogs to approximate policy iteration and off-policy learning.

Conclusion: This new framework not only simplifies algorithms but also unifies various tasks such as behavioral cloning, mechanism design, system identification, and tuning of generative AI models.

Abstract: We develop an optimization framework centered around a core idea: once a
(parametric) policy is specified, control authority is transferred to the
policy, resulting in an autonomous dynamical system. Thus we should be able to
optimize policy parameters without further reference to controls or actions,
and without directly using the machinery of approximate Dynamic Programming and
Reinforcement Learning. Here we derive simpler algorithms at the autonomous
system level, and show that they compute the same quantities as policy
gradients and Hessians, natural gradients, proximal methods. Analogs to
approximate policy iteration and off-policy learning are also available. Since
policy parameters and other system parameters are treated uniformly, the same
algorithms apply to behavioral cloning, mechanism design, system
identification, learning of state estimators. Tuning of generative AI models is
not only possible, but is conceptually closer to the present framework than to
Reinforcement Learning.

</details>


### [84] [An Adaptive Method Stabilizing Activations for Enhanced Generalization](https://arxiv.org/abs/2506.08353)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: AdaAct is an optimization algorithm that adjusts learning rates based on activation variance, enhancing neuron output stability and generalization. It performs competitively on image classification benchmarks like CIFAR and ImageNet, bridging the gap between Adam's convergence speed and SGD's generalization while maintaining efficient execution times.


<details>
  <summary>Details</summary>
Motivation: To improve the stability of neuron outputs during training and achieve better generalization by introducing a method that adjusts learning rates according to activation variance, thus complementing conventional activation regularization techniques.

Method: AdaAct incorporates neuron-wise adaptivity by adjusting learning rates based on activation variance, which enhances the stability of neuron outputs during the training process.

Result: AdaAct demonstrates competitive performance in standard image classification benchmarks such as CIFAR and ImageNet. It bridges the gap between Adam's fast convergence and SGD's strong generalization capabilities while keeping competitive execution times.

Conclusion: AdaAct presents a promising approach for improving generalization and stability in neural network training, offering a balance between convergence speed and generalization capabilities.

Abstract: We introduce AdaAct, a novel optimization algorithm that adjusts learning
rates according to activation variance. Our method enhances the stability of
neuron outputs by incorporating neuron-wise adaptivity during the training
process, which subsequently leads to better generalization -- a complementary
approach to conventional activation regularization methods. Experimental
results demonstrate AdaAct's competitive performance across standard image
classification benchmarks. We evaluate AdaAct on CIFAR and ImageNet, comparing
it with other state-of-the-art methods. Importantly, AdaAct effectively bridges
the gap between the convergence speed of Adam and the strong generalization
capabilities of SGD, all while maintaining competitive execution times. Code is
available at https://github.com/hseung88/adaact.

</details>


### [85] [NysAct: A Scalable Preconditioned Gradient Descent using Nystrom Approximation](https://arxiv.org/abs/2506.08360)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: The paper introduces NysAct, a scalable first-order gradient preconditioning method that balances computational efficiency and generalization. It uses an eigenvalue-shifted Nystrom method to approximate the activation covariance matrix, reducing time and memory complexities while maintaining test accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between computational efficiency and generalization in adaptive gradient methods and second-order methods, the authors aim to develop a scalable optimization method that combines the benefits of both approaches.

Method: NysAct leverages an eigenvalue-shifted Nystrom method to approximate the activation covariance matrix for use as a preconditioning matrix. This approach aims to reduce time and memory complexities compared to existing second-order methods.

Result: Experiments demonstrate that NysAct achieves improved test accuracy compared to both first-order and second-order methods, while demanding significantly less computational resources than traditional second-order methods.

Conclusion: NysAct presents a novel approach to optimization in deep learning that effectively balances computational efficiency and generalization, outperforming existing methods in terms of both accuracy and resource usage.

Abstract: Adaptive gradient methods are computationally efficient and converge quickly,
but they often suffer from poor generalization. In contrast, second-order
methods enhance convergence and generalization but typically incur high
computational and memory costs. In this work, we introduce NysAct, a scalable
first-order gradient preconditioning method that strikes a balance between
state-of-the-art first-order and second-order optimization methods. NysAct
leverages an eigenvalue-shifted Nystrom method to approximate the activation
covariance matrix, which is used as a preconditioning matrix, significantly
reducing time and memory complexities with minimal impact on test accuracy. Our
experiments show that NysAct not only achieves improved test accuracy compared
to both first-order and second-order methods but also demands considerably less
computational resources than existing second-order methods. Code is available
at https://github.com/hseung88/nysact.

</details>


### [86] [AlphaFold Database Debiasing for Robust Inverse Folding](https://arxiv.org/abs/2506.08365)
*Cheng Tan,Zhenxiao Cao,Zhangyang Gao,Siyuan Li,Yufei Huang,Stan Z. Li*

Main category: cs.LG

TL;DR: DeSAE is introduced to debias AFDB structures for better inverse folding performance by reconstructing native-like conformations from corrupted geometries.


<details>
  <summary>Details</summary>
Motivation: The motivation is the critical limitation of using AFDB directly in training deep models due to its systematic geometric bias that deviates from the conformational diversity in PDB structures.

Method: Debiasing Structure AutoEncoder (DeSAE) learns to reconstruct native-like conformations from intentionally corrupted backbone geometries, capturing a more robust and natural structural manifold.

Result: Applying DeSAE to AFDB structures significantly improves inverse folding performance across multiple benchmarks.

Conclusion: This work highlights the impact of systematic biases in predicted structures and presents a framework for debiasing to boost structure-based learning tasks.

Abstract: The AlphaFold Protein Structure Database (AFDB) offers unparalleled
structural coverage at near-experimental accuracy, positioning it as a valuable
resource for data-driven protein design. However, its direct use in training
deep models that are sensitive to fine-grained atomic geometry, such as inverse
folding, exposes a critical limitation. Comparative analysis of structural
feature distributions reveals that AFDB structures exhibit distinct statistical
regularities, reflecting a systematic geometric bias that deviates from the
conformational diversity found in experimentally determined structures from the
Protein Data Bank (PDB). While AFDB structures are cleaner and more idealized,
PDB structures capture the intrinsic variability and physical realism essential
for generalization in downstream tasks. To address this discrepancy, we
introduce a Debiasing Structure AutoEncoder (DeSAE) that learns to reconstruct
native-like conformations from intentionally corrupted backbone geometries. By
training the model to recover plausible structural states, DeSAE implicitly
captures a more robust and natural structural manifold. At inference, applying
DeSAE to AFDB structures produces debiased structures that significantly
improve inverse folding performance across multiple benchmarks. This work
highlights the critical impact of subtle systematic biases in predicted
structures and presents a principled framework for debiasing, significantly
boosting the performance of structure-based learning tasks like inverse
folding.

</details>


### [87] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: This paper proposes DPSDP, a reinforcement learning algorithm that leverages the verify-and-improve paradigm for large language models (LLMs) by modeling a multi-turn refinement process as a Markov Decision Process. It trains an actor-critic LLM system via direct preference learning on self-generated data and demonstrates improvements in both in-distribution and out-of-distribution benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing approaches which suffer from restricted feedback spaces and lack of coordinated training of different parties when leveraging more test-time computation to boost reasoning capabilities of LLMs.

Method: The method involves modeling the multi-turn refinement process as a Markov Decision Process and introducing DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement learning algorithm that trains an actor-critic LLM system to iteratively refine answers through direct preference learning on self-generated data.

Result: Empirical results show improvements on both in- and out-of-distribution benchmarks. For instance, majority voting over five refinement steps increases first-turn accuracy from 58.2% to 63.2% on the MATH 500 benchmark using Ministral-based models. An ablation study confirms the benefits of multi-agent collaboration and out-of-distribution generalization.

Conclusion: DPSDP can match the performance of any policy within the training distribution and shows significant improvements on various benchmarks, demonstrating its effectiveness in enhancing the reasoning capabilities of LLMs.

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [88] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: This paper introduces a new framework that trains Reinforcement-Learned Teachers (RLTs) to provide detailed explanations for students, avoiding RL's exploration challenge and improving distillation performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of training reasoning language models with reinforcement learning, particularly the reliance on initial exploration ability and the need for effective distillation for future use.

Method: RLTs are trained using dense rewards obtained by feeding each explanation to the student and testing its understanding. They are prompted with both the question and solution to connect the dots with tailored explanations.

Result: The 7B RLTs outperform existing pipelines in final performance on competition and graduate-level tasks. They remain effective when training larger students and when applied zero-shot to out-of-distribution tasks.

Conclusion: This new framework unlocks higher efficiency and re-usability for the RL reasoning framework by focusing on effective downstream distillation.

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [89] [Spatiotemporal deep learning models for detection of rapid intensification in cyclones](https://arxiv.org/abs/2506.08397)
*Vamshika Sutar,Amandeep Singh,Rohitash Chandra*

Main category: cs.LG

TL;DR: Cyclone rapid intensification is an extreme event that is relatively rare. This paper evaluates deep learning, ensemble learning and data augmentation frameworks to detect cyclone rapid intensification based on wind intensity and spatial coordinates.


<details>
  <summary>Details</summary>
Motivation: Cyclone rapid intensification is an extreme event that is relatively rare and influenced by a diverse array of factors, making it challenging for conventional machine learning models.

Method: The paper uses deep learning, ensemble learning and data augmentation frameworks to detect cyclone rapid intensification. It employs deep learning models to generate spatial coordinates and wind intensity that replicate cyclones to address the class imbalance problem and also use a deep learning model for the classification module within the data augmentation framework.

Result: Data augmentation improves the results for rapid intensification detection in cyclones, and spatial coordinates play a critical role as input features to the given models.

Conclusion: This research paves the way for research in synthetic data generation for spatiotemporal data with extreme events.

Abstract: Cyclone rapid intensification is the rapid increase in cyclone wind
intensity, exceeding a threshold of 30 knots, within 24 hours. Rapid
intensification is considered an extreme event during a cyclone, and its
occurrence is relatively rare, contributing to a class imbalance in the
dataset. A diverse array of factors influences the likelihood of a cyclone
undergoing rapid intensification, further complicating the task for
conventional machine learning models. In this paper, we evaluate deep learning,
ensemble learning and data augmentation frameworks to detect cyclone rapid
intensification based on wind intensity and spatial coordinates. We note that
conventional data augmentation methods cannot be utilised for generating
spatiotemporal patterns replicating cyclones that undergo rapid
intensification. Therefore, our framework employs deep learning models to
generate spatial coordinates and wind intensity that replicate cyclones to
address the class imbalance problem of rapid intensification. We also use a
deep learning model for the classification module within the data augmentation
framework to differentiate between rapid and non-rapid intensification events
during a cyclone. Our results show that data augmentation improves the results
for rapid intensification detection in cyclones, and spatial coordinates play a
critical role as input features to the given models. This paves the way for
research in synthetic data generation for spatiotemporal data with extreme
events.

</details>


### [90] [FUSE: Measure-Theoretic Compact Fuzzy Set Representation for Taxonomy Expansion](https://arxiv.org/abs/2506.08409)
*Fred Xu,Song Jiang,Zijie Huang,Xiao Luo,Shichang Zhang,Adrian Chen,Yizhou Sun*

Main category: cs.LG

TL;DR: This paper proposes Fuzzy Set Embedding (FUSE), a new method for taxonomy expansion based on fuzzy set representation learning, achieving significant improvements in performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for modeling sets do not fully support set operations and lack the ability to incorporate uncertainty within semantic concepts.

Method: The paper introduces FUSE, which uses volume approximation of fuzzy sets as embeddings. This framework satisfies all set operations, preserves information, and is efficient to learn with minimal neural architecture.

Result: FUSE shows substantial improvements in taxonomy expansion tasks, reaching up to 23% better performance compared to existing baselines.

Conclusion: FUSE represents an effective approach for set representation learning and marks the first effort in understanding and computing fuzzy set embeddings.

Abstract: Taxonomy Expansion, which models complex concepts and their relations, can be
formulated as a set representation learning task. The generalization of set,
fuzzy set, incorporates uncertainty and measures the information within a
semantic concept, making it suitable for concept modeling. Existing works
usually model sets as vectors or geometric objects such as boxes, which are not
closed under set operations. In this work, we propose a sound and efficient
formulation of set representation learning based on its volume approximation as
a fuzzy set. The resulting embedding framework, Fuzzy Set Embedding (FUSE),
satisfies all set operations and compactly approximates the underlying fuzzy
set, hence preserving information while being efficient to learn, relying on
minimum neural architecture. We empirically demonstrate the power of FUSE on
the task of taxonomy expansion, where FUSE achieves remarkable improvements up
to 23% compared with existing baselines. Our work marks the first attempt to
understand and efficiently compute the embeddings of fuzzy sets.

</details>


### [91] [Learning to Hear Broken Motors: Signature-Guided Data Augmentation for Induction-Motor Diagnostics](https://arxiv.org/abs/2506.08412)
*Saraa Ali,Aleksandr Khizhik,Stepan Svirin,Artem Ryzhikov,Denis Derkach*

Main category: cs.LG

TL;DR: The paper proposes Signature-Guided Data Augmentation (SGDA), an unsupervised framework that synthesizes faults in the frequency domain of healthy current signals to enhance diagnostic accuracy and reliability in three-phase engine diagnostics.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for diagnosing three-phase engines largely depend on signature analysis, which is effective but can be improved by incorporating advanced machine learning techniques.

Method: The authors developed SGDA, a novel unsupervised data augmentation methodology that uses the engine physics model to generate physically plausible faults directly in the frequency domain of healthy current signals. This approach is guided by Motor Current Signature Analysis.

Result: The hybrid approach combining supervised ML and unsupervised signature analysis achieved superior diagnostic accuracy and reliability, showing potential for wide industrial application.

Conclusion: The study concludes that the proposed SGDA method offers a robust and efficient solution for real-world applications in engine diagnostics, significantly contributing to the field.

Abstract: The application of machine learning (ML) algorithms in the intelligent
diagnosis of three-phase engines has the potential to significantly enhance
diagnostic performance and accuracy. Traditional methods largely rely on
signature analysis, which, despite being a standard practice, can benefit from
the integration of advanced ML techniques. In our study, we innovate by
combining ML algorithms with a novel unsupervised anomaly generation
methodology that takes into account the engine physics model. We propose
Signature-Guided Data Augmentation (SGDA), an unsupervised framework that
synthesizes physically plausible faults directly in the frequency domain of
healthy current signals. Guided by Motor Current Signature Analysis, SGDA
creates diverse and realistic anomalies without resorting to computationally
intensive simulations. This hybrid approach leverages the strengths of both
supervised ML and unsupervised signature analysis, achieving superior
diagnostic accuracy and reliability along with wide industrial application. The
findings highlight the potential of our approach to contribute significantly to
the field of engine diagnostics, offering a robust and efficient solution for
real-world applications.

</details>


### [92] [Improved Scaling Laws in Linear Regression via Data Reuse](https://arxiv.org/abs/2506.08415)
*Licong Lin,Jingfeng Wu,Peter L. Bartlett*

Main category: cs.LG

TL;DR: 通过数据重用，多遍随机梯度下降（multi-pass SGD）可以改善线性回归中的神经缩放定律。在数据受限的情况下，相比单遍SGD，多遍SGD能实现更优的测试误差。


<details>
  <summary>Details</summary>
Motivation: 神经缩放定律表明，当模型和数据规模增大时，在线训练的大语言模型的测试误差会多项式下降。然而，当新数据耗尽时，这种缩放可能不可持续。因此，研究探索了数据重用对缩放定律的影响。

Method: 研究推导了在N个数据上使用多遍随机梯度下降（multi-pass SGD）训练M维线性模型的精确测试误差界限。假设数据协方差具有a阶幂律谱，真实参数遵循b-a阶对齐幂律谱（其中a>b>1），则多遍SGD的测试误差为Θ(M^(1-b) + L^((1-b)/a))，其中L与N^(a/b)成比例，是迭代次数。相比之下，单遍SGD只能达到Θ(M^(1-b) + N^((1-b)/a))的测试误差。

Result: 研究表明，通过选择L>N（即进行数据重用），在数据受限情况下，可以实现改进的缩放定律。数值模拟验证了理论发现。

Conclusion: 数据重用能够有效改善线性回归中现有的神经缩放定律，尤其是在数据受限的情况下。这为优化模型训练策略提供了新的思路。

Abstract: Neural scaling laws suggest that the test error of large language models
trained online decreases polynomially as the model size and data size increase.
However, such scaling can be unsustainable when running out of new data. In
this work, we show that data reuse can improve existing scaling laws in linear
regression. Specifically, we derive sharp test error bounds on $M$-dimensional
linear models trained by multi-pass stochastic gradient descent (multi-pass
SGD) on $N$ data with sketched features. Assuming that the data covariance has
a power-law spectrum of degree $a$, and that the true parameter follows a prior
with an aligned power-law spectrum of degree $b-a$ (with $a > b > 1$), we show
that multi-pass SGD achieves a test error of $\Theta(M^{1-b} + L^{(1-b)/a})$,
where $L \lesssim N^{a/b}$ is the number of iterations. In the same setting,
one-pass SGD only attains a test error of $\Theta(M^{1-b} + N^{(1-b)/a})$ (see
e.g., Lin et al., 2024). This suggests an improved scaling law via data reuse
(i.e., choosing $L>N$) in data-constrained regimes. Numerical simulations are
also provided to verify our theoretical findings.

</details>


### [93] [Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood](https://arxiv.org/abs/2506.08417)
*Qingmao Yao,Zhichao Lei,Tianyuan Chen,Ziyue Yuan,Xuefan Chen,Jianxiang Liu,Faguo Wu,Xiao Zhang*

Main category: cs.LG

TL;DR: This paper proposes Smooth Bellman Operator (SBO) and Smooth Q-function OOD Generalization (SQOG) to enhance Q-function generalization in offline Reinforcement Learning, leading to better Q-value estimation and policy improvement.


<details>
  <summary>Details</summary>
Motivation: Offline RL faces challenges with distributional shifts causing Q-value overestimation for out-of-distribution actions. Current methods addressing this issue become overly conservative when evaluating OOD regions, which restricts the Q-function generalization.

Method: The authors introduce SBO which updates OOD Q-values by smoothing them with neighboring in-sample Q-values under Convex Hull and its Neighborhood guarantees. They also present SQOG as a practical algorithm that alleviates the over-constraint issue.

Result: SQOG achieves near-accurate Q-value estimation and outperforms existing state-of-the-art methods on D4RL benchmarks in both performance and computational efficiency.

Conclusion: The proposed SBO and SQOG improve Q-function generalization in OOD regions, resulting in better Q-value estimation and policy enhancement.

Abstract: Offline Reinforcement Learning (RL) struggles with distributional shifts,
leading to the $Q$-value overestimation for out-of-distribution (OOD) actions.
Existing methods address this issue by imposing constraints; however, they
often become overly conservative when evaluating OOD regions, which constrains
the $Q$-function generalization. This over-constraint issue results in poor
$Q$-value estimation and hinders policy improvement. In this paper, we
introduce a novel approach to achieve better $Q$-value estimation by enhancing
$Q$-function generalization in OOD regions within Convex Hull and its
Neighborhood (CHN). Under the safety generalization guarantees of the CHN, we
propose the Smooth Bellman Operator (SBO), which updates OOD $Q$-values by
smoothing them with neighboring in-sample $Q$-values. We theoretically show
that SBO approximates true $Q$-values for both in-sample and OOD actions within
the CHN. Our practical algorithm, Smooth Q-function OOD Generalization (SQOG),
empirically alleviates the over-constraint issue, achieving near-accurate
$Q$-value estimation. On the D4RL benchmarks, SQOG outperforms existing
state-of-the-art methods in both performance and computational efficiency.

</details>


### [94] [Online Learning-guided Learning Rate Adaptation via Gradient Alignment](https://arxiv.org/abs/2506.08419)
*Ruichen Jiang,Ali Kavis,Aryan Mokhtari*

Main category: cs.LG

TL;DR: This paper introduces GALA, a framework that dynamically adjusts learning rates for optimizers in deep learning models by tracking gradient alignment and local curvature. It formulates learning rate selection as an online learning problem and demonstrates robust performance with SGD and Adam optimizers across various initial learning rates without tuning.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of fine-tuning learning rates for optimizers in large-scale deep learning models, which often requires extensive grid search over multiple hyperparameters.

Method: The method proposed is called GALA (Gradient Alignment-based Learning rate Adaptation), which dynamically adjusts the learning rate by tracking the alignment between consecutive gradients and using a local curvature estimate. It formulates the problem of selecting the learning rate as a one-dimensional online learning problem paired with algorithms such as Follow-the-Regularized-Leader.

Result: GALA establishes a data-adaptive convergence rate for normalized SGD in smooth, nonconvex settings. Empirical results show that optimizers like SGD and Adam augmented with GALA demonstrate robust performance across a wide range of initial learning rates without the need for tuning.

Conclusion: GALA provides a principled framework to adaptively adjust learning rates, reducing the need for extensive hyperparameter tuning and demonstrating competitive performance with common optimizers.

Abstract: The performance of an optimizer on large-scale deep learning models depends
critically on fine-tuning the learning rate, often requiring an extensive grid
search over base learning rates, schedules, and other hyperparameters. In this
paper, we propose a principled framework called GALA (Gradient Alignment-based
Learning rate Adaptation), which dynamically adjusts the learning rate by
tracking the alignment between consecutive gradients and using a local
curvature estimate. Guided by the convergence analysis, we formulate the
problem of selecting the learning rate as a one-dimensional online learning
problem. When paired with an online learning algorithm such as
Follow-the-Regularized-Leader, our method produces a flexible, adaptive
learning rate schedule that tends to increase when consecutive gradients are
aligned and decrease otherwise. We establish a data-adaptive convergence rate
for normalized SGD equipped with GALA in the smooth, nonconvex setting.
Empirically, common optimizers such as SGD and Adam, when augmented with GALA,
demonstrate robust performance across a wide range of initial learning rates
and perform competitively without the need for tuning.

</details>


### [95] [HASFL: Heterogeneity-aware Split Federated Learning over Edge Computing Systems](https://arxiv.org/abs/2506.08426)
*Zheng Lin,Zhe Chen,Xianhao Chen,Wei Ni,Yue Gao*

Main category: cs.LG

TL;DR: HASFL is a heterogeneity-aware split federated learning framework that adaptively controls batch sizes and model splitting to balance communication-computing latency and training convergence in heterogeneous edge networks.


<details>
  <summary>Details</summary>
Motivation: Existing split federated learning approaches suffer from the straggler effect due to the heterogeneous capabilities of edge devices.

Method: Propose HASFL which derives a tight convergence bound of SFL and then uses this bound to adaptively control batch sizes and model splitting for edge devices.

Result: Extensive experiments with various datasets validate the effectiveness of HASFL and demonstrate its superiority over state-of-the-art benchmarks.

Conclusion: HASFL can effectively address the resource heterogeneity challenge in split federated learning on edge devices.

Abstract: Split federated learning (SFL) has emerged as a promising paradigm to
democratize machine learning (ML) on edge devices by enabling layer-wise model
partitioning. However, existing SFL approaches suffer significantly from the
straggler effect due to the heterogeneous capabilities of edge devices. To
address the fundamental challenge, we propose adaptively controlling batch
sizes (BSs) and model splitting (MS) for edge devices to overcome resource
heterogeneity. We first derive a tight convergence bound of SFL that quantifies
the impact of varied BSs and MS on learning performance. Based on the
convergence bound, we propose HASFL, a heterogeneity-aware SFL framework
capable of adaptively controlling BS and MS to balance communication-computing
latency and training convergence in heterogeneous edge networks. Extensive
experiments with various datasets validate the effectiveness of HASFL and
demonstrate its superiority over state-of-the-art benchmarks.

</details>


### [96] [Learning to Lead: Incentivizing Strategic Agents in the Dark](https://arxiv.org/abs/2506.08438)
*Yuchen Wu,Xinyi Zhong,Zhuoran Yang*

Main category: cs.LG

TL;DR: 研究了一个在线学习版本的广义委托代理模型，提出了首个可证明样本高效的算法，建立了接近最优的$\tilde{O}(\sqrt{T}) $遗憾界限，为涉及私人类型和战略性代理人的博弈论设置设计稳健的在线学习算法开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 在广义委托代理模型中，委托人与具有私人类型、私人奖励和采取不可观察行动的战略性代理人反复互动。代理人会战略性地错误报告类型以操纵委托人的学习，而委托人只能观察到她自己的实现奖励和代理人的报告类型，目标是学习一个最佳协调机制以最小化战略遗憾。

Method: 提出了一种新颖的方法，包括：(i) 延迟机制以激励近似短视的代理行为；(ii) 创新的奖励角度估计框架，使用扇区测试和匹配程序来恢复类型依赖的奖励函数；(iii) 悲观乐观的LinUCB算法，使委托人能够有效探索同时尊重代理人的激励约束。

Result: 建立了接近最优的$\tilde{O}(\sqrt{T}) $遗憾界限，为学习委托人的最佳策略提供了理论支持。

Conclusion: 该结果为设计稳健的在线学习算法，应用于涉及私人类型和战略性代理人的广泛博弈论设置开辟了新的途径。

Abstract: We study an online learning version of the generalized principal-agent model,
where a principal interacts repeatedly with a strategic agent possessing
private types, private rewards, and taking unobservable actions. The agent is
non-myopic, optimizing a discounted sum of future rewards and may strategically
misreport types to manipulate the principal's learning. The principal,
observing only her own realized rewards and the agent's reported types, aims to
learn an optimal coordination mechanism that minimizes strategic regret. We
develop the first provably sample-efficient algorithm for this challenging
setting. Our approach features a novel pipeline that combines (i) a delaying
mechanism to incentivize approximately myopic agent behavior, (ii) an
innovative reward angle estimation framework that uses sector tests and a
matching procedure to recover type-dependent reward functions, and (iii) a
pessimistic-optimistic LinUCB algorithm that enables the principal to explore
efficiently while respecting the agent's incentive constraints. We establish a
near optimal $\tilde{O}(\sqrt{T}) $ regret bound for learning the principal's
optimal policy, where $\tilde{O}(\cdot) $ omits logarithmic factors. Our
results open up new avenues for designing robust online learning algorithms for
a wide range of game-theoretic settings involving private types and strategic
agents.

</details>


### [97] [Time-Aware World Model for Adaptive Prediction and Control](https://arxiv.org/abs/2506.08441)
*Anh N. Nhu,Sanghyun Son,Ming Lin*

Main category: cs.LG

TL;DR: This paper introduces Time-Aware World Model (TAWM), which incorporates temporal dynamics by conditioning on time-step size and training across diverse Δt values. It enhances performance and data efficiency in control problems, outperforming conventional models under varying observation rates.


<details>
  <summary>Details</summary>
Motivation: The motivation of this work is to improve the performance and data efficiency in learning control tasks by explicitly incorporating temporal dynamics into world models. This addresses limitations of conventional models that often use a fixed time-step, ignoring the varying optimal sampling rates for different system dynamics.

Method: The method involves developing TAWM, a model-based approach that conditions on the time-step size (Δt) and trains over a diverse range of Δt values instead of using a fixed time-step. This allows the model to learn both high- and low-frequency task dynamics, leveraging an information-theoretic insight about the relationship between optimal sampling rate and system's underlying dynamics.

Result: Empirical evaluations demonstrate that TAWM consistently outperforms conventional models across different observation rates in various control tasks, while using the same number of training samples and iterations.

Conclusion: TAWM improves both performance and data efficiency in learning control tasks by incorporating temporal dynamics through conditioning on time-step size. The model outperforms traditional approaches across varying observation rates.

Abstract: In this work, we introduce the Time-Aware World Model (TAWM), a model-based
approach that explicitly incorporates temporal dynamics. By conditioning on the
time-step size, {\Delta}t, and training over a diverse range of {\Delta}t
values -- rather than sampling at a fixed time-step -- TAWM learns both high-
and low-frequency task dynamics across diverse control problems. Grounded in
the information-theoretic insight that the optimal sampling rate depends on a
system's underlying dynamics, this time-aware formulation improves both
performance and data efficiency. Empirical evaluations show that TAWM
consistently outperforms conventional models across varying observation rates
in a variety of control tasks, using the same number of training samples and
iterations. Our code can be found online at:
github.com/anh-nn01/Time-Aware-World-Model.

</details>


### [98] [MOBODY: Model Based Off-Dynamics Offline Reinforcement Learning](https://arxiv.org/abs/2506.08460)
*Yihong Guo,Yu Yang,Pan Xu,Anqi Liu*

Main category: cs.LG

TL;DR: MOBODY is a Model-Based Off-Dynamics offline RL algorithm that enables exploration of the target domain beyond offline datasets by generating new synthetic transitions through model rollouts. It learns target dynamics by discovering a shared latent representation and incorporates a Q-weighted behavior cloning loss to stabilize training.


<details>
  <summary>Details</summary>
Motivation: Existing off-dynamics offline RL methods are limited in their ability to explore the target domain beyond the available offline datasets due to constraints such as filtering source transitions or applying reward augmentation based on limited target domain transitions.

Method: MOBODY generates new synthetic transitions in the target domain via model rollouts for data augmentation during offline policy learning. It avoids bias towards source dynamics by discovering a shared latent representation of states and transitions across domains through representation learning, rather than directly merging datasets. Additionally, it incorporates a Q-weighted behavior cloning loss to regularize the policy towards actions with high target-domain Q-values.

Result: MOBODY significantly outperforms state-of-the-art baselines on MuJoCo benchmarks, particularly showing pronounced improvements in challenging scenarios.

Conclusion: MOBODY addresses the limitation of existing methods by enabling exploration beyond offline datasets and effectively handling mismatched dynamics through its unique approach of shared latent representation learning and Q-weighted behavior cloning.

Abstract: We study the off-dynamics offline reinforcement learning problem, where the
goal is to learn a policy from offline datasets collected from source and
target domains with mismatched transition. Existing off-dynamics offline RL
methods typically either filter source transitions that resemble those of the
target domain or apply reward augmentation to source data, both constrained by
the limited transitions available from the target domain. As a result, the
learned policy is unable to explore target domain beyond the offline datasets.
We propose MOBODY, a Model-Based Off-Dynamics offline RL algorithm that
addresses this limitation by enabling exploration of the target domain via
learned dynamics. MOBODY generates new synthetic transitions in the target
domain through model rollouts, which are used as data augmentation during
offline policy learning. Unlike existing model-based methods that learn
dynamics from a single domain, MOBODY tackles the challenge of mismatched
dynamics by leveraging both source and target datasets. Directly merging these
datasets can bias the learned model toward source dynamics. Instead, MOBODY
learns target dynamics by discovering a shared latent representation of states
and transitions across domains through representation learning. To stabilize
training, MOBODY incorporates a behavior cloning loss that regularizes the
policy. Specifically, we introduce a Q-weighted behavior cloning loss that
regularizes the policy toward actions with high target-domain Q-values, rather
than uniformly imitating all actions in the dataset. These Q-values are learned
from an enhanced target dataset composed of offline target data, augmented
source data, and rollout data from the learned target dynamics. We evaluate
MOBODY on MuJoCo benchmarks and show that it significantly outperforms
state-of-the-art baselines, with especially pronounced improvements in
challenging scenarios.

</details>


### [99] [How to Provably Improve Return Conditioned Supervised Learning?](https://arxiv.org/abs/2506.08463)
*Zhishuai Liu,Yu Yang,Ruhan Wang,Pan Xu,Dongruo Zhou*

Main category: cs.LG

TL;DR: In sequential decision-making, RCSL is a stable method but limited by dataset quality. Reinforced RCSL introduces in-distribution optimal return-to-go to overcome this, outperforming standard RCSL theoretically and empirically.


<details>
  <summary>Details</summary>
Motivation: RCSL is recognized for its simplicity and stability in decision-making tasks, but it lacks the stitching property, limiting its performance by the quality of the policy used to generate the offline dataset.

Method: Propose Reinforced RCSL with the introduction of in-distribution optimal return-to-go, which leverages the policy to identify the best achievable in-dataset future return based on the current state.

Result: Theoretical analysis shows Reinforced RCSL consistently outperforms standard RCSL. Empirical results validate significant performance improvements across benchmarks.

Conclusion: Reinforced RCSL addresses the limitation of RCSL by introducing in-distribution optimal return-to-go, leading to better performance.

Abstract: In sequential decision-making problems, Return-Conditioned Supervised
Learning (RCSL) has gained increasing recognition for its simplicity and
stability in modern decision-making tasks. Unlike traditional offline
reinforcement learning (RL) algorithms, RCSL frames policy learning as a
supervised learning problem by taking both the state and return as input. This
approach eliminates the instability often associated with temporal difference
(TD) learning in offline RL. However, RCSL has been criticized for lacking the
stitching property, meaning its performance is inherently limited by the
quality of the policy used to generate the offline dataset. To address this
limitation, we propose a principled and simple framework called Reinforced
RCSL. The key innovation of our framework is the introduction of a concept we
call the in-distribution optimal return-to-go. This mechanism leverages our
policy to identify the best achievable in-dataset future return based on the
current state, avoiding the need for complex return augmentation techniques.
Our theoretical analysis demonstrates that Reinforced RCSL can consistently
outperform the standard RCSL approach. Empirical results further validate our
claims, showing significant performance improvements across a range of
benchmarks.

</details>


### [100] [MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature](https://arxiv.org/abs/2506.08464)
*Hyunseok Seung,Jaewoo Lee,Hyunsuk Ko*

Main category: cs.LG

TL;DR: This paper proposes MAC, an efficient optimization method that applies Kronecker factorization to the Fisher information matrix (FIM) of attention layers in transformers, and integrates attention scores into preconditioning. It outperforms KFAC and other methods in accuracy, training time, and memory usage.


<details>
  <summary>Details</summary>
Motivation: Second-order optimization methods like KFAC offer superior convergence for neural network training but come with high computational costs. The authors aim to develop a more computationally efficient method.

Method: The authors analyze the two components of the layer-wise Fisher information matrix (FIM) used in KFAC: Kronecker factors related to activations and pre-activation gradients. They propose efficient approximations based on empirical observations of their eigenspectra, leading to the development of MAC. MAC explicitly integrates attention scores into the preconditioning and applies Kronecker factorization to the FIM of attention layers in transformers.

Result: MAC outperforms KFAC and other state-of-the-art methods in terms of accuracy, end-to-end training time, and memory usage across various network architectures and datasets.

Conclusion: MAC is presented as a computationally efficient second-order optimization method for training neural networks, particularly effective for transformer architectures.

Abstract: Second-order optimization methods for training neural networks, such as KFAC,
exhibit superior convergence by utilizing curvature information of loss
landscape. However, it comes at the expense of high computational burden. In
this work, we analyze the two components that constitute the layer-wise Fisher
information matrix (FIM) used in KFAC: the Kronecker factors related to
activations and pre-activation gradients. Based on empirical observations on
their eigenspectra, we propose efficient approximations for them, resulting in
a computationally efficient optimization method called MAC. To the best of our
knowledge, MAC is the first algorithm to apply the Kronecker factorization to
the FIM of attention layers used in transformers and explicitly integrate
attention scores into the preconditioning. We also study the convergence
property of MAC on nonlinear neural networks and provide two conditions under
which it converges to global minima. Our extensive evaluations on various
network architectures and datasets show that the proposed method outperforms
KFAC and other state-of-the-art methods in terms of accuracy, end-to-end
training time, and memory usage. Code is available at
https://github.com/hseung88/mac.

</details>


### [101] [AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin](https://arxiv.org/abs/2506.08473)
*Shuo Yang,Qihui Zhang,Yuyang Liu,Yue Huang,Xiaojun Jia,Kunpeng Ning,Jiayu Yao,Jigang Wang,Hailiang Dai,Yibing Song,Li Yuan*

Main category: cs.LG

TL;DR: Large language models (LLMs) face safety risks during fine-tuning. This paper proposes AsFT, a methodology that uses the alignment direction as an anchor to suppress harmful updates and ensure safe fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of LLMs to safety risks during fine-tuning, where even small amounts of malicious or harmless data can compromise safeguards.

Method: The proposed method, AsFT, integrates a regularization term into the training objective that uses the alignment direction as an anchor to suppress updates in harmful directions, ensuring fine-tuning remains within a narrow safety basin.

Result: AsFT outperforms Safe LoRA by reducing harmful behavior by 7.60 percent, improving model performance by 3.44 percent, and maintaining robust performance across various experimental settings.

Conclusion: AsFT provides an effective way to ensure safe fine-tuning of LLMs, preserving model safety and performance.

Abstract: Large language models (LLMs) are vulnerable to safety risks during
fine-tuning, where small amounts of malicious or harmless data can compromise
safeguards. In this paper, building on the concept of alignment direction --
defined by the weight difference between aligned and unaligned models -- we
observe that perturbations along this direction preserve model safety. In
contrast, perturbations along directions orthogonal to this alignment are
strongly linked to harmful direction perturbations, rapidly degrading safety
and framing the parameter space as a narrow safety basin. Based on this
insight, we propose a methodology for safety fine-tuning called AsFT (Anchoring
Safety in Fine-Tuning), which integrates a regularization term into the
training objective. This term uses the alignment direction as an anchor to
suppress updates in harmful directions, ensuring that fine-tuning is
constrained within the narrow safety basin. Extensive experiments on multiple
datasets show that AsFT outperforms Safe LoRA, reducing harmful behavior by
7.60 percent, improving model performance by 3.44 percent, and maintaining
robust performance across various experimental settings. Code is available at
https://github.com/PKU-YuanGroup/AsFT

</details>


### [102] [Thermodynamically Consistent Latent Dynamics Identification for Parametric Systems](https://arxiv.org/abs/2506.08475)
*Xiaolong He,Yeonjong Shin,Anthony Gruber,Sohyeon Jung,Kookjin Lee,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出了一种高效的热力学信息潜在空间动力学识别（tLaSDI）框架，用于参数化非线性动力系统的降阶建模。通过结合自动编码器和新的参数化广义热力学形式神经网络（pGFINNs），实现了参数化潜在动力学的有效学习，同时保留了关键的热力学原理。采用基于物理的主动学习策略，以提高模型性能。实验表明，该方法在保持1-3%相对误差的情况下可实现高达3,528倍的加速，并显著降低训练和推理成本。


<details>
  <summary>Details</summary>
Motivation: 为了高效地对参数化非线性动力系统进行降阶建模，同时保留热力学原则，如自由能守恒和熵生成。

Method: 将自动编码器与参数化广义热力学形式神经网络（pGFINNs）相结合，进行降维和潜在动力学的学习；引入基于物理的主动学习策略，通过贪婪、基于残差的误差指标自适应采样训练数据。

Result: 在Burgers方程和1D/1V Vlasov-Poisson方程上的数值实验表明，该方法可以实现高达3,528倍的加速，相对误差保持在1-3%，并显著降低了训练（50-90%）和推理（57-61%）成本。

Conclusion: 所提出的tLaSDI框架能够有效地学习参数化潜在动力学，同时保留热力学原理，提供关于物理空间动力学的有价值见解，并显著提高了计算效率。

Abstract: We propose an efficient thermodynamics-informed latent space dynamics
identification (tLaSDI) framework for the reduced-order modeling of parametric
nonlinear dynamical systems. This framework integrates autoencoders for
dimensionality reduction with newly developed parametric GENERIC
formalism-informed neural networks (pGFINNs), which enable efficient learning
of parametric latent dynamics while preserving key thermodynamic principles
such as free energy conservation and entropy generation across the parameter
space. To further enhance model performance, a physics-informed active learning
strategy is incorporated, leveraging a greedy, residual-based error indicator
to adaptively sample informative training data, outperforming uniform sampling
at equivalent computational cost. Numerical experiments on the Burgers'
equation and the 1D/1V Vlasov-Poisson equation demonstrate that the proposed
method achieves up to 3,528x speed-up with 1-3% relative errors, and
significant reduction in training (50-90%) and inference (57-61%) cost.
Moreover, the learned latent space dynamics reveal the underlying thermodynamic
behavior of the system, offering valuable insights into the physical-space
dynamics.

</details>


### [103] [Explaining, Fast and Slow: Abstraction and Refinement of Provable Explanations](https://arxiv.org/abs/2506.08505)
*Shahaf Bassan,Yizhak Yisrael Elboher,Tobias Ladner,Matthias Althoff,Guy Katz*

Main category: cs.LG

TL;DR: An abstraction-refinement technique is proposed to efficiently compute provably sufficient explanations of neural network predictions.


<details>
  <summary>Details</summary>
Motivation: Many current methods for explaining neural networks do not provide formally provable guarantees. Recent work shows that formal guarantees can be obtained using neural network verification techniques, but these face significant scalability challenges.

Method: The authors propose a novel abstraction-refinement technique which constructs a reduced network from the original large neural network. A sufficient explanation of this reduced network is also sufficient for the original network, speeding up the verification process. If the explanation is insufficient on the reduced network, the network size is iteratively refined by gradually increasing it until convergence.

Result: Experiments show that this approach improves the efficiency of obtaining provably sufficient explanations for neural network predictions and provides a fine-grained interpretation of the network's predictions across different abstraction levels.

Conclusion: The proposed abstraction-refinement technique addresses the scalability challenge in computing provably sufficient explanations of neural network predictions, making the process more efficient.

Abstract: Despite significant advancements in post-hoc explainability techniques for
neural networks, many current methods rely on heuristics and do not provide
formally provable guarantees over the explanations provided. Recent work has
shown that it is possible to obtain explanations with formal guarantees by
identifying subsets of input features that are sufficient to determine that
predictions remain unchanged using neural network verification techniques.
Despite the appeal of these explanations, their computation faces significant
scalability challenges. In this work, we address this gap by proposing a novel
abstraction-refinement technique for efficiently computing provably sufficient
explanations of neural network predictions. Our method abstracts the original
large neural network by constructing a substantially reduced network, where a
sufficient explanation of the reduced network is also provably sufficient for
the original network, hence significantly speeding up the verification process.
If the explanation is in sufficient on the reduced network, we iteratively
refine the network size by gradually increasing it until convergence. Our
experiments demonstrate that our approach enhances the efficiency of obtaining
provably sufficient explanations for neural network predictions while
additionally providing a fine-grained interpretation of the network's
predictions across different abstraction levels.

</details>


### [104] [DiffGradCAM: A Universal Class Activation Map Resistant to Adversarial Training](https://arxiv.org/abs/2506.08514)
*Jacob Piland,Chris Sweet,Adam Czakja*

Main category: cs.LG

TL;DR: The paper introduces Salience-Hoax Activation Maps (SHAMs) as a benchmark for evaluating the robustness of Class Activation Mapping (CAM) methods under adversarial conditions. It also proposes DiffGradCAM, which is resistant to passive fooling and matches standard CAM outputs in non-adversarial cases.


<details>
  <summary>Details</summary>
Motivation: Current CAM methods focus on individual logits rather than their differences, making them vulnerable to adversarial manipulation like passive fooling where models produce misleading CAMs without impacting decision performance.

Method: The authors introduce SHAMs, an entropy-aware form of passive fooling, to benchmark CAM robustness. They also propose DiffGradCAM, a contrastive approach that is not susceptible to passive fooling and aligns with standard CAM methods in non-adversarial scenarios.

Result: SHAM and DiffGradCAM establish a framework for improving saliency-based explanation robustness, validated across multi-class tasks with few and many classes.

Conclusion: DiffGradCAM offers a robust alternative to existing CAM methods by resisting adversarial manipulation while maintaining performance in standard conditions.

Abstract: Class Activation Mapping (CAM) and its gradient-based variants (e.g.,
GradCAM) have become standard tools for explaining Convolutional Neural Network
(CNN) predictions. However, these approaches typically focus on individual
logits, while for neural networks using softmax, the class membership
probability estimates depend \textit{only} on the \textit{differences} between
logits, not on their absolute values. This disconnect leaves standard CAMs
vulnerable to adversarial manipulation, such as passive fooling, where a model
is trained to produce misleading CAMs without affecting decision performance.
We introduce \textbf{Salience-Hoax Activation Maps (SHAMs)}, an
\emph{entropy-aware form of passive fooling} that serves as a benchmark for CAM
robustness under adversarial conditions. To address the passive fooling
vulnerability, we then propose \textbf{DiffGradCAM}, a novel, lightweight, and
contrastive approach to class activation mapping that is both non-suceptible to
passive fooling, but also matches the output of standard CAM methods such as
GradCAM in the non-adversarial case. Together, SHAM and DiffGradCAM establish a
new framework for probing and improving the robustness of saliency-based
explanations. We validate both contributions across multi-class tasks with few
and many classes.

</details>


### [105] [NeurIPS 2024 ML4CFD Competition: Results and Retrospective Analysis](https://arxiv.org/abs/2506.08516)
*Mouadh Yagoubi,David Danan,Milad Leyli-Abadi,Ahmed Mazari,Jean-Patrick Brunet,Abbas Kabalan,Fabien Casenave,Yuxin Ma,Giovanni Catalani,Jean Fesquet,Jacob Helwig,Xuan Zhang,Haiyang Yu,Xavier Bertrand,Frederic Tost,Michael Baurheim,Joseph Morlier,Shuiwang Ji*

Main category: cs.LG

TL;DR: The ML4CFD competition focused on using machine learning for aerodynamic simulations, attracting 240 teams. Top entries outperformed traditional solvers in accuracy, efficiency, and generalization, providing insights into future scientific ML challenges.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of ML models in scientific domains such as accuracy, generalization, and physical consistency, and to benchmark progress in this field.

Method: Organized the ML4CFD competition centered on surrogate modeling for aerodynamic simulations over two-dimensional airfoils. Teams were provided a dataset generated via OpenFOAM and evaluated through a multi-criteria framework including predictive accuracy, physical fidelity, computational efficiency, and out-of-distribution generalization.

Result: Several approaches outperformed baselines under the global evaluation score, with the top entry exceeding the performance of the original OpenFOAM solver on aggregate metrics.

Conclusion: ML-based surrogates show promise in outperforming traditional solvers under tailored criteria. Key design principles of top submissions are analyzed, the robustness of the evaluation framework is assessed, and guidance for future scientific ML challenges is offered.

Abstract: The integration of machine learning (ML) into the physical sciences is
reshaping computational paradigms, offering the potential to accelerate
demanding simulations such as computational fluid dynamics (CFD). Yet,
persistent challenges in accuracy, generalization, and physical consistency
hinder the practical deployment of ML models in scientific domains. To address
these limitations and systematically benchmark progress, we organized the
ML4CFD competition, centered on surrogate modeling for aerodynamic simulations
over two-dimensional airfoils. The competition attracted over 240 teams, who
were provided with a curated dataset generated via OpenFOAM and evaluated
through a multi-criteria framework encompassing predictive accuracy, physical
fidelity, computational efficiency, and out-of-distribution generalization.
This retrospective analysis reviews the competition outcomes, highlighting
several approaches that outperformed baselines under our global evaluation
score. Notably, the top entry exceeded the performance of the original OpenFOAM
solver on aggregate metrics, illustrating the promise of ML-based surrogates to
outperform traditional solvers under tailored criteria. Drawing from these
results, we analyze the key design principles of top submissions, assess the
robustness of our evaluation framework, and offer guidance for future
scientific ML challenges.

</details>


### [106] [Leveraging chaos in the training of artificial neural networks](https://arxiv.org/abs/2506.08523)
*Pedro Jiménez-González,Miguel C. Soriano,Lucas Lacasa*

Main category: cs.LG

TL;DR: The paper explores how large learning rates can lead to a balance of exploration and exploitation in gradient descent optimization for neural networks, accelerating training by leveraging transient chaotic dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of neural network training with unusually large learning rates and determine if this approach can accelerate learning.

Method: Investigate the behavior of gradient descent optimization under various learning rates, focusing on the transition from exploitation to an exploration-exploitation balance characterized by positive maximum Lyapunov exponents.

Result: For certain learning rate values, gradient descent exhibits a balance between exploration and exploitation, with minimal characteristic training time at the onset of chaos.

Conclusion: Transient chaotic dynamics during neural network training plays a constructive role in optimizing learning performance.

Abstract: Traditional algorithms to optimize artificial neural networks when confronted
with a supervised learning task are usually exploitation-type relaxational
dynamics such as gradient descent (GD). Here, we explore the dynamics of the
neural network trajectory along training for unconventionally large learning
rates. We show that for a region of values of the learning rate, the GD
optimization shifts away from purely exploitation-like algorithm into a regime
of exploration-exploitation balance, as the neural network is still capable of
learning but the trajectory shows sensitive dependence on initial conditions --
as characterized by positive network maximum Lyapunov exponent --.
Interestingly, the characteristic training time required to reach an acceptable
accuracy in the test set reaches a minimum precisely in such learning rate
region, further suggesting that one can accelerate the training of artificial
neural networks by locating at the onset of chaos. Our results -- initially
illustrated for the MNIST classification task -- qualitatively hold for a range
of supervised learning tasks, learning architectures and other hyperparameters,
and showcase the emergent, constructive role of transient chaotic dynamics in
the training of artificial neural networks.

</details>


### [107] [Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)](https://arxiv.org/abs/2506.08533)
*Nihal Acharya Adde,Alexandra Gianzina,Hanno Gottschalk,Andreas Ebert*

Main category: cs.LG

TL;DR: This paper introduces EMNAS, a new method using genetic algorithms to optimize neural network architectures in large-scale RL for autonomous driving. It automates network design, reduces model size, and enhances rewards. Transfer learning is used to improve efficiency and stability.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to enhance the performance of neural networks in reinforcement learning for autonomous driving by reducing model size without compromising performance.

Method: EMNAS uses genetic algorithms to automate network design and employs parallelization techniques to accelerate the search. Teacher-student methodologies are also implemented to ensure scalable optimization.

Result: Experimental results show that EMNAS outperforms manually designed models, achieving higher rewards with fewer parameters.

Conclusion: The findings contribute positively to EMNAS for RL in autonomous driving, advancing the field toward better-performing networks suitable for real-world scenarios.

Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture
Search (EMNAS) for the first time to optimize neural network architectures in
large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses
genetic algorithms to automate network design, tailored to enhance rewards and
reduce model size without compromising performance. Additionally,
parallelization techniques are employed to accelerate the search, and
teacher-student methodologies are implemented to ensure scalable optimization.
This research underscores the potential of transfer learning as a robust
framework for optimizing performance across iterative learning processes by
effectively leveraging knowledge from earlier generations to enhance learning
efficiency and stability in subsequent generations. Experimental results
demonstrate that tailored EMNAS outperforms manually designed models, achieving
higher rewards with fewer parameters. The findings of these strategies
contribute positively to EMNAS for RL in autonomous driving, advancing the
field toward better-performing networks suitable for real-world scenarios.

</details>


### [108] [DeepForm: Reasoning Large Language Model for Communication System Formulation](https://arxiv.org/abs/2506.08551)
*Panlong Wu,Ting Wang,Yifei Zhong,Haoqi Zhang,Zitong Wang,Fangxin Wang*

Main category: cs.LG

TL;DR: The paper introduces DeepForm, the first reasoning LLM for automated communication system formulation. It uses a large-scale open-source dataset (CSFRC) and a two-stage training strategy involving Supervised Fine-Tuning and a novel Reinforcement Learning algorithm (C-ReMax). Experiments show it outperforms larger proprietary LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and expertise required in communication system formulation, and the lack of specialized domain knowledge in existing general-purpose LLMs.

Method: Developed DeepForm using a large-scale open-source dataset (CSFRC) and a two-stage training strategy that includes Supervised Fine-Tuning with Chain-of-Thought data and a novel rule-based Reinforcement Learning algorithm (C-ReMax).

Result: DeepForm achieves state-of-the-art performance, significantly outperforming larger proprietary LLMs in diverse scenarios.

Conclusion: The model and related resources will be released to promote further research in communication system formulation.

Abstract: Communication system formulation is critical for advancing 6G and future
wireless technologies, yet it remains a complex, expertise-intensive task.
While Large Language Models (LLMs) offer potential, existing general-purpose
models often lack the specialized domain knowledge, nuanced reasoning
capabilities, and access to high-quality, domain-specific training data
required for adapting a general LLM into an LLM specially for communication
system formulation. To bridge this gap, we introduce DeepForm, the first
reasoning LLM specially for automated communication system formulation. We
propose the world-first large-scale, open-source dataset meticulously curated
for this domain called Communication System Formulation Reasoning Corpus
(CSFRC). Our framework employs a two-stage training strategy: first, Supervised
Fine-Tuning (SFT) with Chain-of-Thought (CoT) data to distill domain knowledge;
second, a novel rule-based Reinforcement Learning (RL) algorithm, C-ReMax based
on ReMax, to cultivate advanced modeling capabilities and elicit sophisticated
reasoning patterns like self-correction and verification. Extensive experiments
demonstrate that our model achieves state-of-the-art performance, significantly
outperforming larger proprietary LLMs on diverse senerios. We will release
related resources to foster further research in this area after the paper is
accepted.

</details>


### [109] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 尽管大型语言模型（LLMs）表现出色，但其可靠性仍存在疑虑。先前研究提出通过分析推理时的激活情况来判断答案正确性，即学习“真值几何”。然而，本文指出这些“真值几何”具有任务依赖性，难以跨任务迁移。具体表现为不同任务训练出的线性分类器相似度低且支持集几乎不重叠，即使采用更复杂的方法也难以克服这一限制。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs表现出强大的泛化能力，但其在实际应用中的可靠性仍受质疑。因此需要一种方法能够在推理时评估LLM的答案正确性，而目前提出的“真值几何”方法是否有效值得进一步探讨。

Method: 研究者观察并证明了“真值几何”的任务依赖性，通过实验展示了不同任务训练的线性分类器之间相似度低、支持集几乎不重叠的现象，并尝试使用更复杂的模型（如混合探针和任务的方法）来克服这一局限性。

Result: 研究表明，“真值几何”是任务依赖的，无法跨任务迁移。不同任务的线性分类器几乎没有相似性，即使采用更复杂的模型也无法解决这一问题，可能是因为激活向量在跨任务分析时形成明显分离的簇。

Conclusion: “真值几何”方法虽然能够区分正确与错误答案，但其任务依赖性和跨任务失效限制了其实际应用价值。未来需要探索其他方法以实现更普适的LLM答案评估机制。

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [110] [SLEEPYLAND: trust begins with fair evaluation of automatic sleep staging models](https://arxiv.org/abs/2506.08574)
*Alvise Dei Rossi,Matteo Metaldi,Michal Bechny,Irina Filchenko,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Athina Tzovara,Francesca D. Faraci,Luigi Fiorillo*

Main category: cs.LG

TL;DR: Despite advances in deep learning for automatic sleep staging, clinical adoption remains limited due to challenges in fair model evaluation, generalization across diverse datasets, model bias, and variability in human annotations. This paper presents SLEEPYLAND, an open-source sleep staging evaluation framework designed to address these barriers. It includes more than 22'0000 hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain (OOD) sleep recordings. SOMNUS, an ensemble combining models across architectures and channel setups via soft voting, achieves robust performance across twenty-four different datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges in fair model evaluation, generalization across diverse datasets, model bias, and variability in human annotations in the field of automatic sleep staging.

Method: The authors developed SLEEPYLAND, an open-source sleep staging evaluation framework with extensive ID and OOD sleep recordings. They also introduced SOMNUS, an ensemble method that combines models across architectures and channel setups via soft voting.

Result: SOMNUS achieved robust performance across twenty-four different datasets, outperforming individual models in 94.9% of cases. It surpassed previous SoA methods even when compared models were trained ID while SOMNUS treated the same data as OOD. SOMNUS exceeded the best human scorer on OOD multi-annotated datasets and better reproduced the scorer consensus than any individual expert. Ensemble disagreement metrics predicted regions of scorer disagreement with ROC AUCs up to 0.828.

Conclusion: SLEEPYLAND and SOMNUS provide significant advancements in addressing the challenges faced in the clinical adoption of automatic sleep staging, offering improved model performance and a data-driven proxy for human uncertainty.

Abstract: Despite advances in deep learning for automatic sleep staging, clinical
adoption remains limited due to challenges in fair model evaluation,
generalization across diverse datasets, model bias, and variability in human
annotations. We present SLEEPYLAND, an open-source sleep staging evaluation
framework designed to address these barriers. It includes more than 22'0000
hours in-domain (ID) sleep recordings, and more than 84'000 hours out-of-domain
(OOD) sleep recordings, spanning a broad range of ages, sleep-wake disorders,
and hardware setups. We release pre-trained models based on high-performing SoA
architectures and evaluate them under standardized conditions across single-
and multi-channel EEG/EOG configurations. We introduce SOMNUS, an ensemble
combining models across architectures and channel setups via soft voting.
SOMNUS achieves robust performance across twenty-four different datasets, with
macro-F1 scores between 68.7% and 87.2%, outperforming individual models in
94.9% of cases. Notably, SOMNUS surpasses previous SoA methods, even including
cases where compared models were trained ID while SOMNUS treated the same data
as OOD. Using a subset of the BSWR (N=6'633), we quantify model biases linked
to age, gender, AHI, and PLMI, showing that while ensemble improves robustness,
no model architecture consistently minimizes bias in performance and clinical
markers estimation. In evaluations on OOD multi-annotated datasets (DOD-H,
DOD-O), SOMNUS exceeds the best human scorer, i.e., MF1 85.2% vs 80.8% on
DOD-H, and 80.2% vs 75.9% on DOD-O, better reproducing the scorer consensus
than any individual expert (k = 0.89/0.85 and ACS = 0.95/0.94 for healthy/OSA
cohorts). Finally, we introduce ensemble disagreement metrics - entropy and
inter-model divergence based - predicting regions of scorer disagreement with
ROC AUCs up to 0.828, offering a data-driven proxy for human uncertainty.

</details>


### [111] [Diffusion-based Time Series Forecasting for Sewerage Systems](https://arxiv.org/abs/2506.08577)
*Nicholas A. Pearson,Francesca Cairoli,Luca Bortolussi,Davide Russo,Francesca Zanello*

Main category: cs.LG

TL;DR: A new deep learning method uses generative AI for better predictions in sewer systems, even during extreme weather. It combines a diffusion model with conformal inference to ensure reliable prediction intervals.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and reliability of contextual forecasting in sewerage systems, especially during extreme weather events.

Method: The approach involves developing a diffusion-based model that processes multivariate time series data to capture complex correlations. Conformal inference is then used to calibrate the predictions, ensuring statistically reliable prediction intervals.

Result: Empirical tests on real sewerage system data show that the model delivers reliable contextual predictions with maintained accuracy, even under severe weather conditions.

Conclusion: This novel deep learning approach significantly enhances the ability to make accurate and reliable predictions in sewerage systems, which is crucial for managing infrastructure during extreme weather.

Abstract: We introduce a novel deep learning approach that harnesses the power of
generative artificial intelligence to enhance the accuracy of contextual
forecasting in sewerage systems. By developing a diffusion-based model that
processes multivariate time series data, our system excels at capturing complex
correlations across diverse environmental signals, enabling robust predictions
even during extreme weather events. To strengthen the model's reliability, we
further calibrate its predictions with a conformal inference technique,
tailored for probabilistic time series data, ensuring that the resulting
prediction intervals are statistically reliable and cover the true target
values with a desired confidence level. Our empirical tests on real sewerage
system data confirm the model's exceptional capability to deliver reliable
contextual predictions, maintaining accuracy even under severe weather
conditions.

</details>


### [112] [CALT: A Library for Computer Algebra with Transformer](https://arxiv.org/abs/2506.08600)
*Hiroshi Kera,Shun Arakawa,Yuta Sato*

Main category: cs.LG

TL;DR: Recent advances in AI show that symbolic computation can be learned through end-to-end deep learning using Transformer models. A new Python library called Computer Algebra with Transformer (CALT) is introduced to assist non-experts in training models for symbolic computation tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the learnability of symbolic computation through end-to-end deep learning and to provide a tool for non-experts in deep learning to train models for symbolic computation tasks.

Method: The method involves using Transformer models, which are effective learners of sequence-to-sequence functions, to emulate symbolic computations by training them on examples of symbolic expressions before and after the target computation.

Result: The result is the introduction of CALT, a user-friendly Python library designed to facilitate the training of models for symbolic computation tasks.

Conclusion: This development opens up several intriguing challenges and new research directions, requiring active contributions from the symbolic computation community.

Abstract: Recent advances in artificial intelligence have demonstrated the learnability
of symbolic computation through end-to-end deep learning. Given a sufficient
number of examples of symbolic expressions before and after the target
computation, Transformer models - highly effective learners of
sequence-to-sequence functions - can be trained to emulate the computation.
This development opens up several intriguing challenges and new research
directions, which require active contributions from the symbolic computation
community. In this work, we introduce Computer Algebra with Transformer (CALT),
a user-friendly Python library designed to help non-experts in deep learning
train models for symbolic computation tasks.

</details>


### [113] [Flow Matching Meets PDEs: A Unified Framework for Physics-Constrained Generation](https://arxiv.org/abs/2506.08604)
*Giacomo Baldan,Qiang Liu,Alberto Guardone,Nils Thuerey*

Main category: cs.LG

TL;DR: The paper introduces Physics-Based Flow Matching (PBFM), a generative framework embedding physical constraints into flow matching to improve modeling accuracy in complex systems. It minimizes both flow matching and physics-based residual losses without hyperparameter tuning, reduces physical residuals via a stochastic sampling strategy, and performs up to 8 times better than existing methods on PDE benchmarks.


<details>
  <summary>Details</summary>
Motivation: Generative machine learning methods like diffusion models and flow matching have shown potential for modeling complex system behaviors but typically learn underlying physics implicitly from data. There is a need for a method that explicitly incorporates physical constraints to enhance the accuracy of predictions.

Method: Physics-Based Flow Matching (PBFM) is proposed which embeds physical constraints such as PDE residuals and algebraic relations into the flow matching objective. Temporal unrolling during training improves prediction accuracy, and the method jointly minimizes flow matching loss and physics-based residual loss without requiring hyperparameter tuning. A stochastic sampling strategy is also introduced to reduce physical residuals.

Result: Through extensive benchmarks on three representative PDE problems, PBFM yields up to an 8 times more accurate physical residuals compared to Flow Matching (FM) while clearly outperforming existing algorithms in terms of distributional accuracy.

Conclusion: PBFM provides a principled and efficient framework for surrogate modeling, uncertainty quantification, and accelerated simulation in physics and engineering applications.

Abstract: Generative machine learning methods, such as diffusion models and flow
matching, have shown great potential in modeling complex system behaviors and
building efficient surrogate models. However, these methods typically learn the
underlying physics implicitly from data. We propose Physics-Based Flow Matching
(PBFM), a novel generative framework that explicitly embeds physical
constraints, both PDE residuals and algebraic relations, into the flow matching
objective. We also introduce temporal unrolling at training time that improves
the accuracy of the final, noise-free sample prediction. Our method jointly
minimizes the flow matching loss and the physics-based residual loss without
requiring hyperparameter tuning of their relative weights. Additionally, we
analyze the role of the minimum noise level, $\sigma_{\min}$, in the context of
physical constraints and evaluate a stochastic sampling strategy that helps to
reduce physical residuals. Through extensive benchmarks on three representative
PDE problems, we show that our approach yields up to an $8\times$ more accurate
physical residuals compared to FM, while clearly outperforming existing
algorithms in terms of distributional accuracy. PBFM thus provides a principled
and efficient framework for surrogate modeling, uncertainty quantification, and
accelerated simulation in physics and engineering applications.

</details>


### [114] [Sample Efficient Demonstration Selection for In-Context Learning](https://arxiv.org/abs/2506.08607)
*Kiran Purohit,V Venktesh,Sourangshu Bhattacharya,Avishek Anand*

Main category: cs.LG

TL;DR: CASE is a new method for exemplar selection in few-shot learning with LLMs, providing significant efficiency gains without performance loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of selecting effective few-shot examples within context-length budget constraints for in-context learning with LLMs.

Method: Formulate exemplar selection as a top-m best arms identification problem and propose CASE, a sample-efficient selective exploration strategy that reduces sample complexity by maintaining a shortlist of 'challenger' arms. Model exemplar subset scores using a parameterized linear scoring function, resulting in stochastic linear bandits setting.

Result: CASE achieves up to 7x speedup in runtime and requires 7x fewer LLM calls (87% reduction) compared to state-of-the-art methods, while maintaining similar performance.

Conclusion: CASE provides significant efficiency improvements in exemplar selection for few-shot learning with LLMs.

Abstract: The in-context learning paradigm with LLMs has been instrumental in advancing
a wide range of natural language processing tasks. The selection of few-shot
examples (exemplars / demonstration samples) is essential for constructing
effective prompts under context-length budget constraints. In this paper, we
formulate the exemplar selection task as a top-m best arms identification
problem. A key challenge in this setup is the exponentially large number of
arms that need to be evaluated to identify the m-best arms. We propose CASE
(Challenger Arm Sampling for Exemplar selection), a novel sample-efficient
selective exploration strategy that maintains a shortlist of "challenger" arms,
which are current candidates for the top-m arms. In each iteration, only one of
the arms from this shortlist or the current topm set is pulled, thereby
reducing sample complexity and, consequently, the number of LLM evaluations.
Furthermore, we model the scores of exemplar subsets (arms) using a
parameterized linear scoring function, leading to stochastic linear bandits
setting. CASE achieves remarkable efficiency gains of up to 7x speedup in
runtime while requiring 7x fewer LLM calls (87% reduction) without sacrificing
performance compared to state-of-the-art exemplar selection methods. We release
our code and data at https://github.com/kiranpurohit/CASE

</details>


### [115] [HSG-12M: A Large-Scale Spatial Multigraph Dataset](https://arxiv.org/abs/2506.08618)
*Xianquan Yan,Hakan Akgün,Kenji Kawaguchi,N. Duane Loh,Ching Hua Lee*

Main category: cs.LG

TL;DR: The paper introduces HSG-12M, a large-scale dataset of spatial multigraphs embedded in metric space with multiple geometrically distinct trajectories as separate edges. It also presents Poly2Graph, an open-source pipeline for mapping 1-D crystal Hamiltonians to spectral graphs. The dataset and tool create new opportunities for geometry-aware graph learning and data-driven scientific discovery.


<details>
  <summary>Details</summary>
Motivation: Existing graph benchmarks do not account for spatial, multi-edge geometries, collapsing physically distinct paths into single links. This limits the ability to study complex, physics-grounded topologies.

Method: The authors created HSG-12M, which contains millions of static and dynamic Hamiltonian spectral graphs derived from spectral potential data. They also developed Poly2Graph, a pipeline that maps arbitrary 1-D crystal Hamiltonians to spectral graphs.

Result: Benchmarks using popular GNNs reveal challenges in learning from multi-edge geometry at scale. Spectral graphs are shown to be universal topological fingerprints of polynomials, vectors, and matrices.

Conclusion: HSG-12M provides a foundation for geometry-aware graph learning and opens new avenues for data-driven scientific discovery in fields like condensed matter physics.

Abstract: Existing graph benchmarks assume non-spatial, simple edges, collapsing
physically distinct paths into a single link. We introduce HSG-12M, the first
large-scale dataset of $\textbf{spatial multigraphs}-$graphs embedded in a
metric space where multiple geometrically distinct trajectories between two
nodes are retained as separate edges. HSG-12M contains 11.6 million static and
5.1 million dynamic $\textit{Hamiltonian spectral graphs}$ across 1401
characteristic-polynomial classes, derived from 177 TB of spectral potential
data. Each graph encodes the full geometry of a 1-D crystal's energy spectrum
on the complex plane, producing diverse, physics-grounded topologies that
transcend conventional node-coordinate datasets. To enable future extensions,
we release $\texttt{Poly2Graph}$: a high-performance, open-source pipeline that
maps arbitrary 1-D crystal Hamiltonians to spectral graphs. Benchmarks with
popular GNNs expose new challenges in learning from multi-edge geometry at
scale. Beyond its practical utility, we show that spectral graphs serve as
universal topological fingerprints of polynomials, vectors, and matrices,
forging a new algebra-to-graph link. HSG-12M lays the groundwork for
geometry-aware graph learning and new opportunities of data-driven scientific
discovery in condensed matter physics and beyond.

</details>


### [116] [Time Series Representations for Classification Lie Hidden in Pretrained Vision Transformers](https://arxiv.org/abs/2506.08641)
*Simon Roschmann,Quentin Bouniot,Vasilii Feofanov,Ievgen Redko,Zeynep Akata*

Main category: cs.LG

TL;DR: 论文提出了一种名为Time Vision Transformer (TiViT)的框架，通过将时间序列转换为图像，利用大规模图像数据集预训练的Vision Transformers的能力，从而在时间序列分类任务上取得最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在医疗和工业领域是一项基础任务，但由于公开可用的时间序列数据集稀缺，时间序列基础模型（TSFMs）的发展受到限制。因此，需要一种新的方法来克服这一挑战。

Method: 提出了一种名为Time Vision Transformer (TiViT)的框架，该框架将时间序列转换为图像，以利用在大规模图像数据集上预训练的Vision Transformers（ViTs）的表示能力。首先，理论分析表明2D patching可以增加标签相关的token数量并降低样本复杂度。其次，实证研究显示TiViT通过使用大型OpenCLIP模型的隐藏表示，在标准时间序列分类基准上取得了最先进的性能。探索了TiViT表示的结构，发现具有高内在维度的中间层对时间序列分类最为有效。最后，评估了TiViT与TSFM表示空间之间的对齐情况，发现它们之间存在很强的互补性，结合其特征可以进一步提高性能。

Result: TiViT在标准时间序列分类基准上实现了最先进的性能，并揭示了在非视觉领域重用视觉表示的另一方向。

Conclusion: TiViT提供了一种有效的方法，通过将时间序列转换为图像并利用预训练的Vision Transformers的能力，提高了时间序列分类的性能，展示了在非视觉领域重用视觉表示的潜力。

Abstract: Time series classification is a fundamental task in healthcare and industry,
yet the development of time series foundation models (TSFMs) remains limited by
the scarcity of publicly available time series datasets. In this work, we
propose Time Vision Transformer (TiViT), a framework that converts time series
into images to leverage the representational power of frozen Vision
Transformers (ViTs) pretrained on large-scale image datasets. First, we
theoretically motivate our approach by analyzing the 2D patching of ViTs for
time series, showing that it can increase the number of label-relevant tokens
and reduce the sample complexity. Second, we empirically demonstrate that TiViT
achieves state-of-the-art performance on standard time series classification
benchmarks by utilizing the hidden representations of large OpenCLIP models. We
explore the structure of TiViT representations and find that intermediate
layers with high intrinsic dimension are the most effective for time series
classification. Finally, we assess the alignment between TiViT and TSFM
representation spaces and identify a strong complementarity, with further
performance gains achieved by combining their features. Our findings reveal yet
another direction for reusing vision representations in a non-visual domain.

</details>


### [117] [Semi-gradient DICE for Offline Constrained Reinforcement Learning](https://arxiv.org/abs/2506.08644)
*Woosung Kim,JunHo Seo,Jongmin Lee,Byung-Jun Lee*

Main category: cs.LG

TL;DR: DICE解决策略诱导的平稳分布与离线强化学习中目标分布之间的不匹配问题。然而，现有的改进方法虽然提升了离线RL性能，却削弱了其在约束RL场景中的OPE能力。本文发现其根本原因在于半梯度优化改变了原始优化问题，从而导致成本估计失败。为此，我们提出了一种新方法，在保持DICE框架优势的同时，实现准确的成本估计和卓越的约束RL性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中的分布不匹配问题严重影响了策略评估(OPE)和优化的准确性，特别是约束强化学习场景下，需要同时最大化回报并估计成本。现有改进DICE的方法虽然提升了离线RL性能，但削弱了其OPE能力，限制了其在约束RL中的应用。

Method: 通过分析现有方法的缺陷，发现其依赖半梯度优化是导致成本估计失败的根本原因。基于此，提出了一种新的方法，利用改进的半梯度DICE框架，确保在离线约束RL中实现准确的成本估计和有效的策略优化。

Result: 所提出的方法在离线约束RL基准测试DSRL上取得了最先进的性能，同时解决了现有方法在OPE中的不足，实现了准确的成本估计。

Conclusion: 本文提出的新方法成功解决了现有DICE改进方法在约束RL中的局限性，为离线强化学习提供了更可靠的策略评估和优化工具。

Abstract: Stationary Distribution Correction Estimation (DICE) addresses the mismatch
between the stationary distribution induced by a policy and the target
distribution required for reliable off-policy evaluation (OPE) and policy
optimization. DICE-based offline constrained RL particularly benefits from the
flexibility of DICE, as it simultaneously maximizes return while estimating
costs in offline settings. However, we have observed that recent approaches
designed to enhance the offline RL performance of the DICE framework
inadvertently undermine its ability to perform OPE, making them unsuitable for
constrained RL scenarios. In this paper, we identify the root cause of this
limitation: their reliance on a semi-gradient optimization, which solves a
fundamentally different optimization problem and results in failures in cost
estimation. Building on these insights, we propose a novel method to enable OPE
and constrained RL through semi-gradient DICE. Our method ensures accurate cost
estimation and achieves state-of-the-art performance on the offline constrained
RL benchmark, DSRL.

</details>


### [118] [Fusing Cross-modal and Uni-modal Representations: A Kronecker Product Approach](https://arxiv.org/abs/2506.08645)
*Youqi Wu,Jingwei Zhang,Farzan Farnia*

Main category: cs.LG

TL;DR: RP-KrossFuse融合了跨模态和单模态嵌入，通过随机投影的Kronecker积实现，在保持跨模态对齐的同时提升特定模态性能。


<details>
  <summary>Details</summary>
Motivation: 跨模态嵌入在多模态对齐任务中表现出色，但在特定模态任务上可能不如单模态嵌入；而单模态嵌入虽在其领域表现优异，但缺乏跨模态对齐能力。因此需要一种方法统一这两类嵌入的优势。

Method: 提出RP-KrossFuse方法，利用基于随机投影的Kronecker积将跨模态嵌入与单模态嵌入整合，并通过随机Fourier特征支持高效的大规模实现。

Result: 实验表明，RP-KrossFuse结合CLIP嵌入与单模态图像和文本嵌入后，在特定模态任务上达到具有竞争力的性能，同时保留了跨模态对齐能力。

Conclusion: RP-KrossFuse有效弥合了跨模态和单模态嵌入之间的差距，为多模态任务提供了新的解决方案。

Abstract: Cross-modal embeddings, such as CLIP, BLIP and their variants, have achieved
promising results in aligning representations across modalities. However, these
embeddings could underperform compared to state-of-the-art single-modality
embeddings on modality-specific tasks. On the other hand, single-modality
embeddings excel in their domains but lack cross-modal alignment capabilities.
In this work, we focus on the problem of unifying cross-modality and
single-modality embeddings to achieve the performance of modality-expert
embedding within individual modalities while preserving cross-modal alignment.
To this end, we propose RP-KrossFuse, a method that leverages a random
projection-based Kronecker product to integrate cross-modal embeddings with
single-modality embeddings. RP-KrossFuse aims to fuse the sample-pairwise
similarity scores of the fused embeddings and operates efficiently in a
specified kernel space and supports scalable implementations via random Fourier
features for shift-invariant kernels such as the Gaussian kernel. We
demonstrate the effectiveness of RP-KrossFuse through several numerical
experiments, combining CLIP embeddings with uni-modal image and text
embeddings. Our numerical results indicate that RP-KrossFuse achieves
competitive modality-specific performance while retaining cross-modal
alignment, bridging the gap between cross-modal and single-modality embeddings.

</details>


### [119] [JoFormer (Journey-based Transformer): Theory and Empirical Analysis on the Tiny Shakespeare Dataset](https://arxiv.org/abs/2506.08652)
*Mahesh Godavarti*

Main category: cs.LG

TL;DR: JoFormer是一种基于旅程的Transformer架构，通过可学习的方向变换来表示相对位置，从而更有效地整合位置信息。实验表明，JoFormer在Tiny Shakespeare任务上相较于RoFormer基线模型具有更低的困惑度和更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 尽管Transformers在序列建模方面取得了显著成功，但如何有效结合位置信息仍然是一个挑战。为了解决这一问题，提出了一种新的基于旅程的Transformer架构。

Method: JoFormer利用最近提出的非交换代数，通过沿输入序列顺序组合可学习的方向变换来表示相对位置。其注意力机制从第一性原理推导，并概括了现有的方法如旋转变换。

Result: 在Tiny Shakespeare字符级语言建模任务中，JoFormer相较于RoFormer基线模型表现出更低的困惑度和更快的收敛速度。

Conclusion: JoFormer提供了一种有原则的方法将位置结构整合到Transformer架构中，展现出作为更表达性架构概念验证的潜力。

Abstract: Transformers have demonstrated remarkable success in sequence modeling, yet
effectively incorporating positional information remains a challenging and
active area of research. In this paper, we introduce JoFormer, a journey-based
Transformer architecture grounded in a recently proposed non-commutative
algebra for composing transformations across positions. JoFormer represents
relative positions through learnable directional transforms that are
sequentially composed along the input, thereby extending and generalizing
existing approaches based on relative position representations. We derive the
JoFormer attention mechanism from first principles and show that it subsumes
standard methods such as rotary transformations as special cases. To evaluate
its effectiveness, we compare JoFormer to the RoFormer baseline on the Tiny
Shakespeare character-level language modeling task. Our results demonstrate
that
  JoFormer consistently achieves lower perplexity and faster convergence,
highlighting the advantages of its more expressive, journey-based treatment of
position. Notably, the per-token JoFormer is still a primitive, conceptual
variant with layer-independent angles, yet it already demonstrates strong
performance-underscoring its promise as a proof of concept for more expressive
architectures. We conclude by discussing how JoFormer offers a principled
approach to integrating positional structure into Transformer architectures.
The code used in this work is available at
https://github.com/mahesh-godavarti/joformer.

</details>


### [120] [When Simple Model Just Works: Is Network Traffic Classification in Crisis?](https://arxiv.org/abs/2506.08655)
*Kamil Jerabek,Jan Luxemburk,Richard Plny,Josef Koumar,Jaroslav Pesek,Karel Hynek*

Main category: cs.LG

TL;DR: The paper investigates why a simple k-NN baseline performs well in network traffic classification, finding that dataset redundancy contributes to overestimated model performance. It questions standard ML practices for TC and proposes new directions.


<details>
  <summary>Details</summary>
Motivation: To understand why a simple k-NN baseline using packet sequences metadata can perform as well as or better than complex neural networks in network traffic classification.

Method: Evaluate the k-NN baseline across 12 datasets and 15 TC tasks, analyze dataset characteristics, and propose new task formulation and evaluation methods.

Result: Most datasets contain over 50% redundant samples, which leads to overestimated model performance and reduced theoretical maximum accuracy when identical flows have conflicting labels.

Conclusion: Standard machine learning practices may be unsuited for network traffic classification; new task formulation and evaluation methods are proposed.

Abstract: Machine learning has been applied to network traffic classification (TC) for
over two decades. While early efforts used shallow models, the latter 2010s saw
a shift toward complex neural networks, often reporting near-perfect accuracy.
However, it was recently revealed that a simple k-NN baseline using packet
sequences metadata (sizes, times, and directions) can be on par or even
outperform more complex methods. In this paper, we investigate this phenomenon
further and evaluate this baseline across 12 datasets and 15 TC tasks, and
investigate why it performs so well. Our analysis shows that most datasets
contain over 50% redundant samples (identical packet sequences), which
frequently appear in both training and test sets due to common splitting
practices. This redundancy can lead to overestimated model performance and
reduce the theoretical maximum accuracy when identical flows have conflicting
labels. Given its distinct characteristics, we further argue that standard
machine learning practices adapted from domains like NLP or computer vision may
be ill-suited for TC. Finally, we propose new directions for task formulation
and evaluation to address these challenges and help realign the field.

</details>


### [121] [Towards Robust Real-World Multivariate Time Series Forecasting: A Unified Framework for Dependency, Asynchrony, and Missingness](https://arxiv.org/abs/2506.08660)
*Jinkwan Jang,Hyungjin Park,Jinmyeong Choi,Taesup Kim*

Main category: cs.LG

TL;DR: The paper proposes ChannelTokenFormer, a Transformer-based model designed to capture cross-channel interactions, handle asynchronous sampling, and manage missing values for robust time series forecasting in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Real-world time series data have complexities such as multivariate nature, inter-channel dependencies, asynchronous sampling, and missing values which current models fail to address effectively due to oversimplified assumptions.

Method: ChannelTokenFormer is a Transformer-based forecasting model with a flexible architecture that explicitly captures cross-channel interactions, accommodates channel-wise asynchronous sampling, and effectively handles missing values.

Result: Extensive experiments on four datasets (three benchmark datasets modified to reflect practical settings and one real-world industrial dataset) demonstrate superior robustness and accuracy of ChannelTokenFormer under challenging real-world conditions.

Conclusion: ChannelTokenFormer successfully addresses the challenges of channel dependency, sampling asynchrony, and missingness in real-world time series data, providing robust and reliable forecasting.

Abstract: Real-world time series data are inherently multivariate, often exhibiting
complex inter-channel dependencies. Each channel is typically sampled at its
own period and is prone to missing values due to various practical and
operational constraints. These characteristics pose fundamental challenges
related to channel dependency, sampling asynchrony, and missingness, all of
which must be addressed to enable robust and reliable forecasting in practical
settings. However, most existing architectures are built on oversimplified
assumptions, such as identical sampling periods across channels and fully
observed inputs at test time, which often do not hold in real-world scenarios.
To bridge this gap, we propose ChannelTokenFormer, a Transformer-based
forecasting model with a flexible architecture designed to explicitly capture
cross-channel interactions, accommodate channel-wise asynchronous sampling, and
effectively handle missing values. Extensive experiments on three benchmark
datasets modified to reflect practical settings, along with one real-world
industrial dataset, demonstrate the superior robustness and accuracy of
ChannelTokenFormer under challenging real-world conditions.

</details>


### [122] [Optimizing Learned Image Compression on Scalar and Entropy-Constraint Quantization](https://arxiv.org/abs/2506.08662)
*Florian Borzechowski,Michael Schäfer,Heiko Schwarz,Jonathan Pfaff,Detlev Marpe,Thomas Wiegand*

Main category: cs.LG

TL;DR: An additional finetuning training step is proposed to retrain parts of the network on quantized latents, yielding additional coding gain without increasing inference complexity.


<details>
  <summary>Details</summary>
Motivation: The problem of taking quantization into account during the training process in image compression with variational autoencoders leads to suboptimal networks due to incorrect modeling of quantization noise.

Method: Propose an additional finetuning training step where parts of the network are retrained on quantized latents obtained at the inference stage after conventional end-to-end training.

Result: Retraining on correctly quantized data yields additional coding gain for both uniform scalar and entropy-constraint quantization, obtaining average savings between 1% and 2% for the Kodak test set, and up to 2.2% for the TecNick test set in terms of Bjøntegaard-Delta bitrate.

Conclusion: The proposed method achieves additional coding gain without increasing inference complexity.

Abstract: The continuous improvements on image compression with variational
autoencoders have lead to learned codecs competitive with conventional
approaches in terms of rate-distortion efficiency. Nonetheless, taking the
quantization into account during the training process remains a problem, since
it produces zero derivatives almost everywhere and needs to be replaced with a
differentiable approximation which allows end-to-end optimization. Though there
are different methods for approximating the quantization, none of them model
the quantization noise correctly and thus, result in suboptimal networks.
Hence, we propose an additional finetuning training step: After conventional
end-to-end training, parts of the network are retrained on quantized latents
obtained at the inference stage. For entropy-constraint quantizers like
Trellis-Coded Quantization, the impact of the quantizer is particularly
difficult to approximate by rounding or adding noise as the quantized latents
are interdependently chosen through a trellis search based on both the entropy
model and a distortion measure. We show that retraining on correctly quantized
data consistently yields additional coding gain for both uniform scalar and
especially for entropy-constraint quantization, without increasing inference
complexity. For the Kodak test set, we obtain average savings between 1% and
2%, and for the TecNick test set up to 2.2% in terms of Bj{\o}ntegaard-Delta
bitrate.

</details>


### [123] [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://arxiv.org/abs/2506.08669)
*Dongge Han,Menglin Xia,Daniel Madrigal Diaz,Samuel Kessler,Ankur Mallick,Xuchao Zhang,Mirian Del Carmen Hipolito Garcia,Jin Xu,Victor Rühle,Saravan Rajmohan*

Main category: cs.LG

TL;DR: This paper presents a framework to boost SLMs reasoning capabilities via LLM generated blueprints and a prompt template search mechanism, enhancing performance in various tasks without increasing model size or training.


<details>
  <summary>Details</summary>
Motivation: Small language models (SLMs) have limited reasoning capabilities and are sensitive to prompt variations due to their restricted capacity compared to large language models (LLMs).

Method: The method involves creating a novel framework that leverages LLM generated blueprints for structured reasoning guides and integrates a prompt template search mechanism to reduce sensitivity to prompt changes.

Result: The framework successfully improves the performance of SLMs in math (GSM8K), coding (MBPP), and logic reasoning (BBH) tasks.

Conclusion: This approach enhances SLM reasoning without increasing model size or requiring additional training, providing a lightweight solution ideal for on-device or resource-constrained settings.

Abstract: Small language models (SLMs) offer promising and efficient alternatives to
large language models (LLMs). However, SLMs' limited capacity restricts their
reasoning capabilities and makes them sensitive to prompt variations. To
address these challenges, we propose a novel framework that enhances SLM
reasoning capabilities through LLM generated blueprints. The blueprints provide
structured, high-level reasoning guides that help SLMs systematically tackle
related problems. Furthermore, our framework integrates a prompt template
search mechanism to mitigate the SLMs' sensitivity to prompt variations. Our
framework demonstrates improved SLM performance across various tasks, including
math (GSM8K), coding (MBPP), and logic reasoning (BBH). Our approach improves
the reasoning capabilities of SLMs without increasing model size or requiring
additional training, offering a lightweight and deployment-friendly solution
for on-device or resource-constrained environments.

</details>


### [124] [Towards Fair Representation: Clustering and Consensus](https://arxiv.org/abs/2506.08673)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien Long Nguyen,Romina Nobahari*

Main category: cs.LG

TL;DR: 研究了通过公平聚类视角的共识聚类问题，提出首个常数因子近似算法，并探讨了现有聚簇修改为公平聚类的方法。


<details>
  <summary>Details</summary>
Motivation: 共识聚类旨在将数据集的多个输入聚类整合为一个代表数据集体结构的聚类，但未考虑公平性。本文结合公平聚类理念，确保每个受保护群体在每个簇内都有比例代表性，从而解决这一问题。

Method: 1. 提出一种共识聚类方法，该方法不仅具有代表性，而且对特定保护属性公平。
2. 研究如何最小修改现有聚类以强制实现公平性，开发针对不同场景的算法：
   - 对于等群体表示的数据集，开发最优算法。
   - 对于不同比例的两组大小的更一般情况，开发近线性时间常数因子近似算法。
3. 证明对于两个不等大小的组，该问题是NP难的。

Result: 1. 提出了首个解决此问题的常数因子近似算法。
2. 开发了针对不同数据集特性的有效算法。
3. 证明了问题的理论复杂性（NP难）。

Conclusion: 本文首次解决了寻找既具代表性又公平的共识聚类的问题，提供了常数因子近似算法并证明了问题的复杂性。这些结果可能对其他尚未有公平变体近似保证的聚类问题产生广泛影响。

Abstract: Consensus clustering, a fundamental task in machine learning and data
analysis, aims to aggregate multiple input clusterings of a dataset,
potentially based on different non-sensitive attributes, into a single
clustering that best represents the collective structure of the data. In this
work, we study this fundamental problem through the lens of fair clustering, as
introduced by Chierichetti et al. [NeurIPS'17], which incorporates the
disparate impact doctrine to ensure proportional representation of each
protected group in the dataset within every cluster. Our objective is to find a
consensus clustering that is not only representative but also fair with respect
to specific protected attributes. To the best of our knowledge, we are the
first to address this problem and provide a constant-factor approximation.
  As part of our investigation, we examine how to minimally modify an existing
clustering to enforce fairness -- an essential postprocessing step in many
clustering applications that require fair representation. We develop an optimal
algorithm for datasets with equal group representation and near-linear time
constant factor approximation algorithms for more general scenarios with
different proportions of two group sizes. We complement our approximation
result by showing that the problem is NP-hard for two unequal-sized groups.
Given the fundamental nature of this problem, we believe our results on Closest
Fair Clustering could have broader implications for other clustering problems,
particularly those for which no prior approximation guarantees exist for their
fair variants.

</details>


### [125] [Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling](https://arxiv.org/abs/2506.08681)
*Phuc Minh Nguyen,Ngoc-Hieu Nguyen,Duy H. M. Nguyen,Anji Liu,An Mai,Binh T. Nguyen,Daniel Sonntag,Khoa D. Doan*

Main category: cs.LG

TL;DR: Direct Alignment Algorithms (DAAs) like Direct Preference Optimization (DPO) face over-optimization issues where models drift from reference policies. This paper introduces IS-DAAs, an importance-sampling method that adjusts the DAA objective with an importance ratio and clips it to reduce variance. Experiments show IS-DAAs mitigate over-optimization better than other methods under low regularization.


<details>
  <summary>Details</summary>
Motivation: To address the over-optimization problem in DAAs which causes models to drift from reference policies leading to performance degradation.

Method: Propose IS-DAAs by incorporating an importance ratio into the DAA objective and clipping the ratio to control variance.

Result: IS-DAAs effectively mitigate over-optimization particularly under low regularization strength, outperforming other methods aimed at solving this issue.

Conclusion: IS-DAAs provide a solution to the over-optimization challenge in DAAs improving model alignment with human values.

Abstract: Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization
(DPO) have emerged as alternatives to the standard Reinforcement Learning from
Human Feedback (RLHF) for aligning large language models (LLMs) with human
values. However, these methods are more susceptible to over-optimization, in
which the model drifts away from the reference policy, leading to degraded
performance as training progresses. This paper proposes a novel
importance-sampling approach to mitigate the over-optimization problem of
offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective
with an importance ratio that accounts for the reference policy distribution.
IS-DAAs additionally avoid the high variance issue associated with importance
sampling by clipping the importance ratio to a maximum value. Our extensive
experiments demonstrate that IS-DAAs can effectively mitigate
over-optimization, especially under low regularization strength, and achieve
better performance than other methods designed to address this problem. Our
implementations are provided publicly at this link.

</details>


### [126] [Variational Autoencoder-Based Approach to Latent Feature Analysis on Efficient Representation of Power Load Monitoring Data](https://arxiv.org/abs/2506.08698)
*Boyu Xie,Tangtang Xie*

Main category: cs.LG

TL;DR: The paper proposes VAE-LF, a model based on Variational Autoencoder for handling high-dimensional and incomplete power load monitoring data. It efficiently represents and complements missing data, outperforming benchmarks in experiments with the UK-DALE dataset.


<details>
  <summary>Details</summary>
Motivation: Smart grids have led to high-dimensional and incomplete power load monitoring data, which poses challenges for power load forecasting models.

Method: The VAE-LF model uses an Encoder-Decoder structure to learn a low-dimensional latent representation of the data by splitting high-dimensional and incomplete power load monitoring data into vectors and sequentially feeding them into the model to generate complementary data.

Result: Experiments on the UK-DALE dataset demonstrated that VAE-LF outperforms other benchmark models in both 5% and 10% sparsity test cases with significantly lower RMSE and MAE, particularly excelling with low sparsity ratio data.

Conclusion: VAE-LF provides an efficient data-completion solution for electric load management in smart grids.

Abstract: With the development of smart grids, High-Dimensional and Incomplete (HDI)
Power Load Monitoring (PLM) data challenges the performance of Power Load
Forecasting (PLF) models. In this paper, we propose a potential
characterization model VAE-LF based on Variational Autoencoder (VAE) for
efficiently representing and complementing PLM missing data. VAE-LF learns a
low-dimensional latent representation of the data using an Encoder-Decoder
structure by splitting the HDI PLM data into vectors and feeding them
sequentially into the VAE-LF model, and generates the complementary data.
Experiments on the UK-DALE dataset show that VAE-LF outperforms other benchmark
models in both 5% and 10% sparsity test cases, with significantly lower RMSE
and MAE, and especially outperforms on low sparsity ratio data. The method
provides an efficient data-completion solution for electric load management in
smart grids.

</details>


### [127] [Breaking the ICE: Exploring promises and challenges of benchmarks for Inference Carbon & Energy estimation for LLMs](https://arxiv.org/abs/2506.08727)
*Samarth Sikand,Rohit Mehra,Priyavanshi Pathania,Nikhil Bamby,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.LG

TL;DR: Generative AI, including Large Language Models (LLMs), has a significant impact on energy consumption and carbon emissions, posing challenges to sustainability goals. Existing tools for monitoring and estimating energy usage have limitations such as requiring high input data, being intrusive, and having a large error margin. To address these issues, the authors propose R-ICE, a framework that uses existing benchmarks to estimate carbon emissions at the prompt level for LLM inferences. This approach is non-intrusive and supports applications like dynamic LLM routing and carbon accounting. Initial validation results are promising, indicating the potential of benchmark-based modeling for emission estimation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the need for better tools to monitor and estimate the energy consumption and carbon emissions associated with the use of Large Language Models (LLMs). Current tools suffer from drawbacks such as requiring extensive input data, being intrusive, and having high error margins. There is a clear demand for a more practical and accurate method to support sustainability goals.

Method: The method involves leveraging existing state-of-the-art (SOTA) benchmarks related to LLMs to develop a framework called R-ICE. This framework estimates prompt-level inference carbon emissions by using benchmark data points. It aims to provide a non-intrusive way to estimate emissions, supporting emerging use-cases such as dynamic LLM routing and carbon accounting.

Result: The validation results of the R-ICE framework are promising, suggesting that benchmark-based modeling holds great potential for accurately estimating inference emissions. This approach could lead to further exploration and advancements in emission estimation techniques.

Conclusion: The conclusion drawn from this study is that utilizing benchmark-based modeling for estimating carbon emissions from LLM inferences is a viable and promising direction. The proposed R-ICE framework demonstrates potential in providing accurate, non-intrusive emission estimations, which could facilitate various sustainability-related applications.

Abstract: While Generative AI stands to be one of the fastest adopted technologies
ever, studies have made evident that the usage of Large Language Models (LLMs)
puts significant burden on energy grids and our environment. It may prove a
hindrance to the Sustainability goals of any organization. A crucial step in
any Sustainability strategy is monitoring or estimating the energy consumption
of various components. While there exist multiple tools for monitoring energy
consumption, there is a dearth of tools/frameworks for estimating the
consumption or carbon emissions. Current drawbacks of both monitoring and
estimation tools include high input data points, intrusive nature, high error
margin, etc. We posit that leveraging emerging LLM benchmarks and related data
points can help overcome aforementioned challenges while balancing accuracy of
the emission estimations. To that extent, we discuss the challenges of current
approaches and present our evolving framework, R-ICE, which estimates prompt
level inference carbon emissions by leveraging existing state-of-the-art(SOTA)
benchmark. This direction provides a more practical and non-intrusive way to
enable emerging use-cases like dynamic LLM routing, carbon accounting, etc. Our
promising validation results suggest that benchmark-based modelling holds great
potential for inference emission estimation and warrants further exploration
from the scientific community.

</details>


### [128] [Exploration by Random Reward Perturbation](https://arxiv.org/abs/2506.08737)
*Haozhe Ma,Guoji Fu,Zhengding Luo,Jiele Wu,Tze-Yun Leong*

Main category: cs.LG

TL;DR: An abstract about Random Reward Perturbation (RRP) in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To enhance policy diversity during training and expand the range of exploration in reinforcement learning.

Method: Introduce RRP, which adds zero-mean noise to environmental rewards and is compatible with action-perturbation-based exploration strategies.

Result: Experiments show that RRP significantly improves the performance of Proximal Policy Optimization and Soft Actor-Critic, achieving higher sample efficiency and escaping local optima.

Conclusion: RRP is a general, lightweight strategy that can be easily integrated into existing RL algorithms and has a theoretical connection between reward shaping and noise-driven exploration.

Abstract: We introduce Random Reward Perturbation (RRP), a novel exploration strategy
for reinforcement learning (RL). Our theoretical analyses demonstrate that
adding zero-mean noise to environmental rewards effectively enhances policy
diversity during training, thereby expanding the range of exploration. RRP is
fully compatible with the action-perturbation-based exploration strategies,
such as $\epsilon$-greedy, stochastic policies, and entropy regularization,
providing additive improvements to exploration effects. It is general,
lightweight, and can be integrated into existing RL algorithms with minimal
implementation effort and negligible computational overhead. RRP establishes a
theoretical connection between reward shaping and noise-driven exploration,
highlighting their complementary potential. Experiments show that RRP
significantly boosts the performance of Proximal Policy Optimization and Soft
Actor-Critic, achieving higher sample efficiency and escaping local optima
across various tasks, under both sparse and dense reward scenarios.

</details>


### [129] [Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports](https://arxiv.org/abs/2506.08740)
*Sidhika Balachandar,Shuvom Sadhuka,Bonnie Berger,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: The paper proposes a multiview, multioutput GNN-based model for urban incident prediction using both government inspection ratings and crowdsourced reports. It also provides a dataset of NYC incidents and shows the model's effectiveness on real and semi-synthetic data.


<details>
  <summary>Details</summary>
Motivation: Government officials need to know where incidents occur in cities, but inspection ratings are sparse and crowdsourced reports may be biased. There is a need for a model that can effectively predict the true latent state of incidents using these heterogeneous data sources.

Method: The authors developed a multiview, multioutput GNN-based model that integrates unbiased rating data and biased reporting data. They also collected and standardized a large dataset of crowdsourced reports and government inspection ratings from New York City.

Result: The model outperforms those using only reporting or rating data, especially when rating data is sparse and reports are predictive of ratings. The study also quantified demographic biases in crowdsourced reporting.

Conclusion: This approach offers a widely applicable method for latent state prediction using heterogeneous, sparse, and biased data.

Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal
forecasting, such as predicting infrastructure problems. In this setting,
government officials wish to know in which neighborhoods incidents like
potholes or rodent issues occur. The true state of incidents (e.g., street
conditions) for each neighborhood is observed via government inspection
ratings. However, these ratings are only conducted for a sparse set of
neighborhoods and incident types. We also observe the state of incidents via
crowdsourced reports, which are more densely observed but may be biased due to
heterogeneous reporting behavior. First, for such settings, we propose a
multiview, multioutput GNN-based model that uses both unbiased rating data and
biased reporting data to predict the true latent state of incidents. Second, we
investigate a case study of New York City urban incidents and collect,
standardize, and make publicly available a dataset of 9,615,863 crowdsourced
reports and 1,041,415 government inspection ratings over 3 years and across 139
types of incidents. Finally, we show on both real and semi-synthetic data that
our model can better predict the latent state compared to models that use only
reporting data or models that use only rating data, especially when rating data
is sparse and reports are predictive of ratings. We also quantify demographic
biases in crowdsourced reporting, e.g., higher-income neighborhoods report
problems at higher rates. Our analysis showcases a widely applicable approach
for latent state prediction using heterogeneous, sparse, and biased data.

</details>


### [130] [On the Stability of the Jacobian Matrix in Deep Neural Networks](https://arxiv.org/abs/2506.08764)
*Benjamin Dadoun,Soufiane Hayou,Hanan Salam,Mohamed El Amine Seddik,Pierre Youssef*

Main category: cs.LG

TL;DR: 这篇论文通过利用随机矩阵理论的最新进展，建立了一个适用于深度神经网络的一般稳定性定理。该定理不仅考虑了稀疏性（如修剪引入的稀疏性），还考虑了非独立同分布、弱相关的权重（例如训练过程中产生的权重）。这项工作扩展了现代具有结构化和相关随机性的神经网络初始化方案的理论基础。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络随着深度增加，容易出现梯度爆炸或消失的问题，这与输入-输出雅可比矩阵的谱行为密切相关。尽管先前的研究已经确定了一些关键的初始化策略以确保雅可比矩阵的稳定性，但这些分析通常局限于全连接网络且权重为独立同分布的情况。

Method: 作者提出了一种新的稳定性定理，适用于更广泛的网络模型。这个定理利用了随机矩阵理论的最新成果，能够处理稀疏性和非独立同分布、弱相关的权重情况，比如由修剪引起的稀疏性和训练过程中产生的权重相关性。

Result: 该研究提供了关于谱稳定性的严格理论保证，适用于更广泛的网络模型，包括那些具有结构化和相关随机性的现代神经网络。

Conclusion: 这项工作扩展了深度神经网络初始化方案的理论基础，使其可以应用于更复杂的网络架构和实际场景中。

Abstract: Deep neural networks are known to suffer from exploding or vanishing
gradients as depth increases, a phenomenon closely tied to the spectral
behavior of the input-output Jacobian. Prior work has identified critical
initialization schemes that ensure Jacobian stability, but these analyses are
typically restricted to fully connected networks with i.i.d. weights. In this
work, we go significantly beyond these limitations: we establish a general
stability theorem for deep neural networks that accommodates sparsity (such as
that introduced by pruning) and non-i.i.d., weakly correlated weights (e.g.
induced by training). Our results rely on recent advances in random matrix
theory, and provide rigorous guarantees for spectral stability in a much
broader class of network models. This extends the theoretical foundation for
initialization schemes in modern neural networks with structured and dependent
randomness.

</details>


### [131] [Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)
*Luca Beurer-Kellner,Beat Buesser Ana-Maria Creţu,Edoardo Debenedetti,Daniel Dobos,Daniel Fabian,Marc Fischer,David Froelicher,Kathrin Grosse,Daniel Naeff,Ezinwanne Ozoani,Andrew Paverd,Florian Tramèr,Václav Volhejn*

Main category: cs.LG

TL;DR: 本文提出了一套构建对提示注入攻击具有可证明抵抗力的AI代理的原则性设计模式，并通过案例研究分析了这些模式在实用性和安全性方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着由大型语言模型驱动的AI代理变得越来越多功能化，能够处理广泛的任务，确保其安全性已成为一个关键挑战，尤其是提示注入攻击这一威胁。

Method: 作者提出了一套原则性的设计模式来构建对提示注入攻击具有抵抗力的AI代理，并对其进行了系统分析。

Result: 这些设计模式被证明在提供安全性的同时也存在实用性的权衡，并且通过一系列案例研究展示了其在现实世界中的适用性。

Conclusion: 构建安全的AI代理需要在实用性和安全性之间做出权衡，而本文提出的设计模式为实现这一目标提供了有价值的指导。

Abstract: As AI agents powered by Large Language Models (LLMs) become increasingly
versatile and capable of addressing a broad spectrum of tasks, ensuring their
security has become a critical challenge. Among the most pressing threats are
prompt injection attacks, which exploit the agent's resilience on natural
language inputs -- an especially dangerous threat when agents are granted tool
access or handle sensitive information. In this work, we propose a set of
principled design patterns for building AI agents with provable resistance to
prompt injection. We systematically analyze these patterns, discuss their
trade-offs in terms of utility and security, and illustrate their real-world
applicability through a series of case studies.

</details>


### [132] [IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)](https://arxiv.org/abs/2506.08844)
*Siyi Sun,David Antony Selby,Yunchuan Huang,Sebastian Vollmer,Seth Flaxman,Anisoara Calinescu*

Main category: cs.LG

TL;DR: 研究使用世界银行公开的合成数据集创建了IMAGIC-500数据集，该数据集包含50万个人和约10万户家庭的19个社会经济特征。基于此数据集，在不同缺失机制（MCAR、MAR、MNAR）和缺失率下进行了全面的缺失数据填补基准测试，评估了统计学、传统机器学习和深度学习填补技术的准确性、计算效率及对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 实际的社会经济数据集受数据保护协议限制，难以公开分享，导致缺失数据填补方法在该领域的系统评估缺乏足够的基准。同时，公开可用的合成数据集也十分有限。

Method: 利用世界银行公开的合成数据集构建IMAGIC-500数据集，并在此基础上针对多种缺失机制和缺失率进行缺失数据填补基准测试，评估指标包括连续和分类变量的填补准确性、计算效率以及对下游预测任务的影响。

Result: 揭示了统计学、传统机器学习和深度学习（包括最新的扩散模型方法）填补技术各自的优缺点。

Conclusion: IMAGIC-500数据集和基准测试为推动稳健的缺失数据填补算法的发展以及促进可重复的社会科学研究提供了重要支持。

Abstract: Missing data imputation in tabular datasets remains a pivotal challenge in
data science and machine learning, particularly within socioeconomic research.
However, real-world socioeconomic datasets are typically subject to strict data
protection protocols, which often prohibit public sharing, even for synthetic
derivatives. This severely limits the reproducibility and accessibility of
benchmark studies in such settings. Further, there are very few publicly
available synthetic datasets. Thus, there is limited availability of benchmarks
for systematic evaluation of imputation methods on socioeconomic datasets,
whether real or synthetic. In this study, we utilize the World Bank's publicly
available synthetic dataset, Synthetic Data for an Imaginary Country, which
closely mimics a real World Bank household survey while being fully public,
enabling broad access for methodological research. With this as a starting
point, we derived the IMAGIC-500 dataset: we select a subset of 500k
individuals across approximately 100k households with 19 socioeconomic
features, designed to reflect the hierarchical structure of real-world
household surveys. This paper introduces a comprehensive missing data
imputation benchmark on IMAGIC-500 under various missing mechanisms (MCAR, MAR,
MNAR) and missingness ratios (10\%, 20\%, 30\%, 40\%, 50\%). Our evaluation
considers the imputation accuracy for continuous and categorical variables,
computational efficiency, and impact on downstream predictive tasks, such as
estimating educational attainment at the individual level. The results
highlight the strengths and weaknesses of statistical, traditional machine
learning, and deep learning imputation techniques, including recent
diffusion-based methods. The IMAGIC-500 dataset and benchmark aim to facilitate
the development of robust imputation algorithms and foster reproducible social
science research.

</details>


### [133] [Agile Reinforcement Learning for Real-Time Task Scheduling in Edge Computing](https://arxiv.org/abs/2506.08850)
*Amin Avan,Akramul Azim,Qusay Mahmoud*

Main category: cs.LG

TL;DR: In this paper, researchers tackle the challenge of task scheduling in edge computing for soft real-time applications. They propose Agile Reinforcement learning (aRL), which performs informed exploration and executes only relevant actions to enhance predictability and rapid adaptation. Experiments show that aRL outperforms baseline approaches in terms of hit-ratio and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Soft real-time applications are becoming increasingly complex, leading to significant challenges in task scheduling within edge computing environments. Current heuristic/metaheuristic algorithms and reinforcement learning methods face difficulties due to dynamic conditions, exponential search space growth, and prolonged learning times.

Method: The study introduces Agile Reinforcement learning (aRL), where the RL-agent conducts informed exploration and executes only relevant actions. This approach reduces extensive global action space and frequent random exploration, improving predictability and adaptation speed.

Result: Experimental results indicate that the combination of informed exploration and action-masking methods allows aRL to achieve a higher hit-ratio and faster convergence compared to baseline approaches.

Conclusion: Agile Reinforcement learning (aRL) is presented as a suitable candidate for scheduling tasks of soft real-time applications in edge computing, effectively addressing the complexity and dynamic conditions through enhanced predictability and rapid adaptation.

Abstract: Soft real-time applications are becoming increasingly complex, posing
significant challenges for scheduling offloaded tasks in edge computing
environments while meeting task timing constraints. Moreover, the exponential
growth of the search space, presence of multiple objectives and parameters, and
highly dynamic nature of edge computing environments further exacerbate the
complexity of task scheduling. As a result, schedulers based on heuristic and
metaheuristic algorithms frequently encounter difficulties in generating
optimal or near-optimal task schedules due to their constrained ability to
adapt to the dynamic conditions and complex environmental characteristics of
edge computing. Accordingly, reinforcement learning algorithms have been
incorporated into schedulers to address the complexity and dynamic conditions
inherent in task scheduling in edge computing. However, a significant
limitation of reinforcement learning algorithms is the prolonged learning time
required to adapt to new environments and to address medium- and large-scale
problems. This challenge arises from the extensive global action space and
frequent random exploration of irrelevant actions. Therefore, this study
proposes Agile Reinforcement learning (aRL), in which the RL-agent performs
informed exploration and executes only relevant actions. Consequently, the
predictability of the RL-agent is enhanced, leading to rapid adaptation and
convergence, which positions aRL as a suitable candidate for scheduling the
tasks of soft real-time applications in edge computing. The experiments
demonstrate that the combination of informed exploration and action-masking
methods enables aRL to achieve a higher hit-ratio and converge faster than the
baseline approaches.

</details>


### [134] [Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery](https://arxiv.org/abs/2506.08871)
*Victor M. Tenorio,Madeline Navarro,Samuel Rey,Santiago Segarra,Antonio G. Marques*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) often struggle with heterophilic data, where connected nodes may have dissimilar labels. This paper proposes creating alternative graph structures by linking nodes with similar structural attributes and introduces Structure-Guided GNN (SG-GNN), which processes the original graph alongside the newly created structural graphs, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: GNNs typically assume homophily and rely on local message passing, struggling with heterophilic data. The authors aim to improve GNN performance on such data by creating alternative graph structures with higher label homophily.

Method: The method involves theoretically proving that GNN performance can be improved by utilizing graphs with fewer false positive edges and considering multiple graph views. They then introduce SG-GNN, an architecture that processes the original graph alongside newly created structural graphs, adaptively learning to weigh their contributions.

Result: Extensive experiments on various benchmark datasets, particularly those with heterophilic characteristics, demonstrate that SG-GNN achieves state-of-the-art or highly competitive performance.

Conclusion: The conclusion is that exploiting structural information to guide GNNs, as done in SG-GNN, is effective in improving performance on heterophilic data.

Abstract: Graph Neural Networks (GNNs) often struggle with heterophilic data, where
connected nodes may have dissimilar labels, as they typically assume homophily
and rely on local message passing. To address this, we propose creating
alternative graph structures by linking nodes with similar structural
attributes (e.g., role-based or global), thereby fostering higher label
homophily on these new graphs. We theoretically prove that GNN performance can
be improved by utilizing graphs with fewer false positive edges (connections
between nodes of different classes) and that considering multiple graph views
increases the likelihood of finding such beneficial structures. Building on
these insights, we introduce Structure-Guided GNN (SG-GNN), an architecture
that processes the original graph alongside the newly created structural
graphs, adaptively learning to weigh their contributions. Extensive experiments
on various benchmark datasets, particularly those with heterophilic
characteristics, demonstrate that our SG-GNN achieves state-of-the-art or
highly competitive performance, highlighting the efficacy of exploiting
structural information to guide GNNs.

</details>


### [135] [Filling in the Blanks: Applying Data Imputation in incomplete Water Metering Data](https://arxiv.org/abs/2506.08882)
*Dimitrios Amaxilatis,Themistoklis Sarantakos,Ioannis Chatzigiannakis,Georgios Mylonas*

Main category: cs.LG

TL;DR: Recent data imputation techniques are explored to improve water distribution networks' management via smart water meters. Results show effective data imputation can enhance insights from water consumption data for applications like leak detection and predictive maintenance scheduling.


<details>
  <summary>Details</summary>
Motivation: To address the issue of data gaps caused by technical problems in smart water meters which affect operational decisions and efficiency in water distribution networks.

Method: Comparison of various data imputation methods including k-Nearest Neighbors, MissForest, Transformers, and Recurrent Neural Networks.

Result: Effective data imputation methods can significantly improve the quality of insights derived from water consumption data, enhancing accuracy and reliability for applications such as leak detection and predictive maintenance scheduling.

Conclusion: Applying advanced data imputation techniques can lead to better management and monitoring of water distribution networks through improved smart water meter data.

Abstract: In this work, we explore the application of recent data imputation techniques
to enhance monitoring and management of water distribution networks using smart
water meters, based on data derived from a real-world IoT water grid monitoring
deployment. Despite the detailed data produced by such meters, data gaps due to
technical issues can significantly impact operational decisions and efficiency.
Our results, by comparing various imputation methods, such as k-Nearest
Neighbors, MissForest, Transformers, and Recurrent Neural Networks, indicate
that effective data imputation can substantially enhance the quality of the
insights derived from water consumption data as we study their effect on
accuracy and reliability of water metering data to provide solutions in
applications like leak detection and predictive maintenance scheduling.

</details>


### [136] [InfoDPCCA: Information-Theoretic Dynamic Probabilistic Canonical Correlation Analysis](https://arxiv.org/abs/2506.08884)
*Shiqin Tang,Shujian Yu*

Main category: cs.LG

TL;DR: InfoDPCCA is a novel dynamic probabilistic CCA framework that extracts shared and separate latent representations from two interdependent sequences using an information-theoretic objective. It introduces a two-step training scheme and residual connections, showing strong performance in representation learning on synthetic and medical fMRI data.


<details>
  <summary>Details</summary>
Motivation: Extract meaningful latent representations from high-dimensional sequential data via Canonical Correlation Analysis (CCA) while balancing compression and predictive sufficiency.

Method: Leverages a novel information-theoretic objective to extract shared and separate latent representations. Introduces a two-step training scheme and residual connection mechanism for enhanced stability.

Result: Outperforms previous dynamic CCA models in experiments with synthetic and medical fMRI data, excelling as a tool for representation learning.

Conclusion: InfoDPCCA provides a robust and interpretable approach to modeling interdependent sequences of observations.

Abstract: Extracting meaningful latent representations from high-dimensional sequential
data is a crucial challenge in machine learning, with applications spanning
natural science and engineering. We introduce InfoDPCCA, a dynamic
probabilistic Canonical Correlation Analysis (CCA) framework designed to model
two interdependent sequences of observations. InfoDPCCA leverages a novel
information-theoretic objective to extract a shared latent representation that
captures the mutual structure between the data streams and balances
representation compression and predictive sufficiency while also learning
separate latent components that encode information specific to each sequence.
Unlike prior dynamic CCA models, such as DPCCA, our approach explicitly
enforces the shared latent space to encode only the mutual information between
the sequences, improving interpretability and robustness. We further introduce
a two-step training scheme to bridge the gap between information-theoretic
representation learning and generative modeling, along with a residual
connection mechanism to enhance training stability. Through experiments on
synthetic and medical fMRI data, we demonstrate that InfoDPCCA excels as a tool
for representation learning. Code of InfoDPCCA is available at
https://github.com/marcusstang/InfoDPCCA.

</details>


### [137] [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](https://arxiv.org/abs/2506.08889)
*Yizhao Gao,Shuming Guo,Shijie Cao,Yuqing Xia,Yu Cheng,Lei Wang,Lingxiao Ma,Yutao Sun,Tianzhu Ye,Li Dong,Hayden Kwok-Hay So,Yu Hua,Ting Cao,Fan Yang,Mao Yang*

Main category: cs.LG

TL;DR: The paper introduces SeerAttention-R, a sparse attention framework for long decoding in reasoning models. It retains the self-distilled gating mechanism from SeerAttention but removes query pooling to suit auto-regressive decoding. Trained on 0.4B tokens, it maintains high reasoning accuracy within a 4K token budget and achieves significant speedups over FlashAttention-3 at 90% sparsity.


<details>
  <summary>Details</summary>
Motivation: To develop a sparse attention framework that can handle long decoding tasks in reasoning models efficiently while maintaining reasoning accuracy.

Method: SeerAttention-R is an extension of SeerAttention which incorporates a self-distilled gating mechanism for learning attention sparsity. It removes query pooling to allow for auto-regressive decoding and is designed as a lightweight plug-in that can be integrated into existing pretrained models without altering original parameters.

Result: SeerAttention-R achieves near-lossless reasoning accuracy with a 4K token budget when trained on just 0.4B tokens. Additionally, using TileLang, it demonstrates up to 9x speedups over FlashAttention-3 on H100 GPU at 90% sparsity.

Conclusion: SeerAttention-R provides a flexible and efficient solution for incorporating sparse attention into reasoning models, achieving both high accuracy and significant computational speedups.

Abstract: We introduce SeerAttention-R, a sparse attention framework specifically
tailored for the long decoding of reasoning models. Extended from
SeerAttention, SeerAttention-R retains the design of learning attention
sparsity through a self-distilled gating mechanism, while removing query
pooling to accommodate auto-regressive decoding. With a lightweight plug-in
gating, SeerAttention-R is flexible and can be easily integrated into existing
pretrained model without modifying the original parameters. We demonstrate that
SeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning
accuracy with 4K token budget in AIME benchmark under large sparse attention
block sizes (64/128). Using TileLang, we develop a highly optimized sparse
decoding kernel that achieves near-theoretical speedups of up to 9x over
FlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:
https://github.com/microsoft/SeerAttention.

</details>


### [138] [Intention-Conditioned Flow Occupancy Models](https://arxiv.org/abs/2506.08902)
*Chongyi Zheng,Seohong Park,Sergey Levine,Benjamin Eysenbach*

Main category: cs.LG

TL;DR: The paper introduces InFOM, a probabilistic model using flow matching to predict future states in reinforcement learning. It incorporates user intention as a latent variable and shows significant improvements over other pre-training methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of pre-training large models in reinforcement learning where actions have long-term dependencies, leveraging recent advances in generative AI for modeling complex distributions.

Method: Builds a probabilistic model called InFOM which predicts an agent's future states (occupancy measure) using flow matching, including a latent variable for user intention to enhance model expressivity and enable generalized policy improvement.

Result: Experiments on 36 state-based and 4 image-based benchmark tasks demonstrate a 1.8 times median improvement in returns and a 36% increase in success rates compared to alternative pre-training methods.

Conclusion: InFOM provides a compelling approach for pre-training in RL, achieving substantial performance improvements and offering a promising direction for addressing core challenges like sample efficiency and robustness.

Abstract: Large-scale pre-training has fundamentally changed how machine learning
research is done today: large foundation models are trained once, and then can
be used by anyone in the community (including those without data or compute
resources to train a model from scratch) to adapt and fine-tune to specific
tasks. Applying this same framework to reinforcement learning (RL) is appealing
because it offers compelling avenues for addressing core challenges in RL,
including sample efficiency and robustness. However, there remains a
fundamental challenge to pre-train large models in the context of RL: actions
have long-term dependencies, so training a foundation model that reasons across
time is important. Recent advances in generative AI have provided new tools for
modeling highly complex distributions. In this paper, we build a probabilistic
model to predict which states an agent will visit in the temporally distant
future (i.e., an occupancy measure) using flow matching. As large datasets are
often constructed by many distinct users performing distinct tasks, we include
in our model a latent variable capturing the user intention. This intention
increases the expressivity of our model, and enables adaptation with
generalized policy improvement. We call our proposed method
intention-conditioned flow occupancy models (InFOM). Comparing with alternative
methods for pre-training, our experiments on $36$ state-based and $4$
image-based benchmark tasks demonstrate that the proposed method achieves $1.8
\times$ median improvement in returns and increases success rates by $36\%$.
Website: https://chongyi-zheng.github.io/infom Code:
https://github.com/chongyi-zheng/infom

</details>


### [139] [Enhancing generalizability of model discovery across parameter space with multi-experiment equation learning (ME-EQL)](https://arxiv.org/abs/2506.08916)
*Maria-Veronica Ciocanel,John T. Nardini,Kevin B. Flores,Erica M. Rutter,Suzanne S. Sindi,Alexandria Volkening*

Main category: cs.LG

TL;DR: Agent-based modeling (ABM) is a powerful tool for understanding self-organizing biological systems, but it is computationally intensive and often not analytically tractable. To address this, the paper introduces Multi-experiment equation learning (ME-EQL) as an extension of Equation learning (EQL), with two methods: one-at-a-time ME-EQL (OAT ME-EQL) and embedded structure ME-EQL (ES ME-EQL). These methods significantly reduce the relative error in recovering parameters from agent-based simulations, with OAT ME-EQL offering better generalizability across parameter space.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to improve the generalizability and interpretability of learned models for complex biological systems by extending Equation learning (EQL) methods to Multi-experiment equation learning (ME-EQL).

Method: The study introduces two methods under ME-EQL framework: one-at-a-time ME-EQL (OAT ME-EQL) which learns individual models for each parameter set and connects them via interpolation, and embedded structure ME-EQL (ES ME-EQL) which builds a unified model library across parameters.

Result: Both methods significantly reduce the relative error in recovering parameters from agent-based simulations. OAT ME-EQL offers better generalizability across parameter space.

Conclusion: The findings highlight the potential of equation learning from multiple experiments to enhance the generalizability and interpretability of learned models for complex biological systems.

Abstract: Agent-based modeling (ABM) is a powerful tool for understanding
self-organizing biological systems, but it is computationally intensive and
often not analytically tractable. Equation learning (EQL) methods can derive
continuum models from ABM data, but they typically require extensive
simulations for each parameter set, raising concerns about generalizability. In
this work, we extend EQL to Multi-experiment equation learning (ME-EQL) by
introducing two methods: one-at-a-time ME-EQL (OAT ME-EQL), which learns
individual models for each parameter set and connects them via interpolation,
and embedded structure ME-EQL (ES ME-EQL), which builds a unified model library
across parameters. We demonstrate these methods using a birth--death mean-field
model and an on-lattice agent-based model of birth, death, and migration with
spatial structure. Our results show that both methods significantly reduce the
relative error in recovering parameters from agent-based simulations, with OAT
ME-EQL offering better generalizability across parameter space. Our findings
highlight the potential of equation learning from multiple experiments to
enhance the generalizability and interpretability of learned models for complex
biological systems.

</details>


### [140] [Local MDI+: Local Feature Importances for Tree-Based Models](https://arxiv.org/abs/2506.08928)
*Zhongyuan Liang,Zachary T. Rewolinski,Abhineet Agarwal,Tiffany M. Tang,Bin Yu*

Main category: cs.LG

TL;DR: 尽管基于树的集成模型（如随机森林）在表格数据上比深度学习模型具有预测性能和计算效率的优势，但现有的局部特征重要性方法（如LIME和TreeSHAP）存在依赖近似值、忽略模型内部结构以及可能产生不稳定扰动的问题。虽然全局方法MDI+通过将决策树与线性模型等效来解决这些问题，但它无法解释具有异质个体特性的预测。本文提出了一种新的扩展框架Local MDI+ (LMDI+)，用于样本特定设置。LMDI+在识别实例特定信号特征方面优于现有基线，并在多个真实世界基准数据集上表现出更高的稳定性。此外，LMDI+还支持局部可解释性用例，包括更接近的反事实识别和同质子组的发现。


<details>
  <summary>Details</summary>
Motivation: 基于树的集成模型（如随机森林）在表格数据上的优势使其广泛应用于高风险领域，因此对这些模型进行可解释性分析至关重要。然而，现有的局部特征重要性方法（如LIME和TreeSHAP）存在依赖近似值、忽略模型内部结构以及可能产生不稳定扰动的问题。同时，全局方法MDI+虽然解决了这些问题，但它无法解释具有异质个体特性的预测。这促使了对更精确和稳定的局部特征重要性方法的需求。

Method: 本文提出了Local MDI+ (LMDI+)，这是MDI+框架在样本特定设置下的扩展。LMDI+利用决策树与线性模型之间的等效关系，在变换后的节点基础上计算特征重要性。该方法通过以下方式改进了现有方法：1. 提供了更准确的实例特定信号特征识别；2. 在多次随机森林拟合中保持了更高的稳定性；3. 支持局部可解释性用例，如识别更接近的反事实和发现同质子组。

Result: LMDI+在十二个真实世界基准数据集上的实验结果表明，它在识别实例特定信号特征方面平均比LIME和TreeSHAP高出10%的下游任务性能。此外，LMDI+在多次随机森林拟合中表现出更高的稳定性，能够一致地生成相似的实例级特征重要性排名。

Conclusion: LMDI+是一种有效的局部特征重要性方法，能够更准确地识别实例特定信号特征并提供更高的稳定性。其应用不仅限于解释随机森林模型，还可以用于其他局部可解释性任务，如识别更接近的反事实和发现同质子组。

Abstract: Tree-based ensembles such as random forests remain the go-to for tabular data
over deep learning models due to their prediction performance and computational
efficiency. These advantages have led to their widespread deployment in
high-stakes domains, where interpretability is essential for ensuring
trustworthy predictions. This has motivated the development of popular local
(i.e. sample-specific) feature importance (LFI) methods such as LIME and
TreeSHAP. However, these approaches rely on approximations that ignore the
model's internal structure and instead depend on potentially unstable
perturbations. These issues are addressed in the global setting by MDI+, a
feature importance method which exploits an equivalence between decision trees
and linear models on a transformed node basis. However, the global MDI+ scores
are not able to explain predictions when faced with heterogeneous individual
characteristics. To address this gap, we propose Local MDI+ (LMDI+), a novel
extension of the MDI+ framework to the sample specific setting. LMDI+
outperforms existing baselines LIME and TreeSHAP in identifying
instance-specific signal features, averaging a 10% improvement in downstream
task performance across twelve real-world benchmark datasets. It further
demonstrates greater stability by consistently producing similar instance-level
feature importance rankings across multiple random forest fits. Finally, LMDI+
enables local interpretability use cases, including the identification of
closer counterfactuals and the discovery of homogeneous subgroups.

</details>


### [141] [BioLangFusion: Multimodal Fusion of DNA, mRNA, and Protein Language Models](https://arxiv.org/abs/2506.08936)
*Amina Mollaysa,Artem Moskale,Pushpak Pati,Tommaso Mansi,Mangal Prakash,Rui Liao*

Main category: cs.LG

TL;DR: The paper introduces BioLangFusion, a method for integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations by aligning embeddings at the codon level. It studies three fusion techniques and demonstrates improved performance on molecular property prediction tasks compared to unimodal baselines.


<details>
  <summary>Details</summary>
Motivation: To create a unified molecular representation that integrates information from DNA, mRNA, and protein using pre-trained language models, motivated by the central dogma of molecular biology.

Method: BioLangFusion uses three standard fusion techniques: codon-level embedding concatenation, entropy-regularized attention pooling, and cross-modal multi-head attention. These methods align per-modality embeddings at the biologically meaningful codon level.

Result: BioLangFusion outperforms strong unimodal baselines across five molecular property prediction tasks, showing that even simple fusion of pre-trained models can capture complementary multi-omic information with minimal overhead.

Conclusion: Integrating pre-trained DNA, mRNA, and protein language models into unified molecular representations through simple fusion techniques can effectively capture complementary multi-omic information.

Abstract: We present BioLangFusion, a simple approach for integrating pre-trained DNA,
mRNA, and protein language models into unified molecular representations.
Motivated by the central dogma of molecular biology (information flow from gene
to transcript to protein), we align per-modality embeddings at the biologically
meaningful codon level (three nucleotides encoding one amino acid) to ensure
direct cross-modal correspondence. BioLangFusion studies three standard fusion
techniques: (i) codon-level embedding concatenation, (ii) entropy-regularized
attention pooling inspired by multiple-instance learning, and (iii) cross-modal
multi-head attention -- each technique providing a different inductive bias for
combining modality-specific signals. These methods require no additional
pre-training or modification of the base models, allowing straightforward
integration with existing sequence-based foundation models. Across five
molecular property prediction tasks, BioLangFusion outperforms strong unimodal
baselines, showing that even simple fusion of pre-trained models can capture
complementary multi-omic information with minimal overhead.

</details>


### [142] [KARMA: A Multilevel Decomposition Hybrid Mamba Framework for Multivariate Long-Term Time Series Forecasting](https://arxiv.org/abs/2506.08939)
*Hang Ye,Gaoxiang Duan,Haoran Zeng,Yangxin Zhu,Lingxue Meng,Xiaoying Zheng,Yongxin Zhu*

Main category: cs.LG

TL;DR: KARMA is a new model for multivariate long-term time series forecasting which uses Adaptive Time Channel Decomposition and Hybrid Frequency-Time Decomposition modules to improve prediction accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing traditional time series decomposition methods are limited by fixed rules and insufficient potential information mining. Transformer-based models have high computational complexity that limits their ability to effectively model long sequences and intricate dynamic relationships.

Method: KARMA incorporates an Adaptive Time Channel Decomposition module (ATCD) for extracting trend and seasonal components dynamically, and a Hybrid Frequency-Time Decomposition module (HFTD) for further decomposing series into frequency and time domains. The model also utilizes multi-scale Mamba-based KarmaBlock for efficient processing of global and local information in a coordinated way.

Result: Experiments on eight real-world datasets from diverse domains showed that KARMA significantly outperforms mainstream baseline methods in both predictive accuracy and computational efficiency.

Conclusion: KARMA addresses the limitations of traditional decomposition methods and Transformer-based models by introducing ATCD, HFTD, and KarmaBlock, leading to superior performance in multivariate long-term time series forecasting.

Abstract: Multivariate long-term and efficient time series forecasting is a key
requirement for a variety of practical applications, and there are complex
interleaving time dynamics in time series data that require decomposition
modeling. Traditional time series decomposition methods are single and rely on
fixed rules, which are insufficient for mining the potential information of the
series and adapting to the dynamic characteristics of complex series. On the
other hand, the Transformer-based models for time series forecasting struggle
to effectively model long sequences and intricate dynamic relationships due to
their high computational complexity. To overcome these limitations, we
introduce KARMA, with an Adaptive Time Channel Decomposition module (ATCD) to
dynamically extract trend and seasonal components. It further integrates a
Hybrid Frequency-Time Decomposition module (HFTD) to further decompose Series
into frequency-domain and time-domain. These components are coupled with
multi-scale Mamba-based KarmaBlock to efficiently process global and local
information in a coordinated manner. Experiments on eight real-world datasets
from diverse domains well demonstrated that KARMA significantly outperforms
mainstream baseline methods in both predictive accuracy and computational
efficiency. Code and full results are available at this repository:
https://github.com/yedadasd/KARMA

</details>


### [143] [Towards Robust Deep Reinforcement Learning against Environmental State Perturbation](https://arxiv.org/abs/2506.08961)
*Chenxu Wang,Huaping Liu*

Main category: cs.LG

TL;DR: This paper explores adversarial attacks and robustness in Deep Reinforcement Learning (DRL) by focusing on environmental state perturbations, proposing a defense framework called Boosted Adversarial Training (BAT).


<details>
  <summary>Details</summary>
Motivation: To improve the robustness of DRL agents against environmental state perturbations which are natural in embodied scenarios.

Method: The problem of environmental state perturbation is formulated, introducing a preliminary non-targeted attack method as a calibration adversary. Then, a defense framework named Boosted Adversarial Training (BAT) is proposed which tunes the agents via supervised learning to avoid catastrophic failure and subsequently adversarially trains the agent with reinforcement learning.

Result: Experimental results show mainstream agents' vulnerability under environmental state perturbations and the effectiveness of the proposed attack. Defense results indicate existing robust reinforcement learning algorithms may not be suitable but the BAT framework can significantly enhance agents' robustness against environmental state perturbations across various situations.

Conclusion: Environmental state perturbations pose a challenge for DRL agents' robustness. The BAT framework effectively enhances the robustness of DRL agents.

Abstract: Adversarial attacks and robustness in Deep Reinforcement Learning (DRL) have
been widely studied in various threat models; however, few consider
environmental state perturbations, which are natural in embodied scenarios. To
improve the robustness of DRL agents, we formulate the problem of environmental
state perturbation, introducing a preliminary non-targeted attack method as a
calibration adversary, and then propose a defense framework, named Boosted
Adversarial Training (BAT), which first tunes the agents via supervised
learning to avoid catastrophic failure and subsequently adversarially trains
the agent with reinforcement learning. Extensive experimental results
substantiate the vulnerability of mainstream agents under environmental state
perturbations and the effectiveness of our proposed attack. The defense results
demonstrate that while existing robust reinforcement learning algorithms may
not be suitable, our BAT framework can significantly enhance the robustness of
agents against environmental state perturbations across various situations.

</details>


### [144] [GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO](https://arxiv.org/abs/2506.08965)
*Yiyang Zhao,Huiyu Bai,Xuejiao Zhao*

Main category: cs.LG

TL;DR: 本研究提出了一种数据增强和扩展框架，通过引入偏好精炼、Chain-of-Thought (CoT) 采样、困惑度评分机制和多级直接偏好优化（M-DPO）等技术，使在少量数据上训练的生成式奖励模型能够达到与大规模数据集训练模型相当的性能。实验表明，该方法显著提高了数据效率和模型性能，为低资源强化学习应用提供了稳健的解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前生成式奖励模型的训练方法如直接偏好优化（DPO）受到样本配对效率低下和数据多样性不足的限制，尤其是在小数据集情况下表现不佳。因此，需要一种新的框架来提高奖励模型在小数据集上的训练效果和数据利用效率。

Method: 研究提出了一个数据增强和扩展框架，具体包括：1) 偏好精炼，使用Chain-of-Thought (CoT) 采样发现多样且高质量的偏好关系；2) 困惑度评分机制，分配细致的偏好等级；3) 多级直接偏好优化（M-DPO），帮助模型捕捉样本间的细微偏好差异。这些方法共同提升了奖励模型在小数据集上的表现。

Result: 实验结果表明，所提出的框架显著提高了数据效率和模型性能，使得在少量数据上训练的奖励模型可以达到与大规模数据集训练模型相媲美的效果。这为低资源环境下的强化学习应用提供了一种有效的方法。

Conclusion: 本研究表明，数据高效策略在提升奖励模型优化方面具有巨大潜力，特别是在低资源强化学习反馈应用中，可作为一种稳健的解决方案。

Abstract: The ability to train high-performing reward models with few-shot data is
critical for enhancing the efficiency and scalability of Reinforcement Learning
from Human Feedback (RLHF). We propose a data augmentation and expansion
framework that enables generative reward models trained on small datasets to
achieve comparable performance to those trained on large-scale datasets.
Traditional methods to train a generative reward model, such as Direct
Preference Optimization (DPO), are constrained by inefficiencies in sample
pairing and limited data diversity. This work introduces preference refinement,
which employs Chain-of-Thought (CoT) sampling to uncover diverse and
high-quality preference relationships. It also incorporates a perplexity-based
scoring mechanism to assign nuanced preference levels and utilizes Multi-level
Direct Preference Optimization (M-DPO) to enable the model to capture
finer-grained preference differences between samples. Experimental results
demonstrate that the proposed method significantly enhances data efficiency and
model performance, enabling reward models trained in a few-shot setting to
achieve results on par with those trained on large-scale datasets. This study
underscores the potential of data-efficient strategies in advancing reward
model optimization, offering a robust solution for low-resource RLHF
applications.

</details>


### [145] [Tailored Architectures for Time Series Forecasting: Evaluating Deep Learning Models on Gaussian Process-Generated Data](https://arxiv.org/abs/2506.08977)
*Victoria Hankemeier,Malte Schilling*

Main category: cs.LG

TL;DR: The paper explores connections between time series characteristics and model performance, introduces a Gaussian Processes-generated dataset for targeted evaluations, and presents TimeFlex, a modular architecture model designed to handle diverse temporal dynamics.


<details>
  <summary>Details</summary>
Motivation: To uncover clear connections between time series characteristics and the strengths of individual models, addressing the limitation of demonstrating model effectiveness on limited real-world data sets.

Method: Introduction of a novel dataset generated using Gaussian Processes with distinct, known characteristics for targeted evaluations. Development of TimeFlex, a modular architecture model designed to handle diverse temporal dynamics including trends and periodic patterns.

Result: TimeFlex is compared against current state-of-the-art models providing deeper insights into model performance under varied time series conditions.

Conclusion: Clear connections exist between time series characteristics and specific model architectures; the use of tailored datasets and flexible models like TimeFlex can enhance understanding and improve forecasting accuracy.

Abstract: Developments in Deep Learning have significantly improved time series
forecasting by enabling more accurate modeling of complex temporal dependencies
inherent in sequential data. The effectiveness of such models is often
demonstrated on limited sets of specific real-world data. Although this allows
for comparative analysis, it still does not demonstrate how specific data
characteristics align with the architectural strengths of individual models.
Our research aims at uncovering clear connections between time series
characteristics and particular models. We introduce a novel dataset generated
using Gaussian Processes, specifically designed to display distinct, known
characteristics for targeted evaluations of model adaptability to them.
Furthermore, we present TimeFlex, a new model that incorporates a modular
architecture tailored to handle diverse temporal dynamics, including trends and
periodic patterns. This model is compared to current state-of-the-art models,
offering a deeper understanding of how models perform under varied time series
conditions.

</details>


### [146] [Propositional Logic for Probing Generalization in Neural Networks](https://arxiv.org/abs/2506.08978)
*Anna Langedijk,Jaap Jumelet,Willem Zuidema*

Main category: cs.LG

TL;DR: 研究了三种神经网络架构在命题逻辑任务中的泛化行为，发现即使在引入结构偏差的情况下，对未见模式（尤其是涉及否定的模式）的泛化仍然是一个重大挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨神经网络获取和表示符号规则的能力，特别是在命题逻辑任务中研究其组成性泛化的局限性。

Method: 通过一个基于命题逻辑的任务，评估Transformer、图卷积网络和LSTM三种架构的泛化能力，并引入平衡扩展的数据集以消除表面模式并测试未见操作符组合。

Result: 所有模型在训练分布内表现良好，但在泛化到未见模式时遇到困难，特别是涉及否定的模式。Transformer只有在引入结构偏差时才能较好地处理否定的组成性应用。

Conclusion: 标准神经网络架构在学习系统性的逻辑运算符表示方面存在持续的局限性，需要更强的归纳偏差来支持稳健的基于规则的推理。

Abstract: The extent to which neural networks are able to acquire and represent
symbolic rules remains a key topic of research and debate. Much current work
focuses on the impressive capabilities of large language models, as well as
their often ill-understood failures on a wide range of reasoning tasks. In this
paper, in contrast, we investigate the generalization behavior of three key
neural architectures (Transformers, Graph Convolution Networks and LSTMs) in a
controlled task rooted in propositional logic. The task requires models to
generate satisfying assignments for logical formulas, making it a structured
and interpretable setting for studying compositionality. We introduce a
balanced extension of an existing dataset to eliminate superficial patterns and
enable testing on unseen operator combinations. Using this dataset, we evaluate
the ability of the three architectures to generalize beyond the training
distribution. While all models perform well in-distribution, we find that
generalization to unseen patterns, particularly those involving negation,
remains a significant challenge. Transformers fail to apply negation
compositionally, unless structural biases are introduced. Our findings
highlight persistent limitations in the ability of standard architectures to
learn systematic representations of logical operators, suggesting the need for
stronger inductive biases to support robust rule-based reasoning.

</details>


### [147] [On Finetuning Tabular Foundation Models](https://arxiv.org/abs/2506.08982)
*Ivan Rubachev,Akim Kotelnikov,Nikolay Kartashev*

Main category: cs.LG

TL;DR: 研究了TabPFNv2在不同数据集上的微调策略，发现全量微调是最有效的方法，并揭示了微调如何改变模型内部机制。微调成功的关键在于通过梯度适应，使测试对象和上下文训练对象的表示更准确地反映目标相似性，从而提升基于检索的预测逻辑。实际应用中，微调后的TabPFNv2在多达5万样本的数据集上表现出色，但在具有渐变时间偏移和丰富特征集的数据集上稳定性较差。


<details>
  <summary>Details</summary>
Motivation: 尽管TabPFNv2在小规模数据集上表现优异，但其最优微调方法及其对模型内部机制的影响尚未被充分研究。由于先前工作的发现不一致以及TabPFNv2的独特架构，需要进行新的探索。

Method: 系统评估了多种微调策略在多样数据集上的效果，确定全量微调为TabPFNv2的最佳选择。进一步分析了微调如何改变TabPFNv2的内部机制，将其与检索增强模型类比。揭示了微调后测试对象与上下文训练对象表示之间的点积更能准确反映目标相似性。

Result: 微调后的TabPFNv2在高达5万样本的数据集上表现出性能提升，在学术数据集上达到最先进的结果；但在具有渐变时间偏移和丰富特征集的数据集上稳定性不足，先前方法仍表现更优。

Conclusion: 全量微调是TabPFNv2最实用的微调方法，能够显著提高模型性能并改善其内部机制，但在特定类型数据集上的稳定性仍有待改进。

Abstract: Foundation models are an emerging research direction in tabular deep
learning. Notably, TabPFNv2 recently claimed superior performance over
traditional GBDT-based methods on small-scale datasets using an in-context
learning paradigm, which does not adapt model parameters to target datasets.
However, the optimal finetuning approach for adapting tabular foundational
models, and how this adaptation reshapes their internal mechanisms, remains
underexplored. While prior works studied finetuning for earlier foundational
models, inconsistent findings and TabPFNv2's unique architecture necessitate
fresh investigation. To address these questions, we first systematically
evaluate various finetuning strategies on diverse datasets. Our findings
establish full finetuning as the most practical solution for TabPFNv2 in terms
of time-efficiency and effectiveness. We then investigate how finetuning alters
TabPFNv2's inner mechanisms, drawing an analogy to retrieval-augmented models.
We reveal that the success of finetuning stems from the fact that after
gradient-based adaptation, the dot products of the query-representations of
test objects and the key-representations of in-context training objects more
accurately reflect their target similarity. This improved similarity allows
finetuned TabPFNv2 to better approximate target dependency by appropriately
weighting relevant in-context samples, improving the retrieval-based prediction
logic. From the practical perspective, we managed to finetune TabPFNv2 on
datasets with up to 50K objects, observing performance improvements on almost
all tasks. More precisely, on academic datasets with I.I.D. splits, finetuning
allows TabPFNv2 to achieve state-of-the-art results, while on datasets with
gradual temporal shifts and rich feature sets, TabPFNv2 is less stable and
prior methods remain better.

</details>


### [148] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: RLVR is effective for training LLMs on complex reasoning tasks like mathematical problem solving. The scalability of RLVR requires high-quality problem sets with precise and verifiable answers. To address the scarcity of well-crafted human-labeled math problems, a Self-aware Weakness-driven problem Synthesis framework (SwS) is introduced to systematically identify model deficiencies and leverage them for problem augmentation. This enables robust generalization by empowering the model to self-identify and address its weaknesses in RL, yielding average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to improve the scalability of RLVR for training LLMs on complex reasoning tasks by addressing the limitations of existing datasets and problem synthesis strategies. Existing distillation-oriented synthetic datasets have limited verification answers and indiscriminately expand the problem set without considering the model's capabilities, leading to low efficiency in generating useful questions.

Method: The method involves defining weaknesses as questions that the model consistently fails to learn through its iterative sampling during RL training. Core concepts are extracted from these failure cases to synthesize new problems that strengthen the model's weak areas in subsequent augmented training. This allows the model to focus on and gradually overcome its weaknesses without relying on external knowledge distillation.

Result: The SwS framework yields average performance gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning benchmarks.

Conclusion: The Self-aware Weakness-driven problem Synthesis framework enables robust generalization by empowering the model to self-identify and address its weaknesses in RL, thus improving the effectiveness and scalability of RLVR for training LLMs on complex reasoning tasks.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [149] [Branched Schrödinger Bridge Matching](https://arxiv.org/abs/2506.09007)
*Sophia Tang,Yinuo Zhang,Alexander Tong,Pranam Chatterjee*

Main category: cs.LG

TL;DR: The paper introduces Branched Schrödinger Bridge Matching (BranchSBM), a new framework that overcomes limitations of existing methods by modeling branched transitions, making it more expressive for tasks such as multi-path surface navigation and modeling cell fate bifurcations.


<details>
  <summary>Details</summary>
Motivation: Current methods for learning mappings between two distributions, such as flow matching and Schrödinger Bridge Matching, are limited to unimodal transitions and cannot capture branched or divergent evolution from a common origin to multiple distinct outcomes.

Method: Branched Schrödinger Bridge Matching (BranchSBM) parameterizes multiple time-dependent velocity fields and growth processes, enabling the representation of population-level divergence into multiple terminal distributions.

Result: BranchSBM is shown to be more expressive and essential for tasks involving multi-path surface navigation, modeling cell fate bifurcations from homogeneous progenitor states, and simulating diverging cellular responses to perturbations.

Conclusion: BranchSBM provides a novel framework that learns branched Schrödinger bridges, overcoming the limitations of existing methods in capturing complex transitions.

Abstract: Predicting the intermediate trajectories between an initial and target
distribution is a central problem in generative modeling. Existing approaches,
such as flow matching and Schr\"odinger Bridge Matching, effectively learn
mappings between two distributions by modeling a single stochastic path.
However, these methods are inherently limited to unimodal transitions and
cannot capture branched or divergent evolution from a common origin to multiple
distinct outcomes. To address this, we introduce Branched Schr\"odinger Bridge
Matching (BranchSBM), a novel framework that learns branched Schr\"odinger
bridges. BranchSBM parameterizes multiple time-dependent velocity fields and
growth processes, enabling the representation of population-level divergence
into multiple terminal distributions. We show that BranchSBM is not only more
expressive but also essential for tasks involving multi-path surface
navigation, modeling cell fate bifurcations from homogeneous progenitor states,
and simulating diverging cellular responses to perturbations.

</details>


### [150] [Effective Data Pruning through Score Extrapolation](https://arxiv.org/abs/2506.09010)
*Sebastian Schmidt,Prasanga Dhungel,Christoffer Löffler,Björn Nieth,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: A new importance score extrapolation framework is introduced which needs training on only a small subset of data. Two approaches, k-nearest neighbors and graph neural networks, are presented to predict sample importance for the entire dataset. The effectiveness is demonstrated for several pruning methods, datasets, and training paradigms.


<details>
  <summary>Details</summary>
Motivation: Training advanced machine learning models requires massive datasets causing prohibitive computational costs. Data pruning techniques help but require a full initial training pass to identify removable samples, negating efficiency benefits for single training runs.

Method: Introduced a novel importance score extrapolation framework requiring training on only a small subset of data. Presented two approaches within this framework - k-nearest neighbors and graph neural networks - to predict sample importance for the entire dataset using patterns learned from the minimal subset.

Result: Demonstrated the effectiveness of the approach for 2 state-of-the-art pruning methods, 4 different datasets, and 3 training paradigms. Results indicate that score extrapolation is promising to scale expensive score calculation methods.

Conclusion: Score extrapolation is a promising direction to scale expensive score calculation methods like pruning, data attribution, or other tasks.

Abstract: Training advanced machine learning models demands massive datasets, resulting
in prohibitive computational costs. To address this challenge, data pruning
techniques identify and remove redundant training samples while preserving
model performance. Yet, existing pruning techniques predominantly require a
full initial training pass to identify removable samples, negating any
efficiency benefits for single training runs. To overcome this limitation, we
introduce a novel importance score extrapolation framework that requires
training on only a small subset of data. We present two initial approaches in
this framework - k-nearest neighbors and graph neural networks - to accurately
predict sample importance for the entire dataset using patterns learned from
this minimal subset. We demonstrate the effectiveness of our approach for 2
state-of-the-art pruning methods (Dynamic Uncertainty and TDDS), 4 different
datasets (CIFAR-10, CIFAR-100, Places-365, and ImageNet), and 3 training
paradigms (supervised, unsupervised, and adversarial). Our results indicate
that score extrapolation is a promising direction to scale expensive score
calculation methods, such as pruning, data attribution, or other tasks.

</details>


### [151] [SPEED-RL: Faster Training of Reasoning Models via Online Curriculum Learning](https://arxiv.org/abs/2506.09016)
*Ruiqi Zhang,Daman Arora,Song Mei,Andrea Zanette*

Main category: cs.LG

TL;DR: 通过引入SPEED方法，选择性地使用中等难度的训练样本进行强化学习，可以提高大语言模型的推理能力，并且加速训练过程2到6倍。


<details>
  <summary>Details</summary>
Motivation: 尽管用强化学习训练大型语言模型能显著提高其推理能力，但因非高效的均匀提示采样，计算成本仍然很高。

Method: 提出了SPEED（Selective Prompting with Efficient Estimation of Difficulty），一种自适应在线强化学习课程，它选择性地挑选中等难度的训练样本来最大化学习效率。理论和实证上证明了这种方法的有效性。

Result: 该方法使训练速度加快了2至6倍，同时不降低准确性，无需手动调整参数，并且可以无缝集成到标准的强化学习算法中。

Conclusion: SPEED方法通过选择中等难度的提示提高了梯度估计器的信噪比，从而加速了收敛过程，提升了训练效率。

Abstract: Training large language models with reinforcement learning (RL) against
verifiable rewards significantly enhances their reasoning abilities, yet
remains computationally expensive due to inefficient uniform prompt sampling.
We introduce Selective Prompting with Efficient Estimation of Difficulty
(SPEED), an adaptive online RL curriculum that selectively chooses training
examples of intermediate difficulty to maximize learning efficiency.
Theoretically, we establish that intermediate-difficulty prompts improve the
gradient estimator's signal-to-noise ratio, accelerating convergence.
Empirically, our efficient implementation leads to 2x to 6x faster training
without degrading accuracy, requires no manual tuning, and integrates
seamlessly into standard RL algorithms.

</details>


### [152] [Edit Flows: Flow Matching with Edit Operations](https://arxiv.org/abs/2506.09018)
*Marton Havasi,Brian Karrer,Itai Gat,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: Edit Flows是一种非自回归模型，通过编辑操作（插入、删除和替换）定义序列上的离散流，其在图像描述、文本和代码生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 非自回归模型在生成可变长度序列时面临困难，通常会施加僵化的逐标记结构。为了克服这些限制，需要一种新的非自回归模型。

Method: 提出了一种名为Edit Flows的非自回归模型，通过编辑操作（插入、删除和替换）定义序列上的离散流，并将其建模为序列空间上的连续时间马尔可夫链，从而实现灵活的位置相关生成。训练方法利用扩展状态空间中的辅助变量，使学习过程高效且可行。

Result: 实证结果表明，Edit Flows在图像描述任务中优于自回归和掩码模型，在文本和代码生成任务中显著优于掩码构造。

Conclusion: Edit Flows作为一种非自回归模型，能够有效克服传统非自回归模型的局限性，生成更符合序列数据结构的可变长度序列，并在多个生成任务中展现出优越性能。

Abstract: Autoregressive generative models naturally generate variable-length
sequences, while non-autoregressive models struggle, often imposing rigid,
token-wise structures. We propose Edit Flows, a non-autoregressive model that
overcomes these limitations by defining a discrete flow over sequences through
edit operations-insertions, deletions, and substitutions. By modeling these
operations within a Continuous-time Markov Chain over the sequence space, Edit
Flows enable flexible, position-relative generation that aligns more closely
with the structure of sequence data. Our training method leverages an expanded
state space with auxiliary variables, making the learning process efficient and
tractable. Empirical results show that Edit Flows outperforms both
autoregressive and mask models on image captioning and significantly
outperforms the mask construction in text and code generation.

</details>


### [153] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 通过训练LLM进行情境探索，提出配方e3以实现更好的推理模型外推能力，并产生了最佳的1.7B模型。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的推理模型在外推能力上表现不佳，需要一种方法使LLM能够更有效地利用测试时间预算来提高性能。

Method: 通过训练LLM执行情境探索，包括链接操作、测试多个假设等；识别配方e3的三个关键成分：链接不对称能力技能、利用错误轨迹的'负梯度'放大探索以及结合任务难度与训练令牌预算设计特定课程。

Result: 配方e3生成了已知最好的1.7B模型，在AIME'25和HMMT'25评分中表现出色，并能外推到两倍于训练令牌预算。该模型不仅获得高pass@1分数，还提高了pass@k分数。

Conclusion: 通过情境探索训练LLM可以显著提高其推理能力及外推性能，配方e3为构建高效推理模型提供了有效途径。

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>


### [154] [FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed](https://arxiv.org/abs/2506.09034)
*Sizhe Dang,Yangyang Guo,Yanjun Zhao,Haishan Ye,Xiaodong Zheng,Guang Dai,Ivor Tsang*

Main category: cs.LG

TL;DR: 提出了一种快速零阶优化器FZOO，通过减少前向传递次数和加速每批计算，实现了与Adam相近的速度，同时显著降低了内存使用。


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型时，常遇到GPU内存瓶颈问题。一阶优化器如Adam在反向传播时会大幅增加内存使用，而现有的零阶优化方法如MeZO虽然避免了这一成本，但通常需要更多步骤才能收敛。为解决速度与内存之间的权衡问题，提出了FZOO（快速零阶优化器）。

Method: FZOO采用批量单侧估计来适应基于批次损失标准差的步长，并通过Rademacher随机向量扰动与CUDA的并行处理加速每批次计算。此外，还提供了理论分析证明FZOO的形式等价于归一化SGD更新规则及其收敛性保证。FZOO可以平滑地集成到PEFT技术中，从而实现更大的内存节省。

Result: 实验结果表明，FZOO在11个任务上平均比MeZO高出3%的准确率，且所需的前向传递次数减少了3倍。对于RoBERTa-large模型，FZOO在准确率上平均提高了5.6%，前向传递次数减少了18倍，收敛速度与Adam相当。

Conclusion: FZOO使得单GPU、高速、全参数微调成为可能，并为未来的高效预训练工作指明了方向。

Abstract: Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks:
the backward pass of first-order optimizers like Adam increases memory usage to
more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order
(ZO) optimizers avoid this cost by estimating gradients only from forward
passes, yet existing methods like MeZO usually require many more steps to
converge. Can this trade-off between speed and memory in ZO be fundamentally
improved? Normalized-SGD demonstrates strong empirical performance with greater
memory efficiency than Adam. In light of this, we introduce FZOO, a Fast
Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward
passes needed for convergence by employing batched one-sided estimates that
adapt step sizes based on the standard deviation of batch losses. It also
accelerates per-batch computation through the use of Rademacher random vector
perturbations coupled with CUDA's parallel processing. Extensive experiments on
diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3,
across 11 tasks validate FZOO's effectiveness. On average, FZOO outperforms
MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For
RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy
and an 18 times reduction in forward passes compared to MeZO, achieving
convergence speeds comparable to Adam. We also provide theoretical analysis
proving FZOO's formal equivalence to a normalized-SGD update rule and its
convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling
even larger memory savings. Overall, our results make single-GPU, high-speed,
full-parameter fine-tuning practical and point toward future work on
memory-efficient pre-training.

</details>


### [155] [The Decoupled Risk Landscape in Performative Prediction](https://arxiv.org/abs/2506.09044)
*Javier Sanguino,Thomas Kehrenberg,Jose A. Lozano,Novi Quadrianto*

Main category: cs.LG

TL;DR: The paper discusses performative prediction scenarios where deploying a model induces distribution shifts in input data. It introduces a decoupled risk visualization method and an extended performative prediction setting.


<details>
  <summary>Details</summary>
Motivation: To address the issue of distribution shift caused by deploying models, and to provide practical insights into theoretical advances in performative prediction.

Method: Introduce a decoupled risk visualization method for analyzing the risk landscape with respect to model parameters and data parameters. Propose an extended performative prediction setting capturing scenarios where data distribution reacts to a model different from the decision-making one.

Result: The decoupled risk visualization method provides new properties of interest points and examines algorithm performance under realistic conditions. The extended performative prediction setting reflects real-world situations where agents lack full access to deployed models.

Conclusion: Visualization of loss landscape can complement theoretical guarantees with practical insights in performative prediction. The proposed methods offer valuable tools for understanding and improving algorithms in dynamic environments.

Abstract: Performative Prediction addresses scenarios where deploying a model induces a
distribution shift in the input data, such as individuals modifying their
features and reapplying for a bank loan after rejection. Literature has had a
theoretical perspective giving mathematical guarantees for convergence (either
to the stable or optimal point). We believe that visualization of the loss
landscape can complement this theoretical advances with practical insights.
Therefore, (1) we introduce a simple decoupled risk visualization method
inspired in the two-step process that performative prediction is. Our approach
visualizes the risk landscape with respect to two parameter vectors: model
parameters and data parameters. We use this method to propose new properties of
the interest points, to examine how existing algorithms traverse the risk
landscape and perform under more realistic conditions, including strategic
classification with non-linear models. (2) Building on this decoupled risk
visualization, we introduce a novel setting - extended Performative Prediction
- which captures scenarios where the distribution reacts to a model different
from the decision-making one, reflecting the reality that agents often lack
full access to the deployed model.

</details>


### [156] [Agentic Neural Networks: Self-Evolving Multi-Agent Systems via Textual Backpropagation](https://arxiv.org/abs/2506.09046)
*Xiaowen Ma,Chenyang Lin,Yao Zhang,Volker Tresp,Yunpu Ma*

Main category: cs.LG

TL;DR: ANN is a novel framework that treats multi-agent collaboration as a layered neural network architecture, surpassing existing multi-agent systems in accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current methods for leveraging multiple LLMs often rely on static, manually engineered multi-agent configurations which limits their flexibility and scalability.

Method: The Agentic Neural Network (ANN) conceptualizes multi-agent collaboration as a layered neural network architecture. Each agent operates as a node and each layer forms a cooperative team focused on a specific subtask. ANN follows a two-phase optimization strategy: Forward Phase for task decomposition and team construction, and Backward Phase for refining global and local collaboration through iterative feedback.

Result: ANN surpasses leading multi-agent baselines under the same configurations across four benchmark datasets, showing consistent performance improvements in accuracy and adaptability.

Conclusion: ANN provides a scalable, data-driven framework for multi-agent systems, combining the collaborative capabilities of LLMs with the efficiency and flexibility of neural network principles.

Abstract: Leveraging multiple Large Language Models(LLMs) has proven effective for
addressing complex, high-dimensional tasks, but current approaches often rely
on static, manually engineered multi-agent configurations. To overcome these
constraints, we present the Agentic Neural Network(ANN), a framework that
conceptualizes multi-agent collaboration as a layered neural network
architecture. In this design, each agent operates as a node, and each layer
forms a cooperative "team" focused on a specific subtask. Agentic Neural
Network follows a two-phase optimization strategy: (1) Forward Phase-Drawing
inspiration from neural network forward passes, tasks are dynamically
decomposed into subtasks, and cooperative agent teams with suitable aggregation
methods are constructed layer by layer. (2) Backward Phase-Mirroring
backpropagation, we refine both global and local collaboration through
iterative feedback, allowing agents to self-evolve their roles, prompts, and
coordination. This neuro-symbolic approach enables ANN to create new or
specialized agent teams post-training, delivering notable gains in accuracy and
adaptability. Across four benchmark datasets, ANN surpasses leading multi-agent
baselines under the same configurations, showing consistent performance
improvements. Our findings indicate that ANN provides a scalable, data-driven
framework for multi-agent systems, combining the collaborative capabilities of
LLMs with the efficiency and flexibility of neural network principles. We plan
to open-source the entire framework.

</details>


### [157] [Understanding Task Vectors in In-Context Learning: Emergence, Functionality, and Limitations](https://arxiv.org/abs/2506.09048)
*Yuxin Dong,Jiachen Jiang,Zhihui Zhu,Xia Ning*

Main category: cs.LG

TL;DR: Task vectors in in-context learning (ICL) are explored through the Linear Combination Conjecture, which explains their emergence and limitations. Findings suggest task vectors result from linear combinations of original demonstrations, fail at high-rank mappings, and can be improved with multiple injections.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying principles governing the emergence and functionality of task vectors in ICL, as their empirical success lacks clear theoretical explanation.

Method: Propose the Linear Combination Conjecture suggesting task vectors form through linear combinations of original demonstrations. Validate this conjecture using loss landscape analysis in linear transformers trained on triplet-formatted prompts, predict and confirm task vector failures in high-rank mappings, and validate findings through saliency analyses and parameter visualization.

Result: Confirmed that task vectors naturally emerge in linear transformers through loss landscape analysis. Predicted and confirmed their failure to represent high-rank mappings. Suggested improvement of task vectors by injecting multiple ones into few-shot prompts.

Conclusion: This work advances the understanding of task vectors and sheds light on the mechanisms underlying ICL in transformer-based models.

Abstract: Task vectors offer a compelling mechanism for accelerating inference in
in-context learning (ICL) by distilling task-specific information into a
single, reusable representation. Despite their empirical success, the
underlying principles governing their emergence and functionality remain
unclear. This work proposes the Linear Combination Conjecture, positing that
task vectors act as single in-context demonstrations formed through linear
combinations of the original ones. We provide both theoretical and empirical
support for this conjecture. First, we show that task vectors naturally emerge
in linear transformers trained on triplet-formatted prompts through loss
landscape analysis. Next, we predict the failure of task vectors on
representing high-rank mappings and confirm this on practical LLMs. Our
findings are further validated through saliency analyses and parameter
visualization, suggesting an enhancement of task vectors by injecting multiple
ones into few-shot prompts. Together, our results advance the understanding of
task vectors and shed light on the mechanisms underlying ICL in
transformer-based models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [158] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: This paper presents GradEscape, a gradient-based method to attack AI-generated text detectors. It solves undifferentiable computation in text through weighted embeddings and adapts to various detector architectures using warm-started evader method. Evaluated on four datasets and three language models, it surpasses state-of-the-art evaders with fewer parameters and successfully attacks real-world commercial detectors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop an effective method to attack AI-generated text (AIGT) detectors by overcoming challenges such as the discrete nature of text and tokenizer mismatch between evader and detector.

Method: GradEscape constructs weighted embeddings for detector input to address the undifferentiable computation problem and updates its model parameters based on feedback from victim detectors. It uses a warm-started evader method to adapt to different language model architectures and employs tokenizer inference and model extraction techniques for effective evasion.

Result: GradEscape outperforms existing evaders across various scenarios, including when using an 11B paraphrase model, while only utilizing 139M parameters. It has been successfully applied to two real-world commercial AIGT detectors.

Conclusion: The primary vulnerability of AIGT detectors lies in the disparity of text expression styles within training data. The authors propose a defense strategy and open-source GradEscape to help develop more robust AIGT detectors.

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


### [159] [Interpreting Agent Behaviors in Reinforcement-Learning-Based Cyber-Battle Simulation Platforms](https://arxiv.org/abs/2506.08192)
*Jared Claypoole,Steven Cheung,Ashish Gehani,Vinod Yegneswaran,Ahmad Ridley*

Main category: cs.CR

TL;DR: This paper analyzes two open source deep reinforcement learning agents used in a cyber defense challenge, demonstrating methods to interpret agent successes and failures by simplifying state/action spaces and tracking important events. It identifies patterns in infiltration/clearing events, evaluates action effectiveness, examines decoy service impacts, and discusses the realism of the challenge.


<details>
  <summary>Details</summary>
Motivation: To understand and interpret the successes and failures of deep reinforcement learning agents in defending a simulated network against attack agents in the CAGE Challenge 2.

Method: Simplify complex state and action spaces and track important events within evaluation episodes to analyze the behavior of both defense and attack agents.

Result: Defenders can generally clear infiltrations within one or two timesteps after exploitation. Certain actions are found to be between 40% and 99% ineffective. Decoy services block up to 94% of exploits that would grant privileged access.

Conclusion: The analysis provides insights into the interpretation of agent behaviors in cyber defense scenarios, highlighting the importance of certain actions and the effectiveness of decoy services. The authors also discuss the realism of the challenge and improvements made in CAGE Challenge 4.

Abstract: We analyze two open source deep reinforcement learning agents submitted to
the CAGE Challenge 2 cyber defense challenge, where each competitor submitted
an agent to defend a simulated network against each of several provided
rules-based attack agents. We demonstrate that one can gain interpretability of
agent successes and failures by simplifying the complex state and action spaces
and by tracking important events, shedding light on the fine-grained behavior
of both the defense and attack agents in each experimental scenario. By
analyzing important events within an evaluation episode, we identify patterns
in infiltration and clearing events that tell us how well the attacker and
defender played their respective roles; for example, defenders were generally
able to clear infiltrations within one or two timesteps of a host being
exploited. By examining transitions in the environment's state caused by the
various possible actions, we determine which actions tended to be effective and
which did not, showing that certain important actions are between 40% and 99%
ineffective. We examine how decoy services affect exploit success, concluding
for instance that decoys block up to 94% of exploits that would directly grant
privileged access to a host. Finally, we discuss the realism of the challenge
and ways that the CAGE Challenge 4 has addressed some of our concerns.

</details>


### [160] [gh0stEdit: Exploiting Layer-Based Access Vulnerability Within Docker Container Images](https://arxiv.org/abs/2506.08218)
*Alan Mills,Jonathan White,Phil Legg*

Main category: cs.CR

TL;DR: 研究人员发现并展示了一种名为gh0stEdit的漏洞，该漏洞可以恶意修改Docker镜像而不留下痕迹，甚至对已签名镜像也有效。通过两个攻击案例研究，揭示了Docker镜像在安全和信任机制上的问题。


<details>
  <summary>Details</summary>
Motivation: 当前Docker镜像的安全性和透明性依赖于其历史记录、层级结构和命令显示，但缺乏对潜在深层次篡改的有效检测手段。

Method: 利用gh0stEdit漏洞，可以在不被图像历史、层次结构或命令显示的情况下恶意编辑Docker镜像，并且不会使已签名图像的签名失效。

Result: 成功展示了两种使用gh0stEdit进行镜像投毒的案例，这些篡改无法通过静态或动态扫描工具检测到。

Conclusion: gh0stEdit暴露了Docker镜像安全与信任机制中的重大缺陷，可能在实际环境中被利用而不被察觉，这是首次对此漏洞进行详细讨论。

Abstract: Containerisation is a popular deployment process for application-level
virtualisation using a layer-based approach. Docker is a leading provider of
containerisation, and through the Docker Hub, users can supply Docker images
for sharing and re-purposing popular software application containers. Using a
combination of in-built inspection commands, publicly displayed image layer
content, and static image scanning, Docker images are designed to ensure end
users can clearly assess the content of the image before running them. In this
paper we present \textbf{\textit{gh0stEdit}}, a vulnerability that
fundamentally undermines the integrity of Docker images and subverts the
assumed trust and transparency they utilise. The use of gh0stEdit allows an
attacker to maliciously edit Docker images, in a way that is not shown within
the image history, hierarchy or commands. This attack can also be carried out
against signed images (Docker Content Trust) without invalidating the image
signature. We present two use case studies for this vulnerability, and showcase
how gh0stEdit is able to poison an image in a way that is not picked up through
static or dynamic scanning tools. Our attack case studies highlight the issues
in the current approach to Docker image security and trust, and expose an
attack method which could potentially be exploited in the wild without being
detected. To the best of our knowledge we are the first to provide detailed
discussion on the exploit of this vulnerability.

</details>


### [161] [PoSyn: Secure Power Side-Channel Aware Synthesis](https://arxiv.org/abs/2506.08252)
*Amisha Srivastava,Samit S. Miftah,Hyunmin Kim,Debjit Pal,Kanad Basu*

Main category: cs.CR

TL;DR: Power Side-Channel (PSC) attacks use power consumption patterns to extract sensitive information. Traditional countermeasures face challenges. This paper introduces PoSyn, a novel logic synthesis framework designed to enhance cryptographic hardware resistance against PSC attacks. It centers on optimal bipartite mapping of vulnerable RTL components to standard cells from the technology library and utilizes a cost function integrating critical characteristics from both the RTL design and the standard cell library. Experimental results demonstrate a substantial reduction in success rates for Differential Power Analysis (DPA) and Correlation Power Analysis (CPA) attacks.


<details>
  <summary>Details</summary>
Motivation: Traditional countermeasures for PSC attacks face challenges including complex integration during synthesis, substantial area overhead, and susceptibility to optimization removal during logic synthesis.

Method: PoSyn is a novel logic synthesis framework that uses optimal bipartite mapping of vulnerable RTL components to standard cells from the technology library. It utilizes a cost function integrating critical characteristics from both the RTL design and the standard cell library.

Result: Experimental results demonstrate a substantial reduction in success rates for Differential Power Analysis (DPA) and Correlation Power Analysis (CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further confirms that synthesized netlists exhibit negligible leakage. Additionally, compared to conventional countermeasures like masking and shuffling, PoSyn significantly lowers attack success rates, achieving reductions of up to 72%, while simultaneously enhancing area efficiency by as much as 3.79 times.

Conclusion: PoSyn enhances cryptographic hardware resistance against PSC attacks by minimizing mutual information leakage and strategically modifying mapping criteria during RTL-to-netlist conversion without altering design functionality.

Abstract: Power Side-Channel (PSC) attacks exploit power consumption patterns to
extract sensitive information, posing risks to cryptographic operations crucial
for secure systems. Traditional countermeasures, such as masking, face
challenges including complex integration during synthesis, substantial area
overhead, and susceptibility to optimization removal during logic synthesis. To
address these issues, we introduce PoSyn, a novel logic synthesis framework
designed to enhance cryptographic hardware resistance against PSC attacks. Our
method centers on optimal bipartite mapping of vulnerable RTL components to
standard cells from the technology library, aiming to minimize PSC leakage. By
utilizing a cost function integrating critical characteristics from both the
RTL design and the standard cell library, we strategically modify mapping
criteria during RTL-to-netlist conversion without altering design
functionality. Furthermore, we theoretically establish that PoSyn minimizes
mutual information leakage, strengthening its security against PSC
vulnerabilities. We evaluate PoSyn across various cryptographic hardware
implementations, including AES, RSA, PRESENT, and post-quantum cryptographic
algorithms such as Saber and CRYSTALS-Kyber, at technology nodes of 65nm, 45nm,
and 15nm. Experimental results demonstrate a substantial reduction in success
rates for Differential Power Analysis (DPA) and Correlation Power Analysis
(CPA) attacks, achieving lows of 3% and 6%, respectively. TVLA analysis further
confirms that synthesized netlists exhibit negligible leakage. Additionally,
compared to conventional countermeasures like masking and shuffling, PoSyn
significantly lowers attack success rates, achieving reductions of up to 72%,
while simultaneously enhancing area efficiency by as much as 3.79 times.

</details>


### [162] [How Good LLM-Generated Password Policies Are?](https://arxiv.org/abs/2506.08320)
*Vivek Vaidya,Aditya Patwardhan,Ashish Kundu*

Main category: cs.CR

TL;DR: Generative AI, especially LLMs, despite their strengths in natural language processing, face challenges with output inconsistency and unpredictability. This paper examines the use of LLMs for generating password policies in Cybersecurity Access Control Systems, assessing soundness, accuracy, and consistency through two approaches.


<details>
  <summary>Details</summary>
Motivation: To address the inconsistency and unpredictability issues of LLM outputs, particularly focusing on their application in security-critical domains such as access control systems.

Method: The study uses two methods to generate password policies: one involves using pre-trained LLMs with only natural language prompts, and the other provides the models with official pwquality.conf documentation as guidance.

Result: The research found significant challenges with the current generation of LLMs when generating configurations, highlighting areas that need improvement.

Conclusion: LLMs present substantial challenges in consistency and reliability for access control systems, necessitating further refinement before deployment in such critical domains.

Abstract: Generative AI technologies, particularly Large Language Models (LLMs), are
rapidly being adopted across industry, academia, and government sectors, owing
to their remarkable capabilities in natural language processing. However,
despite their strengths, the inconsistency and unpredictability of LLM outputs
present substantial challenges, especially in security-critical domains such as
access control. One critical issue that emerges prominently is the consistency
of LLM-generated responses, which is paramount for ensuring secure and reliable
operations.
  In this paper, we study the application of LLMs within the context of
Cybersecurity Access Control Systems. Specifically, we investigate the
consistency and accuracy of LLM-generated password policies, translating
natural language prompts into executable pwquality.conf configuration files.
Our experimental methodology adopts two distinct approaches: firstly, we
utilize pre-trained LLMs to generate configuration files purely from natural
language prompts without additional guidance. Secondly, we provide these models
with official pwquality.conf documentation to serve as an informative baseline.
We systematically assess the soundness, accuracy, and consistency of these
AI-generated configurations. Our findings underscore significant challenges in
the current generation of LLMs and contribute valuable insights into refining
the deployment of LLMs in Access Control Systems.

</details>


### [163] [Distortion Search, A Web Search Privacy Heuristic](https://arxiv.org/abs/2506.08330)
*Kato Mivule,Kenneth Hopkinson*

Main category: cs.CR

TL;DR: 通过用户中心的启发式方法Distortion Search，可以在不依赖搜索引擎或其他第三方的情况下实现网络搜索查询和特定用户意图的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 当前搜索引擎能够记录用户的搜索日志，存在泄露用户意图的重大隐私漏洞。此外，现有的许多网络搜索隐私增强工具需要用户信任第三方，增加了用户意图保密的难度。

Method: 提出了一种名为Distortion Search的用户中心启发式方法，通过混淆搜索关键词类别、应用k-匿名化的网络点击策略，生成扭曲的用户画像以保护用户意图和查询隐私。

Result: 初步实验结果表明，通过评估被扭曲的网络搜索查询在检索结果和相关广告方面的表现，证明了在用户端实现网络搜索查询和特定用户意图隐私保护的可能性。

Conclusion: Distortion Search方法为用户提供了一种无需依赖搜索引擎或第三方的隐私保护手段，增强了用户对自身隐私的控制能力。

Abstract: Search engines have vast technical capabilities to retain Internet search
logs for each user and thus present major privacy vulnerabilities to both
individuals and organizations in revealing user intent. Additionally, many of
the web search privacy enhancing tools available today require that the user
trusts a third party, which make confidentiality of user intent even more
challenging. The user is left at the mercy of the third party without the
control over his or her own privacy. In this article, we suggest a user-centric
heuristic, Distortion Search, a web search query privacy methodology that works
by the formation of obfuscated search queries via the permutation of query
keyword categories, and by strategically applying k-anonymised web navigational
clicks on URLs and Ads to generate a distorted user profile and thus providing
specific user intent and query confidentiality. We provide empirical results
via the evaluation of distorted web search queries in terms of retrieved search
results and the resulting web ads from search engines. Preliminary experimental
results indicate that web search query and specific user intent privacy might
be achievable from the user side without the involvement of the search engine
or other third parties.

</details>


### [164] [Your Agent Can Defend Itself against Backdoor Attacks](https://arxiv.org/abs/2506.08336)
*Li Changjiang,Liang Jiacheng,Cao Bochuan,Chen Jinghui,Wang Ting*

Main category: cs.CR

TL;DR: ReAgent is a novel defense mechanism against backdoor attacks on LLM-based agents, employing a two-level approach to detect inconsistencies in the agent's actions and thoughts, significantly reducing attack success rates.


<details>
  <summary>Details</summary>
Motivation: Large language model-powered agents face significant security risks from backdoor attacks during training and fine-tuning, which can lead to malicious operations when triggered.

Method: ReAgent uses a two-level approach: at the execution level, it verifies consistency between the agent's thoughts and actions; at the planning level, it reconstructs the instruction based on the agent's thought trajectory and checks for consistency with the user's instruction.

Result: ReAgent effectively reduces the attack success rate by up to 90% in database operation tasks, outperforming existing defenses.

Conclusion: This work highlights the potential of using compromised agents themselves to mitigate backdoor risks.

Abstract: Despite their growing adoption across domains, large language model
(LLM)-powered agents face significant security risks from backdoor attacks
during training and fine-tuning. These compromised agents can subsequently be
manipulated to execute malicious operations when presented with specific
triggers in their inputs or environments. To address this pressing risk, we
present ReAgent, a novel defense against a range of backdoor attacks on
LLM-based agents. Intuitively, backdoor attacks often result in inconsistencies
among the user's instruction, the agent's planning, and its execution. Drawing
on this insight, ReAgent employs a two-level approach to detect potential
backdoors. At the execution level, ReAgent verifies consistency between the
agent's thoughts and actions; at the planning level, ReAgent leverages the
agent's capability to reconstruct the instruction based on its thought
trajectory, checking for consistency between the reconstructed instruction and
the user's instruction. Extensive evaluation demonstrates ReAgent's
effectiveness against various backdoor attacks across tasks. For instance,
ReAgent reduces the attack success rate by up to 90\% in database operation
tasks, outperforming existing defenses by large margins. This work reveals the
potential of utilizing compromised agents themselves to mitigate backdoor
risks.

</details>


### [165] [GPS Spoofing Attacks on AI-based Navigation Systems with Obstacle Avoidance in UAV](https://arxiv.org/abs/2506.08445)
*Ji Hyuk Jung,Mi Yeon Hong,Ji Won Yoon*

Main category: cs.CR

TL;DR: The paper explores security vulnerabilities in DRL-based UAV navigation systems, focusing on GPS spoofing attacks. It presents an attack model combining GPS spoofing with DRL systems and experimentally demonstrates the feasibility of such attacks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic studies on security aspects in DRL-based UAV navigation systems, particularly concerning GPS spoofing attacks.

Method: Develop an attack model that incorporates GPS spoofing against Extended Kalman Filter (EKF) sensor fusion in PX4 autopilot systems, combined with DRL-based navigation systems to create realistic attack scenarios.

Result: Experimental results show that GPS spoofing attacks are possible both in basic DRL systems and in more complex systems integrated with PX4 autopilot.

Conclusion: DRL-based UAV navigation systems are vulnerable to GPS spoofing attacks, highlighting the need for further research into securing these systems.

Abstract: Recently, approaches using Deep Reinforcement Learning (DRL) have been
proposed to solve UAV navigation systems in complex and unknown environments.
However, despite extensive research and attention, systematic studies on
various security aspects have not yet been conducted. Therefore, in this paper,
we conduct research on security vulnerabilities in DRL-based navigation
systems, particularly focusing on GPS spoofing attacks against the system. Many
recent basic DRL-based navigation systems fundamentally share an efficient
structure. This paper presents an attack model that operates through GPS
spoofing attacks briefly modeling the range of spoofing attack against EKF
sensor fusion of PX4 autopilot, and combine this with the DRL-based system to
design attack scenarios that are closer to reality. Finally, this paper
experimentally demonstrated that attacks are possible both in the basic DRL
system and in attack models combining the DRL system with PX4 autopilot system.

</details>


### [166] [One Patch to Rule Them All: Transforming Static Patches into Dynamic Attacks in the Physical World](https://arxiv.org/abs/2506.08482)
*Xingshuo Han,Chen Ling,Shiyi Yao,Haozhao Wang,Hangcheng Liu,Yutong Wu,Shengmin Xu,Changhai Ou,Xinyi Huang,Tianwei Zhang*

Main category: cs.CR

TL;DR: 提出了一种新的物理对抗补丁SwitchPatch，它在静态条件下能依据实时场景实现动态和可控的攻击效果。通过改变预定义条件（如投影不同自然色光），无需重新生成或部署即可切换攻击目标，同时在无启用条件下保持良性，增强了隐蔽性。实验验证了其在交通标志识别和深度估计任务中的可行性、有效性和物理世界的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的物理对抗补丁（PAPs）只能支持单一固定的攻击目标，在动态环境中（如自动驾驶）实用性受限。例如，当目标车辆周围无障碍物时，攻击可能无法产生有意义的后果。因此需要一种更灵活的PAP来适应不同的攻击目标和环境变化。

Method: 提出SwitchPatch，一种静态却能够根据实时场景实现动态和可控攻击结果的新方法。通过改变预定义条件（如投影不同自然色彩的光），可以无缝切换攻击目标，而无需重新生成或部署新的PAP。此外，SwitchPatch在未激活条件下表现为良性，从而增强了隐蔽性。

Result: 1. 理论分析与实证研究证明了SwitchPatch的可行性，并探讨了其可以通过颜色投影和遮挡等技术支持的攻击目标数量。
2. 模拟实验和消融研究验证了SwitchPatch的有效性和可迁移性。
3. 在无人地面车辆（UGV）上的户外测试表明SwitchPatch在物理世界中的鲁棒性。
整体而言，SwitchPatch展示出适应多种任务和真实世界条件的能力。

Conclusion: SwitchPatch是一种灵活且实用的对抗策略，能够在不重新生成或部署的情况下实现多目标攻击，适应多样化的任务和现实条件，为实际应用提供了更大的灵活性和更低的成本。

Abstract: Numerous methods have been proposed to generate physical adversarial patches
(PAPs) against real-world machine learning systems. However, each existing PAP
typically supports only a single, fixed attack goal, and switching to a
different objective requires re-generating and re-deploying a new PAP. This
rigidity limits their practicality in dynamic environments like autonomous
driving, where traffic conditions and attack goals can change rapidly. For
example, if no obstacles are present around the target vehicle, the attack may
fail to cause meaningful consequences.
  To overcome this limitation, we propose SwitchPatch, a novel PAP that is
static yet enables dynamic and controllable attack outcomes based on real-time
scenarios. Attackers can alter pre-defined conditions, e.g., by projecting
different natural-color lights onto SwitchPatch to seamlessly switch between
attack goals. Unlike prior work, SwitchPatch does not require re-generation or
re-deployment for different objectives, significantly reducing cost and
complexity. Furthermore, SwitchPatch remains benign when the enabling
conditions are absent, enhancing its stealth.
  We evaluate SwitchPatch on two key tasks: traffic sign recognition
(classification and detection) and depth estimation. First, we conduct
theoretical analysis and empirical studies to demonstrate the feasibility of
SwitchPatch and explore how many goals it can support using techniques like
color light projection and occlusion. Second, we perform simulation-based
experiments and ablation studies to verify its effectiveness and
transferability. Third, we conduct outdoor tests using a Unmanned Ground
Vehicle (UGV) to confirm its robustness in the physical world. Overall,
SwitchPatch introduces a flexible and practical adversarial strategy that can
be adapted to diverse tasks and real-world conditions.

</details>


### [167] [WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks](https://arxiv.org/abs/2506.08602)
*Tingzhi Li,Xuefeng Liu*

Main category: cs.CR

TL;DR: The paper introduces WGLE, a new black-box watermarking method for GNNs that embeds multi-bit strings as ownership information without using backdoors. It achieves 100% verification accuracy with minimal fidelity degradation.


<details>
  <summary>Details</summary>
Motivation: Current fingerprinting and black-box watermarking methods for GNNs are either computationally expensive or expose watermarked models to backdoor attacks. Additionally, they do not convey additional information and require multiple queries, increasing financial cost and detection risk.

Method: WGLE is based on Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between feature distance and prediction distance of two connected nodes. By predefining positive or negative LDDE values for selected edges, WGLE embeds watermark encoding intended information without introducing incorrect mappings.

Result: WGLE was evaluated on six public datasets and six GNN architectures. It achieved 100% ownership verification accuracy, an average fidelity degradation of 0.85%, comparable robustness against attacks, and low embedding overhead.

Conclusion: WGLE offers a novel approach to watermark GNNs that overcomes limitations of existing methods by enabling multi-bit string embedding without backdoors, ensuring high verification accuracy and minimal performance impact.

Abstract: Graph Neural Networks (GNNs) are increasingly deployed in graph-related
applications, making ownership verification critical to protect their
intellectual property against model theft. Fingerprinting and black-box
watermarking are two main methods. However, the former relies on determining
model similarity, which is computationally expensive and prone to ownership
collisions after model post-processing such as model pruning or fine-tuning.
The latter embeds backdoors, exposing watermarked models to the risk of
backdoor attacks. Moreover, both methods enable ownership verification but do
not convey additional information. As a result, each distributed model requires
a unique trigger graph, and all trigger graphs must be used to query the
suspect model during verification. Multiple queries increase the financial cost
and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box
watermarking paradigm for GNNs that enables embedding the multi-bit string as
the ownership information without using backdoors. WGLE builds on a key insight
we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the
difference between the feature distance and the prediction distance of two
connected nodes. By predefining positive or negative LDDE values for multiple
selected edges, WGLE embeds the watermark encoding the intended information
without introducing incorrect mappings that compromise the primary task. WGLE
is evaluated on six public datasets and six mainstream GNN architectures along
with state-of-the-art methods. The results show that WGLE achieves 100%
ownership verification accuracy, an average fidelity degradation of 0.85%,
comparable robustness against potential attacks, and low embedding overhead.
The code is available in the repository.

</details>


### [168] [On the Ethics of Using LLMs for Offensive Security](https://arxiv.org/abs/2506.08693)
*Andreas Happe,Jürgen Cito*

Main category: cs.CR

TL;DR: This paper examines a collection of studies using Large Language Models (LLMs) in offensive security, focusing on the communication of ethical considerations within this research area. Results indicate that most reviewed prototypes acknowledge dual-use risks and provide motivations such as enhancing penetration-testing access and readiness against AI-based attacks.


<details>
  <summary>Details</summary>
Motivation: To explore how ethical considerations are expressed and justified in research leveraging LLMs for offensive security.

Method: Analysis of a set of papers utilizing LLMs in offensive security to assess the culture of AI in this research field concerning ethics communication.

Result: 86.6% of the reviewed prototypes mentioned ethical considerations, showing awareness of dual-use risks. Main motivations include improving access to penetration-testing and preparing defenders for AI-guided attackers.

Conclusion: The academic community is navigating the balance between innovation and ethical responsibility in offensive security research with LLMs.

Abstract: Large Language Models (LLMs) have rapidly evolved over the past few years and
are currently evaluated for their efficacy within the domain of offensive
cyber-security. While initial forays showcase the potential of LLMs to enhance
security research, they also raise critical ethical concerns regarding the
dual-use of offensive security tooling.
  This paper analyzes a set of papers that leverage LLMs for offensive
security, focusing on how ethical considerations are expressed and justified in
their work. The goal is to assess the culture of AI in offensive security
research regarding ethics communication, highlighting trends, best practices,
and gaps in current discourse.
  We provide insights into how the academic community navigates the fine line
between innovation and ethical responsibility. Particularly, our results show
that 13 of 15 reviewed prototypes (86.6\%) mentioned ethical considerations and
are thus aware of the potential dual-use of their research. Main motivation
given for the research was allowing broader access to penetration-testing as
well as preparing defenders for AI-guided attackers.

</details>


### [169] [Lightweight and High-Throughput Secure Logging for Internet of Things and Cold Cloud Continuum](https://arxiv.org/abs/2506.08781)
*Saif E. Nouma,Attila A. Yavuz*

Main category: cs.CR

TL;DR: This paper proposes POSLO, a new digital signature framework that provides constant-size signatures and public keys, near-optimal signing efficiency, and scalable verification for secure logging in the IoT-Cold-STaaS ecosystem.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the need for cryptographic primitives that balance security, performance, and scalability across the IoT-Cold-STaaS continuum. Existing solutions either burden low-end IoT devices with heavy computation or create verification delays and storage inefficiencies at Cold-STaaS.

Method: POSLO achieves its properties through efficient randomness management, flexible aggregation, and multiple algorithmic instantiations. It introduces a GPU-accelerated batch verification framework that exploits homomorphic signature aggregation for ultra-fast performance.

Result: POSLO can verify 231 log entries per second on a mid-range consumer GPU (NVIDIA GTX 3060) while being significantly more compact than state-of-the-art. It preserves signer-side efficiency, offering substantial battery savings for IoT devices.

Conclusion: POSLO is well-suited for the IoT-Cold-STaaS ecosystem, providing a novel solution for secure logging with constant-size signatures and public keys, near-optimal signing efficiency, and tunable fine-to-coarse-grained verification.

Abstract: The growing deployment of resource-limited Internet of Things (IoT) devices
and their expanding attack surfaces demand efficient and scalable security
mechanisms. System logs are vital for the trust and auditability of IoT, and
offloading their maintenance to a Cold Storage-as-a-Service (Cold-STaaS)
enhances cost-effectiveness and reliability. However, existing cryptographic
logging solutions either burden low-end IoT devices with heavy computation or
create verification delays and storage inefficiencies at Cold-STaaS. There is a
pressing need for cryptographic primitives that balance security, performance,
and scalability across IoT-Cold-STaaS continuum.
  In this work, we present Parallel Optimal Signatures for Secure Logging
(POSLO), a novel digital signature framework that, to our knowledge, is the
first to offer constant-size signatures and public keys, near-optimal signing
efficiency, and tunable fine-to-coarse-grained verification for log auditing.
POSLO achieves these properties through efficient randomness management,
flexible aggregation, and multiple algorithmic instantiations. It also
introduces a GPU-accelerated batch verification framework that exploits
homomorphic signature aggregation to deliver ultra-fast performance. For
example, POSLO can verify 231 log entries per second on a mid-range consumer
GPU (NVIDIA GTX 3060) while being significantly more compact than
state-of-the-art. POSLO also preserves signer-side efficiency, offering
substantial battery savings for IoT devices, and is well-suited for the
IoT-Cold-STaaS ecosystem.

</details>


### [170] [Lightweight Electronic Signatures and Reliable Access Control Included in Sensor Networks to Prevent Cyber Attacks from Modifying Patient Data](https://arxiv.org/abs/2506.08828)
*Mishall Al-Zubaidie*

Main category: cs.CR

TL;DR: Digital terrorism poses a threat to patient and healthcare provider data. Current strategies have weaknesses. This study proposes a new method combining symmetric cryptography, asymmetric cryptography, and Lesamnta-LW for enhanced security. Experimental results show improvement over previous methods.


<details>
  <summary>Details</summary>
Motivation: 数字恐怖主义对患者和医疗提供者的数据和信息安全构成了重大威胁，尽管已有策略，但仍然存在许多弱点。

Method: 将对称加密、非对称加密和Lesamnta-LW签名方法结合使用，以加强传感器收集并存储在基站数据集中的信息安全。

Result: 实验对比、安全分析和详细结果表明，所提出的方法优于以前的方法。

Conclusion: 该研究提出了一种新的可靠方法，通过结合多种安全机制来增强信息和数据的安全性。

Abstract: Digital terrorism is a major cause of securing patient/healthcare providers
data and information. Sensitive topics that may have an impact on a patient's
health or even national security include patient health records and information
on healthcare providers. Health databases and data sets have been continually
breached by many, regular assaults, as well as local and remote servers
equipped with wireless sensor networks (WSNs) in diverse locations. The problem
was addressed by some contemporary strategies that were created to stop these
assaults and guarantee the privacy of patient data and information transferred
and gathered by sensors. Nevertheless, the literature analysis outlines many
indications of weakness that persist in these methods. This study suggests a
novel, reliable method that bolsters the information security and data gathered
by sensors and kept on base station datasets. The proposed approach combines a
number of security mechanisms, including symmetric cryptography for encryption,
asymmetric cryptography for access control and signatures, and the Lesamnta-LW
method in the signature process. Users' information is shielded from prying
eyes by the careful application of these measures and a sound approach.
Investigational comparisons, security studies, and thorough results show that
the suggested method is better than earlier methods.

</details>


### [171] [ZTaint-Havoc: From Havoc Mode to Zero-Execution Fuzzing-Driven Taint Inference](https://arxiv.org/abs/2506.08838)
*Yuchong Xie,Wenhui Zhang,Dongdong She*

Main category: cs.CR

TL;DR: The paper introduces ZTaint-Havoc, an efficient Fuzzing-Driven Taint Inference (FTI) method that operates with minimal overhead by adapting the havoc mutation scheme in fuzzing. It improves edge coverage significantly compared to vanilla AFL++ in fuzzing campaigns.


<details>
  <summary>Details</summary>
Motivation: Fuzzing is crucial for finding software vulnerabilities, but identifying hot bytes influencing program behavior is challenging. Traditional taint analysis and existing FTI methods have limitations in scalability or runtime overhead.

Method: The authors develop a computational model of the havoc mode in fuzzing, showing it can perform FTI while generating new test cases. They propose ZTaint-Havoc, which uses this model for efficient FTI with low overhead. An effective mutation algorithm utilizing identified hot bytes is also designed.

Result: ZTaint-Havoc, implemented in AFL++, achieves significant improvements in edge coverage over vanilla AFL++. On FuzzBench, it improves coverage by up to 33.71% with an average gain of 2.97%, and on UniBench, it improves by up to 51.12% with an average gain of 6.12% in 24-hour fuzzing campaigns.

Conclusion: ZTaint-Havoc offers an efficient way to perform FTI with minimal overhead, enhancing fuzzing effectiveness in terms of edge coverage.

Abstract: Fuzzing is a widely used technique for discovering software vulnerabilities,
but identifying hot bytes that influence program behavior remains challenging.
Traditional taint analysis can track such bytes white-box, but suffers from
scalability issue. Fuzzing-Driven Taint Inference (FTI) offers a black-box
alternative, yet typically incurs significant runtime overhead due to extra
program executions. We observe that the commonly used havoc mutation scheme in
fuzzing can be adapted for lightweight FTI with zero extra executions. We
present a computational model of havoc mode, demonstrating that it can perform
FTI while generating new test cases. Building on this, we propose ZTaint-Havoc,
a novel, efficient FTI with minimal overhead (3.84% on UniBench, 12.58% on
FuzzBench). We further design an effective mutation algorithm utilizing the
identified hot bytes. Our comprehensive evaluation shows that ZTaint-Havoc,
implemented in AFL++, improves edge coverage by up to 33.71% on FuzzBench and
51.12% on UniBench over vanilla AFL++, with average gains of 2.97% and 6.12% in
24-hour fuzzing campaigns.

</details>


### [172] [SmartAttack: Air-Gap Attack via Smartwatches](https://arxiv.org/abs/2506.08866)
*Mordechai Guri*

Main category: cs.CR

TL;DR: SmartAttack uses smartwatches to receive ultrasonic signals for covert communication in air-gapped systems, demonstrating feasible data exfiltration under various conditions and discussing mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of smartwatches as an effective attack vector for ultrasonic covert channels in air-gapped systems, despite their physical isolation from external networks.

Method: Propose SmartAttack, leveraging smartwatch microphones to capture ultrasonic signals (18-22 kHz) in real time. Evaluate feasibility through experiments varying environmental conditions, distances, orientations, and noise levels. Analyze smartwatch-specific factors impacting signal propagation.

Result: Experimental results show that ultrasonic covert communication via smartwatches is feasible under various conditions, with factors such as human body presence and microphone directionality influencing effectiveness.

Conclusion: Smartwatches pose security risks in high-security environments due to their ability to facilitate ultrasonic covert channels; mitigation strategies are necessary to counteract this threat.

Abstract: Air-gapped systems are considered highly secure against data leaks due to
their physical isolation from external networks. Despite this protection,
ultrasonic communication has been demonstrated as an effective method for
exfiltrating data from such systems. While smartphones have been extensively
studied in the context of ultrasonic covert channels, smartwatches remain an
underexplored yet effective attack vector.
  In this paper, we propose and evaluate SmartAttack, a novel method that
leverages smartwatches as receivers for ultrasonic covert communication in
air-gapped environments. Our approach utilizes the built-in microphones of
smartwatches to capture covert signals in real time within the ultrasonic
frequency range of 18-22 kHz. Through experimental validation, we assess the
feasibility of this attack under varying environmental conditions, distances,
orientations, and noise levels. Furthermore, we analyze smartwatch-specific
factors that influence ultrasonic covert channels, including their continuous
presence on the user's wrist, the impact of the human body on signal
propagation, and the directional constraints of built-in microphones. Our
findings highlight the security risks posed by smartwatches in high-security
environments and outline mitigation strategies to counteract this emerging
threat.

</details>


### [173] [Quantifying Mix Network Privacy Erosion with Generative Models](https://arxiv.org/abs/2506.08918)
*Vasilios Mavroudis,Tariq Elahi*

Main category: cs.CR

TL;DR: Modern mix networks offer stronger privacy than Tor by obfuscating metadata. This study uses a generative model trained on mixnet traffic to estimate privacy loss over time, revealing differences in privacy levels among mix strategies and demonstrating the limitations of traditional privacy metrics.


<details>
  <summary>Details</summary>
Motivation: Modern mix networks provide robust privacy by obfuscating metadata, but the complexity of mixing mechanisms makes it hard to estimate cumulative privacy erosion over time.

Method: The researchers train a large-language model from scratch on specialized network traffic data to measure sender-message unlinkability in various settings, including different mixing strategies and security parameters.

Result: Notable differences in privacy levels were found among mix strategies with similar mean latencies. Traditional privacy metrics like entropy and log-likelihood were shown to be insufficient for capturing an adversary's potential to synthesize information from multiple observations. Larger models exhibited greater sample efficiency and superior capabilities.

Conclusion: Advancements in transformers will enhance the accuracy of model-based privacy estimates.

Abstract: Modern mix networks improve over Tor and provide stronger privacy guarantees
by robustly obfuscating metadata. As long as a message is routed through at
least one honest mixnode, the privacy of the users involved is safeguarded.
However, the complexity of the mixing mechanisms makes it difficult to estimate
the cumulative privacy erosion occurring over time. This work uses a generative
model trained on mixnet traffic to estimate the loss of privacy when users
communicate persistently over a period of time. We train our large-language
model from scratch on our specialized network traffic ``language'' and then use
it to measure the sender-message unlinkability in various settings (e.g. mixing
strategies, security parameters, observation window). Our findings reveal
notable differences in privacy levels among mix strategies, even when they have
similar mean latencies. In comparison, we demonstrate the limitations of
traditional privacy metrics, such as entropy and log-likelihood, in fully
capturing an adversary's potential to synthesize information from multiple
observations. Finally, we show that larger models exhibit greater sample
efficiency and superior capabilities implying that further advancements in
transformers will consequently enhance the accuracy of model-based privacy
estimates.

</details>


### [174] [Striking Back At Cobalt: Using Network Traffic Metadata To Detect Cobalt Strike Masquerading Command and Control Channels](https://arxiv.org/abs/2506.08922)
*Clément Parssegny,Johan Mazel,Olivier Levillain,Pierre Chifflier*

Main category: cs.CR

TL;DR: A machine learning method is presented to detect Cobalt Strike Command and Control activity based on network traffic metadata. The method adapts its model to observed traffic for optimized performance, performing equal to or better than current standards while being more production-friendly and explainable.


<details>
  <summary>Details</summary>
Motivation: Cobalt Strike is a popular tool used by attackers which can mimic popular websites' network traffic, making it difficult for Security Operation Centers and defense actors to detect malicious Command and Control traffic, especially when encryption protocols like TLS are used.

Method: The paper proposes a machine learning-based method that uses widely adopted network traffic metadata to detect Cobalt Strike Command and Control activities. It introduces a model that adapts to the observed traffic to enhance detection performance.

Result: The proposed method performs equally or better than existing state-of-the-art methods while utilizing standard features, making it easier to implement in a production environment and more understandable.

Conclusion: This is reportedly the first method of its kind that can adapt its model to observed traffic for improved performance in detecting Cobalt Strike Command and Control activity using network traffic metadata.

Abstract: Off-the-shelf software for Command and Control is often used by attackers and
legitimate pentesters looking for discretion. Among other functionalities,
these tools facilitate the customization of their network traffic so it can
mimic popular websites, thereby increasing their secrecy. Cobalt Strike is one
of the most famous solutions in this category, used by known advanced attacker
groups such as "Mustang Panda" or "Nobelium". In response to these threats,
Security Operation Centers and other defense actors struggle to detect Command
and Control traffic, which often use encryption protocols such as TLS. Network
traffic metadata-based machine learning approaches have been proposed to detect
encrypted malware communications or fingerprint websites over Tor network. This
paper presents a machine learning-based method to detect Cobalt Strike Command
and Control activity based only on widely used network traffic metadata. The
proposed method is, to the best of our knowledge, the first of its kind that is
able to adapt the model it uses to the observed traffic to optimize its
performance. This specificity permits our method to performs equally or better
than the state of the art while using standard features. Our method is thus
easier to use in a production environment and more explainable.

</details>


### [175] [Navigating Cookie Consent Violations Across the Globe](https://arxiv.org/abs/2506.08996)
*Brian Tang,Duc Bui,Kang G. Shin*

Main category: cs.CR

TL;DR: An end-to-end system called ConsentChk is proposed to detect and analyze cookie banner behavior, revealing that cookie consent violation rates are highly dependent on region and often due to incorrect interpretations of regional privacy laws.


<details>
  <summary>Details</summary>
Motivation: To ensure that cookie consent is compliant with privacy laws around the globe and address the problem of cookie consent violations where cookies are placed on browsers even after their explicit rejection by users.

Method: Propose an end-to-end system named ConsentChk which uses a formal model to systematically detect and categorize cookie consent violations, investigating eight English-speaking regions across the world and analyzing cookie banner behavior across 1,793 globally-popular websites.

Result: Cookie behavior, consent violation rates, and banner implementations are found to be highly dependent on region. CMPs and developers likely tailor cookie banner configurations based on their (often incorrect) interpretations of regional privacy laws.

Conclusion: The implementations produce misleading cookie banners, indicating inconsistently implemented and enforced cookie consent between various regions.

Abstract: Online services provide users with cookie banners to accept/reject the
cookies placed on their web browsers. Despite the increased adoption of cookie
banners, little has been done to ensure that cookie consent is compliant with
privacy laws around the globe. Prior studies have found that cookies are often
placed on browsers even after their explicit rejection by users. These
inconsistencies in cookie banner behavior circumvent users' consent preferences
and are known as cookie consent violations. To address this important problem,
we propose an end-to-end system, called ConsentChk, that detects and analyzes
cookie banner behavior. ConsentChk uses a formal model to systematically detect
and categorize cookie consent violations. We investigate eight English-speaking
regions across the world, and analyze cookie banner behavior across 1,793
globally-popular websites. Cookie behavior, cookie consent violation rates, and
cookie banner implementations are found to be highly dependent on region. Our
evaluation reveals that consent management platforms (CMPs) and website
developers likely tailor cookie banner configurations based on their (often
incorrect) interpretations of regional privacy laws. We discuss various root
causes behind these cookie consent violations. The resulting implementations
produce misleading cookie banners, indicating the prevalence of inconsistently
implemented and enforced cookie consent between various regions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [176] [SPBA: Utilizing Speech Large Language Model for Backdoor Attacks on Speech Classification Models](https://arxiv.org/abs/2506.08346)
*Wenhan Yao,Fen Xiao,Xiarun Chen,Jia Liu,YongQiang He,Weiping Wen*

Main category: cs.SD

TL;DR: The paper proposes Speech Prompt Backdoor Attack (SPBA) that focuses on speech elements using Speech Large Language Model (SLLM) to generate diverse triggers and introduces MGDA as a mitigation strategy. Experiments show SPBA's effectiveness in two speech classification tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of creating only a limited number of backdoors in speech-based human-computer interaction technologies due to the inherent constraints of the trigger function.

Method: Propose SPBA which strategically focuses on speech elements like timbre and emotion, leveraging SLLM to generate diverse triggers and introduce MGDA as a mitigation strategy.

Result: SPBA demonstrates significant trigger effectiveness and achieves exceptional performance in attack metrics through experiments on two speech classification tasks.

Conclusion: SPBA is effective in speech backdoor attacks with diverse triggers and MGDA can be used as a mitigation strategy.

Abstract: Deep speech classification tasks, including keyword spotting and speaker
verification, are vital in speech-based human-computer interaction. Recently,
the security of these technologies has been revealed to be susceptible to
backdoor attacks. Specifically, attackers use noisy disruption triggers and
speech element triggers to produce poisoned speech samples that train models to
become vulnerable. However, these methods typically create only a limited
number of backdoors due to the inherent constraints of the trigger function. In
this paper, we propose that speech backdoor attacks can strategically focus on
speech elements such as timbre and emotion, leveraging the Speech Large
Language Model (SLLM) to generate diverse triggers. Increasing the number of
triggers may disproportionately elevate the poisoning rate, resulting in higher
attack costs and a lower success rate per trigger. We introduce the Multiple
Gradient Descent Algorithm (MGDA) as a mitigation strategy to address this
challenge. The proposed attack is called the Speech Prompt Backdoor Attack
(SPBA). Building on this foundation, we conducted attack experiments on two
speech classification tasks, demonstrating that SPBA shows significant trigger
effectiveness and achieves exceptional performance in attack metrics.

</details>


### [177] [MD-ViSCo: A Unified Model for Multi-Directional Vital Sign Waveform Conversion](https://arxiv.org/abs/2506.08357)
*Franck Meyer,Kyunghoon Hur,Edward Choi*

Main category: cs.SD

TL;DR: 提出了一种统一框架MD-ViSCo，能够通过单一模型从任意单一输入波形生成目标波形（如ECG、PPG或ABP），在公开数据集上超越现有基线模型，并满足医学标准。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型大多专为特定的源到目标波形对设计，需要不同的模型架构、优化过程和预处理流程，导致在临床应用中不便。

Method: 提出了Multi-Directional Vital-Sign Converter (MD-ViSCo)，一种统一框架，采用浅层1D U-Net结合Swin Transformer，并利用Adaptive Instance Normalization (AdaIN)捕捉不同波形风格，从而实现从任何单一输入波形生成目标波形的功能。

Result: 在两个公开数据集上的多方向波形生成实验中，MD-ViSCo平均超越了现有基线模型（NabNet & PPG2ABP），MAE降低8.8%，PC提高4.9%；生成的ABP波形满足AAMI标准并达到BHS Grade B，优于所有基线模型。

Conclusion: MD-ViSCo提供了一个统一框架，能够通过单一模型处理各种生命体征波形，在医疗监控领域具有潜在的应用价值。

Abstract: Despite the remarkable progress of deep-learning methods generating a target
vital sign waveform from a source vital sign waveform, most existing models are
designed exclusively for a specific source-to-target pair. This requires
distinct model architectures, optimization procedures, and pre-processing
pipelines, resulting in multiple models that hinder usability in clinical
settings. To address this limitation, we propose the Multi-Directional
Vital-Sign Converter (MD-ViSCo), a unified framework capable of generating any
target waveform such as electrocardiogram (ECG), photoplethysmogram (PPG), or
arterial blood pressure (ABP) from any single input waveform with a single
model. MD-ViSCo employs a shallow 1-Dimensional U-Net integrated with a Swin
Transformer that leverages Adaptive Instance Normalization (AdaIN) to capture
distinct waveform styles. To evaluate the efficacy of MD-ViSCo, we conduct
multi-directional waveform generation on two publicly available datasets. Our
framework surpasses state-of-the-art baselines (NabNet & PPG2ABP) on average
across all waveform types, lowering Mean absolute error (MAE) by 8.8% and
improving Pearson correlation (PC) by 4.9% over two datasets. In addition, the
generated ABP waveforms satisfy the Association for the Advancement of Medical
Instrumentation (AAMI) criterion and achieve Grade B on the British
Hypertension Society (BHS) standard, outperforming all baselines. By
eliminating the need for developing a distinct model for each task, we believe
that this work offers a unified framework that can deal with any kind of vital
sign waveforms with a single model in healthcare monitoring.

</details>


### [178] [Teaching Physical Awareness to LLMs through Sounds](https://arxiv.org/abs/2506.08524)
*Weiguo Wang,Andy Nie,Wenrui Zhou,Yi Kai,Chengchen Hu*

Main category: cs.SD

TL;DR: ACORN is a framework that teaches LLMs physical awareness through sound by introducing a physics-based simulator, building an Audio Question-Answer dataset (AQA-PHY), and proposing an audio encoder. This enables LLMs to perform tasks like line-of-sight detection and Doppler effect estimation.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LLMs lacking physical awareness, especially in understanding real-world physical phenomena such as the Doppler effect and spatial relationships.

Method: Developed ACORN framework which includes a physics-based simulator for generating diverse training data, constructed AQA-PHY dataset, and proposed an audio encoder processing magnitude and phase information. Connected this audio encoder with state-of-the-art LLMs.

Result: Demonstrated reasonable results in simulated and real-world tasks including line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival estimation.

Conclusion: The ACORN framework paves the way for enabling LLMs to understand the physical world through sound.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in text and
multimodal processing, yet they fundamentally lack physical
awareness--understanding of real-world physical phenomena. In this work, we
present ACORN, a framework that teaches LLMs physical awareness through sound,
focusing on fundamental physical phenomena like the Doppler effect, multipath
effect, and spatial relationships. To overcome data scarcity, ACORN introduce a
physics-based simulator combining real-world sound sources with controlled
physical channels to generate diverse training data. Using this simulator, we
build AQA-PHY, a comprehensive Audio Question-Answer dataset, and propose an
audio encoder that processes both magnitude and phase information. By
connecting our audio encoder to state-of-the-art LLMs, we demonstrate
reasonable results in both simulated and real-world tasks, such as
line-of-sight detection, Doppler effect estimation, and Direction-of-Arrival
estimation, paving the way for enabling LLMs to understand physical world.

</details>


### [179] [Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling Paradigms for Text-to-Music Generation](https://arxiv.org/abs/2506.08570)
*Or Tal,Felix Kreuk,Yossi Adi*

Main category: cs.SD

TL;DR: Recent progress in text-to-music generation motivates a systematic empirical analysis focusing on modeling paradigm, specifically comparing Auto-Regressive decoding and Conditional Flow-Matching. This study provides insights into trade-offs and behaviors to guide future systems.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to evaluate the effects of different modeling paradigms in text-to-music generation models, providing insights into which design choices most influence performance.

Method: The method involves conducting a systematic empirical analysis by comparing two common modeling paradigms: Auto-Regressive decoding and Conditional Flow-Matching. The comparison is controlled by training all models from scratch using identical datasets, training configurations, and similar backbone architectures.

Result: The results highlight distinct strengths and limitations of each paradigm across multiple evaluation axes such as generation quality, robustness, scalability, adherence to conditioning, and editing capabilities.

Conclusion: This comparative study offers actionable insights that can inform future architectural and training decisions in text-to-music generation.

Abstract: Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly across many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and pinpoint which design choices most
influence performance. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: Auto-Regressive decoding and Conditional
Flow-Matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [180] [PerfTracker: Online Performance Troubleshooting for Large-scale Model Training in Production](https://arxiv.org/abs/2506.08528)
*Yu Guan,Zhiyu Yin,Haoyu Chen,Sheng Cheng,Chaojie Yang,Tianyin Xu,Yang Zhang,Hanyu Zhao,Yong Li,Dennis Cai,Ennan Zhai*

Main category: cs.DC

TL;DR: PerfTracker is an online troubleshooting system for diagnosing performance issues in large-scale model training on modern GPU clusters.


<details>
  <summary>Details</summary>
Motivation: Troubleshooting performance problems of large model training is immensely challenging due to the unprecedented scales of modern GPU clusters, the complexity of software-hardware interactions, and the data intensity of the training process.

Method: PerfTracker utilizes fine-grained profiling to diagnose performance issues rooted in both hardware (e.g., GPUs and their interconnects) and software (e.g., Python functions and GPU operations). It effectively summarizes runtime behavior patterns of fine-grained LMT functions via online profiling, and leverages differential observability to localize the root cause with minimal production impact.

Result: PerfTracker has been deployed as a production service for large-scale GPU clusters of O(10, 000) GPUs. It has been used to diagnose a variety of difficult performance issues.

Conclusion: PerfTracker is the first online troubleshooting system utilizing fine-grained profiling, to diagnose performance issues of large-scale model training in production.

Abstract: Troubleshooting performance problems of large model training (LMT) is
immensely challenging, due to unprecedented scales of modern GPU clusters, the
complexity of software-hardware interactions, and the data intensity of the
training process. Existing troubleshooting approaches designed for traditional
distributed systems or datacenter networks fall short and can hardly apply to
real-world training systems. In this paper, we present PerfTracker, the first
online troubleshooting system utilizing fine-grained profiling, to diagnose
performance issues of large-scale model training in production. PerfTracker can
diagnose performance issues rooted in both hardware (e.g., GPUs and their
interconnects) and software (e.g., Python functions and GPU operations). It
scales to LMT on modern GPU clusters. PerfTracker effectively summarizes
runtime behavior patterns of fine-grained LMT functions via online profiling,
and leverages differential observability to localize the root cause with
minimal production impact. PerfTracker has been deployed as a production
service for large-scale GPU clusters of O(10, 000) GPUs (product homepage
https://help.aliyun.com/zh/pai/user-guide/perftracker-online-performance-analysis-diagnostic-tool).
It has been used to diagnose a variety of difficult performance issues.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [181] [Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions](https://arxiv.org/abs/2506.08065)
*Ye Zhu,Duo Xu,Zhiwei Deng,Jonathon C. Tan,Olga Russakovsky*

Main category: astro-ph.IM

TL;DR: The paper introduces Astro-DSB, a variant of Diffusion Schrödinger Bridge models designed for astrophysical dynamics. It enhances interpretability, learning efficiency, and prediction performance in star formation tasks within GMCs compared to traditional methods. Additionally, it highlights the advantages of probabilistic generative modeling over discriminative pixel-to-pixel modeling in OOD testing.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability, learning efficiency, and prediction performance in dynamical astrophysical systems, specifically for inverse prediction tasks related to star formation in GMCs.

Method: Astro-DSB model is introduced, which is a variant of DSB with the pairwise domain assumption tailored for astrophysical dynamics. The method investigates learning process and prediction performance using both physically simulated data and real observations (Taurus B213 data).

Result: Astro-DSB improves interpretability, learning efficiency, and prediction performance over conventional astrostatistical and machine learning methods. Probabilistic generative modeling shows improvements over discriminative pixel-to-pixel modeling in OOD testing cases.

Conclusion: The study expands research into diffusion models beyond visual synthesis applications and provides evidence that these models can learn beyond pure data statistics, paving the way for future physics-aware generative models.

Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of
dynamical astrophysical systems, specifically tackling observational inverse
prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We
introduce the Astro-DSB model, a variant of DSB with the pairwise domain
assumption tailored for astrophysical dynamics. By investigating its learning
process and prediction performance in both physically simulated data and in
real observations (the Taurus B213 data), we present two main takeaways. First,
from the astrophysical perspective, our proposed paired DSB method improves
interpretability, learning efficiency, and prediction performance over
conventional astrostatistical and other machine learning methods. Second, from
the generative modeling perspective, probabilistic generative modeling reveals
improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution
(OOD) testing cases of physical simulations with unseen initial conditions and
different dominant physical processes. Our study expands research into
diffusion models beyond the traditional visual synthesis application and
provides evidence of the models' learning abilities beyond pure data
statistics, paving a path for future physics-aware generative models which can
align dynamics between machine learning and real (astro)physical systems.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [182] [KP-PINNs: Kernel Packet Accelerated Physics Informed Neural Networks](https://arxiv.org/abs/2506.08563)
*Siyuan Yang,Cheng Song,Zhilu Lai,Wenjia Wang*

Main category: cs.CE

TL;DR: An improved PINNs framework called KP-PINNs is proposed, which uses RKHS norm for loss function and KP method for acceleration. It shows stability and effectiveness in solving differential equations.


<details>
  <summary>Details</summary>
Motivation: The L2 loss function used in traditional PINNs can lead to incorrect and unstable solutions for complex differential equations.

Method: Propose KP-PINNs that redefine the loss function with RKHS norm and use Kernel Packet method to speed up computation.

Result: Theoretical analysis proves stability across different differential equations, and numerical experiments confirm its efficiency and effectiveness.

Conclusion: KP-PINNs provides a promising way to enhance stability and accuracy of PINNs-based solvers in scientific computing.

Abstract: Differential equations are involved in modeling many engineering problems.
Many efforts have been devoted to solving differential equations. Due to the
flexibility of neural networks, Physics Informed Neural Networks (PINNs) have
recently been proposed to solve complex differential equations and have
demonstrated superior performance in many applications. While the L2 loss
function is usually a default choice in PINNs, it has been shown that the
corresponding numerical solution is incorrect and unstable for some complex
equations. In this work, we propose a new PINNs framework named Kernel Packet
accelerated PINNs (KP-PINNs), which gives a new expression of the loss function
using the reproducing kernel Hilbert space (RKHS) norm and uses the Kernel
Packet (KP) method to accelerate the computation. Theoretical results show that
KP-PINNs can be stable across various differential equations. Numerical
experiments illustrate that KP-PINNs can solve differential equations
effectively and efficiently. This framework provides a promising direction for
improving the stability and accuracy of PINNs-based solvers in scientific
computing.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [183] [Generalizing while preserving monotonicity in comparison-based preference learning models](https://arxiv.org/abs/2506.08616)
*Julien Fageot,Peva Blanchard,Gilles Bareilles,Lê-Nguyên Hoang*

Main category: math.ST

TL;DR: An analysis of preference learning models reveals that many lack the expected monotonicity guarantee. The paper proposes a new class of Linear Generalized Bradley-Terry models with Diffusion Priors, which ensures monotonicity and improves accuracy in limited datasets.


<details>
  <summary>Details</summary>
Motivation: To address the issue where many comparison-based preference learning models, including large language models, do not guarantee monotonicity despite it being an expected property.

Method: Propose a new class of Linear Generalized Bradley-Terry models with Diffusion Priors and identify sufficient conditions on alternatives' embeddings to ensure monotonicity.

Result: Experiments demonstrate that monotonicity is not a common guarantee among models and that the proposed model class enhances accuracy, particularly with limited data.

Conclusion: The advancement in understanding models with generalization ability that are monotone could lead to more reliable preference learning models.

Abstract: If you tell a learning model that you prefer an alternative $a$ over another
alternative $b$, then you probably expect the model to be monotone, that is,
the valuation of $a$ increases, and that of $b$ decreases. Yet, perhaps
surprisingly, many widely deployed comparison-based preference learning models,
including large language models, fail to have this guarantee. Until now, the
only comparison-based preference learning algorithms that were proved to be
monotone are the Generalized Bradley-Terry models. Yet, these models are unable
to generalize to uncompared data. In this paper, we advance the understanding
of the set of models with generalization ability that are monotone. Namely, we
propose a new class of Linear Generalized Bradley-Terry models with Diffusion
Priors, and identify sufficient conditions on alternatives' embeddings that
guarantee monotonicity. Our experiments show that this monotonicity is far from
being a general guarantee, and that our new class of generalizing models
improves accuracy, especially when the dataset is limited.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [184] [MasHost Builds It All: Autonomous Multi-Agent System Directed by Reinforcement Learning](https://arxiv.org/abs/2506.08507)
*Kuo Yang,Xingjie Yang,Linhui Yu,Qing Xu,Yan Fang,Xu Wang,Zhengyang Zhou,Yang Wang*

Main category: cs.MA

TL;DR: Large Language Model (LLM)-driven Multi-agent systems (Mas) are powerful for complex tasks, but existing construction methods have limitations. This work proposes MasHost, a Reinforcement Learning (RL)-based framework for autonomous and query-adaptive Mas design, which introduces component rationality as a new principle.


<details>
  <summary>Details</summary>
Motivation: Existing Multi-agent systems (Mas) construction methods rely on manually crafted interaction mechanisms or heuristic rules, introducing human biases and constraining the autonomous ability. Even with recent advances in adaptive Mas construction, existing systems largely remain within the paradigm of semi-autonomous patterns.

Method: The proposed method, MasHost, formulates Mas construction as a graph search problem and jointly samples agent roles and their interactions through a unified probabilistic sampling mechanism. It uses Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy that collaboratively integrates group-relative advantages and action-wise rewards.

Result: Extensive experiments on six benchmarks demonstrate that MasHost consistently outperforms most competitive baselines, validating its effectiveness, efficiency, and structure rationality.

Conclusion: MasHost is the first RL-driven framework for autonomous Mas graph construction and shows superior performance in various benchmarks.

Abstract: Large Language Model (LLM)-driven Multi-agent systems (Mas) have recently
emerged as a powerful paradigm for tackling complex real-world tasks. However,
existing Mas construction methods typically rely on manually crafted
interaction mechanisms or heuristic rules, introducing human biases and
constraining the autonomous ability. Even with recent advances in adaptive Mas
construction, existing systems largely remain within the paradigm of
semi-autonomous patterns. In this work, we propose MasHost, a Reinforcement
Learning (RL)-based framework for autonomous and query-adaptive Mas design. By
formulating Mas construction as a graph search problem, our proposed MasHost
jointly samples agent roles and their interactions through a unified
probabilistic sampling mechanism. Beyond the accuracy and efficiency objectives
pursued in prior works, we introduce component rationality as an additional and
novel design principle in Mas. To achieve this multi-objective optimization, we
propose Hierarchical Relative Policy Optimization (HRPO), a novel RL strategy
that collaboratively integrates group-relative advantages and action-wise
rewards. To our knowledge, our proposed MasHost is the first RL-driven
framework for autonomous Mas graph construction. Extensive experiments on six
benchmarks demonstrate that MasHost consistently outperforms most competitive
baselines, validating its effectiveness, efficiency, and structure rationality.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [185] [TS-PIELM: Time-Stepping Physics-Informed Extreme Learning Machine Facilitates Soil Consolidation Analyses](https://arxiv.org/abs/2506.08381)
*He Yang,Fei Ren,Hai-Sui Yu,Xueyu Geng,Pei-Zhi Zhuang*

Main category: physics.geo-ph

TL;DR: 提出了一种新的物理信息机器学习方法TS-PIELM，用于土壤固结分析，相比传统的PINN，其计算效率和准确性分别提高了1000倍和100倍。


<details>
  <summary>Details</summary>
Motivation: 传统的物理信息神经网络（PINN）在土壤固结分析中的准确性和效率需要改进。

Method: 通过将固结过程划分为多个时间间隔，并采用单层前馈极限学习机（ELM）近似解，而不是PINN中的全连接神经网络。输入层权重随机生成并固定，输出层权重通过求解线性方程组直接计算，从而显著提高训练效率。

Result: 对于一维情况，与PINN相比，新型TS-PIELM框架的计算效率和准确性分别提高了1000倍和100倍。

Conclusion: TS-PIELM展示了卓越性能，证明物理信息机器学习可以成为计算岩土力学的强大工具。

Abstract: Accuracy and efficiency of the conventional physics-informed neural network
(PINN) need to be improved before it can be a competitive alternative for soil
consolidation analyses. This paper aims to overcome these limitations by
proposing a highly accurate and efficient physics-informed machine learning
(PIML) approach, termed time-stepping physics-informed extreme learning machine
(TS-PIELM). In the TS-PIELM framework the consolidation process is divided into
numerous time intervals, which helps overcome the limitation of PIELM in
solving differential equations with sharp gradients. To accelerate network
training, the solution is approximated by a single-layer feedforward extreme
learning machine (ELM), rather than using a fully connected neural network in
PINN. The input layer weights of the ELM network are generated randomly and
fixed during the training process. Subsequently, the output layer weights are
directly computed by solving a system of linear equations, which significantly
enhances the training efficiency compared to the time-consuming gradient
descent method in PINN. Finally, the superior performance of TS-PIELM is
demonstrated by solving three typical Terzaghi consolidation problems. Compared
to PINN, results show that the computational efficiency and accuracy of the
novel TS-PIELM framework are improved by more than 1000 times and 100 times for
one-dimensional cases, respectively. This paper provides compelling evidence
that PIML can be a powerful tool for computational geotechnics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [186] [UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs](https://arxiv.org/abs/2506.08045)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee*

Main category: cs.RO

TL;DR: Agentic UAVs are advanced autonomous aerial systems that integrate perception, decision-making, memory and collaborative planning. They go beyond traditional UAVs by incorporating AI-driven goal-oriented behavior, contextual reasoning, and interactive autonomy.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the architectural components and enabling technologies that distinguish Agentic UAVs from traditional UAVs, highlighting advancements in autonomy with AI agents, learning, and mission flexibility.

Method: The study explores seven high-impact application domains for Agentic UAVs, identifies key challenges in technical constraints, regulatory limitations, and data-model reliability, and presents emerging solutions across hardware innovation, learning architectures, and human-AI interaction.

Result: Illustrates the broad societal value of agentic aerial intelligence in diverse fields such as precision agriculture, disaster response, environmental monitoring, among others, while addressing current challenges and proposing future pathways.

Conclusion: A foundational framework is established for the future development, deployment, and governance of Agentic UAVs across various societal and industrial domains.

Abstract: Agentic UAVs represent a new frontier in autonomous aerial intelligence,
integrating perception, decision-making, memory, and collaborative planning to
operate adaptively in complex, real-world environments. Driven by recent
advances in Agentic AI, these systems surpass traditional UAVs by exhibiting
goal-driven behavior, contextual reasoning, and interactive autonomy. We
provide a comprehensive foundation for understanding the architectural
components and enabling technologies that distinguish Agentic UAVs from
traditional autonomous UAVs. Furthermore, a detailed comparative analysis
highlights advancements in autonomy with AI agents, learning, and mission
flexibility. This study explores seven high-impact application domains
precision agriculture, construction & mining, disaster response, environmental
monitoring, infrastructure inspection, logistics, security, and wildlife
conservation, illustrating the broad societal value of agentic aerial
intelligence. Furthermore, we identify key challenges in technical constraints,
regulatory limitations, and data-model reliability, and we present emerging
solutions across hardware innovation, learning architectures, and human-AI
interaction. Finally, a future roadmap is proposed, outlining pathways toward
self-evolving aerial ecosystems, system-level collaboration, and sustainable,
equitable deployments. This survey establishes a foundational framework for the
future development, deployment, and governance of agentic aerial systems
(Agentic UAVs) across diverse societal and industrial domains.

</details>


### [187] [Ego-centric Learning of Communicative World Models for Autonomous Driving](https://arxiv.org/abs/2506.08149)
*Hang Wang,Dechen Gao,Junshan Zhang*

Main category: cs.RO

TL;DR: The paper introduces CALL, a new method for multi-agent reinforcement learning (MARL) that uses world models and latent representations to enable efficient information sharing among agents, improving performance in complex tasks like autonomous driving.


<details>
  <summary>Details</summary>
Motivation: MARL faces challenges such as partial observability and non-stationarity in high-dimensional environments. Current information-sharing methods have issues with communication overhead and scalability.

Method: CALL allows each agent to learn a world model that encodes its state and intention into a low-dimensional latent representation. This can be shared with other agents through lightweight communication. Agents perform ego-centric learning while using this shared information to enhance their world models and improve prediction accuracy.

Result: Experiments on the CARLA platform show that CALL improves prediction accuracy and reduces the performance gap in local trajectory planning tasks.

Conclusion: CALL addresses the challenges of MARL by enabling efficient information sharing through world models and latent representations, leading to improved performance in complex tasks.

Abstract: We study multi-agent reinforcement learning (MARL) for tasks in complex
high-dimensional environments, such as autonomous driving. MARL is known to
suffer from the \textit{partial observability} and \textit{non-stationarity}
issues. To tackle these challenges, information sharing is often employed,
which however faces major hurdles in practice, including overwhelming
communication overhead and scalability concerns. By making use of generative AI
embodied in world model together with its latent representation, we develop
{\it CALL}, \underline{C}ommunic\underline{a}tive Wor\underline{l}d
Mode\underline{l}, for MARL, where 1) each agent first learns its world model
that encodes its state and intention into low-dimensional latent representation
with smaller memory footprint, which can be shared with other agents of
interest via lightweight communication; and 2) each agent carries out
ego-centric learning while exploiting lightweight information sharing to enrich
her world model, and then exploits its generalization capacity to improve
prediction for better planning. We characterize the gain on the prediction
accuracy from the information sharing and its impact on performance gap.
Extensive experiments are carried out on the challenging local trajectory
planning tasks in the CARLA platform to demonstrate the performance gains of
using \textit{CALL}.

</details>


### [188] [Re4MPC: Reactive Nonlinear MPC for Multi-model Motion Planning via Deep Reinforcement Learning](https://arxiv.org/abs/2506.08344)
*Neşet Ünver Akmandor,Sarvesh Prajapati,Mark Zolotas,Taşkın Padır*

Main category: cs.RO

TL;DR: The paper presents Re4MPC, a multi-model motion planning pipeline for robots that uses NMPC and DRL to improve computational efficiency and success rates in reaching end-effector goals.


<details>
  <summary>Details</summary>
Motivation: Traditional motion planning methods are computationally prohibitive for real-world settings with many degrees-of-freedom.

Method: Re4MPC generates trajectories using NMPC and reactively selects the model, cost, and constraints based on task complexity and robot state. The reactive decision-making policy is learned via DRL framework.

Result: Re4MPC is more computationally efficient and achieves higher success rates than the NMPC baseline in physics-based simulations with a mobile manipulator.

Conclusion: Re4MPC improves computational efficiency and success rates for motion planning in complex tasks.

Abstract: Traditional motion planning methods for robots with many degrees-of-freedom,
such as mobile manipulators, are often computationally prohibitive for
real-world settings. In this paper, we propose a novel multi-model motion
planning pipeline, termed Re4MPC, which computes trajectories using Nonlinear
Model Predictive Control (NMPC). Re4MPC generates trajectories in a
computationally efficient manner by reactively selecting the model, cost, and
constraints of the NMPC problem depending on the complexity of the task and
robot state. The policy for this reactive decision-making is learned via a Deep
Reinforcement Learning (DRL) framework. We introduce a mathematical formulation
to integrate NMPC into this DRL framework. To validate our methodology and
design choices, we evaluate DRL training and test outcomes in a physics-based
simulation involving a mobile manipulator. Experimental results demonstrate
that Re4MPC is more computationally efficient and achieves higher success rates
in reaching end-effector goals than the NMPC baseline, which computes
whole-body trajectories without our learning mechanism.

</details>


### [189] [Diffusion Models for Safety Validation of Autonomous Driving Systems](https://arxiv.org/abs/2506.08459)
*Juanran Wang,Marc R. Schlichting,Harrison Delecki,Mykel J. Kochenderfer*

Main category: cs.RO

TL;DR: This paper proposes a denoising diffusion model for generating potential failure cases of autonomous vehicles without needing an external training dataset, performing efficiently with modest computing resources and no prior system knowledge.


<details>
  <summary>Details</summary>
Motivation: Safety validation of autonomous driving systems is difficult due to high risks and costs in real-world testing, along with the rarity and diversity of potential failures.

Method: A denoising diffusion model is trained to generate potential failure cases of an autonomous vehicle given any initial traffic state. The model doesn't require any external training dataset or prior knowledge of the system under test.

Result: Experiments on a four-way intersection problem demonstrate that the diffusion model can generate realistic failure samples while capturing a wide variety of potential failures.

Conclusion: The proposed diffusion model is effective for safety validation in traffic intersections as it can perform training and inference with modest computing resources.

Abstract: Safety validation of autonomous driving systems is extremely challenging due
to the high risks and costs of real-world testing as well as the rarity and
diversity of potential failures. To address these challenges, we train a
denoising diffusion model to generate potential failure cases of an autonomous
vehicle given any initial traffic state. Experiments on a four-way intersection
problem show that in a variety of scenarios, the diffusion model can generate
realistic failure samples while capturing a wide variety of potential failures.
Our model does not require any external training dataset, can perform training
and inference with modest computing resources, and does not assume any prior
knowledge of the system under test, with applicability to safety validation for
traffic intersections.

</details>


### [190] [Bayesian Inverse Physics for Neuro-Symbolic Robot Learning](https://arxiv.org/abs/2506.08756)
*Octavio Arriaga,Rebecca Adam,Melvin Laux,Lisa Gutzeit,Marco Ragni,Jan Peters,Frank Kirchner*

Main category: cs.RO

TL;DR: 在实际的机器人应用中，尽管深度学习架构和基础模型推动了多样化机器人应用的显著进步，但它们在未知和动态环境中的高效和可靠操作能力仍然有限。本文提出了一种结合数据驱动学习与结构化推理的概念框架，强调使用可微物理进行世界建模、贝叶斯推断进行不确定性决策以及元学习实现快速适应新任务。这种混合神经-符号架构对于下一代自主系统至关重要，并提供了研究路线图以指导其发展。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习架构和基础模型虽然在许多机器人应用中取得了显著进展，但在面对未知和动态环境时，其数据效率、适应性和可靠性仍显不足。这促使研究者探索一种新的学习范式，能够更好地适应这些挑战性场景。

Method: 提出结合数据驱动学习与结构化推理的框架，具体包括：1) 使用可微物理进行高效的世界建模；2) 利用贝叶斯推断进行不确定性感知的决策；3) 通过元学习实现对新任务的快速适应。此外，还建议将物理符号推理嵌入神经模型中，以提高泛化能力。

Result: 该概念框架有望使机器人超越训练数据进行泛化，在新情况下进行推理，并持续扩展知识，从而实现更高效的适应和更可靠的性能。

Conclusion: 混合神经-符号架构对于开发下一代自主系统至关重要，本文提供了一个研究路线图，以促进此类架构的发展和实际应用。

Abstract: Real-world robotic applications, from autonomous exploration to assistive
technologies, require adaptive, interpretable, and data-efficient learning
paradigms. While deep learning architectures and foundation models have driven
significant advances in diverse robotic applications, they remain limited in
their ability to operate efficiently and reliably in unknown and dynamic
environments. In this position paper, we critically assess these limitations
and introduce a conceptual framework for combining data-driven learning with
deliberate, structured reasoning. Specifically, we propose leveraging
differentiable physics for efficient world modeling, Bayesian inference for
uncertainty-aware decision-making, and meta-learning for rapid adaptation to
new tasks. By embedding physical symbolic reasoning within neural models,
robots could generalize beyond their training data, reason about novel
situations, and continuously expand their knowledge. We argue that such hybrid
neuro-symbolic architectures are essential for the next generation of
autonomous systems, and to this end, we provide a research roadmap to guide and
accelerate their development.

</details>


### [191] [Towards Biosignals-Free Autonomous Prosthetic Hand Control via Imitation Learning](https://arxiv.org/abs/2506.08795)
*Kaijie Shi,Wanglong Lu,Hanli Zhao,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.RO

TL;DR: This paper presents a fully autonomous prosthetic hand control system using a camera, reducing the physical and mental burden on users with limb loss.


<details>
  <summary>Details</summary>
Motivation: To address the challenges faced by individuals with limb loss when using traditional sEMG and semi-autonomous methods that require generating myoelectric signals for each control.

Method: Developed a teleoperation system to collect human demonstration data for training the prosthetic hand control model using imitation learning, which mimics prosthetic hand actions from humans.

Result: The imitation learning algorithm achieved high success rates, generalizing to more individuals and unseen objects with a variation of weights, using data from only a few objects and one participant.

Conclusion: This autonomous system provides an easy-to-use prosthetic control interface, significantly reducing the mental effort required for operation.

Abstract: Limb loss affects millions globally, impairing physical function and reducing
quality of life. Most traditional surface electromyographic (sEMG) and
semi-autonomous methods require users to generate myoelectric signals for each
control, imposing physically and mentally taxing demands. This study aims to
develop a fully autonomous control system that enables a prosthetic hand to
automatically grasp and release objects of various shapes using only a camera
attached to the wrist. By placing the hand near an object, the system will
automatically execute grasping actions with a proper grip force in response to
the hand's movements and the environment. To release the object being grasped,
just naturally place the object close to the table and the system will
automatically open the hand. Such a system would provide individuals with limb
loss with a very easy-to-use prosthetic control interface and greatly reduce
mental effort while using. To achieve this goal, we developed a teleoperation
system to collect human demonstration data for training the prosthetic hand
control model using imitation learning, which mimics the prosthetic hand
actions from human. Through training the model using only a few objects' data
from one single participant, we have shown that the imitation learning
algorithm can achieve high success rates, generalizing to more individuals and
unseen objects with a variation of weights. The demonstrations are available at
\href{https://sites.google.com/view/autonomous-prosthetic-hand}{https://sites.google.com/view/autonomous-prosthetic-hand}

</details>


### [192] [FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency](https://arxiv.org/abs/2506.08822)
*Yifei Su,Ning Liu,Dong Chen,Zhen Zhao,Kun Wu,Meng Li,Zhiyuan Xu,Zhengping Che,Jian Tang*

Main category: cs.RO

TL;DR: The paper introduces FreqPolicy, a new method that improves real-time robotic manipulation by efficiently generating high-quality action sequences with temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Generative modeling-based visuomotor policies are effective for robotic manipulation but suffer from high inference costs due to multi-step sampling. Current acceleration techniques adapted from image generation do not fully address the temporal dependencies required in robotic manipulation.

Method: The authors propose FreqPolicy, which incorporates frequency consistency constraints into flow-based visuomotor policies. This allows for efficient one-step action generation while maintaining temporal coherence. An adaptive consistency loss is also introduced to handle structural temporal variations in robotic tasks.

Result: FreqPolicy outperforms existing one-step action generators on 53 tasks across 3 simulation benchmarks. When integrated into the VLA model, it maintains performance without degradation on 40 Libero tasks and demonstrates efficiency in real-world scenarios with an inference frequency of 93.5Hz.

Conclusion: FreqPolicy successfully exploits temporal information for robotic manipulation, providing both efficiency and effectiveness in generating action trajectories. The approach shows superiority over existing methods and will be made publicly available.

Abstract: Generative modeling-based visuomotor policies have been widely adopted in
robotic manipulation attributed to their ability to model multimodal action
distributions. However, the high inference cost of multi-step sampling limits
their applicability in real-time robotic systems. To address this issue,
existing approaches accelerate the sampling process in generative
modeling-based visuomotor policies by adapting acceleration techniques
originally developed for image generation. Despite this progress, a major
distinction remains: image generation typically involves producing independent
samples without temporal dependencies, whereas robotic manipulation involves
generating time-series action trajectories that require continuity and temporal
coherence. To effectively exploit temporal information in robotic manipulation,
we propose FreqPolicy, a novel approach that first imposes frequency
consistency constraints on flow-based visuomotor policies. Our work enables the
action model to capture temporal structure effectively while supporting
efficient, high-quality one-step action generation. We introduce a frequency
consistency constraint that enforces alignment of frequency-domain action
features across different timesteps along the flow, thereby promoting
convergence of one-step action generation toward the target distribution. In
addition, we design an adaptive consistency loss to capture structural temporal
variations inherent in robotic manipulation tasks. We assess FreqPolicy on 53
tasks across 3 simulation benchmarks, proving its superiority over existing
one-step action generators. We further integrate FreqPolicy into the
vision-language-action (VLA) model and achieve acceleration without performance
degradation on the 40 tasks of Libero. Besides, we show efficiency and
effectiveness in real-world robotic scenarios with an inference frequency
93.5Hz. The code will be publicly available.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [193] [midr: Learning from Black-Box Models by Maximum Interpretation Decomposition](https://arxiv.org/abs/2506.08338)
*Ryoichi Asashiba,Reiji Kozuma,Hirokazu Iwasawa*

Main category: stat.ME

TL;DR: This paper presents the R package midr, which implements Maximum Interpretation Decomposition (MID) as a tool for interpreting black-box models through constructing a global surrogate model.


<details>
  <summary>Details</summary>
Motivation: There is a need for interpretable machine learning methods in fields where model explainability is required. The authors aim to provide a novel tool for interpreting black-box models.

Method: The method introduced is Maximum Interpretation Decomposition (MID), implemented in the R package midr. MID creates a low-order additive representation of a black-box model by minimizing the squared error between the model's prediction function and this representation.

Result: The package midr was created and demonstrated, providing a way to learn from black-box models using a global surrogate model with advanced analytical capabilities.

Conclusion: midr offers a valuable tool for interpreting black-box models via the MID approach, contributing to the field of Interpretable Machine Learning.

Abstract: The use of appropriate methods of Interpretable Machine Learning (IML) and
eXplainable Artificial Intelligence (XAI) is essential for adopting black-box
predictive models in fields where model and prediction explainability is
required. As a novel tool for interpreting black-box models, we introduce the R
package midr, which implements Maximum Interpretation Decomposition (MID). MID
is a functional decomposition approach that derives a low-order additive
representation of a black-box model by minimizing the squared error between the
model's prediction function and this additive representation. midr enables
learning from black-box models by constructing a global surrogate model with
advanced analytical capabilities. After reviewing related work and the
theoretical foundation of MID, we demonstrate the package's usage and discuss
some of its key features.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [194] [Structured Variational $D$-Decomposition for Accurate and Stable Low-Rank Approximation](https://arxiv.org/abs/2506.08535)
*Ronald Katende*

Main category: math.NA

TL;DR: 提出了一种名为D-decomposition的非正交矩阵分解方法，通过最小化正则化的Frobenius损失来进行变分定义，允许对秩、稀疏性和条件进行控制。与代数分解不同，该方法通过交替最小化计算，并在多个数据集上展现出更好的重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有的矩阵分解方法（如LU或SVD）可能无法很好地控制秩、稀疏性和条件性，且计算方式可能不够灵活。因此，需要一种新的矩阵分解方法来克服这些限制，并在实际数据集上表现更好。

Method: 引入了D-decomposition，形式为A ≈ P D Q，其中P、D和Q分别为n×k、k×k和k×n的矩阵。该分解通过最小化正则化的Frobenius损失进行变分定义，使用交替最小化算法进行计算，每次更新的复杂度为O(n^2k)。

Result: 在MovieLens、MNIST、Olivetti Faces和基因表达矩阵等数据集上，与截断SVD、CUR和非负矩阵分解相比，D-decomposition展现出更高的重建精度，特别是在稀疏性和噪声条件下。

Conclusion: D-decomposition提供了一种有效的矩阵分解方法，具有灵活性和稳定性，适用于多种应用场景，尤其在稀疏性和噪声条件下表现出色。

Abstract: We introduce the $D$-decomposition, a non-orthogonal matrix factorization of
the form $A \approx P D Q$, where $P \in \mathbb{R}^{n \times k}$, $D \in
\mathbb{R}^{k \times k}$, and $Q \in \mathbb{R}^{k \times n}$. The
decomposition is defined variationally by minimizing a regularized Frobenius
loss, allowing control over rank, sparsity, and conditioning. Unlike algebraic
factorizations such as LU or SVD, it is computed by alternating minimization.
We establish existence and perturbation stability of the solution and show that
each update has complexity $\mathcal{O}(n^2k)$. Benchmarks against truncated
SVD, CUR, and nonnegative matrix factorization show improved reconstruction
accuracy on MovieLens, MNIST, Olivetti Faces, and gene expression matrices,
particularly under sparsity and noise.

</details>


### [195] [sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation](https://arxiv.org/abs/2506.08670)
*Renjie Xu,Chong Wu,Maolin Che,Zhuoheng Ran,Yimin Wei,Hong Yan*

Main category: math.NA

TL;DR: This paper presents sparseGeoHOPCA, a novel framework for SHOPCA that incorporates a geometric perspective to high-dimensional tensor decomposition. It transforms nonconvex sparse objectives into tractable geometric forms, enhancing computational efficiency and interpretability. Theoretical equivalence and error bounds are established, with experiments showing accurate recovery, preserved classification performance under compression, and high-quality image reconstruction.


<details>
  <summary>Details</summary>
Motivation: To introduce a geometric perspective to high-dimensional tensor decomposition in the context of sparse higher-order principal component analysis (SHOPCA), improving computational efficiency and interpretability, particularly in high-dimensional and unbalanced data scenarios.

Method: The method unfolds the input tensor along each mode, reformulating subproblems as structured binary linear optimization problems. This approach eliminates the need for explicit covariance estimation and iterative deflation, transforming the original nonconvex sparse objective into a tractable geometric form.

Result: sparseGeoHOPCA accurately recovers sparse supports in synthetic settings, preserves classification performance under 10$\times$ compression, and achieves high-quality image reconstruction on ImageNet, demonstrating its robustness and versatility.

Conclusion: sparseGeoHOPCA offers significant gains in computational efficiency and interpretability for high-dimensional tensor decomposition, with theoretical guarantees and strong empirical results.

Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order
principal component analysis (SHOPCA) that introduces a geometric perspective
to high-dimensional tensor decomposition. By unfolding the input tensor along
each mode and reformulating the resulting subproblems as structured binary
linear optimization problems, our method transforms the original nonconvex
sparse objective into a tractable geometric form. This eliminates the need for
explicit covariance estimation and iterative deflation, enabling significant
gains in both computational efficiency and interpretability, particularly in
high-dimensional and unbalanced data scenarios. We theoretically establish the
equivalence between the geometric subproblems and the original SHOPCA
formulation, and derive worst-case approximation error bounds based on
classical PCA residuals, providing data-dependent performance guarantees. The
proposed algorithm achieves a total computational complexity of
$O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with
tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately
recovers sparse supports in synthetic settings, preserves classification
performance under 10$\times$ compression, and achieves high-quality image
reconstruction on ImageNet, highlighting its robustness and versatility.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [196] [A Privacy-Preserving Federated Learning Framework for Generalizable CBCT to Synthetic CT Translation in Head and Neck](https://arxiv.org/abs/2506.08654)
*Ciro Benito Raggio,Paolo Zaffino,Maria Francesca Spadea*

Main category: physics.med-ph

TL;DR: Cone-beam computed tomography (CBCT) has issues with noise, contrast and artifacts that hinder direct dose calculation. Synthetic CT (sCT) generation from CBCT can address these problems but is limited by institutional heterogeneity and data privacy regulations. The authors propose a cross-silo horizontal federated learning approach for CBCT-to-sCT synthesis in the head and neck region, which they call FedSynthCT. This model was trained on data from three European medical centers and showed effective generalization across centers.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing approaches for synthetic CT generation from CBCT which are limited by institutional heterogeneity, scanner-dependent variations, and data privacy regulations that prevent multi-center data sharing.

Method: The authors propose a cross-silo horizontal federated learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region. They extend their FedSynthCT framework using a conditional generative adversarial network collaboratively trained on data from three European medical centers in the public SynthRAD2025 challenge dataset.

Result: The federated model demonstrated effective generalization across centers, with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$ HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$, and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB. On an external validation dataset of 60 patients, comparable performance was achieved without additional training.

Conclusion: The findings demonstrate the technical feasibility of federated learning for CBCT-to-sCT synthesis while preserving data privacy and offer a collaborative solution for developing generalizable models across institutions without centralized data sharing or site-specific fine-tuning.

Abstract: Shortened Abstract
  Cone-beam computed tomography (CBCT) has become a widely adopted modality for
image-guided radiotherapy (IGRT). However, CBCT suffers from increased noise,
limited soft-tissue contrast, and artifacts, resulting in unreliable Hounsfield
unit values and hindering direct dose calculation. Synthetic CT (sCT)
generation from CBCT addresses these issues, especially using deep learning
(DL) methods. Existing approaches are limited by institutional heterogeneity,
scanner-dependent variations, and data privacy regulations that prevent
multi-center data sharing.
  To overcome these challenges, we propose a cross-silo horizontal federated
learning (FL) approach for CBCT-to-sCT synthesis in the head and neck region,
extending our FedSynthCT framework. A conditional generative adversarial
network was collaboratively trained on data from three European medical centers
in the public SynthRAD2025 challenge dataset.
  The federated model demonstrated effective generalization across centers,
with mean absolute error (MAE) ranging from $64.38\pm13.63$ to $85.90\pm7.10$
HU, structural similarity index (SSIM) from $0.882\pm0.022$ to $0.922\pm0.039$,
and peak signal-to-noise ratio (PSNR) from $32.86\pm0.94$ to $34.91\pm1.04$ dB.
Notably, on an external validation dataset of 60 patients, comparable
performance was achieved (MAE: $75.22\pm11.81$ HU, SSIM: $0.904\pm0.034$, PSNR:
$33.52\pm2.06$ dB) without additional training, confirming robust
generalization despite protocol, scanner differences and registration errors.
  These findings demonstrate the technical feasibility of FL for CBCT-to-sCT
synthesis while preserving data privacy and offer a collaborative solution for
developing generalizable models across institutions without centralized data
sharing or site-specific fine-tuning.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [197] [ChemGraph: An Agentic Framework for Computational Chemistry Workflows](https://arxiv.org/abs/2506.06363)
*Thang D. Pham,Aditya Tanikanti,Murat Keçeli*

Main category: physics.chem-ph

TL;DR: ChemGraph is an AI-powered framework that automates computational chemistry workflows using graph neural networks and language models, showing strong performance across 13 benchmark tasks with different model sizes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in running atomistic simulations due to diverse methods, software ecosystems, and the need for expert knowledge, streamlining and automating computational chemistry workflows is necessary.

Method: ChemGraph uses graph neural network-based foundation models for efficient calculations and large language models for natural language understanding, task planning, and scientific reasoning. It supports various simulation methods and can handle tasks like molecular structure generation, energy calculation, geometry optimization, etc.

Result: ChemGraph performs well on 13 benchmark tasks. Smaller LLMs are sufficient for simple workflows, while complex tasks benefit from larger models. Decomposing complex tasks into subtasks via a multi-agent framework allows smaller LLMs to match or exceed GPT-4o's performance in certain scenarios.

Conclusion: ChemGraph successfully automates computational chemistry workflows, demonstrating the effectiveness of combining AI and state-of-the-art simulation tools.

Abstract: Atomistic simulations are essential tools in chemistry and materials science,
accelerating the discovery of novel catalysts, energy storage materials, and
pharmaceuticals. However, running these simulations remains challenging due to
the wide range of computational methods, diverse software ecosystems, and the
need for expert knowledge and manual effort for the setup, execution, and
validation stages. In this work, we present ChemGraph, an agentic framework
powered by artificial intelligence and state-of-the-art simulation tools to
streamline and automate computational chemistry and materials science
workflows. ChemGraph leverages graph neural network-based foundation models for
accurate yet computationally efficient calculations and large language models
(LLMs) for natural language understanding, task planning, and scientific
reasoning to provide an intuitive and interactive interface. Users can perform
tasks such as molecular structure generation, single-point energy, geometry
optimization, vibrational analysis, and thermochemistry calculations with
methods ranging from tight-binding and machine learning interatomic potentials
to density functional theory or wave function theory-based methods. We evaluate
ChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs
(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,
while more complex tasks benefit from using larger models like GPT-4o.
Importantly, we show that decomposing complex tasks into smaller subtasks
through a multi-agent framework enables smaller LLM models to match or exceed
GPT-4o's performance in specific scenarios.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [198] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: This paper presents a novel physics-informed neural simulator for fast and accurate soft tissue deformation simulation, integrating Kelvinlet-based priors into neural simulators for real-time surgical applications.


<details>
  <summary>Details</summary>
Motivation: Fast and accurate simulation of soft tissue deformation is crucial in surgical robotics and medical training, but existing methods lack the balance between speed and accuracy.

Method: The authors introduce a physics-informed neural simulator that incorporates Kelvinlet-based priors into neural networks. This approach uses large-scale Finite Element Method (FEM) simulations to improve predictions across different network architectures, ensuring both accuracy and physical consistency while maintaining low latency.

Result: The simulator successfully performs accurate surgical maneuvers, simulating standard laparoscopic tissue grasping tools with high fidelity. This demonstrates the effectiveness of Kelvinlet-augmented learning in enhancing real-time, physics-aware soft tissue simulation.

Conclusion: Kelvinlet-augmented learning represents a powerful and efficient strategy for real-time, physics-aware soft tissue simulation in surgical applications, improving both accuracy and performance.

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [199] [Do Concept Replacement Techniques Really Erase Unacceptable Concepts?](https://arxiv.org/abs/2506.08991)
*Anudeep Das,Gurjot Singh,Prach Chantasantitam,N. Asokan*

Main category: cs.CV

TL;DR: This paper explores the ineffectiveness of current Concept Replacement Techniques (CRTs) in image-to-image (I2I) models compared to text-to-image (T2I) models when handling unacceptable content. It emphasizes the need for fidelity—preserving other concepts while replacing unwanted ones—and introduces AntiMirror, a targeted image-editing technique designed to achieve both effectiveness and fidelity.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is the challenge faced by generative models, specifically I2I models, in effectively erasing unacceptable content using existing CRTs. While CRTs have shown success in T2I pipelines, they fail to do so in I2I settings, necessitating further investigation into this discrepancy.

Method: The authors first use an I2I model to empirically show that state-of-the-art CRTs do not erase unacceptable concepts. They then introduce the concept of fidelity, which refers to preserving other input-specified concepts while replacing unacceptable ones. Lastly, they propose and demonstrate the viability of AntiMirror, a targeted image-editing technique aimed at achieving both effectiveness and fidelity.

Result: The study reveals that existing CRTs are ineffective in I2I scenarios. The proposed technique, AntiMirror, demonstrates potential in achieving both effectiveness in removing unacceptable content and fidelity in preserving other specified concepts.

Conclusion: Current CRTs are insufficient for I2I models due to their inability to erase unacceptable content while preserving fidelity. AntiMirror shows promise as a solution, combining effectiveness and fidelity in concept replacement.

Abstract: Generative models, particularly diffusion-based text-to-image (T2I) models,
have demonstrated astounding success. However, aligning them to avoid
generating content with unacceptable concepts (e.g., offensive or copyrighted
content, or celebrity likenesses) remains a significant challenge. Concept
replacement techniques (CRTs) aim to address this challenge, often by trying to
"erase" unacceptable concepts from models. Recently, model providers have
started offering image editing services which accept an image and a text prompt
as input, to produce an image altered as specified by the prompt. These are
known as image-to-image (I2I) models. In this paper, we first use an I2I model
to empirically demonstrate that today's state-of-the-art CRTs do not in fact
erase unacceptable concepts. Existing CRTs are thus likely to be ineffective in
emerging I2I scenarios, despite their proven ability to remove unwanted
concepts in T2I pipelines, highlighting the need to understand this discrepancy
between T2I and I2I settings. Next, we argue that a good CRT, while replacing
unacceptable concepts, should preserve other concepts specified in the inputs
to generative models. We call this fidelity. Prior work on CRTs have neglected
fidelity in the case of unacceptable concepts. Finally, we propose the use of
targeted image-editing techniques to achieve both effectiveness and fidelity.
We present such a technique, AntiMirror, and demonstrate its viability.

</details>


### [200] [Towards Reliable AR-Guided Surgical Navigation: Interactive Deformation Modeling with Data-Driven Biomechanics and Prompts](https://arxiv.org/abs/2506.08048)
*Zheng Han,Jun Zhou,Jialun Pei,Jing Qin,Yingfang Fan,Qi Dou*

Main category: cs.CV

TL;DR: In augmented reality (AR)-guided surgical navigation, preoperative organ models are superimposed onto the patient's intraoperative anatomy. Accurate deformation modeling is essential for maintaining alignment between these models and dynamically changing anatomy. The finite element method (FEM) offers physically plausible modeling but is computationally expensive and struggles with large anatomical changes. This paper proposes a data-driven biomechanics algorithm that preserves FEM-level accuracy while improving computational efficiency and introduces a human-in-the-loop mechanism to allow surgeons to interactively correct anatomical misalignments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current deformation modeling techniques in AR-guided surgical navigation, which struggle with large anatomical changes and are computationally expensive.

Method: A data-driven biomechanics algorithm preserving FEM-level accuracy while improving computational efficiency. A human-in-the-loop mechanism enabling surgeons to interactively provide prompts to correct anatomical misalignments.

Result: Experiments on a publicly available dataset show a mean target registration error of 3.42 mm. Incorporating surgeon prompts through the interactive framework reduces the error to 2.78 mm, surpassing state-of-the-art methods in volumetric accuracy.

Conclusion: The proposed framework delivers efficient and accurate deformation modeling, enhances surgeon-algorithm collaboration, and paves the way for safer and more reliable computer-assisted surgeries.

Abstract: In augmented reality (AR)-guided surgical navigation, preoperative organ
models are superimposed onto the patient's intraoperative anatomy to visualize
critical structures such as vessels and tumors. Accurate deformation modeling
is essential to maintain the reliability of AR overlays by ensuring alignment
between preoperative models and the dynamically changing anatomy. Although the
finite element method (FEM) offers physically plausible modeling, its high
computational cost limits intraoperative applicability. Moreover, existing
algorithms often fail to handle large anatomical changes, such as those induced
by pneumoperitoneum or ligament dissection, leading to inaccurate anatomical
correspondences and compromised AR guidance. To address these challenges, we
propose a data-driven biomechanics algorithm that preserves FEM-level accuracy
while improving computational efficiency. In addition, we introduce a novel
human-in-the-loop mechanism into the deformation modeling process. This enables
surgeons to interactively provide prompts to correct anatomical misalignments,
thereby incorporating clinical expertise and allowing the model to adapt
dynamically to complex surgical scenarios. Experiments on a publicly available
dataset demonstrate that our algorithm achieves a mean target registration
error of 3.42 mm. Incorporating surgeon prompts through the interactive
framework further reduces the error to 2.78 mm, surpassing state-of-the-art
methods in volumetric accuracy. These results highlight the ability of our
framework to deliver efficient and accurate deformation modeling while
enhancing surgeon-algorithm collaboration, paving the way for safer and more
reliable computer-assisted surgeries.

</details>


### [201] [IGraSS: Learning to Identify Infrastructure Networks from Satellite Imagery by Iterative Graph-constrained Semantic Segmentation](https://arxiv.org/abs/2506.08137)
*Oishee Bintey Hoque,Abhijin Adiga,Aniruddha Adiga,Siddharth Chaudhary,Madhav V. Marathe,S. S. Ravi,Kirti Rajagopalan,Amanda Wilson,Samarth Swarup*

Main category: cs.CV

TL;DR: 开发了结合语义分割和图优化的迭代框架IGraSS，用于改进基础设施网络（如运河和道路）的地图绘制和地面真值精化。实验表明，该方法显著提高了运河和道路网络的映射准确性。


<details>
  <summary>Details</summary>
Motivation: 准确的运河网络地图对于水资源管理至关重要，但现有基于大规模注释数据集的语义分割模型受限于不完整或不足的地面真值。利用基础设施网络的图级属性（如可达性和连通性）可以改善这一问题。

Method: 提出了一种新型迭代框架IGraSS，将语义分割模块（结合RGB、NDWI和DEM模态）与基于图的地面真值精化模块相结合。语义分割模块处理卫星图像块，而精化模块将基础设施网络视为图进行操作。

Result: 实验表明，IGraSS将不可达运河段的比例从18%降低到3%，并显著提高了运河识别能力。此外，该框架在道路网络中也表现出有效性和泛化能力。

Conclusion: IGraSS是一个强大的框架，可用于精化噪声地面真值并从遥感影像中绘制运河网络，同时具有良好的通用性，适用于其他基础设施网络（如道路）。

Abstract: Accurate canal network mapping is essential for water management, including
irrigation planning and infrastructure maintenance. State-of-the-art semantic
segmentation models for infrastructure mapping, such as roads, rely on large,
well-annotated remote sensing datasets. However, incomplete or inadequate
ground truth can hinder these learning approaches. Many infrastructure networks
have graph-level properties such as reachability to a source (like canals) or
connectivity (roads) that can be leveraged to improve these existing ground
truth. This paper develops a novel iterative framework IGraSS, combining a
semantic segmentation module-incorporating RGB and additional modalities (NDWI,
DEM)-with a graph-based ground-truth refinement module. The segmentation module
processes satellite imagery patches, while the refinement module operates on
the entire data viewing the infrastructure network as a graph. Experiments show
that IGraSS reduces unreachable canal segments from around 18% to 3%, and
training with refined ground truth significantly improves canal identification.
IGraSS serves as a robust framework for both refining noisy ground truth and
mapping canal networks from remote sensing imagery. We also demonstrate the
effectiveness and generalizability of IGraSS using road networks as an example,
applying a different graph-theoretic constraint to complete road networks.

</details>


### [202] [Surgeon Style Fingerprinting and Privacy Risk Quantification via Discrete Diffusion Models in a Vision-Language-Action Framework](https://arxiv.org/abs/2506.08185)
*Huixin Zhan,Jason H. Moore*

Main category: cs.CV

TL;DR: 研究人员提出了一种新的方法，通过离散扩散框架和视觉-语言-动作（VLA）管道来对机器人手术中的外科医生特定指纹进行建模。该方法使用多模态输入预测手势序列，并通过自然语言提示编码个性化外科医生指纹，同时保护隐私。实验表明个性化嵌入可以提高任务性能，但也增加了身份泄露的风险。


<details>
  <summary>Details</summary>
Motivation: 当前的AI系统在处理外科手术数据时通常忽略了外科医生个人风格的信号，这促使研究者探索一种能够捕捉外科医生独特操作风格的新方法。

Method: 该方法将手势预测视为结构化序列去噪任务，基于包括内窥镜视频、手术意图语言和外科医生身份及技能的隐私感知嵌入等多模态输入。使用第三方语言模型通过自然语言提示编码个性化的外科医生指纹，以保留个体行为风格而不暴露明确的身份信息。

Result: 在JIGSAWS数据集上的评估表明，该方法可以准确地重建手势序列并学习到每个外科医生独特的运动指纹。然而，更富有表现力的嵌入虽然提高了任务性能，但同时也增加了身份泄露的敏感性。

Conclusion: 个性化嵌入可以提高任务性能，但也增加了身份泄露的风险，因此在手术建模中需要平衡个性化与隐私风险。

Abstract: Surgeons exhibit distinct operating styles due to differences in training,
experience, and motor behavior - yet current AI systems often ignore this
personalization signal. We propose a novel approach to model fine-grained,
surgeon-specific fingerprinting in robotic surgery using a discrete diffusion
framework integrated with a vision-language-action (VLA) pipeline. Our method
formulates gesture prediction as a structured sequence denoising task,
conditioned on multimodal inputs including endoscopic video, surgical intent
language, and a privacy-aware embedding of surgeon identity and skill.
Personalized surgeon fingerprinting is encoded through natural language prompts
using third-party language models, allowing the model to retain individual
behavioral style without exposing explicit identity. We evaluate our method on
the JIGSAWS dataset and demonstrate that it accurately reconstructs gesture
sequences while learning meaningful motion fingerprints unique to each surgeon.
To quantify the privacy implications of personalization, we perform membership
inference attacks and find that more expressive embeddings improve task
performance but simultaneously increase susceptibility to identity leakage.
These findings demonstrate that while personalized embeddings improve
performance, they also increase vulnerability to identity leakage, revealing
the importance of balancing personalization with privacy risk in surgical
modeling. Code is available at:
https://github.com/huixin-zhan-ai/Surgeon_style_fingerprinting.

</details>


### [203] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: This paper explores using modern decoder-only LLMs as text encoders for text-to-image models, finding that layer-normalized averaging across all layers improves performance and most LLMs outperform the baseline T5 model.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve text-to-image generation by investigating the use of modern decoder-only LLMs as text encoders instead of outdated models like T5 and CLIP.

Method: The method involves building a standardized training and evaluation pipeline to isolate and evaluate different text embeddings. They train 27 text-to-image models with 12 different text encoders, analyzing various aspects such as embedding extraction approaches, LLM variants, and model sizes.

Result: The results show that using last-layer embeddings leads to inferior performance, while layer-normalized averaging across all layers significantly improves alignment with complex prompts. Most LLMs outperform the baseline T5 model in advanced visio-linguistic reasoning skills.

Conclusion: The conclusion is that modern decoder-only LLMs can be effectively used as text encoders for text-to-image models, with specific embedding techniques enhancing performance.

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [204] [Highly Compressed Tokenizer Can Generate Without Training](https://arxiv.org/abs/2506.08257)
*L. Lao Beyer,T. Li,X. Chen,S. Karaman,K. He*

Main category: cs.CV

TL;DR: 本论文提出了一种1D图像标记器，能够通过高度压缩的离散标记序列实现细粒度的图像编辑和生成。该方法无需训练生成模型，利用基于梯度的测试时间优化和即插即用损失函数来完成图像修复和文本引导的图像编辑任务。


<details>
  <summary>Details</summary>
Motivation: 当前常用的图像标记器生成的是二维网格标记，而1D图像标记器可以将图像表示为高度压缩的一维序列，这为通过简单操作（如复制和替换标记）实现复杂的图像编辑提供了可能性。

Method: 作者构建了一个图像生成管道，利用1D标记器的高度表达能力，结合基于梯度的测试时间优化技术和即插即用的损失函数（如重建或CLIP相似性），实现了图像修复和文本引导的图像编辑等功能。

Result: 该方法能够在不训练任何生成模型的情况下生成多样且逼真的样本，并成功应用于图像修复和文本引导的图像编辑等使用场景。

Conclusion: 1D图像标记器的高表达能力和基于梯度的优化技术为图像编辑和生成提供了一种新的有效途径，无需依赖生成模型的训练过程。

Abstract: Commonly used image tokenizers produce a 2D grid of spatially arranged
tokens. In contrast, so-called 1D image tokenizers represent images as highly
compressed one-dimensional sequences of as few as 32 discrete tokens. We find
that the high degree of compression achieved by a 1D tokenizer with vector
quantization enables image editing and generative capabilities through
heuristic manipulation of tokens, demonstrating that even very crude
manipulations -- such as copying and replacing tokens between latent
representations of images -- enable fine-grained image editing by transferring
appearance and semantic attributes. Motivated by the expressivity of the 1D
tokenizer's latent space, we construct an image generation pipeline leveraging
gradient-based test-time optimization of tokens with plug-and-play loss
functions such as reconstruction or CLIP similarity. Our approach is
demonstrated for inpainting and text-guided image editing use cases, and can
generate diverse and realistic samples without requiring training of any
generative model.

</details>


### [205] [Seeing Voices: Generating A-Roll Video from Audio with Mirage](https://arxiv.org/abs/2506.08279)
*Aditi Sundararaman,Amogh Adishesha,Andrew Jaegle,Dan Bigioi,Hyoung-Kyu Song,Jon Kyl,Justin Mao,Kevin Lan,Mojtaba Komeili,ShahRukh Athar,Sheila Babayan,Stanislau Beliasau,William Buchwalter*

Main category: cs.CV

TL;DR: Mirage is an audio-to-video foundation model that generates realistic imagery from audio input, producing high-quality multimodal video when combined with speech synthesis methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current video generation approaches which either ignore sound or focus on restricted application domains, the authors aim to create a model that can generate realistic and expressive video imagery from scratch based on audio input.

Method: The central technical contribution is a unified method for training self-attention-based audio-to-video generation models, either from scratch or using existing weights. This allows Mirage to retain generality while producing high-quality outputs.

Result: Mirage generates compelling multimodal video, especially when integrated with speech synthesis methods. It produces superior subjective quality compared to methods incorporating audio-specific architectures or loss components.

Conclusion: Mirage represents a significant advancement in audio-to-video generation, offering a general approach with high-quality outputs. Readers are encouraged to experience the results firsthand.

Abstract: From professional filmmaking to user-generated content, creators and
consumers have long recognized that the power of video depends on the
harmonious integration of what we hear (the video's audio track) with what we
see (the video's image sequence). Current approaches to video generation either
ignore sound to focus on general-purpose but silent image sequence generation
or address both visual and audio elements but focus on restricted application
domains such as re-dubbing. We introduce Mirage, an audio-to-video foundation
model that excels at generating realistic, expressive output imagery from
scratch given an audio input. When integrated with existing methods for speech
synthesis (text-to-speech, or TTS), Mirage results in compelling multimodal
video. When trained on audio-video footage of people talking (A-roll) and
conditioned on audio containing speech, Mirage generates video of people
delivering a believable interpretation of the performance implicit in input
audio. Our central technical contribution is a unified method for training
self-attention-based audio-to-video generation models, either from scratch or
given existing weights. This methodology allows Mirage to retain generality as
an approach to audio-to-video generation while producing outputs of superior
subjective quality to methods that incorporate audio-specific architectures or
loss components specific to people, speech, or details of how images or audio
are captured. We encourage readers to watch and listen to the results of Mirage
for themselves (see paper and comments for links).

</details>


### [206] [SEMA: a Scalable and Efficient Mamba like Attention via Token Localization and Averaging](https://arxiv.org/abs/2506.08297)
*Nhat Thanh Tran,Fanghui Xue,Shuai Zhang,Jiancheng Lyu,Yunling Zheng,Yingyong Qi,Jack Xin*

Main category: cs.CV

TL;DR: 在图像分类任务中，SEMA（Scalable and Efficient Mamba like Attention）作为一种可扩展且有效的注意力机制，超越了线性注意力和近期的视觉Mamba模型。


<details>
  <summary>Details</summary>
Motivation: 传统的全注意力机制计算复杂度高，而线性注意力机制又无法聚焦，这为计算机视觉任务带来了挑战。受注意力分散特性和Mamba形式注意力发展的启发，研究者设计了一种新的注意力机制SEMA，以避免分散并保持聚焦。

Method: 研究者首先提供了广义注意力的数学定义，并将传统的softmax注意力和线性注意力纳入这一通用框架。接着证明了广义注意力具有分散特性，即随着键数量趋于无穷大，查询会赋予所有键相等的权重。基于此特性，研究者设计了SEMA，利用令牌定位来避免分散并保持聚焦，并通过理论上一致的算术平均来捕捉注意力的全局方面。

Result: 在Imagenet-1k数据集上的分类结果显示，SEMA是一种可扩展且有效的注意力机制替代方案，超越了线性注意力和近期的视觉Mamba模型，尤其是在处理更大规模图像时表现更优，同时模型参数量相似。

Conclusion: SEMA提供了一种新的注意力机制，在解决传统注意力机制问题的同时，提高了模型的可扩展性和有效性，适用于大规模图像分类任务。

Abstract: Attention is the critical component of a transformer. Yet the quadratic
computational complexity of vanilla full attention in the input size and the
inability of its linear attention variant to focus have been challenges for
computer vision tasks. We provide a mathematical definition of generalized
attention and formulate both vanilla softmax attention and linear attention
within the general framework. We prove that generalized attention disperses,
that is, as the number of keys tends to infinity, the query assigns equal
weights to all keys. Motivated by the dispersion property and recent
development of Mamba form of attention, we design Scalable and Efficient Mamba
like Attention (SEMA) which utilizes token localization to avoid dispersion and
maintain focusing, complemented by theoretically consistent arithmetic
averaging to capture global aspect of attention. We support our approach on
Imagenet-1k where classification results show that SEMA is a scalable and
effective alternative beyond linear attention, outperforming recent vision
Mamba models on increasingly larger scales of images at similar model parameter
sizes.

</details>


### [207] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: In this paper, researchers introduce Step AG, a new adaptive guidance strategy for text-to-vision generation diffusion models which restricts classifier-free guidance to the first several denoising steps. This approach generates high-quality images with an average speedup of 20% to 30%, applicable across different settings and models.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation of classifier-free guidance in text-to-vision generation diffusion models, which requires twice as many steps as unconditional generation and thus incurs higher costs. Previous attempts at adaptive guidance lack solid analysis and empirical results, making them inapplicable to general diffusion models.

Method: The authors propose Step AG, a simple and universally applicable adaptive guidance strategy. This method focuses classifier-free guidance on the initial denoising steps, reducing the number of required steps while maintaining image quality and alignment.

Result: Step AG achieves an average speedup of 20% to 30% without compromising image quality or image-text alignment. The improvement remains consistent across various inference steps and models, including video generation models.

Conclusion: Step AG offers a superior alternative to existing methods by providing a cost-effective way to maintain high-quality, well-conditioned image generation in text-to-vision diffusion models.

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [208] [MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding](https://arxiv.org/abs/2506.08512)
*Zhiyi Zhu,Xiaoyu Wu,Zihao Liu,Linlin Yang*

Main category: cs.CV

TL;DR: A new framework MLVTG with MambaAligner and LLMRefiner is proposed for Video Temporal Grounding, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Existing Transformer-based methods for Video Temporal Grounding suffer from redundant attention and suboptimal multi-modal alignment.

Method: MLVTG integrates MambaAligner using Vision Mamba blocks for temporal dependencies and robust video representations, and LLMRefiner leveraging a frozen layer of pre-trained LLM for semantic priors to enhance multi-modal alignment without fine-tuning.

Result: MLVTG achieves state-of-the-art performance on QVHighlights, Charades-STA, and TVSum datasets.

Conclusion: MLVTG significantly outperforms existing baselines in Video Temporal Grounding.

Abstract: Video Temporal Grounding (VTG), which aims to localize video clips
corresponding to natural language queries, is a fundamental yet challenging
task in video understanding. Existing Transformer-based methods often suffer
from redundant attention and suboptimal multi-modal alignment. To address these
limitations, we propose MLVTG, a novel framework that integrates two key
modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba
blocks as a backbone instead of Transformers to model temporal dependencies and
extract robust video representations for multi-modal alignment. LLMRefiner
leverages the specific frozen layer of a pre-trained Large Language Model (LLM)
to implicitly transfer semantic priors, enhancing multi-modal alignment without
fine-tuning. This dual alignment strategy, temporal modeling via structured
state-space dynamics and semantic purification via textual priors, enables more
precise localization. Extensive experiments on QVHighlights, Charades-STA, and
TVSum demonstrate that MLVTG achieves state-of-the-art performance and
significantly outperforms existing baselines.

</details>


### [209] [TrajFlow: Multi-modal Motion Prediction via Flow Matching](https://arxiv.org/abs/2506.08541)
*Qi Yan,Brian Zhang,Yutong Zhang,Daniel Yang,Joshua White,Di Chen,Jiachao Liu,Langechuan Liu,Binnan Zhuang,Shaoshuai Shi,Renjie Liao*

Main category: cs.CV

TL;DR: TrajFlow is a new motion prediction framework for autonomous driving that predicts multiple future trajectories efficiently and accurately.


<details>
  <summary>Details</summary>
Motivation: Efficient and accurate motion prediction is crucial for ensuring safety and informed decision-making in autonomous driving, particularly under dynamic real-world conditions that necessitate multi-modal forecasts.

Method: TrajFlow uses flow matching-based approach to predict multiple plausible future trajectories in a single pass. It also proposes a ranking loss based on the Plackett-Luce distribution for uncertainty estimation and a self-conditioning training technique for improving generalization and accelerating inference.

Result: Extensive experiments on the Waymo Open Motion Dataset show TrajFlow achieves state-of-the-art performance across various key metrics.

Conclusion: TrajFlow addresses scalability and efficiency challenges of existing generative trajectory prediction methods and is effective for safety-critical autonomous driving applications.

Abstract: Efficient and accurate motion prediction is crucial for ensuring safety and
informed decision-making in autonomous driving, particularly under dynamic
real-world conditions that necessitate multi-modal forecasts. We introduce
TrajFlow, a novel flow matching-based motion prediction framework that
addresses the scalability and efficiency challenges of existing generative
trajectory prediction methods. Unlike conventional generative approaches that
employ i.i.d. sampling and require multiple inference passes to capture diverse
outcomes, TrajFlow predicts multiple plausible future trajectories in a single
pass, significantly reducing computational overhead while maintaining coherence
across predictions. Moreover, we propose a ranking loss based on the
Plackett-Luce distribution to improve uncertainty estimation of predicted
trajectories. Additionally, we design a self-conditioning training technique
that reuses the model's own predictions to construct noisy inputs during a
second forward pass, thereby improving generalization and accelerating
inference. Extensive experiments on the large-scale Waymo Open Motion Dataset
(WOMD) demonstrate that TrajFlow achieves state-of-the-art performance across
various key metrics, underscoring its effectiveness for safety-critical
autonomous driving applications. The code and other details are available on
the project website https://traj-flow.github.io/.

</details>


### [210] [Transformers Meet Hyperspectral Imaging: A Comprehensive Study of Models, Challenges and Open Problems](https://arxiv.org/abs/2506.08596)
*Guyang Zhang,Waleed Abdulla*

Main category: cs.CV

TL;DR: Transformers are becoming important for learning long-range dependencies, but their use in hyperspectral imaging (HSI) is still developing. This paper reviews over 300 papers to provide the first comprehensive survey of Transformer-based HSI classification methods, covering every stage from preprocessing to loss design. It addresses challenges such as limited labeled data and computational overhead while suggesting future research directions including public datasets, lightweight models, robustness, and interpretable attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the gap in understanding and utilizing Transformers for hyperspectral imaging (HSI) classification, by providing a comprehensive survey that covers all stages of a typical pipeline and compares design choices with respect to the unique properties of HSI data.

Method: The method involves reviewing more than 300 papers published up to 2025 and categorizing each stage of the Transformer-based HSI classification pipeline, including pre-processing, tokenization, positional encoding, feature extraction, self-attention variants, skip connections, and loss design. The study also contrasts these choices with the spatial-spectral properties of HSI.

Result: The result is a detailed mapping of the field's progress against persistent obstacles like scarce labeled data, high spectral dimensionality, computational costs, and limited model explainability. Additionally, the paper outlines a research agenda to guide future work.

Conclusion: The conclusion emphasizes guiding researchers in selecting, combining, or extending Transformer components that are appropriate for next-generation HSI applications, ensuring they are fit-for-purpose.

Abstract: Transformers have become the architecture of choice for learning long-range
dependencies, yet their adoption in hyperspectral imaging (HSI) is still
emerging. We reviewed more than 300 papers published up to 2025 and present the
first end-to-end survey dedicated to Transformer-based HSI classification. The
study categorizes every stage of a typical pipeline-pre-processing, patch or
pixel tokenization, positional encoding, spatial-spectral feature extraction,
multi-head self-attention variants, skip connections, and loss design-and
contrasts alternative design choices with the unique spatial-spectral
properties of HSI. We map the field's progress against persistent obstacles:
scarce labeled data, extreme spectral dimensionality, computational overhead,
and limited model explainability. Finally, we outline a research agenda
prioritizing valuable public data sets, lightweight on-edge models,
illumination and sensor shifts robustness, and intrinsically interpretable
attention mechanisms. Our goal is to guide researchers in selecting, combining,
or extending Transformer components that are truly fit for purpose for
next-generation HSI applications.

</details>


### [211] [ECMNet:Lightweight Semantic Segmentation with Efficient CNN-Mamba Network](https://arxiv.org/abs/2506.08629)
*Feixiang Du,Shengkun Wu*

Main category: cs.CV

TL;DR: This paper proposes ECMNet, a lightweight Efficient CNN-Mamba Network for semantic segmentation that combines CNN with Mamba in a capsule-based framework to address their complementary weaknesses.


<details>
  <summary>Details</summary>
Motivation: Although CNNs with Transformer models greatly improve performance, the global context modeling remains inadequate. Mamba has shown advantages in modeling long-range dependency, which motivates this work.

Method: The authors propose ECMNet which integrates Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck, Multi-Scale Attention Unit (MSAU) for integrating multi-scale feature aggregation, spatial aggregation and channel aggregation, and Mamba enhanced Feature Fusion Module (FFM) for merging diverse level features.

Result: Extensive experiments on two representative datasets demonstrate that the proposed model excels in accuracy and efficiency balance, achieving 70.6% mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

Conclusion: ECMNet achieves superior performance in semantic segmentation tasks, effectively balancing accuracy and efficiency.

Abstract: In the past decade, Convolutional Neural Networks (CNNs) and Transformers
have achieved wide applicaiton in semantic segmentation tasks. Although CNNs
with Transformer models greatly improve performance, the global context
modeling remains inadequate. Recently, Mamba achieved great potential in vision
tasks, showing its advantages in modeling long-range dependency. In this paper,
we propose a lightweight Efficient CNN-Mamba Network for semantic segmentation,
dubbed as ECMNet. ECMNet combines CNN with Mamba skillfully in a capsule-based
framework to address their complementary weaknesses. Specifically, We design a
Enhanced Dual-Attention Block (EDAB) for lightweight bottleneck. In order to
improve the representations ability of feature, We devise a Multi-Scale
Attention Unit (MSAU) to integrate multi-scale feature aggregation, spatial
aggregation and channel aggregation. Moreover, a Mamba enhanced Feature Fusion
Module (FFM) merges diverse level feature, significantly enhancing segmented
accuracy. Extensive experiments on two representative datasets demonstrate that
the proposed model excels in accuracy and efficiency balance, achieving 70.6%
mIoU on Cityscapes and 73.6% mIoU on CamVid test datasets, with 0.87M
parameters and 8.27G FLOPs on a single RTX 3090 GPU platform.

</details>


### [212] [Geometric deep learning for local growth prediction on abdominal aortic aneurysm surfaces](https://arxiv.org/abs/2506.08729)
*Dieuwertje Alblas,Patryk Rygiel,Julian Suk,Kaj O. Kappe,Marieke Hofman,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: The paper proposes an SE(3)-symmetric transformer model for predicting abdominal aortic aneurysm (AAA) growth using 3D vascular models and multi-physical features. The model is trained on longitudinal CTA scans, achieving a median diameter error of 1.18 mm and high accuracy in identifying patients likely to need repair within two years.


<details>
  <summary>Details</summary>
Motivation: Current clinical guidelines for monitoring AAAs rely solely on maximum diameter thresholds, which do not account for the complex relationship between 3D AAA shape and its growth. This can lead to suboptimal surveillance intervals. Personalized growth predictions could improve patient monitoring strategies.

Method: An SE(3)-symmetric transformer model is used to predict AAA growth directly on the vascular surface enriched with local, multi-physical features. The model preserves anatomical structure and geometric fidelity unlike other parameterized approaches. It is trained on a longitudinal dataset of 113 CTA scans from 24 patients at irregular intervals.

Result: The model achieves a median diameter error of 1.18 mm when predicting AAA growth to the next scan moment. It also accurately identifies patients who will become eligible for elective repair within two years with an accuracy of 0.93. External validation on a different hospital's dataset demonstrates generalization potential.

Conclusion: Local directional AAA growth prediction from the vascular surface is feasible and may contribute to personalized surveillance strategies for AAA patients.

Abstract: Abdominal aortic aneurysms (AAAs) are progressive focal dilatations of the
abdominal aorta. AAAs may rupture, with a survival rate of only 20\%. Current
clinical guidelines recommend elective surgical repair when the maximum AAA
diameter exceeds 55 mm in men or 50 mm in women. Patients that do not meet
these criteria are periodically monitored, with surveillance intervals based on
the maximum AAA diameter. However, this diameter does not take into account the
complex relation between the 3D AAA shape and its growth, making standardized
intervals potentially unfit. Personalized AAA growth predictions could improve
monitoring strategies. We propose to use an SE(3)-symmetric transformer model
to predict AAA growth directly on the vascular model surface enriched with
local, multi-physical features. In contrast to other works which have
parameterized the AAA shape, this representation preserves the vascular
surface's anatomical structure and geometric fidelity. We train our model using
a longitudinal dataset of 113 computed tomography angiography (CTA) scans of 24
AAA patients at irregularly sampled intervals. After training, our model
predicts AAA growth to the next scan moment with a median diameter error of
1.18 mm. We further demonstrate our model's utility to identify whether a
patient will become eligible for elective repair within two years (acc = 0.93).
Finally, we evaluate our model's generalization on an external validation set
consisting of 25 CTAs from 7 AAA patients from a different hospital. Our
results show that local directional AAA growth prediction from the vascular
surface is feasible and may contribute to personalized surveillance strategies.

</details>


### [213] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: The paper presents CulturalFrames, a benchmark for evaluating how well text-to-image models meet cultural expectations across 10 countries and 5 socio-cultural domains. It finds that these models fail to meet both explicit and implicit cultural expectations at high rates, and current evaluation metrics poorly correlate with human judgments.


<details>
  <summary>Details</summary>
Motivation: There is growing concern about the ability of text-to-image (T2I) models to accurately represent diverse cultural contexts as they become more prevalent in visual content generation.

Method: Introduced CulturalFrames, a novel benchmark consisting of 983 prompts, 3637 images generated by 4 state-of-the-art T2I models, and over 10k human annotations. This benchmark evaluates cultural representation in visual generations across 10 countries and 5 socio-cultural domains.

Result: T2I models fail to meet cultural expectations an average of 44% of the time, with explicit expectations missed at a rate of 68% and implicit expectation failures averaging 49%. Existing T2I evaluation metrics poorly correlate with human judgments of cultural alignment.

Conclusion: The findings reveal critical gaps in T2I models and evaluation methodologies, providing directions for developing more culturally informed T2I models.

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [214] [Spatial Transcriptomics Expression Prediction from Histopathology Based on Cross-Modal Mask Reconstruction and Contrastive Learning](https://arxiv.org/abs/2506.08854)
*Junzhuo Liu,Markus Eckstein,Zhixiang Wang,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: An abstract about a study developing a contrastive learning-based deep learning method to predict spatially resolved gene expression from whole-slide images, improving prediction accuracy and exhibiting potential in cancer tissue localization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of obtaining large-scale spatial transcriptomics data due to high cost, and to improve the prediction accuracy of spatially resolved gene expression from whole-slide images.

Method: Develop a contrastive learning-based deep learning method for predicting spatially resolved gene expression from whole-slide images.

Result: Improved Pearson Correlation Coefficient (PCC) in predicting highly expressed genes, highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26% respectively, preserved gene-gene correlations, and applied to datasets with limited samples.

Conclusion: The developed method improves prediction accuracy of spatially resolved gene expression and shows potential in cancer tissue localization based on biomarker expression.

Abstract: Spatial transcriptomics is a technology that captures gene expression levels
at different spatial locations, widely used in tumor microenvironment analysis
and molecular profiling of histopathology, providing valuable insights into
resolving gene expression and clinical diagnosis of cancer. Due to the high
cost of data acquisition, large-scale spatial transcriptomics data remain
challenging to obtain. In this study, we develop a contrastive learning-based
deep learning method to predict spatially resolved gene expression from
whole-slide images. Evaluation across six different disease datasets
demonstrates that, compared to existing studies, our method improves Pearson
Correlation Coefficient (PCC) in the prediction of highly expressed genes,
highly variable genes, and marker genes by 6.27%, 6.11%, and 11.26%
respectively. Further analysis indicates that our method preserves gene-gene
correlations and applies to datasets with limited samples. Additionally, our
method exhibits potential in cancer tissue localization based on biomarker
expression.

</details>


### [215] [Product of Experts for Visual Generation](https://arxiv.org/abs/2506.08894)
*Yunzhi Zhang,Carson Murtuza-Lanier,Zizhang Li,Yilun Du,Jiajun Wu*

Main category: cs.CV

TL;DR: This paper proposes a Product of Experts (PoE) framework that performs inference-time knowledge composition from heterogeneous models using Annealed Importance Sampling (AIS), demonstrating its effectiveness in image and video synthesis tasks.


<details>
  <summary>Details</summary>
Motivation: Modern neural models have rich priors and complementary knowledge over shared data domains, but integrating diverse knowledge from multiple sources remains under-explored.

Method: The proposed method uses a Product of Experts (PoE) framework to perform inference-time knowledge composition from heterogeneous models. This training-free approach samples from the product distribution across experts via Annealed Importance Sampling (AIS).

Result: The framework shows practical benefits in image and video synthesis tasks, yielding better controllability than monolithic methods and providing flexible user interfaces for specifying visual generation goals.

Conclusion: The PoE framework with AIS provides an effective way to integrate diverse knowledge from multiple sources, enhancing controllability and user interaction in visual generation tasks.

Abstract: Modern neural models capture rich priors and have complementary knowledge
over shared data domains, e.g., images and videos. Integrating diverse
knowledge from multiple sources -- including visual generative models, visual
language models, and sources with human-crafted knowledge such as graphics
engines and physics simulators -- remains under-explored. We propose a Product
of Experts (PoE) framework that performs inference-time knowledge composition
from heterogeneous models. This training-free approach samples from the product
distribution across experts via Annealed Importance Sampling (AIS). Our
framework shows practical benefits in image and video synthesis tasks, yielding
better controllability than monolithic methods and additionally providing
flexible user interfaces for specifying visual generation goals.

</details>


### [216] [Inherently Faithful Attention Maps for Vision Transformers](https://arxiv.org/abs/2506.08915)
*Ananthu Aniraj,Cassio F. Dantas,Dino Ienco,Diego Marcos*

Main category: cs.CV

TL;DR: An attention-based method with learned binary masks is introduced to ensure only attended image regions influence prediction, improving robustness against spurious correlations and out-of-distribution backgrounds.


<details>
  <summary>Details</summary>
Motivation: Context can strongly affect object perception, sometimes leading to biased representations, particularly when objects appear in out-of-distribution backgrounds. Many image-level object-centric tasks require identifying relevant regions, often requiring context.

Method: A two-stage framework is proposed: stage 1 processes the full image to discover object parts and identify task-relevant regions, while stage 2 leverages input attention masking to restrict its receptive field to these regions, enabling a focused analysis while filtering out potentially spurious information. Both stages are trained jointly.

Result: Extensive experiments across diverse benchmarks demonstrate that the approach significantly improves robustness against spurious correlations and out-of-distribution backgrounds.

Conclusion: The attention-based method using learned binary attention masks successfully ensures that only attended image regions influence the prediction, enhancing the model's robustness.

Abstract: We introduce an attention-based method that uses learned binary attention
masks to ensure that only attended image regions influence the prediction.
Context can strongly affect object perception, sometimes leading to biased
representations, particularly when objects appear in out-of-distribution
backgrounds. At the same time, many image-level object-centric tasks require
identifying relevant regions, often requiring context. To address this
conundrum, we propose a two-stage framework: stage 1 processes the full image
to discover object parts and identify task-relevant regions, while stage 2
leverages input attention masking to restrict its receptive field to these
regions, enabling a focused analysis while filtering out potentially spurious
information. Both stages are trained jointly, allowing stage 2 to refine stage
1. Extensive experiments across diverse benchmarks demonstrate that our
approach significantly improves robustness against spurious correlations and
out-of-distribution backgrounds.

</details>


### [217] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 通过受蒙特卡洛树搜索（MCTS）启发的算法，研究提出了一种无需额外训练或监督即可从非推理模型中提取隐藏知识并诱导长推理轨迹的方法。该方法在三个基准测试中表现出一致的改进，特别是在MMMU-PRO上总体提高了2%，在文科领域显著提高了9%。


<details>
  <summary>Details</summary>
Motivation: 当前关于视觉-语言模型（VLMs）的研究主要集中在通过蒸馏和强化学习赋予其隐式的长篇链式推理能力。然而，已经训练和部署在互联网上的非推理模型是否可以被利用？是否有办法通过搜索机制来激发这些模型中的隐藏知识，而不需要额外的训练或监督？

Method: 研究者采用了一种受蒙特卡洛树搜索（MCTS）启发的算法，将子问题-子答案对注入到模型的输出流中。通过将推理视为一个搜索过程，其中子问题作为更广泛推理轨迹中的潜在决策点，帮助模型连接碎片化的知识，生成扩展的推理轨迹。

Result: 该方法在三个基准测试中显示出一致的改进。特别地，在MMMU-PRO数据集上，总体性能提高了2%，而在文科领域中，性能提升了9%。

Conclusion: 研究表明，通过使用MCTS启发的算法，可以在不增加额外训练或监督的情况下，有效提升非推理模型的知识连接和长推理能力。这为重新利用现有的非推理模型提供了一条新的途径。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [218] [Segment Concealed Objects with Incomplete Supervision](https://arxiv.org/abs/2506.08955)
*Chunming He,Kai Li,Yachao Zhang,Ziyun Yang,Youwei Pang,Longxiang Tang,Chengyu Fang,Yulun Zhang,Linghe Kong,Xiu Li,Sina Farsiu*

Main category: cs.CV

TL;DR: An unified method SEE is proposed for Incompletely-Supervised Concealed Object Segmentation (ISCOS). It uses a mean-teacher framework with the Segment Anything Model (SAM) to generate pseudo-labels and introduces strategies for pseudo-label handling and a hybrid-granularity feature grouping module. Experiments show state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The task of Incompletely-Supervised Concealed Object Segmentation (ISCOS) remains highly challenging due to limited supervision from incompletely annotated training data and the difficulty of distinguishing concealed objects from the background because of intrinsic similarities in concealed scenarios.

Method: A unified mean-teacher framework named SEE is proposed which leverages SAM to generate pseudo-labels using coarse masks produced by the teacher model as prompts. Strategies are introduced for pseudo-label generation, storage, and supervision to ensure robust network training. Additionally, a hybrid-granularity feature grouping module is designed to promote segmentation coherence by clustering similar features.

Result: The experimental results demonstrate that the proposed method achieves state-of-the-art performance across multiple ISCOS tasks.

Conclusion: SEE can serve as a plug-and-play solution enhancing the performance of existing models.

Abstract: Incompletely-Supervised Concealed Object Segmentation (ISCOS) involves
segmenting objects that seamlessly blend into their surrounding environments,
utilizing incompletely annotated data, such as weak and semi-annotations, for
model training. This task remains highly challenging due to (1) the limited
supervision provided by the incompletely annotated training data, and (2) the
difficulty of distinguishing concealed objects from the background, which
arises from the intrinsic similarities in concealed scenarios. In this paper,
we introduce the first unified method for ISCOS to address these challenges. To
tackle the issue of incomplete supervision, we propose a unified mean-teacher
framework, SEE, that leverages the vision foundation model, ``\emph{Segment
Anything Model (SAM)}'', to generate pseudo-labels using coarse masks produced
by the teacher model as prompts. To mitigate the effect of low-quality
segmentation masks, we introduce a series of strategies for pseudo-label
generation, storage, and supervision. These strategies aim to produce
informative pseudo-labels, store the best pseudo-labels generated, and select
the most reliable components to guide the student model, thereby ensuring
robust network training. Additionally, to tackle the issue of intrinsic
similarity, we design a hybrid-granularity feature grouping module that groups
features at different granularities and aggregates these results. By clustering
similar features, this module promotes segmentation coherence, facilitating
more complete segmentation for both single-object and multiple-object images.
We validate the effectiveness of our approach across multiple ISCOS tasks, and
experimental results demonstrate that our method achieves state-of-the-art
performance. Furthermore, SEE can serve as a plug-and-play solution, enhancing
the performance of existing models.

</details>


### [219] [Diversity-Guided MLP Reduction for Efficient Large Vision Transformers](https://arxiv.org/abs/2506.08591)
*Chengchao Shen,Hourun Zhu,Gongfan Fang,Jianxin Wang,Xinchao Wang*

Main category: cs.CV

TL;DR: Transformer模型尽管性能随规模增加而提升，但参数量过大导致计算和内存成本高昂。本文提出了一种Diversity-Guided MLP Reduction (DGMR)方法，通过Gram-Schmidt权重剪枝策略减少大型视觉Transformer中的MLP模块参数，同时保留权重多样性以在蒸馏过程中更好地恢复性能。该方法在几乎不损失性能的情况下显著减少了参数和FLOPs。


<details>
  <summary>Details</summary>
Motivation: Transformer模型的性能随着模型容量的增加而提高，但是大规模模型参数带来了难以承受的计算和内存成本。分析发现MLP模块占据了模型参数的主要部分，因此需要一种方法来压缩这些模型，同时尽量减少性能下降。

Method: 提出DGMR方法，使用Gram-Schmidt权重剪枝策略消除MLP隐藏层中的冗余神经元，同时保持权重多样性以便在蒸馏过程中更好地恢复性能。与从头开始训练的模型相比，该方法只需要LAION-2B数据集0.06%的数据（无标签）即可恢复原始性能。

Result: 实验结果表明，DGMR方法可以在几乎不失性能的情况下实现超过57.0%的参数和FLOPs减少。对于EVA-CLIP-E（4.4B），实现了71.5%的参数和FLOPs减少且没有性能下降。

Conclusion: DGMR方法能够显著减少大型视觉Transformer的参数和计算量，同时保持模型性能，为高效部署大型模型提供了可能。

Abstract: Transformer models achieve excellent scaling property, where the performance
is improved with the increment of model capacity. However, large-scale model
parameters lead to an unaffordable cost of computing and memory. We analyze
popular transformer architectures and find that multilayer perceptron (MLP)
modules take up the majority of model parameters. To this end, we focus on the
recoverability of the compressed models and propose a Diversity-Guided MLP
Reduction (DGMR) method to significantly reduce the parameters of large vision
transformers with only negligible performance degradation. Specifically, we
conduct a Gram-Schmidt weight pruning strategy to eliminate redundant neurons
of MLP hidden layer, while preserving weight diversity for better performance
recover during distillation. Compared to the model trained from scratch, our
pruned model only requires 0.06\% data of LAION-2B (for the training of large
vision transformers) without labels (ImageNet-1K) to recover the original
performance. Experimental results on several state-of-the-art large vision
transformers demonstrate that our method achieves a more than 57.0\% parameter
and FLOPs reduction in a near lossless manner. Notably, for EVA-CLIP-E (4.4B),
our method accomplishes a 71.5\% parameter and FLOPs reduction without
performance degradation. The source code and trained weights are available at
https://github.com/visresearch/DGMR.

</details>


### [220] [Efficient Medical Vision-Language Alignment Through Adapting Masked Vision Models](https://arxiv.org/abs/2506.08990)
*Chenyu Lian,Hong-Yu Zhou,Dongyun Liang,Jing Qin,Liansheng Wang*

Main category: cs.CV

TL;DR: The paper proposes ALTA, an efficient medical vision-language alignment method that adapts pretrained vision models for superior performance in vision-language matching tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional cross-modal contrastive learning methods have suboptimal visual representation capabilities while multimodal masked modeling methods struggle with direct cross-modal matching despite excelling in visual representation.

Method: ALTA utilizes only about 8% of the trainable parameters and less than 1/5 of the computational consumption required for masked record modeling. It adapts the pretrained vision model from masked record modeling and integrates temporal-multiview radiograph inputs to enhance information consistency between radiographs and their descriptions.

Result: ALTA outperforms the best-performing counterpart by over 4% absolute points in text-to-image accuracy and approximately 6% absolute points in image-to-text retrieval accuracy.

Conclusion: ALTA achieves superior performance in vision-language matching tasks and promotes better vision and language understanding.

Abstract: Medical vision-language alignment through cross-modal contrastive learning
shows promising performance in image-text matching tasks, such as retrieval and
zero-shot classification. However, conventional cross-modal contrastive
learning (CLIP-based) methods suffer from suboptimal visual representation
capabilities, which also limits their effectiveness in vision-language
alignment. In contrast, although the models pretrained via multimodal masked
modeling struggle with direct cross-modal matching, they excel in visual
representation. To address this contradiction, we propose ALTA (ALign Through
Adapting), an efficient medical vision-language alignment method that utilizes
only about 8% of the trainable parameters and less than 1/5 of the
computational consumption required for masked record modeling. ALTA achieves
superior performance in vision-language matching tasks like retrieval and
zero-shot classification by adapting the pretrained vision model from masked
record modeling. Additionally, we integrate temporal-multiview radiograph
inputs to enhance the information consistency between radiographs and their
corresponding descriptions in reports, further improving the vision-language
alignment. Experimental evaluations show that ALTA outperforms the
best-performing counterpart by over 4% absolute points in text-to-image
accuracy and approximately 6% absolute points in image-to-text retrieval
accuracy. The adaptation of vision-language models during efficient alignment
also promotes better vision and language understanding. Code is publicly
available at https://github.com/DopamineLcy/ALTA.

</details>


### [221] [Diffuse and Disperse: Image Generation with Representation Regularization](https://arxiv.org/abs/2506.09027)
*Runqian Wang,Kaiming He*

Main category: cs.CV

TL;DR: 研究人员提出了一种名为Dispersive Loss的新型损失函数，用于改进扩散生成模型。该方法通过鼓励隐藏空间中的表示分散来实现正则化效果，无需正样本对或干扰回归采样过程。与REPA方法相比，Dispersive Loss无需预训练、额外参数或外部数据。在ImageNet数据集上的实验表明，该方法在多个模型上均优于强大基线模型。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型在过去十年中独立于表征学习发展，通常依赖回归目标且缺乏显式正则化。为了改善这一情况并弥合生成建模和表征学习之间的差距，需要一种新的正则化方法。

Method: 提出了一种名为Dispersive Loss的损失函数，它鼓励内部表示在隐藏空间中分散。这种方法类似于对比自监督学习，但不需要正样本对，因此不会干扰回归采样过程。此外，Dispersive Loss是自包含的，不需要预训练、额外参数或外部数据。

Result: 在ImageNet数据集上的实验结果表明，使用Dispersive Loss可以一致地提高多个模型的性能，优于广泛使用的强大基线模型。

Conclusion: Dispersive Loss是一种简单有效的插件式正则化方法，可以显著提升扩散生成模型的性能，并有望弥合生成建模和表征学习之间的差距。

Abstract: The development of diffusion-based generative models over the past decade has
largely proceeded independently of progress in representation learning. These
diffusion models typically rely on regression-based objectives and generally
lack explicit regularization. In this work, we propose \textit{Dispersive
Loss}, a simple plug-and-play regularizer that effectively improves
diffusion-based generative models. Our loss function encourages internal
representations to disperse in the hidden space, analogous to contrastive
self-supervised learning, with the key distinction that it requires no positive
sample pairs and therefore does not interfere with the sampling process used
for regression. Compared to the recent method of representation alignment
(REPA), our approach is self-contained and minimalist, requiring no
pre-training, no additional parameters, and no external data. We evaluate
Dispersive Loss on the ImageNet dataset across a range of models and report
consistent improvements over widely used and strong baselines. We hope our work
will help bridge the gap between generative modeling and representation
learning.

</details>


### [222] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: The paper introduces Autoregressive Semantic Visual Reconstruction (ASVR) to address limitations in large vision-language models by enabling joint learning of visual and textual modalities, leading to significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current large vision-language models primarily focus on text-based autoregressive supervision, potentially overlooking fine-grained visual details. This leads to challenges such as underutilization of images without captions, omission of critical visual details in captions, and difficulty conveying vision-centric content through text.

Method: The authors propose ASVR, which involves autoregressively reconstructing the semantic representation of images within a unified framework. They demonstrate that this approach is more effective than raw visual appearance reconstruction, leading to consistent improvements in multimodal understanding.

Result: ASVR results in stable and consistent improvements across various multimodal benchmarks. Specifically, it improves LLaVA-1.5 by 5% in average scores across 14 benchmarks, showcasing its effectiveness across different data scales and LLM backbones.

Conclusion: ASVR successfully enhances multimodal understanding by effectively integrating visual and textual modalities through autoregressive semantic reconstruction, offering a promising direction for future advancements in LVLMs.

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


### [223] [Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models](https://arxiv.org/abs/2506.08780)
*Isaac Corley,Lakshay Sharma,Ruth Crasto*

Main category: cs.CV

TL;DR: The paper presents Landsat-Bench, a set of three benchmarks based on Landsat imagery, and shows that SSL4EO-L pretrained Geospatial Foundation Models (GFMs) offer better representations for downstream tasks compared to ImageNet.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of benchmarks for Landsat data which limits progress towards developing Landsat-based Geospatial Foundation Models (GFM).

Method: The method involves introducing Landsat-Bench, consisting of three benchmarks adapted from existing remote sensing datasets (EuroSAT-L, BigEarthNet-L, and LC100-L), and establishing baseline and standardized evaluation methods across common architectures and Landsat foundation models pretrained on the SSL4EO-L dataset.

Result: SSL4EO-L pretrained GFMs extract better representations for downstream tasks than ImageNet, with performance gains of +4% Overall Accuracy (OA) and +5.1% mean Average Precision (mAP) on EuroSAT-L and BigEarthNet-L.

Conclusion: Landsat-Bench provides valuable benchmarks for Landsat imagery and demonstrates the effectiveness of SSL4EO-L pretrained GFMs for downstream tasks.

Abstract: The Landsat program offers over 50 years of globally consistent Earth
imagery. However, the lack of benchmarks for this data constrains progress
towards Landsat-based Geospatial Foundation Models (GFM). In this paper, we
introduce Landsat-Bench, a suite of three benchmarks with Landsat imagery that
adapt from existing remote sensing datasets -- EuroSAT-L, BigEarthNet-L, and
LC100-L. We establish baseline and standardized evaluation methods across both
common architectures and Landsat foundation models pretrained on the SSL4EO-L
dataset. Notably, we provide evidence that SSL4EO-L pretrained GFMs extract
better representations for downstream tasks in comparison to ImageNet,
including performance gains of +4% OA and +5.1% mAP on EuroSAT-L and
BigEarthNet-L.

</details>


### [224] [StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams](https://arxiv.org/abs/2506.08862)
*Zike Wu,Qi Yan,Xuanyu Yi,Lele Wang,Renjie Liao*

Main category: cs.CV

TL;DR: StreamSplat is a novel framework that transforms uncalibrated video streams into 3D Gaussian Splatting representations in real time, excelling in reconstruction quality and dynamic scene modeling.


<details>
  <summary>Details</summary>
Motivation: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams is essential for many applications, yet existing methods face challenges in processing inputs efficiently, modeling scene dynamics accurately, and maintaining long-term stability.

Method: The paper introduces StreamSplat, a fully feed-forward framework that converts uncalibrated video streams into dynamic 3D Gaussian Splatting representations online. It includes a probabilistic sampling mechanism in the static encoder for position prediction and a bidirectional deformation field in the dynamic decoder for robust dynamic modeling.

Result: Extensive experiments show that StreamSplat surpasses previous methods in reconstruction quality and dynamic scene modeling, while uniquely supporting online reconstruction of arbitrarily long video streams.

Conclusion: StreamSplat addresses key challenges in real-time 3D scene reconstruction from uncalibrated video streams, offering superior performance and unique capabilities for handling long video sequences.

Abstract: Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams
is crucial for numerous real-world applications. However, existing methods
struggle to jointly address three key challenges: 1) processing uncalibrated
inputs in real time, 2) accurately modeling dynamic scene evolution, and 3)
maintaining long-term stability and computational efficiency. To this end, we
introduce StreamSplat, the first fully feed-forward framework that transforms
uncalibrated video streams of arbitrary length into dynamic 3D Gaussian
Splatting (3DGS) representations in an online manner, capable of recovering
scene dynamics from temporally local observations. We propose two key technical
innovations: a probabilistic sampling mechanism in the static encoder for 3DGS
position prediction, and a bidirectional deformation field in the dynamic
decoder that enables robust and efficient dynamic modeling. Extensive
experiments on static and dynamic benchmarks demonstrate that StreamSplat
consistently outperforms prior works in both reconstruction quality and dynamic
scene modeling, while uniquely supporting online reconstruction of arbitrarily
long video streams. Code and models are available at
https://github.com/nickwzk/StreamSplat.

</details>


### [225] [Data Augmentation For Small Object using Fast AutoAugment](https://arxiv.org/abs/2506.08956)
*DaeEun Yoon,Semin Kim,SangWook Yoo,Jongha Lee*

Main category: cs.CV

TL;DR: 为了提升小目标检测性能，本文提出了一种基于Fast AutoAugment的最优数据增强方法，该方法可以快速找到克服小目标检测退化的最优增强策略，在DOTA数据集上实现了20%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来目标检测性能取得了巨大进展，但小目标的检测性能远不如大目标，因此提升小目标检测性能是一个具有挑战性且重要的问题。

Method: 提出了一种使用Fast AutoAugment的最优数据增强方法，通过该方法可以快速找到能够克服小目标检测退化的最优增强策略。

Result: 在DOTA数据集上实现了20%的性能提升。

Conclusion: 所提出的方法有效地提高了小目标检测的性能。

Abstract: In recent years, there has been tremendous progress in object detection
performance. However, despite these advances, the detection performance for
small objects is significantly inferior to that of large objects. Detecting
small objects is one of the most challenging and important problems in computer
vision. To improve the detection performance for small objects, we propose an
optimal data augmentation method using Fast AutoAugment. Through our proposed
method, we can quickly find optimal augmentation policies that can overcome
degradation when detecting small objects, and we achieve a 20% performance
improvement on the DOTA dataset.

</details>


### [226] [DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging](https://arxiv.org/abs/2506.09024)
*Felix Wagner,Pramit Saha,Harry Anthony,J. Alison Noble,Konstantinos Kamnitsas*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Decentralized Isolation Networks（DIsoN）的新型框架，用于解决机器学习模型在部署后无法有效检测出分布外数据的问题。该方法通过比较测试数据和训练数据，量化将目标测试样本与训练数据分离的难度，并且无需共享原始数据，仅需交换模型参数即可实现跨节点的数据比较。进一步地，研究还提出了基于类别条件的扩展方法，提高了检测性能。实验表明，DIsoN在四个医学影像数据集上的表现优于现有方法，同时确保了数据隐私。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的OOD检测方法要么在部署后丢弃训练数据，要么假设测试数据和训练数据集中存储在一起，这在实际应用中难以满足。特别是在医疗影像领域，由于数据规模庞大以及隐私和专有权的限制，通常无法随模型一起传输训练数据。因此，需要一种能够在不共享数据的情况下进行有效OOD检测的方法。

Method: 研究者首先引入了Isolation Network框架，通过解决二分类任务来量化将目标测试样本从训练数据中分离的难度。接着，他们提出了Decentralized Isolation Networks（DIsoN），允许在无法共享数据的情况下，通过交换模型参数在远程计算节点之间比较训练和测试数据。此外，研究还扩展了DIsoN，加入了类别条件，使目标样本仅与其预测类别的训练数据进行比较。

Result: DIsoN在四个医学影像数据集（皮肤科、胸部X光、乳腺超声、组织病理学）上的12个OOD检测任务中表现出色，相较于现有方法具有明显优势，同时尊重了数据隐私。

Conclusion: DIsoN提供了一种新的去中心化OOD检测框架，为机器学习开发者提供了一种新服务：在不共享数据的前提下，利用训练数据进行安全的OOD检测服务。

Abstract: Safe deployment of machine learning (ML) models in safety-critical domains
such as medical imaging requires detecting inputs with characteristics not seen
during training, known as out-of-distribution (OOD) detection, to prevent
unreliable predictions. Effective OOD detection after deployment could benefit
from access to the training data, enabling direct comparison between test
samples and the training data distribution to identify differences.
State-of-the-art OOD detection methods, however, either discard training data
after deployment or assume that test samples and training data are centrally
stored together, an assumption that rarely holds in real-world settings. This
is because shipping training data with the deployed model is usually impossible
due to the size of training databases, as well as proprietary or privacy
constraints. We introduce the Isolation Network, an OOD detection framework
that quantifies the difficulty of separating a target test sample from the
training data by solving a binary classification task. We then propose
Decentralized Isolation Networks (DIsoN), which enables the comparison of
training and test data when data-sharing is impossible, by exchanging only
model parameters between the remote computational nodes of training and
deployment. We further extend DIsoN with class-conditioning, comparing a target
sample solely with training data of its predicted class. We evaluate DIsoN on
four medical imaging datasets (dermatology, chest X-ray, breast ultrasound,
histopathology) across 12 OOD detection tasks. DIsoN performs favorably against
existing methods while respecting data-privacy. This decentralized OOD
detection framework opens the way for a new type of service that ML developers
could provide along with their models: providing remote, secure utilization of
their training data for OOD detection services. Code will be available upon
acceptance at: *****

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [227] [ABC-FHE : A Resource-Efficient Accelerator Enabling Bootstrappable Parameters for Client-Side Fully Homomorphic Encryption](https://arxiv.org/abs/2506.08461)
*Sungwoong Yune,Hyojeong Lee,Adiwena Putra,Hyunjun Cho,Cuong Duong Manh,Jaeho Jeon,Joo-Young Kim*

Main category: cs.AR

TL;DR: To overcome the computational challenges of Fully Homomorphic Encryption (FHE) on client-side, this paper proposes ABC-FHE, an advanced FHE accelerator with a streaming architecture and key innovations such as reconfigurable Fourier engine, on-chip PRNG, and optimized task scheduling. It achieves significant performance improvements over CPUs and existing accelerators.


<details>
  <summary>Details</summary>
Motivation: The demand for privacy-preserving computation is growing, but the adoption of Fully Homomorphic Encryption (FHE) is hindered by substantial computational overhead, especially on the client side under bootstrappable parameter configurations.

Method: ABC-FHE employs a streaming architecture to maximize performance density, minimize area usage, and reduce off-chip memory access. Innovations include a reconfigurable Fourier engine that can switch between NTT and FFT modes, an on-chip pseudo-random number generator, and a unified on-the-fly twiddle factor generator. Optimized task scheduling enhances CKKS client-side processing.

Result: ABC-FHE occupies a die area of 28.638 mm² and consumes 5.654 W in 28 nm technology. It achieves a 1112x speed-up in encoding and encryption execution time compared to a CPU, and 214x over the state-of-the-art client-side accelerator. For decoding and decryption, it achieves a 963x speed-up over the CPU and 82x over the state-of-the-art accelerator.

Conclusion: ABC-FHE is an efficient FHE accelerator designed for client-side operations, providing significant improvements in performance, power efficiency, and area usage compared to existing solutions.

Abstract: As the demand for privacy-preserving computation continues to grow, fully
homomorphic encryption (FHE)-which enables continuous computation on encrypted
data-has become a critical solution. However, its adoption is hindered by
significant computational overhead, requiring 10000-fold more computation
compared to plaintext processing. Recent advancements in FHE accelerators have
successfully improved server-side performance, but client-side computations
remain a bottleneck, particularly under bootstrappable parameter
configurations, which involve combinations of encoding, encrypt, decoding, and
decrypt for large-sized parameters. To address this challenge, we propose
ABC-FHE, an area- and power-efficient FHE accelerator that supports
bootstrappable parameters on the client side. ABC-FHE employs a streaming
architecture to maximize performance density, minimize area usage, and reduce
off-chip memory access. Key innovations include a reconfigurable Fourier engine
capable of switching between NTT and FFT modes. Additionally, an on-chip
pseudo-random number generator and a unified on-the-fly twiddle factor
generator significantly reduce memory demands, while optimized task scheduling
enhances the CKKS client-side processing, achieving reduced latency. Overall,
ABC-FHE occupies a die area of 28.638 mm2 and consumes 5.654 W of power in 28
nm technology. It delivers significant performance improvements, achieving a
1112x speed-up in encoding and encryption execution time compared to a CPU, and
214x over the state-of-the-art client-side accelerator. For decoding and
decryption, it achieves a 963x speed-up over the CPU and 82x over the
state-of-the-art accelerator.

</details>


### [228] [POLARON: Precision-aware On-device Learning and Adaptive Runtime-cONfigurable AI acceleration](https://arxiv.org/abs/2506.08785)
*Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: PARV-CE is a multi-precision MAC engine that efficiently handles various precision formats, optimizes performance and energy usage with layer adaptive precision strategy, shows significant improvements over current designs, and supports both training and inference for multiple AI models at the edge.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop flexible hardware capable of supporting diverse precision formats in increasingly complex AI models, especially for energy-constrained edge platforms.

Method: The method involves creating PARV-CE, a SIMD-enabled, multi-precision MAC engine that uses a unified data-path for different bit fixed-point, floating point, and posit formats. It incorporates a layer adaptive precision strategy and integrates quantization-aware execution with a reconfigurable SIMD pipeline.

Result: The results show up to 2x improvement in PDP and 3x reduction in resource usage compared to state-of-the-art designs while retaining accuracy within 1.8% FP32 baseline.

Conclusion: PARV-CE incorporated POLARON is established as a scalable and energy-efficient solution for precision-adaptive AI acceleration at the edge.

Abstract: The increasing complexity of AI models requires flexible hardware capable of
supporting diverse precision formats, particularly for energy-constrained edge
platforms. This work presents PARV-CE, a SIMD-enabled, multi-precision MAC
engine that performs efficient multiply-accumulate operations using a unified
data-path for 4/8/16-bit fixed-point, floating point, and posit formats. The
architecture incorporates a layer adaptive precision strategy to align
computational accuracy with workload sensitivity, optimizing both performance
and energy usage. PARV-CE integrates quantization-aware execution with a
reconfigurable SIMD pipeline, enabling high-throughput processing with minimal
overhead through hardware-software co-design. The results demonstrate up to 2x
improvement in PDP and 3x reduction in resource usage compared to SoTA designs,
while retaining accuracy within 1.8% FP32 baseline. The architecture supports
both on-device training and inference across a range of workloads, including
DNNs, RNNs, RL, and Transformer models. The empirical analysis establish PARVCE
incorporated POLARON as a scalable and energy-efficient solution for
precision-adaptive AI acceleration at edge.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [229] [QUITE: A Query Rewrite System Beyond Rules with LLM Agents](https://arxiv.org/abs/2506.07675)
*Yuyang Song,Hanxu Yan,Jiale Lao,Yibo Wang,Yufei Li,Yuanchun Zhou,Jianguo Wang,Mingjie Tang*

Main category: cs.DB

TL;DR: QUITE is a system leveraging LLMs to rewrite SQL queries into semantically equivalent forms with better performance, overcoming the limitations of rule-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing query rewrite methods relying on predefined rules are limited in discovering new rules, generalizing to new patterns, and expressing certain techniques. Human experts perform better but lack scalability, prompting the exploration of LLMs for this task.

Method: A training-free, feedback-aware system called QUITE based on LLM agents is proposed. It includes a multi-agent framework controlled by a FSM for real-time database feedback, a rewrite middleware to generate optimized queries, and a hint injection technique to improve execution plans.

Result: Experiments indicate that QUITE reduces query execution time by up to 35.8% compared to state-of-the-art approaches and produces 24.1% more rewrites than prior methods, covering previously unhandled query cases.

Conclusion: QUITE successfully overcomes the limitations of rule-based query rewriting by utilizing LLMs, enhancing query performance across a broader range of patterns and strategies.

Abstract: Query rewrite transforms SQL queries into semantically equivalent forms that
run more efficiently. Existing approaches mainly rely on predefined rewrite
rules, but they handle a limited subset of queries and can cause performance
regressions. This limitation stems from three challenges of rule-based query
rewrite: (1) it is hard to discover and verify new rules, (2) fixed rewrite
rules do not generalize to new query patterns, and (3) some rewrite techniques
cannot be expressed as fixed rules. Motivated by the fact that human experts
exhibit significantly better rewrite ability but suffer from scalability, and
Large Language Models (LLMs) have demonstrated nearly human-level semantic and
reasoning abilities, we propose a new approach of using LLMs to rewrite SQL
queries beyond rules. Due to the hallucination problems in LLMs, directly
applying LLMs often leads to nonequivalent and suboptimal queries. To address
this issue, we propose QUITE (query rewrite), a training-free and
feedback-aware system based on LLM agents that rewrites SQL queries into
semantically equivalent forms with significantly better performance, covering a
broader range of query patterns and rewrite strategies compared to rule-based
methods. Firstly, we design a multi-agent framework controlled by a finite
state machine (FSM) to equip LLMs with the ability to use external tools and
enhance the rewrite process with real-time database feedback. Secondly, we
develop a rewrite middleware to enhance the ability of LLMs to generate
optimized query equivalents. Finally, we employ a novel hint injection
technique to improve execution plans for rewritten queries. Extensive
experiments show that QUITE reduces query execution time by up to 35.8% over
state-of-the-art approaches and produces 24.1% more rewrites than prior
methods, covering query cases that earlier systems did not handle.

</details>


### [230] [LEANN: A Low-Storage Vector Index](https://arxiv.org/abs/2506.08276)
*Yichuan Wang,Shu Liu,Zhifei Li,Yongji Wu,Ziming Mao,Yilong Zhao,Xiao Yan,Zhiying Xu,Yang Zhou,Ion Stoica,Sewon Min,Matei Zaharia,Joseph E. Gonzalez*

Main category: cs.DB

TL;DR: LEANN is a storage-efficient ANN search index for personal devices, reducing index size to 5% of raw data with 90% recall and fast retrieval.


<details>
  <summary>Details</summary>
Motivation: There is a growing need to perform embedding-based search on personal data stored locally on devices, but the high storage overhead of traditional methods makes local deployment impractical.

Method: LEANN combines a compact graph-based structure with an efficient recomputation strategy to achieve fast and accurate retrieval with minimal storage.

Result: LEANN reduces index size to under 5% of the original data, achieving 50 times smaller storage than standard indexes, while maintaining 90% top-3 recall in under 2 seconds.

Conclusion: LEANN provides a solution for resource-constrained personal devices by significantly reducing storage requirements for embedding-based search without sacrificing quality or speed.

Abstract: Embedding-based search is widely used in applications such as recommendation
and retrieval-augmented generation (RAG). Recently, there is a growing demand
to support these capabilities over personal data stored locally on devices.
However, maintaining the necessary data structure associated with the
embedding-based search is often infeasible due to its high storage overhead.
For example, indexing 100 GB of raw data requires 150 to 700 GB of storage,
making local deployment impractical. Reducing this overhead while maintaining
search quality and latency becomes a critical challenge. In this paper, we
present LEANN, a storage-efficient approximate nearest neighbor (ANN) search
index optimized for resource-constrained personal devices. LEANN combines a
compact graph-based structure with an efficient on-the-fly recomputation
strategy to enable fast and accurate retrieval with minimal storage overhead.
Our evaluation shows that LEANN reduces index size to under 5% of the original
raw data, achieving up to 50 times smaller storage than standard indexes, while
maintaining 90% top-3 recall in under 2 seconds on real-world question
answering benchmarks.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [231] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: The paper introduces EDINET-Bench, an open-source Japanese financial benchmark to evaluate LLMs on tasks like fraud detection and earnings forecasting. Experiments show LLMs perform only slightly better than logistic regression in binary classification for these tasks. The dataset and code are publicly available.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the gap in challenging financial datasets, particularly for Japanese financial data, which impedes academic innovation in financial analytics and hinders the development and evaluation of LLMs in this specialized domain.

Method: The authors constructed EDINET-Bench by downloading annual reports from Japan's EDINET over the past 10 years and automatically assigning labels corresponding to evaluation tasks such as accounting fraud detection, earnings forecasting, and industry prediction.

Result: Experiments reveal that state-of-the-art LLMs struggle with these tasks, performing only slightly better than logistic regression in binary classification for fraud detection and earnings forecasting.

Conclusion: These results highlight significant challenges in applying LLMs to real-world financial applications and emphasize the need for domain-specific adaptation.

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [232] [Domain Switching on the Pareto Front: Multi-Objective Deep Kernel Learning in Automated Piezoresponse Force Microscopy](https://arxiv.org/abs/2506.08073)
*Yu Liu,Utkarsh Pratiush,Kamyar Barakati,Hiroshi Funakubo,Ching-Che Lin,Jaegyu Kim,Lane W. Martin,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: A multi-objective kernel-learning workflow infers microstructural rules governing ferroelectric polarization switching from high-resolution imaging data, enabling high-throughput active learning and mechanistic insights.


<details>
  <summary>Details</summary>
Motivation: Ferroelectric polarization switching is crucial for material performance, but its dependence on complex microstructural features makes manual exploration impractical.

Method: Introduced a multi-objective kernel-learning workflow applied to automated piezoresponse force microscopy (PFM) experiments. This framework identifies relationships between domain-wall configurations and local switching kinetics.

Result: The workflow revealed how specific wall geometries and defect distributions modulate polarization reversal, and projected abstract reward functions onto physically interpretable descriptors.

Conclusion: This approach not only provides high-throughput active learning for ferroelectric domain switching but also serves as a generalizable tool for navigating complex design spaces in various fields.

Abstract: Ferroelectric polarization switching underpins the functional performance of
a wide range of materials and devices, yet its dependence on complex local
microstructural features renders systematic exploration by manual or grid-based
spectroscopic measurements impractical. Here, we introduce a multi-objective
kernel-learning workflow that infers the microstructural rules governing
switching behavior directly from high-resolution imaging data. Applied to
automated piezoresponse force microscopy (PFM) experiments, our framework
efficiently identifies the key relationships between domain-wall configurations
and local switching kinetics, revealing how specific wall geometries and defect
distributions modulate polarization reversal. Post-experiment analysis projects
abstract reward functions, such as switching ease and domain symmetry, onto
physically interpretable descriptors including domain configuration and
proximity to boundaries. This enables not only high-throughput active learning,
but also mechanistic insight into the microstructural control of switching
phenomena. While demonstrated for ferroelectric domain switching, our approach
provides a powerful, generalizable tool for navigating complex,
non-differentiable design spaces, from structure-property correlations in
molecular discovery to combinatorial optimization across diverse imaging
modalities.

</details>


### [233] [Mic-hackathon 2024: Hackathon on Machine Learning for Electron and Scanning Probe Microscopy](https://arxiv.org/abs/2506.08423)
*Utkarsh Pratiush,Austin Houston,Kamyar Barakati,Aditya Raghavan,Dasol Yoon,Harikrishnan KP,Zhaslan Baraissov,Desheng Ma,Samuel S. Welborn,Mikolaj Jakowski,Shawn-Patrick Barhorst,Alexander J. Pattison,Panayotis Manganaris,Sita Sirisha Madugula,Sai Venkata Gayathri Ayyagari,Vishal Kennedy,Ralph Bulanadi,Michelle Wang,Kieran J. Pang,Ian Addison-Smith,Willy Menacho,Horacio V. Guzman,Alexander Kiefer,Nicholas Furth,Nikola L. Kolev,Mikhail Petrov,Viktoriia Liu,Sergey Ilyev,Srikar Rairao,Tommaso Rodani,Ivan Pinto-Huguet,Xuli Chen,Josep Cruañes,Marta Torrens,Jovan Pomar,Fanzhi Su,Pawan Vedanti,Zhiheng Lyu,Xingzhi Wang,Lehan Yao,Amir Taqieddin,Forrest Laskowski,Xiangyu Yin,Yu-Tsun Shao,Benjamin Fein-Ashley,Yi Jiang,Vineet Kumar,Himanshu Mishra,Yogesh Paul,Adib Bazgir,Rama chandra Praneeth Madugula,Yuwen Zhang,Pravan Omprakash,Jian Huang,Eric Montufar-Morales,Vivek Chawla,Harshit Sethi,Jie Huang,Lauri Kurki,Grace Guinan,Addison Salvador,Arman Ter-Petrosyan,Madeline Van Winkle,Steven R. Spurgeon,Ganesh Narasimha,Zijie Wu,Richard Liu,Yongtao Liu,Boris Slautin,Andrew R Lupini,Rama Vasudevan,Gerd Duscher,Sergei V. Kalinin*

Main category: cond-mat.mtrl-sci

TL;DR: Hackathons foster collaboration between ML researchers and microscopy experts to bridge the gap, producing benchmark datasets and digital twins of microscopes.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in data usage and extensive analysis time in microscopy due to lack of standardized code ecosystems, benchmarks, and integration strategies.

Method: Organizing hackathons that bring together ML researchers and microscopy experts to develop novel solutions applying ML to microscopy.

Result: Produced benchmark datasets and digital twins of microscopes, supporting community growth and standardized workflows.

Conclusion: All related code is made available publicly on GitHub to facilitate further development and collaboration.

Abstract: Microscopy is a primary source of information on materials structure and
functionality at nanometer and atomic scales. The data generated is often
well-structured, enriched with metadata and sample histories, though not always
consistent in detail or format. The adoption of Data Management Plans (DMPs) by
major funding agencies promotes preservation and access. However, deriving
insights remains difficult due to the lack of standardized code ecosystems,
benchmarks, and integration strategies. As a result, data usage is inefficient
and analysis time is extensive. In addition to post-acquisition analysis, new
APIs from major microscope manufacturers enable real-time, ML-based analytics
for automated decision-making and ML-agent-controlled microscope operation.
Yet, a gap remains between the ML and microscopy communities, limiting the
impact of these methods on physics, materials discovery, and optimization.
Hackathons help bridge this divide by fostering collaboration between ML
researchers and microscopy experts. They encourage the development of novel
solutions that apply ML to microscopy, while preparing a future workforce for
instrumentation, materials science, and applied ML. This hackathon produced
benchmark datasets and digital twins of microscopes to support community growth
and standardized workflows. All related code is available at GitHub:
https://github.com/KalininGroup/Mic-hackathon-2024-codes-publication/tree/1.0.0.1

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [234] [MOSS: Multi-Objective Optimization for Stable Rule Sets](https://arxiv.org/abs/2506.08030)
*Brian Liu,Rahul Mazumder*

Main category: math.OC

TL;DR: The paper introduces MOSS, a framework for creating stable decision rule sets with a focus on interpretability through sparsity, accuracy, and stability. It provides a tool for practitioners to evaluate the trade-off between accuracy and stability using a specialized cutting plane algorithm that computes the Pareto frontier and scales beyond commercial solvers. Experiments demonstrate MOSS outperforms current rule ensembles in predictive performance and stability.


<details>
  <summary>Details</summary>
Motivation: There is a need for interpretable decision rule sets that balance accuracy, sparsity, and stability, which are crucial for understanding and trusting machine learning models. Existing methods may not adequately address these aspects simultaneously or efficiently explore the trade-offs between them.

Method: MOSS integrates three criteria (sparsity, accuracy, and stability) into a multi-objective optimization framework. A specialized cutting plane algorithm is developed to compute the Pareto frontier between accuracy and stability, allowing users to explore trade-offs and select suitable models. This algorithm can handle larger problem instances than commercial solvers.

Result: Experiments indicate that MOSS surpasses state-of-the-art rule ensembles in both predictive performance and stability, demonstrating its effectiveness and efficiency.

Conclusion: MOSS provides an effective framework for constructing interpretable decision rule sets with a good balance of sparsity, accuracy, and stability. The specialized cutting plane algorithm enables rapid evaluation of trade-offs and selection of appropriate models, outperforming existing methods.

Abstract: We present MOSS, a multi-objective optimization framework for constructing
stable sets of decision rules. MOSS incorporates three important criteria for
interpretability: sparsity, accuracy, and stability, into a single
multi-objective optimization framework. Importantly, MOSS allows a practitioner
to rapidly evaluate the trade-off between accuracy and stability in sparse rule
sets in order to select an appropriate model. We develop a specialized cutting
plane algorithm in our framework to rapidly compute the Pareto frontier between
these two objectives, and our algorithm scales to problem instances beyond the
capabilities of commercial optimization solvers. Our experiments show that MOSS
outperforms state-of-the-art rule ensembles in terms of both predictive
performance and stability.

</details>


### [235] [Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence](https://arxiv.org/abs/2506.08121)
*Qi Feng,Gu Wang*

Main category: math.OC

TL;DR: A continuous policy-value iteration algorithm is introduced, which updates value function and optimal control via Langevin-type dynamics for both entropy-regularized and classical control problems.


<details>
  <summary>Details</summary>
Motivation: To develop an algorithm that can simultaneously update approximations of the value function and optimal control in stochastic control problems using Langevin-type dynamics.

Method: The algorithm applies to both entropy-regularized relaxed control and classical control problems with infinite horizon. It uses policy improvement and converges to the optimal control under a monotonicity condition of the Hamiltonian. It employs Langevin-type stochastic differential equations for continuous updates along the policy iteration direction.

Result: The approach enables distribution sampling and non-convex learning techniques in machine learning to optimize the value function and identify the optimal control simultaneously.

Conclusion: This method provides a novel way to solve stochastic control problems by integrating machine learning techniques into the optimization process.

Abstract: We introduce a continuous policy-value iteration algorithm where the
approximations of the value function of a stochastic control problem and the
optimal control are simultaneously updated through Langevin-type dynamics. This
framework applies to both the entropy-regularized relaxed control problems and
the classical control problems, with infinite horizon. We establish policy
improvement and demonstrate convergence to the optimal control under the
monotonicity condition of the Hamiltonian. By utilizing Langevin-type
stochastic differential equations for continuous updates along the policy
iteration direction, our approach enables the use of distribution sampling and
non-convex learning techniques in machine learning to optimize the value
function and identify the optimal control simultaneously.

</details>


### [236] [Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(ε^{-4/7})$ Second-Order Oracle Complexity](https://arxiv.org/abs/2506.08362)
*Lesi Chen,Chengchang Liu,Luo Luo,Jingzhao Zhang*

Main category: math.OC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in
\mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with
$\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type
methods. This result has been speculated to be optimal because the upper bound
is achieved by a natural generalization of the optimal first-order method. In
this work, we show an improved upper bound of
$\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order
method for convex optimization to solve the convex-concave minimax problem. We
further apply a similar technique to lazy Hessian algorithms and show that our
proposed algorithm can also be seen as a second-order ``Catalyst'' framework
(Lin et al., JMLR 2018) that could accelerate any globally convergent
algorithms for solving minimax problems.

</details>


### [237] [Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings](https://arxiv.org/abs/2506.08428)
*Evan Markou,Thalaiyasingam Ajanthan,Stephen Gould*

Main category: math.OC

TL;DR: The paper explores how reduction mappings that exploit geometric structures in high-dimensional optimisation problems can improve the optimisation landscape, leading to faster convergence for gradient-based methods.


<details>
  <summary>Details</summary>
Motivation: Many high-dimensional optimisation problems have rich geometric structures in their minimisers, which can form smooth manifolds due to over-parametrisation or symmetries. Exploiting these structures could potentially enhance optimisation algorithms.

Method: The authors introduce a general framework to understand the influence of reduction mappings on the optimisation landscape. These mappings reparametrise part of the parameter space to lie on the solution manifold, effectively removing redundant directions and yielding a lower-dimensional objective.

Result: Well-designed reduction mappings are shown to improve curvature properties of the objective, resulting in better-conditioned problems and theoretically faster convergence for gradient-based methods.

Conclusion: This analysis unifies various scenarios where structural information is used to accelerate convergence, providing a principled explanation for the empirical gains observed in such optimisation algorithms.

Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures
in their set of minimisers, often forming smooth manifolds due to
over-parametrisation or symmetries. When this structure is known, at least
locally, it can be exploited through reduction mappings that reparametrise part
of the parameter space to lie on the solution manifold. These reductions
naturally arise from inner optimisation problems and effectively remove
redundant directions, yielding a lower-dimensional objective. In this work, we
introduce a general framework to understand how such reductions influence the
optimisation landscape. We show that well-designed reduction mappings improve
curvature properties of the objective, leading to better-conditioned problems
and theoretically faster convergence for gradient-based methods. Our analysis
unifies a range of scenarios where structural information at optimality is
leveraged to accelerate convergence, offering a principled explanation for the
empirical gains observed in such optimisation algorithms.

</details>


### [238] [Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees](https://arxiv.org/abs/2506.08558)
*William de Vazelhes,Xiao-Tong Yuan,Bin Gu*

Main category: math.OC

TL;DR: 在稀疏优化中，使用ℓ₀伪范数强制硬约束比凸松弛有优势，但许多实际应用需要额外的约束条件。本文研究了带有额外支持保留约束的稀疏优化问题，并提出了一种新的迭代硬阈值算法变体，提供全局目标值保证并改进了现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管先前的算法已经开发出来以解决具有混合组合和凸约束的复杂场景，但它们通常需要封闭形式的投影到混合约束上，这可能不存在，并且只提供局部收敛保证，与稀疏优化中常见的全局保证不同。

Method: 作者提出了一个新的迭代硬阈值算法变体，配备了为这些混合约束定制的两步连续投影算子，作为欧几里德投影到混合约束上的简单替代方案。通过引入稀疏性松弛和次优性的新权衡，作者提供了在确定性、随机和零阶设置下的目标值全局保证。此外，作者还发展了经典三点引理的新扩展到考虑的两步非凸投影算子，允许以一种优雅的方式分析目标值的收敛性。

Result: 该方法在零阶情况下改进了de Vazelhes等人的最新结果（2022年），即使在没有额外约束的情况下，也能去除他们工作中存在的非消失系统误差。

Conclusion: 本文提出了一种新的迭代硬阈值算法变体，适用于带有额外支持保留约束的稀疏优化问题，并通过引入稀疏性松弛和次优性的权衡，提供了全局目标值保证。此外，提出的证明技术也为分析此类问题的收敛性提供了新的视角。

Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$
pseudo-norm offers advantages like controlled sparsity compared to convex
relaxations. However, many real-world applications demand not only sparsity
constraints but also some extra constraints. While prior algorithms have been
developed to address this complex scenario with mixed combinatorial and convex
constraints, they typically require the closed form projection onto the mixed
constraints which might not exist, and/or only provide local guarantees of
convergence which is different from the global guarantees commonly sought in
sparse optimization. To fill this gap, in this paper, we study the problem of
sparse optimization with extra \qw{\textit{support-preserving}} constraints
commonly encountered in the literature. We present a new variant of iterative
hard-thresholding algorithm equipped with a two-step consecutive projection
operator customized for these mixed constraints, serving as a simple
alternative to the Euclidean projection onto the mixed constraint. By
introducing a novel trade-off between sparsity relaxation and sub-optimality,
we provide global guarantees in objective value for the output of our
algorithm, in the deterministic, stochastic, and zeroth-order settings, under
the conventional restricted strong-convexity/smoothness assumptions. As a
fundamental contribution in proof techniques, we develop a novel extension of
the classic three-point lemma to the considered two-step non-convex projection
operator, which allows us to analyze the convergence in objective value in an
elegant way that has not been possible with existing techniques. In the
zeroth-order case, such technique also improves upon the state-of-the-art
result from de Vazelhes et. al. (2022), even in the case without additional
constraints, by allowing us to remove a non-vanishing system error present in
their work.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [239] [Aligning Proteins and Language: A Foundation Model for Protein Retrieval](https://arxiv.org/abs/2506.08023)
*Qifeng Wu,Zhengzhe Liu,Han Zhu,Yizhou Zhao,Daisuke Kihara,Min Xu*

Main category: q-bio.BM

TL;DR: This paper aims to retrieve proteins with similar structures and semantics from large-scale protein dataset using a CLIP-style framework, demonstrating promising zero-shot retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Motivated by the recent progress of vision-language models (VLMs).

Method: Propose a CLIP-style framework for aligning 3D protein structures with functional annotations using contrastive learning, and a large-scale dataset of approximately 200,000 protein-caption pairs for model training.

Result: Demonstrates promising zero-shot retrieval performance in both in-domain and cross-database retrieval on Protein Data Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset.

Conclusion: Highlights the potential of multimodal foundation models for structure-function understanding in protein biology.

Abstract: This paper aims to retrieve proteins with similar structures and semantics
from large-scale protein dataset, facilitating the functional interpretation of
protein structures derived by structural determination methods like
cryo-Electron Microscopy (cryo-EM). Motivated by the recent progress of
vision-language models (VLMs), we propose a CLIP-style framework for aligning
3D protein structures with functional annotations using contrastive learning.
For model training, we propose a large-scale dataset of approximately 200,000
protein-caption pairs with rich functional descriptors. We evaluate our model
in both in-domain and more challenging cross-database retrieval on Protein Data
Bank (PDB) and Electron Microscopy Data Bank (EMDB) dataset, respectively. In
both cases, our approach demonstrates promising zero-shot retrieval
performance, highlighting the potential of multimodal foundation models for
structure-function understanding in protein biology.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [240] [DCD: A Semantic Segmentation Model for Fetal Ultrasound Four-Chamber View](https://arxiv.org/abs/2506.08534)
*Donglian Li,Hui Guo,Minglang Chen,Huizhen Chen,Jialing Chen,Bocheng Liang,Pengchen Liang,Ying Tan*

Main category: eess.IV

TL;DR: An advanced deep learning model, DCD, is proposed for accurate segmentation of anatomical structures in fetal A4C view to help diagnose CHD.


<details>
  <summary>Details</summary>
Motivation: Accurate segmentation of anatomical structures in the apical four-chamber (A4C) view of fetal echocardiography is crucial for early diagnosis and prenatal evaluation of congenital heart disease (CHD). But it's difficult due to ultrasound artifacts, speckle noise, anatomical variability, and boundary ambiguity.

Method: The proposed model DCD incorporates Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module for multi-scale feature extraction and Convolutional Block Attention Module (CBAM) for adaptive feature representation. It captures both local and global contextual information effectively.

Result: DCD achieves precise and robust segmentation which can reduce sonographers' workload and improve segmentation accuracy.

Conclusion: DCD contributes to improved prenatal cardiac assessment by providing an automatic way for precise segmentation of key anatomical structures in the fetal A4C view.

Abstract: Accurate segmentation of anatomical structures in the apical four-chamber
(A4C) view of fetal echocardiography is essential for early diagnosis and
prenatal evaluation of congenital heart disease (CHD). However, precise
segmentation remains challenging due to ultrasound artifacts, speckle noise,
anatomical variability, and boundary ambiguity across different gestational
stages. To reduce the workload of sonographers and enhance segmentation
accuracy, we propose DCD, an advanced deep learning-based model for automatic
segmentation of key anatomical structures in the fetal A4C view. Our model
incorporates a Dense Atrous Spatial Pyramid Pooling (Dense ASPP) module,
enabling superior multi-scale feature extraction, and a Convolutional Block
Attention Module (CBAM) to enhance adaptive feature representation. By
effectively capturing both local and global contextual information, DCD
achieves precise and robust segmentation, contributing to improved prenatal
cardiac assessment.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [241] [Inverse Design in Distributed Circuits Using Single-Step Reinforcement Learning](https://arxiv.org/abs/2506.08029)
*Jiayu Li,Masood Mortazavi,Ning Yan,Yihong Ma,Reza Zafarani*

Main category: eess.SY

TL;DR: The paper proposes DCIDA, a design exploration framework that learns to generate near-optimal distributed circuit designs for target transfer functions, showing significant improvements over existing methods especially for complex functions.


<details>
  <summary>Details</summary>
Motivation: Inverse design in distributed circuits aims to create designs matching desirable transfer function specifications. Current methods have limitations when faced with real-world requirements such as non-differentiable evaluations, varying topologies, and near-continuous placement spaces.

Method: DCIDA employs a Transformer-based policy network to sample near-optimal design actions in a single step from jointly-trained conditional distributions. It uses an injective interdependent mapping to transform raw sampled actions into physical representations, capturing conditional dependencies among design decisions.

Result: Experiments indicate that DCIDA significantly reduces design error compared to state-of-the-art methods, particularly excelling in fitting complex transfer functions.

Conclusion: DCIDA offers a robust framework for inverse design in distributed circuits by effectively handling non-differentiable procedures, varying topologies, and continuous placement spaces, providing superior performance for complex transfer functions.

Abstract: The goal of inverse design in distributed circuits is to generate
near-optimal designs that meet a desirable transfer function specification.
Existing design exploration methods use some combination of strategies
involving artificial grids, differentiable evaluation procedures, and specific
template topologies. However, real-world design practices often require
non-differentiable evaluation procedures, varying topologies, and
near-continuous placement spaces. In this paper, we propose DCIDA, a design
exploration framework that learns a near-optimal design sampling policy for a
target transfer function. DCIDA decides all design factors in a compound
single-step action by sampling from a set of jointly-trained conditional
distributions generated by the policy. Utilizing an injective interdependent
``map", DCIDA transforms raw sampled design ``actions" into uniquely equivalent
physical representations, enabling the framework to learn the conditional
dependencies among joint ``raw'' design decisions. Our experiments demonstrate
DCIDA's Transformer-based policy network achieves significant reductions in
design error compared to state-of-the-art approaches, with significantly better
fit in cases involving more complex transfer functions.

</details>


### [242] [Feasibility Study of CNNs and MLPs for Radiation Heat Transfer in 2-D Furnaces with Spectrally Participative Gases](https://arxiv.org/abs/2506.08033)
*Axel TahmasebiMoradi,Vincent Ren,Benjamin Le-Creurer,Chetra Mang*

Main category: eess.SY

TL;DR: To decrease the computational cost of numerical simulations, this paper introduces a CNN and an MLP to create a surrogate model for approximating radiative heat transfer solutions in a 2D domain with participative gases. The inputs (gas and wall properties) are adapted to fit the CNN architecture. Two precision datasets were created using ICARUS2D solver. Performance of both architectures is compared in terms of speed and accuracy with optimized hyperparameters via Optuna. Results indicate significant speedup with acceptable errors, with CNN outperforming MLP in precision, robustness, and stability. A performance analysis on dataset size was also conducted.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to reduce the computational cost associated with numerical simulations of radiative heat transfer in a 2-D walled domain with participative gases by employing machine learning techniques, specifically convolutional neural networks (CNNs) and multi-layer perceptrons (MLPs).

Method: The method involves adapting the inputs of the problem (gas and wall properties) to fit the CNN architecture and creating two precision datasets using the classical solver, ICARUS2D. Both CNN and MLP architectures are used to build surrogate models for approximating radiative heat transfer solutions. Hyperparameters are optimized using Optuna, and the performance of both architectures is compared in terms of speed and accuracy.

Result: The results show a significant speedup with industrially acceptable relative errors compared to the classical solver for both architectures. Additionally, the CNN outperforms the MLP in terms of precision and is more robust and stable to changes in hyper-parameters. A deeper understanding of the model behavior was gained through a performance analysis on the dataset size of the samples.

Conclusion: In conclusion, the use of CNN and MLP architectures as surrogate models significantly reduces computational cost while maintaining acceptable accuracy in approximating radiative heat transfer solutions. The CNN shows superior performance in terms of precision, robustness, and stability.

Abstract: Aiming to reduce the computational cost of numerical simulations, a
convolutional neural network (CNN) and a multi-layer perceptron (MLP) are
introduced to build a surrogate model to approximate radiative heat transfer
solutions in a 2-D walled domain with participative gases. The originality of
this work lays in the adaptation of the inputs of the problem (gas and wall
properties) in order to fit with the CNN architecture, more commonly used for
image processing. Two precision datasets have been created with the classical
solver, ICARUS2D, that uses the discrete transfer radiation method with the
statistical narrow bands model. The performance of the CNN architecture is
compared to a more classical MLP architecture in terms of speed and accuracy.
Thanks to Optuna, all results are obtained using the optimized hyper parameters
networks. The results show a significant speedup with industrially acceptable
relative errors compared to the classical solver for both architectures.
Additionally, the CNN outperforms the MLP in terms of precision and is more
robust and stable to changes in hyper-parameters. A performance analysis on the
dataset size of the samples have also been carried out to gain a deeper
understanding of the model behavior.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [243] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: Instruction-tuned multimodal large language models show superior brain alignment compared to non-instruction-tuned and unimodal models, with hierarchical layer alignment in processing video and audio tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate the brain alignment of instruction-tuned multimodal large language models (MLLMs) using naturalistic movies as stimuli, addressing the gap in prior work which mainly focused on unimodal settings or non-instruction-tuned models.

Method: The study utilized instruction-specific embeddings from six video and two audio instruction-tuned MLLMs. Experiments were conducted with 13 video task-specific instructions to measure predictivity of neural activity recorded while participants watched naturalistic movies.

Result: Instruction-tuned video MLLMs significantly outperformed non-instruction-tuned multimodal models by 15% and unimodal models by 20%. Task-specific representations from MLLMs showed clear disentanglement, leading to precise differentiation of multimodal functional processing in the brain. MLLM layers align hierarchically with the brain.

Conclusion: Task-specific instructions play a crucial role in improving the alignment between brain activity and MLLMs, opening new avenues for understanding joint information processing in both systems.

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [244] [Real-Time Cascade Mitigation in Power Systems Using Influence Graph Improved by Reinforcement Learning](https://arxiv.org/abs/2506.08893)
*Kai Zhou,Youbiao He,Chong Zhong,Yifu Wu*

Main category: physics.soc-ph

TL;DR: The paper proposes a Markov decision process model (MDP) extended from the influence graph for real-time mitigation of cascading outages in power transmission systems under uncertainties, solved using reinforcement learning. It validates the model on IEEE 14-bus and IEEE 118-bus systems.


<details>
  <summary>Details</summary>
Motivation: Modern power systems with increasing renewable energy penetration face higher risks of cascading outages. Real-time mitigation requires fast and complex operational decisions under uncertainty.

Method: The influence graph is extended into an MDP model for real-time cascade mitigation in power transmission systems. The MDP includes uncertainties in generation, load, and initial contingencies and incorporates a do-nothing action for conservative decision-making. Reinforcement learning is used to solve the MDP, and a policy gradient learning algorithm initialized with a policy corresponding to the unmitigated case is presented to handle invalid actions.

Result: Proactive line disconnections can effectively reduce cascading risk in power transmission systems. Certain lines consistently emerge as critical in mitigating cascade propagation. The proposed learning method converges faster than conventional algorithms.

Conclusion: A Markov decision process model extended from the influence graph can be effectively used for real-time mitigation of cascading outages in power transmission systems under uncertainties, with validation on IEEE test systems.

Abstract: Despite high reliability, modern power systems with growing renewable
penetration face an increasing risk of cascading outages. Real-time cascade
mitigation requires fast, complex operational decisions under uncertainty. In
this work, we extend the influence graph into a Markov decision process model
(MDP) for real-time mitigation of cascading outages in power transmission
systems, accounting for uncertainties in generation, load, and initial
contingencies. The MDP includes a do-nothing action to allow for conservative
decision-making and is solved using reinforcement learning. We present a policy
gradient learning algorithm initialized with a policy corresponding to the
unmitigated case and designed to handle invalid actions. The proposed learning
method converges faster than the conventional algorithm. Through careful reward
design, we learn a policy that takes conservative actions without deteriorating
system conditions. The model is validated on the IEEE 14-bus and IEEE 118-bus
systems. The results show that proactive line disconnections can effectively
reduce cascading risk, and certain lines consistently emerge as critical in
mitigating cascade propagation.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [245] [The World of AI: A Novel Approach to AI Literacy for First-year Engineering Students](https://arxiv.org/abs/2506.08041)
*Siddharth Siddharth,Brainerd Prince,Amol Harsh,Shreyas Ramachandran*

Main category: cs.CY

TL;DR: A new interdisciplinary course 'The World of AI' for first-year engineering students improved their understanding of AI challenges and its societal implications.


<details>
  <summary>Details</summary>
Motivation: Engineering students often lack foundational knowledge of AI and its societal implications at the outset of their academic journeys.

Method: Design and deliver an interdisciplinary course with three modules (planetary, societal impact, workplace) co-taught by faculty from engineering and humanities, focusing on understanding AI without mathematics and appreciating its implications.

Result: Students showed improved comprehension of AI challenges across diverse metrics, increased awareness of environmental impact, and efficient solutions for AI fairness. Their perception of AI's transformative impact also evolved.

Conclusion: The interdisciplinary curriculum design combining technical instruction with societal discourse is effective in enhancing first-year engineering students' understanding of AI.

Abstract: This work presents a novel course titled The World of AI designed for
first-year undergraduate engineering students with little to no prior exposure
to AI. The central problem addressed by this course is that engineering
students often lack foundational knowledge of AI and its broader societal
implications at the outset of their academic journeys. We believe the way to
address this gap is to design and deliver an interdisciplinary course that can
a) be accessed by first-year undergraduate engineering students across any
domain, b) enable them to understand the basic workings of AI systems sans
mathematics, and c) make them appreciate AI's far-reaching implications on our
lives. The course was divided into three modules co-delivered by faculty from
both engineering and humanities. The planetary module explored AI's dual role
as both a catalyst for sustainability and a contributor to environmental
challenges. The societal impact module focused on AI biases and concerns around
privacy and fairness. Lastly, the workplace module highlighted AI-driven job
displacement, emphasizing the importance of adaptation. The novelty of this
course lies in its interdisciplinary curriculum design and pedagogical
approach, which combines technical instruction with societal discourse. Results
revealed that students' comprehension of AI challenges improved across diverse
metrics like (a) increased awareness of AI's environmental impact, and (b)
efficient corrective solutions for AI fairness. Furthermore, it also indicated
the evolution in students' perception of AI's transformative impact on our
lives.

</details>


### [246] [Evaluation of Machine Learning Models in Student Academic Performance Prediction](https://arxiv.org/abs/2506.08047)
*A. G. R. Sandeepa,Sanka Mohottala*

Main category: cs.CY

TL;DR: This paper explores using machine learning, particularly multi-layer perceptron classifier (MLPC), to predict students' academic performance, achieving 86.46% accuracy with feature selection enhancing results.


<details>
  <summary>Details</summary>
Motivation: To identify an effective method for forecasting students' academic performance using available data with behavioral, academic, and demographic details.

Method: Implementing standard classical machine learning models including MLPC on student data, utilizing feature selection, multiple evaluation approaches, and explainable machine learning methods.

Result: MLPC achieved 86.46% maximum accuracy on the test set, with an average of 79.58% under 10-fold cross validation, showing neural networks' potential as data-efficient models.

Conclusion: Neural networks like MLP are promising for predicting student performance with enhanced data efficiency through feature selection.

Abstract: This research investigates the use of machine learning methods to forecast
students' academic performance in a school setting. Students' data with
behavioral, academic, and demographic details were used in implementations with
standard classical machine learning models including multi-layer perceptron
classifier (MLPC). MLPC obtained 86.46% maximum accuracy for test set across
all implementations. Under 10-fold cross validation, MLPC obtained 79.58%
average accuracy for test set while for train set, it was 99.65%. MLP's better
performance over other machine learning models strongly suggest the potential
use of neural networks as data-efficient models. Feature selection approach
played a crucial role in improving the performance and multiple evaluation
approaches were used in order to compare with existing literature. Explainable
machine learning methods were utilized to demystify the black box models and to
validate the feature selection approach.

</details>


### [247] [WIP: Large Language Model-Enhanced Smart Tutor for Undergraduate Circuit Analysis](https://arxiv.org/abs/2506.08962)
*Liangliang Chen,Huiru Xie,Jacqueline Rohde,Ying Zhang*

Main category: cs.CY

TL;DR: This paper introduces an AI-enabled smart tutor designed for homework assessment and feedback in an undergraduate circuit analysis course. Deployed on Microsoft Azure, it offers personalized instruction, collects student interaction data, and provides instructors with insights into student difficulties. Student satisfaction is high (90.9%), and future work includes a full dataset analysis and exploring broader applications.


<details>
  <summary>Details</summary>
Motivation: To provide effective homework assessment and feedback for students in an undergraduate circuit analysis course using AI technology, enhancing personalized learning experiences and improving teaching strategies based on real-time student data.

Method: Design and implementation of an AI-enabled smart tutor that includes open-ended question answering and homework feedback generation. Carefully crafted prompts optimize responses across different problems, deployed on the Microsoft Azure platform.

Result: 90.9% of student feedback indicates satisfaction with the tutor. Data analysis reveals tutor usage frequency per problem and frequently asked questions, helping instructors understand student difficulties better.

Conclusion: The AI-enabled smart tutor effectively supports personalized instruction and feedback in circuit analysis courses. Future work will involve comprehensive data analysis and expanding its application to other engineering disciplines.

Abstract: This research-to-practice work-in-progress (WIP) paper presents an AI-enabled
smart tutor designed to provide homework assessment and feedback for students
in an undergraduate circuit analysis course. We detail the tutor's design
philosophy and core components, including open-ended question answering and
homework feedback generation. The prompts are carefully crafted to optimize
responses across different problems. The smart tutor was deployed on the
Microsoft Azure platform and is currently in use in an undergraduate circuit
analysis course at the School of Electrical and Computer Engineering in a
large, public, research-intensive institution in the Southeastern United
States. Beyond offering personalized instruction and feedback, the tutor
collects student interaction data, which is summarized and shared with the
course instructor. To evaluate its effectiveness, we collected student
feedback, with 90.9% of responses indicating satisfaction with the tutor.
Additionally, we analyze a subset of collected data on preliminary circuit
analysis topics to assess tutor usage frequency for each problem and identify
frequently asked questions. These insights help instructors gain real-time
awareness of student difficulties, enabling more targeted classroom
instruction. In future work, we will release a full analysis once the complete
dataset is available after the Spring 2025 semester. We also explore the
potential applications of this smart tutor across a broader range of
engineering disciplines by developing improved prompts, diagram-recognition
methods, and database management strategies, which remain ongoing areas of
research.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [248] [syren-baryon: Analytic emulators for the impact of baryons on the matter power spectrum](https://arxiv.org/abs/2506.08783)
*Lukas Kammerer,Deaglan J. Bartlett,Gabriel Kronberger,Harry Desmond,Pedro G. Ferreira*

Main category: astro-ph.CO

TL;DR: 研究人员通过符号回归方法构建了分析近似值，以表示有无重子效应下物质功率谱的比率。这些表达式适用于不同重子物理模型，并能解释不确定性来源。误差与先前数值模拟相当，且在大尺度和高红移时行为正确。此研究有助于区分重子物理模型并提供公开代码。


<details>
  <summary>Details</summary>
Motivation: 重子物理对宇宙中物质分布有着显著影响，特别是在当前和未来的宇宙学调查所涉及的尺度上。这种影响是此类分析中的关键系统性因素，因此需要简单的参数化方法来描述重子物理对物质功率谱的影响。

Method: 使用符号回归技术构建解析近似值，表示包含重子效应和不包含重子效应下的物质功率谱之比。基于CAMELS模拟套件中的四种不同重子物理亚网格模型（Astrid, IllustrisTNG, SIMBA 和 Swift-EAGLE）以及一种重子化算法，分别获得对应的函数。同时提供了描述这些预测不确定性的函数，考虑了重子物理的随机性质和拟合误差。

Result: 对于水动力学模拟，近似误差与通过变化初始条件估计的样本方差相当；重子化表达式的均方根误差优于1%，尽管在小尺度上有所增加。这些误差与之前这些模型的数值仿真器误差相当。表达式在大尺度和高红移时具有正确的物理行为。可以解析地解释宇宙学和反馈参数的变化影响，并识别出几乎无影响的参数。

Conclusion: 得到的每个函数基于不同的重子物理实现方式，可应用于真实数据以区分这些模型。所有符号近似值的代码已公开。

Abstract: Baryonic physics has a considerable impact on the distribution of matter in
our Universe on scales probed by current and future cosmological surveys,
acting as a key systematic in such analyses. We seek simple symbolic
parametrisations for the impact of baryonic physics on the matter power
spectrum for a range of physically motivated models, as a function of
wavenumber, redshift, cosmology, and parameters controlling the baryonic
feedback. We use symbolic regression to construct analytic approximations for
the ratio of the matter power spectrum in the presence of baryons to that
without such effects. We obtain separate functions of each of four distinct
sub-grid prescriptions of baryonic physics from the CAMELS suite of
hydrodynamical simulations (Astrid, IllustrisTNG, SIMBA and Swift-EAGLE) as
well as for a baryonification algorithm. We also provide functions which
describe the uncertainty on these predictions, due to both the stochastic
nature of baryonic physics and the errors on our fits. The error on our
approximations to the hydrodynamical simulations is comparable to the sample
variance estimated through varying initial conditions, and our baryonification
expression has a root mean squared error of better than one percent, although
this increases on small scales. These errors are comparable to those of
previous numerical emulators for these models. Our expressions are enforced to
have the physically correct behaviour on large scales and at high redshift. Due
to their analytic form, we are able to directly interpret the impact of varying
cosmology and feedback parameters, and we can identify parameters which have
little to no effect. Each function is based on a different implementation of
baryonic physics, and can therefore be used to discriminate between these
models when applied to real data. We provide publicly available code for all
symbolic approximations found.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [249] [CaliciBoost: Performance-Driven Evaluation of Molecular Representations for Caco-2 Permeability Prediction](https://arxiv.org/abs/2506.08059)
*Huong Van Le,Weibin Ren,Junhong Kim,Yukyung Yun,Young Bin Park,Young Jun Kim,Bok Kyung Han,Inho Choi,Jong IL Park,Hwi-Yeol Yun,Jae-Mun Choi*

Main category: q-bio.QM

TL;DR: This paper explores the impact of various molecular feature representation types combined with AutoML techniques to predict Caco-2 permeability, finding that PaDEL, Mordred, and RDKit descriptors are particularly effective, and CaliciBoost achieved the best MAE performance. Incorporating 3D descriptors led to a significant reduction in MAE.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and efficiency of computational predictions for Caco-2 permeability, which is a critical indicator for predicting oral drug absorption in early-stage drug discovery.

Method: Systematically investigated eight molecular feature representation types including 2D/3D descriptors, structural fingerprints, and deep learning-based embeddings combined with automated machine learning techniques. Used two datasets (TDC benchmark and curated OCHEM data) to assess model performance across representations.

Result: Identified PaDEL, Mordred, and RDKit descriptors as particularly effective for Caco-2 prediction. The AutoML-based model CaliciBoost achieved the best MAE performance. Incorporation of 3D descriptors resulted in a 15.73% reduction in MAE compared to using 2D features alone.

Conclusion: AutoML approaches are effective in ADMET modeling and offer practical guidance for feature selection in data-limited prediction tasks.

Abstract: Caco-2 permeability serves as a critical in vitro indicator for predicting
the oral absorption of drug candidates during early-stage drug discovery. To
enhance the accuracy and efficiency of computational predictions, we
systematically investigated the impact of eight molecular feature
representation types including 2D/3D descriptors, structural fingerprints, and
deep learning-based embeddings combined with automated machine learning
techniques to predict Caco-2 permeability. Using two datasets of differing
scale and diversity (TDC benchmark and curated OCHEM data), we assessed model
performance across representations and identified PaDEL, Mordred, and RDKit
descriptors as particularly effective for Caco-2 prediction. Notably, the
AutoML-based model CaliciBoost achieved the best MAE performance. Furthermore,
for both PaDEL and Mordred representations, the incorporation of 3D descriptors
resulted in a 15.73% reduction in MAE compared to using 2D features alone, as
confirmed by feature importance analysis. These findings highlight the
effectiveness of AutoML approaches in ADMET modeling and offer practical
guidance for feature selection in data-limited prediction tasks.

</details>


### [250] [Protriever: End-to-End Differentiable Protein Homology Search for Fitness Prediction](https://arxiv.org/abs/2506.08954)
*Ruben Weitzman,Peter Mørch Groth,Lood Van Niekerk,Aoi Otani,Yarin Gal,Debora Marks,Pascal Notin*

Main category: q-bio.QM

TL;DR: Protriever is a new framework that retrieves homologous protein sequences more efficiently and accurately than traditional methods, achieving state-of-the-art performance in protein fitness prediction while being significantly faster.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for retrieving homologous protein sequences using Multiple Sequence Alignments (MSA) are computationally expensive, struggle with highly divergent sequences or complex patterns, and are independent of downstream modeling objectives.

Method: Protriever is an end-to-end differentiable framework that learns to retrieve relevant homologs while simultaneously training for the target task. It uses efficient vector search instead of MSA-based retrieval.

Result: Protriever achieves state-of-the-art performance in protein fitness prediction compared to sequence-based models that rely on MSA-based homolog retrieval, while being two orders of magnitude faster.

Conclusion: Protriever offers a scalable alternative to alignment-centric approaches and can flexibly adapt to different retrieval strategies and protein databases at inference time.

Abstract: Retrieving homologous protein sequences is essential for a broad range of
protein modeling tasks such as fitness prediction, protein design, structure
modeling, and protein-protein interactions. Traditional workflows have relied
on a two-step process: first retrieving homologs via Multiple Sequence
Alignments (MSA), then training models on one or more of these alignments.
However, MSA-based retrieval is computationally expensive, struggles with
highly divergent sequences or complex insertions & deletions patterns, and
operates independently of the downstream modeling objective. We introduce
Protriever, an end-to-end differentiable framework that learns to retrieve
relevant homologs while simultaneously training for the target task. When
applied to protein fitness prediction, Protriever achieves state-of-the-art
performance compared to sequence-based models that rely on MSA-based homolog
retrieval, while being two orders of magnitude faster through efficient vector
search. Protriever is both architecture- and task-agnostic, and can flexibly
adapt to different retrieval strategies and protein databases at inference time
-- offering a scalable alternative to alignment-centric approaches.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [251] [Flow-Lenia: Emergent evolutionary dynamics in mass conservative continuous cellular automata](https://arxiv.org/abs/2506.08569)
*Erwan Plantec,Gautier Hamon,Mayalen Etcheverry,Bert Wang-Chak Chan,Pierre-Yves Oudeyer,Clément Moulin-Frier*

Main category: nlin.CG

TL;DR: The paper introduces Flow-Lenia, a mass conservative extension of Lenia (a continuous cellular automaton), which can generate complex life-like patterns and behaviors. Experiments show its effectiveness in creating spatially-localized patterns with interesting behaviors, and the model parameters can be embedded within its own dynamics for multispecies simulations. The evolutionary activity framework and other metrics reveal emergent evolutionary dynamics.


<details>
  <summary>Details</summary>
Motivation: To create artificial systems capable of spontaneously generating properties found in living organisms such as autopoiesis, self-replication, evolution, and open-endedness, particularly focusing on extending Lenia to conserve mass and produce more complex life-like behaviors.

Method: Flow-Lenia is proposed as a mass conservative extension of Lenia. It involves experiments demonstrating the generation of spatially-localized patterns with complex behaviors, optimization of update rule parameters, embedding model parameters within its dynamics for multispecies simulations, and using the evolutionary activity framework and other metrics to study emergent evolutionary dynamics.

Result: Flow-Lenia effectively generates spatially-localized patterns with complex behaviors. The update rule parameters can be optimized to produce creatures showing behaviors of interest. The model allows embedding of parameters within its own dynamics for multispecies simulations and reveals emergent evolutionary dynamics through various metrics.

Conclusion: Flow-Lenia represents an advancement in creating artificial systems with lifelike properties by conserving mass, producing complex patterns and behaviors, enabling multispecies simulations, and revealing emergent evolutionary dynamics.

Abstract: Central to the artificial life endeavour is the creation of artificial
systems spontaneously generating properties found in the living world such as
autopoiesis, self-replication, evolution and open-endedness. While numerous
models and paradigms have been proposed, cellular automata (CA) have taken a
very important place in the field notably as they enable the study of
phenomenons like self-reproduction and autopoiesis. Continuous CA like Lenia
have been showed to produce life-like patterns reminiscent, on an aesthetic and
ontological point of view, of biological organisms we call creatures. We
propose in this paper Flow-Lenia, a mass conservative extension of Lenia. We
present experiments demonstrating its effectiveness in generating
spatially-localized patters (SLPs) with complex behaviors and show that the
update rule parameters can be optimized to generate complex creatures showing
behaviors of interest. Furthermore, we show that Flow-Lenia allows us to embed
the parameters of the model, defining the properties of the emerging patterns,
within its own dynamics thus allowing for multispecies simulations. By using
the evolutionary activity framework as well as other metrics, we shed light on
the emergent evolutionary dynamics taking place in this system.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [252] [Learning-Based Multiuser Scheduling in MIMO-OFDM Systems with Hybrid Beamforming](https://arxiv.org/abs/2506.08263)
*Pouya Agheli,Tugce Kobal,François Durand,Matthew Andrews*

Main category: cs.IT

TL;DR: In this paper, we investigate the multiuser scheduling problem in MIMO systems using OFDM and hybrid beamforming for mmWave channels. We aim to maximize proportional fairness by designing analog and digital precoders and selecting users based on the number of RF chains.


<details>
  <summary>Details</summary>
Motivation: Improved scheduling is critical for enhancing spectral efficiency and long-term performance from the perspective of proportional fairness (PF) metric in hybrid beamforming systems due to its limited multiplexing gain.

Method: Leveraging the characteristics of mmWave channels, a two-timescale protocol is applied. On a long timescale, an analog beam is assigned to each user. Scheduling the users and designing the digital precoder are done accordingly on a short timescale. Combinatorial solutions like greedy and sorting algorithms followed by a machine learning approach are proposed for scheduling.

Result: Numerical results show the trade-off between the performance and complexity of the proposed approaches.

Conclusion: The choice of approach depends on the specific criteria within a given scenario.

Abstract: We investigate the multiuser scheduling problem in multiple-input
multiple-output (MIMO) systems using orthogonal frequency division multiplexing
(OFDM) and hybrid beamforming in which a base station (BS) communicates with
multiple users over millimeter wave (mmWave) channels in the downlink. Improved
scheduling is critical for enhancing spectral efficiency and the long-term
performance of the system from the perspective of proportional fairness (PF)
metric in hybrid beamforming systems due to its limited multiplexing gain. Our
objective is to maximize PF by properly designing the analog and digital
precoders within the hybrid beamforming and selecting the users subject to the
number of radio frequency (RF) chains. Leveraging the characteristics of mmWave
channels, we apply a two-timescale protocol. On a long timescale, we assign an
analog beam to each user. Scheduling the users and designing the digital
precoder are done accordingly on a short timescale. To conduct scheduling, we
propose combinatorial solutions, such as greedy and sorting algorithms,
followed by a machine learning (ML) approach. Our numerical results highlight
the trade-off between the performance and complexity of the proposed
approaches. Consequently, we show that the choice of approach depends on the
specific criteria within a given scenario.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [253] [Solving excited states for long-range interacting trapped ions with neural networks](https://arxiv.org/abs/2506.08594)
*Yixuan Ma,Chang Liu,Weikang Li,Shun-Yao Zhang,L. -M. Duan,Yukai Wu,Dong-Ling Deng*

Main category: quant-ph

TL;DR: This paper presents a neural network-based algorithm, NQES, which can accurately and efficiently compute multiple low-lying excited states of quantum many-body spin systems without explicit orthogonalization. Demonstrated through various models, including Haldane-Shastry model and trapped-ion systems, the algorithm reveals spatial correlation patterns matching experimental observations and uncovers gap scaling and correlation features for large ion systems.


<details>
  <summary>Details</summary>
Motivation: The computation of excited states in strongly interacting quantum many-body systems is crucial but challenging due to the exponential growth of Hilbert space dimension with system size.

Method: A neural network-based algorithm (NQES) is introduced to simultaneously output multiple low-lying excited states of quantum many-body spin systems, without requiring explicit orthogonalization and applicable to higher dimensions.

Result: Through examples like the Haldane-Shastry model and trapped-ion systems, the NQES algorithm successfully computes multiple excited states and their observable expectations, revealing spatial correlation patterns and uncovering gap scaling and correlation features.

Conclusion: The NQES algorithm is established as a scalable and efficient tool for computing excited states in interacting quantum many-body systems, with potential applications in benchmarking quantum devices and processes like photoisomerization.

Abstract: The computation of excited states in strongly interacting quantum many-body
systems is of fundamental importance. Yet, it is notoriously challenging due to
the exponential scaling of the Hilbert space dimension with the system size.
Here, we introduce a neural network-based algorithm that can simultaneously
output multiple low-lying excited states of a quantum many-body spin system in
an accurate and efficient fashion. This algorithm, dubbed the neural quantum
excited-state (NQES) algorithm, requires no explicit orthogonalization of the
states and is generally applicable to higher dimensions. We demonstrate,
through concrete examples including the Haldane-Shastry model with all-to-all
interactions, that the NQES algorithm is capable of efficiently computing
multiple excited states and their related observable expectations. In addition,
we apply the NQES algorithm to two classes of long-range interacting
trapped-ion systems in a two-dimensional Wigner crystal. For non-decaying
all-to-all interactions with alternating signs, our computed low-lying excited
states bear spatial correlation patterns similar to those of the ground states,
which closely match recent experimental observations that the
quasi-adiabatically prepared state accurately reproduces analytical
ground-state correlations. For a system of up to 300 ions with power-law
decaying antiferromagnetic interactions, we successfully uncover its gap
scaling and correlation features. Our results establish a scalable and
efficient algorithm for computing excited states of interacting quantum
many-body systems, which holds potential applications ranging from benchmarking
quantum devices to photoisomerization.

</details>


### [254] [Systematic and Efficient Construction of Quadratic Unconstrained Binary Optimization Forms for High-order and Dense Interactions](https://arxiv.org/abs/2506.08448)
*Hyakka Nakada,Shu Tanaka*

Main category: quant-ph

TL;DR: Quantum Annealing (QA) can solve QUBO problems efficiently. To apply QA to higher-order problems in Machine Learning, this paper proposes modeling target functions with the sum of rectified linear unit bases. This method is proven effective both numerically and analytically. A new black-box optimization scheme combining QA and the proposed quadratization is designed for ML surrogate regressors.


<details>
  <summary>Details</summary>
Motivation: To broaden the applicability of Quantum Annealing to complex Machine Learning problems that involve strong nonlinearity and dense interactions, which cannot be addressed by conventional quadratization methods.

Method: Model target functions using the sum of rectified linear unit bases, which allows universal approximation and has an equivalent quadratic-polynomial representation. Verify the concept through numerical and analytical means. Combine Quantum Annealing with the proposed quadratization to create a new black-box optimization scheme for Machine Learning surrogate regressors.

Result: The proof of concept was verified both numerically and analytically, demonstrating the effectiveness of the proposed method. A new optimization scheme was successfully designed.

Conclusion: The proposed method of modeling target functions with rectified linear unit bases and combining it with Quantum Annealing provides a viable solution for complex Machine Learning problems, expanding the applicability of Quantum Annealing.

Abstract: Quantum Annealing (QA) can efficiently solve combinatorial optimization
problems whose objective functions are represented by Quadratic Unconstrained
Binary Optimization (QUBO) formulations. For broader applicability of QA,
quadratization methods are used to transform higher-order problems into QUBOs.
However, quadratization methods for complex problems involving Machine Learning
(ML) remain largely unknown. In these problems, strong nonlinearity and dense
interactions prevent conventional methods from being applied. Therefore, we
model target functions by the sum of rectified linear unit bases, which not
only have the ability of universal approximation, but also have an equivalent
quadratic-polynomial representation. In this study, the proof of concept is
verified both numerically and analytically. In addition, by combining QA with
the proposed quadratization, we design a new black-box optimization scheme, in
which ML surrogate regressors are inputted to QA after the quadratization
process.

</details>


### [255] [The interplay of robustness and generalization in quantum machine learning](https://arxiv.org/abs/2506.08455)
*Julian Berberich,Tobias Fellner,Christian Holm*

Main category: quant-ph

TL;DR: 本章探讨了变分量子模型中对抗鲁棒性和泛化能力之间的相互作用，提出了基于Lipschitz界的结果，强调了可训练数据编码策略的重要性，并通过时间序列分析展示了其实际应用。


<details>
  <summary>Details</summary>
Motivation: 尽管对抗鲁棒性和泛化能力在量子机器学习的近期文献中分别受到了大量关注，但它们之间的相互作用却很少被研究。

Method: 讨论了最近使用Lipschitz界量化鲁棒性和泛化性的结果，这些结果明确依赖于模型参数，从而引发了一种基于正则化的训练方法。

Result: 这种方法突显了可训练数据编码策略对于构建鲁棒且具有泛化能力的量子模型的重要性，并通过时间序列分析展示了理论结果的实际影响。

Conclusion: 可训练的数据编码策略对于提高变分量子模型的鲁棒性和泛化能力至关重要。

Abstract: While adversarial robustness and generalization have individually received
substantial attention in the recent literature on quantum machine learning,
their interplay is much less explored. In this chapter, we address this
interplay for variational quantum models, which were recently proposed as
function approximators in supervised learning. We discuss recent results
quantifying both robustness and generalization via Lipschitz bounds, which
explicitly depend on model parameters. Thus, they give rise to a
regularization-based training approach for robust and generalizable quantum
models, highlighting the importance of trainable data encoding strategies. The
practical implications of the theoretical results are demonstrated with an
application to time series analysis.

</details>


### [256] [Quantum Adiabatic Generation of Human-Like Passwords](https://arxiv.org/abs/2506.08917)
*Sascha Mücke,Raoul Heese,Thore Gerlach,David Biesner,Loong Kuan Lee,Nico Piatkowski*

Main category: quant-ph

TL;DR: This paper explores the use of adiabatic quantum computers for password generation, proposing novel approaches based on QUBO and UD-MIS problems. The results demonstrate that small samples of passwords generated on a quantum computer can contain human-like passwords.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate whether Quantum Computing (QC) has the potential to reduce the vast resource requirements for training and operating GenAI models, specifically focusing on the task of generating passwords that mimic real user behavior.

Method: The study focuses on different encodings of token strings and proposes novel approaches based on Quadratic Unconstrained Binary Optimization (QUBO) and Unit-Disk Maximum Independent Set (UD-MIS) problems. They estimate the token distribution from data and adiabatically prepare a quantum state from which they eventually sample the generated passwords via measurements.

Result: Relatively small samples of 128 passwords, generated on the QuEra Aquila 256-qubit neutral atom quantum computer, contain human-like passwords such as 'Tunas200992' or 'teedem28iglove'.

Conclusion: Adiabatic quantum computers show utility in generating human-like passwords using the proposed QUBO and UD-MIS methods.

Abstract: Generative Artificial Intelligence (GenAI) for Natural Language Processing
(NLP) is the predominant AI technology to date. An important perspective for
Quantum Computing (QC) is the question whether QC has the potential to reduce
the vast resource requirements for training and operating GenAI models. While
large-scale generative NLP tasks are currently out of reach for practical
quantum computers, the generation of short semantic structures such as
passwords is not. Generating passwords that mimic real user behavior has many
applications, for example to test an authentication system against realistic
threat models. Classical password generation via deep learning have recently
been investigated with significant progress in their ability to generate novel,
realistic password candidates. In the present work we investigate the utility
of adiabatic quantum computers for this task. More precisely, we study
different encodings of token strings and propose novel approaches based on the
Quadratic Unconstrained Binary Optimization (QUBO) and the Unit-Disk Maximum
Independent Set (UD-MIS) problems. Our approach allows us to estimate the token
distribution from data and adiabatically prepare a quantum state from which we
eventually sample the generated passwords via measurements. Our results show
that relatively small samples of 128 passwords, generated on the QuEra Aquila
256-qubit neutral atom quantum computer, contain human-like passwords such as
"Tunas200992" or "teedem28iglove".

</details>


### [257] [Superposed Parameterised Quantum Circuits](https://arxiv.org/abs/2506.08749)
*Viktoria Patapovich,Mo Kordzanganeh,Alexey Melnikov*

Main category: quant-ph

TL;DR: Quantum machine learning uses superposed parameterised quantum circuits for high-dimensional data analysis, which can overcome the limitations of linear unitary operations and shared trainable parameters across outputs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a more expressive and scalable quantum machine learning model that goes beyond the constraints of linear unitary operations and shared parameters.

Method: By combining flip-flop quantum random-access memory with repeat-until-success protocols, a superposed parameterised quantum circuit embeds an exponential number of parameterised sub-models in a single circuit and induces polynomial activation functions through amplitude transformations and post-selection.

Result: On a 1D step-function regression task, this method reduces mean-squared error by three orders of magnitude compared to a variational baseline. On a 2D star-shaped classification task, introducing a quadratic activation increases accuracy to 81.4% and reduces run-to-run variance three-fold.

Conclusion: Superposed parameterised quantum circuits provide a hardware-efficient approach towards deeper and more versatile quantum circuits that can learn complex decision boundaries.

Abstract: Quantum machine learning has shown promise for high-dimensional data
analysis, yet many existing approaches rely on linear unitary operations and
shared trainable parameters across outputs. These constraints limit
expressivity and scalability relative to the multi-layered, non-linear
architectures of classical deep networks. We introduce superposed parameterised
quantum circuits to overcome these limitations. By combining flip-flop quantum
random-access memory with repeat-until-success protocols, a superposed
parameterised quantum circuit embeds an exponential number of parameterised
sub-models in a single circuit and induces polynomial activation functions
through amplitude transformations and post-selection. We provide an analytic
description of the architecture, showing how multiple parameter sets are
trained in parallel while non-linear amplitude transformations broaden
representational power beyond conventional quantum kernels. Numerical
experiments underscore these advantages: on a 1D step-function regression a
two-qubit superposed parameterised quantum circuit cuts the mean-squared error
by three orders of magnitude versus a parameter-matched variational baseline;
on a 2D star-shaped two-dimensional classification task, introducing a
quadratic activation lifts accuracy to 81.4% and reduces run-to-run variance
three-fold. These results position superposed parameterised quantum circuits as
a hardware-efficient route toward deeper, more versatile parameterised quantum
circuits capable of learning complex decision boundaries.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [258] [A Systematic Literature Review on Continuous Integration and Deployment (CI/CD) for Secure Cloud Computing](https://arxiv.org/abs/2506.08055)
*Sabbir M. Saleh,Nazim Madhavji,John Steinbacher*

Main category: cs.SE

TL;DR: In this paper, the authors conduct a systematic literature review of 66 papers focused on cloud security in continuous software engineering, specifically CI/CD pipelines. They identify key tools (Harbor, SonarQube, GitHub Actions), challenges (image manipulation, unauthorized access, weak authentication), and research gaps in improving cloud-based security solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is the increasing importance of cybersecurity in cloud environments due to the rise in cyberattacks across various sectors. Ensuring secure app deployment in the cloud has become a significant challenge that requires substantial effort.

Method: The authors performed a systematic literature review (SLR) of 66 papers. They examined tools, approaches, and challenges related to securing CI/CD processes in cloud environments.

Result: The SLR identified several key tools used in cloud security such as Harbor, SonarQube, and GitHub Actions. It also highlighted challenges like image manipulation, unauthorized access, and weak authentication within CI/CD pipelines. Research gaps were uncovered regarding how current tools and practices address these security issues.

Conclusion: There is a need for further research to enhance cloud-based security solutions, particularly within CI/CD pipelines, to effectively counteract the identified security challenges.

Abstract: As cloud environments become widespread, cybersecurity has emerged as a top
priority across areas such as networks, communication, data privacy, response
times, and availability. Various sectors, including industries, healthcare, and
government, have recently faced cyberattacks targeting their computing systems.
Ensuring secure app deployment in cloud environments requires substantial
effort. With the growing interest in cloud security, conducting a systematic
literature review (SLR) is critical to identifying research gaps. Continuous
Software Engineering, which includes continuous integration (CI), delivery
(CDE), and deployment (CD), is essential for software development and
deployment. In our SLR, we reviewed 66 papers, summarising tools, approaches,
and challenges related to the security of CI/CD in the cloud. We addressed key
aspects of cloud security and CI/CD and reported on tools such as Harbor,
SonarQube, and GitHub Actions. Challenges such as image manipulation,
unauthorised access, and weak authentication were highlighted. The review also
uncovered research gaps in how tools and practices address these security
issues in CI/CD pipelines, revealing a need for further study to improve
cloud-based security solutions.

</details>


### [259] [A Metrics-Oriented Architectural Model to Characterize Complexity on Machine Learning-Enabled Systems](https://arxiv.org/abs/2506.08153)
*Renato Cordeiro Ferreira*

Main category: cs.SE

TL;DR: The paper explores how to manage the complexity of ML-enabled systems by introducing a metrics-based architectural model, starting with an extension of a reference architecture for collecting metrics.


<details>
  <summary>Details</summary>
Motivation: To investigate how complexity affects ML-Enabled Systems and manage their complexity effectively.

Method: Introducing a metrics-based architectural model to characterize the complexity of ML-Enabled Systems, beginning with extending a reference architecture to collect metrics.

Result: Showcased the first step in creating the metrics-based architectural model, which is the extension of a reference architecture for collecting metrics on ML-Enabled Systems.

Conclusion: This research aims to support architectural decisions and provide guidelines for the development and growth of ML-Enabled Systems through a metrics-based architectural model.

Abstract: How can the complexity of ML-enabled systems be managed effectively? The goal
of this research is to investigate how complexity affects ML-Enabled Systems
(MLES). To address this question, this research aims to introduce a
metrics-based architectural model to characterize the complexity of MLES. The
goal is to support architectural decisions, providing a guideline for the
inception and growth of these systems. This paper showcases the first step for
creating the metrics-based architectural model: an extension of a reference
architecture that can describe MLES to collect their metrics.

</details>


### [260] [Worst-Case Symbolic Constraints Analysis and Generalisation with Large Language Models](https://arxiv.org/abs/2506.08171)
*Daniel Koh,Yannic Noller,Corina S. Pasareanu,Adrians Skapars,Youcheng Sun*

Main category: cs.SE

TL;DR: 大型语言模型（LLMs）在代码任务中表现出色，但在复杂符号推理任务中的潜力尚未充分开发。本文研究了LLMs通过符号约束分析对程序最坏情况执行进行推理的能力，并通过SMT约束求解和特定设计的数据集进一步提升其能力。实验结果表明，改进后的模型WARP-1.0-3B在该任务上显著优于其他基线模型，证明了LLMs可以深入参与符号推理，为神经网络学习与形式化方法的结合提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多种编码任务中表现出色，但其在更复杂的符号推理任务中的应用仍较少探索。因此，研究者希望评估LLMs是否能够处理涉及程序最坏情况执行的符号约束分析任务，并通过改进模型增强其符号推理能力。

Method: 定义了一个新的任务——最坏情况符号约束分析，用于评估LLMs的符号推理能力；使用SMT约束求解技术，并基于专门设计的符号约束数据集，对LLMs进行了符号推理引导的微调；开发了名为WARP-1.0-3B的新模型，通过强化学习方法使其能够恢复算法的最坏情况行为对应的约束条件。

Result: 经过微调的WARP-1.0-3B模型在最坏情况符号约束分析任务中表现优异，超越了规模相当甚至更大的基线模型，成功恢复了与算法最坏情况行为相关的约束条件。这表明LLMs可以通过适当的训练和优化实现更深层次的符号推理。

Conclusion: 研究表明，LLMs具备较强的符号推理能力，可以在程序分析中结合形式化方法和神经网络学习，为严谨的程序分析提供支持。这一发现为未来将深度学习与形式化验证技术相结合的研究奠定了基础。

Abstract: Large language models (LLMs) have been successfully applied to a variety of
coding tasks, including code generation, completion, and repair. However, more
complex symbolic reasoning tasks remain largely unexplored by LLMs. This paper
investigates the capacity of LLMs to reason about worst-case executions in
programs through symbolic constraints analysis, aiming to connect LLMs and
symbolic reasoning approaches. Specifically, we define and address the problem
of worst-case symbolic constraints analysis as a measure to assess the
comprehension of LLMs. We evaluate the performance of existing LLMs on this
novel task and further improve their capabilities through symbolic
reasoning-guided fine-tuning, grounded in SMT (Satisfiability Modulo Theories)
constraint solving and supported by a specially designed dataset of symbolic
constraints. Experimental results show that our solver-aligned model,
WARP-1.0-3B, consistently surpasses size-matched and even much larger
baselines, demonstrating that a 3B LLM can recover the very constraints that
pin down an algorithm's worst-case behaviour through reinforcement learning
methods. These findings suggest that LLMs are capable of engaging in deeper
symbolic reasoning, supporting a closer integration between neural
network-based learning and formal methods for rigorous program analysis.

</details>


### [261] [Repeton: Structured Bug Repair with ReAct-Guided Patch-and-Test Cycles](https://arxiv.org/abs/2506.08173)
*Nguyen Phu Vinh,Anh Chung Hoang,Chris Ngo,Truong-Son Hy*

Main category: cs.SE

TL;DR: Repeton is an open-source framework that uses LLMs for precise and automated code manipulation in Git repositories through a structured patch-and-test pipeline, showing good performance in patch validity and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the low precision and limited interpretability of LLMs in complex software engineering tasks.

Method: Repeton operates via a structured patch-and-test pipeline: it iteratively diagnoses issues, proposes code changes, and validates each patch through automated testing, guided by lightweight heuristics and development tools.

Result: Evaluated on the SWE-bench Lite benchmark, Repeton shows good performance compared to RAG-based methods in both patch validity and interpretability.

Conclusion: Repeton provides a practical path toward scalable and transparent autonomous debugging by decomposing software engineering tasks into modular, verifiable stages.

Abstract: Large Language Models (LLMs) have shown strong capabilities in code
generation and comprehension, yet their application to complex software
engineering tasks often suffers from low precision and limited
interpretability. We present Repeton, a fully open-source framework that
leverages LLMs for precise and automated code manipulation in real-world Git
repositories. Rather than generating holistic fixes, Repeton operates through a
structured patch-and-test pipeline: it iteratively diagnoses issues, proposes
code changes, and validates each patch through automated testing. This stepwise
process is guided by lightweight heuristics and development tools, avoiding
reliance on embedding-based retrieval systems. Evaluated on the SWE-bench Lite
benchmark, our method shows good performance compared to RAG-based methods in
both patch validity and interpretability. By decomposing software engineering
tasks into modular, verifiable stages, Repeton provides a practical path toward
scalable and transparent autonomous debugging.

</details>


### [262] [Understanding Software Engineering Agents Through the Lens of Traceability: An Empirical Study](https://arxiv.org/abs/2506.08311)
*Ira Ceka,Saurabh Pujar,Shyam Ramji,Luca Buratti,Gail Kaiser,Baishakhi Ray*

Main category: cs.SE

TL;DR: The paper systematically studies SWE agent behavior via execution traces, proposes a taxonomy of decision-making pathways, identifies core components for agent success, examines the impact of test generation on patch production, and conducts a large-scale code clone analysis.


<details>
  <summary>Details</summary>
Motivation: To deepen the understanding of internal decision-making workflows in SWE agents to improve their reliability and efficiency.

Method: Propose a taxonomy of decision-making pathways for five representative agents, identify core components for agent success, study the impact of test generation on patch production, and conduct a large-scale code clone analysis.

Result: Provides insights into successful bug localization, patch generation, and reproduction test generation, as well as strategies leading to successful test generation. Reveals structural and stylistic differences between agent-generated and developer-written patches.

Conclusion: The findings offer novel insights into agent design and open new avenues for building more effective and human-aligned SWE agents.

Abstract: With the advent of large language models (LLMs), software engineering agents
(SWE agents) have emerged as a powerful paradigm for automating a range of
software tasks -- from code generation and repair to test case synthesis. These
agents operate autonomously by interpreting user input and responding to
environmental feedback. While various agent architectures have demonstrated
strong empirical performance, the internal decision-making worfklows that drive
their behavior remain poorly understood. Deeper insight into these workflows
hold promise for improving both agent reliability and efficiency. In this work,
we present the first systematic study of SWE agent behavior through the lens of
execution traces. Our contributions are as follows: (1) we propose the first
taxonomy of decision-making pathways across five representative agents; (2)
using this taxonomy, we identify three core components essential to agent
success -- bug localization, patch generation, and reproduction test generation
-- and study each in depth; (3) we study the impact of test generation on
successful patch production; and analyze strategies that can lead to successful
test generation; (4) we further conduct the first large-scale code clone
analysis comparing agent-generated and developer-written patches and provide a
qualitative study revealing structural and stylistic differences in patch
content. Together, these findings offer novel insights into agent design and
open avenues for building agents that are both more effective and more aligned
with human development practices.

</details>


### [263] [Do Generative AI Tools Ensure Green Code? An Investigative Study](https://arxiv.org/abs/2506.08790)
*Samarth Sikand,Rohit Mehra,Vibhu Saujanya Sharma,Vikrant Kaulgud,Sanjay Podder,Adam P. Burden*

Main category: cs.SE

TL;DR: This paper explores the sustainability of AI-generated code from three popular tools, finding them generally non-green and in need of improvement.


<details>
  <summary>Details</summary>
Motivation: To understand how environmentally friendly AI-generated code is based on its adoption of sustainable coding practices.

Method: Investigation into the sustainability aspects of AI-generated code across ChatGPT, BARD, and Copilot.

Result: These generative AI tools exhibit default non-green behavior in producing code across multiple rules and scenarios.

Conclusion: There is a need for further investigations and remediation strategies to enhance the sustainability of AI-generated code.

Abstract: Software sustainability is emerging as a primary concern, aiming to optimize
resource utilization, minimize environmental impact, and promote a greener,
more resilient digital ecosystem. The sustainability or "greenness" of software
is typically determined by the adoption of sustainable coding practices. With a
maturing ecosystem around generative AI, many software developers now rely on
these tools to generate code using natural language prompts. Despite their
potential advantages, there is a significant lack of studies on the
sustainability aspects of AI-generated code. Specifically, how environmentally
friendly is the AI-generated code based upon its adoption of sustainable coding
practices? In this paper, we present the results of an early investigation into
the sustainability aspects of AI-generated code across three popular generative
AI tools - ChatGPT, BARD, and Copilot. The results highlight the default
non-green behavior of tools for generating code, across multiple rules and
scenarios. It underscores the need for further in-depth investigations and
effective remediation strategies.

</details>


### [264] [On The Impact of Merge Request Deviations on Code Review Practices](https://arxiv.org/abs/2506.08860)
*Samah Kansab,Francis Bordeleau,Ali Tizghadam*

Main category: cs.SE

TL;DR: In code review, many Merge Requests (MRs) deviate from standard review processes. These deviations, found in 37.02% of MRs and categorized into seven types, can bias analytics and ML models for review analysis. The authors propose a few-shot learning detection method with 91% accuracy to identify these deviations, which improves ML model performance on predicting review completion time by up to 2.25 times.


<details>
  <summary>Details</summary>
Motivation: Code reviews are essential in software engineering, but many MRs in industrial workflows do not follow standardized review processes. Ignoring these deviations may bias analytics and undermine the reliability of ML models used for review analysis.

Method: The researchers identified seven categories of deviations that occur in MRs and proposed a few-shot learning method to detect them with 91% accuracy. By excluding these deviations, they retrained ML models to predict review completion time and analyzed changes in model performance and feature importance.

Result: Excluding deviations led to improved ML model performance in 53.33% of cases, with some models performing up to 2.25 times better. There were also significant shifts in feature importance, indicating the impact of deviations on model behavior.

Conclusion: This work contributes a taxonomy of MR deviations, an AI-driven detection approach, and empirical evidence of their effects on ML-based review analytics. It helps practitioners optimize code review processes and ensures more reliable insights.

Abstract: Code review is a key practice in software engineering, ensuring quality and
collaboration. However, industrial Merge Request (MR) workflows often deviate
from standardized review processes, with many MRs serving non-review purposes
(e.g., drafts, rebases, or dependency updates). We term these cases deviations
and hypothesize that ignoring them biases analytics and undermines ML models
for review analysis.
  We identify seven deviation categories, occurring in 37.02% of MRs, and
propose a few-shot learning detection method (91% accuracy). By excluding
deviations, ML models predicting review completion time improve performance in
53.33% of cases (up to 2.25x) and exhibit significant shifts in feature
importance (47% overall, 60% top-*k*).
  Our contributions include: (1) a taxonomy of MR deviations, (2) an AI-driven
detection approach, and (3) empirical evidence of their impact on ML-based
review analytics. This work aids practitioners in optimizing review efforts and
ensuring reliable insights.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [265] [Exp4Fuse: A Rank Fusion Framework for Enhanced Sparse Retrieval using Large Language Model-based Query Expansion](https://arxiv.org/abs/2506.04760)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.IR

TL;DR: Exp4Fuse是一种新的融合排名框架，通过零样本LLM查询扩展提升稀疏检索性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在生成用于查询扩展的假设文档方面显示出潜力，但其效果高度依赖于生成文档的质量，并且需要复杂的提示策略和高级密集检索技术，这既昂贵又计算密集。为了解决这些问题，探索了零样本LLM查询扩展方法以改善稀疏检索，特别是对于学习型稀疏检索器。

Method: 引入了名为Exp4Fuse的新颖融合排名框架。该框架通过间接应用零样本LLM查询扩展来增强稀疏检索器的性能。具体来说，Exp4Fuse同时考虑两个检索路径：一个基于原始查询，另一个基于LLM增强查询。然后使用稀疏检索器生成两个排序列表，并使用修改后的互惠等级融合方法将它们融合。

Result: 实验结果表明，Exp4Fuse不仅超越了现有的基于LLM的查询扩展方法以增强稀疏检索器，而且当与先进的稀疏检索器结合时，在多个基准测试中取得了最先进（SOTA）的结果。

Conclusion: Exp4Fuse展示了卓越的性能和有效性，能够显著改善稀疏检索中的查询扩展。

Abstract: Large Language Models (LLMs) have shown potential in generating hypothetical
documents for query expansion, thereby enhancing information retrieval
performance. However, the efficacy of this method is highly dependent on the
quality of the generated documents, which often requires complex prompt
strategies and the integration of advanced dense retrieval techniques. This can
be both costly and computationally intensive. To mitigate these limitations, we
explore the use of zero-shot LLM-based query expansion to improve sparse
retrieval, particularly for learned sparse retrievers. We introduce a novel
fusion ranking framework, Exp4Fuse, which enhances the performance of sparse
retrievers through an indirect application of zero-shot LLM-based query
expansion. Exp4Fuse operates by simultaneously considering two retrieval
routes-one based on the original query and the other on the LLM-augmented
query. It then generates two ranked lists using a sparse retriever and fuses
them using a modified reciprocal rank fusion method. We conduct extensive
evaluations of Exp4Fuse against leading LLM-based query expansion methods and
advanced retrieval techniques on three MS MARCO-related datasets and seven
low-resource datasets. Experimental results reveal that Exp4Fuse not only
surpasses existing LLM-based query expansion methods in enhancing sparse
retrievers but also, when combined with advanced sparse retrievers, achieves
SOTA results on several benchmarks. This highlights the superior performance
and effectiveness of Exp4Fuse in improving query expansion for sparse
retrieval.

</details>


### [266] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: The paper introduces Hierarchical Lexical Graph (HLG) and two retrievers, StatementGraphRAG and TopicGraphRAG, to enhance Retrieval-Augmented Generation (RAG) systems. It also presents a synthetic dataset generation pipeline for evaluating multi-hop summarization systems.


<details>
  <summary>Details</summary>
Motivation: Current RAG models struggle with answers that require information from semantically distant documents. To address this, the authors propose HLG as a three-tier index system.

Method: 1. Developed HLG which traces propositions to sources, clusters them into topics, and links entities.
2. Built two retrievers: StatementGraphRAG for high-precision questions and TopicGraphRAG for exploratory queries.
3. Created a synthetic dataset generation pipeline for robust evaluation of multi-hop retrieval systems.

Result: Extensive experiments show an average relative improvement of 23.1% in retrieval recall and correctness across five datasets compared to naive chunk-based RAG.

Conclusion: The proposed methods outperform existing RAG approaches, and an open-source library is available.

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


### [267] [Bridging RDF Knowledge Graphs with Graph Neural Networks for Semantically-Rich Recommender Systems](https://arxiv.org/abs/2506.08743)
*Michael Färber,David Lamprecht,Yuni Susanti*

Main category: cs.IR

TL;DR: Graph Neural Networks (GNNs) have advanced recommender systems, but the semantic information of RDF knowledge graphs hasn't been fully utilized. This paper proposes integrating RDF KGs with GNNs, demonstrating that using RDF's rich semantics enhances recommendation performance.


<details>
  <summary>Details</summary>
Motivation: To leverage the rich semantic information from over a thousand RDF knowledge graphs in GNN-based recommender systems.

Method: Comprehensive integration of RDF KGs with GNNs using both topological and content information from RDF properties. In-depth evaluation of various GNNs considering different semantic feature initializations and graph structure heterogeneity.

Result: Experiments with multi-million-node RDF graphs show significant improvement in recommender systems by utilizing the semantic richness of RDF KGs.

Conclusion: The approach lays the foundation for future GNN-based recommender systems for the Linked Open Data cloud.

Abstract: Graph Neural Networks (GNNs) have substantially advanced the field of
recommender systems. However, despite the creation of more than a thousand
knowledge graphs (KGs) under the W3C standard RDF, their rich semantic
information has not yet been fully leveraged in GNN-based recommender systems.
To address this gap, we propose a comprehensive integration of RDF KGs with
GNNs that utilizes both the topological information from RDF object properties
and the content information from RDF datatype properties. Our main focus is an
in-depth evaluation of various GNNs, analyzing how different semantic feature
initializations and types of graph structure heterogeneity influence their
performance in recommendation tasks. Through experiments across multiple
recommendation scenarios involving multi-million-node RDF graphs, we
demonstrate that harnessing the semantic richness of RDF KGs significantly
improves recommender systems and lays the groundwork for GNN-based recommender
systems for the Linked Open Data cloud. The code and data are available on our
GitHub repository: https://github.com/davidlamprecht/rdf-gnn-recommendation

</details>


### [268] [Multimodal Representation Alignment for Cross-modal Information Retrieval](https://arxiv.org/abs/2506.08774)
*Fan Xu,Luis A. Leiva*

Main category: cs.IR

TL;DR: This paper explores the alignment of visual and textual embeddings using various similarity metrics for multimodal retrieval. It finds that cosine similarity performs best and highlights challenges in capturing complex interactions between modalities.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal retrieval by understanding how to effectively align representations from different modalities, such as images and text.

Method: Investigating geometric relationships between embeddings from vision-language models and unimodal models, then aligning them using six similarity metrics (four standard and two learned via neural networks).

Result: Wasserstein distance is useful for measuring modality gaps, cosine similarity outperforms other metrics in feature alignment tasks, and simple architectures like multilayer perceptrons are insufficient for complex interactions.

Conclusion: The study provides insights and practical advice for multimodal information retrieval, especially for real-world cross-modal applications.

Abstract: Different machine learning models can represent the same underlying concept
in different ways. This variability is particularly valuable for in-the-wild
multimodal retrieval, where the objective is to identify the corresponding
representation in one modality given another modality as input. This challenge
can be effectively framed as a feature alignment problem. For example, given a
sentence encoded by a language model, retrieve the most semantically aligned
image based on features produced by an image encoder, or vice versa. In this
work, we first investigate the geometric relationships between visual and
textual embeddings derived from both vision-language models and combined
unimodal models. We then align these representations using four standard
similarity metrics as well as two learned ones, implemented via neural
networks. Our findings indicate that the Wasserstein distance can serve as an
informative measure of the modality gap, while cosine similarity consistently
outperforms alternative metrics in feature alignment tasks. Furthermore, we
observe that conventional architectures such as multilayer perceptrons are
insufficient for capturing the complex interactions between image and text
representations. Our study offers novel insights and practical considerations
for researchers working in multimodal information retrieval, particularly in
real-world, cross-modal applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [269] [MOSAIC-F: A Framework for Enhancing Students' Oral Presentation Skills through Personalized Feedback](https://arxiv.org/abs/2506.08634)
*Alvaro Becerra,Daniel Andres,Pablo Villegas,Roberto Daza,Ruth Cobos*

Main category: cs.HC

TL;DR: The paper introduces MOSAIC-F, a multimodal feedback framework that combines human-based and data-driven evaluations to generate personalized feedback on student learning activities. Tested for improving oral presentation skills, it integrates assessments, multimodal data collection, AI analysis, and self-assessment.


<details>
  <summary>Details</summary>
Motivation: To address the need for more accurate, personalized, and actionable feedback in student learning activities by integrating human-based evaluations with advanced data-driven techniques such as AI and multimodal analytics.

Method: MOSAIC-F consists of four steps: standardized assessments by peers and professors, multimodal data collection during learning activities (video, audio, gaze, physiological signals, etc.), AI-generated personalized feedback combining human evaluations and multimodal insights, and student self-assessment involving video review and comparison with external evaluations.

Result: The framework was tested for improving oral presentation skills, demonstrating its potential to enhance feedback accuracy and personalization by effectively integrating diverse evaluation methods.

Conclusion: MOSAIC-F successfully merges human-based and data-driven approaches to provide richer, more actionable feedback for students, setting a promising direction for future educational feedback systems.

Abstract: In this article, we present a novel multimodal feedback framework called
MOSAIC-F, an acronym for a data-driven Framework that integrates Multimodal
Learning Analytics (MMLA), Observations, Sensors, Artificial Intelligence (AI),
and Collaborative assessments for generating personalized feedback on student
learning activities. This framework consists of four key steps. First, peers
and professors' assessments are conducted through standardized rubrics (that
include both quantitative and qualitative evaluations). Second, multimodal data
are collected during learning activities, including video recordings, audio
capture, gaze tracking, physiological signals (heart rate, motion data), and
behavioral interactions. Third, personalized feedback is generated using AI,
synthesizing human-based evaluations and data-based multimodal insights such as
posture, speech patterns, stress levels, and cognitive load, among others.
Finally, students review their own performance through video recordings and
engage in self-assessment and feedback visualization, comparing their own
evaluations with peers and professors' assessments, class averages, and
AI-generated recommendations. By combining human-based and data-based
evaluation techniques, this framework enables more accurate, personalized and
actionable feedback. We tested MOSAIC-F in the context of improving oral
presentation skills.

</details>


### [270] [Stop Misusing t-SNE and UMAP for Visual Analytics](https://arxiv.org/abs/2506.08725)
*Hyeon Jeon,Jeongin Park,Sungbok Shin,Jinwook Seo*

Main category: cs.HC

TL;DR: Misuses of t-SNE and UMAP in visual analytics are prevalent because of limited discourse on their appropriate use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate why misuses of t-SNE and UMAP occur and how to prevent them as these misuses are increasingly common.

Method: Conduct a literature review of 114 papers to verify the prevalence of misuse and analyze the reasonings, then execute an interview study to uncover practitioners' implicit motivations.

Result: Findings indicate that misuse primarily stems from limited discourse on appropriate use in visual analytics.

Conclusion: Propose future directions and concrete action items to promote more reasonable use of DR.

Abstract: Misuses of t-SNE and UMAP in visual analytics have become increasingly
common. For example, although t-SNE and UMAP projections often do not
faithfully reflect true distances between clusters, practitioners frequently
use them to investigate inter-cluster relationships. In this paper, we bring
this issue to the surface and comprehensively investigate why such misuse
occurs and how to prevent it. We conduct a literature review of 114 papers to
verify the prevalence of the misuse and analyze the reasonings behind it. We
then execute an interview study to uncover practitioners' implicit motivations
for using these techniques -- rationales often undisclosed in the literature.
Our findings indicate that misuse of t-SNE and UMAP primarily stems from
limited discourse on their appropriate use in visual analytics. We conclude by
proposing future directions and concrete action items to promote more
reasonable use of DR.

</details>


### [271] [Implementing Keyword Spotting on the MCUX947 Microcontroller with Integrated NPU](https://arxiv.org/abs/2506.08911)
*Petar Jakuš,Hrvoje Džapo*

Main category: cs.HC

TL;DR: This paper presents a keyword spotting (KWS) system on NXP MCXN947 microcontroller with an integrated Neural Processing Unit (NPU), which enables real-time voice interaction. The system combines MFCC feature extraction with a CNN classifier, optimized using Quantization Aware Training. Experimental results demonstrate a 59x speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy with a model size of 30.58 KB.


<details>
  <summary>Details</summary>
Motivation: To enable real-time voice interaction on resource-constrained devices by implementing an efficient, low-power KWS system.

Method: The KWS system combines MFCC feature extraction with a CNN classifier and is optimized using Quantization Aware Training to reduce model size with minimal accuracy drop.

Result: A 59x speedup in inference time when leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy with a model size of 30.58 KB.

Conclusion: The implementation demonstrates the feasibility of efficient, low-power voice interfaces on embedded platforms.

Abstract: This paper presents a keyword spotting (KWS) system implemented on the NXP
MCXN947 microcontroller with an integrated Neural Processing Unit (NPU),
enabling real-time voice interaction on resource-constrained devices. The
system combines MFCC feature extraction with a CNN classifier, optimized using
Quantization Aware Training to reduce model size with minimal accuracy drop.
Experimental results demonstrate a 59x speedup in inference time when
leveraging the NPU compared to CPU-only execution, achieving 97.06% accuracy
with a model size of 30.58 KB, demonstrating the feasibility of efficient,
low-power voice interfaces on embedded platforms.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [272] [Werewolf: A Straightforward Game Framework with TTS for Improved User Engagement](https://arxiv.org/abs/2506.00160)
*Qihui Fan,Enfu Nan,Wenbo Li,Lei Lu,Pu Zhao,Yanzhi Wang*

Main category: cs.CL

TL;DR: The paper presents a new LLM-based Werewolf game system with tuned TTS models for better user engagement, arguing that with improving LLM reasoning, extra components may be unnecessary.


<details>
  <summary>Details</summary>
Motivation: To create a more engaging experience for human players in LLM-agent-based social deduction games like Werewolf by leveraging advancements in LLMs.

Method: Proposing a novel LLM-based Werewolf game system with tuned Text-to-Speech (TTS) models to improve compatibility with various LLM models and enhance user engagement.

Result: Argues that as LLM reasoning improves, additional components such as fine-tuning or advanced prompting engineering may become unnecessary for creating an engaging Werewolf game experience.

Conclusion: A straightforward LLM-based Werewolf game system with tuned TTS models can provide an enhanced and engaging experience without the need for extra components.

Abstract: The growing popularity of social deduction game systems for both business
applications and AI research has greatly benefited from the rapid advancements
in Large Language Models (LLMs), which now demonstrate stronger reasoning and
persuasion capabilities. Especially with the raise of DeepSeek R1 and V3
models, LLMs should enable a more engaging experience for human players in
LLM-agent-based social deduction games like Werewolf. Previous works either
fine-tuning, advanced prompting engineering, or additional experience pool to
achieve engaging text-format Werewolf game experience. We propose a novel yet
straightforward LLM-based Werewolf game system with tuned Text-to-Speech(TTS)
models designed for enhanced compatibility with various LLM models, and
improved user engagement. We argue with ever enhancing LLM reasoning, extra
components will be unnecessary in the case of Werewolf.

</details>


### [273] [Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework](https://arxiv.org/abs/2506.05695)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: The paper proposes a novel curriculum learning framework called POCL for Knowledge Distillation of large language models, which enhances stability and efficiency in training student models.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods for LLMs often fail to prevent significant shifts in the student model's distribution during training, leading to catastrophic forgetting, mode collapse, and training-inference mismatch.

Method: The framework comprises two core components: (1) a difficulty measurer that ranks and partitions training samples from easy to hard, and (2) a training scheduler that incrementally introduces these subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures.

Result: Extensive experiments in instruction-following settings demonstrate that POCL consistently improves the performance of distilled student models across various white-box KD methods and model families.

Conclusion: The findings highlight the effectiveness of sorted training samples in KD for LLMs and demonstrate how to structure training data within the KD process to enhance the stability and performance of distilled LLMs.

Abstract: Knowledge Distillation (KD) compresses large language models (LLMs) by
transferring the teacher model's capabilities to a smaller student model,
reducing inference cost and memory usage while maintaining performance.
However, existing KD methods for LLMs often fail to prevent significant shifts
in the student model's distribution during training, leading to issues such as
catastrophic forgetting, mode collapse, and training-inference mismatch. To
address these challenges, we propose a novel, plug-in curriculum learning
framework inspired by the strength training principle of "progressive overload"
(POCL), which can be seamlessly integrated into existing white-box KD
approaches with minimal computational overhead. The framework comprises two
core components: (1) a difficulty measurer that ranks and partitions training
samples from easy to hard, and (2) a training scheduler that incrementally
introduces these subsets into the distillation process at fixed intervals while
applying loss functions with progressively rising temperatures. By starting
with the easiest samples and progressively increasing the difficulty, the
approach enhances both the stability and efficiency of learning. Extensive
experiments in instruction-following settings demonstrate that POCL
consistently improves the performance of distilled student models across
various white-box KD methods and model families. Our findings highlight the
effectiveness of sorted training samples in KD for LLMs. More generally, our
work demonstrates how to structure training data within the KD process to
enhance the stability and performance of distilled LLMs.

</details>


### [274] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: The paper presents a trilingual dataset for hate speech detection in English, Urdu, and Spanish, using advanced models like GPT-3.5 Turbo and Qwen 2.5 72B. It achieves significant performance improvements over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on hate speech detection in Urdu and improve multilingual hate speech detection overall, especially using translation-based approaches.

Method: Created a trilingual dataset with balanced labels, used attention layers before transformer/LLM models, applied TF-IDF for non-transformer models, and benchmarked with state-of-the-art models including GPT-3.5 Turbo and Qwen 2.5 72B.

Result: Achieved macro F1 scores of 0.87 for English, 0.85 for Spanish, 0.81 for Urdu, and 0.88 for the joint multilingual model, showing substantial improvements over baselines.

Conclusion: The proposed framework provides an effective solution for multilingual hate speech detection, promoting safer online environments.

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [275] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: 信息检索在大语言模型（LLMs）中与生成能力紧密相关。研究发现，随着干扰信息的积累，LLM的信息检索准确率会下降至零，提示存在工作记忆瓶颈。这需要改进模型抑制无关内容的能力。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在信息检索中的干扰效应，特别是语境内干扰对检索准确性的影响，以揭示其工作记忆容量限制。

Method: 采用认知科学中的 proactive interference (PI) 范式，引入 PI-LLM 评估方法，通过连续提供语义相关的键值更新并仅查询最终值来测试 LLM 的检索能力。

Result: 随着干扰信息的增加，LLM 的检索准确率以对数线性方式下降到零，错误主要来源于检索了已被覆盖的旧值，且通过提示工程减轻干扰的效果有限。

Conclusion: 大语言模型存在工作记忆瓶颈，需开发新方法增强模型在检索过程中抑制无关内容的能力。

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [276] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: Recent advancements in large language models and AI systems have caused a paradigm shift in the design and optimization of complex AI workflows. This paper reviews recent progress in optimizing compound AI systems, formalizes the concept, classifies methods, and highlights research challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenges arising from the increasing complexity of compound AI systems and their interactions, as well as to explore new approaches such as natural language feedback for optimizing non-differentiable systems.

Method: Provide a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques, formalize the notion of compound AI system optimization, classify existing methods along several key dimensions.

Result: A comprehensive review and classification of optimization methods for compound AI systems, highlighting open research challenges and future directions in this evolving field.

Conclusion: The field of compound AI system optimization is rapidly evolving with promising new approaches emerging, including natural language feedback.

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [277] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: Large language models (LLMs) have unexplored abilities in understanding complex scientific papers. This study introduces CLAIM-BENCH, a benchmark for evaluating LLMs' capabilities in claim-evidence extraction and validation across six diverse LLMs. Closed-source models like GPT-4 outperform open-source ones. Strategic prompting approaches improve LLMs' accuracy but increase computational cost.


<details>
  <summary>Details</summary>
Motivation: To explore and evaluate the ability of LLMs to understand and process complex relationships within research papers, specifically focusing on the logical links between claims and supporting evidence.

Method: The study systematically compares three divide and conquer inspired approaches across six diverse LLMs using CLAIM-BENCH, a comprehensive benchmark for scientific claim-evidence extraction and validation. Evaluation involves over 300 claim-evidence pairs across multiple research domains.

Result: Closed-source models such as GPT-4 and Claude outperform open-source counterparts in precision and recall. Strategically designed prompting approaches significantly enhance LLMs' ability to link evidence with claims, albeit with increased computational cost.

Conclusion: CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, providing a diagnostic tool and suggesting ways to build systems with deeper, more reliable reasoning.

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [278] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: This paper explores inference making in reading comprehension, introduces a taxonomy of inference types, and evaluates the use of GPT-4o for generating diagnostic RC items. While GPT-4o produced mostly good-quality questions, it struggled with accurately matching targeted inference types. The authors suggest combining AI generation with human judgment for better assessments.


<details>
  <summary>Details</summary>
Motivation: To improve reading instruction and interventions for students by providing educators with more effective diagnostic tools.

Method: Introduced a taxonomy of inference types, analyzed an item bank, and conducted experiments using GPT-4o to generate bridging-inference RC items with and without chain-of-thought prompts.

Result: GPT-4o generated 93.8% good-quality questions but only 42.6% accurately matched the targeted inference type. High inter-rater agreements were achieved in evaluations.

Conclusion: Combining automatic item generation with human judgment is a promising approach for creating scalable, high-quality diagnostic RC assessments.

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [279] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: Large language models (LLMs) show great performance on medical benchmarks, but their capabilities across different cognitive levels are underexplored. This study proposes a multi-cognitive-level evaluation framework inspired by Bloom's Taxonomy for assessing LLMs in the medical domain.


<details>
  <summary>Details</summary>
Motivation: To explore the capabilities of LLMs across different cognitive levels in the medical domain.

Method: Propose a multi-cognitive-level evaluation framework inspired by Bloom's Taxonomy that integrates existing medical datasets and introduces tasks targeting three cognitive levels: preliminary knowledge grasp, comprehensive knowledge application, and scenario-based problem solving.

Result: State-of-the-art general and medical LLMs from six families were evaluated using this framework. Findings reveal a significant performance decline as cognitive complexity increases, with model size playing a more critical role in performance at higher cognitive levels.

Conclusion: The study highlights the need to enhance LLMs' medical capabilities at higher cognitive levels and provides insights for developing LLMs suited to real-world medical applications.

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [280] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: This paper argues that text embedding research should focus more on implicit semantics rather than just surface-level semantics to better capture real-world language complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is the observation that current text embedding models are predominantly focused on surface-level semantics, which leads to difficulties in handling tasks that require interpretive reasoning, speaker stance, or social meaning. It highlights a gap where even state-of-the-art models perform only marginally better than simple baselines on implicit semantics tasks.

Method: The authors propose a paradigm shift in text embedding research by prioritizing more diverse and linguistically grounded training data, designing benchmarks that evaluate deeper semantic understanding, and explicitly framing implicit meaning as a core modeling objective.

Result: A pilot study conducted by the authors shows that there is a significant gap in performance between current models and the requirements for understanding implicit semantics.

Conclusion: The paper concludes by calling for the text embedding research community to embrace implicit semantics as a central modeling goal to align embeddings with the complexity of real-world language.

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [281] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: 通过使用小型草稿模型更准确地预测token和KV对的重要性，提出了一种新的LLM推理近似框架SpecKV和SpecPC。实验表明，该方法在保持内存使用、延迟和吞吐量改进的同时，比现有基线方法具有更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 优化长上下文大语言模型（LLMs）的推理至关重要，因为Transformer的计算复杂度为二次方，而内存复杂度为线性。现有的近似方法通常依赖于对token或KV对重要性的粗略预测，这限制了它们的有效性。

Method: 提出了一个利用小型草稿模型来更准确预测token和KV对重要性的新框架。具体来说，引入了两个实例：SpecKV（利用草稿输出评估每个KV对的重要性以进行更有效的KV缓存丢弃）和SpecPC（使用草稿模型的注意力激活来识别和丢弃不重要的提示token）。这是首次将草稿模型用于加速LLM推理。

Result: 广泛的实验证明，在长上下文基准测试中，所提出的方法在保持相同的内存使用、延迟和吞吐量改进的同时，始终比现有基线方法实现更高的准确性。

Conclusion: 本研究提出了一种新的框架，通过使用草稿模型提高LLM推理的准确性。SpecKV和SpecPC展示了显著的优势，且代码已开源。

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [282] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: The paper proposes TACTIC, a multi-agent framework for machine translation inspired by cognitive translation strategies. It consists of six agents handling different translation aspects and shows state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: To enhance machine translation quality by leveraging cognitive insights from human translators that existing multi-agent systems largely ignore.

Method: TACTIC is composed of six functionally distinct agents: drafting, refinement, evaluation, scoring, context reasoning, and external knowledge gathering. These agents simulate the interactive translation workflow observed in human translators.

Result: On benchmarks FLORES-200 and WMT24, TACTIC surpasses GPT-4.1 by +0.6 XCOMET and +1.18 COMETKIWI-23, and improves over DeepSeek-R1 by +0.84 XCOMET and +2.99 COMETKIWI-23.

Conclusion: TACTIC effectively utilizes the full capacity of LLMs for high-quality translation through cognitively informed multi-agent collaboration.

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [283] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: Adaptive-k retrieval is a single-pass method that adaptively selects the number of passages based on similarity scores, improving accuracy and efficiency in QA tasks with up to 10x fewer tokens.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed retrieval size in existing methods which either waste tokens or omit key evidence, especially in aggregation QA where context size varies.

Method: Adaptive-k retrieval uses the distribution of similarity scores between the query and candidate passages to select an optimal number of passages without needing model fine-tuning, extra LLM inferences, or changes to retriever-reader pipelines.

Result: Matches or outperforms fixed-k baselines on both factoid and aggregation QA benchmarks, uses up to 10x fewer tokens than full-context input while retrieving 70% of relevant passages, and improves accuracy across multiple LCLMs and embedding models.

Conclusion: Dynamically adjusting context size via Adaptive-k retrieval leads to more efficient and accurate QA systems.

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [284] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: The paper explores the shortcomings of current text-to-image model evaluations and proposes improvements.


<details>
  <summary>Details</summary>
Motivation: To enhance the evaluation frameworks for text-to-image models by identifying critical properties that are currently overlooked.

Method: Identifying key aspects of reliable evaluations, demonstrating failures of current frameworks across metrics and models, and proposing recommendations for improvement.

Result: Current mainstream evaluation frameworks fail to fully satisfy the identified properties, necessitating improvements.

Conclusion: Recommendations are provided to improve image-text alignment evaluation in text-to-image generation.

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [285] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: The first large-scale audit of instruction-tuned Small Language Models (SLMs) with 0.5 to 5 billion parameters reveals insights into the balance between competence, fairness, and ethical risks. Phi models achieve high F1 scores with minimal bias, Qwen 2.5 models show vacuous neutrality, LLaMA 3.2 models exhibit stereotypical bias, and compression through quantization introduces trade-offs in performance and bias.


<details>
  <summary>Details</summary>
Motivation: To understand the ethical risks associated with the rapid adoption of Small Language Models (SLMs) for on-device and resource-constrained deployments, particularly focusing on the 'middle tier' models that have been overlooked in previous studies.

Method: Audited nine open-source SLMs from Qwen 2.5, LLaMA 3.2, Gemma 3, and Phi families using the BBQ benchmark under zero-shot prompting to evaluate utility and fairness across ambiguous and disambiguated contexts.

Result: Phi models achieve high F1 scores with minimal bias; Qwen 2.5 models appear fair due to neutrality or random guessing; LLaMA 3.2 models exhibit strong stereotypical bias; 4-bit AWQ quantization improves F1 scores but increases disability-related bias in some models.

Conclusion: Provides practical guidance for responsibly deploying SLMs in applications demanding fairness and efficiency, especially beneficial for small enterprises and resource-constrained environments.

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [286] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: This paper introduces EtiCor++, a corpus of worldwide etiquettes, and proposes tasks and metrics to evaluate LLMs' understanding and bias regarding etiquettes.


<details>
  <summary>Details</summary>
Motivation: Cultural sensitivity in LLMs is crucial, especially concerning region-specific etiquettes, but there lacks sufficient resources for evaluating LLMs on this aspect.

Method: Introduced EtiCor++ as a resource, designed tasks for assessing LLMs' knowledge about global etiquettes, and proposed metrics to measure bias in LLMs.

Result: Experiments revealed inherent bias in LLMs towards certain regions.

Conclusion: EtiCor++ provides a valuable tool for improving cultural sensitivity in LLMs by evaluating their understanding and biases related to global etiquettes.

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [287] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: 在检索增强生成（RAG）中，提出了一种新的知识冲突类型分类法以及相应的模型行为，并引入了CONFLICTS基准，用于评估模型解决知识冲突的能力。尽管提示LLM显式推理检索到的文档中的潜在冲突可以显著提高其响应的质量和适当性，但未来的研究仍有很大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法虽然能够增强大型语言模型（LLM），但检索到的来源可能包含冲突的信息，而模型应该如何处理这些差异尚不明确。

Method: 1. 提出一种新的知识冲突类型分类法以及每种类型的期望模型行为。
2. 引入CONFLICTS基准，这是一个高质量的基准，具有专家对现实RAG设置中冲突类型的注释。
3. 在该基准上进行广泛的实验，以评估LLM解决知识冲突的能力。

Result: 实验结果表明，LLM经常难以适当地解决来源之间的冲突。然而，提示LLM显式推理检索到的文档中的潜在冲突可以显著提高其响应的质量和适当性。

Conclusion: 尽管通过提示LLM显式推理可以改善其解决冲突的能力，但未来的研究仍需努力进一步提高模型在这方面的表现。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [288] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: The paper introduces CoMuMDR, a new corpus for discourse parsing in conversations that is code-mixed in Hindi and English, multi-modal (audio and text), and multi-domain. Experiments with SoTA models show poor performance, indicating challenges in handling such complex data.


<details>
  <summary>Details</summary>
Motivation: Existing discourse parsing datasets are limited to written English dialogues within a single domain, failing to represent the complexity of real-world conversations.

Method: Created a new corpus called CoMuMDR which includes audio and transcribed text, annotated with nine discourse relations. Tested various state-of-the-art baseline models on this corpus.

Result: State-of-the-art models performed poorly on the CoMuMDR corpus, demonstrating the difficulties posed by multi-domain and code-mixed data.

Conclusion: There is a need for developing better models that can handle the complexities of multi-domain and code-mixed conversational data.

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [289] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: 提出了一种轻量级的后训练框架，通过对比推理反馈和残差嵌入细化策略改进潜在推理轨迹，提升模型准确性。实验表明在MathQA上提升了5%的准确率，且无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 当前链式思维提示方法存在令牌开销大和固定推理轨迹的问题，而潜在推理虽然改进了这些限制，但在后训练期间有效更新推理嵌入仍是一个挑战。

Method: 采用了两种新策略：1）对比推理反馈，通过与强弱基线比较推理嵌入来推断有效的更新方向；2）残差嵌入细化，通过逐步整合当前和历史梯度来稳定更新，实现快速且受控的收敛。

Result: 在五个推理基准上的广泛实验和案例研究证明了该框架的有效性，特别是在MathQA上实现了5%的准确率提升，且没有增加额外的训练。

Conclusion: 提出的轻量级后训练框架能够有效地优化潜在推理轨迹，提高模型的推理能力，且不需要额外的训练数据或计算资源。

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [290] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: TableDreamer is a progressive and weakness-guided data synthesis framework for table instruction tuning which enhances LLM's table understanding ability.


<details>
  <summary>Details</summary>
Motivation: Recent LLM-based data synthesis methods have limitations in generating table instruction tuning data, including insufficient data diversity and suboptimal data efficiency due to ignoring the weaknesses in table understanding.

Method: The method involves synthesizing diverse tables and related instructions as seed data, followed by an iterative exploration of the input space guided by newly identified weakness data. This serves as the final training data for fine-tuning the target LLM.

Result: Experiments on 10 tabular benchmarks show that TableDreamer boosts the average accuracy of Llama3.1-8B-instruct by 11.62% using 27K GPT-4o synthetic data, outperforming state-of-the-art baselines with more training data.

Conclusion: TableDreamer effectively addresses the limitations of current LLM-based data synthesis methods for table instruction tuning, improving both data diversity and efficiency.

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [291] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: The paper explores a generative relation extraction pipeline for studying interactions in the intestinal microbiome, using summarization with LLMs to refine context before extracting relations. Summarization improves performance by reducing noise but BERT-based methods still outperform generative models.


<details>
  <summary>Details</summary>
Motivation: To develop a method tailored for relation extraction in the complex and low-resource domain of the intestinal microbiome.

Method: Using a generative relation extraction pipeline that leverages summarization with large language models to refine context before extracting relations via instruction-tuned generation.

Result: Preliminary results show that summarization improves generative RE performance by reducing noise and guiding the model, although BERT-based RE approaches still outperform generative models.

Conclusion: This work shows the potential of generative methods to support the study of specialized domains in low-resource settings.

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [292] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: The paper introduces ConfPO, a method for preference learning in LLMs that optimizes preference-critical tokens based on training policy's confidence. It improves alignment quality and mitigates overoptimization without extra computational cost.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing Direct Alignment Algorithms (DAAs) that adjust all token probabilities uniformly regardless of their relevance to preference, leading to inefficiency and potential reward hacking.

Method: ConfPO identifies and optimizes preference-critical tokens solely based on the training policy's confidence, focusing optimization on the most impactful tokens using KL divergence budget more efficiently. It does not require any auxiliary models or compute.

Result: Experimental results on challenging alignment benchmarks show that ConfPO consistently outperforms uniform DAAs across various LLMs with zero additional computational overhead.

Conclusion: ConfPO is a simple, lightweight, and model-free method that improves alignment quality while mitigating overoptimization in LLMs.

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [293] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 尽管大型语言模型在许多自然语言处理任务中表现出色，但在处理包含表格和文本数据的金融文件的数值问题回答时仍然存在困难。本文探讨了在没有标准标签的情况下传统批评者代理的表现，并提出了一种改进的批评者代理和计算器代理，其表现优于之前的最先进方法（思维程序），并且更安全。此外，还研究了代理之间的相互作用及其对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理涉及表格和文本数据的金融文件数值问题时效果不佳，而先前的研究表明，在给定标准标签的情况下，批评者代理（即自我修正）对此任务非常有效。

Method: 1. 研究在无标准标签情况下传统批评者代理的效果。
2. 提出一种改进的批评者代理和计算器代理。
3. 分析代理之间的相互作用及其对性能的影响。

Result: 改进的批评者代理和计算器代理在数值问题回答任务上超越了先前最先进的方法（思维程序），并且表现更加稳定和安全。

Conclusion: 改进的批评者代理和计算器代理为数值问题回答提供了一个更有效的解决方案，特别是在没有标准标签的情况下。代理间的相互作用也对其性能产生了重要影响。

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [294] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: This study analyzes over 100,000 AI-related papers to examine how ethical values and societal concerns are integrated into technical AI research.


<details>
  <summary>Details</summary>
Motivation: To determine whether interdisciplinary research teams are leading the shift towards aligning AI development with ethical and societal values.

Method: Developed a classifier to identify societal content in AI-related papers published on ArXiv between 2014 and 2024.

Result: Interdisciplinary teams remain more likely to produce societally-oriented research, but computer science-only teams now account for a growing share of the field's overall societal output.

Conclusion: The findings challenge common assumptions about the drivers of societal AI and raise questions about the implications for AI safety and governance if most societally-oriented research is being undertaken by exclusively technical teams.

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [295] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: This study explores the application of in-context learning (ICL) to dialogue state tracking (DST), using a sentence embedding based k-nearest neighbour method for demonstration selection and testing on the MultiWoZ2.4 dataset with several LLMs.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness of in-context learning in dialogue state tracking and understand the factors influencing its performance.

Method: A sentence embedding based k-nearest neighbour method is used to retrieve suitable demonstrations for ICL, which are then structured within a template as input to LLMs such as OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and Llama3.2-3B-Instruct.

Result: The systematic study analyses the impact of factors related to demonstration selection and prompt context on DST performance, providing useful insights into the in-context learning abilities of LLMs.

Conclusion: In-context learning can be effectively applied to dialogue state tracking, with significant impacts from demonstration selection and prompt context.

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [296] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: 保险文件中交通事故信息的提取对量化保险公司成本至关重要。本文提出了一种两步法：首先通过两种方法分割文档以识别最相关的段落，然后利用大型语言模型进行实体提取。经过微调，LLaMA-2 7b 的幻觉问题显著减少。在开源模型中，微调后的LLaMA-2 70B表现最佳，准确率达79.4%，而GPT-4 Turbo则达到了86.1%的最高准确率。


<details>
  <summary>Details</summary>
Motivation: 从法律文件中提取有关交通意外的信息对于量化保险公司的成本至关重要，然而这一过程充满挑战，即使是对专家来说也是如此。因此，需要一种有效的方法来解决这个问题。

Method: 提出了一种两步法：首先，通过基于正则表达式的方法和将文档分为n个标记块并使用多语言模型进行语义搜索的方法来分割文档；其次，利用大型语言模型（如LLaMA-2、GPT-4 Turbo等）进行实体提取，并对LLaMA模型进行了LoRA微调。

Result: 与传统方法相比，基于段落向量化和后续使用LLMs的方法显著提高了性能。在开源模型中，微调后的LLaMA-2 70B准确率达到79.4%，超过基础版本的61.7%。值得注意的是，基础LLaMA-3 8B模型的表现已与微调后的LLaMA-2 70B相当，达到76.6%。同时，GPT-4 Turbo的准确率最高，达到86.1%。

Conclusion: 本文提出的方法在实体提取任务上表现出色，尤其在微调后大幅减少了LLaMA-2 7b的幻觉问题。结果表明，该方法显著优于传统方法，其中GPT-4 Turbo表现最佳，但开源模型也取得了令人瞩目的进步。

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [297] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) excel in various tasks but lack standardized evaluation for low-resource languages. This paper introduces mSTEB, a benchmark addressing this gap by evaluating LLMs on tasks like language identification, text classification, question answering, and translation across speech and text modalities. Evaluations of leading LLMs reveal significant performance disparities between high and low-resource languages, particularly in Africa and Americas/Oceania, indicating the need for more investment to address under-representation.


<details>
  <summary>Details</summary>
Motivation: To create a standardized evaluation benchmark for low-resource languages, which are currently underrepresented in the evaluation of Large Language Models (LLMs).

Method: Introduction of mSTEB, a new benchmark that evaluates LLMs on tasks including language identification, text classification, question answering, and translation across both speech and text modalities.

Result: Significant performance gaps exist between high-resource and low-resource languages, especially affecting languages spoken in Africa and Americas/Oceania.

Conclusion: More investment is required to improve the representation of low-resource languages in LLMs.

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [298] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 本研究探讨了不同数值精度和数据并行化策略对训练速度及模型准确性的影响，旨在为低资源环境下的领域适配提供支持。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的训练在能源、硬件和标注数据方面成本高昂，并且往往导致根植于主流文化与价值观的倾向性。尽管领域适配能更好地使模型与多元文化和价值背景相匹配，但其计算成本仍然是一个重大障碍，尤其对于缺乏大规模基础设施的研究团队而言。

Method: 评估使用不同数值精度和数据并行化策略对训练速度（作为能源和硬件消耗的代理）以及模型准确性的双重影响。

Result: 研究结果适用于任何以能源效率、可访问性或有限硬件可用性为核心关注点的场景。

Conclusion: 通过优化数值精度和数据并行化策略，可以有效提升训练效率，降低资源需求，从而推动低资源环境下领域的适配进程。

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [299] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: The paper proposes Olica, a pruning framework for LLMs that eliminates the need for retraining by using PCA and linear calibration method, achieving efficient compression without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Most structured pruning methods for LLMs require significant computational and data resources for retraining to restore disrupted correlations.

Method: Olica uses PCA to extract important information from unified matrix products in MHA layers and a linear calibration method involving SVD for low-rank reconstruction of residual errors in FFN layers.

Result: Olica reduces the complexity of PCA, mitigates error accumulation, and delivers superior performance across benchmarks while being efficient in data usage, GPU memory, and running time.

Conclusion: Olica is an efficient pruning framework for LLMs that eliminates the need for retraining.

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [300] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: The paper introduces PlantBert, a domain-specific language model for plant stress-response literature based on DeBERTa architecture. It is fine-tuned on a corpus focused on lentil responses to stressors and combines transformer-based modeling with rule-enhanced linguistic post-processing. The model shows strong generalization capabilities and provides a framework for entity recognition in agricultural NLP.


<details>
  <summary>Details</summary>
Motivation: There is a lack of domain-adapted tools in plant science despite advancements in transformer-based language models for biomedical and clinical natural language processing.

Method: PlantBert is built upon the DeBERTa architecture and fine-tuned on a curated corpus of expert-annotated abstracts focusing on lentil stress responses. The methodology integrates transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization.

Result: PlantBert demonstrates strong generalization capabilities across entity types and proves the feasibility of robust domain adaptation in low-resource scientific fields related to plant science.

Conclusion: PlantBert addresses a critical gap in agricultural NLP by providing a scalable and reproducible framework for high-resolution entity recognition, promoting transparency and accelerating innovation in computational plant science.

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [301] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: The paper introduces an approach using LLMs for automating semantic analysis of legal texts, transforming them into formal representations in Defeasible Deontic Logic (DDL). It proposes a pipeline that segments normative language, extracts deontic rules, and evaluates coherence. Evaluated across various LLM configurations, the results indicate promising alignment between machine-generated and expert-crafted formalizations.


<details>
  <summary>Details</summary>
Motivation: To automate the semantic analysis of legal texts and transform them into formal representations using Defeasible Deontic Logic (DDL) to improve scalability and accuracy in legal informatics.

Method: A structured pipeline is proposed which includes segmenting complex normative language into atomic snippets, extracting deontic rules, and evaluating these rules for syntactic and semantic coherence. This methodology is tested across different LLM configurations including prompt engineering strategies, fine-tuned models, and multi-stage pipelines.

Result: Empirical results show a promising alignment between machine-generated formalizations and those crafted by experts, especially when LLMs are effectively prompted.

Conclusion: LLMs can significantly contribute to scalable legal informatics, particularly when appropriate prompting techniques are applied.

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [302] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: The paper introduces PropMEND, a hypernetwork-based approach for knowledge propagation in large language models (LLMs), which enhances the ability to answer multi-hop questions using injected knowledge.


<details>
  <summary>Details</summary>
Motivation: Existing knowledge editing techniques for LLMs can inject knowledge but fail to propagate it effectively, meaning models struggle with reasoning tasks involving the injected knowledge.

Method: PropMEND uses a hypernetwork-based approach where gradients of a language modeling loss are modified through meta-learning. This method extends the meta-objective of MEND to transform gradient updates on knowledge, enabling answers to multi-hop questions that involve the injected knowledge.

Result: PropMEND shows nearly 2x accuracy improvement on challenging multi-hop questions in the RippleEdit dataset. It also performs well on a new dataset, Controlled RippleEdit, though the performance gap decreases for unseen entity-relation pairs.

Conclusion: PropMEND outperforms existing methods in knowledge propagation, yet there is room for improvement in generalizing to a wide range of relations.

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [303] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: 通过结合强化学习和内存优化技术，证明单个普通游戏GPU可以训练出强大的数学推理模型，挑战了需要大量基础设施的范式，促进了高性能AI研究的普及。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学推理等任务中表现出色，但其开发通常需要高昂的计算资源。尽管最近的进展降低了训练有能力模型的成本，但这些方法仍然依赖于高端硬件集群。

Method: 在RTX 3080 Ti（16GB内存）上，利用强化学习和内存优化技术，训练一个15亿参数的数学推理模型。

Result: 该模型在资源受限环境下，在数学推理基准测试中的表现与大几倍的模型相当或更好。

Conclusion: 单一平均游戏GPU可以训练出强大的数学推理模型，不必依赖高端硬件集群。

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [304] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: This paper explores the ability of large language models (LLMs) to manage common ground in political conversations, especially when they lack knowledge or encounter misinformation.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs handle conversational grounding and mutual understanding in situations where they may not have perfect knowledge, particularly in the political domain where misinformation is prevalent.

Method: The study investigates LLMs' responses to direct knowledge questions and loaded questions that presuppose misinformation. It evaluates if these questions lead LLMs to engage in active grounding and correct false user beliefs, considering their level of knowledge and political bias.

Result: LLMs face significant challenges in engaging in grounding and rejecting false user beliefs, indicating limitations in mitigating misinformation in political discourse.

Conclusion: There are concerns regarding the role of LLMs in reducing misinformation due to their struggles with conversational grounding and managing discrepancies in beliefs.

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [305] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 本文探讨了文本编码器在细粒度实体或事件识别上的局限性，并通过引入中文评估数据集CapRetrieval进行验证。尽管不同来源训练和模型大小的编码器都可能无法完成这些匹配任务，但通过微调编码器和提出的数据生成策略可以提升性能。此外，研究还揭示了嵌入表示在表达细粒度显著性与整体语义对齐之间的粒度困境。


<details>
  <summary>Details</summary>
Motivation: 观察到文本编码器的一个局限性：嵌入可能无法识别语义中的细粒度实体或事件，导致即使在简单情况下密集检索也可能失败。为了解决这个问题，需要一个专门的评估数据集来深入研究这种行为。

Method: 1. 构建了一个新的中文评估数据集CapRetrieval，其中段落是图像标题，查询是询问各种形式的实体或事件的短语。
2. 通过零样本评估测试不同来源训练和模型大小的编码器在细粒度匹配任务上的表现。
3. 提出了一种数据生成策略，并通过微调编码器来增强其性能。
4. 识别并讨论了嵌入表示中存在的粒度困境问题。

Result: 1. 零样本评估表明，现有的文本编码器在细粒度匹配任务上表现不佳。
2. 微调后的编码器在CapRetrieval数据集上达到了最佳性能。
3. 发现了粒度困境问题，即嵌入表示在表达细粒度显著性时难以与整体语义对齐。

Conclusion: 本文通过构建CapRetrieval数据集揭示了文本编码器在细粒度匹配任务上的局限性，并通过微调和数据生成策略提升了性能。同时提出了粒度困境问题，为进一步研究提供了方向。所有资源已公开发布。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [306] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: 通过使用新颖的SpeechMaturity数据集，研究提高了儿童语音识别任务的模型性能，超越了现有技术水平，并在多种环境下具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的儿童语音技术系统在处理下游任务时效果不佳，主要原因是训练语料库规模小以及儿童语音本身的复杂性。为了改善这一状况，本研究引入了一个全新的数据集SpeechMaturity，以应对基础分类任务：识别儿童发声。

Method: 研究使用了最先进的Transformer模型，并在一个名为SpeechMaturity的新数据集上进行训练。该数据集包含了生态效度极高的儿童发声样本，覆盖了美国、玻利维亚、瓦努阿图、巴布亚新几内亚、所罗门群岛和法国等地区，涉及25种以上语言的儿童。数据集中有242,004个标注的发声样本，远超以往的研究规模。模型被训练用于区分哭声、笑声、成熟语音（辅音+元音）和不成熟语音（仅辅音或元音）。

Result: 基于SpeechMaturity数据集训练的模型表现优于先前数据集上训练的最先进模型，其分类准确率与人类相当，并且在城市和农村环境中都表现出色。

Conclusion: SpeechMaturity数据集为儿童语音识别任务提供了显著改进，提升了模型性能并展示了跨环境的鲁棒性。这为未来儿童语音技术的发展奠定了坚实的基础。

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [307] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出了一种基于强化学习的多语言模型路由框架Router-R1，通过序列决策过程实现复杂任务中的模型分配与集成，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型路由器通常采用单轮一对一映射方式，无法充分利用多个语言模型的互补优势来解决复杂任务。

Method: Router-R1将多语言模型路由和聚合视为序列决策过程，利用强化学习进行训练；路由器本身被实例化为一个强大的语言模型，结合“思考”和“路由”动作，并整合每个响应到其上下文中；使用轻量级规则奖励指导学习，包括格式奖励、最终结果奖励和成本奖励。

Result: 在七个通用和多跳问答基准测试中，Router-R1优于多个强基线，在保持稳健泛化和成本管理的同时实现了优越性能。

Conclusion: Router-R1通过强化学习优化了性能-成本权衡，仅需简单模型描述符即可实现对未见模型选择的强大泛化能力。

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [308] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: 本文介绍了一种用于核应用的领域专用大型语言模型，该模型基于公开可获取的《Essential CANDU》教科书构建。采用紧凑的Transformer架构，在单个GPU上训练以保护核操作中的敏感数据。尽管数据集较小，但模型展示了捕捉专业核词汇的潜力，尽管生成的文本有时缺乏语法连贯性。专注于核内容的方法证明了符合严格网络安全和数据保密标准的内部LLM解决方案的可行性。早期的文本生成成功突显了模型在特定任务中的实用性，同时也揭示了需要更丰富的语料库、更复杂的预处理和指令微调来提高领域准确性。未来方向包括扩展数据集覆盖更多核子主题、改进分词以减少噪音以及系统评估模型在核领域的实际应用准备情况。


<details>
  <summary>Details</summary>
Motivation: 随着对网络安全和数据保密的要求日益严格，开发一种符合这些标准的领域专用大型语言模型对于核工业至关重要。现有的通用大型语言模型可能无法满足核应用的特殊需求，因此需要一种专注于核内容的解决方案。

Method: 使用来自《Essential CANDU》教科书的数据，构建了一个基于Transformer架构的领域专用大型语言模型。该模型在单个GPU上进行训练，以确保敏感数据的安全性。通过专注于核相关内容，模型试图捕捉专业的核词汇并生成相关的文本。

Result: 尽管数据集相对较小，模型在捕捉专业核词汇方面显示出积极的迹象。然而，生成的文本有时缺乏语法连贯性。初步结果表明，该方法可以实现符合网络安全和数据保密标准的内部LLM解决方案。

Conclusion: 虽然当前模型在语法连贯性和领域准确性方面还有提升空间，但它展示了在核应用中使用领域专用大型语言模型的潜力。未来工作将集中在扩展数据集、改进预处理和分词技术，以及进一步评估模型的实际应用准备情况。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [309] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: 在核电厂数据检索中，传统的自然语言到SQL（NL-to-SQL）方法存在风险和挑战。本文提出了一种基于函数调用的大规模语言模型的替代方案，通过预定义功能特定的函数来封装验证过的SQL逻辑，从而提高准确性和可维护性，同时降低风险。


<details>
  <summary>Details</summary>
Motivation: 核电厂数据检索要求极高的准确性和透明度，但传统的NL-to-SQL方法存在生成的SQL查询难以验证、以及复杂且结构不良的数据库带来的挑战，这些问题增加了不准确性并降低了对该方法的信任。

Method: 提出使用函数调用的大规模语言模型（LLMs），定义一组预批准的特定用途函数以代表常见使用案例。这些函数封装了经过验证的SQL逻辑，从而避免直接生成SQL查询，并确保专家在部署前对SQL查询进行审查和优化。此外，还探讨了如何利用NL-to-SQL工具协助生成初始函数代码。

Result: 研究表明，与直接NL-to-SQL生成相比，所提出的基于函数的方法提高了准确性和可维护性。

Conclusion: 本文强调了在关键系统中平衡用户易用性和操作安全的重要性，并提供了一个新的、可行的框架用于稳健的数据检索。

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [310] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: Adversarial threats against LLMs are increasing faster than defenses can adapt. The paper exposes a critical geometric blind spot in alignment and introduces ALKALI, an adversarial benchmark, and GRACE, an alignment framework to mitigate this vulnerability.


<details>
  <summary>Details</summary>
Motivation: The motivation is the escalating adversarial threats against LLMs that current defenses cannot adapt fast enough to counteract.

Method: ALKALI is introduced as an adversarial benchmark spanning 9,000 prompts across multiple categories and subtypes. GRACE, an alignment framework coupling preference learning with latent space regularization, enforces constraints for latent separation and adversarial cohesion.

Result: Evaluation of 21 leading LLMs revealed high Attack Success Rates (ASRs) exposing the vulnerability termed latent camouflage. GRACE achieved up to 39% ASR reduction. AVQI, a geometry aware metric, quantifies latent alignment failure.

Conclusion: The paper concludes by making the code publicly available and introducing methods to reshape internal geometry of models without modifying the base model.

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [311] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: Pretrained language models often make arithmetic errors due to unreliable embeddings for exact quantities. The existing probing methods are inadequate for the learned number embeddings with sinusoidal patterns. This paper proposes a new probing technique that decodes numeric values from input embeddings with high accuracy, proving that LMs can represent numbers precisely after pre-training. The probe's accuracy explains many of the LMs' arithmetic errors, and aligning embeddings with the discovered pattern reduces these errors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of arithmetic errors in pretrained language models and improve the understanding of how these models represent numeric values.

Method: The authors propose a novel probing technique that decodes numeric values from input embeddings, which works with near-perfect accuracy across various open-source LMs.

Result: The proposed probing method successfully decodes numeric values with remarkable precision, explaining a significant portion of arithmetic errors in LMs. Aligning embeddings with the discovered pattern mitigates these errors.

Conclusion: LMs can represent numbers with great precision after pre-training, and the proposed probing technique effectively identifies this ability. Improving the alignment of embeddings with the discovered pattern can reduce arithmetic errors.

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [312] [Physics-Informed Teleconnection-Aware Transformer for Global Subseasonal-to-Seasonal Forecasting](https://arxiv.org/abs/2506.08049)
*Tengfei Lyu,Weijia Zhang,Hao Liu*

Main category: stat.ML

TL;DR: The paper introduces TelePiT, a deep learning model for S2S forecasting that integrates multi-scale physics and teleconnection awareness, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: S2S forecasting faces challenges due to chaotic atmospheric dynamics and complex interactions across scales. Current methods fail to explicitly model crucial physical processes and teleconnections at S2S timescales.

Method: TelePiT consists of three components: Spherical Harmonic Embedding for encoding global atmospheric variables, Multi-Scale Physics-Informed Neural ODE for capturing atmospheric processes, and Teleconnection-Aware Transformer for modeling climate interactions.

Result: TelePiT significantly outperforms state-of-the-art data-driven baselines and operational numerical weather prediction systems, with notable improvements such as a 57.7% reduction in RMSE for 2-meter temperature compared to previous best models.

Conclusion: TelePiT enhances global S2S forecasting through integrated multi-scale physics and teleconnection awareness.

Abstract: Subseasonal-to-seasonal (S2S) forecasting, which predicts climate conditions
from several weeks to months in advance, presents significant challenges due to
the chaotic dynamics of atmospheric systems and complex interactions across
multiple scales. Current approaches often fail to explicitly model underlying
physical processes and teleconnections that are crucial at S2S timescales. We
introduce TelePiT, a novel deep learning architecture that enhances global S2S
forecasting through integrated multi-scale physics and teleconnection
awareness. Our approach consists of three key components: (1) Spherical
Harmonic Embedding, which accurately encodes global atmospheric variables onto
spherical geometry; (2) Multi-Scale Physics-Informed Neural ODE, which
explicitly captures atmospheric physical processes across multiple learnable
frequency bands; (3) Teleconnection-Aware Transformer, which models critical
global climate interactions through tactfully injecting teleconnection patterns
into the self-attention. Extensive experiments demonstrate that TelePiT
significantly outperforms state-of-the-art data-driven baselines and
operational numerical weather prediction systems, with remarkable improvements
for atmospheric variables including a 57.7% reduction in RMSE for 2-meter
temperature compared to previous best models.

</details>


### [313] [WWAggr: A Window Wasserstein-based Aggregation for Ensemble Change Point Detection](https://arxiv.org/abs/2506.08066)
*Alexander Stepikin,Evgenia Romanenkova,Alexey Zaytsev*

Main category: stat.ML

TL;DR: The paper explores ensembles of deep change point detectors and proposes WWAggr, a new ensemble aggregation method based on the Wasserstein distance, which is more effective than standard techniques and addresses the issue of decision threshold selection.


<details>
  <summary>Details</summary>
Motivation: Current deep neural network-based CPD methods have not reached perfect quality due to data pattern complexity and violation of common assumptions. Ensembling provides more robust solutions but standard prediction aggregation techniques like averaging are suboptimal.

Method: The authors introduce WWAggr, a novel task-specific ensemble aggregation method based on the Wasserstein distance, designed for ensembles of deep CPD models. This method aims to account for problem peculiarities and improve performance.

Result: WWAggr is versatile and works effectively with various ensembles of deep CPD models. It also addresses the long-standing problem of decision threshold selection for CPD.

Conclusion: Ensembles of deep change point detectors with the proposed WWAggr method offer improved performance in high-dimensional CPD tasks.

Abstract: Change Point Detection (CPD) aims to identify moments of abrupt distribution
shifts in data streams. Real-world high-dimensional CPD remains challenging due
to data pattern complexity and violation of common assumptions. Resorting to
standalone deep neural networks, the current state-of-the-art detectors have
yet to achieve perfect quality. Concurrently, ensembling provides more robust
solutions, boosting the performance. In this paper, we investigate ensembles of
deep change point detectors and realize that standard prediction aggregation
techniques, e.g., averaging, are suboptimal and fail to account for problem
peculiarities. Alternatively, we introduce WWAggr -- a novel task-specific
method of ensemble aggregation based on the Wasserstein distance. Our procedure
is versatile, working effectively with various ensembles of deep CPD models.
Moreover, unlike existing solutions, we practically lift a long-standing
problem of the decision threshold selection for CPD.

</details>


### [314] [Constrained Pareto Set Identification with Bandit Feedback](https://arxiv.org/abs/2506.08127)
*Cyrille Kone,Emilie Kaufmann,Laura Richert*

Main category: stat.ML

TL;DR: 本论文探讨了在多变量老虎机情境下，如何在可行性约束条件下识别帕累托集的问题。具体来说，给定一个K臂老虎机，目标是在满足某些线性约束条件下，识别出均值向量不被其他任何臂均值向量完全支配的臂集合。论文提出了一种新算法，在固定置信度识别问题上显著优于竞赛类算法和两阶段方法。此外，还证明了一个信息论下的样本复杂度下界，表明该算法接近最优。最后通过一系列基准测试进行了广泛的实证评估。


<details>
  <summary>Details</summary>
Motivation: 在现实世界中，很多决策问题需要同时考虑多个目标，并且这些目标之间可能存在冲突。例如，在资源分配、投资组合优化等领域，我们需要找到一组折衷方案，使得任何一个目标都不会变得更差。这就引出了帕累托最优的概念。然而，当加入可行性约束条件时（如每个目标至少需要达到某个最低要求），识别帕累托集变得更加困难。因此，研究在约束条件下如何高效地识别帕累托集具有重要意义。

Method: 作者提出了一种针对约束帕累托集识别的固定置信度算法。该算法避免了传统竞赛类算法或两阶段方法可能带来的效率低下问题。具体而言，新算法直接在考虑约束的同时进行臂的探索与淘汰，从而更有效地利用样本信息。

Result: 相比竞赛类算法和两阶段方法，所提出的算法显著提高了识别效率。理论分析表明，该算法的样本复杂度接近信息论下界，意味着其性能几乎无法进一步提升。实验结果也验证了该算法在多个基准测试上的优越性。

Conclusion: 本研究解决了在多目标带约束条件下识别帕累托集的问题，提出了高效的固定置信度算法，并证明了其接近最优的样本复杂度。这为解决实际中的多目标优化问题提供了有力工具。

Abstract: In this paper, we address the problem of identifying the Pareto Set under
feasibility constraints in a multivariate bandit setting. Specifically, given a
$K$-armed bandit with unknown means $\mu_1, \dots, \mu_K \in \mathbb{R}^d$, the
goal is to identify the set of arms whose mean is not uniformly worse than that
of another arm (i.e., not smaller for all objectives), while satisfying some
known set of linear constraints, expressing, for example, some minimal
performance on each objective. Our focus lies in fixed-confidence
identification, for which we introduce an algorithm that significantly
outperforms racing-like algorithms and the intuitive two-stage approach that
first identifies feasible arms and then their Pareto Set. We further prove an
information-theoretic lower bound on the sample complexity of any algorithm for
constrained Pareto Set identification, showing that the sample complexity of
our approach is near-optimal. Our theoretical results are supported by an
extensive empirical evaluation on a series of benchmarks.

</details>


### [315] [Model-Free Kernel Conformal Depth Measures Algorithm for Uncertainty Quantification in Regression Models in Separable Hilbert Spaces](https://arxiv.org/abs/2506.08325)
*Marcos Matabuena,Rahul Ghosal,Pavlo Mozharovskyi,Oscar Hernan Madrid Padilla,Jukka-Pekka Onnela*

Main category: stat.ML

TL;DR: 本论文提出了一种基于条件深度测量的无模型不确定性量化算法，结合核均值嵌入和一致性预测变体，用于定义预测和容忍区域，并证明了其在功能数据和欧几里得场景中的有效性和实用性。


<details>
  <summary>Details</summary>
Motivation: 深度测量是处理高维多变量数据、功能数据和随机图等复杂随机对象的强大工具，但将其整合到回归模型中以提供预测区域的研究尚不充分。

Method: 提出了一种基于条件深度测量的新颖无模型不确定性量化算法，利用条件核均值嵌入和综合深度测量来定义预测和容忍区域；引入了符合预测变体以提高有限样本的实际效用。

Result: 通过广泛的模拟研究验证了该模型在各种功能数据和传统欧几里得场景中的有限样本性能，并通过数字健康应用展示了其实际相关性。

Conclusion: 该算法在预测区域估计中具有更快的收敛速度，并且在某些同方差设置下具有一致性和快速收敛率。

Abstract: Depth measures are powerful tools for defining level sets in emerging,
non--standard, and complex random objects such as high-dimensional multivariate
data, functional data, and random graphs. Despite their favorable theoretical
properties, the integration of depth measures into regression modeling to
provide prediction regions remains a largely underexplored area of research. To
address this gap, we propose a novel, model-free uncertainty quantification
algorithm based on conditional depth measures--specifically, conditional kernel
mean embeddings and an integrated depth measure. These new algorithms can be
used to define prediction and tolerance regions when predictors and responses
are defined in separable Hilbert spaces. The use of kernel mean embeddings
ensures faster convergence rates in prediction region estimation. To enhance
the practical utility of the algorithms with finite samples, we also introduce
a conformal prediction variant that provides marginal, non-asymptotic
guarantees for the derived prediction regions. Additionally, we establish both
conditional and unconditional consistency results, as well as fast convergence
rates in certain homoscedastic settings. We evaluate the finite--sample
performance of our model in extensive simulation studies involving various
types of functional data and traditional Euclidean scenarios. Finally, we
demonstrate the practical relevance of our approach through a digital health
application related to physical activity, aiming to provide personalized
recommendations

</details>


### [316] [Asymptotic Normality of Infinite Centered Random Forests -Application to Imbalanced Classification](https://arxiv.org/abs/2506.08548)
*Moria Mayala,Erwan Scornet,Charles Tillier,Olivier Wintenberger*

Main category: stat.ML

TL;DR: This paper investigates the theoretical benefits of training Centered Random Forests (CRF) on rebalanced datasets for imbalanced classification tasks, proposing an importance sampling-based debiased estimator (IS-ICRF) that exhibits variance reduction in high imbalance settings.


<details>
  <summary>Details</summary>
Motivation: Imbalanced data poses challenges for classification tasks where one class is largely underrepresented. The motivation is to explore the theoretical advantages of rebalancing datasets before training classifiers such as CRFs, and to develop techniques to remove potential bias introduced by this process.

Method: The authors establish a Central Limit Theorem (CLT) for infinite CRFs with explicit rates and constants. They analyze the bias in CRFs trained on rebalanced data and propose a debiasing method based on importance sampling (IS), resulting in the IS-ICRF estimator. This estimator satisfies a CLT centered at the prediction function value and demonstrates variance reduction in high imbalance scenarios.

Result: The IS-ICRF estimator shows variance reduction compared to the ICRF trained on original data in high imbalance settings. Experimental results suggest that these theoretical findings also apply to Breiman's random forests.

Conclusion: Training random forests on rebalanced datasets followed by a debiasing procedure offers advantages over using original imbalanced data, particularly in terms of variance reduction.

Abstract: Many classification tasks involve imbalanced data, in which a class is
largely underrepresented. Several techniques consists in creating a rebalanced
dataset on which a classifier is trained. In this paper, we study theoretically
such a procedure, when the classifier is a Centered Random Forests (CRF). We
establish a Central Limit Theorem (CLT) on the infinite CRF with explicit rates
and exact constant. We then prove that the CRF trained on the rebalanced
dataset exhibits a bias, which can be removed with appropriate techniques.
Based on an importance sampling (IS) approach, the resulting debiased
estimator, called IS-ICRF, satisfies a CLT centered at the prediction function
value. For high imbalance settings, we prove that the IS-ICRF estimator enjoys
a variance reduction compared to the ICRF trained on the original data.
Therefore, our theoretical analysis highlights the benefits of training random
forests on a rebalanced dataset (followed by a debiasing procedure) compared to
using the original data. Our theoretical results, especially the variance rates
and the variance reduction, appear to be valid for Breiman's random forests in
our experiments.

</details>


### [317] [Flexible and Efficient Drift Detection without Labels](https://arxiv.org/abs/2506.08734)
*Nelvin Tan,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: stat.ML

TL;DR: The paper proposes a flexible and efficient label-less concept drift detection algorithm using classical statistical process control, demonstrating better statistical power under computational constraints and introducing a new drift detection framework.


<details>
  <summary>Details</summary>
Motivation: There is a need for early detection of concept drift in machine learning models to ensure their performance. Current research mainly focuses on supervised cases where true labels are immediately available after predictions, but in scenarios with extremely large datasets where true labels are not instantly accessible, controlling false positives becomes challenging.

Method: The authors propose an algorithm that uses classical statistical process control in a label-less setting to detect concept drifts. They also introduce a new drift detection framework to model the scenario of detecting drift without labels given prior detections.

Result: Empirical results show that the proposed approach has better statistical power than previous methods under computational constraints. The algorithm performs promisingly in numerical simulations.

Conclusion: The proposed concept drift detection algorithm is effective in a label-less setting and can be incorporated into a new drift detection framework, offering promising performance.

Abstract: Machine learning models are being increasingly used to automate decisions in
almost every domain, and ensuring the performance of these models is crucial
for ensuring high quality machine learning enabled services. Ensuring concept
drift is detected early is thus of the highest importance. A lot of research on
concept drift has focused on the supervised case that assumes the true labels
of supervised tasks are available immediately after making predictions.
Controlling for false positives while monitoring the performance of predictive
models used to make inference from extremely large datasets periodically, where
the true labels are not instantly available, becomes extremely challenging. We
propose a flexible and efficient concept drift detection algorithm that uses
classical statistical process control in a label-less setting to accurately
detect concept drifts. We shown empirically that under computational
constraints, our approach has better statistical power than previous known
methods. Furthermore, we introduce a new drift detection framework to model the
scenario of detecting drift (without labels) given prior detections, and show
our how our drift detection algorithm can be incorporated effectively into this
framework. We demonstrate promising performance via numerical simulations.

</details>
