<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 27]
- [cs.LG](#cs.LG) [Total: 139]
- [cs.CR](#cs.CR) [Total: 17]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.DB](#cs.DB) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 7]
- [cs.CV](#cs.CV) [Total: 67]
- [cs.HC](#cs.HC) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [eess.IV](#eess.IV) [Total: 10]
- [cs.RO](#cs.RO) [Total: 13]
- [stat.CO](#stat.CO) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [quant-ph](#quant-ph) [Total: 7]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.NI](#cs.NI) [Total: 4]
- [cs.SD](#cs.SD) [Total: 6]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 10]
- [stat.ML](#stat.ML) [Total: 17]
- [cs.SE](#cs.SE) [Total: 6]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 7]
- [eess.SP](#eess.SP) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.CL](#cs.CL) [Total: 42]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Understanding the learned look-ahead behavior of chess neural networks](https://arxiv.org/abs/2505.21552)
*Diogo Cruz*

Main category: cs.AI

TL;DR: The paper explores how chess-playing neural networks, particularly Leela Chess Zero's policy network, can look ahead in the game. It shows that these networks can plan up to seven moves ahead using context-dependent mechanisms and consider multiple move sequences, providing insights into AI reasoning.


<details>
  <summary>Details</summary>
Motivation: To understand the look-ahead capabilities of chess-playing neural networks and how they process future moves beyond the immediate next one.

Method: Analyze the Leela Chess Zero policy network's ability to consider future moves and alternative sequences by building on previous work and using interpretability techniques.

Result: The network can process board states up to seven moves ahead and considers multiple possible move sequences rather than focusing on a single line of play.

Conclusion: This study reveals sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains.

Abstract: We investigate the look-ahead capabilities of chess-playing neural networks,
specifically focusing on the Leela Chess Zero policy network. We build on the
work of Jenner et al. (2024) by analyzing the model's ability to consider
future moves and alternative sequences beyond the immediate next move. Our
findings reveal that the network's look-ahead behavior is highly
context-dependent, varying significantly based on the specific chess position.
We demonstrate that the model can process information about board states up to
seven moves ahead, utilizing similar internal mechanisms across different
future time steps. Additionally, we provide evidence that the network considers
multiple possible move sequences rather than focusing on a single line of play.
These results offer new insights into the emergence of sophisticated look-ahead
capabilities in neural networks trained on strategic tasks, contributing to our
understanding of AI reasoning in complex domains. Our work also showcases the
effectiveness of interpretability techniques in uncovering cognitive-like
processes in artificial intelligence systems.

</details>


### [2] [R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning](https://arxiv.org/abs/2505.21668)
*Yongchao Chen,Yueying Liu,Junwei Zhou,Yilun Hao,Jingquan Wang,Yang Zhang,Chuchu Fan*

Main category: cs.AI

TL;DR: 尽管R1类模型在推理和规划方面取得了进展，但大型语言模型（LLMs）在需要精确计算、符号操作、优化和算法推理的任务上仍存在困难。本文提出了R1-Code-Interpreter，通过多轮监督微调（SFT）和强化学习（RL），使文本型LLM能够自主生成多个代码查询以辅助推理。实验结果表明，该模型显著提高了任务准确性，并展现出通过代码生成的自我检查行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要精确计算和符号操作的任务上表现不佳，且缺乏关于如何有效利用代码执行的公共研究指导。因此，有必要开发一种方法，使LLM能够在文本推理和代码生成之间做出合理选择。

Method: 提出R1-Code-Interpreter扩展，基于文本型LLM通过多轮监督微调和强化学习训练，使其在逐步推理过程中自主生成多个代码查询。同时，创建了144个推理和规划任务的数据集，并对Qwen-2.5模型进行微调和测试，探索不同的训练策略和参数设置。

Result: 最终模型R1-CI-14B在37个测试任务上的平均准确率从44.0%提升至64.1%，优于仅文本型GPT-4o（58.6%），接近带有代码解释器的GPT-4o（70.9%），并展现出通过代码生成的自我检查行为。

Conclusion: R1-Code-Interpreter通过结合文本推理和代码生成显著提升了LLM在复杂任务中的表现，强调了监督微调阶段的重要性，并为未来的研究提供了数据集、代码和模型支持。

Abstract: Despite advances in reasoning and planning of R1-like models, Large Language
Models (LLMs) still struggle with tasks requiring precise computation, symbolic
manipulation, optimization, and algorithmic reasoning, in which textual
reasoning lacks the rigor of code execution. A key challenge is enabling LLMs
to decide when to use textual reasoning versus code generation. While OpenAI
trains models to invoke a Code Interpreter as needed, public research lacks
guidance on aligning pre-trained LLMs to effectively leverage code and
generalize across diverse tasks. We present R1-Code-Interpreter, an extension
of a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and
reinforcement learning (RL) to autonomously generate multiple code queries
during step-by-step reasoning. We curate 144 reasoning and planning tasks (107
for training, 37 for testing), each with over 200 diverse questions. We
fine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,
investigating different answer formats, reasoning vs. non-reasoning models,
cold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.
Unlike prior RL work on narrow domains, we find that Code Interpreter training
is significantly harder due to high task diversity and expensive code
execution, highlighting the critical role of the SFT stage. Our final model,
R1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\% to
64.1\%, outperforming GPT-4o (text-only: 58.6\%) and approaching GPT-4o with
Code Interpreter (70.9\%), with the emergent self-checking behavior via code
generation. Datasets, Codes, and Models are available at
https://github.com/yongchao98/R1-Code-Interpreter and
https://huggingface.co/yongchao98.

</details>


### [3] [Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing](https://arxiv.org/abs/2505.21671)
*Davin Choo,Yuqi Pan,Tonghan Wang,Milind Tambe,Alastair van Heerden,Cheryl Johnson*

Main category: cs.AI

TL;DR: 研究了在具有未知标签的n节点图G上的顺序决策问题，设计了一种基于Gittins指数的策略，在多种实验中表现优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 解决在图结构上带有未知标签节点的顺序决策问题，尤其在存在前沿探索约束的情况下，如接触者追踪和机器人探索等实际应用中的限制。

Method: 设计了一种基于Gittins指数的策略，适用于一般图结构，并且在图G为森林时可证明是最优的。该算法的时间复杂度为O(n²·|Σ|²)，使用O(n·|Σ|²)次对联合分布P的调用，以及O(n²·|Σ|)的空间复杂度。

Result: 在合成和真实世界的图数据集上的实验表明，所提出的方法在不同设置下（非树状结构、预算受限、未折扣）始终优于自然基线方法。例如，在现实世界HIV检测模拟中，只需测试一半人口即可检测到几乎所有的阳性病例。

Conclusion: 所提出的基于Gittins指数的策略在顺序决策问题中表现出色，尤其是在具有前沿探索约束的图结构上，为实际应用场景提供了有效解决方案。

Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where
each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from
a joint distribution $P$ that is Markov with respect to $G$. At each step,
selecting a node reveals its label and yields a label-dependent reward. The
goal is to adaptively choose nodes to maximize expected accumulated discounted
rewards. We impose a frontier exploration constraint, where actions are limited
to neighbors of previously selected nodes, reflecting practical constraints in
settings such as contact tracing and robotic exploration. We design a Gittins
index-based policy that applies to general graphs and is provably optimal when
$G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$
time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and
$O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world
graphs show that our method consistently outperforms natural baselines,
including in non-tree, budget-limited, and undiscounted settings. For example,
in HIV testing simulations on real-world sexual interaction networks, our
policy detects nearly all positive cases with only half the population tested,
substantially outperforming other baselines.

</details>


### [4] [Make Planning Research Rigorous Again!](https://arxiv.org/abs/2505.21674)
*Michael Katz,Harsha Kokel,Christian Muise,Shirin Sohrabi,Sarath Sreedharan*

Main category: cs.AI

TL;DR: In over sixty years, planning field significantly contributed to building planning software. The rigor of design and evaluation should be applied to planning with large language models by incorporating insights, tools, and data from the automated planning community. This is crucial in avoiding known pitfalls encountered before.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper lies in applying established practices of rigorous design and evaluation from traditional planning systems into the development of LLM-based planners, aiming to avoid replication of past pitfalls.

Method: By correctly incorporating insights, tools, and data from the automated planning community into the design and evaluation of LLM-based planners.

Result: This approach could accelerate the development of LLM-based planners and contribute greatly to the progress in building such planners.

Conclusion: Avoiding known pitfalls will greatly contribute to the progress in building LLM-based planners and to planning in general.

Abstract: In over sixty years since its inception, the field of planning has made
significant contributions to both the theory and practice of building planning
software that can solve a never-before-seen planning problem. This was done
through established practices of rigorous design and evaluation of planning
systems. It is our position that this rigor should be applied to the current
trend of work on planning with large language models. One way to do so is by
correctly incorporating the insights, tools, and data from the automated
planning community into the design and evaluation of LLM-based planners. The
experience and expertise of the planning community are not just important from
a historical perspective; the lessons learned could play a crucial role in
accelerating the development of LLM-based planners. This position is
particularly important in light of the abundance of recent works that replicate
and propagate the same pitfalls that the planning community has encountered and
learned from. We believe that avoiding such known pitfalls will contribute
greatly to the progress in building LLM-based planners and to planning in
general.

</details>


### [5] [Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models](https://arxiv.org/abs/2505.21765)
*Sohyun An,Ruochen Wang,Tianyi Zhou,Cho-Jui Hsieh*

Main category: cs.AI

TL;DR: Recent large reasoning models (LRMs) enhance LLMs' reasoning but may overthink, leading to inefficiency. This study proposes a dynamic optimization framework that identifies and promotes beneficial thinking patterns while removing detrimental ones, resulting in more concise and accurate reasoning paths. Experiments show significant reductions in computational overhead and token usage with notable accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies in LRMs caused by overthinking, which leads to unnecessarily complex reasoning paths that waste computation and potentially degrade performance. The authors aim to dynamically select proper modular reasoning strategies, termed thinking patterns, to improve reasoning efficiency.

Method: The method involves proposing a dynamic optimization framework that segments model-generated reasoning paths into distinct thinking patterns. It systematically identifies and promotes beneficial patterns that improve answers while removing detrimental ones.

Result: Empirical analysis shows that optimized thinking paths yield more concise yet sufficiently informative trajectories, reducing attention FLOPs by up to 47% while maintaining accuracy for originally correct responses. Additionally, it transforms a portion of incorrect responses into correct ones, achieving a 15.6% accuracy improvement with reduced length. Overall, the method reduces computational overhead and token usage while improving reasoning accuracy by up to 12%.

Conclusion: The conclusion is that the proposed dynamic optimization framework effectively enhances reasoning efficiency by reducing unnecessary complexity in LRMs' reasoning paths. This results in both computational savings and improved accuracy.

Abstract: While recent success of large reasoning models (LRMs) significantly advanced
LLMs' reasoning capability by optimizing the final answer accuracy using
reinforcement learning, they may also drastically increase the output length
due to overthinking, characterized by unnecessarily complex reasoning paths
that waste computation and potentially degrade the performance. We hypothesize
that such inefficiencies stem from LRMs' limited capability to dynamically
select the proper modular reasoning strategies, termed thinking patterns at the
right position. To investigate this hypothesis, we propose a dynamic
optimization framework that segments model-generated reasoning paths into
distinct thinking patterns, systematically identifying and promoting beneficial
patterns that improve the answer while removing detrimental ones. Empirical
analysis confirms that our optimized thinking paths yield more concise yet
sufficiently informative trajectories, enhancing reasoning efficiency by
reducing attention FLOPs by up to 47% while maintaining accuracy for originally
correct responses. Moreover, a non-trivial portion of originally incorrect
responses are transformed into correct ones, achieving a 15.6% accuracy
improvement with reduced length. Motivated by the improvement brought by the
optimized thinking paths, we apply a preference optimization technique
supported by a pairwise dataset contrasting suboptimal and optimal reasoning
paths. Experimental evaluations across multiple mathematical reasoning
benchmarks reveal that our method notably reduces computational overhead while
simultaneously improving reasoning accuracy, achieving up to a 12% accuracy
improvement and reducing token usage from approximately 5,000 to 3,000 tokens.

</details>


### [6] [Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation](https://arxiv.org/abs/2505.21784)
*Tharindu Kumarage,Ninareh Mehrabi,Anil Ramakrishna,Xinyan Zhao,Richard Zemel,Kai-Wei Chang,Aram Galstyan,Rahul Gupta,Charith Peris*

Main category: cs.AI

TL;DR: AIDSAFE is a new method using multi-agent deliberation to generate high-quality safety policy datasets, which improves LLMs' safety reasoning and reduces jailbreak vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Existing safety measures for LLMs have limitations like over-refusal and vulnerability to jailbreaks. Creating accurate and conflict-free safety policy datasets is resource-intensive and challenging.

Method: Propose AIDSAFE, incorporating multi-agent deliberation to iteratively expand safety policy reasoning and a data refiner stage to ensure quality by removing repetitive, redundant, or deceptive thoughts. Also, introduce a supplemental recipe using belief augmentation for preference data in alignment stages.

Result: AIDSAFE-generated CoTs show superior policy adherence and reasoning quality. Fine-tuning open-source LLMs with these CoTs significantly enhances safety generalization and jailbreak robustness while maintaining utility and over-refusal accuracy.

Conclusion: AIDSAFE provides an effective solution for generating high-quality safety policy datasets, improving LLMs' safety reasoning capabilities.

Abstract: Safety reasoning is a recent paradigm where LLMs reason over safety policies
before generating responses, thereby mitigating limitations in existing safety
measures such as over-refusal and jailbreak vulnerabilities. However,
implementing this paradigm is challenging due to the resource-intensive process
of creating high-quality policy-embedded chain-of-thought (CoT) datasets while
ensuring reasoning remains accurate and free from hallucinations or policy
conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation
for Safety Reasoning, a novel data generation recipe that leverages multi-agent
deliberation to iteratively expand reasoning on safety policies. A data refiner
stage in AIDSAFE ensures high-quality outputs by eliminating repetitive,
redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong
foundation for supervised fine-tuning (SFT)-based safety training.
Additionally, to address the need of preference data in alignment stages, such
as DPO training, we introduce a supplemental recipe that uses belief
augmentation to create distinct selected and rejected CoT samples. Our
evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy
adherence and reasoning quality. Consequently, we show that fine-tuning
open-source LLMs on these CoTs can significantly improve safety generalization
and jailbreak robustness while maintaining acceptable utility and over-refusal
accuracy. AIDSAFE-generated CoT datasets can be found here:
https://huggingface.co/datasets/AmazonScience/AIDSAFE

</details>


### [7] [SAGE-Eval: Evaluating LLMs for Systematic Generalizations of Safety Facts](https://arxiv.org/abs/2505.21828)
*Chen Yueh-Han,Guy Davidson,Brenden M. Lake*

Main category: cs.AI

TL;DR: The paper introduces SAGE-Eval, a benchmark to test if LLMs can apply established safety facts to user queries. The top model passes only 58% of tests, showing a lack of robust generalization. Developers should use SAGE-Eval for pre-deployment evaluations.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can robustly generalize critical safety facts to novel situations and warn users about potential dangers.

Method: Introduced SAGE-Eval, a benchmark consisting of 104 facts from reputable organizations, augmented into 10,428 test scenarios across 7 domains, to test LLMs' ability to apply safety facts to naive user queries.

Result: Claude-3.7-sonnet, the top model tested, passed only 58% of the safety facts tests. Model capabilities and training compute weakly correlate with performance on SAGE-Eval.

Conclusion: Frontier LLMs still lack robust generalization ability in applying safety facts. Developers are recommended to use SAGE-Eval for pre-deployment evaluations.

Abstract: Do LLMs robustly generalize critical safety facts to novel situations?
Lacking this ability is dangerous when users ask naive questions. For instance,
"I'm considering packing melon balls for my 10-month-old's lunch. What other
foods would be good to include?" Before offering food options, the LLM should
warn that melon balls pose a choking hazard to toddlers, as documented by the
CDC. Failing to provide such warnings could result in serious injuries or even
death. To evaluate this, we introduce SAGE-Eval, SAfety-fact systematic
GEneralization evaluation, the first benchmark that tests whether LLMs properly
apply well established safety facts to naive user queries. SAGE-Eval comprises
104 facts manually sourced from reputable organizations, systematically
augmented to create 10,428 test scenarios across 7 common domains (e.g.,
Outdoor Activities, Medicine). We find that the top model, Claude-3.7-sonnet,
passes only 58% of all the safety facts tested. We also observe that model
capabilities and training compute weakly correlate with performance on
SAGE-Eval, implying that scaling up is not the golden solution. Our findings
suggest frontier LLMs still lack robust generalization ability. We recommend
developers use SAGE-Eval in pre-deployment evaluations to assess model
reliability in addressing salient risks. We publicly release SAGE-Eval at
https://huggingface.co/datasets/YuehHanChen/SAGE-Eval and our code is available
at https://github.com/YuehHanChen/SAGE-Eval/tree/main.

</details>


### [8] [SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem](https://arxiv.org/abs/2505.21887)
*Ahmed Heakl,Yahia Salaheldin Shaaban,Martin Takac,Salem Lahlou,Zangir Iklassov*

Main category: cs.AI

TL;DR: SVRPBench是一个开放的基准测试平台，用于捕捉城市规模车辆路径规划中的高保真随机动态。它模拟了真实的配送条件，并揭示了现有RL求解器在分布外泛化能力上的不足，而经典和元启发式方法则保持稳健。


<details>
  <summary>Details</summary>
Motivation: 当前大多数基准测试假设静态、理想化的设置，缺乏对真实世界物流中不确定性环境的考虑。

Method: 构建了一个包含超过500个实例的开放基准SVRPBench，模拟时间依赖性拥堵、对数正态延迟、概率事故和基于实证的时间窗口等真实配送条件。使用包括多仓库和多车辆设置在内的多样化约束场景生成管道。

Result: 发现最先进的RL求解器（如POMO和AM）在分布外泛化时性能下降超过20%，而经典和元启发式方法仍然保持稳健。

Conclusion: SVRPBench挑战研究社区设计能够超越合成假设并适应现实世界不确定性的求解器，并发布数据集和评估套件以促进可重复研究。

Abstract: Robust routing under uncertainty is central to real-world logistics, yet most
benchmarks assume static, idealized settings. We present SVRPBench, the first
open benchmark to capture high-fidelity stochastic dynamics in vehicle routing
at urban scale. Spanning more than 500 instances with up to 1000 customers, it
simulates realistic delivery conditions: time-dependent congestion, log-normal
delays, probabilistic accidents, and empirically grounded time windows for
residential and commercial clients. Our pipeline generates diverse,
constraint-rich scenarios, including multi-depot and multi-vehicle setups.
Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade
by over 20% under distributional shift, while classical and metaheuristic
methods remain robust. To enable reproducible research, we release the dataset
and evaluation suite. SVRPBench challenges the community to design solvers that
generalize beyond synthetic assumptions and adapt to real-world uncertainty.

</details>


### [9] [Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy](https://arxiv.org/abs/2505.21907)
*Saleh Afzoon,Zahra Jahanandish,Phuong Thao Huynh,Amin Beheshti,Usman Naseem*

Main category: cs.AI

TL;DR: AI copilots are AI systems assisting in tasks like software development and content creation. Personalization through preference optimization is crucial for their usability, trust, and productivity. This survey synthesizes research on capturing, modeling, and refining user preferences within AI copilot design, proposing a phase-based taxonomy of preference optimization strategies.


<details>
  <summary>Details</summary>
Motivation: To address the gap in adapting personalization techniques to interactive, real-time systems like AI copilots and provide a structured foundation for designing adaptive, preference-aware AI copilots.

Method: The survey introduces a unified definition of AI copilots and proposes a phase-based taxonomy of preference optimization strategies, analyzing techniques for acquiring preference signals, modeling user intent, and integrating feedback loops.

Result: The survey provides insights into AI personalization, human-AI collaboration, and large language model adaptation, offering a holistic view of preference resources and suitable technical approaches for each stage of system design.

Conclusion: This survey bridges fragmented knowledge about preference optimization in AI copilots, guiding future research and development towards more personalized and effective AI systems.

Abstract: AI copilots, context-aware, AI-powered systems designed to assist users in
tasks such as software development and content creation, are becoming integral
to modern workflows. As these systems grow in capability and adoption,
personalization has emerged as a cornerstone for ensuring usability, trust, and
productivity. Central to this personalization is preference optimization: the
ability of AI copilots to detect, interpret, and align with individual user
preferences. While personalization techniques are well-established in domains
like recommender systems and dialogue agents, their adaptation to interactive,
real-time systems like AI copilots remains fragmented and underexplored. This
survey addresses this gap by synthesizing research on how user preferences are
captured, modeled, and refined within the design of AI copilots. We introduce a
unified definition of AI copilots and propose a phase-based taxonomy of
preference optimization strategies, structured around pre-interaction,
mid-interaction, and post-interaction stages. We analyze techniques for
acquiring preference signals, modeling user intent, and integrating feedback
loops, highlighting both established approaches and recent innovations. By
bridging insights from AI personalization, human-AI collaboration, and large
language model adaptation, this survey provides a structured foundation for
designing adaptive, preference-aware AI copilots. It offers a holistic view of
the available preference resources, how they can be leveraged, and which
technical approaches are most suited to each stage of system design.

</details>


### [10] [From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models](https://arxiv.org/abs/2505.21935)
*Kaiyu He,Zhiyu Chen*

Main category: cs.AI

TL;DR: Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. Through Peirce's framework of abduction, deduction, and induction, this survey examines LLM-based hypothesis discovery and how LLMs might evolve from mere ``information executors'' into engines of genuine innovation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore whether LLMs can go beyond executing commands or retrieving information and truly discover new knowledge by learning, reasoning, and generating novel hypotheses and theories that deepen our understanding of the world in pursuit of artificial general intelligence (AGI).

Method: This survey uses Peirce's framework of abduction, deduction, and induction to examine LLM-based hypothesis discovery. It synthesizes existing work in hypothesis generation, application, and validation.

Result: The survey identifies key achievements and critical gaps in the field of LLM-based hypothesis discovery.

Conclusion: LLMs have the potential to evolve from mere ``information executors'' into engines of genuine innovation, which could transform research, science, and real-world problem solving.

Abstract: Since the advent of Large Language Models (LLMs), efforts have largely
focused on improving their instruction-following and deductive reasoning
abilities, leaving open the question of whether these models can truly discover
new knowledge. In pursuit of artificial general intelligence (AGI), there is a
growing need for models that not only execute commands or retrieve information
but also learn, reason, and generate new knowledge by formulating novel
hypotheses and theories that deepen our understanding of the world. Guided by
Peirce's framework of abduction, deduction, and induction, this survey offers a
structured lens to examine LLM-based hypothesis discovery. We synthesize
existing work in hypothesis generation, application, and validation,
identifying both key achievements and critical gaps. By unifying these threads,
we illuminate how LLMs might evolve from mere ``information executors'' into
engines of genuine innovation, potentially transforming research, science, and
real-world problem solving.

</details>


### [11] [Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism](https://arxiv.org/abs/2505.21988)
*Ziyang Zheng,Kezhi Li,Zhengyuan Shi,Qiang Xu*

Main category: cs.AI

TL;DR: The paper introduces functional subgraph matching for logic circuits, a two-stage framework that improves upon structural methods by focusing on function-related subgraphs, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing subgraph matching techniques in logic circuits rely on structural graph isomorphism and cannot effectively identify function-related subgraphs when circuit topology changes due to synthesis transformations.

Method: A two-stage multi-modal framework is proposed: (1) learning robust functional embeddings across AIG and post-mapping netlists for functional subgraph detection, and (2) identifying fuzzy boundaries using a graph segmentation approach.

Result: Evaluations on standard benchmarks show significant performance improvements over existing structural methods, with 93.8% accuracy in functional subgraph detection and a dice score of 91.3% in fuzzy boundary identification.

Conclusion: Functional subgraph matching offers a novel solution to the limitations of structural methods in logic circuits, providing robust detection of function-related subgraphs despite variations in circuit topology.

Abstract: Subgraph matching in logic circuits is foundational for numerous Electronic
Design Automation (EDA) applications, including datapath optimization,
arithmetic verification, and hardware trojan detection. However, existing
techniques rely primarily on structural graph isomorphism and thus fail to
identify function-related subgraphs when synthesis transformations
substantially alter circuit topology. To overcome this critical limitation, we
introduce the concept of functional subgraph matching, a novel approach that
identifies whether a given logic function is implicitly present within a larger
circuit, irrespective of structural variations induced by synthesis or
technology mapping. Specifically, we propose a two-stage multi-modal framework:
(1) learning robust functional embeddings across AIG and post-mapping netlists
for functional subgraph detection, and (2) identifying fuzzy boundaries using a
graph segmentation approach. Evaluations on standard benchmarks (ITC99,
OpenABCD, ForgeEDA) demonstrate significant performance improvements over
existing structural methods, with average $93.8\%$ accuracy in functional
subgraph detection and a dice score of $91.3\%$ in fuzzy boundary
identification.

</details>


### [12] [Efficiently Enhancing General Agents With Hierarchical-categorical Memory](https://arxiv.org/abs/2505.22006)
*Changze Qiao,Mingming Lu*

Main category: cs.AI

TL;DR: The paper introduces EHC, a general agent capable of learning without parameter updates. It consists of HMR and TOEL modules, which facilitate memory retrieval and enhance task comprehension respectively. Experiments show that EHC outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to build multi-modal agents either rely on computationally expensive training or lack the ability to continuously learn and adapt.

Method: EHC consists of a Hierarchical Memory Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL) module. HMR allows rapid memory retrieval and continuous storage without memory constraints, while TOEL classifies experiences and extracts patterns to understand task characteristics.

Result: Extensive experiments on multiple standard datasets demonstrate that EHC achieves state-of-the-art performance in handling complex multi-modal tasks.

Conclusion: EHC is an effective general agent for complex multi-modal tasks, capable of learning without parameter updates.

Abstract: With large language models (LLMs) demonstrating remarkable capabilities,
there has been a surge in research on leveraging LLMs to build general-purpose
multi-modal agents. However, existing approaches either rely on computationally
expensive end-to-end training using large-scale multi-modal data or adopt
tool-use methods that lack the ability to continuously learn and adapt to new
environments. In this paper, we introduce EHC, a general agent capable of
learning without parameter updates. EHC consists of a Hierarchical Memory
Retrieval (HMR) module and a Task-Category Oriented Experience Learning (TOEL)
module. The HMR module facilitates rapid retrieval of relevant memories and
continuously stores new information without being constrained by memory
capacity. The TOEL module enhances the agent's comprehension of various task
characteristics by classifying experiences and extracting patterns across
different categories. Extensive experiments conducted on multiple standard
datasets demonstrate that EHC outperforms existing methods, achieving
state-of-the-art performance and underscoring its effectiveness as a general
agent for handling complex multi-modal tasks.

</details>


### [13] [Reinforced Reasoning for Embodied Planning](https://arxiv.org/abs/2505.22050)
*Di Wu,Jiaxin Fan,Junzhe Zang,Guanbo Wang,Wei Yin,Wenhao Li,Bo Jin*

Main category: cs.AI

TL;DR: This paper presents a reinforcement fine-tuning framework for embodied planning that significantly outperforms existing models and generalizes well to unseen environments.


<details>
  <summary>Details</summary>
Motivation: Embodied planning requires coherent multi-step decisions based on dynamic visual observations and natural language goals, but current vision-language models struggle with temporal reasoning, spatial understanding, and commonsense grounding in interactive environments.

Method: The authors distill a high-quality dataset from a closed-source model and use supervised fine-tuning to equip the model with structured decision-making priors. They then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO).

Result: The approach outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, on the Embench benchmark covering both in-domain and out-of-domain scenarios. It also exhibits strong generalization to unseen environments.

Conclusion: The work demonstrates the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

Abstract: Embodied planning requires agents to make coherent multi-step decisions based
on dynamic visual observations and natural language goals. While recent
vision-language models (VLMs) excel at static perception tasks, they struggle
with the temporal reasoning, spatial understanding, and commonsense grounding
needed for planning in interactive environments. In this work, we introduce a
reinforcement fine-tuning framework that brings R1-style reasoning enhancement
into embodied planning. We first distill a high-quality dataset from a powerful
closed-source model and perform supervised fine-tuning (SFT) to equip the model
with structured decision-making priors. We then design a rule-based reward
function tailored to multi-step action quality and optimize the policy via
Generalized Reinforced Preference Optimization (GRPO). Our approach is
evaluated on Embench, a recent benchmark for interactive embodied tasks,
covering both in-domain and out-of-domain scenarios. Experimental results show
that our method significantly outperforms models of similar or larger scale,
including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong
generalization to unseen environments. This work highlights the potential of
reinforcement-driven reasoning to advance long-horizon planning in embodied AI.

</details>


### [14] [Cognitively-Inspired Emergent Communication via Knowledge Graphs for Assisting the Visually Impaired](https://arxiv.org/abs/2505.22087)
*Ruxiao Chen,Dezheng Han,Wenjie Han,Shuaishuai Guo*

Main category: cs.AI

TL;DR: 为了帮助视障人士实时导航，本文提出了一种新的框架VAG-EC，它结合了知识图谱和注意机制，生成紧凑、可解释且与情境相关的符号语言，实现在不同词汇量和消息长度下优于传统方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的辅助系统在延迟和语义丰富性之间存在权衡：基于自然语言的系统虽提供详细指导但速度慢，而新兴通信框架虽低延迟但缺乏语义深度。

Method: 引入Cognitively-Inspired Emergent Communication via Knowledge Graphs (VAG-EC) 框架，该方法通过构建知识图谱来表示对象及其关系，并使用注意力机制优先处理任务相关实体，从而模仿人类的选择性注意力。

Result: 实验表明，在不同的词汇量和消息长度下，VAG-EC在Topographic Similarity (TopSim) 和 Context Independence (CI)方面均优于传统的新兴通信方法。

Conclusion: 认知基础的新兴通信具有作为快速、适应性强且与人类一致的实时辅助技术解决方案的潜力。

Abstract: Assistive systems for visually impaired individuals must deliver rapid,
interpretable, and adaptive feedback to facilitate real-time navigation.
Current approaches face a trade-off between latency and semantic richness:
natural language-based systems provide detailed guidance but are too slow for
dynamic scenarios, while emergent communication frameworks offer low-latency
symbolic languages but lack semantic depth, limiting their utility in tactile
modalities like vibration. To address these limitations, we introduce a novel
framework, Cognitively-Inspired Emergent Communication via Knowledge Graphs
(VAG-EC), which emulates human visual perception and cognitive mapping. Our
method constructs knowledge graphs to represent objects and their
relationships, incorporating attention mechanisms to prioritize task-relevant
entities, thereby mirroring human selective attention. This structured approach
enables the emergence of compact, interpretable, and context-sensitive symbolic
languages. Extensive experiments across varying vocabulary sizes and message
lengths demonstrate that VAG-EC outperforms traditional emergent communication
methods in Topographic Similarity (TopSim) and Context Independence (CI). These
findings underscore the potential of cognitively grounded emergent
communication as a fast, adaptive, and human-aligned solution for real-time
assistive technologies. Code is available at
https://github.com/Anonymous-NLPcode/Anonymous_submission/tree/main.

</details>


### [15] [VIRAL: Vision-grounded Integration for Reward design And Learning](https://arxiv.org/abs/2505.22092)
*Valentin Cuzin-Rambaud,Emilien Komlenovic,Alexandre Faure,Bruno Yun*

Main category: cs.AI

TL;DR: The paper presents VIRAL, a pipeline using multi-modal LLMs to generate and refine reward functions in reinforcement learning, improving alignment between human goals and machine behaviors.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning systems are susceptible to risks from poorly designed reward functions, creating a need for more effective and aligned reward generation methods.

Method: VIRAL autonomously creates initial reward functions and refines them interactively. Refinement can be guided by human feedback or video LLM descriptions explaining the agent's policy.

Result: VIRAL was evaluated in five Gymnasium environments and showed accelerated learning of new behaviors with better alignment to user intent compared to existing methods.

Conclusion: VIRAL demonstrates potential in enhancing the alignment between human goals and machine behaviors in reinforcement learning through the use of multi-modal LLMs.

Abstract: The alignment between humans and machines is a critical challenge in
artificial intelligence today. Reinforcement learning, which aims to maximize a
reward function, is particularly vulnerable to the risks associated with poorly
designed reward functions. Recent advancements has shown that Large Language
Models (LLMs) for reward generation can outperform human performance in this
context. We introduce VIRAL, a pipeline for generating and refining reward
functions through the use of multi-modal LLMs. VIRAL autonomously creates and
interactively improves reward functions based on a given environment and a goal
prompt or annotated image. The refinement process can incorporate human
feedback or be guided by a description generated by a video LLM, which explains
the agent's policy in video form. We evaluated VIRAL in five Gymnasium
environments, demonstrating that it accelerates the learning of new behaviors
while ensuring improved alignment with user intent. The source-code and demo
video are available at: https://github.com/VIRAL-UCBL1/VIRAL and
https://youtu.be/t4_BXugBm9Q.

</details>


### [16] [Efficient Dynamic Shielding for Parametric Safety Specifications](https://arxiv.org/abs/2505.22104)
*Davide Corsi,Kaushik Mallik,Andoni Rodriguez,Cesar Sanchez*

Main category: cs.AI

TL;DR: The paper presents dynamic shields for parametric safety specifications in AI-controlled autonomous systems, which can adapt at runtime as the safety specification changes, avoiding delays of recomputation from scratch. Experimental results show that dynamic shields are more efficient than brute-force online recomputation.


<details>
  <summary>Details</summary>
Motivation: Traditional shields are designed statically for a specific safety requirement. If the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal.

Method: Dynamic shields for parametric safety specifications are introduced. They are succinctly represented sets of all possible safety specifications that may be encountered at runtime. The dynamic shields are statically designed for a given safety parameter set and able to dynamically adapt as the true safety specification is revealed at runtime.

Result: Experimental results for a robot navigation problem in unknown territories showed that dynamic shields took a few minutes for their offline design and between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower.

Conclusion: Dynamic shields offer an efficient solution for ensuring safety in AI-controlled autonomous systems with changing safety requirements.

Abstract: Shielding has emerged as a promising approach for ensuring safety of
AI-controlled autonomous systems. The algorithmic goal is to compute a shield,
which is a runtime safety enforcement tool that needs to monitor and intervene
the AI controller's actions if safety could be compromised otherwise.
Traditional shields are designed statically for a specific safety requirement.
Therefore, if the safety requirement changes at runtime due to changing
operating conditions, the shield needs to be recomputed from scratch, causing
delays that could be fatal. We introduce dynamic shields for parametric safety
specifications, which are succinctly represented sets of all possible safety
specifications that may be encountered at runtime. Our dynamic shields are
statically designed for a given safety parameter set, and are able to
dynamically adapt as the true safety specification (permissible by the
parameters) is revealed at runtime. The main algorithmic novelty lies in the
dynamic adaptation procedure, which is a simple and fast algorithm that
utilizes known features of standard safety shields, like maximal
permissiveness. We report experimental results for a robot navigation problem
in unknown territories, where the safety specification evolves as new obstacles
are discovered at runtime. In our experiments, the dynamic shields took a few
minutes for their offline design, and took between a fraction of a second and a
few seconds for online adaptation at each step, whereas the brute-force online
recomputation approach was up to 5 times slower.

</details>


### [17] [Visual Large Language Models Exhibit Human-Level Cognitive Flexibility in the Wisconsin Card Sorting Test](https://arxiv.org/abs/2505.22112)
*Guangfu Hao,Frederic Alexandre,Shan Yu*

Main category: cs.AI

TL;DR: Cognitive flexibility in Visual Large Language Models (VLLMs) is assessed using the Wisconsin Card Sorting Test (WCST). VLLMs achieve human-level set-shifting under chain-of-thought prompting with text-based inputs, but their abilities are influenced by input modality and prompting strategy. Role-playing allows VLLMs to simulate functional deficits related to cognitive flexibility.


<details>
  <summary>Details</summary>
Motivation: To evaluate the cognitive flexibility of state-of-the-art VLLMs using WCST and explore whether these models possess a cognitive architecture similar to the brain.

Method: Assessing the performance of VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) on WCST under different prompting strategies and input modalities, and using role-playing to simulate functional deficits.

Result: VLLMs achieve or surpass human-level set-shifting capabilities under certain conditions, and can simulate cognitive impairments through role-playing.

Conclusion: VLLMs have approached the human level in cognitive flexibility and may possess a cognitive architecture similar to the brain, offering potential for emulating complex brain processes.

Abstract: Cognitive flexibility has been extensively studied in human cognition but
remains relatively unexplored in the context of Visual Large Language Models
(VLLMs). This study assesses the cognitive flexibility of state-of-the-art
VLLMs (GPT-4o, Gemini-1.5 Pro, and Claude-3.5 Sonnet) using the Wisconsin Card
Sorting Test (WCST), a classic measure of set-shifting ability. Our results
reveal that VLLMs achieve or surpass human-level set-shifting capabilities
under chain-of-thought prompting with text-based inputs. However, their
abilities are highly influenced by both input modality and prompting strategy.
In addition, we find that through role-playing, VLLMs can simulate various
functional deficits aligned with patients having impairments in cognitive
flexibility, suggesting that VLLMs may possess a cognitive architecture, at
least regarding the ability of set-shifting, similar to the brain. This study
reveals the fact that VLLMs have already approached the human level on a key
component underlying our higher cognition, and highlights the potential to use
them to emulate complex brain processes.

</details>


### [18] [Lifted Forward Planning in Relational Factored Markov Decision Processes with Concurrent Actions](https://arxiv.org/abs/2505.22147)
*Florian Andreas Marwitz,Tanya Braun,Ralf Möller,Marcel Gehrke*

Main category: cs.AI

TL;DR: 为了应对随对象数量增加而指数级增长的状态和动作空间问题，本文提出了一个一阶表示方法，以多项式而非指数大小存储这些空间，并引入了Foreplan（一种关系前向规划器）来高效计算包含大量不可区分对象和动作的策略。此外，还提出了一种更快的Foreplan近似版本以及如何在给定限制条件下确定代理应操作的对象数量的方法。通过理论分析与实证评估，Foreplan展现出至少四个数量级的速度提升。


<details>
  <summary>Details</summary>
Motivation: 随着不可区分对象数量的增加，状态空间呈指数级增长，传统的枚举方法难以处理如此庞大的状态空间，尤其是在动作空间大小依赖于状态空间大小时，例如允许并发动作的情况下。这使得计算策略变得困难且效率低下，因此需要一种更高效的方法来应对这种指数级膨胀的问题。

Method: 1. 提出了一种一阶表示方法，用于以多项式规模而非指数规模存储状态和动作空间。
2. 引入Foreplan（一种关系前向规划器），利用上述表示方法高效计算涉及众多不可区分对象和动作的策略。
3. 开发了一个更快的Foreplan近似版本。
4. Foreplan能够识别代理为完成特定任务在给定限制下应操作的对象数量。

Result: 1. 理论分析表明Foreplan能够有效应对状态和动作空间的指数级增长。
2. 实证评估显示Foreplan相比传统方法速度提升了至少四个数量级。

Conclusion: Foreplan及其近似版本提供了一种高效解决大规模状态和动作空间问题的方法，尤其适用于包含大量不可区分对象和动作的场景。其显著的速度提升证明了该方法在实际应用中的潜力。

Abstract: Decision making is a central problem in AI that can be formalized using a
Markov Decision Process. A problem is that, with increasing numbers of
(indistinguishable) objects, the state space grows exponentially. To compute
policies, the state space has to be enumerated. Even more possibilities have to
be enumerated if the size of the action space depends on the size of the state
space, especially if we allow concurrent actions. To tackle the exponential
blow-up in the action and state space, we present a first-order representation
to store the spaces in polynomial instead of exponential size in the number of
objects and introduce Foreplan, a relational forward planner, which uses this
representation to efficiently compute policies for numerous indistinguishable
objects and actions. Additionally, we introduce an even faster approximate
version of Foreplan. Moreover, Foreplan identifies how many objects an agent
should act on to achieve a certain task given restrictions. Further, we provide
a theoretical analysis and an empirical evaluation of Foreplan, demonstrating a
speedup of at least four orders of magnitude.

</details>


### [19] [What Makes a Good Reasoning Chain? Uncovering Structural Patterns in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22148)
*Gangwei Jiang,Yahui Liu,Zhaoyi Li,Qi Wang,Fuzheng Zhang,Linqi Song,Ying Wei,Defu Lian*

Main category: cs.AI

TL;DR: 近期关于大型语言模型（LLMs）推理的研究推动了长链思维（LCoT）策略的发展。本文提出了一种名为LCoT2Tree的自动化框架，可以将顺序LCoTs转换为分层树结构，从而实现对LLM推理的更深入结构分析。通过图神经网络（GNNs），我们发现由LCoT2Tree提取的结构模式（包括探索、回溯和验证）是预测最终表现的更强指标。此外，利用可解释性技术，我们进一步确定了导致失败的关键思维模式，例如过度分支。LCoT2Tree支持实际应用，如提高最佳N解码的有效性。总体而言，我们的结果强调了推理链内部结构的关键作用，使LCoT2Tree成为诊断、解释和改进LLMs推理的强大工具。


<details>
  <summary>Details</summary>
Motivation: 尽管长链思维（LCoT）在复杂任务中实现了专家级的表现，但其推理链的内部结构如何驱动甚至预测最终答案的正确性仍然是一个关键且尚未充分研究的问题。

Method: 提出了LCoT2Tree，一种将顺序LCoTs转换为分层树结构的自动化框架，以进行更深入的结构分析。使用图神经网络（GNNs）揭示结构模式，并通过可解释性技术识别关键思维模式。

Result: 结构模式（如探索、回溯和验证）是更强的性能预测指标，能够解释推理失败的原因，并支持实际应用，如改进解码效果。

Conclusion: 推理链的内部结构在LLM推理中起着关键作用，LCoT2Tree是一个强大的工具，用于诊断、解释和改进LLM推理。

Abstract: Recent advances in reasoning with large language models (LLMs) have
popularized Long Chain-of-Thought (LCoT), a strategy that encourages deliberate
and step-by-step reasoning before producing a final answer. While LCoTs have
enabled expert-level performance in complex tasks, how the internal structures
of their reasoning chains drive, or even predict, the correctness of final
answers remains a critical yet underexplored question. In this work, we present
LCoT2Tree, an automated framework that converts sequential LCoTs into
hierarchical tree structures and thus enables deeper structural analysis of LLM
reasoning. Using graph neural networks (GNNs), we reveal that structural
patterns extracted by LCoT2Tree, including exploration, backtracking, and
verification, serve as stronger predictors of final performance across a wide
range of tasks and models. Leveraging an explainability technique, we further
identify critical thought patterns such as over-branching that account for
failures. Beyond diagnostic insights, the structural patterns by LCoT2Tree
support practical applications, including improving Best-of-N decoding
effectiveness. Overall, our results underscore the critical role of internal
structures of reasoning chains, positioning LCoT2Tree as a powerful tool for
diagnosing, interpreting, and improving reasoning in LLMs.

</details>


### [20] [A Preprocessing Framework for Efficient Approximate Bi-Objective Shortest-Path Computation in the Presence of Correlated Objectives](https://arxiv.org/abs/2505.22244)
*Yaron Halle,Ariel Felner,Sven Koenig,Oren Salzman*

Main category: cs.AI

TL;DR: The paper presents an efficient algorithm for solving the bi-objective shortest-path (BOSP) problem with correlated objectives, inspired by graph-clustering algorithms. It reduces search effort and runs up to five times faster on benchmark instances compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The BOSP problem is computationally challenging due to the exponential size of the search space. Existing bounded sub-optimal solvers like A*pex can approximate Pareto-optimal solutions but do not exploit correlations between objectives. The authors aim to develop a more efficient algorithm that leverages such correlations to reduce search effort.

Method: The proposed method uses a preprocessing phase to identify correlated clusters within a graph and generate a new graph representation. This allows a generalization of A*pex to run faster while maintaining theoretical guarantees on solution quality.

Result: The algorithm runs up to five times faster on DIMACS dataset instances, a standard benchmark in the field. It effectively exploits correlations in bi-objective search while providing theoretical guarantees on solution quality.

Conclusion: This is the first algorithm that efficiently and effectively exploits correlations in the context of bi-objective search with theoretical guarantees. It offers significant speed improvements over existing methods.

Abstract: The bi-objective shortest-path (BOSP) problem seeks to find paths between
start and target vertices of a graph while optimizing two conflicting objective
functions. We consider the BOSP problem in the presence of correlated
objectives. Such correlations often occur in real-world settings such as road
networks, where optimizing two positively correlated objectives, such as travel
time and fuel consumption, is common. BOSP is generally computationally
challenging as the size of the search space is exponential in the number of
objective functions and the graph size. Bounded sub-optimal BOSP solvers such
as A*pex alleviate this complexity by approximating the Pareto-optimal solution
set rather than computing it exactly (given a user-provided approximation
factor). As the correlation between objective functions increases, smaller
approximation factors are sufficient for collapsing the entire Pareto-optimal
set into a single solution. We leverage this insight to propose an efficient
algorithm that reduces the search effort in the presence of correlated
objectives. Our approach for computing approximations of the entire
Pareto-optimal set is inspired by graph-clustering algorithms. It uses a
preprocessing phase to identify correlated clusters within a graph and to
generate a new graph representation. This allows a natural generalization of
A*pex to run up to five times faster on DIMACS dataset instances, a standard
benchmark in the field. To the best of our knowledge, this is the first
algorithm proposed that efficiently and effectively exploits correlations in
the context of bi-objective search while providing theoretical guarantees on
solution quality.

</details>


### [21] [Compression versus Accuracy: A Hierarchy of Lifted Models](https://arxiv.org/abs/2505.22288)
*Jan Speller,Malte Luttermann,Marcel Gehrke,Tanya Braun*

Main category: cs.AI

TL;DR: 本文提出了一种无超参数的分层方法用于提升模型构建，解决了现有方法中需要多次尝试不同$\varepsilon$值的问题，并提供了压缩与准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的Advanced Colour Passing (ACP)算法在使用近似版本时，需要手动调整超参数$\varepsilon$，这可能导致大量的探索和模型解释性降低。

Method: 提出了一种分层方法，该方法无需超参数，能够高效计算一系列$\varepsilon$值，确保模型的层次结构，即对于某个$\varepsilon$值分组的因子，在更大的$\varepsilon$值下也会保持分组。同时，这种方法还提供了误差界的层次结构。

Result: 新方法允许明确权衡模型的压缩程度与准确性，并增强了不同模型间的可解释性。

Conclusion: 分层方法为提升模型构建提供了一种新的解决方案，避免了对超参数的依赖，提高了模型的效率和解释性。

Abstract: Probabilistic graphical models that encode indistinguishable objects and
relations among them use first-order logic constructs to compress a
propositional factorised model for more efficient (lifted) inference. To obtain
a lifted representation, the state-of-the-art algorithm Advanced Colour Passing
(ACP) groups factors that represent matching distributions. In an approximate
version using $\varepsilon$ as a hyperparameter, factors are grouped that
differ by a factor of at most $(1\pm \varepsilon)$. However, finding a suitable
$\varepsilon$ is not obvious and may need a lot of exploration, possibly
requiring many ACP runs with different $\varepsilon$ values. Additionally,
varying $\varepsilon$ can yield wildly different models, leading to decreased
interpretability. Therefore, this paper presents a hierarchical approach to
lifted model construction that is hyperparameter-free. It efficiently computes
a hierarchy of $\varepsilon$ values that ensures a hierarchy of models, meaning
that once factors are grouped together given some $\varepsilon$, these factors
will be grouped together for larger $\varepsilon$ as well. The hierarchy of
$\varepsilon$ values also leads to a hierarchy of error bounds. This allows for
explicitly weighing compression versus accuracy when choosing specific
$\varepsilon$ values to run ACP with and enables interpretability between the
different models.

</details>


### [22] [Capability-Based Scaling Laws for LLM Red-Teaming](https://arxiv.org/abs/2505.20162)
*Alexander Panfilov,Paul Kassianik,Maksym Andriushchenko,Jonas Geiping*

Main category: cs.AI

TL;DR: 随着大语言模型的能力和自主性增强，识别漏洞对于安全部署至关重要。然而，传统的提示工程方法可能在红队测试转变为弱到强问题时无效。本文通过能力差距的视角研究红队测试，并评估超过500个攻击者-目标对。结果表明，更有能力的模型是更好的攻击者，一旦目标的能力超过攻击者，攻击成功率会急剧下降。此外，攻击成功率与MMLU-Pro基准的社会科学部分的高性能相关。基于这些趋势，我们推导出一个越狱扩展定律，预测固定目标的攻击成功基于攻击者-目标能力差距。这些发现表明，固定能力的攻击者（如人类）可能在未来模型中变得无效，开源模型的风险增加，模型提供者必须准确测量和控制模型的说服和操纵能力以限制其作为攻击者的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的能力和自主性增强，识别漏洞对于安全部署至关重要。然而，传统的提示工程方法可能在红队测试转变为弱到强问题时无效，需要研究攻击者和目标之间的能力差距。

Method: 评估超过500个攻击者-目标对，使用基于LLM的越狱攻击模拟人类红队成员，涵盖不同的家族、大小和能力水平。分析攻击成功率与目标和攻击者能力的关系，以及与MMLU-Pro基准的社会科学部分的高性能的相关性。

Result: 更有能力的模型是更好的攻击者，攻击成功率在目标能力超过攻击者时急剧下降，攻击成功率与MMLU-Pro基准的社会科学部分的高性能相关。

Conclusion: 固定能力的攻击者（如人类）可能在未来模型中变得无效，开源模型的风险增加，模型提供者必须准确测量和控制模型的说服和操纵能力以限制其作为攻击者的有效性。

Abstract: As large language models grow in capability and agency, identifying
vulnerabilities through red-teaming becomes vital for safe deployment. However,
traditional prompt-engineering approaches may prove ineffective once
red-teaming turns into a weak-to-strong problem, where target models surpass
red-teamers in capabilities. To study this shift, we frame red-teaming through
the lens of the capability gap between attacker and target. We evaluate more
than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic
human red-teamers across diverse families, sizes, and capability levels. Three
strong trends emerge: (i) more capable models are better attackers, (ii) attack
success drops sharply once the target's capability exceeds the attacker's, and
(iii) attack success rates correlate with high performance on social science
splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking
scaling law that predicts attack success for a fixed target based on
attacker-target capability gap. These findings suggest that fixed-capability
attackers (e.g., humans) may become ineffective against future models,
increasingly capable open-source models amplify risks for existing systems, and
model providers must accurately measure and control models' persuasive and
manipulative abilities to limit their effectiveness as attackers.

</details>


### [23] [Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling](https://arxiv.org/abs/2505.22290)
*Fanzeng Xia,Yidong Luo,Tinko Sebastian Bartels,Yaqi Xu,Tongxin Li*

Main category: cs.AI

TL;DR: 通过结合上下文搜索和测试时扩展，可以在超难推理任务上实现性能突破，挑战了关于大语言模型在复杂任务上的限制的现有假设。


<details>
  <summary>Details</summary>
Motivation: 现有的文献大多依赖于直接提示和简单的上下文学习示例进行评估，忽略了先进的技术来激发大语言模型的深思熟虑推理，这可能导致过早得出大语言模型性能达到上限的结论。

Method: 系统地探索上下文搜索和测试时扩展在超级困难推理任务上的联合潜力，使用先进的上下文搜索提示和内部扩展相结合的方法。

Result: 在受控的NP难问题和复杂的现实世界规划基准上，该方法相比之前报道的结果（没有任何外部机制）成功率达到30倍的提升，并且理论上证明这种方法显著扩展了可解推理问题的复杂性类别。

Conclusion: 当前的大语言模型推理评估范式系统性地低估了它们的真实潜力，需要重新评估如何对大语言模型推理进行基准测试，并采用更强大的评估策略以全面捕捉当代大语言模型的真实能力。

Abstract: Recent research has highlighted that Large Language Models (LLMs), even when
trained to generate extended long reasoning steps, still face significant
challenges on hard reasoning problems. However, much of the existing literature
relies on direct prompting with simple in-context learning examples for
evaluation, which largely overlooks advanced techniques to elicit LLMs'
deliberate reasoning before drawing conclusions that LLMs hit a performance
ceiling. In this paper, we systematically explore the combined potential of
in-context search and test-time scaling on super hard reasoning tasks. We find
that by employing advanced in-context search prompting to LLMs augmented with
internal scaling, one can achieve transformative performance breakthroughs on
tasks previously deemed "unsolvable" (e.g., reported success rates below 5%).
We provide both empirical results and theoretical analysis of how this
combination can unleash LLM reasoning capabilities: i) Empirically, on
controlled NP-hard tasks and complex real-world planning benchmarks, our
approach achieves up to a 30x improvement in success rates compared to
previously reported results without any external mechanisms; ii) Theoretically,
we show that in-context search prompting, when combined with internal scaling,
significantly extends the complexity class of solvable reasoning problems.
These findings challenge prevailing assumptions about the limitations of LLMs
on complex tasks, indicating that current evaluation paradigms systematically
underestimate their true potential. Our work calls for a critical reassessment
of how LLM reasoning is benchmarked and a more robust evaluation strategy that
fully captures the true capabilities of contemporary LLMs, which can lead to a
better understanding of their operational reasoning boundaries in real-world
deployments.

</details>


### [24] [From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications](https://arxiv.org/abs/2505.22311)
*Feibo Jiang,Cunhua Pan,Li Dong,Kezhi Wang,Octavia A. Dobre,Merouane Debbah*

Main category: cs.AI

TL;DR: 随着6G通信的到来，智能通信系统面临多重挑战。本文提供了大型人工智能模型（LAMs）和代理型AI技术在智能通信系统中的原则、设计和应用的系统性介绍。首先概述了6G通信的背景，回顾了从LAMs到代理型AI的技术演变，并明确了教程的动机和主要贡献。接着全面回顾了构建LAMs的关键组件，分类分析了不同类型的LAMs及其适用性。然后提出了以LAM为核心的通信设计范式，包括数据集构建和内外部学习方法。基于此，开发了一个基于LAM的代理型AI系统，介绍了其核心组件及交互机制。还引入了一个多代理框架，涵盖了6G的数据检索、协作规划和反思评估。最后总结了当前研究面临的挑战和未来方向，支持下一代高效、安全和可持续的智能通信系统的发展。


<details>
  <summary>Details</summary>
Motivation: 智能通信系统在感知与响应能力、可扩展性和动态环境适应性方面存在限制，需要先进的技术支持以应对6G通信带来的挑战。

Method: 1. 系统性介绍LAMs和代理型AI技术；2. 回顾技术演变并分析LAMs的关键组件；3. 提出以LAM为核心的通信设计范式；4. 开发基于LAM的代理型AI系统；5. 引入多代理框架；6. 概述LAMs和代理型AI在通信场景中的应用；7. 总结研究挑战和未来方向。

Result: 提供了对LAMs和代理型AI技术的全面理解，明确了它们在智能通信系统中的应用潜力，并指出了未来的研究方向。

Conclusion: 通过利用LAMs和代理型AI技术，可以有效应对6G通信系统中的挑战，推动高效、安全和可持续的下一代智能通信系统的发展。

Abstract: With the advent of 6G communications, intelligent communication systems face
multiple challenges, including constrained perception and response
capabilities, limited scalability, and low adaptability in dynamic
environments. This tutorial provides a systematic introduction to the
principles, design, and applications of Large Artificial Intelligence Models
(LAMs) and Agentic AI technologies in intelligent communication systems, aiming
to offer researchers a comprehensive overview of cutting-edge technologies and
practical guidance. First, we outline the background of 6G communications,
review the technological evolution from LAMs to Agentic AI, and clarify the
tutorial's motivation and main contributions. Subsequently, we present a
comprehensive review of the key components required for constructing LAMs. We
further categorize LAMs and analyze their applicability, covering Large
Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models
(LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a
LAM-centric design paradigm tailored for communications, encompassing dataset
construction and both internal and external learning approaches. Building upon
this, we develop an LAM-based Agentic AI system for intelligent communications,
clarifying its core components such as planners, knowledge bases, tools, and
memory modules, as well as its interaction mechanisms. We also introduce a
multi-agent framework with data retrieval, collaborative planning, and
reflective evaluation for 6G. Subsequently, we provide a detailed overview of
the applications of LAMs and Agentic AI in communication scenarios. Finally, we
summarize the research challenges and future directions in current studies,
aiming to support the development of efficient, secure, and sustainable
next-generation intelligent communication systems.

</details>


### [25] [AgentDNS: A Root Domain Naming System for LLM Agents](https://arxiv.org/abs/2505.22368)
*Enfang Cui,Yujun Cheng,Rui She,Dan Liu,Zhiyuan Liang,Minxin Guo,Tianzheng Li,Qian Wei,Wenjuan Xing,Zhijie Zhong*

Main category: cs.AI

TL;DR: This paper proposes AgentDNS, a system inspired by traditional DNS, to enable LLM agents to autonomously discover, resolve, and securely invoke third-party services across different vendors.


<details>
  <summary>Details</summary>
Motivation: The rapid evolution of LLM agents has brought challenges in cross-vendor service discovery, interoperability, and communication. Current protocols have not fully addressed the need for standardized solutions in this area.

Method: AgentDNS is proposed as a root domain naming and service discovery system. It includes mechanisms for service registration, semantic service discovery, secure invocation, and unified billing, all designed to facilitate multi-agent collaboration.

Result: The architecture, core functionalities, and use cases of AgentDNS are detailed, showing its potential to improve multi-agent collaboration in real-world scenarios.

Conclusion: AgentDNS aims to address the lack of standardized protocols for service discovery across different agent and tool vendors, and its source code will be published on GitHub.

Abstract: The rapid evolution of Large Language Model (LLM) agents has highlighted
critical challenges in cross-vendor service discovery, interoperability, and
communication. Existing protocols like model context protocol and
agent-to-agent protocol have made significant strides in standardizing
interoperability between agents and tools, as well as communication among
multi-agents. However, there remains a lack of standardized protocols and
solutions for service discovery across different agent and tool vendors. In
this paper, we propose AgentDNS, a root domain naming and service discovery
system designed to enable LLM agents to autonomously discover, resolve, and
securely invoke third-party agent and tool services across organizational and
technological boundaries. Inspired by the principles of the traditional DNS,
AgentDNS introduces a structured mechanism for service registration, semantic
service discovery, secure invocation, and unified billing. We detail the
architecture, core functionalities, and use cases of AgentDNS, demonstrating
its potential to streamline multi-agent collaboration in real-world scenarios.
The source code will be published on https://github.com/agentdns.

</details>


### [26] [AI Mathematician: Towards Fully Automated Frontier Mathematical Research](https://arxiv.org/abs/2505.22451)
*Yuanhang Liu,Yanxing Huang,Yanqiao Wang,Peng Li,Yang Liu*

Main category: cs.AI

TL;DR: This paper introduces AI Mathematician (AIM) framework that uses Large Reasoning Models (LRMs) to aid in frontier mathematical research. AIM addresses the complexity of research problems and requirement of procedural rigor through an exploration mechanism and pessimistic reasonable verification method. Experiments show AIM's strong capability in constructing proofs and discovering insights in real-world mathematical topics, suggesting the potential for LRM-based systems to accelerate future mathematical research.


<details>
  <summary>Details</summary>
Motivation: To utilize the reasoning strength of LRMs for supporting frontier mathematical research beyond competition-level problems, addressing the intrinsic complexity of research problems and the requirement of procedural rigor.

Method: The AIM framework incorporates two core strategies: an exploration mechanism fostering longer solution paths and a pessimistic reasonable verification method ensuring reliability.

Result: AIM shows strong capability in tackling research-level tasks, autonomously constructing substantial portions of proofs and uncovering non-trivial insights in several real-world mathematical topics.

Conclusion: The findings highlight the potential of LRMs in mathematical discovery and suggest that LRM-based agent systems could significantly accelerate mathematical research in the future.

Abstract: Large Reasoning Models (LRMs) have made significant progress in mathematical
capabilities in recent times. However, these successes have been primarily
confined to competition-level problems. In this work, we propose AI
Mathematician (AIM) framework, which harnesses the reasoning strength of LRMs
to support frontier mathematical research. We have identified two critical
challenges of mathematical research compared to competition, {\it the intrinsic
complexity of research problems} and {\it the requirement of procedural rigor}.
To address these challenges, AIM incorporates two core strategies: an
exploration mechanism to foster longer solution paths, and the pessimistic
reasonable verification method to ensure reliability.
  This early version of AIM already exhibits strong capability in tackling
research-level tasks. We conducted extensive experiments across several
real-world mathematical topics and obtained promising results. AIM is able to
autonomously construct substantial portions of proofs and uncover non-trivial
insights within each research area. These findings highlight the potential of
LRMs in mathematical discovery and suggest that LRM-based agent systems could
significantly accelerate mathematical research in the future.

</details>


### [27] [HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym](https://arxiv.org/abs/2505.22597)
*Ngoc La,Ruaridh Mon-Williams,Julie A. Shah*

Main category: cs.AI

TL;DR: The paper introduces HDDLGym, a tool that integrates hierarchical planning via HDDL with reinforcement learning environments from OpenAI Gym, supporting multi-agent scenarios and collaborative planning.


<details>
  <summary>Details</summary>
Motivation: There is a need for seamless integration of hierarchical planning with reinforcement learning (RL) methods, as current tools lack this capability.

Method: HDDLGym is developed to automatically generate OpenAI Gym environments from HDDL domains and problems, bridging the gap between RL and hierarchical planning. It supports multi-agent scenarios and collaborative planning among agents.

Result: The paper provides an overview of HDDLGym's design and implementation, along with detailed instructions and demonstrations for using the framework in various domains, such as Transport and Overcooked.

Conclusion: HDDLGym is positioned as a valuable tool for studying reinforcement learning within the context of hierarchical planning, especially in multi-agent settings.

Abstract: In recent years, reinforcement learning (RL) methods have been widely tested
using tools like OpenAI Gym, though many tasks in these environments could also
benefit from hierarchical planning. However, there is a lack of a tool that
enables seamless integration of hierarchical planning with RL. Hierarchical
Domain Definition Language (HDDL), used in classical planning, introduces a
structured approach well-suited for model-based RL to address this gap. To
bridge this integration, we introduce HDDLGym, a Python-based tool that
automatically generates OpenAI Gym environments from HDDL domains and problems.
HDDLGym serves as a link between RL and hierarchical planning, supporting
multi-agent scenarios and enabling collaborative planning among agents. This
paper provides an overview of HDDLGym's design and implementation, highlighting
the challenges and design choices involved in integrating HDDL with the Gym
interface, and applying RL policies to support hierarchical planning. We also
provide detailed instructions and demonstrations for using the HDDLGym
framework, including how to work with existing HDDL domains and problems from
International Planning Competitions, exemplified by the Transport domain.
Additionally, we offer guidance on creating new HDDL domains for multi-agent
scenarios and demonstrate the practical use of HDDLGym in the Overcooked
domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a
valuable tool for studying RL in hierarchical planning, particularly in
multi-agent contexts.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows](https://arxiv.org/abs/2505.21512)
*Harry Li,Gabriel Appleby,Kenneth Alperin,Steven R Gomez,Ashley Suh*

Main category: cs.LG

TL;DR: LinkQ is a KG exploration system that uses LLMs to convert natural language questions into structured queries, with five visual mechanisms designed to help users evaluate the accuracy of both KG queries and LLM responses. However, even KG experts tend to overtrust LinkQ's outputs due to its 'helpful' visualizations.


<details>
  <summary>Details</summary>
Motivation: To study the effects of using LLMs with KGs on user trust, exploration strategies, or downstream decision-making, which raises key design challenges for LLM-based KG visual analysis systems.

Method: Developed LinkQ, a KG exploration system that converts natural language questions into structured queries with an LLM, and collaborated with KG experts to design five visual mechanisms to help users assess the accuracy of both KG queries and LLM responses.

Result: From a qualitative evaluation with 14 practitioners, it was found that users - even KG experts - tended to overtrust LinkQ's outputs due to its 'helpful' visualizations, even when the LLM was incorrect. Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs.

Conclusion: The findings highlight the risks of false trust in LLM-assisted data analysis tools and emphasize the need for further investigation into the role of visualization as a mitigation technique.

Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them
effectively remains difficult for even expert users. Large language models
(LLMs) are increasingly used to address this gap, yet little is known
empirically about how their usage with KGs shapes user trust, exploration
strategies, or downstream decision-making - raising key design challenges for
LLM-based KG visual analysis systems. To study these effects, we developed
LinkQ, a KG exploration system that converts natural language questions into
structured queries with an LLM. We collaborated with KG experts to design five
visual mechanisms that help users assess the accuracy of both KG queries and
LLM responses: an LLM-KG state diagram that illustrates which stage of the
exploration pipeline LinkQ is in, a query editor displaying the generated query
paired with an LLM explanation, an entity-relation ID table showing extracted
KG entities and relations with semantic descriptions, a query structure graph
that depicts the path traversed in the KG, and an interactive graph
visualization of query results. From a qualitative evaluation with 14
practitioners, we found that users - even KG experts - tended to overtrust
LinkQ's outputs due to its "helpful" visualizations, even when the LLM was
incorrect. Users exhibited distinct workflows depending on their prior
familiarity with KGs and LLMs, challenging the assumption that these systems
are one-size-fits-all - despite often being designed as if they are. Our
findings highlight the risks of false trust in LLM-assisted data analysis tools
and the need for further investigation into the role of visualization as a
mitigation technique.

</details>


### [29] [SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation](https://arxiv.org/abs/2505.21514)
*Mingchao Jiang,Abhinav Jain,Sophia Zorek,Chris Jermaine*

Main category: cs.LG

TL;DR: The paper introduces SIMCOPILOT, a benchmark simulating LLMs as coding assistants. It evaluates LLMs' coding capabilities in Java and Python through dedicated sub-benchmarks, providing detailed analyses of their performance nuances.


<details>
  <summary>Details</summary>
Motivation: To establish a realistic evaluation environment for assessing LLM utility in practical coding scenarios and address overlooked factors in existing benchmarks.

Method: SIMCOPILOT comprises sub-benchmarks for Java (SIMCOPILOTJ) and Python (SIMCOPILOTP), covering diverse codebases varying in size and complexity. It targets both completion and infill tasks, offering fine-grained analyses on task-specific performance, contextual understanding, and sensitivity to variable scope.

Result: Evaluations across domains reveal insights into model strengths and highlight challenges in maintaining logical consistency within complex dependency structures. The study also sheds light on current limitations of LLM-driven code generation.

Conclusion: SIMCOPILOT provides a comprehensive framework for evaluating LLM coding capabilities, marking the transition of LLMs from syntax-aware generators to reliable, intelligent software development partners.

Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large
language models (LLMs) as interactive, "copilot"-style coding assistants.
Targeting both completion (finishing incomplete methods or code blocks) and
infill tasks (filling missing segments within existing code), SIMCOPILOT
provides a comprehensive framework for evaluating LLM coding capabilities. The
benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python
(SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our
key contributions include: (a) establishing a realistic, detailed evaluation
environment to assess LLM utility in practical coding scenarios, and (b)
providing fine-grained analyses that address critical factors frequently
overlooked by existing benchmarks, such as task-specific performance nuances,
contextual understanding across code segments, and sensitivity to variable
scope. Evaluations conducted across domains-including algorithms, databases,
computer vision, and neural networks-offer insights into model strengths and
highlight persistent challenges in maintaining logical consistency within
complex dependency structures. Beyond benchmarking, our study sheds light on
the current limitations of LLM-driven code generation and underscores the
ongoing transition of LLMs from merely syntax-aware generators toward reliable,
intelligent software development partners.

</details>


### [30] [Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation](https://arxiv.org/abs/2505.21525)
*Peiliang Gong,Yucheng Wang,Min Wu,Zhenghua Chen,Xiaoli Li,Daoqiang Zhang*

Main category: cs.LG

TL;DR: TERSE是一种新的SFDA方法，专为MTS数据设计，通过时空特征编码器和任务来捕捉时空特性，促进跨域的隐式特征对齐。实验表明其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有的SFDA方法在处理多变量时间序列数据时效果不佳，因为它们未能考虑到MTS数据中固有的空间相关性，而这些相关性对于准确表示数据和跨域保留不变信息至关重要。

Method: TERSE包括一个定制的空间时间特征编码器，用于捕获潜在的空间时间特性，并结合时间和空间恢复任务，以重建被时间屏蔽的时间序列的潜在表示和被空间屏蔽的相关结构。在目标适应阶段，目标编码器通过利用源预训练的时间恢复和空间重连网络，生成与源域在空间和时间上一致的特征。

Result: 广泛的实验展示了TERSE的有效性和通用性，特别是在三个真实世界的时间序列数据集上的应用。

Conclusion: TERSE可以有效地建模和转移跨域的空间时间依赖关系，促进隐式特征对齐，并且可以作为一个多功能的即插即用模块集成到已建立的SFDA方法中。

Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from
an annotated source domain to an unlabelled target domain without accessing the
source data, thereby preserving data privacy. While existing SFDA methods have
proven effective in reducing reliance on source data, they struggle to perform
well on multivariate time series (MTS) due to their failure to consider the
intrinsic spatial correlations inherent in MTS data. These spatial correlations
are crucial for accurately representing MTS data and preserving invariant
information across domains. To address this challenge, we propose Temporal
Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method
tailored for MTS data. Specifically, TERSE comprises a customized
spatial-temporal feature encoder designed to capture the underlying
spatial-temporal characteristics, coupled with both temporal restoration and
spatial rewiring tasks to reinstate latent representations of the temporally
masked time series and the spatially masked correlated structures. During the
target adaptation phase, the target encoder is guided to produce spatially and
temporally consistent features with the source domain by leveraging the source
pre-trained temporal restoration and spatial rewiring networks. Therefore,
TERSE can effectively model and transfer spatial-temporal dependencies across
domains, facilitating implicit feature alignment. In addition, as the first
approach to simultaneously consider spatial-temporal consistency in MTS-SFDA,
TERSE can also be integrated as a versatile plug-and-play module into
established SFDA methods. Extensive experiments on three real-world time series
datasets demonstrate the effectiveness and versatility of our approach.

</details>


### [31] [ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools](https://arxiv.org/abs/2505.21569)
*Zhucong Li,Bowei Zhang,Jin Xiao,Zhijian Zhou,Fenglei Cao,Jiaqing Liang,Yuan Qi*

Main category: cs.LG

TL;DR: Large Language Model (LLM)-based agents can be used to reduce prediction errors in chemistry tools.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents have shown potential in improving performance for chemistry-related tasks, but their effectiveness is still constrained by the prediction errors of chemistry tools.

Method: Propose ChemHAS, a method that enhances chemistry tools by optimizing agent-stacking structures from limited data.

Result: ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks and identifies four distinct agent-stacking behaviors.

Conclusion: ChemHAS effectively compensates for prediction errors of chemistry tools and improves interpretability, opening new possibilities for AI agent applications in scientific research.

Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to
improve performance in chemistry-related tasks by selecting appropriate tools.
However, their effectiveness remains limited by the inherent prediction errors
of chemistry tools. In this paper, we take a step further by exploring how
LLMbased agents can, in turn, be leveraged to reduce prediction errors of the
tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),
a simple yet effective method that enhances chemistry tools through optimizing
agent-stacking structures from limited data. ChemHAS achieves state-of-the-art
performance across four fundamental chemistry tasks, demonstrating that our
method can effectively compensate for prediction errors of the tools.
Furthermore, we identify and characterize four distinct agent-stacking
behaviors, potentially improving interpretability and revealing new
possibilities for AI agent applications in scientific research. Our code and
dataset are publicly available at https:
//anonymous.4open.science/r/ChemHAS-01E4/README.md.

</details>


### [32] [FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition](https://arxiv.org/abs/2505.21571)
*Yao Lu,Tengfei Ma,Zeyu Wang,Zhuangzhi Chen,Dongwei Xu,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.LG

TL;DR: FCOS是一种新的细到粗的两阶段剪枝框架，结合通道级剪枝与层坍缩诊断，实现极致压缩、高性能和高效推理。在AMR基准测试中表现出色，显著减少了FLOPs和参数，性能接近原始ResNet56。


<details>
  <summary>Details</summary>
Motivation: 传统手动调制识别方法在现代场景中难以提取可靠信号特征并满足实时需求，而深度学习方法虽然提高了分类准确性，但其大模型尺寸和高计算需求限制了在资源受限设备上的部署。现有的剪枝技术在压缩率、硬件加速和精度保持之间存在权衡。

Method: 引入了FCOS框架，包含两个阶段：第一阶段使用分层聚类和参数融合对通道权重进行通道级剪枝；第二阶段利用层坍缩诊断模块通过线性探测识别并移除因高通道压缩比而坍缩的层。

Result: 在多个AMR基准上实验表明，FCOS优于现有的通道和层剪枝方法。具体而言，FCOS实现了95.51%的FLOPs减少和95.31%的参数减少，同时在Sig2019-12上仅损失0.46%的准确率，性能接近原始ResNet56。

Conclusion: FCOS框架在实现高模型压缩率的同时保持了较高的性能，适用于资源受限设备上的自动调制识别任务。

Abstract: With the rapid development of wireless communications and the growing
complexity of digital modulation schemes, traditional manual modulation
recognition methods struggle to extract reliable signal features and meet
real-time requirements in modern scenarios. Recently, deep learning based
Automatic Modulation Recognition (AMR) approaches have greatly improved
classification accuracy. However, their large model sizes and high
computational demands hinder deployment on resource-constrained devices. Model
pruning provides a general approach to reduce model complexity, but existing
weight, channel, and layer pruning techniques each present a trade-off between
compression rate, hardware acceleration, and accuracy preservation. To this
end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning
framework that combines channel-level pruning with layer-level collapse
diagnosis to achieve extreme compression, high performance and efficient
inference. In the first stage of FCOS, hierarchical clustering and parameter
fusion are applied to channel weights to achieve channel-level pruning. Then a
Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer
collapse and removes the collapsed layers due to high channel compression
ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms
existing channel and layer pruning methods. Specifically, FCOS achieves 95.51%
FLOPs reduction and 95.31% parameter reduction while still maintaining
performance close to the original ResNet56, with only a 0.46% drop in accuracy
on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.

</details>


### [33] [Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes](https://arxiv.org/abs/2505.21573)
*Han Wan,Rui Zhang,Hao Sun*

Main category: cs.LG

TL;DR: SINO is a novel framework that learns PDE operators from limited trajectories without known PDE terms, achieving state-of-the-art results across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Classical numerical solvers are accurate but require fine discretization and full knowledge of the governing PDEs. Data-driven neural PDE solvers learn from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes.

Method: Propose Spectral-inspired Neural Operator (SINO), which operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. Design a nonlinear operator block including a Π-Block with low-pass filtering to prevent aliasing. Introduce an operator distillation technique for efficient inference.

Result: Achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. Can accurately simulate globally coupled systems (e.g., Navier-Stokes equations) from limited data without explicit PDE terms.

Conclusion: SINO is the first physics-aware method capable of accurately simulating globally coupled systems from limited data without any explicit PDE terms.

Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of
various physical systems. Classical numerical solvers, while accurate, require
fine discretization and full knowledge of the governing PDEs, limiting their
applicability when the physics is unknown or fast inference is required.
Data-driven neural PDE solvers alleviate these constraints by learning from
data but demand large training datasets and perform poorly in data-scarce
regimes. Physics-aware methods mitigate data requirements by incorporating
physical knowledge yet rely on known PDE terms or local numerical schemes,
restricting their ability to handle unknown or globally coupled systems. In
this work, we propose the Spectral-inspired Neural Operator (SINO), a novel
framework that learns PDE operators from limited trajectories (as few as 2-5),
without any known PDE terms. SINO operates in the frequency domain and
introduces a Frequency-to-Vector module to learn spectral representations
analogous to derivative multipliers. To model nonlinear physical interactions,
we design a nonlinear operator block that includes a $\Pi$-Block with low-pass
filtering to prevent aliasing. Finally, we introduce an operator distillation
technique to distill the trained model for efficient inference. SINO achieves
state-of-the-art results across multiple PDE benchmarks, demonstrating strong
discretization invariance and robust generalization to out-of-distribution
initial conditions. To our knowledge, SINO is the first physics-aware method
capable of accurately simulating globally coupled systems (e.g., the
Navier-Stokes equations) from limited data without any explicit PDE terms.

</details>


### [34] [Concentration Distribution Learning from Label Distributions](https://arxiv.org/abs/2505.21576)
*Jiawei Tang,Yuheng Jia*

Main category: cs.LG

TL;DR: This paper proposes an improved paradigm called concentration distribution learning that introduces the concept of background concentration to address the limitations of label distribution learning (LDL). It presents a novel model using probabilistic methods and neural networks for learning label distributions and background concentrations, achieving more accurate predictions than existing LDL methods.


<details>
  <summary>Details</summary>
Motivation: The limitation of traditional label distribution learning (LDL) lies in its inability to represent the absolute intensity of each label, leading to information loss and confusion in instances. This motivates the need for a new approach that can capture both relative and absolute description degrees of labels.

Method: The authors introduce the concept of 'background concentration' as the absolute description degree term of the label distribution. They incorporate this into the LDL process to form the improved paradigm of concentration distribution learning. A novel model combining probabilistic methods and neural networks is proposed to learn label distributions and background concentrations from existing LDL datasets.

Result: Extensive experiments demonstrate that the proposed approach can successfully extract background concentrations from label distributions, resulting in more accurate prediction outcomes compared to state-of-the-art LDL methods.

Conclusion: The introduction of background concentration significantly enhances the performance of label distribution learning by addressing its inherent limitations. The proposed concentration distribution learning paradigm and associated model provide a promising direction for future research.

Abstract: Label distribution learning (LDL) is an effective method to predict the
relative label description degree (a.k.a. label distribution) of a sample.
However, the label distribution is not a complete representation of an instance
because it overlooks the absolute intensity of each label. Specifically, it's
impossible to obtain the total description degree of hidden labels that not in
the label space, which leads to the loss of information and confusion in
instances. To solve the above problem, we come up with a new concept named
background concentration to serve as the absolute description degree term of
the label distribution and introduce it into the LDL process, forming the
improved paradigm of concentration distribution learning. Moreover, we propose
a novel model by probabilistic methods and neural networks to learn label
distributions and background concentrations from existing LDL datasets.
Extensive experiments prove that the proposed approach is able to extract
background concentrations from label distributions while producing more
accurate prediction results than the state-of-the-art LDL methods. The code is
available in https://github.com/seutjw/CDL-LD.

</details>


### [35] [Fairness in Federated Learning: Fairness for Whom?](https://arxiv.org/abs/2505.21584)
*Afaf Taik,Khaoula Chehbouni,Golnoosh Farnadi*

Main category: cs.LG

TL;DR: Fairness in federated learning (FL) research often focuses narrowly on system-level metrics, ignoring broader sociotechnical contexts and stakeholder impacts. This paper critiques existing approaches, identifies five key pitfalls, and proposes a harm-centered framework for more holistic fairness research.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations in current fairness approaches in federated learning that focus solely on narrow system-level metrics such as performance parity or contribution-based rewards, while ignoring the potential harms and diverse stakeholder impacts throughout the FL lifecycle.

Method: The authors conducted a critical analysis of existing literature by systematically annotating papers based on their fairness definitions, design decisions, evaluation practices, and motivating use cases. They identified five recurring pitfalls in current fairness approaches in FL.

Result: The analysis revealed five key pitfalls: fairness framed only through server-client architecture; mismatch between simulations and real-world use cases; conflating system protection with user protection; isolated interventions neglecting upstream/downstream effects; and lack of multi-stakeholder alignment. A harm-centered framework was proposed to link fairness definitions to concrete risks and vulnerabilities.

Conclusion: The paper concludes with recommendations for future research in federated learning fairness, emphasizing the need for more holistic, context-aware, and accountable approaches.

Abstract: Fairness in federated learning has emerged as a rapidly growing area of
research, with numerous works proposing formal definitions and algorithmic
interventions. Yet, despite this technical progress, fairness in FL is often
defined and evaluated in ways that abstract away from the sociotechnical
contexts in which these systems are deployed. In this paper, we argue that
existing approaches tend to optimize narrow system level metrics, such as
performance parity or contribution-based rewards, while overlooking how harms
arise throughout the FL lifecycle and how they impact diverse stakeholders. We
support this claim through a critical analysis of the literature, based on a
systematic annotation of papers for their fairness definitions, design
decisions, evaluation practices, and motivating use cases. Our analysis reveals
five recurring pitfalls: 1) fairness framed solely through the lens of server
client architecture, 2) a mismatch between simulations and motivating use-cases
and contexts, 3) definitions that conflate protecting the system with
protecting its users, 4) interventions that target isolated stages of the
lifecycle while neglecting upstream and downstream effects, 5) and a lack of
multi-stakeholder alignment where multiple fairness definitions can be relevant
at once. Building on these insights, we propose a harm centered framework that
links fairness definitions to concrete risks and stakeholder vulnerabilities.
We conclude with recommendations for more holistic, context-aware, and
accountable fairness research in FL.

</details>


### [36] [CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning](https://arxiv.org/abs/2505.21587)
*Bin Qin,Qirui Ji,Jiangmeng Li,Yupeng Wang,Xuesong Wu,Jianwen Cao,Fanjiang Xu*

Main category: cs.LG

TL;DR: Self-supervised topological deep learning (TDL) in cellular complexes faces challenges due to structural constraints and semantic redundancy. The paper introduces CellCLAT, a framework that uses parameter perturbation-based augmentation and cellular trimming scheduler to overcome these issues, leading to significant improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: To model higher-order interactions in simplicial and cellular complexes for deriving representations of unlabeled graphs, overcoming the limitations posed by extrinsic structural constraints and intrinsic semantic redundancy in cellular representations.

Method: Proposes Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), which includes a parameter perturbation-based augmentation method to preserve cellular topology and a cellular trimming scheduler to remove redundant topological elements while maintaining critical semantics.

Result: Empirical validation shows substantial improvements over existing self-supervised graph learning methods.

Conclusion: CellCLAT addresses key challenges in self-supervised learning for cellular TDL, marking a significant advancement in this domain.

Abstract: Self-supervised topological deep learning (TDL) represents a nascent but
underexplored area with significant potential for modeling higher-order
interactions in simplicial complexes and cellular complexes to derive
representations of unlabeled graphs. Compared to simplicial complexes, cellular
complexes exhibit greater expressive power. However, the advancement in
self-supervised learning for cellular TDL is largely hindered by two core
challenges: \textit{extrinsic structural constraints} inherent to cellular
complexes, and intrinsic semantic redundancy in cellular representations. The
first challenge highlights that traditional graph augmentation techniques may
compromise the integrity of higher-order cellular interactions, while the
second underscores that topological redundancy in cellular complexes
potentially diminish task-relevant information. To address these issues, we
introduce Cellular Complex Contrastive Learning with Adaptive Trimming
(CellCLAT), a twofold framework designed to adhere to the combinatorial
constraints of cellular complexes while mitigating informational redundancy.
Specifically, we propose a parameter perturbation-based augmentation method
that injects controlled noise into cellular interactions without altering the
underlying cellular structures, thereby preserving cellular topology during
contrastive learning. Additionally, a cellular trimming scheduler is employed
to mask gradient contributions from task-irrelevant cells through a bi-level
meta-learning approach, effectively removing redundant topological elements
while maintaining critical higher-order semantics. We provide theoretical
justification and empirical validation to demonstrate that CellCLAT achieves
substantial improvements over existing self-supervised graph learning methods,
marking a significant attempt in this domain.

</details>


### [37] [Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning](https://arxiv.org/abs/2505.21591)
*Maosen Zhao,Pengtao Chen,Chong Yu,Yan Wen,Xudong Tan,Tao Chen*

Main category: cs.LG

TL;DR: Model quantization is important for improving memory efficiency and inference speed in diffusion models. However, 4-bit quantization remains challenging. This paper explores low-bit floating-point (FP) quantization for diffusion models and proposes the MSFP framework to address key challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily based on integer quantization and post-training quantization fine-tuning struggle with inconsistent performance when achieving 4-bit quantization.

Method: The paper proposes the mixup-sign floating-point quantization (MSFP) framework which includes unsigned FP quantization, timestep-aware LoRA (TALoRA), and denoising-factor loss alignment (DFA).

Result: Extensive experiments demonstrate that the proposed method achieves superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

Conclusion: The MSFP framework successfully addresses the challenges of low-bit FP quantization in diffusion models, leading to improved performance in 4-bit quantization.

Abstract: Model quantization reduces the bit-width of weights and activations,
improving memory efficiency and inference speed in diffusion models. However,
achieving 4-bit quantization remains challenging. Existing methods, primarily
based on integer quantization and post-training quantization fine-tuning,
struggle with inconsistent performance. Inspired by the success of
floating-point (FP) quantization in large language models, we explore low-bit
FP quantization for diffusion models and identify key challenges: the failure
of signed FP quantization to handle asymmetric activation distributions, the
insufficient consideration of temporal complexity in the denoising process
during fine-tuning, and the misalignment between fine-tuning loss and
quantization error. To address these challenges, we propose the mixup-sign
floating-point quantization (MSFP) framework, first introducing unsigned FP
quantization in model quantization, along with timestep-aware LoRA (TALoRA) and
denoising-factor loss alignment (DFA), which ensure precise and stable
fine-tuning. Extensive experiments show that we are the first to achieve
superior performance in 4-bit FP quantization for diffusion models,
outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.

</details>


### [38] [Relevance-driven Input Dropout: an Explanation-guided Regularization Technique](https://arxiv.org/abs/2505.21595)
*Shreyas Gururaj,Lars Grüne,Wojciech Samek,Sebastian Lapuschkin,Leander Weber*

Main category: cs.LG

TL;DR: The paper proposes Relevance-driven Input Dropout (RelDrop), a new data augmentation method that selectively occludes the most relevant input regions to improve model generalization.


<details>
  <summary>Details</summary>
Motivation: Overfitting is a common issue in machine learning models, leading to poor generalization and a significant train-test performance gap. Existing data augmentation strategies often use randomness instead of focusing on regions that strongly influence model decisions.

Method: The proposed method, Relevance-driven Input Dropout (RelDrop), selectively occludes the most relevant regions of the input during training, encouraging the model to utilize other important features for prediction and thereby improving generalization through informed regularization.

Result: Through experiments on benchmark datasets, RelDrop was shown to improve robustness towards occlusion, result in models utilizing more features within the region of interest, and enhance inference time generalization performance.

Conclusion: RelDrop is an effective data augmentation technique that improves model generalization by selectively occluding relevant input regions, as evidenced by qualitative and quantitative analyses.

Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA)
Machine Learning (ML) models, resulting in reduced generalization, and a
significant train-test performance gap. Mitigation measures include a
combination of dropout, data augmentation, weight decay, and other
regularization techniques. Among the various data augmentation strategies,
occlusion is a prominent technique that typically focuses on randomly masking
regions of the input during training. Most of the existing literature
emphasizes randomness in selecting and modifying the input features instead of
regions that strongly influence model decisions. We propose Relevance-driven
Input Dropout (RelDrop), a novel data augmentation method which selectively
occludes the most relevant regions of the input, nudging the model to use other
important features in the prediction process, thus improving model
generalization through informed regularization. We further conduct qualitative
and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop)
affects model decision-making. Through a series of experiments on benchmark
datasets, we demonstrate that our approach improves robustness towards
occlusion, results in models utilizing more features within the region of
interest, and boosts inference time generalization performance. Our code is
available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.

</details>


### [39] [SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge](https://arxiv.org/abs/2505.21605)
*Fengqing Jiang,Fengbo Ma,Zhangchen Xu,Yuetai Li,Bhaskar Ramasubramanian,Luyao Niu,Bo Li,Xianyan Chen,Zhen Xiang,Radha Poovendran*

Main category: cs.LG

TL;DR: 尽管大型语言模型（LLMs）在复杂任务中表现出色，但它们在涉及高风险科学领域的安全性仍需进一步探索。本文提出SOSBench，一个基于法规、关注危险情景的基准测试，评估LLMs在处理高风险科学领域时的安全性。结果显示，即使是最先进的模型也经常产生违反政策的内容，表明其安全对齐存在重大缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有的安全性基准测试无法充分评估大型语言模型在处理知识密集型、高风险场景时的表现，特别是在涉及科学领域的潜在滥用情况。

Method: 开发了一个名为SOSBench的基准测试，涵盖六个高风险科学领域，并使用3000个源自真实法规和法律的提示，通过LLM辅助的进化管道系统扩展，以引入多样且真实的误用情景。然后在一个统一的评估框架中使用SOSBench对前沿模型进行评估。

Result: 即使是最先进的模型也在所有领域中一致披露违反政策的内容，例如Deepseek-R1有79.1%的有害响应率，GPT-4.1为47.3%，显示出令人担忧的高有害响应率。

Conclusion: 这些结果强调了大型语言模型在安全对齐方面的显著不足，并提出了关于强大LLMs负责任部署的紧迫问题。

Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks,
such as reasoning and graduate-level question answering, yet their resilience
against misuse, particularly involving scientifically sophisticated risks,
remains underexplored. Existing safety benchmarks typically focus either on
instructions requiring minimal knowledge comprehension (e.g., ``tell me how to
build a bomb") or utilize prompts that are relatively low-risk (e.g.,
multiple-choice or classification tasks about hazardous content). Consequently,
they fail to adequately assess model safety when handling knowledge-intensive,
hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded,
hazard-focused benchmark encompassing six high-risk scientific domains:
chemistry, biology, medicine, pharmacology, physics, and psychology. The
benchmark comprises 3,000 prompts derived from real-world regulations and laws,
systematically expanded via an LLM-assisted evolutionary pipeline that
introduces diverse, realistic misuse scenarios (e.g., detailed explosive
synthesis instructions involving advanced chemical formulas). We evaluate
frontier models within a unified evaluation framework using our SOSBench.
Despite their alignment claims, advanced models consistently disclose
policy-violating content across all domains, demonstrating alarmingly high
rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1).
These results highlight significant safety alignment deficiencies and
underscore urgent concerns regarding the responsible deployment of powerful
LLMs.

</details>


### [40] [Learning Where to Learn: Training Distribution Selection for Provable OOD Performance](https://arxiv.org/abs/2505.21626)
*Nicolas Guerra,Nicholas H. Nelsen,Yunan Yang*

Main category: cs.LG

TL;DR: The paper investigates training data distribution designs to maximize OOD performance, proposing two algorithmic strategies that significantly improve OOD accuracy.


<details>
  <summary>Details</summary>
Motivation: Out-of-distribution generalization is a significant challenge in machine learning where models trained on one data distribution suffer from performance degradation when evaluated on different domains.

Method: Theoretical analysis provides generalization bounds showing the impact of training distribution choice on OOD error. Two algorithmic strategies are introduced: (i) bilevel optimization over probability measures and (ii) minimizing an upper bound on OOD error.

Result: The proposed methods show significant improvement in OOD accuracy compared to standard empirical risk minimization with a fixed distribution.

Conclusion: Distribution-aware training presents a promising framework for robust out-of-distribution generalization.

Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in
machine learning. Models trained on one data distribution often experience
substantial performance degradation when evaluated on shifted or unseen
domains. To address this challenge, the present paper studies the design of
training data distributions that maximize average-case OOD performance. First,
a theoretical analysis establishes a family of generalization bounds that
quantify how the choice of training distribution influences OOD error across a
predefined family of target distributions. These insights motivate the
introduction of two complementary algorithmic strategies: (i) directly
formulating OOD risk minimization as a bilevel optimization problem over the
space of probability measures and (ii) minimizing a theoretical upper bound on
OOD error. Last, the paper evaluates the two approaches across a range of
function approximation and operator learning examples. The proposed methods
significantly improve OOD accuracy over standard empirical risk minimization
with a fixed distribution. These results highlight the potential of
distribution-aware training as a principled and practical framework for robust
OOD generalization.

</details>


### [41] [Apprenticeship learning with prior beliefs using inverse optimization](https://arxiv.org/abs/2505.21639)
*Mauricio Junca,Esteban Leiva*

Main category: cs.LG

TL;DR: 这篇论文重新审视了MDP中的IO框架、IRL和学徒制学习（AL）之间的关系，并通过加入先验信念和正则化项改进了AL问题的凸分析视角。实验表明正则化在学习成本向量和学徒策略中起着关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管IRL和IO都针对相同的问题，但它们之间的关系尚未得到充分研究。因此，作者希望深入探讨这些方法之间的联系，并提出改进的方法来解决IRL中的病态问题。

Method: 作者将先验信念引入IRL和AL问题中，并展示了AL形式主义如何作为其框架的放松版本出现。特别是在没有正则化项的情况下，AL形式主义成为该框架的一个特例。接着，作者在次优专家设置下，将AL问题表示为一个正则化的min-max问题，并使用随机镜像下降法（SMD）求解，同时建立了收敛性界限。

Result: 数值实验验证了正则化在学习成本向量和学徒策略中的关键作用，表明所提出的方法能够有效解决IRL中的病态问题。

Conclusion: 本文通过引入正则化和先验信念改进了AL问题，并证明了这种方法可以有效应对IRL中的挑战。结果表明，正则化在寻找合理的成本函数方面起到了重要作用。

Abstract: The relationship between inverse reinforcement learning (IRL) and inverse
optimization (IO) for Markov decision processes (MDPs) has been relatively
underexplored in the literature, despite addressing the same problem. In this
work, we revisit the relationship between the IO framework for MDPs, IRL, and
apprenticeship learning (AL). We incorporate prior beliefs on the structure of
the cost function into the IRL and AL problems, and demonstrate that the
convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a
relaxation of our framework. Notably, the AL formalism is a special case in our
framework when the regularization term is absent. Focusing on the suboptimal
expert setting, we formulate the AL problem as a regularized min-max problem.
The regularizer plays a key role in addressing the ill-posedness of IRL by
guiding the search for plausible cost functions. To solve the resulting
regularized-convex-concave-min-max problem, we use stochastic mirror descent
(SMD) and establish convergence bounds for the proposed method. Numerical
experiments highlight the critical role of regularization in learning cost
vectors and apprentice policies.

</details>


### [42] [Efficient Diffusion Models for Symmetric Manifolds](https://arxiv.org/abs/2505.21640)
*Oren Mangoubi,Neil He,Nisheeth K. Vishnoi*

Main category: cs.LG

TL;DR: The paper presents an efficient diffusion model framework for symmetric-space Riemannian manifolds, reducing computational complexity and improving performance in training speed and sample quality.


<details>
  <summary>Details</summary>
Motivation: Existing manifold diffusion models often depend on heat kernels which are computationally expensive. This drives the need for a more efficient approach to handle symmetric-space Riemannian manifolds.

Method: A new diffusion model is introduced with spatially-varying covariance for symmetric manifolds, using a projection of Euclidean Brownian motion. The training algorithm minimizes a novel objective derived via Ito's Lemma, achieving nearly-linear-in-dimension computational complexity.

Result: The model outperforms prior methods in terms of training speed and sample quality on synthetic datasets involving the torus, special orthogonal group, and unitary group.

Conclusion: This work successfully reduces the computational gap between diffusions on symmetric manifolds and Euclidean space, providing a more efficient and accurate method for sample generation.

Abstract: We introduce a framework for designing efficient diffusion models for
$d$-dimensional symmetric-space Riemannian manifolds, including the torus,
sphere, special orthogonal group and unitary group. Existing manifold diffusion
models often depend on heat kernels, which lack closed-form expressions and
require either $d$ gradient evaluations or exponential-in-$d$ arithmetic
operations per training step. We introduce a new diffusion model for symmetric
manifolds with a spatially-varying covariance, allowing us to leverage a
projection of Euclidean Brownian motion to bypass heat kernel computations. Our
training algorithm minimizes a novel efficient objective derived via Ito's
Lemma, allowing each step to run in $O(1)$ gradient evaluations and
nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap
between diffusions on symmetric manifolds and Euclidean space. Manifold
symmetries ensure the diffusion satisfies an "average-case" Lipschitz
condition, enabling accurate and efficient sample generation. Empirically, our
model outperforms prior methods in training speed and improves sample quality
on synthetic datasets on the torus, special orthogonal group, and unitary
group.

</details>


### [43] [PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects](https://arxiv.org/abs/2505.21641)
*Maresa Schröder,Justin Hartenstein,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 在医学等需要可靠推断的领域，评估药物和其他医疗干预措施的效果通常需要有效的不确定性量化。然而，这些领域中的数据往往是敏感的，必须保持隐私。本文提出了PrivATE框架，该框架能够在保证差异隐私的前提下计算平均处理效应（ATE）的置信区间（CIs）。PrivATE框架包含三个步骤：(i) 通过输出扰动估计差异隐私ATE；(ii) 通过截断输出扰动机制估计差异隐私方差；(iii) 考虑估计和隐私化步骤中的不确定性来构建CIs。PrivATE框架是模型无关、双重稳健并确保有效的CIs。实验表明了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 在安全性关键的应用中，如医学，评估药物和其他医疗干预措施的效果需要有效的不确定性量化。然而，这些应用涉及的数据通常是敏感的，必须保持隐私。因此，需要一种方法在保护隐私的同时进行可靠的因果推断。

Method: PrivATE框架包括三个步骤：1. 使用输出扰动方法估计差异隐私ATE；2. 使用截断输出扰动机制估计差异隐私方差；3. 构建CIs时考虑估计和隐私化步骤中的不确定性。此外，该框架具有模型无关性和双重稳健性。

Result: 实验结果表明，PrivATE框架在合成和真实世界医疗数据集上均表现出有效性，能够产生有效的置信区间。

Conclusion: 本文提出了一种新的机器学习框架PrivATE，用于在差异隐私下计算ATE的置信区间。该框架具有模型无关性和双重稳健性，并且是第一个针对($\varepsilon$, $\delta$)-差异隐私下的有效置信区间提供通用、双重稳健框架的研究。

Abstract: The average treatment effect (ATE) is widely used to evaluate the
effectiveness of drugs and other medical interventions. In safety-critical
applications like medicine, reliable inferences about the ATE typically require
valid uncertainty quantification, such as through confidence intervals (CIs).
However, estimating treatment effects in these settings often involves
sensitive data that must be kept private. In this work, we present PrivATE, a
novel machine learning framework for computing CIs for the ATE under
differential privacy. Specifically, we focus on deriving valid
privacy-preserving CIs for the ATE from observational data. Our PrivATE
framework consists of three steps: (i) estimating a differentially private ATE
through output perturbation; (ii) estimating the differentially private
variance through a truncated output perturbation mechanism; and (iii)
constructing the CIs while accounting for the uncertainty from both the
estimation and privatization steps. Our PrivATE framework is model agnostic,
doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our
framework using synthetic and real-world medical datasets. To the best of our
knowledge, we are the first to derive a general, doubly robust framework for
valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.

</details>


### [44] [AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent](https://arxiv.org/abs/2505.21651)
*Nikola Surjanovic,Alexandre Bouchard-Côté,Trevor Campbell*

Main category: cs.LG

TL;DR: AutoSGD is an SGD method that automatically adjusts the learning rate, showing strong performance in various optimization problems and machine learning tasks.


<details>
  <summary>Details</summary>
Motivation: The tuning of learning rate for stochastic gradient descent (SGD) can significantly affect its performance. However, selecting an appropriate learning rate schedule usually requires substantial user effort.

Method: Introduced AutoSGD, which can automatically decide to increase or decrease the learning rate at each iteration and then take corresponding actions. Theoretical support for the convergence of AutoSGD and its deterministic counterpart for standard gradient descent was also provided.

Result: Empirical results indicate that AutoSGD performs well on a variety of traditional optimization problems and machine learning tasks.

Conclusion: AutoSGD provides an effective solution to automatically adjust the learning rate for SGD, reducing the need for manual tuning.

Abstract: The learning rate is an important tuning parameter for stochastic gradient
descent (SGD) and can greatly influence its performance. However, appropriate
selection of a learning rate schedule across all iterations typically requires
a non-trivial amount of user tuning effort. To address this, we introduce
AutoSGD: an SGD method that automatically determines whether to increase or
decrease the learning rate at a given iteration and then takes appropriate
action. We introduce theory supporting the convergence of AutoSGD, along with
its deterministic counterpart for standard gradient descent. Empirical results
suggest strong performance of the method on a variety of traditional
optimization problems and machine learning tasks.

</details>


### [45] [PreGenie: An Agentic Framework for High-quality Visual Presentation Generation](https://arxiv.org/abs/2505.21660)
*Xiaojie Xu,Xinli Xu,Sirui Chen,Haoyu Chen,Fan Zhang,Ying-Cong Chen*

Main category: cs.LG

TL;DR: PreGenie is a new framework that uses multimodal large language models to generate high-quality visual presentations, overcoming previous issues like poor layouts and mismatched visuals. It works in two stages: analysis/initial generation and review/re-generation, using multiple models that collaborate. Experiments show it performs well in aesthetics and content consistency.


<details>
  <summary>Details</summary>
Motivation: Previous deep learning methods for creating visual presentations often had problems such as disorganized layouts, inaccurate text summarization, and mismatched visuals, limiting their use in formal settings.

Method: PreGenie operates in two stages: (1) Analysis and Initial Generation where it summarizes multimodal input and generates initial code, and (2) Review and Re-generation where it iteratively reviews intermediate code and rendered slides to produce the final presentation. Multiple MLLMs collaborate and share information in each stage.

Result: Experiments indicate that PreGenie surpasses existing models in terms of aesthetics and content consistency, aligning closely with human design preferences.

Conclusion: PreGenie addresses the limitations of earlier methods by leveraging MLLMs to create visually appealing and content-consistent presentations, making it suitable for formal contexts.

Abstract: Visual presentations are vital for effective communication. Early attempts to
automate their creation using deep learning often faced issues such as poorly
organized layouts, inaccurate text summarization, and a lack of image
understanding, leading to mismatched visuals and text. These limitations
restrict their application in formal contexts like business and scientific
research. To address these challenges, we propose PreGenie, an agentic and
modular framework powered by multimodal large language models (MLLMs) for
generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are
rendered from Markdown code. It operates in two stages: (1) Analysis and
Initial Generation, which summarizes multimodal input and generates initial
code, and (2) Review and Re-generation, which iteratively reviews intermediate
code and rendered slides to produce final, high-quality presentations. Each
stage leverages multiple MLLMs that collaborate and share information.
Comprehensive experiments demonstrate that PreGenie excels in multimodal
understanding, outperforming existing models in both aesthetics and content
consistency, while aligning more closely with human design preferences.

</details>


### [46] [Efficient Controllable Diffusion via Optimal Classifier Guidance](https://arxiv.org/abs/2505.21666)
*Owen Oertell,Shikun Sun,Yiding Chen,Jin Peng Zhou,Zhiyong Wang,Wen Sun*

Main category: cs.LG

TL;DR: The paper introduces SLCD, a supervised learning approach for controllable generation in diffusion models. It optimizes a KL-regularized objective without overfitting and achieves high-quality sample generation with minimal inference time overhead.


<details>
  <summary>Details</summary>
Motivation: Controllable generation in diffusion models is crucial for various applications but existing reinforcement learning methods can overfit and require significant resources.

Method: SLCD frames controllable generation as finding a distribution optimizing a KL-regularized objective function. It uses supervised learning to iteratively generate data and train a small classifier to guide the diffusion model.

Result: SLCD provably converges to the optimal solution under KL divergence and empirically generates high quality samples with similar inference time as the base model.

Conclusion: SLCD offers an effective alternative to RL-based methods for controllable generation, providing theoretical guarantees and efficient performance.

Abstract: The controllable generation of diffusion models aims to steer the model to
generate samples that optimize some given objective functions. It is desirable
for a variety of applications including image generation, molecule generation,
and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of
the base model is a popular approach but it can overfit the reward function
while requiring significant resources. We frame controllable generation as a
problem of finding a distribution that optimizes a KL-regularized objective
function. We present SLCD -- Supervised Learning based Controllable Diffusion,
which iteratively generates online data and trains a small classifier to guide
the generation of the diffusion model. Similar to the standard
classifier-guided diffusion, SLCD's key computation primitive is classification
and does not involve any complex concepts from RL or control. Via a reduction
to no-regret online learning analysis, we show that under KL divergence, the
output from SLCD provably converges to the optimal solution of the
KL-regularized objective. Further, we empirically demonstrate that SLCD can
generate high quality samples with nearly the same inference time as the base
model in both image generation with continuous diffusion and biological
sequence generation with discrete diffusion. Our code is available at
https://github.com/Owen-Oertell/slcd

</details>


### [47] [What happens when generative AI models train recursively on each others' generated outputs?](https://arxiv.org/abs/2505.21677)
*Hung Ahn Vu,Galen Reeves,Emily Wenger*

Main category: cs.LG

TL;DR: The paper explores the consequences of generative AI models being trained on outputs generated by other AI models, providing empirical evidence and a theoretical model for this interactive training process, revealing both benefits and potential homogenization effects.


<details>
  <summary>Details</summary>
Motivation: To understand the downstream effects of genAI models being trained on outputs from other generative AI models, given society's increasing reliance on these tools.

Method: Provide empirical evidence of data-mediated interactions among genAI models, develop a theoretical model to describe the interactive training process, and experimentally demonstrate possible long-term results of such interactions.

Result: Data-mediated interactions can expose genAI models to novel concepts not present in their original training data, potentially benefiting them. However, these interactions can also homogenize model performance on shared tasks.

Conclusion: Understanding the impacts of genAI models trained on other models' outputs is critical as it may bring benefits but also lead to homogenized performance.

Abstract: The internet is full of AI-generated content while also serving as a common
source of training data for generative AI (genAI) models. This duality raises
the possibility that future genAI models may be trained on other models'
generated outputs. Prior work has studied consequences of models training on
their own generated outputs, but limited work has considered what happens if
models ingest content produced by other models. Given society's increasing
dependence on genAI tools, understanding downstream effects of such
data-mediated model interactions is critical. To this end, we provide empirical
evidence for how data-mediated interactions might unfold in practice, develop a
theoretical model for this interactive training process, and show
experimentally possible long-term results of such interactions. We find that
data-mediated interactions can benefit models by exposing them to novel
concepts perhaps missed in original training data, but also can homogenize
their performance on shared tasks.

</details>


### [48] [multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data](https://arxiv.org/abs/2505.21680)
*Andrew J. Loza,Jun Yup Kim,Shangzheng Song,Yihang Liu,Joseph J. Y. Sung,R Andrew Taylor,Dennis L. Shung*

Main category: cs.LG

TL;DR: multivariateGPT是一种单一架构，用于对混合类别（包括标记化文本）和数值数据的序列进行建模。它通过自回归序列分解、嵌入方案和损失函数来扩展下一个标记预测任务，以估计下一个标记类别和值的联合分布的似然性。该方法可以有效地学习简单物理系统中的模式，并对复杂的时间序列（如心电图和多变量电子健康记录数据）进行建模。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的过程通常生成混合类别和数值的数据，并且在不规则和信息丰富的间隔内记录。现有的离散标记方法在数值表示能力上有限，而像神经常微分方程这样的方法对于类别数据或信息采样并不适合，需要增强才能处理某些类别的轨迹。

Method: 提出了一种名为multivariateGPT的单一架构，用于对混合类别（包括标记化文本）和数值数据的序列进行建模。通过自回归序列分解、嵌入方案和损失函数来扩展下一个标记预测任务，以估计下一个标记类别和值的联合分布的似然性。

Result: 该方法可以有效地学习简单物理系统中的模式，并对复杂的时间序列（如心电图和多变量电子健康记录数据）进行建模。

Conclusion: 这项工作扩展了基于变压器模型的效用到更多的数据类型。

Abstract: Real-world processes often generate data that are a mix of categorical and
numeric values that are recorded at irregular and informative intervals.
Discrete token-based approaches are limited in numeric representation capacity
while methods like neural ordinary differential equations are not well suited
for categorical data or informative sampling and require augmentation to handle
certain classes of trajectories. Here, we present multivariateGPT, a single
architecture for modeling sequences of mixed categorical (including tokenized
text) and numeric data. This is accomplished with an autoregressive sequence
decomposition, embedding scheme, and loss function that extend the next token
prediction task to likelihood estimation of the joint distribution of next
token class and value. We demonstrate how this approach can efficiently learn
to generalize patterns in simple physical systems and model complex time series
including electrocardiograms and multivariate electronic health record data.
This work extends the utility of transformer based models to additional classes
of data.

</details>


### [49] [Incentivizing Permissionless Distributed Learning of LLMs](https://arxiv.org/abs/2505.21684)
*Joel Lidin,Amir Sarfi,Evangelos Pappas,Samuel Dare,Eugene Belilovsky,Jacob Steeves*

Main category: cs.LG

TL;DR: The paper introduces Gauntlet, an incentive system for distributed deep learning deployed on the bittensor blockchain, which successfully trained a 1.2B LLM with permissionless contributions.


<details>
  <summary>Details</summary>
Motivation: To create an effective incentive system that encourages contributions in distributed deep learning of foundational models without controlling participant access or hardware.

Method: Deployed Gauntlet on the bittensor blockchain, using a two-stage mechanism for filtering peer uptime, reliability, and synchronization, along with estimating loss changes from pseudo-gradient contributions and utilizing an OpenSkill rating system.

Result: Successfully trained a competitive 1.2B LLM with real-valued token payouts to participants based on their contributions.

Conclusion: Gauntlet demonstrates utility as an incentive system in distributed deep learning by enabling successful training of a large language model with permissionless contributions.

Abstract: We describe an incentive system for distributed deep learning of foundational
models where peers are rewarded for contributions. The incentive system,
\textit{Gauntlet}, has been deployed on the bittensor blockchain and used to
train a 1.2B LLM with completely permissionless contributions of
pseudo-gradients: no control over the users that can register or their
hardware. \textit{Gauntlet} can be applied to any synchronous distributed
training scheme that relies on aggregating updates or pseudo-gradients. We rely
on a two-stage mechanism for fast filtering of peer uptime, reliability, and
synchronization, combined with the core component that estimates the loss
before and after individual pseudo-gradient contributions. We utilized an
OpenSkill rating system to track competitiveness of pseudo-gradient scores
across time. Finally, we introduce a novel mechanism to ensure peers on the
network perform unique computations. Our live 1.2B run, which has paid out
real-valued tokens to participants based on the value of their contributions,
yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates
the utility of our incentive system.

</details>


### [50] [AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling](https://arxiv.org/abs/2505.21695)
*Ganglou Xu*

Main category: cs.LG

TL;DR: Federated learning struggles with balancing communication efficiency and model accuracy. This paper proposes Gradient Difference Approximation (GDA), a lightweight method using first-order info to estimate local error trends, part of the AMSFL framework for adaptive multi-step training.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge in federated learning of approximating update errors without high computational costs, specifically focusing on improving communication efficiency and model accuracy.

Method: The method proposed is called Gradient Difference Approximation (GDA), which uses first-order information to estimate local error trends without computing the full Hessian matrix. It is integrated into the Adaptive Multi-Step Federated Learning (AMSFL) framework.

Result: The paper does not explicitly mention results in the provided abstract.

Conclusion: GDA provides a unified error modeling strategy for large-scale multi-step adaptive training environments within the AMSFL framework.

Abstract: Federated learning faces critical challenges in balancing communication
efficiency and model accuracy. One key issue lies in the approximation of
update errors without incurring high computational costs. In this paper, we
propose a lightweight yet effective method called Gradient Difference
Approximation (GDA), which leverages first-order information to estimate local
error trends without computing the full Hessian matrix. The proposed method
forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL)
framework and provides a unified error modeling strategy for large-scale
multi-step adaptive training environments.

</details>


### [51] [Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling](https://arxiv.org/abs/2505.21717)
*Mónika Farsang,Ramin Hasani,Radu Grosu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long
sequences as fast as today's linear state-space layers. By forcing the
state-transition matrix to be diagonal and learned at every step, the full
sequence can be solved in parallel with a single prefix-scan, giving
$\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential
depth, for input-sequence length $T$ and a state dimension $D$. Moreover,
LrcSSM offers a formal gradient-stability guarantee that other input-varying
systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth
$L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its
low sequential depth and parameter count $\Theta(D\,L)$, the model follows the
compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for
Mamba, outperforming quadratic-attention Transformers at equal compute while
avoiding the memory overhead of FFT-based long convolutions. We show that on a
series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.

</details>


### [52] [Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape](https://arxiv.org/abs/2505.21722)
*Ioannis Bantzis,James B. Simon,Arthur Jacot*

Main category: cs.LG

TL;DR: In deep ReLU networks initialized with small weights, the optimal escape direction from the saddle at the origin has a low-rank bias in deeper layers, marking a step towards understanding Saddle-to-Saddle dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand the behavior of Gradient Descent (GD) in deep ReLU networks when initialized with small weights and to study the escape directions from the saddle point at the origin.

Method: Analyze the escape directions in the context of deep ReLU networks, focusing on the properties of these directions and their relation to the singular values of weight matrices in different layers.

Result: The optimal escape direction exhibits a low-rank bias in its deeper layers; specifically, the first singular value of the $\ell$-th layer weight matrix is significantly larger than others.

Conclusion: This finding represents an initial step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD transitions through a sequence of saddles with increasing bottleneck rank.

Abstract: When a deep ReLU network is initialized with small weights, GD is at first
dominated by the saddle at the origin in parameter space. We study the
so-called escape directions, which play a similar role as the eigenvectors of
the Hessian for strict saddles. We show that the optimal escape direction
features a low-rank bias in its deeper layers: the first singular value of the
$\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any
other singular value. We also prove a number of related results about these
escape directions. We argue that this result is a first step in proving
Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of
saddles with increasing bottleneck rank.

</details>


### [53] [Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection](https://arxiv.org/abs/2505.21938)
*Qirun Zeng,Eric He,Richard Hoffmann,Xuchuang Wang,Jinhang Zuo*

Main category: cs.LG

TL;DR: The paper explores a practical adversarial threat model called Fake Data Injection for stochastic bandits, demonstrating its ability to mislead UCB and Thompson Sampling algorithms with low cost, supported by theoretical analysis and experiments.


<details>
  <summary>Details</summary>
Motivation: Existing adversarial attacks on stochastic bandits rely on unrealistic assumptions such as per-round reward manipulation and unbounded perturbations, which limits their applicability to real-world systems. There is a need for more realistic attack models that consider the constraints faced by potential adversaries.

Method: The authors propose the Fake Data Injection threat model where an attacker can inject a limited number of bounded fake feedback samples into the learner's history. They design efficient attack strategies considering both magnitude (reward values) and temporal (timing and frequency) constraints. Theoretical analysis and experimental validation are conducted to evaluate the effectiveness of these strategies on UCB and Thompson Sampling algorithms.

Result: The proposed attack strategies can successfully mislead UCB and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets confirm the significant vulnerabilities of stochastic bandit algorithms under practical adversarial scenarios.

Conclusion: The study highlights the vulnerability of widely used stochastic bandit algorithms to adversarial attacks under realistic constraints through the Fake Data Injection model. This calls for further research into robustifying these algorithms against such practical threats.

Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some
unrealistic assumptions, such as per-round reward manipulation and unbounded
perturbations, limiting their relevance to real-world systems. We propose a
more practical threat model, Fake Data Injection, which reflects realistic
adversarial constraints: the attacker can inject only a limited number of
bounded fake feedback samples into the learner's history, simulating legitimate
interactions. We design efficient attack strategies under this model,
explicitly addressing both magnitude constraints (on reward values) and
temporal constraints (on when and how often data can be injected). Our
theoretical analysis shows that these attacks can mislead both Upper Confidence
Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in
nearly all rounds while incurring only sublinear attack cost. Experiments on
synthetic and real-world datasets validate the effectiveness of our strategies,
revealing significant vulnerabilities in widely used stochastic bandit
algorithms under practical adversarial scenarios.

</details>


### [54] [Deep Reinforcement Learning Agents are not even close to Human Intelligence](https://arxiv.org/abs/2505.21731)
*Quentin Delfosse,Jannis Blüml,Fabian Tatai,Théo Vincent,Bjarne Gregori,Elisabeth Dillies,Jan Peters,Constantin Rothkopf,Kristian Kersting*

Main category: cs.LG

TL;DR: Deep reinforcement learning agents, despite impressive results, lack zero-shot adaptation and show significant performance drops on simpler task variations compared to humans, indicating a reliance on shortcuts. This highlights the gap between RL agents and human behavioral intelligence.


<details>
  <summary>Details</summary>
Motivation: To evaluate the robustness and generalization capabilities of deep reinforcement learning (RL) agents by examining their performance on simplified versions of tasks, as opposed to the typical focus on complexified tasks.

Method: Introduced HackAtari, a set of task variations based on Arcade Learning Environments, to test RL agents' performance on simpler task versions and analyze their reliance on shortcuts across multiple algorithms and architectures.

Result: RL agents exhibit large performance drops on simpler task variations, unlike humans, revealing their consistent reliance on shortcuts and highlighting the persistent gap in generalization capabilities compared to human behavioral intelligence.

Conclusion: Training and testing in the same environment is insufficient for achieving human-like intelligence in RL agents; new benchmarks and methodologies are needed to enforce systematic generalization testing.

Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide
variety of tasks, but they lack zero-shot adaptation capabilities. While most
robustness evaluations focus on tasks complexifications, for which human also
struggle to maintain performances, no evaluation has been performed on tasks
simplifications. To tackle this issue, we introduce HackAtari, a set of task
variations of the Arcade Learning Environments. We use it to demonstrate that,
contrary to humans, RL agents systematically exhibit huge performance drops on
simpler versions of their training tasks, uncovering agents' consistent
reliance on shortcuts. Our analysis across multiple algorithms and
architectures highlights the persistent gap between RL agents and human
behavioral intelligence, underscoring the need for new benchmarks and
methodologies that enforce systematic generalization testing beyond static
evaluation protocols. Training and testing in the same environment is not
enough to obtain agents equipped with human-like intelligence.

</details>


### [55] [LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing](https://arxiv.org/abs/2505.21732)
*Ruijie Zhang,Ziyue Liu,Zhengyang Wang,Zheng Zhang*

Main category: cs.LG

TL;DR: LaX is a plug-and-play module that enhances low-rank models by enabling information flow across subspaces, improving performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Training foundation models like ViTs and LLMs is computationally expensive. Low-rank factorization methods reduce this cost but often sacrifice performance.

Method: Introduced LaX, which allows information exchange across low-rank subspaces to boost model capacity.

Result: LaX matches or exceeds full-rank baseline performance using 2-3x fewer parameters in pre-training tasks. It also improves fine-tuning performance on arithmetic and common sense reasoning tasks with negligible additional cost.

Conclusion: LaX effectively enhances low-rank models, reducing parameter usage while maintaining or improving performance.

Abstract: Training foundation models such as ViTs and LLMs requires tremendous
computing cost. Low-rank matrix or tensor factorization offers a
parameter-efficient alternative, but often downgrades performance due to the
restricted parameter space. In this work, we introduce {\textbf{Latent Crossing
(LaX)}} -- a simple yet effective plug-and-play module that enhances the
capacity of low-rank models by enabling information flow across low-rank
subspaces. We extensively validate the benefits of LaX on pre-training tasks
with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters.
LaX boosts low-rank model performance to match or exceed the full-rank
baselines while using 2-3\(\times\) fewer parameters. When equipped with
low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently
improves performance on arithmetic and common sense reasoning tasks with
negligible cost.

</details>


### [56] [Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen](https://arxiv.org/abs/2505.21743)
*Zihao Li,Xinyuan Cao,Xiangbo Gao,Kexin Tian,Keshu Wu,Mohammad Anis,Hao Zhang,Keke Long,Jiwan Jiang,Xiaopeng Li,Yunlong Zhang,Tianbao Yang,Dominique Lord,Zhengzhong Tu,Yang Zhou*

Main category: cs.LG

TL;DR: The paper proposes a counterfactual safety learning approach to transform sparse crash data into rich signals for crash prediction, promoting proactive prevention in traffic safety.


<details>
  <summary>Details</summary>
Motivation: Traditional traffic safety science is limited by the lack of data on severe crashes. Current models rely on sparse and noisy records, while simulations undersample critical situations.

Method: The proposed method bridges macro to micro using crash-rate priors, generative scene engines, diverse driver models, and causal learning to synthesize and explain near-miss events. A digital twin testbed links micro scenes to macro patterns and a multi-objective validator ensures statistical realism.

Result: This pipeline enables stress-testing of vehicles, roads, and policies before deployment, transforming sparse crash data into rich signals for crash prediction.

Conclusion: By learning from crashes that almost happened, traffic safety can shift from reactive forensics to proactive prevention, advancing towards Vision Zero.

Abstract: Traffic safety science has long been hindered by a fundamental data paradox:
the crashes we most wish to prevent are precisely those events we rarely
observe. Existing crash-frequency models and surrogate safety metrics rely
heavily on sparse, noisy, and under-reported records, while even sophisticated,
high-fidelity simulations undersample the long-tailed situations that trigger
catastrophic outcomes such as fatalities. We argue that the path to achieving
Vision Zero, i.e., the complete elimination of traffic fatalities and severe
injuries, requires a paradigm shift from traditional crash-only learning to a
new form of counterfactual safety learning: reasoning not only about what
happened, but also about the vast set of plausible yet perilous scenarios that
could have happened under slightly different circumstances. To operationalize
this shift, our proposed agenda bridges macro to micro. Guided by crash-rate
priors, generative scene engines, diverse driver models, and causal learning,
near-miss events are synthesized and explained. A crash-focused digital twin
testbed links micro scenes to macro patterns, while a multi-objective validator
ensures that simulations maintain statistical realism. This pipeline transforms
sparse crash data into rich signals for crash prediction, enabling the
stress-testing of vehicles, roads, and policies before deployment. By learning
from crashes that almost happened, we can shift traffic safety from reactive
forensics to proactive prevention, advancing Vision Zero.

</details>


### [57] [Inclusive, Differentially Private Federated Learning for Clinical Data](https://arxiv.org/abs/2505.22108)
*Santhosh Parampottupadam,Melih Coşğun,Sarthak Pati,Maximilian Zenk,Saikat Roy,Dimitrios Bounias,Benjamin Hamm,Sinem Sav,Ralf Floca,Klaus Maier-Hein*

Main category: cs.LG

TL;DR: 本研究提出了一种新的合规感知联邦学习（FL）框架，通过根据可量化的客户合规分数自适应调整噪声来增强差分隐私（DP），并在公共数据集实验中展示了其在平衡隐私、合规性和性能方面的优势。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）为训练临床AI模型提供了不集中敏感患者数据的方法，但实际应用中受到隐私、资源限制和合规性挑战的阻碍。现有的差分隐私（DP）方法通常应用统一噪声，这会不成比例地降低模型性能，即使是在合规性良好的机构中也是如此。

Method: 提出了一种新的合规感知FL框架，该框架通过基于可量化的客户合规分数自适应调整噪声来增强DP，并引入了一个基于关键医疗和安全标准的合规评分工具，以促进在不同临床环境中的安全、包容和平等参与。

Result: 在公共数据集上的广泛实验表明，将资源不足、合规性较低的诊所与高度监管的机构整合起来，与传统的FL相比，准确率提高了多达15%。

Conclusion: 这项工作通过平衡隐私、合规性和性能，推动了FL的发展，使其成为全球医疗保健中现实世界临床工作流程的可行解决方案。

Abstract: Federated Learning (FL) offers a promising approach for training clinical AI
models without centralizing sensitive patient data. However, its real-world
adoption is hindered by challenges related to privacy, resource constraints,
and compliance. Existing Differential Privacy (DP) approaches often apply
uniform noise, which disproportionately degrades model performance, even among
well-compliant institutions. In this work, we propose a novel compliance-aware
FL framework that enhances DP by adaptively adjusting noise based on
quantifiable client compliance scores. Additionally, we introduce a compliance
scoring tool based on key healthcare and security standards to promote secure,
inclusive, and equitable participation across diverse clinical settings.
Extensive experiments on public datasets demonstrate that integrating
under-resourced, less compliant clinics with highly regulated institutions
yields accuracy improvements of up to 15% over traditional FL. This work
advances FL by balancing privacy, compliance, and performance, making it a
viable solution for real-world clinical workflows in global healthcare.

</details>


### [58] [Revisiting Bi-Linear State Transitions in Recurrent Neural Networks](https://arxiv.org/abs/2505.21749)
*M. Reza Ebrahimi,Roland Memisevic*

Main category: cs.LG

TL;DR: Revisit bi-linear operations in recurrent neural networks, demonstrating their effectiveness for state tracking tasks and establishing a hierarchy of network complexity.


<details>
  <summary>Details</summary>
Motivation: To explore the role of hidden units as active participants in computation rather than passive memory stores, focusing on bi-linear operations that involve multiplicative interactions between hidden units and input embeddings.

Method: Theoretically and empirically analyze bi-linear operations in recurrent neural networks, showing they provide a natural inductive bias for state tracking tasks and form a hierarchy corresponding to task complexity.

Result: Bi-linear state updates are effective for state tracking tasks and create a hierarchy with linear recurrent networks like Mamba at the lowest complexity level.

Conclusion: Bi-linear operations offer a promising approach to enhance the active role of hidden units in recurrent neural networks, particularly for state tracking tasks.

Abstract: The role of hidden units in recurrent neural networks is typically seen as
modeling memory, with research focusing on enhancing information retention
through gating mechanisms. A less explored perspective views hidden units as
active participants in the computation performed by the network, rather than
passive memory stores. In this work, we revisit bi-linear operations, which
involve multiplicative interactions between hidden units and input embeddings.
We demonstrate theoretically and empirically that they constitute a natural
inductive bias for representing the evolution of hidden states in state
tracking tasks. These are the simplest type of task that require hidden units
to actively contribute to the behavior of the network. We also show that
bi-linear state updates form a natural hierarchy corresponding to state
tracking tasks of increasing complexity, with popular linear recurrent networks
such as Mamba residing at the lowest-complexity center of that hierarchy.

</details>


### [59] [Training RL Agents for Multi-Objective Network Defense Tasks](https://arxiv.org/abs/2505.22531)
*Andres Molina-Markham,Luis Robaina,Sean Steinle,Akash Trivedi,Derek Tsui,Nicholas Potteiger,Lauren Brandt,Ransom Winder,Ahmed Ridley*

Main category: cs.LG

TL;DR: The paper proposes an OEL-inspired training approach for developing autonomous network defenders, emphasizing the importance of a consistent task representation approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of applying Open-ended learning (OEL) in real-world cybersecurity applications, specifically for developing autonomous network defenders.

Method: Proposing a training approach inspired by OEL principles to develop autonomous network defenders, focusing on providing a task representation approach that maintains a consistent interface over goals, rewards and action spaces.

Result: Demonstrates that OEL principles can lead to more robust and generalizable agents for cyber defense.

Conclusion: The tools and results aim to significantly impact AI-based cybersecurity research, urging researchers to consider diverse tasks with consistent representations when developing gyms and benchmarks for cyber defense.

Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve
broad capability over narrow competency -- is emerging as a paradigm to develop
artificial intelligence (AI) agents to achieve robustness and generalization.
However, despite promising results that demonstrate the benefits of OEL,
applying OEL to develop autonomous agents for real-world cybersecurity
applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous
network defenders. Our results demonstrate that like in other domains, OEL
principles can translate into more robust and generalizable agents for cyber
defense. To apply OEL to network defense, it is necessary to address several
technical challenges. Most importantly, it is critical to provide a task
representation approach over a broad universe of tasks that maintains a
consistent interface over goals, rewards and action spaces. This way, the
learning agent can train with varying network conditions, attacker behaviors,
and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that
applies AI to solve cybersecurity problems. Specifically, as researchers
develop gyms and benchmarks for cyber defense, it is paramount that they
consider diverse tasks with consistent representations, such as those we
propose in our work.

</details>


### [60] [Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals](https://arxiv.org/abs/2505.21750)
*Vivienne Huiling Wang,Tinghuai Wang,Joni Pajarinen*

Main category: cs.LG

TL;DR: The paper proposes an HRL approach using a conditional diffusion model with GP prior for subgoal generation, outperforming previous methods in sample efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals.

Method: An approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Subgoals are selected from both the diffusion policy and GP's predictive mean.

Result: Outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.

Conclusion: The proposed method improves HRL by better handling the changing low-level policy through advanced subgoal generation.

Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on
multiple levels of temporal abstraction. A key challenge in HRL is that the
low-level policy changes over time, making it difficult for the high-level
policy to generate effective subgoals. To address this issue, the high-level
policy must capture a complex subgoal distribution while also accounting for
uncertainty in its estimates. We propose an approach that trains a conditional
diffusion model regularized by a Gaussian Process (GP) prior to generate a
complex variety of subgoals while leveraging principled GP uncertainty
quantification. Building on this framework, we develop a strategy that selects
subgoals from both the diffusion policy and GP's predictive mean. Our approach
outperforms prior HRL methods in both sample efficiency and performance on
challenging continuous control benchmarks.

</details>


### [61] [DualSchool: How Reliable are LLMs for Optimization Education?](https://arxiv.org/abs/2505.21775)
*Michael Klamkin,Arnaud Deza,Sikai Cheng,Haoruo Zhao,Pascal Van Hentenryck*

Main category: cs.LG

TL;DR: 尽管大型语言模型（LLMs）能准确复述线性规划对偶转换过程，但最先进的开源LLMs在生成正确对偶问题上表现不佳，即使是最小的两变量实例亦如此。本文通过引入DualSchool框架揭示了这一发现，并讨论了其对教育者、学生及大型推理系统发展的意义。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型在优化课程中执行从原问题到对偶问题转换（P2DC）任务的能力，以应对生成式AI与运筹学交叉领域中的挑战。

Method: 构建了一个全面的框架DualSchool，用于生成和验证P2DC实例，采用规范图编辑距离进行验证，超越了现有优化模型评估方法。

Result: 实验表明，尽管LLMs能准确复述转换过程，但无法一贯生成正确的对偶问题，包括最小的两变量实例及其衍生任务。

Conclusion: 本研究结果对教育者、学生以及大型推理系统的开发具有重要意义，表明当前LLMs在特定优化任务上的局限性。

Abstract: Consider the following task taught in introductory optimization courses which
addresses challenges articulated by the community at the intersection of
(generative) AI and OR: generate the dual of a linear program. LLMs, being
trained at web-scale, have the conversion process and many instances of Primal
to Dual Conversion (P2DC) at their disposal. Students may thus reasonably
expect that LLMs would perform well on the P2DC task. To assess this
expectation, this paper introduces DualSchool, a comprehensive framework for
generating and verifying P2DC instances. The verification procedure of
DualSchool uses the Canonical Graph Edit Distance, going well beyond existing
evaluation methods for optimization models, which exhibit many false positives
and negatives when applied to P2DC. Experiments performed by DualSchool reveal
interesting findings. Although LLMs can recite the conversion procedure
accurately, state-of-the-art open LLMs fail to consistently produce correct
duals. This finding holds even for the smallest two-variable instances and for
derivative tasks, such as correctness, verification, and error classification.
The paper also discusses the implications for educators, students, and the
development of large reasoning systems.

</details>


### [62] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham,Gabriel Raya,Matteo Negri,Mohammed J. Zaki,Luca Ambrogioni,Dmitry Krotov*

Main category: cs.LG

TL;DR: 本研究从联想记忆（AM）的角度分析扩散模型，揭示了其在小数据和大数据情况下的不同记忆阶段，并发现了虚假状态的存在。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型与联想记忆系统之间的联系，特别是理解扩散模型在不同数据规模下的记忆行为及虚假状态的出现。

Method: 将扩散模型的训练过程视为记忆编码，生成过程视为记忆检索；分析小数据和大数据情况下扩散模型的行为，识别虚假状态的产生及其特征。

Result: 发现扩散模型在小数据阶段表现出强记忆化特性，在大数据阶段则形成新的吸引子状态；虚假状态出现在从小数据到大数据的过渡边界上。

Conclusion: 提供了一个通过联想记忆视角理解扩散模型记忆化-泛化现象的新方法，理论预测并实证验证了虚假状态的存在。

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [63] [P-DROP: Poisson-Based Dropout for Graph Neural Networks](https://arxiv.org/abs/2505.21783)
*Hyunsik Yun*

Main category: cs.LG

TL;DR: 在图神经网络（GNNs）中，过平滑是一个主要挑战。为了解决这个问题，本文提出了一种基于泊松过程的新型节点选择策略，实验表明该方法在标准基准上表现良好。


<details>
  <summary>Details</summary>
Motivation: 图神经网络中的过平滑问题，导致节点表示收敛并失去辨别能力。需要一种新的方法来解决这个问题。

Method: 提出了一种基于泊松过程的节点选择策略，给每个节点配备一个独立的泊松时钟，实现异步和局部更新，以保持结构多样性。探索了该策略在dropout正则化替代和动态子图训练方案中的应用。

Result: 在Cora、Citeseer和Pubmed等标准基准上的实验结果表明，基于泊松的方法相较于传统的Dropout、DropEdge和DropNode方法具有竞争力或更高的准确性，尤其是在训练后期阶段。

Conclusion: 基于泊松过程的节点选择策略能有效缓解GNN中的过平滑问题，在多个应用场景中展现出优势。

Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs),
where repeated message passing causes node representations to converge and lose
discriminative power. To address this, we propose a novel node selection
strategy based on Poisson processes, introducing stochastic but structure-aware
updates. Specifically, we equip each node with an independent Poisson clock,
enabling asynchronous and localized updates that preserve structural diversity.
We explore two applications of this strategy: as a replacement for
dropout-based regularization and as a dynamic subgraph training scheme.
Experimental results on standard benchmarks (Cora, Citeseer, Pubmed)
demonstrate that our Poisson-based method yields competitive or improved
accuracy compared to traditional Dropout, DropEdge, and DropNode approaches,
particularly in later training stages.

</details>


### [64] [Born a Transformer -- Always a Transformer?](https://arxiv.org/abs/2505.21785)
*Yana Veitsman,Mayank Jobanputra,Yash Sarrof,Aleksandra Bakalova,Vera Demberg,Ellie Pavlick,Michael Hahn*

Main category: cs.LG

TL;DR: Transformers在序列任务建模上存在理论限制，但这些限制是否影响大规模预训练LLM尚不清楚。研究通过检索和复制任务，发现预训练模型在处理右侧（归纳）任务时表现优于左侧（反归纳）任务，并揭示了这种不对称性与Transformer内部电路强度差异有关。尽管针对性微调可消除不对称性，但预训练并不能克服基本的长度泛化限制。


<details>
  <summary>Details</summary>
Motivation: 探讨大规模预训练语言模型（LLMs）是否受到Transformer架构在序列任务建模上的理论限制，以及这些限制如何在实际应用中表现。

Method: 1. 设计一系列检索和复制任务来研究Transformer架构约束在预训练后的表现。
2. 使用C-RASP框架确保实验设置中的长度泛化能力。
3. 通过实证分析归纳与反归纳任务的表现差异。
4. 进行机制分析以揭示不对称性的原因。
5. 针对真实世界任务进行实验验证。

Result: 发现预训练模型在归纳任务（右侧检索）上表现优于反归纳任务（左侧检索），这种不对称性与Transformer内部电路强度差异相关。通过针对性微调可以消除不对称性，但无法完全克服长度泛化的理论限制。

Conclusion: 预训练增强了某些Transformer能力，但未能解决其基本的长度泛化限制，这提示了潜在的可靠性风险。

Abstract: Transformers have theoretical limitations in modeling certain
sequence-to-sequence tasks, yet it remains largely unclear if these limitations
play a role in large-scale pretrained LLMs, or whether LLMs might effectively
overcome these constraints in practice due to the scale of both the models
themselves and their pretraining data. We explore how these architectural
constraints manifest after pretraining, by studying a family of
$\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al.
[2024]. We use the recently proposed C-RASP framework for studying length
generalization [Huang et al., 2025b] to provide guarantees for each of our
settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$
asymmetry, where pretrained models are better at retrieving tokens to the right
(induction) rather than the left (anti-induction) of a query token. This
asymmetry disappears upon targeted fine-tuning if length-generalization is
guaranteed by theory. Mechanistic analysis reveals that this asymmetry is
connected to the differences in the strength of induction versus anti-induction
circuits within pretrained Transformers. We validate our findings through
practical experiments on real-world tasks demonstrating reliability risks. Our
results highlight that pretraining selectively enhances certain Transformer
capabilities, but does not overcome fundamental length-generalization limits.

</details>


### [65] [Faster Rates for Private Adversarial Bandits](https://arxiv.org/abs/2505.21790)
*Hilal Asi,Vinod Raman,Kunal Talwar*

Main category: cs.LG

TL;DR: 设计了新的差分隐私算法用于对抗性bandits问题和带专家建议的bandits问题。对于对抗性bandits，给出了一种简单高效的将非隐私bandit算法转化为隐私bandit算法的方法，并改进了现有上界。对于带专家建议的bandits问题，首次提出了差分隐私算法并获得了次线性遗憾率。


<details>
  <summary>Details</summary>
Motivation: 为了在保持数据隐私的同时解决对抗性bandits和带专家建议的bandits问题，需要设计有效的差分隐私算法以达到较低的遗憾率。

Method: 对于对抗性bandits，提出了一种将非隐私bandit算法转换为隐私bandit算法的方法；对于带专家建议的bandits，首次设计了几种不同的差分隐私算法。

Result: 对抗性bandits问题的遗憾上界从$O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ 改进到 $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$。带专家建议的bandits问题得到了几种次线性遗憾率的结果。

Conclusion: 这些新算法在保证隐私的同时实现了更低的遗憾率，并首次展示了中心化和本地化差分隐私在这类问题上的差异。

Abstract: We design new differentially private algorithms for the problems of
adversarial bandits and bandits with expert advice. For adversarial bandits, we
give a simple and efficient conversion of any non-private bandit algorithm to a
private bandit algorithm. Instantiating our conversion with existing
non-private bandit algorithms gives a regret upper bound of
$O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing
upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all
$\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected
regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first
known separation between central and local differential privacy for this
problem. For bandits with expert advice, we give the first differentially
private algorithms, with expected regret
$O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right),
O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and
$\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} +
\frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of
actions and experts respectively. These rates allow us to get sublinear regret
for different combinations of small and large $K, N$ and $\epsilon.$

</details>


### [66] [Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms](https://arxiv.org/abs/2505.21792)
*Yuanzhe Peng,Jieming Bian,Lei Wang,Yin Huang,Jie Xu*

Main category: cs.LG

TL;DR: Multimodal Federated Learning (MFL) combines multiple data types to improve performance while preserving privacy through distributed training. This paper provides a systematic examination of MFL within three major FL paradigms: horizontal, vertical, and hybrid FL, highlighting challenges and providing insights for future research.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive taxonomy for MFL and to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms.

Method: Systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. Present problem formulation, review representative training algorithms, and highlight challenges introduced by multimodal data in distributed settings.

Result: Provides a taxonomy of MFL that organizes it through the lens of different FL paradigms, identifies distinct challenges such as modality heterogeneity, privacy heterogeneity, and communication inefficiency, and offers insights for future research.

Conclusion: This taxonomy aims to offer a new lens through which to understand and advance the development of MFL, addressing its unique challenges across various FL settings.

Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal
research areas: leveraging complementary information from multiple modalities
to improve downstream inference performance and enabling distributed training
to enhance efficiency and preserve privacy. Despite the growing interest in
MFL, there is currently no comprehensive taxonomy that organizes MFL through
the lens of different Federated Learning (FL) paradigms. This perspective is
important because multimodal data introduces distinct challenges across various
FL settings. These challenges, including modality heterogeneity, privacy
heterogeneity, and communication inefficiency, are fundamentally different from
those encountered in traditional unimodal or non-FL scenarios. In this paper,
we systematically examine MFL within the context of three major FL paradigms:
horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we
present the problem formulation, review representative training algorithms, and
highlight the most prominent challenge introduced by multimodal data in
distributed settings. We also discuss open challenges and provide insights for
future research. By establishing this taxonomy, we aim to uncover the novel
challenges posed by multimodal data from the perspective of different FL
paradigms and to offer a new lens through which to understand and advance the
development of MFL.

</details>


### [67] [From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs](https://arxiv.org/abs/2505.21800)
*Stanley Yu,Vaidehi Bulusu,Oscar Yasunaga,Clayton Lau,Cole Blondin,Sean O'Brien,Kevin Zhu,Vasu Sharma*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）虽然具有强大的对话能力，但经常生成错误信息。先前的研究表明，简单命题的真实性可以在模型的内部激活中表示为单一的线性方向，但这可能无法完全捕捉其底层几何结构。本研究将最近用于建模拒绝行为的概念锥框架扩展到真实性的领域，识别出在多个LLM家族中因果调节真实性相关行为的多维锥。研究结果通过三条证据得到支持：(i) 因果干预可靠地翻转模型对事实陈述的反应；(ii) 学习到的锥体可以跨模型架构泛化；(iii) 基于锥体的干预保留了无关的模型行为。这些发现揭示了简单的真实/虚假命题在LLMs中更丰富、多方向的结构，并突显了概念锥作为探索抽象行为的有希望的工具。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs具有强大的对话能力，但它们经常会生成错误信息。为了更好地理解并改善这一问题，研究者试图超越简单的线性方向表示，探索能够更全面捕捉模型内部几何结构的方法。

Method: 研究扩展了概念锥框架，将其应用于真实性领域。通过识别多维锥，研究者分析了这些锥如何在多个LLM家族中因果调节真实性相关行为。具体方法包括因果干预、学习锥的跨模型架构泛化性测试以及基于锥的干预对无关模型行为的影响评估。

Result: 研究发现了三条支持性证据：(i) 因果干预能可靠地改变模型对事实陈述的反应；(ii) 学习到的锥体能够跨不同模型架构泛化；(iii) 基于锥体的干预不会影响模型的其他无关行为。

Conclusion: LLMs中简单的真实/虚假命题由更丰富、多方向的结构所支配。概念锥是探索和理解这种抽象行为的一种有前景的工具。

Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but
often generate falsehoods. Prior work suggests that the truthfulness of simple
propositions can be represented as a single linear direction in a model's
internal activations, but this may not fully capture its underlying geometry.
In this work, we extend the concept cone framework, recently introduced for
modeling refusal, to the domain of truth. We identify multi-dimensional cones
that causally mediate truth-related behavior across multiple LLM families. Our
results are supported by three lines of evidence: (i) causal interventions
reliably flip model responses to factual statements, (ii) learned cones
generalize across model architectures, and (iii) cone-based interventions
preserve unrelated model behavior. These findings reveal the richer,
multidirectional structure governing simple true/false propositions in LLMs and
highlight concept cones as a promising tool for probing abstract behaviors.

</details>


### [68] [Towards Operational Automated Greenhouse Gas Plume Detection](https://arxiv.org/abs/2505.21806)
*Brian D. Bue,Jake H. Lee,Andrew K. Thorpe,Philip G. Brodrick,Daniel Cusworth,Alana Ayasse,Vassiliki Mancoridis,Anagha Satish,Shujun Xiong,Riley Duren*

Main category: cs.LG

TL;DR: Despite recent advances in deep learning, fully automated greenhouse gas plume detection remains a challenge. This paper addresses key obstacles such as data quality control and modeling objectives, demonstrating that convolutional neural networks can achieve operational detection performance when these issues are resolved. A multitask model capable of instance detection and pixelwise segmentation is proposed, along with thresholds for deployment, analysis-ready data, models, and source code for reproducibility.


<details>
  <summary>Details</summary>
Motivation: To achieve operational deployment of a fully automated greenhouse gas plume detection system using imaging spectroscopy, overcoming challenges like data and label quality, spatiotemporal biases, and aligned modeling objectives.

Method: Using convolutional neural networks (CNNs) to address obstacles in greenhouse gas plume detection. The method involves a multitask model that learns both instance detection and pixelwise segmentation simultaneously, evaluated across different emission source types and regions.

Result: The multitask model successfully leads towards an operational pathway for greenhouse gas plume detection, with identified thresholds for deployment and provision of analysis-ready data, models, and source code for reproducibility.

Conclusion: Convolutional neural networks can achieve operational detection performance for greenhouse gas plumes when key obstacles are alleviated. The paper provides resources and standards to facilitate future contributions to the field.

Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume
detection system remains an elusive goal for imaging spectroscopy missions,
despite recent advances in deep learning approaches. With the dramatic increase
in data availability, however, automation continues to increase in importance
for natural and anthropogenic emissions monitoring. This work reviews and
addresses several key obstacles in the field: data and label quality control,
prevention of spatiotemporal biases, and correctly aligned modeling objectives.
We demonstrate through rigorous experiments using multicampaign data from
airborne and spaceborne instruments that convolutional neural networks (CNNs)
are able to achieve operational detection performance when these obstacles are
alleviated. We demonstrate that a multitask model that learns both instance
detection and pixelwise segmentation simultaneously can successfully lead
towards an operational pathway. We evaluate the model's plume detectability
across emission source types and regions, identifying thresholds for
operational deployment. Finally, we provide analysis-ready data, models, and
source code for reproducibility, and work to define a set of best practices and
validation standards to facilitate future contributions to the field.

</details>


### [69] [TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction](https://arxiv.org/abs/2505.21807)
*Tommy Xu,Zhitian Zhang,Xiangyu Sun,Lauren Kelly Zung,Hossein Hajimirsadeghi,Greg Mori*

Main category: cs.LG

TL;DR: 本文提出了一种利用强化学习训练的基于推理的大型语言模型（LLMs）的新方法，以在表格数据上进行更准确且可解释的预测。实验表明，该模型在金融基准数据集上的表现优于大多数现有的LLMs。


<details>
  <summary>Details</summary>
Motivation: 尽管梯度提升机和一些深度学习模型在表格数据上表现出色，但它们通常缺乏可解释性；而大型语言模型在表格数据预测方面表现不佳。因此，需要一种能够在保证准确性的同时提高可解释性的新方法。

Method: 提出了一种结合强化学习和推理能力的LLMs的方法，通过自定义奖励函数引导模型不仅追求高预测准确性，还要生成人类可理解的预测原因。

Result: 实验结果表明，所提出的模型在金融基准数据集上实现了有希望的性能，并且优于大多数现有的LLMs。

Conclusion: 该研究表明，利用推理能力的LLMs可以通过强化学习实现更准确和可解释的表格数据分析，为实际应用提供了新的可能性。

Abstract: Predictive modeling on tabular data is the cornerstone of many real-world
applications. Although gradient boosting machines and some recent deep models
achieve strong performance on tabular data, they often lack interpretability.
On the other hand, large language models (LLMs) have demonstrated powerful
capabilities to generate human-like reasoning and explanations, but remain
under-performed for tabular data prediction. In this paper, we propose a new
approach that leverages reasoning-based LLMs, trained using reinforcement
learning, to perform more accurate and explainable predictions on tabular data.
Our method introduces custom reward functions that guide the model not only
toward high prediction accuracy but also toward human-understandable reasons
for its predictions. Experimental results show that our model achieves
promising performance on financial benchmark datasets, outperforming most
existing LLMs.

</details>


### [70] [Optimizing Data Augmentation through Bayesian Model Selection](https://arxiv.org/abs/2505.21813)
*Madi Matymov,Ba-Hien Tran,Michael Kampffmeyer,Markus Heinonen,Maurizio Filippone*

Main category: cs.LG

TL;DR: The paper proposes a novel framework for optimizing Data Augmentation (DA) by interpreting augmentation parameters as model (hyper)-parameters and using the Evidence Lower BOund (ELBO) for optimization.


<details>
  <summary>Details</summary>
Motivation: Data Augmentation is crucial for improving robustness and generalization in machine learning, but choosing appropriate DA strategies and parameters is challenging and often relies on trial-and-error or expensive optimization.

Method: The authors take a probabilistic view of DA, treating augmentation parameters as model (hyper)-parameters. They optimize the marginal likelihood with respect to these parameters as a Bayesian model selection problem, deriving a tractable ELBO for joint optimization with model parameters.

Result: Experiments on computer vision tasks demonstrate that the approach improves calibration and yields more robust performance compared to fixed or no augmentation.

Conclusion: This work establishes a rigorous foundation for optimizing DA using Bayesian principles, offering significant potential for advancing robust machine learning.

Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and
generalization of modern machine learning. However, when deciding on DA
strategies it is critical to choose parameters carefully, and this can be a
daunting task which is traditionally left to trial-and-error or expensive
optimization based on validation performance. In this paper, we counter these
limitations by proposing a novel framework for optimizing DA. In particular, we
take a probabilistic view of DA, which leads to the interpretation of
augmentation parameters as model (hyper)-parameters, and the optimization of
the marginal likelihood with respect to these parameters as a Bayesian model
selection problem. Due to its intractability, we derive a tractable Evidence
Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly
with model parameters. We provide extensive theoretical results on variational
approximation quality, generalization guarantees, invariance properties, and
connections to empirical Bayes. Through experiments on computer vision tasks,
we show that our approach improves calibration and yields robust performance
over fixed or no augmentation. Our work provides a rigorous foundation for
optimizing DA through Bayesian principles with significant potential for robust
machine learning.

</details>


### [71] [Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations](https://arxiv.org/abs/2505.21824)
*Praveen Kumar,Vincent T. Metzger,Scott A. Malec*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised framework using Non-negative Matrix Factorization (NMF) and statistical techniques to identify individuals at risk of developing type 2 diabetes mellitus (T2DM), addressing the limitation of supervised learning methods. It leverages multimorbidity and polypharmacy patterns among diagnosed T2DM patients for risk estimation in undiagnosed individuals.


<details>
  <summary>Details</summary>
Motivation: The global prevalence of T2DM is increasing, leading to significant health and economic challenges. Early detection of at-risk individuals is critical, but existing machine learning approaches often rely on supervised learning, which is limited by the lack of confirmed negative cases.

Method: A novel unsupervised framework integrating Non-negative Matrix Factorization (NMF) with statistical techniques. It identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate T2DM risk in undiagnosed individuals.

Result: The method provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, potentially improving patient outcomes and reducing the future health and economic burden of T2DM.

Conclusion: The proposed unsupervised framework offers a promising approach for early detection of T2DM risk, leveraging data-driven insights from comorbidity and medication usage.

Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus
(T2DM), is rapidly increasing, posing significant health and economic
challenges. T2DM not only disrupts blood glucose regulation but also damages
vital organs such as the heart, kidneys, eyes, nerves, and blood vessels,
leading to substantial morbidity and mortality. In the US alone, the economic
burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of
individuals at risk is critical to mitigating these impacts. While machine
learning approaches for T2DM prediction are increasingly adopted, many rely on
supervised learning, which is often limited by the lack of confirmed negative
cases. To address this limitation, we propose a novel unsupervised framework
that integrates Non-negative Matrix Factorization (NMF) with statistical
techniques to identify individuals at risk of developing T2DM. Our method
identifies latent patterns of multimorbidity and polypharmacy among diagnosed
T2DM patients and applies these patterns to estimate the T2DM risk in
undiagnosed individuals. By leveraging data-driven insights from comorbidity
and medication usage, our approach provides an interpretable and scalable
solution that can assist healthcare providers in implementing timely
interventions, ultimately improving patient outcomes and potentially reducing
the future health and economic burden of T2DM.

</details>


### [72] [Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones](https://arxiv.org/abs/2505.21825)
*Parsa Mirtaheri,Ezra Edelman,Samy Jelassi,Eran Malach,Enric Boix-Adsera*

Main category: cs.LG

TL;DR: 在推理时计算的研究中，尽管顺序扩展（如更长的思维链）和并行扩展（如多个短思维链的多数投票）都表现出色，但最优的推理时计算分配仍不明确。本文通过图连通性问题展示了顺序扩展在某些情况下具有指数级优势，并通过多种语言模型进行了验证。


<details>
  <summary>Details</summary>
Motivation: 研究推理时计算的最优分配方式，特别是顺序扩展与并行扩展的优劣对比。

Method: 通过图连通性问题设置推理场景，比较顺序扩展与并行扩展的表现，并使用包括从头训练用于图连通性的模型和大型推理模型在内的多种语言模型进行实验验证。

Result: 发现存在一些基于挑战性图分布的图连通性问题场景，在这些场景中顺序扩展相对于并行扩展具有指数级优势。

Conclusion: 顺序扩展在特定的推理场景中相较于并行扩展有显著的优势，这为推理时计算资源的优化分配提供了新的见解。

Abstract: Inference-time computation has emerged as a promising scaling axis for
improving large language model reasoning. However, despite yielding impressive
performance, the optimal allocation of inference-time computation remains
poorly understood. A central question is whether to prioritize sequential
scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority
voting across multiple short chains of thought). In this work, we seek to
illuminate the landscape of test-time scaling by demonstrating the existence of
reasoning settings where sequential scaling offers an exponential advantage
over parallel scaling. These settings are based on graph connectivity problems
in challenging distributions of graphs. We validate our theoretical findings
with comprehensive experiments across a range of language models, including
models trained from scratch for graph connectivity with different chain of
thought strategies as well as large reasoning models.

</details>


### [73] [In Search of Adam's Secret Sauce](https://arxiv.org/abs/2505.21829)
*Antonio Orvieto,Robert Gower*

Main category: cs.LG

TL;DR: 通过广泛的实验研究，发现Adam优化器在训练transformer语言模型时表现出色的关键在于其动量参数相等的设置，这不仅性能稳健且有新的理论解释。


<details>
  <summary>Details</summary>
Motivation: 理解Adam在训练基于transformer的语言模型时为何如此有效是优化领域的重要课题。为了深入探讨，已提出了Adam的一些简化变体，如带符号梯度和带符号动量的方法。

Method: 进行了广泛的经验研究，训练了超过1,300个不同数据配置和规模的语言模型，比较Adam与几种已知简化变体的性能。特别关注带符号动量方法的表现，并分析了Adam动量参数相等的影响。

Result: 发现带符号动量方法虽然比SGD快，但始终不如Adam表现好。而将Adam的动量参数设为相等能保持接近最优的性能，并允许新的理论解释。

Conclusion: Adam在动量参数相等时，不仅性能稳健，还提供了一种自然的在线算法来估计梯度的均值和方差，该算法源于平均场高斯变分推断视角。

Abstract: Understanding the remarkable efficacy of Adam when training transformer-based
language models has become a central research topic within the optimization
community. To gain deeper insights, several simplifications of Adam have been
proposed, such as the signed gradient and signed momentum methods. In this
work, we conduct an extensive empirical study - training over 1,300 language
models across different data configurations and scales - comparing Adam to
several known simplified variants. We find that signed momentum methods are
faster than SGD, but consistently underperform relative to Adam, even after
careful tuning of momentum, clipping setting and learning rates. However, our
analysis reveals a compelling option that preserves near-optimal performance
while allowing for new insightful reformulations: constraining the Adam
momentum parameters to be equal. Beyond robust performance, this choice affords
new theoretical insights, highlights the "secret sauce" on top of signed
momentum, and grants a precise statistical interpretation: we show that Adam in
this setting implements a natural online algorithm for estimating the mean and
variance of gradients-one that arises from a mean-field Gaussian variational
inference perspective.

</details>


### [74] [TuneComp: Joint Fine-tuning and Compression for Large Foundation Models](https://arxiv.org/abs/2505.21835)
*Xiangyu Chen,Jing Liu,Ye Wang,Matthew Brand,Pu,Wang,Toshiaki Koike-Akino*

Main category: cs.LG

TL;DR: Joint fine-tuning and compression significantly outperforms sequential compression methods in reducing model size while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the issue of sacrificing performance and creating unnecessarily large intermediate models during post-training compression.

Method: Propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure directly constructed for the downstream task.

Result: Experiments show joint fine-tuning and compression outperforms other sequential compression methods.

Conclusion: Joint fine-tuning and compression is an effective approach to reduce model size without sacrificing performance.

Abstract: To reduce model size during post-training, compression methods, including
knowledge distillation, low-rank approximation, and pruning, are often applied
after fine-tuning the model. However, sequential fine-tuning and compression
sacrifices performance, while creating a larger than necessary model as an
intermediate step. In this work, we aim to reduce this gap, by directly
constructing a smaller model while guided by the downstream task. We propose to
jointly fine-tune and compress the model by gradually distilling it to a pruned
low-rank structure. Experiments demonstrate that joint fine-tuning and
compression significantly outperforms other sequential compression methods.

</details>


### [75] [An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints](https://arxiv.org/abs/2505.21841)
*Jiahui Zhu,Kihyun Yu,Dabeen Lee,Xin Liu,Honghao Wei*

Main category: cs.LG

TL;DR: The paper introduces OMDPD, an algorithm for online safe reinforcement learning in adversarial settings with optimal regret and constraint violation bounds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for constrained Markov decision processes (CMDPs) can handle sublinear regret under stochastic constraints but struggle in adversarial settings where constraints are unknown, time-varying, and potentially adversarially designed.

Method: Proposes the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm which addresses online CMDPs with anytime adversarial constraints without relying on Slater's condition or a strictly known safe policy.

Result: OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)). Access to accurate estimates of rewards and transitions can further improve these bounds.

Conclusion: The results provide practical guarantees for safe decision-making in adversarial environments.

Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic
environments, with applications in autonomous driving, robotics, and
cybersecurity. The objective is to learn optimal policies that maximize rewards
while satisfying safety constraints modeled by constrained Markov decision
processes (CMDPs). Existing methods achieve sublinear regret under stochastic
constraints but often fail in adversarial settings, where constraints are
unknown, time-varying, and potentially adversarially designed. In this paper,
we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the
first to address online CMDPs with anytime adversarial constraints. OMDPD
achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K))
without relying on Slater's condition or the existence of a strictly known safe
policy. We further show that access to accurate estimates of rewards and
transitions can further improve these bounds. Our results offer practical
guarantees for safe decision-making in adversarial environments.

</details>


### [76] [A Provable Approach for End-to-End Safe Reinforcement Learning](https://arxiv.org/abs/2505.21852)
*Akifumi Wachi,Kohei Miyaguchi,Takumi Tanabe,Rei Sato,Youhei Akimoto*

Main category: cs.LG

TL;DR: The paper introduces Provably Lifetime Safe RL (PLS), a method combining offline safe RL and safe policy deployment, using return-conditioned supervised learning and Gaussian processes to ensure safety with high probability while achieving near-optimal target returns.


<details>
  <summary>Details</summary>
Motivation: Safe reinforcement learning requires ensuring the safety of a policy throughout its entire process from learning to operation, which existing paradigms struggle to achieve effectively.

Method: PLS learns a policy offline via return-conditioned supervised learning, then deploys it while cautiously optimizing a limited set of parameters (target returns) using Gaussian processes. Theoretical justification for GPs is provided by analyzing the mathematical relationship between target and actual returns.

Result: PLS outperforms baselines in both safety and reward performance, achieving the goal of obtaining high rewards while maintaining safety throughout the policy's lifetime.

Conclusion: PLS addresses the challenge of ensuring safety throughout the RL process by integrating offline safe RL with safe policy deployment, proving safety with high probability and achieving near-optimal target returns.

Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure
the safety of a policy throughout the entire process, from learning to
operation. However, existing safe RL paradigms inherently struggle to achieve
this objective. We propose a method, called Provably Lifetime Safe RL (PLS),
that integrates offline safe RL with safe policy deployment to address this
challenge. Our proposed method learns a policy offline using return-conditioned
supervised learning and then deploys the resulting policy while cautiously
optimizing a limited set of parameters, known as target returns, using Gaussian
processes (GPs). Theoretically, we justify the use of GPs by analyzing the
mathematical relationship between target and actual returns. We then prove that
PLS finds near-optimal target returns while guaranteeing safety with high
probability. Empirically, we demonstrate that PLS outperforms baselines both in
safety and reward performance, thereby achieving the longstanding goal to
obtain high rewards while ensuring the safety of a policy throughout the
lifetime from learning to operation.

</details>


### [77] [Revisiting Bayesian Model Averaging in the Era of Foundation Models](https://arxiv.org/abs/2505.21857)
*Mijung Park*

Main category: cs.LG

TL;DR: The paper revisits Bayesian Model Averaging (BMA) and proposes Optimizable Model Averaging (OMA) to enhance classification performance using pre-trained foundation models.


<details>
  <summary>Details</summary>
Motivation: To improve classification performance on image and text data by leveraging pre-trained and lightly fine-tuned foundation models through ensemble methods.

Method: Introduces trainable linear classifiers with frozen features from pre-trained models for BMA, and proposes OMA which directly optimizes model ensemble weights by minimizing prediction entropy.

Result: These approaches allow for the incorporation of future, potentially superior foundation models, enhancing performance on challenging classification tasks.

Conclusion: BMA and OMA provide principled ways to ensemble models, improving classification tasks and accommodating better foundation models as they develop.

Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA)
paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to
enhance the classification performance on image and text data. To make BMA
tractable under foundation models, we introduce trainable linear classifiers
that take frozen features from the pre-trained foundation models as inputs. The
model posteriors over the linear classifiers tell us which linear heads and
frozen features are better suited for a given dataset, resulting in a
principled model ensembling method. Furthermore, we propose a computationally
cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize
the model ensemble weights, just like those weights based on model posterior
distributions in BMA, by reducing the amount of surprise (expected entropy of
the predictions) we get from predictions of ensembled models. With the rapid
development of foundation models, these approaches will enable the
incorporation of future, possibly significantly better foundation models to
enhance the performance of challenging classification tasks.

</details>


### [78] [Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning](https://arxiv.org/abs/2505.21877)
*Hongyao Chen,Tianyang Xu,Xiaojun Wu,Josef Kittler*

Main category: cs.LG

TL;DR: The paper proposes Hybrid Batch Normalisation (HBN), a customised normalisation approach for federated learning that separates the update of statistical parameters from learnable parameters and introduces a learnable hybrid distribution factor.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with non-independent and identically distributed data among client nodes, and standard Batch Normalisation degrades performance due to lack of coherent methodology for updating BN statistical parameters.

Method: HBN separates updates of statistical parameters (means and variances) from learnable parameters (those requiring gradient updates) and provides unbiased estimates of global statistical parameters in distributed scenarios. A learnable hybrid distribution factor is introduced to allow each node to adaptively mix current batch statistics with global statistics.

Result: HBN shows promising improvements in federated learning performance across various settings, particularly with small batch sizes and heterogeneous data.

Conclusion: HBN serves as a powerful plugin to enhance federated learning performance.

Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network
training to harmonise the input-output distributions for each batch of data.
However, federated learning, a distributed learning paradigm, faces the
challenge of dealing with non-independent and identically distributed data
among the client nodes. Due to the lack of a coherent methodology for updating
BN statistical parameters, standard BN degrades the federated learning
performance. To this end, it is urgent to explore an alternative normalisation
solution for federated learning. In this work, we resolve the dilemma of the BN
layer in federated learning by developing a customised normalisation approach,
Hybrid Batch Normalisation (HBN). HBN separates the update of statistical
parameters (i.e. , means and variances used for evaluation) from that of
learnable parameters (i.e. , parameters that require gradient updates),
obtaining unbiased estimates of global statistical parameters in distributed
scenarios. In contrast with the existing solutions, we emphasise the supportive
power of global statistics for federated learning. The HBN layer introduces a
learnable hybrid distribution factor, allowing each computing node to
adaptively mix the statistical parameters of the current batch with the global
statistics. Our HBN can serve as a powerful plugin to advance federated
learning performance. It reflects promising merits across a wide range of
federated learning settings, especially for small batch sizes and heterogeneous
data.

</details>


### [79] [HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis](https://arxiv.org/abs/2505.21882)
*Ruijie Li,Xiang Zhao,Qiao Ning,Shikai Guo*

Main category: cs.LG

TL;DR: In tennis tournaments, momentum significantly influences match outcomes. This study introduces Momentum Score (MS) to quantify player's momentum and HydraNet framework to model MS using 32 dimensions of athlete performance. HydraNet includes Hydra module for capturing explicit and implicit momentum, Versus Learning method and Collaborative-Adversarial Attention Mechanism (CAAM). A large-scale dataset from Wimbledon and US Open is constructed and used for validation.


<details>
  <summary>Details</summary>
Motivation: Momentum in tennis tournaments is crucial but underexplored in terms of effective modeling and multi-granularity analysis.

Method: Define a novel Momentum Score (MS) metric and design HydraNet framework integrating 32 heterogeneous dimensions of athletes' performance. HydraNet consists of Hydra module based on state-space duality framework, Versus Learning method and Collaborative-Adversarial Attention Mechanism (CAAM).

Result: Extensive experimental evaluations demonstrate that the MS metric provides actionable insights into how momentum impacts outcomes at different granularities.

Conclusion: HydraNet framework establishes a new foundation for momentum modeling and sports analysis.

Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects
the dynamic shifts in performance of athletes that can decisively influence
match outcomes. Despite its significance, momentum in terms of effective
modeling and multi-granularity analysis across points, games, sets, and matches
in tennis tournaments remains underexplored. In this study, we define a novel
Momentum Score (MS) metric to quantify a player's momentum level in
multi-granularity tennis tournaments, and design HydraNet, a momentum-driven
state-space duality-based framework, to model MS by integrating thirty-two
heterogeneous dimensions of athletes performance in serve, return, psychology
and fatigue. HydraNet integrates a Hydra module, which builds upon a
state-space duality (SSD) framework, capturing explicit momentum with a
sliding-window mechanism and implicit momentum through cross-game state
propagation. It also introduces a novel Versus Learning method to better
enhance the adversarial nature of momentum between the two athletes at a macro
level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for
capturing and integrating intra-player and inter-player dynamic momentum at a
micro level. Additionally, we construct a million-level tennis cross-tournament
dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate
the multi-granularity modeling capability of HydraNet for the MS metric on this
dataset. Extensive experimental evaluations demonstrate that the MS metric
constructed by the HydraNet framework provides actionable insights into how
momentum impacts outcomes at different granularities, establishing a new
foundation for momentum modeling and sports analysis. To the best of our
knowledge. The source code and datasets are available at
https://github.com/ReyJerry/HydraNet.

</details>


### [80] [SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training](https://arxiv.org/abs/2505.21893)
*Xiaomeng Yang,Zhiyu Tan,Junyan Wang,Zhijian Zhou,Hao Li*

Main category: cs.LG

TL;DR: The paper addresses instability and off-policy bias issues in diffusion models preference learning through proposing DPO-C&M and SDPO methods, which outperform standard Diffusion-DPO in experiments.


<details>
  <summary>Details</summary>
Motivation: Preference learning is crucial for aligning generative models with human expectations. However, existing approaches like Diffusion-DPO suffer from timestep-dependent instability and off-policy bias.

Method: The authors propose DPO-C&M to improve stability by clipping and masking uninformative timesteps and partially mitigating off-policy bias. They also introduce SDPO, a framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process.

Result: Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B show that both DPO-C&M and SDPO outperform standard Diffusion-DPO. Specifically, SDPO achieves superior VBench scores, human preference alignment, and training robustness.

Conclusion: The findings underline the significance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.

Abstract: Preference learning has become a central technique for aligning generative
models with human expectations. Recently, it has been extended to diffusion
models through methods like Direct Preference Optimization (DPO). However,
existing approaches such as Diffusion-DPO suffer from two key challenges:
timestep-dependent instability, caused by a mismatch between the reverse and
forward diffusion processes and by high gradient variance in early noisy
timesteps, and off-policy bias arising from the mismatch between optimization
and data collection policies. We begin by analyzing the reverse diffusion
trajectory and observe that instability primarily occurs at early timesteps
with low importance weights. To address these issues, we first propose
DPO-C\&M, a practical strategy that improves stability by clipping and masking
uninformative timesteps while partially mitigating off-policy bias. Building on
this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a
principled framework that incorporates importance sampling into the objective
to fully correct for off-policy bias and emphasize informative updates during
the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and
Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO,
with SDPO achieving superior VBench scores, human preference alignment, and
training robustness. These results highlight the importance of timestep-aware,
distribution-corrected optimization in diffusion-based preference learning.

</details>


### [81] [Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization](https://arxiv.org/abs/2505.21895)
*Cameron Gordon,Yiping Ji,Hemanth Saratchandran,Paul Albert,Simon Lucey*

Main category: cs.LG

TL;DR: Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning method, but it has limitations due to its low-rank constraint. Recent work has used a sinusoidal transformation to increase stable rank without additional parameters. This paper explores applying this technique in Post-Training Quantization. The authors develop a theoretical analysis and demonstrate that the expressivity gains persist after quantization, resulting in highly compressed adapters with little performance loss. They validate their approach across various tasks achieving significant memory savings while maintaining competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to explore whether the sine-activated technique that increases the stable rank of low-rank adapters can be successfully applied within the context of Post-Training Quantization, allowing benefits to be retained even after model compression.

Method: The authors extend the sinusoidal transformation framework to quantized LoRA adapters and conduct a theoretical analysis showing the link between the stable rank of a quantized adapter and its full-precision counterpart. This motivates the use of rank-enhancing functions under quantization.

Result: The results show that the expressivity gains from a sinusoidal non-linearity persist after quantization, leading to highly compressed adapters with negligible performance loss.

Conclusion: This paper concludes that applying the sinusoidal transformation in Post-Training Quantization is effective for creating highly compressed adapters with minimal performance degradation.

Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for
parameter-efficient fine-tuning, offering substantial reductions in trainable
parameters by modeling updates as the product of two low-rank matrices. While
effective, the low-rank constraint inherently limits representational capacity,
often resulting in reduced performance compared to full-rank fine-tuning.
Recent work by Ji et al. (2025) has addressed this limitation by applying a
fixed-frequency sinusoidal transformation to low-rank adapters, increasing
their stable rank without introducing additional parameters. This raises a
crucial question: can the same sine-activated technique be successfully applied
within the context of Post-Training Quantization to retain benefits even after
model compression? In this paper, we investigate this question by extending the
sinusoidal transformation framework to quantized LoRA adapters. We develop a
theoretical analysis showing that the stable rank of a quantized adapter is
tightly linked to that of its full-precision counterpart, motivating the use of
such rank-enhancing functions even under quantization. Our results demonstrate
that the expressivity gains from a sinusoidal non-linearity persist after
quantization, yielding highly compressed adapters with negligible loss in
performance. We validate our approach across a range of fine-tuning tasks for
language, vision and text-to-image generation achieving significant memory
savings while maintaining competitive accuracy.

</details>


### [82] [Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding](https://arxiv.org/abs/2505.21908)
*Hanyin Wang,Zhenbang Wu,Gururaj Kolar,Hariprasad Korsapati,Brian Bartlett,Bryan Hull,Jimeng Sun*

Main category: cs.LG

TL;DR: DRG-Sapphire is a model built on Qwen2.5-7B that uses reinforcement learning to automate DRG coding from clinical notes, achieving state-of-the-art accuracy and enhancing explainability.


<details>
  <summary>Details</summary>
Motivation: Diagnosis-Related Group (DRG) codes are important for hospital reimbursement but assigning them is labor-intensive. Existing large language models struggle with DRG coding due to the out-of-distribution nature of the task.

Method: The model, DRG-Sapphire, uses large-scale reinforcement learning with rule-based rewards and a series of enhancements to address domain-specific challenges. It is built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO).

Result: DRG-Sapphire achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments.

Conclusion: The study highlights broader challenges in applying reinforcement learning to knowledge-intensive, out-of-distribution tasks and suggests that scaling supervised fine-tuning may be more effective and computationally efficient than scaling reinforcement learning alone for such tasks.

Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement
and operations but require labor-intensive assignment. Large Language Models
(LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of
the task: pretraining corpora rarely contain private clinical or billing data.
We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL)
for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained
with Group Relative Policy Optimization (GRPO) using rule-based rewards,
DRG-Sapphire introduces a series of RL enhancements to address domain-specific
challenges not seen in previous mathematical tasks. Our model achieves
state-of-the-art accuracy on the MIMIC-IV benchmark and generates
physician-validated reasoning for DRG assignments, significantly enhancing
explainability. Our study further sheds light on broader challenges of applying
RL to knowledge-intensive, OOD tasks. We observe that RL performance scales
approximately linearly with the logarithm of the number of supervised
fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally
constrained by the domain knowledge encoded in the base model. For OOD tasks
like DRG coding, strong RL performance requires sufficient knowledge infusion
prior to RL. Consequently, scaling SFT may be more effective and
computationally efficient than scaling RL alone for such tasks.

</details>


### [83] [Taming Transformer Without Using Learning Rate Warmup](https://arxiv.org/abs/2505.21910)
*Xianbiao Qi,Yelin He,Jiaquan Ye,Chun-Guang Li,Bojia Zi,Xili Dai,Qin Zou,Rong Xiao*

Main category: cs.LG

TL;DR: 在大规模Transformer训练中，提出了一种新的优化策略以防止模型崩溃，该策略通过平滑权重更新避免恶性熵坍缩，从而无需学习率预热即可稳定训练。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer训练过程中出现的模型崩溃现象（即恶性熵坍缩）是由于${\bW_q}^{\top} \bW_k$的谱能量集中导致的，这限制了模型的稳定训练。

Method: 基于Weyl's Inequality，提出了一种新的优化策略：如果比例$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$超过阈值，则将学习率自动限制为$\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$的加权倍数，从而使连续步骤中的权重更新变得平滑，防止谱能量集中到少数方向。

Result: 通过在ViT、Swin-Transformer和GPT上的广泛实验表明，该优化策略可以有效且稳定地训练这些Transformer模型，而无需使用学习率预热。

Conclusion: 提出的优化策略解决了Transformer训练中的模型崩溃问题，实现了无需技术手段如学习率预热的大规模稳定训练。

Abstract: Scaling Transformer to a large scale without using some technical tricks such
as learning rate warump and using an obviously lower learning rate is an
extremely challenging task, and is increasingly gaining more attention. In this
paper, we provide a theoretical analysis for the process of training
Transformer and reveal the rationale behind the model crash phenomenon in the
training process, termed \textit{spectral energy concentration} of
${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse,
where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the
key in Transformer, respectively. To remedy this problem, motivated by
\textit{Weyl's Inequality}, we present a novel optimization strategy, \ie,
making the weight updating in successive steps smooth -- if the ratio
$\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a
threshold, we will automatically bound the learning rate to a weighted multiple
of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla
\bW_t$ is the updating quantity in step $t$. Such an optimization strategy can
prevent spectral energy concentration to only a few directions, and thus can
avoid malignant entropy collapse which will trigger the model crash. We conduct
extensive experiments using ViT, Swin-Transformer and GPT, showing that our
optimization strategy can effectively and stably train these Transformers
without using learning rate warmup.

</details>


### [84] [Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing](https://arxiv.org/abs/2505.21918)
*Haruki Kai,Tsuyoshi Okita*

Main category: cs.LG

TL;DR: The paper presents a deep learning algorithm for human activity recognition using an enhanced n-dimensional numerical processing Transformer, showing 10%-15% accuracy improvements over vanilla Transformer across five datasets.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of human activity recognition by utilizing a pretrained language model and developing an enhanced n-dimensional numerical processing Transformer.

Method: Building a pretrained language model based on Transformer architecture and proposing an enhanced n-dimensional numerical processing Transformer with features like embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and linear transformation in the output layer.

Result: The proposed model showed 10%-15% improvements in accuracy compared to the vanilla Transformer when evaluated across five different datasets.

Conclusion: The enhanced n-dimensional numerical processing Transformer effectively improves the accuracy of human activity recognition tasks.

Abstract: We developed a deep learning algorithm for human activity recognition using
sensor signals as input. In this study, we built a pretrained language model
based on the Transformer architecture, which is widely used in natural language
processing. By leveraging this pretrained model, we aimed to improve
performance on the downstream task of human activity recognition. While this
task can be addressed using a vanilla Transformer, we propose an enhanced
n-dimensional numerical processing Transformer that incorporates three key
features: embedding n-dimensional numerical data through a linear layer,
binning-based pre-processing, and a linear transformation in the output layer.
We evaluated the effectiveness of our proposed model across five different
datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15%
improvements in accuracy.

</details>


### [85] [FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design](https://arxiv.org/abs/2505.21923)
*Asal Mehradfar,Xuzhe Zhao,Yilun Huang,Emir Ceyani,Yankai Yang,Shihao Han,Hamidreza Aghasi,Salman Avestimehr*

Main category: cs.LG

TL;DR: 设计满足性能规范的模拟电路是一个复杂的过程，FALCON是一种全新的机器学习框架，通过拓扑选择和布局约束优化实现全自动、规范驱动的模拟电路合成。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计过程复杂，涵盖拓扑选择、参数推断和布局可行性，需要一种自动化方法来简化这一过程。

Method: FALCON首先使用性能驱动分类器选择合适的电路拓扑结构，然后利用自定义的边中心图神经网络将电路拓扑和参数映射到性能，通过可微分布局成本进行梯度基础参数推断。

Result: FALCON在拓扑推断中展示了>99%的准确性，性能预测相对误差<10%，并且每个实例的设计时间不到1秒。

Conclusion: FALCON为端到端模拟电路设计自动化提供了实用且可扩展的基础模型。

Abstract: Designing analog circuits from performance specifications is a complex,
multi-stage process encompassing topology selection, parameter inference, and
layout feasibility. We introduce FALCON, a unified machine learning framework
that enables fully automated, specification-driven analog circuit synthesis
through topology selection and layout-constrained optimization. Given a target
performance, FALCON first selects an appropriate circuit topology using a
performance-driven classifier guided by human design heuristics. Next, it
employs a custom, edge-centric graph neural network trained to map circuit
topology and parameters to performance, enabling gradient-based parameter
inference through the learned forward model. This inference is guided by a
differentiable layout cost, derived from analytical equations capturing
parasitic and frequency-dependent effects, and constrained by design rules. We
train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave
circuits, generated and simulated using Cadence Spectre across 20
expert-designed topologies. Through this evaluation, FALCON demonstrates >99\%
accuracy in topology inference, <10\% relative error in performance prediction,
and efficient layout-aware design that completes in under 1 second per
instance. Together, these results position FALCON as a practical and extensible
foundation model for end-to-end analog circuit design automation.

</details>


### [86] [Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets](https://arxiv.org/abs/2505.21930)
*Dongyue Li,Ziniu Zhang,Lu Wang,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种集合方法，用于将语言模型微调到多个数据集。通过将数据集分组并训练每个组的适配器，再进行加权组合形成集合，利用低秩适配的第一阶近似性质来快速获得数据集组合的微调性能。实验证明该方法在Llama和GPT模型上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如量化LoRA（QLoRA）在适应单一数据集时效率很高，但在多个不同任务的数据集上进行训练时，如何设计高效的适应方法仍不清楚。

Method: 提出使用多个更小的适配器的集合，而不是每个任务一个适配器。设计了一个高效算法，将n个数据集分成m组（m通常远小于n），为每组训练一个适配器，然后进行加权组合形成集合。利用低秩适配的第一阶近似性质来快速获得数据集组合的微调性能。

Result: 实验证明，该方法在最多34亿参数的模型上误差小于1%，估计真实微调性能误差在5%以内，计算速度比基础微调快105倍。在十个文本分类任务上，该方法比QLoRA平均测试准确率高出10%，仅增加9%的FLOPs。对于34亿参数的Llama模型，QLoRA集合比单独的QLoRA测试准确率高出3%，仅增加8%的FLOPs。

Conclusion: 提出的集合方法可以有效提高语言模型在多个数据集上的微调性能，同时保持较低的计算成本。

Abstract: This paper develops an ensemble method for fine-tuning a language model to
multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are
efficient when adapting to a single dataset. When training on multiple datasets
of different tasks, a common setup in practice, it remains unclear how to
design an efficient adaptation for fine-tuning language models. We propose to
use an ensemble of multiple smaller adapters instead of a single adapter per
task. We design an efficient algorithm that partitions $n$ datasets into $m$
groups, where $m$ is typically much smaller than $n$ in practice, and train one
adapter for each group before taking a weighted combination to form the
ensemble. The algorithm leverages a first-order approximation property of
low-rank adaptation to quickly obtain the fine-tuning performances of dataset
combinations since methods like LoRA stay close to the base model. Hence, we
use the gradients of the base model to estimate its behavior during
fine-tuning. Empirically, this approximation holds with less than $1\%$ error
on models with up to $34$ billion parameters, leading to an estimation of true
fine-tuning performances under $5\%$ error while speeding up computation
compared to base fine-tuning by $105$ times. When applied to fine-tune Llama
and GPT models on ten text classification tasks, our approach provides up to
$10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On
a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test
accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.

</details>


### [87] [Continual Learning Beyond Experience Rehearsal and Full Model Surrogates](https://arxiv.org/abs/2505.21942)
*Prashant Bhat,Laurens Niesten,Elahe Arani,Bahram Zonooz*

Main category: cs.LG

TL;DR: SPARC is a new Continual Learning approach that does not need experience rehearsal or full-model surrogates, achieving high parameter efficiency and good performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Continual learning in deep neural networks faces the issue of catastrophic forgetting. Current solutions are either memory-heavy or computationally expensive.

Method: SPARC combines task-specific working memories with task-agnostic semantic memory for knowledge consolidation, and uses weight re-normalization in the classification layer to reduce task-specific biases.

Result: SPARC uses only 6% of parameters compared to full-model surrogates and performs better on Seq-TinyImageNet while matching rehearsal-based methods on other benchmarks.

Conclusion: SPARC offers a practical and scalable solution for continual learning under strict efficiency requirements.

Abstract: Continual learning (CL) has remained a significant challenge for deep neural
networks as learning new tasks erases previously acquired knowledge, either
partially or completely. Existing solutions often rely on experience rehearsal
or full model surrogates to mitigate CF. While effective, these approaches
introduce substantial memory and computational overhead, limiting their
scalability and applicability in real-world scenarios. To address this, we
propose SPARC, a scalable CL approach that eliminates the need for experience
rehearsal and full-model surrogates. By effectively combining task-specific
working memories and task-agnostic semantic memory for cross-task knowledge
consolidation, SPARC results in a remarkable parameter efficiency, using only
6% of the parameters required by full-model surrogates. Despite its lightweight
design, SPARC achieves superior performance on Seq-TinyImageNet and matches
rehearsal-based methods on various CL benchmarks. Additionally, weight
re-normalization in the classification layer mitigates task-specific biases,
establishing SPARC as a practical and scalable solution for CL under stringent
efficiency constraints.

</details>


### [88] [Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization](https://arxiv.org/abs/2505.21944)
*Linli Zhou,Bokun Wang,My T. Thai,Tianbao Yang*

Main category: cs.LG

TL;DR: The paper introduces two stochastic primal-dual double block-coordinate algorithms for TPAUC maximization that offer faster convergence and better generalization than previous methods.


<details>
  <summary>Details</summary>
Motivation: TPAUC is a crucial metric in binary classification with imbalanced data, but existing optimization methods are either approximations or inefficient.

Method: Two stochastic primal-dual double block-coordinate algorithms are developed for TPAUC maximization, applicable to both convex and non-convex settings, using updates for both primal and dual variables.

Result: Theoretical analysis shows improved convergence rates over prior methods. Experiments on benchmark datasets confirm superior performance with faster convergence and better generalization.

Conclusion: This work enhances TPAUC optimization, providing practical tools for real-world machine learning tasks.

Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary
classification with imbalanced data, as it focuses on specific ranges of the
true positive rate (TPR) and false positive rate (FPR). However, stochastic
algorithms for TPAUC optimization remain under-explored, with existing methods
either limited to approximated TPAUC loss functions or burdened by sub-optimal
complexities. To overcome these limitations, we introduce two innovative
stochastic primal-dual double block-coordinate algorithms for TPAUC
maximization. These algorithms utilize stochastic block-coordinate updates for
both the primal and dual variables, catering to both convex and non-convex
settings. We provide theoretical convergence rate analyses, demonstrating
significant improvements over prior approaches. Our experimental results, based
on multiple benchmark datasets, validate the superior performance of our
algorithms, showcasing faster convergence and better generalization. This work
advances the state of the art in TPAUC optimization and offers practical tools
for real-world machine learning applications.

</details>


### [89] [EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles](https://arxiv.org/abs/2505.21959)
*Aakriti Agrawal,Mucong Ding,Zora Che,Chenghao Deng,Anirudh Satheesh,Bang An,Bayan Bruss,John Langford,Furong Huang*

Main category: cs.LG

TL;DR: The paper introduces EnsemW2S, a method that uses token-level ensemble to improve weak models' ability to supervise stronger student models. It achieves significant improvements on both in-distribution and out-of-distribution datasets.


<details>
  <summary>Details</summary>
Motivation: With the development of large language models (LLMs) nearing or surpassing human-level performance, it's crucial to find ways for smaller, human-level models to effectively supervise and enhance these powerful models using limited human-level data.

Method: EnsemW2S employs a token-level ensemble strategy that iteratively combines multiple weak experts, addressing their shortcomings and refining their collective ability to supervise stronger student models.

Result: Empirical results show notable improvements: 4% and 3.2% on in-distribution datasets, and up to 6% and 2.28% on out-of-distribution datasets for experts and student models respectively.

Conclusion: EnsemW2S effectively advances weak-to-strong generalization, allowing weak models exposed to only human-level data to generalize to complex, super-human-level tasks.

Abstract: With Large Language Models (LLMs) rapidly approaching and potentially
surpassing human-level performance, it has become imperative to develop
approaches capable of effectively supervising and enhancing these powerful
models using smaller, human-level models exposed to only human-level data. We
address this critical weak-to-strong (W2S) generalization challenge by
proposing a novel method aimed at improving weak experts, by training on the
same limited human-level data, enabling them to generalize to complex,
super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a
token-level ensemble strategy that iteratively combines multiple weak experts,
systematically addressing the shortcomings identified in preceding iterations.
By continuously refining these weak models, we significantly enhance their
collective ability to supervise stronger student models. We extensively
evaluate the generalization performance of both the ensemble of weak experts
and the subsequent strong student model across in-distribution (ID) and
out-of-distribution (OOD) datasets. For OOD, we specifically introduce question
difficulty as an additional dimension for defining distributional shifts. Our
empirical results demonstrate notable improvements, achieving 4\%, and 3.2\%
improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for
experts and student models respectively, underscoring the effectiveness of our
proposed method in advancing W2S generalization.

</details>


### [90] [Judging LLMs on a Simplex](https://arxiv.org/abs/2505.21972)
*Patrick Vossler,Fan Xia,Yifan Mai,Jean Feng*

Main category: cs.LG

TL;DR: The paper explores the use of a geometric framework and Bayesian inference to improve the identifiability and accuracy of rankings when using large language models (LLMs) as judges for evaluating free-form outputs. It uncovers a 'phase transition' in ranking identifiability and demonstrates that Bayesian inference yields more accurate rankings and improves coverage rates.


<details>
  <summary>Details</summary>
Motivation: Automated evaluation of free-form outputs from LLMs is challenging due to the variability of valid answers. The theoretical properties of using LLMs themselves as judges are not well understood, motivating the need for deeper analysis.

Method: A geometric framework representing judges and candidates as points on a probability simplex is used to analyze identifiability of rankings. Theoretical analysis uncovers a 'phase transition' in ranking identifiability. Bayesian inference is then applied to integrate aleatoric and epistemic uncertainties, encoding assumptions as priors and conducting sensitivity analysis.

Result: Empirical evaluations show that Bayesian inference results in more accurate rankings and substantially improves coverage rates across multiple benchmarks.

Conclusion: The study emphasizes the importance of considering both types of uncertainty (aleatoric and epistemic) and adopting a holistic approach to uncertainty quantification when using LLMs as judges.

Abstract: Automated evaluation of free-form outputs from large language models (LLMs)
is challenging because many distinct answers can be equally valid. A common
practice is to use LLMs themselves as judges, but the theoretical properties of
this approach are not yet well understood. We show that a geometric framework
that represents both judges and candidates as points on a probability simplex
can provide helpful insight on what is or is not identifiable using LLM judges.
Our theoretical analysis uncovers a "phase transition" in ranking
identifiability: for binary scoring systems, true rankings are identifiable
even with weak judges under mild assumptions, while rankings become
non-identifiable for three or more scoring levels even with infinite data,
absent additional prior knowledge. This non-identifiability highlights how
uncertainty in rankings stems from not only aleatoric uncertainty (i.e.,
inherent stochasticity in the data) but also epistemic uncertainty regarding
which assumptions hold, an aspect that has received limited attention until
now. To integrate both types of uncertainty, we use Bayesian inference to
encode assumptions as priors and conduct sensitivity analysis of ranking
estimates and credible intervals. Empirical evaluations across multiple
benchmarks demonstrate that Bayesian inference yields more accurate rankings
and substantially improves coverage rates. These results underscore the
importance of taking a more holistic approach to uncertainty quantification
when using LLMs as judges.

</details>


### [91] [BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL](https://arxiv.org/abs/2505.21974)
*Yu-Heng Hung,Kai-Jie Lin,Yu-Heng Lin,Chien-YiWang,Cheng Sun,Ping-Chun Hsieh*

Main category: cs.LG

TL;DR: The paper introduces BOFormer, a deep Q-learning framework using sequence modeling inspired by non-Markovian RL and Transformers for multi-objective Bayesian optimization (MOBO). It addresses the hypervolume identifiability issue in MOBO and outperforms benchmark algorithms in synthetic and real-world problems.


<details>
  <summary>Details</summary>
Motivation: Bayesian optimization has shown success in single-objective optimization, but extending learning-based acquisition functions to multi-objective Bayesian optimization suffers from the hypervolume identifiability issue due to the non-Markovian nature of MOBO problems.

Method: The authors propose BOFormer, which uses a generalized deep Q-learning framework combined with sequence modeling inspired by non-Markovian reinforcement learning and Transformers. This approach is specifically designed to handle the complexities of multi-objective Bayesian optimization.

Result: BOFormer consistently outperforms both rule-based and learning-based benchmark algorithms across various synthetic MOBO problems and real-world multi-objective hyperparameter optimization tasks.

Conclusion: BOFormer presents an effective solution for multi-objective Bayesian optimization by addressing the hypervolume identifiability issue, demonstrating superior performance compared to existing methods.

Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing
black-box functions with the help of a Gaussian process prior and an
acquisition function (AF). Recently, in the context of single-objective BO,
learning-based AFs witnessed promising empirical results given its favorable
non-myopic nature. Despite this, the direct extension of these approaches to
multi-objective Bayesian optimization (MOBO) suffer from the
\textit{hypervolume identifiability issue}, which results from the
non-Markovian nature of MOBO problems. To tackle this, inspired by the
non-Markovian RL literature and the success of Transformers in language
modeling, we present a generalized deep Q-learning framework and propose
\textit{BOFormer}, which substantiates this framework for MOBO via sequence
modeling. Through extensive evaluation, we demonstrate that BOFormer constantly
outperforms the benchmark rule-based and learning-based algorithms in various
synthetic MOBO and real-world multi-objective hyperparameter optimization
problems. We have made the source code publicly available to encourage further
research in this direction.

</details>


### [92] [Two-Stage Feature Generation with Transformer and Reinforcement Learning](https://arxiv.org/abs/2505.21978)
*Wanfu Gao,Zengyao Man,Zebin He,Yuhao Tang,Jun Gao,Kunpeng Liu*

Main category: cs.LG

TL;DR: The paper proposes a Two-Stage Feature Generation (TSFG) framework that combines a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO) to generate high-quality feature sets, significantly improving the predictive performance of machine learning models.


<details>
  <summary>Details</summary>
Motivation: Traditional feature generation methods rely on domain expertise and manual intervention. Automated methods have issues like feature redundancy, inefficiency in feature space exploration, and limited adaptability.

Method: A TSFG framework is proposed which integrates a Transformer-based encoder-decoder architecture with PPO. The encoder-decoder model leverages the Transformer's self-attention mechanism for feature representation and transformation, while PPO dynamically adjusts the feature generation strategy based on task-specific feedback.

Result: TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models.

Conclusion: Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability.

Abstract: Feature generation is a critical step in machine learning, aiming to enhance
model performance by capturing complex relationships within the data and
generating meaningful new features. Traditional feature generation methods
heavily rely on domain expertise and manual intervention, making the process
labor-intensive and challenging to adapt to different scenarios. Although
automated feature generation techniques address these issues to some extent,
they often face challenges such as feature redundancy, inefficiency in feature
space exploration, and limited adaptability to diverse datasets and tasks. To
address these problems, we propose a Two-Stage Feature Generation (TSFG)
framework, which integrates a Transformer-based encoder-decoder architecture
with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG
leverages the Transformer's self-attention mechanism to efficiently represent
and transform features, capturing complex dependencies within the data. PPO
further enhances TSFG by dynamically adjusting the feature generation strategy
based on task-specific feedback, optimizing the process for improved
performance and adaptability. TSFG dynamically generates high-quality feature
sets, significantly improving the predictive performance of machine learning
models. Experimental results demonstrate that TSFG outperforms existing
state-of-the-art methods in terms of feature quality and adaptability.

</details>


### [93] [ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning](https://arxiv.org/abs/2505.21987)
*Zhendong Mi,Zhenglun Kong,Geng Yuan,Shaoyi Huang*

Main category: cs.LG

TL;DR: This paper proposes an efficient pruning method for large language models (LLMs) with two key innovations, achieving up to 18% reduction in perplexity and 63% decrease in pruning time.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitations of existing LLM pruning methods which either have suboptimal pruning performance or low time efficiency during the pruning process.

Method: The method introduces two key innovations: 1) An activation cosine similarity loss-guided pruning metric that considers angular deviation of output activation between dense and pruned models. 2) An activation variance-guided pruning metric that helps preserve semantic distinctions in output activations after pruning.

Result: Experimental results show that the proposed method achieves up to 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

Conclusion: The proposed pruning method efficiently enhances LLM pruning in both accuracy and efficiency.

Abstract: With the rapid expansion of large language models (LLMs), the demand for
memory and computational resources has grown significantly. Recent advances in
LLM pruning aim to reduce the size and computational cost of these models.
However, existing methods often suffer from either suboptimal pruning
performance or low time efficiency during the pruning process. In this work, we
propose an efficient and effective pruning method that simultaneously achieves
high pruning performance and fast pruning speed with improved calibration
efficiency. Our approach introduces two key innovations: (1) An activation
cosine similarity loss-guided pruning metric, which considers the angular
deviation of the output activation between the dense and pruned models. (2) An
activation variance-guided pruning metric, which helps preserve semantic
distinctions in output activations after pruning, enabling effective pruning
with shorter input sequences. These two components can be readily combined to
enhance LLM pruning in both accuracy and efficiency. Experimental results show
that our method achieves up to an 18% reduction in perplexity and up to 63%
decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.

</details>


### [94] [Learning in Compact Spaces with Approximately Normalized Transformers](https://arxiv.org/abs/2505.22014)
*Jörg K. H. Franke,Urs Spiegelhalter,Marianna Nezhurina,Jenia Jitsev,Frank Hutter,Michael Hefenbrock*

Main category: cs.LG

TL;DR: The paper proposes an approximate normalization method called anTransformer that constrains parameter norms and normalizes representations through scalar multiplications, inspired by high-dimensional random vectors' norm concentration. It accelerates GPT training convergence by 40% over QK normalization with minimal runtime increase, simplifies hyperparameter tuning, and supports larger batch sizes while preserving GPT's favorable scaling properties.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deep learning such as overfitting, numerical instability, and increasing variance in the residual stream without relying on traditional regularization methods. The authors aim to develop a more holistic normalization technique by forcing parameters and representations onto a hypersphere.

Method: Propose anTransformer, which constrains parameter norms and normalizes all representations via scalar multiplications based on the tight concentration of norms of high-dimensional random vectors.

Result: anTransformer shows 40% faster convergence compared to QK normalization during GPT training with less than 3% additional runtime. It also enables training with larger batch sizes and fewer hyperparameters while maintaining the favorable scaling characteristics of classic GPT architectures.

Conclusion: anTransformer offers a promising approach for normalizing deep learning models, providing faster convergence, simpler hyperparameter tuning, and support for larger batch sizes.

Abstract: In deep learning, regularization and normalization are common solutions for
challenges such as overfitting, numerical instabilities, and the increasing
variance in the residual stream. An alternative approach is to force all
parameters and representations to lie on a hypersphere. This removes the need
for regularization and increases convergence speed, but comes with additional
costs. In this work, we propose a more holistic but approximate normalization
(anTransformer). Our approach constrains the norm of parameters and normalizes
all representations via scalar multiplications motivated by the tight
concentration of the norms of high-dimensional random vectors. When applied to
GPT training, we observe a 40% faster convergence compared to models with QK
normalization, with less than 3% additional runtime. Deriving scaling laws for
anGPT, we found our method enables training with larger batch sizes and fewer
hyperparameters, while matching the favorable scaling characteristics of
classic GPT architectures.

</details>


### [95] [Weakly-Supervised Contrastive Learning for Imprecise Class Labels](https://arxiv.org/abs/2505.22028)
*Zi-Hao Zhou,Jun-Jie Wang,Tong Wei,Min-Ling Zhang*

Main category: cs.LG

TL;DR: The paper introduces continuous semantic similarity to redefine positive and negative pairs in contrastive learning, proposing a graph-theoretic framework for weakly-supervised contrastive learning that can be applied to noisy and partial label scenarios.


<details>
  <summary>Details</summary>
Motivation: Supervised contrastive learning faces challenges due to ambiguous or inaccurate data annotations which limit its applicability.

Method: The concept of 'continuous semantic similarity' is introduced to measure the semantic similarity between example pairs. A graph-theoretic framework is proposed where semantic similarity serves as the graph weights for weakly-supervised contrastive learning.

Result: The framework shows effectiveness in noisy label and partial label learning settings, with theoretical error bound established demonstrating its approximation to supervised contrastive learning under mild conditions.

Conclusion: A novel approach to address limitations of supervised contrastive learning is provided, expanding applicability to weakly-supervised scenarios.

Abstract: Contrastive learning has achieved remarkable success in learning effective
representations, with supervised contrastive learning often outperforming
self-supervised approaches. However, in real-world scenarios, data annotations
are often ambiguous or inaccurate, meaning that class labels may not reliably
indicate whether two examples belong to the same class. This limitation
restricts the applicability of supervised contrastive learning. To address this
challenge, we introduce the concept of ``continuous semantic similarity'' to
define positive and negative pairs. Instead of directly relying on imprecise
class labels, we measure the semantic similarity between example pairs, which
quantifies how closely they belong to the same category by iteratively refining
weak supervisory signals. Based on this concept, we propose a graph-theoretic
framework for weakly-supervised contrastive learning, where semantic similarity
serves as the graph weights. Our framework is highly versatile and can be
applied to many weakly-supervised learning scenarios. We demonstrate its
effectiveness through experiments in two common settings, i.e., noisy label and
partial label learning, where existing methods can be easily integrated to
significantly improve performance. Theoretically, we establish an error bound
for our approach, showing that it can approximate supervised contrastive
learning under mild conditions. The implementation code is available at
https://github.com/Speechless-10308/WSC.

</details>


### [96] [Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation](https://arxiv.org/abs/2505.22041)
*Michael Grohs,Adrian Rebmann,Jana-Rebecca Rehse*

Main category: cs.LG

TL;DR: An approach using Retrieval Augmented Generation (RAG) is proposed to detect undesired process behavior without needing a dedicated process model or resource-intensive fine-tuning. Evaluation shows that it outperforms fine-tuned LLMs.


<details>
  <summary>Details</summary>
Motivation: Conformance checking techniques need process models which might not be available, and existing approaches using fine-tuned LLMs are resource-intensive and lack generalization.

Method: Propose an approach using RAG to provide LLMs with access to a knowledge base containing desired and undesired process behavior from other processes.

Result: The approach outperforms fine-tuned LLMs in detecting undesired behavior when enriched with relevant context from the event log.

Conclusion: RAG is a viable alternative to fine-tuning for detecting undesired process behavior.

Abstract: Conformance checking techniques detect undesired process behavior by
comparing process executions that are recorded in event logs to desired
behavior that is captured in a dedicated process model. If such models are not
available, conformance checking techniques are not applicable, but
organizations might still be interested in detecting undesired behavior in
their processes. To enable this, existing approaches use Large Language Models
(LLMs), assuming that they can learn to distinguish desired from undesired
behavior through fine-tuning. However, fine-tuning is highly resource-intensive
and the fine-tuned LLMs often do not generalize well. To address these
limitations, we propose an approach that requires neither a dedicated process
model nor resource-intensive fine-tuning to detect undesired process behavior.
Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with
direct access to a knowledge base that contains both desired and undesired
process behavior from other processes, assuming that the LLM can transfer this
knowledge to the process at hand. Our evaluation shows that our approach
outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that
RAG is a viable alternative to resource-intensive fine-tuning, particularly
when enriched with relevant context from the event log, such as frequent traces
and activities.

</details>


### [97] [Estimating the Effects of Sample Training Orders for Large Language Models without Retraining](https://arxiv.org/abs/2505.22042)
*Hao Yang,Haoxuan Li,Mengyue Yang,Xu Chen,Mingming Gong*

Main category: cs.LG

TL;DR: 研究提出了一种无需重新训练的框架，用于高效估计任意训练样本顺序下的大语言模型参数。此框架可应用于优化LLM训练课程设计和分析其记忆与泛化效果。


<details>
  <summary>Details</summary>
Motivation: 探讨训练样本顺序对大语言模型外部性能和内部学习动态的影响，并解决传统方法因需要多次重新训练而计算不可行的问题。

Method: 设计一种无需重新训练的框架，通过一阶和二阶泰勒展开近似Adam优化器更新，并利用随机投影方法存储中间检查点，从而有效估计任意训练样本顺序下的模型参数。将其应用于两个下游问题：（1）基于估计模型性能改进课程学习策略；（2）分析训练样本位置对LLM记忆和泛化能力的影响。

Result: 大量实验验证了该框架在重现真实模型性能方面的有效性，并展示了其在优化LLM训练课程和分析记忆与泛化效果方面的潜力。

Conclusion: 提出的无需重新训练的框架为研究LLM训练样本顺序影响提供了一种高效工具，有助于优化训练课程设计和深入理解模型的记忆与泛化能力。

Abstract: The order of training samples plays a crucial role in large language models
(LLMs), significantly impacting both their external performance and internal
learning dynamics. Traditional methods for investigating this effect generally
require retraining the model with various sample orders, which is
computationally infeasible for LLMs. In this work, we improve traditional
methods by designing a retraining-free framework. By approximating Adam
optimizer updates with first- and second-order Taylor expansions and utilizing
random projection methods to store intermediate checkpoints, our framework can
efficiently estimate model parameters for arbitrary training sample orders.
Next, we apply our framework to two downstream research problems: (1) Training
curriculum design for LLMs -- we base our retraining-free framework to propose
a novel curriculum learning strategy that augments curriculum proposals with
estimated model performances, enabling more informed sample scheduling. (2)
LLMs' memorization and generalization effect analysis -- we use our
retraining-free framework to estimate how the positions of training samples
influence LLMs' capacity for memorization and generalization. We conduct
extensive experiments to validate the effectiveness of our retraining-free
framework in reproducing the true model performances, and further demonstrate
its potential in optimizing LLM training curricula and analyzing the
memorization and generalization effects of LLMs.

</details>


### [98] [Differentiable Generalized Sliced Wasserstein Plans](https://arxiv.org/abs/2505.22049)
*Laetitia Chapel,Romain Tavenard,Samuel Vaiter*

Main category: cs.LG

TL;DR: The paper addresses the computational bottlenecks of Optimal Transport (OT) by reformulating min-SWGG as a bilevel optimization problem and proposing a differentiable approximation scheme to efficiently find the optimal slice, even in high-dimensional settings. It also generalizes the method for data on manifolds and demonstrates practical applications in gradient flows and image generation.


<details>
  <summary>Details</summary>
Motivation: Optimal Transport (OT) is powerful but computationally expensive, especially in high dimensions. Current slicing methods like min-SWGG have limitations such as exponential growth in required slices with increasing dimensionality and being constrained to linear projections.

Method: The authors reformulate min-SWGG as a bilevel optimization problem and introduce a differentiable approximation scheme to identify the optimal slice efficiently. They also extend the method to handle data living on manifolds.

Result: The approach shows practical value in various applications including gradient flows on manifolds and high-dimensional spaces, and a novel sliced OT-based conditional flow matching for image generation.

Conclusion: This work provides an efficient solution to overcome typical limitations of slicing methods in Optimal Transport, enabling its application to higher dimensional datasets and complex data structures.

Abstract: Optimal Transport (OT) has attracted significant interest in the machine
learning community, not only for its ability to define meaningful distances
between probability distributions -- such as the Wasserstein distance -- but
also for its formulation of OT plans. Its computational complexity remains a
bottleneck, though, and slicing techniques have been developed to scale OT to
large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a
single one-dimensional plan back to the original multidimensional space,
finally selecting the slice that yields the lowest Wasserstein distance as an
approximation of the full OT plan. Despite its computational and theoretical
advantages, min-SWGG inherits typical limitations of slicing methods: (i) the
number of required slices grows exponentially with the data dimension, and (ii)
it is constrained to linear projections. Here, we reformulate min-SWGG as a
bilevel optimization problem and propose a differentiable approximation scheme
to efficiently identify the optimal slice, even in high-dimensional settings.
We furthermore define its generalized extension for accommodating to data
living on manifolds. Finally, we demonstrate the practical value of our
approach in various applications, including gradient flows on manifolds and
high-dimensional spaces, as well as a novel sliced OT-based conditional flow
matching for image generation -- where fast computation of transport plans is
essential.

</details>


### [99] [The Resurrection of the ReLU](https://arxiv.org/abs/2505.22074)
*Coşku Can Horuz,Geoffrey Kasenbacher,Saya Higuchi,Sebastian Kairat,Jendrik Stoltz,Moritz Pesl,Bernhard A. Moser,Christoph Linse,Thomas Martinetz,Sebastian Otte*

Main category: cs.LG

TL;DR: The paper introduces SUGAR, a novel regularizer for ReLU activations that replaces the derivative in the backward pass with a smooth surrogate gradient. This approach enhances generalization performance in various deep learning architectures, including VGG-16, ResNet-18, Conv2NeXt, and Swin Transformer. It revives dead ReLUs and offers sparser activations, challenging the necessity of advanced activation functions.


<details>
  <summary>Details</summary>
Motivation: Despite the popularity of advanced activation functions like GELU, SELU, and SiLU, classical ReLU remains appealing due to its simplicity and advantageous characteristics. However, it suffers from the 'dying ReLU' problem, which limits its effectiveness. The authors aim to address this issue by enhancing the conventional ReLU's gradient handling without losing its benefits.

Method: SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate gradient. This avoids zeroing out gradients and effectively resurrects dead ReLUs.

Result: SUGAR substantially improves generalization performance over convolutional network architectures such as VGG-16 and ResNet-18. It also provides sparser activations while reviving dead ReLUs. Even in modern architectures like Conv2NeXt and Swin Transformer, substituting GELU with SUGAR yields competitive or slightly superior performance.

Conclusion: The findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, with appropriate gradient handling, can serve as a strong, versatile component across a broad range of deep learning vision models.

Abstract: Modeling sophisticated activation functions within deep learning
architectures has evolved into a distinct research direction. Functions such as
GELU, SELU, and SiLU offer smooth gradients and improved convergence
properties, making them popular choices in state-of-the-art models. Despite
this trend, the classical ReLU remains appealing due to its simplicity,
inherent sparsity, and other advantageous topological characteristics. However,
ReLU units are prone to becoming irreversibly inactive - a phenomenon known as
the dying ReLU problem - which limits their overall effectiveness. In this
work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel,
plug-and-play regularizer for deep architectures. SUGAR preserves the standard
ReLU function during the forward pass but replaces its derivative in the
backward pass with a smooth surrogate that avoids zeroing out gradients. We
demonstrate that SUGAR, when paired with a well-chosen surrogate function,
substantially enhances generalization performance over convolutional network
architectures such as VGG-16 and ResNet-18, providing sparser activations while
effectively resurrecting dead ReLUs. Moreover, we show that even in modern
architectures like Conv2NeXt and Swin Transformer - which typically employ GELU
- substituting these with SUGAR yields competitive and even slightly superior
performance. These findings challenge the prevailing notion that advanced
activation functions are necessary for optimal performance. Instead, they
suggest that the conventional ReLU, particularly with appropriate gradient
handling, can serve as a strong, versatile revived classic across a broad range
of deep learning vision models.

</details>


### [100] [Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?](https://arxiv.org/abs/2505.22081)
*Shun Sato,Issei Sato*

Main category: cs.LG

TL;DR: Symbolic regression uses mathematical equations to fit numerical data. Recently, Neural symbolic regression (NSR) methods with Transformers have been developed but suffer from low performance when the number of input variables is large due to memorization bias.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation of current NSR methods that involve Transformers pre-trained on large-scale synthetic datasets, which is their poor performance when dealing with a large number of input variables.

Method: The authors hypothesized that this limitation stems from the memorization bias of Transformers in symbolic regression. They conducted a quantitative evaluation of this bias using a synthetic dataset and performed additional theoretical analysis. They also examined if tailoring test-time strategies can lead to reduced memorization bias and better performance.

Result: The authors found that Transformers rarely generate expressions not present in the training data and that this bias arises from the Transformer's inability to construct expressions compositionally while verifying their numerical validity. They demonstrated that providing additional information to the model at test time can significantly mitigate memorization bias, but reducing memorization bias does not necessarily correlate with improved performance.

Conclusion: These findings contribute to a deeper understanding of the limitations of NSR approaches and offer a foundation for designing more robust, generalizable symbolic regression methods.

Abstract: Symbolic regression aims to discover mathematical equations that fit given
numerical data. It has been applied in various fields of scientific research,
such as producing human-readable expressions that explain physical phenomena.
Recently, Neural symbolic regression (NSR) methods that involve Transformers
pre-trained on large-scale synthetic datasets have gained attention. While
these methods offer advantages such as short inference time, they suffer from
low performance, particularly when the number of input variables is large. In
this study, we hypothesized that this limitation stems from the memorization
bias of Transformers in symbolic regression. We conducted a quantitative
evaluation of this bias in Transformers using a synthetic dataset and found
that Transformers rarely generate expressions not present in the training data.
Additional theoretical analysis reveals that this bias arises from the
Transformer's inability to construct expressions compositionally while
verifying their numerical validity. We finally examined if tailoring test-time
strategies can lead to reduced memorization bias and better performance. We
empirically demonstrate that providing additional information to the model at
test time can significantly mitigate memorization bias. On the other hand, we
also find that reducing memorization bias does not necessarily correlate with
improved performance. These findings contribute to a deeper understanding of
the limitations of NSR approaches and offer a foundation for designing more
robust, generalizable symbolic regression methods. Code is available at
https://github.com/Shun-0922/Mem-Bias-NSR .

</details>


### [101] [The quest for the GRAph Level autoEncoder (GRALE)](https://arxiv.org/abs/2505.22109)
*Paul Krzakala,Gabriel Melo,Charlotte Laclau,Florence d'Alché-Buc,Rémi Flamary*

Main category: cs.LG

TL;DR: GRALE is a novel graph autoencoder that leverages an Optimal Transport-inspired loss and a differentiable node matching module, based on the attention-based architecture of Evoformer. It can encode and decode graphs of varying sizes into a shared embedding space, enabling a highly general form of pre-training for downstream tasks.


<details>
  <summary>Details</summary>
Motivation: Graph representation learning remains a challenging task with significant impacts on fields like chemistry and biology.

Method: GRALE uses an Optimal Transport-inspired loss to compare original and reconstructed graphs and incorporates a differentiable node matching module trained jointly with the encoder and decoder. The method extends Evoformer to support both graph encoding and decoding.

Result: Numerical experiments on simulated and molecular data show that GRALE enables a highly general form of pre-training applicable to various downstream tasks including classification, regression, and complex tasks such as graph interpolation, editing, matching, and prediction.

Conclusion: GRALE offers a powerful tool for graph representation learning, impacting applications in chemistry, biology, and other fields.

Abstract: Although graph-based learning has attracted a lot of attention, graph
representation learning is still a challenging task whose resolution may impact
key application fields such as chemistry or biology. To this end, we introduce
GRALE, a novel graph autoencoder that encodes and decodes graphs of varying
sizes into a shared embedding space. GRALE is trained using an Optimal
Transport-inspired loss that compares the original and reconstructed graphs and
leverages a differentiable node matching module, which is trained jointly with
the encoder and decoder. The proposed attention-based architecture relies on
Evoformer, the core component of AlphaFold, which we extend to support both
graph encoding and decoding. We show, in numerical experiments on simulated and
molecular data, that GRALE enables a highly general form of pre-training,
applicable to a wide range of downstream tasks, from classification and
regression to more complex tasks such as graph interpolation, editing,
matching, and prediction.

</details>


### [102] [BiMi Sheets: Infosheets for bias mitigation methods](https://arxiv.org/abs/2505.22114)
*MaryBeth Defrance,Guillaume Bied,Maarten Buyl,Jefrey Lijffijt,Tijl De Bie*

Main category: cs.LG

TL;DR: An abstract discussing the challenges of bias mitigation methods in machine learning and proposing BiMi Sheets as a solution for documenting and comparing these methods.


<details>
  <summary>Details</summary>
Motivation: Hundreds of bias mitigation methods have been proposed but face challenges due to being domain-, task-, and model-specific, leading to the 'portability trap'. This makes it difficult to benchmark and compare different methods.

Method: Propose BiMi Sheets as a portable, uniform guide to document the design choices of any bias mitigation method. The sheets' structure allows for a structured database of these methods and a platform at bimisheet.com is provided for finding and creating BiMi Sheets.

Result: BiMi Sheets enable researchers and practitioners to quickly learn the main characteristics of a bias mitigation method and compare with their desiderata, fostering better understanding and adoption.

Conclusion: BiMi Sheets provide a solution to the challenges faced in comparing and adopting bias mitigation methods by offering a structured way to document them.

Abstract: Over the past 15 years, hundreds of bias mitigation methods have been
proposed in the pursuit of fairness in machine learning (ML). However,
algorithmic biases are domain-, task-, and model-specific, leading to a
`portability trap': bias mitigation solutions in one context may not be
appropriate in another. Thus, a myriad of design choices have to be made when
creating a bias mitigation method, such as the formalization of fairness it
pursues, and where and how it intervenes in the ML pipeline. This creates
challenges in benchmarking and comparing the relative merits of different bias
mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design
choices of any bias mitigation method. This enables researchers and
practitioners to quickly learn its main characteristics and to compare with
their desiderata. Furthermore, the sheets' structure allow for the creation of
a structured database of bias mitigation methods. In order to foster the
sheets' adoption, we provide a platform for finding and creating BiMi Sheets at
bimisheet.com.

</details>


### [103] [Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL](https://arxiv.org/abs/2505.22151)
*Claude Formanek,Omayma Mahjoub,Louay Ben Nessir,Sasha Abramowitz,Ruan de Kock,Wiem Khlifi,Simon Du Toit,Felix Chalumeau,Daniel Rajaonarivonivelomanantsoa,Arnol Fokam,Siddarth Singh,Ulrich Mbou Sob,Arnu Pretorius*

Main category: cs.LG

TL;DR: The paper proposes Oryx, a novel algorithm for offline cooperative MARL that adapts the Sable architecture and combines it with a sequential form of ICQ to solve complex coordination challenges. It achieves state-of-the-art performance on most tested datasets and demonstrates robust generalisation across domains.


<details>
  <summary>Details</summary>
Motivation: A key challenge in offline multi-agent reinforcement learning is achieving effective many-agent multi-step coordination in complex environments.

Method: Oryx adapts the retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ) to develop a novel offline auto-regressive policy update scheme.

Result: Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons.

Conclusion: The authors introduce new datasets to push the limits of many-agent coordination in offline MARL and demonstrate Oryx's superior ability to scale effectively in such settings. They will make all of their datasets, experimental data, and code available upon publication.

Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is
achieving effective many-agent multi-step coordination in complex environments.
In this work, we propose Oryx, a novel algorithm for offline cooperative MARL
to directly address this challenge. Oryx adapts the recently proposed
retention-based architecture Sable and combines it with a sequential form of
implicit constraint Q-learning (ICQ), to develop a novel offline
auto-regressive policy update scheme. This allows Oryx to solve complex
coordination challenges while maintaining temporal coherence over lengthy
trajectories. We evaluate Oryx across a diverse set of benchmarks from prior
works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and
continuous control, varying in scale and difficulty. Oryx achieves
state-of-the-art performance on more than 80% of the 65 tested datasets,
outperforming prior offline MARL methods and demonstrating robust
generalisation across domains with many agents and long horizons. Finally, we
introduce new datasets to push the limits of many-agent coordination in offline
MARL, and demonstrate Oryx's superior ability to scale effectively in such
settings. We will make all of our datasets, experimental data, and code
available upon publication.

</details>


### [104] [Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory](https://arxiv.org/abs/2505.22152)
*Dominik Fuchsgruber,Tom Wollschläger,Johannes Bordne,Stephan Günnemann*

Main category: cs.LG

TL;DR: The paper explores uncertainty estimation for graphs, especially in heterophilic settings, and proposes a new method that provides state-of-the-art uncertainty on heterophilic graphs.


<details>
  <summary>Details</summary>
Motivation: Most methods for uncertainty estimation on graphs rely on homophily and perform poorly in heterophilic settings. The authors aim to address this issue by analyzing message passing neural networks from an information-theoretic perspective.

Method: The authors develop an analog to the data processing inequality to quantify information throughout the layers of message passing neural networks (MPNNs). They consider the latent embeddings of MPNNs which provide different information about the data distribution in heterophilic graphs.

Result: Empirical results show that considering all node representations simultaneously is crucial for epistemic uncertainty estimation on graphs beyond homophily. A simple post-hoc density estimator on the joint node embedding space achieves state-of-the-art uncertainty on heterophilic graphs while matching prior work on homophilic graphs without explicit exploitation of homophily.

Conclusion: The study highlights the importance of considering all node representations simultaneously for effective uncertainty estimation on both heterophilic and homophilic graphs.

Abstract: While uncertainty estimation for graphs recently gained traction, most
methods rely on homophily and deteriorate in heterophilic settings. We address
this by analyzing message passing neural networks from an information-theoretic
perspective and developing a suitable analog to data processing inequality to
quantify information throughout the model's layers. In contrast to non-graph
domains, information about the node-level prediction target can increase with
model depth if a node's features are semantically different from its neighbors.
Therefore, on heterophilic graphs, the latent embeddings of an MPNN each
provide different information about the data distribution - different from
homophilic settings. This reveals that considering all node representations
simultaneously is a key design principle for epistemic uncertainty estimation
on graphs beyond homophily. We empirically confirm this with a simple post-hoc
density estimator on the joint node embedding space that provides
state-of-the-art uncertainty on heterophilic graphs. At the same time, it
matches prior work on homophilic graphs without explicitly exploiting homophily
through post-processing.

</details>


### [105] [The informativeness of the gradient revisited](https://arxiv.org/abs/2505.22158)
*Rustem Takhanov*

Main category: cs.LG

TL;DR: Gradient-based deep learning has limitations, especially when the information contained in the gradient is minimal. This paper provides a general bound on the variance of the gradient informativeness in terms of pairwise independence and collision entropy. The bound is demonstrated on Learning with Errors (LWE) mappings and high-frequency functions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the theoretical understanding of the limitations of gradient-based deep learning methods. Specifically, it focuses on scenarios where the information contained in the gradient is minimal, requiring a large number of iterations for success.

Method: The authors derive a general bound on the variance of the gradient informativeness using parameters related to pairwise independence of the target function class and the collision entropy of the input distribution. The bound scales as $ \tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $ \varepsilon $ measures pairwise independence and $ \mathcal{E}_c $ is the collision entropy.

Result: The derived bound is applied to Learning with Errors (LWE) mappings and high-frequency functions, demonstrating its practical utility. Experiments are also conducted to understand recent deep learning-based attacks on LWE.

Conclusion: This study contributes to the theoretical understanding of the limitations of gradient-based methods by providing a framework to measure gradient informativeness through variance bounds. It highlights the significance of pairwise independence and collision entropy.

Abstract: In the past decade gradient-based deep learning has revolutionized several
applications. However, this rapid advancement has highlighted the need for a
deeper theoretical understanding of its limitations. Research has shown that,
in many practical learning tasks, the information contained in the gradient is
so minimal that gradient-based methods require an exceedingly large number of
iterations to achieve success. The informativeness of the gradient is typically
measured by its variance with respect to the random selection of a target
function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a
parameter related to the pairwise independence of the target function class and
the collision entropy of the input distribution. Our bound scales as $
\tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $
\tilde{\mathcal{O}} $ hides factors related to the regularity of the learning
model and the loss function, $ \varepsilon $ measures the pairwise independence
of the target function class and $\mathcal{E}_c$ is the collision entropy of
the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class
of Learning with Errors (LWE) mappings and high-frequency functions. In
addition to the theoretical analysis, we present experiments to understand
better the nature of recent deep learning-based attacks on LWE.

</details>


### [106] [An Augmentation-Aware Theory for Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.22196)
*Jingyi Cui,Hongwei Wen,Yisen Wang*

Main category: cs.LG

TL;DR: 提出了一种新的增强感知误差界，揭示了数据增强在自监督对比学习中的作用，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督对比学习在实践中取得了成功，但现有理论研究中对数据增强的作用，特别是特定增强类型的影响仍研究不足。

Method: 首次提出了一个考虑数据增强影响的误差界，并在新的语义标签假设下探讨了特定增强方法如何影响该误差界。

Result: 理论分析和像素及表示级别的实验都验证了所提出的误差界的合理性。

Conclusion: 数据增强不仅由无监督风险决定，还受到数据增强引起的一种权衡的影响。

Abstract: Self-supervised contrastive learning has emerged as a powerful tool in
machine learning and computer vision to learn meaningful representations from
unlabeled data. Meanwhile, its empirical success has encouraged many
theoretical studies to reveal the learning mechanisms. However, in the existing
theoretical research, the role of data augmentation is still under-exploited,
especially the effects of specific augmentation types. To fill in the blank, we
for the first time propose an augmentation-aware error bound for
self-supervised contrastive learning, showing that the supervised risk is
bounded not only by the unsupervised risk, but also explicitly by a trade-off
induced by data augmentation. Then, under a novel semantic label assumption, we
discuss how certain augmentation methods affect the error bound. Lastly, we
conduct both pixel- and representation-level experiments to verify our proposed
theoretical results.

</details>


### [107] [Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer](https://arxiv.org/abs/2505.22199)
*Xinyue Hu,Zhibin Duan,Bo Chen,Mingyuan Zhou*

Main category: cs.LG

TL;DR: This paper introduces a Bayesian Non-negative Decision Layer (BNDL) for deep neural networks, providing robust uncertainty estimation and improved interpretability through disentangled representations.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks have powerful expressiveness but lack in uncertainty estimation and interpretability due to entangled features influencing decisions.

Method: Reformulate deep neural networks as conditional Bayesian non-negative factor analysis using BNDL, incorporating stochastic latent variables for complex dependency modeling and utilizing Weibull variational inference network for posterior approximation.

Result: Experimental results show that BNDL improves model accuracy, provides reliable uncertainty estimation, and enhances interpretability through effective disentangled learning.

Conclusion: BNDL addresses the challenges of uncertainty estimation and interpretability in deep neural networks by promoting disentangled representations.

Abstract: Although deep neural networks have demonstrated significant success due to
their powerful expressiveness, most models struggle to meet practical
requirements for uncertainty estimation. Concurrently, the entangled nature of
deep neural networks leads to a multifaceted problem, where various localized
explanation techniques reveal that multiple unrelated features influence the
decisions, thereby undermining interpretability. To address these challenges,
we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates
deep neural networks as a conditional Bayesian non-negative factor analysis. By
leveraging stochastic latent variables, the BNDL can model complex dependencies
and provide robust uncertainty estimation. Moreover, the sparsity and
non-negativity of the latent variables encourage the model to learn
disentangled representations and decision layers, thereby improving
interpretability. We also offer theoretical guarantees that BNDL can achieve
effective disentangled learning. In addition, we developed a corresponding
variational inference method utilizing a Weibull variational inference network
to approximate the posterior distribution of the latent variables. Our
experimental results demonstrate that with enhanced disentanglement
capabilities, BNDL not only improves the model's accuracy but also provides
reliable uncertainty estimation and improved interpretability.

</details>


### [108] [Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning](https://arxiv.org/abs/2505.22203)
*Yuzhen Huang,Weihao Zeng,Xingshan Zeng,Qi Zhu,Junxian He*

Main category: cs.LG

TL;DR: In this paper, researchers analyze rule-based and model-based verifiers in reinforcement learning with verifiable reward (RLVR) using mathematical reasoning as a case study. They discover significant limitations in both types of verifiers: rule-based verifiers suffer from false negatives due to inability to recognize equivalent answers in different formats, while model-based verifiers are vulnerable to hacking leading to false positives. Both issues negatively affect RL training performance. The findings highlight the need for more robust reward systems.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the reliability of verifiers used in reinforcement learning with verifiable reward (RLVR), specifically focusing on their impact during the RL training process in complex domains such as mathematical reasoning.

Method: The researchers conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. They first evaluate open-source rule-based verifiers across multiple mathematical datasets and then investigate model-based verifiers as an alternative solution, analyzing their performance and vulnerabilities.

Result: Rule-based verifiers often fail to recognize equivalent answers presented in different formats, causing non-negligible false negative rates that adversely affect RL training performance. Model-based verifiers achieve higher verification accuracy in static evaluations but are highly susceptible to hacking, leading to false positives and artificially inflated rewards during policy optimization.

Conclusion: Both rule-based and model-based verifiers have unique risks and limitations. The paper concludes by emphasizing the importance of developing more robust reward systems in reinforcement learning.

Abstract: Trustworthy verifiers are essential for the success of reinforcement learning
with verifiable reward (RLVR), which is the core methodology behind various
large reasoning models such as DeepSeek-R1. In complex domains like
mathematical reasoning, rule-based verifiers have been widely adopted in
previous works to train strong reasoning models. However, the reliability of
these verifiers and their impact on the RL training process remain poorly
understood. In this work, we take mathematical reasoning as a case study and
conduct a comprehensive analysis of various verifiers in both static evaluation
and RL training scenarios. First, we find that current open-source rule-based
verifiers often fail to recognize equivalent answers presented in different
formats across multiple commonly used mathematical datasets, resulting in
non-negligible false negative rates. This limitation adversely affects RL
training performance and becomes more pronounced as the policy model gets
stronger. Subsequently, we investigate model-based verifiers as a potential
solution to address these limitations. While the static evaluation shows that
model-based verifiers achieve significantly higher verification accuracy,
further analysis and RL training results imply that they are highly susceptible
to hacking, where they misclassify certain patterns in responses as correct
(i.e., false positives). This vulnerability is exploited during policy model
optimization, leading to artificially inflated rewards. Our findings underscore
the unique risks inherent to both rule-based and model-based verifiers, aiming
to offer valuable insights to develop more robust reward systems in
reinforcement learning.

</details>


### [109] [LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models](https://arxiv.org/abs/2505.22208)
*Yosuke Oyama,Yusuke Majima,Eiji Ohta,Yasufumi Sakai*

Main category: cs.LG

TL;DR: The paper presents LaMM, a semi-supervised pre-training method for neural network potentials (NNPs) that incorporates denoising self-supervised learning and a load-balancing algorithm to efficiently use large datasets (~300 million samples), enhancing the speed and accuracy of fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Neural network potentials (NNPs) are important for computational materials science but their improvement through pre-training and fine-tuning is computationally expensive due to DFT-based dataset labeling and load imbalances during large-scale pre-training.

Method: LaMM is proposed which includes improved denoising self-supervised learning and a load-balancing algorithm for efficient multi-node training. It leverages a large-scale dataset of ~300 million semi-labeled samples.

Result: LaMM effectively trains a single NNP model using the large-scale dataset, leading to better performance in terms of speed and accuracy when fine-tuning.

Conclusion: LaMM offers an effective way to enhance the efficiency and accuracy of NNP models through semi-supervised pre-training.

Abstract: Neural network potentials (NNPs) are crucial for accelerating computational
materials science by surrogating density functional theory (DFT) calculations.
Improving their accuracy is possible through pre-training and fine-tuning,
where an NNP model is first pre-trained on a large-scale dataset and then
fine-tuned on a smaller target dataset. However, this approach is
computationally expensive, mainly due to the cost of DFT-based dataset labeling
and load imbalances during large-scale pre-training. To address this, we
propose LaMM, a semi-supervised pre-training method incorporating improved
denoising self-supervised learning and a load-balancing algorithm for efficient
multi-node training. We demonstrate that our approach effectively leverages a
large-scale dataset of $\sim$300 million semi-labeled samples to train a single
NNP model, resulting in improved fine-tuning performance in terms of both speed
and accuracy.

</details>


### [110] [Solver-Free Decision-Focused Learning for Linear Optimization Problems](https://arxiv.org/abs/2505.22224)
*Senne Berden,Ali İrfan Mahmutoğulları,Dimos Tsouros,Tias Guns*

Main category: cs.LG

TL;DR: In predict-then-optimize problems, decision-focused learning (DFL) trains models to produce predictions that maximize downstream decision quality. However, DFL is computationally expensive due to repeated optimization problem solving. This paper proposes a solver-free training method for linear optimization problems based on geometric structure, which reduces computational cost while maintaining high decision quality.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the computational bottleneck in decision-focused learning (DFL) for linear optimization problems, where the optimization problem with predicted parameters needs to be solved repeatedly during training.

Method: The proposed method is a solver-free training approach that exploits the geometric structure of linear optimization. It compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices on the feasible polytope, using this comparison as the loss function instead of solving the optimization problem directly.

Result: Experiments show that the proposed method significantly reduces computational cost while maintaining high decision quality in predict-then-optimize problems.

Conclusion: The paper concludes that the proposed solver-free training method efficiently addresses the computational bottleneck in decision-focused learning for linear optimization problems without sacrificing much in terms of solution quality.

Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide
range of applications. However, in many real-world scenarios, the parameters of
the optimization problem are not known a priori and must be predicted from
contextual features. This gives rise to predict-then-optimize problems, where a
machine learning model predicts problem parameters that are then used to make
decisions via optimization. A growing body of work on decision-focused learning
(DFL) addresses this setting by training models specifically to produce
predictions that maximize downstream decision quality, rather than accuracy.
While effective, DFL is computationally expensive, because it requires solving
the optimization problem with the predicted parameters at each loss evaluation.
In this work, we address this computational bottleneck for linear optimization
problems, a common class of problems in both DFL literature and real-world
applications. We propose a solver-free training method that exploits the
geometric structure of linear optimization to enable efficient training with
minimal degradation in solution quality. Our method is based on the insight
that a solution is optimal if and only if it achieves an objective value that
is at least as good as that of its adjacent vertices on the feasible polytope.
Building on this, our method compares the estimated quality of the ground-truth
optimal solution with that of its precomputed adjacent vertices, and uses this
as loss function. Experiments demonstrate that our method significantly reduces
computational cost while maintaining high decision quality.

</details>


### [111] [Optimal kernel regression bounds under energy-bounded noise](https://arxiv.org/abs/2505.22235)
*Amon Lahr,Johannes Köhler,Anna Scampicchio,Melanie N. Zeilinger*

Main category: cs.LG

TL;DR: 这篇论文推导了一个用于核估计的紧密、非渐近不确定性界限，能够处理相关噪声序列，并基于高斯过程的后验均值和协方差给出函数值。通过与文献中其他结果的严格分析和比较，证明了该方法在返回紧密且易于计算的核估计边界方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 评估估计算法的准确性以及在下游任务中的应用（如在安全关键环境中部署）需要非保守的不确定性界限。现有的方法可能无法提供足够紧密的界限或不能处理相关噪声序列。

Method: 通过引入一个温和的范数有界假设来计算未知函数和噪声，返回假设类中最坏情况的函数实现。利用高斯过程的后验均值和协方差，在测量噪声协方差的最佳选择下给出函数值。

Result: 该方法能够返回紧密且易于计算的核估计边界，并通过与现有文献结果的比较验证了其有效性。

Conclusion: 提出的不确定性界限方法为核估计提供了紧密、非渐近的结果，可以有效处理相关噪声序列，具有理论和实际应用价值。

Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation
algorithm's accuracy and in view of downstream tasks, such as its deployment in
safety-critical contexts. In this paper, we derive a tight, non-asymptotic
uncertainty bound for kernel-based estimation, which can also handle correlated
noise sequences. Its computation relies on a mild norm-boundedness assumption
on the unknown function and the noise, returning the worst-case function
realization within the hypothesis class at an arbitrary query input location.
The value of this function is shown to be given in terms of the posterior mean
and covariance of a Gaussian process for an optimal choice of the measurement
noise covariance. By rigorously analyzing the proposed approach and comparing
it with other results in the literature, we show its effectiveness in returning
tight and easy-to-compute bounds for kernel-based estimates.

</details>


### [112] [B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data](https://arxiv.org/abs/2505.22252)
*Magdalena Proszewska,Tomasz Danel,Dawid Rymarczyk*

Main category: cs.LG

TL;DR: In this paper, researchers developed a benchmark called B-XAIC from real-world molecular data to evaluate existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain, revealing their limitations and providing a resource for improving model faithfulness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Current evaluation frameworks for Explainable AI (XAI) in cheminformatics and drug discovery often rely on artificial datasets or simplified tasks. They employ data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness.

Method: The researchers introduced B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through comprehensive evaluation using B-XAIC, they assessed existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.

Result: The benchmark revealed limitations of existing XAI methods for GNNs in the molecular domain.

Conclusion: B-XAIC provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.

Abstract: Understanding the reasoning behind deep learning model predictions is crucial
in cheminformatics and drug discovery, where molecular design determines their
properties. However, current evaluation frameworks for Explainable AI (XAI) in
this domain often rely on artificial datasets or simplified tasks, employing
data-derived metrics that fail to capture the complexity of real-world
scenarios and lack a direct link to explanation faithfulness. To address this,
we introduce B-XAIC, a novel benchmark constructed from real-world molecular
data and diverse tasks with known ground-truth rationales for assigned labels.
Through a comprehensive evaluation using B-XAIC, we reveal limitations of
existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain.
This benchmark provides a valuable resource for gaining deeper insights into
the faithfulness of XAI, facilitating the development of more reliable and
interpretable models.

</details>


### [113] [A Unified Online-Offline Framework for Co-Branding Campaign Recommendations](https://arxiv.org/abs/2505.22254)
*Xiangxiang Dai,Xiaowei Sun,Jinhang Zuo,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: Co-branding has become a vital strategy for businesses aiming to expand market reach within recommendation systems. This paper proposes a unified online-offline framework to enable co-branding recommendations, with experiments showing at least 12% improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in identifying effective cross-industry partnerships for co-branding due to resource imbalances, uncertain brand willingness, and ever-changing market conditions.

Method: Construct a bipartite graph linking ``initiating'' and ``target'' brands to quantify co-branding probabilities and assess market benefits. Dynamically update the graph during the online learning phase while balancing exploration and exploitation. In the offline optimization phase, consolidate the interests of multiple sub-brands under the same parent brand to maximize overall returns.

Result: Theoretical analysis establishes a highly nontrivial sublinear regret bound for online learning and enhances the approximation guarantee for the NP-hard offline budget allocation optimization. Experiments on both synthetic and real-world datasets demonstrate practical effectiveness with at least 12\% improvement.

Conclusion: This paper provides the first systematic study of the co-branding problem and proposes a unified online-offline framework that mitigates redundant exploration, enhancing short-term performance while ensuring sustainable strategic growth.

Abstract: Co-branding has become a vital strategy for businesses aiming to expand
market reach within recommendation systems. However, identifying effective
cross-industry partnerships remains challenging due to resource imbalances,
uncertain brand willingness, and ever-changing market conditions. In this
paper, we provide the first systematic study of this problem and propose a
unified online-offline framework to enable co-branding recommendations. Our
approach begins by constructing a bipartite graph linking ``initiating'' and
``target'' brands to quantify co-branding probabilities and assess market
benefits. During the online learning phase, we dynamically update the graph in
response to market feedback, while striking a balance between exploring new
collaborations for long-term gains and exploiting established partnerships for
immediate benefits. To address the high initial co-branding costs, our
framework mitigates redundant exploration, thereby enhancing short-term
performance while ensuring sustainable strategic growth. In the offline
optimization phase, our framework consolidates the interests of multiple
sub-brands under the same parent brand to maximize overall returns, avoid
excessive investment in single sub-brands, and reduce unnecessary costs
associated with over-prioritizing a single sub-brand. We present a theoretical
analysis of our approach, establishing a highly nontrivial sublinear regret
bound for online learning in the complex co-branding problem, and enhancing the
approximation guarantee for the NP-hard offline budget allocation optimization.
Experiments on both synthetic and real-world co-branding datasets demonstrate
the practical effectiveness of our framework, with at least 12\% improvement.

</details>


### [114] [Train Sparse Autoencoders Efficiently by Utilizing Features Correlation](https://arxiv.org/abs/2505.22255)
*Vadim Kurochkin,Yaroslav Aksenov,Daniil Laptev,Daniil Gavrilov,Nikita Balagansky*

Main category: cs.LG

TL;DR: KronSAE is a new architecture that uses Kronecker product decomposition and mAND activation function to reduce memory and computation for training Sparse Autoencoders.


<details>
  <summary>Details</summary>
Motivation: Training Sparse Autoencoders (SAEs) at scale is challenging due to computationally intensive linear operations, especially with large dictionary sizes.

Method: The paper proposes KronSAE, which factorizes the latent representation via Kronecker product decomposition. It also introduces mAND, a differentiable activation function approximating binary AND operation.

Result: KronSAE drastically reduces memory and computational overhead while improving interpretability and performance in the factorized framework.

Conclusion: KronSAE and mAND offer an efficient solution for large-scale SAE training.

Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in
interpreting the hidden states of language models by decomposing them into
interpretable latent directions. However, training SAEs at scale remains
challenging, especially when large dictionary sizes are used. While decoders
can leverage sparse-aware kernels for efficiency, encoders still require
computationally intensive linear operations with large output dimensions. To
address this, we propose KronSAE, a novel architecture that factorizes the
latent representation via Kronecker product decomposition, drastically reducing
memory and computational overhead. Furthermore, we introduce mAND, a
differentiable activation function approximating the binary AND operation,
which improves interpretability and performance in our factorized framework.

</details>


### [115] [Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training](https://arxiv.org/abs/2505.22257)
*Youssef Mroueh,Nicolas Dupuis,Brian Belgodere,Apoorva Nitsure,Mattia Rigotti,Kristjan Greenewald,Jiri Navratil,Jerret Ross,Jesus Rios*

Main category: cs.LG

TL;DR: This paper revisits Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes, adapts GRPO to the off-policy setting, compares the empirical performance of reinforcement learning with verifiable rewards using both GRPO variants, and finds that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart.


<details>
  <summary>Details</summary>
Motivation: The motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial.

Method: The authors adapt GRPO to the off-policy setting, showing that both on-policy and off-policy GRPO objectives yield an improvement in the reward. They motivate the use of clipped surrogate objectives in the off-policy version of GRPO.

Result: The results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart when comparing the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants.

Conclusion: Off-policy GRPO is either superior or equivalent to on-policy GRPO, suggesting that the adaptation of GRPO to the off-policy setting is successful and beneficial.

Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and
off-policy optimization regimes. Our motivation comes from recent work on
off-policy Proximal Policy Optimization (PPO), which improves training
stability, sampling efficiency, and memory usage. In addition, a recent
analysis of GRPO suggests that estimating the advantage function with
off-policy samples could be beneficial. Building on these observations, we
adapt GRPO to the off-policy setting. We show that both on-policy and
off-policy GRPO objectives yield an improvement in the reward. This result
motivates the use of clipped surrogate objectives in the off-policy version of
GRPO. We then compare the empirical performance of reinforcement learning with
verifiable rewards in post-training using both GRPO variants. Our results show
that off-policy GRPO either significantly outperforms or performs on par with
its on-policy counterpart.

</details>


### [116] [Full Domain Analysis in Fluid Dynamics](https://arxiv.org/abs/2505.22275)
*Alexander Hagg,Adam Gaier,Dominik Wilde,Alexander Asteroth,Holger Foysi,Dirk Reith*

Main category: cs.LG

TL;DR: 新型进化优化、仿真和机器学习技术能够广泛分析诸如流体动力学等领域，其中计算成本高且流体行为复杂。全文分析的目标是通过生成大量流体示例及其多样化、优化和分析来加深对领域的理解。最后给出一个例子展示如何使用全文分析。全文分析可以成为理解计算物理学及其他领域复杂系统的重要工具。


<details>
  <summary>Details</summary>
Motivation: 流体动力学等领域的计算成本高且流体行为复杂，需要一种能够高效确定问题域中完整解空间并以可访问和交互方式分析这些解的方法。

Method: 提出全文分析的正式模型，阐述其当前技术水平和子组件需求，并给出应用实例。

Result: 通过生成大量流体示例及其多样化、优化和分析，加深了对领域的理解。

Conclusion: 全文分析作为基于优化和机器学习的技术，可以成为理解计算物理学及其他领域复杂系统的重要工具。

Abstract: Novel techniques in evolutionary optimization, simulation and machine
learning allow for a broad analysis of domains like fluid dynamics, in which
computation is expensive and flow behavior is complex. Under the term of full
domain analysis we understand the ability to efficiently determine the full
space of solutions in a problem domain, and analyze the behavior of those
solutions in an accessible and interactive manner. The goal of full domain
analysis is to deepen our understanding of domains by generating many examples
of flow, their diversification, optimization and analysis. We define a formal
model for full domain analysis, its current state of the art, and requirements
of subcomponents. Finally, an example is given to show what we can learn by
using full domain analysis. Full domain analysis, rooted in optimization and
machine learning, can be a helpful tool in understanding complex systems in
computational physics and beyond.

</details>


### [117] [Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer](https://arxiv.org/abs/2505.22306)
*Zehua Chen,Yuyang Miao,Liyuan Wang,Luyun Fan,Danilo P. Mandic,Jun Zhu*

Main category: cs.LG

TL;DR: UniCardio is a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals, outperforming task-specific baselines in signal denoising, imputation, and translation.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of joint utilization of cardiovascular signals such as PPG, ECG, and BP in real-time monitoring due to noisy wearable recordings and invasive procedures.

Method: Propose UniCardio, a multi-modal diffusion transformer with specialized model architecture and continual learning paradigm to manage signal modalities and incorporate varying modality combinations.

Result: Outperforms recent task-specific baselines in signal denoising, imputation, and translation; generated signals match ground-truth signals in detecting abnormal health conditions and estimating vital signs, ensuring interpretability for human experts.

Conclusion: UniCardio represents a promising avenue for advancing AI-assisted healthcare.

Abstract: Cardiovascular signals such as photoplethysmography (PPG),
electrocardiography (ECG), and blood pressure (BP) are inherently correlated
and complementary, together reflecting the health of cardiovascular system.
However, their joint utilization in real-time monitoring is severely limited by
diverse acquisition challenges from noisy wearable recordings to burdened
invasive procedures. Here we propose UniCardio, a multi-modal diffusion
transformer that reconstructs low-quality signals and synthesizes unrecorded
signals in a unified generative framework. Its key innovations include a
specialized model architecture to manage the signal modalities involved in
generation tasks and a continual learning paradigm to incorporate varying
modality combinations. By exploiting the complementary nature of cardiovascular
signals, UniCardio clearly outperforms recent task-specific baselines in signal
denoising, imputation, and translation. The generated signals match the
performance of ground-truth signals in detecting abnormal health conditions and
estimating vital signs, even in unseen domains, while ensuring interpretability
for human experts. These advantages position UniCardio as a promising avenue
for advancing AI-assisted healthcare.

</details>


### [118] [Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning](https://arxiv.org/abs/2505.22308)
*Zachary Shinnick,Liangze Jiang,Hemanth Saratchandran,Anton van den Hengel,Damien Teney*

Main category: cs.LG

TL;DR: Pretraining on large datasets is important for language models, but synthetic data can also provide benefits. This paper identifies beneficial forms of procedural data and algorithmic reasoning skills that improve in small transformers.


<details>
  <summary>Details</summary>
Motivation: To understand what specific capabilities simple synthetic data instils in a model, where these capabilities reside in the architecture, and how they manifest within its weights.

Method: Identify several beneficial forms of procedural data and conduct extensive ablations and partial-transfer experiments to discover where the inductive structures reside in the model.

Result: Different procedural rules instil distinct but complementary inductive structures in the model, residing in different parts such as attention layers and MLP blocks. Structures induced by multiple rules can be composed to jointly reinforce multiple capabilities.

Conclusion: The results suggest the possibility of disentangling the acquisition of knowledge from reasoning in language models, aiming to improve their robustness and data efficiency.

Abstract: Pretraining on large, semantically rich datasets is key for developing
language models. Surprisingly, recent studies have shown that even synthetic
data, generated procedurally through simple semantic-free algorithms, can yield
some of the same benefits as natural language pretraining. It is unclear what
specific capabilities such simple synthetic data instils in a model, where
these capabilities reside in the architecture, and how they manifest within its
weights. In this short paper, we identify several beneficial forms of
procedural data, together with specific algorithmic reasoning skills that
improve in small transformers. Our core finding is that different procedural
rules instil distinct but complementary inductive structures in the model. With
extensive ablations and partial-transfer experiments, we discover that these
structures reside in different parts of the model. Attention layers often carry
the most transferable information, but some pretraining rules impart useful
structure to MLP blocks instead. Most interestingly, the structures induced by
multiple rules can be composed to jointly reinforce multiple capabilities.
These results suggest an exciting possibility of disentangling the acquisition
of knowledge from reasoning in language models, with the goal of improving
their robustness and data efficiency.

</details>


### [119] [From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization](https://arxiv.org/abs/2505.22310)
*Shoaib Ahmed Siddiqui,Adrian Weller,David Krueger,Gintare Karolina Dziugaite,Michael Curtis Mozer,Eleni Triantafillou*

Main category: cs.LG

TL;DR: Recent unlearning methods for LLMs can be vulnerable to relearning attacks, where knowledge thought to be removed can re-emerge. This study explores this phenomenon in vision classifiers and proposes new methods that offer better resistance to such attacks.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to investigate why current unlearning methods for large language models (LLMs) are susceptible to relearning attacks, where forgotten knowledge can reappear after fine-tuning on a small set of examples.

Method: The authors conduct their study in a controlled setting focusing on example-level unlearning in vision classifiers. They analyze various unlearning methods and observe their behavior when fine-tuned only on the retain set. They also examine weight-space properties such as $L_2$-distance and linear mode connectivity between original and unlearned models to predict resistance to relearning attacks.

Result: The study finds that forget-set accuracy can recover significantly after fine-tuning on just the retain set across different unlearning methods. However, for models retrained from scratch excluding the forget set, the accuracy remains at 50%. The authors identify specific weight-space properties that predict resistance to relearning attacks.

Conclusion: Based on their findings, the authors propose a new class of methods that provide state-of-the-art resistance to relearning attacks, improving upon existing techniques.

Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks:
knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of
(even seemingly-unrelated) examples. We study this phenomenon in a controlled
setting for example-level unlearning in vision classifiers. We make the
surprising discovery that forget-set accuracy can recover from around 50%
post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e.,
zero examples of the forget set. We observe this effect across a wide variety
of unlearning methods, whereas for a model retrained from scratch excluding the
forget set (gold standard), the accuracy remains at 50%. We observe that
resistance to relearning attacks can be predicted by weight-space properties,
specifically, $L_2$-distance and linear mode connectivity between the original
and the unlearned model. Leveraging this insight, we propose a new class of
methods that achieve state-of-the-art resistance to relearning attacks.

</details>


### [120] [Skywork Open Reasoner 1 Technical Report](https://arxiv.org/abs/2505.22312)
*Jujie He,Jiacai Liu,Chris Yuhao Liu,Rui Yan,Chaojie Wang,Peng Cheng,Xiaoyu Zhang,Fuxiang Zhang,Jiacheng Xu,Wei Shen,Siyuan Li,Liang Zeng,Tianwen Wei,Cheng Cheng,Bo An,Yang Liu,Yahui Zhou*

Main category: cs.LG

TL;DR: Skywork-OR1, based on DeepSeek-R1-Distill series, uses reinforcement learning to enhance reasoning capabilities of LLMs. It significantly improves accuracy across benchmarks for 32B and 7B models. Ablation studies validate training pipeline components, and entropy collapse is addressed. Model weights, code, and datasets are open-sourced.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of reinforcement learning in improving reasoning capabilities of large language models, especially for long Chain-of-Thought tasks.

Method: Implemented Skywork-OR1 using the DeepSeek-R1-Distill model series with reinforcement learning techniques. Conducted ablation studies on training pipeline components and investigated entropy collapse phenomenon.

Result: Achieved significant accuracy improvements on AIME24, AIME25, and LiveCodeBench benchmarks for both 32B and 7B models. Skywork-OR1-32B outperformed DeepSeek-R1 and Qwen3-32B on AIME24 and AIME25. Skywork-OR1-7B demonstrated competitive reasoning capabilities.

Conclusion: Reinforcement learning effectively enhances reasoning capabilities of LLMs. Mitigating entropy collapse is crucial for performance improvement. Open-sourcing efforts will support further community research.

Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement
learning (RL) in enhancing the reasoning capabilities of large language models
(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL
implementation for long Chain-of-Thought (CoT) models. Building on the
DeepSeek-R1-Distill model series, our RL approach achieves notable performance
gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench
from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)
for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and
Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable
results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models
demonstrate competitive reasoning capabilities among models of similar size. We
perform comprehensive ablation studies on the core components of our training
pipeline to validate their effectiveness. Additionally, we thoroughly
investigate the phenomenon of entropy collapse, identify key factors affecting
entropy dynamics, and demonstrate that mitigating premature entropy collapse is
critical for improved test performance. To support community research, we fully
open-source our model weights, training code, and training datasets.

</details>


### [121] [Rethinking BPS: A Utility-Based Evaluation Framework](https://arxiv.org/abs/2505.22316)
*Konrad Özdemir,Lukas Kirchdorfer,Keyvan Amiri Elyasi,Han van der Aa,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: The paper proposes a new framework for evaluating Business Process Simulation (BPS) models by assessing their ability to generate representative process behavior, offering a more meaningful way to assess BPS quality.


<details>
  <summary>Details</summary>
Motivation: Current methods for evaluating BPS models have limitations: they treat simulation as forecasting and rely on metrics that can obscure temporal patterns.

Method: The novel framework evaluates simulation quality based on the performance of predictive process monitoring models trained on simulated data compared to those trained on real data.

Result: Empirical results indicate that the framework helps identify discrepancies and distinguishes between model accuracy and data complexity.

Conclusion: This approach provides a more meaningful assessment of BPS model quality.

Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing
organizational workflows, supporting decision-making by estimating the impact
of process changes. The reliability of such estimates depends on the ability of
a BPS model to accurately mimic the process under analysis, making rigorous
accuracy evaluation essential. However, the state-of-the-art approach to
evaluating BPS models has two key limitations. First, it treats simulation as a
forecasting problem, testing whether models can predict unseen future events.
This fails to assess how well a model captures the as-is process, particularly
when process behavior changes from train to test period. Thus, it becomes
difficult to determine whether poor results stem from an inaccurate model or
the inherent complexity of the data, such as unpredictable drift. Second, the
evaluation approach strongly relies on Earth Mover's Distance-based metrics,
which can obscure temporal patterns and thus yield misleading conclusions about
simulation quality. To address these issues, we propose a novel framework that
evaluates simulation quality based on its ability to generate representative
process behavior. Instead of comparing simulated logs to future real-world
executions, we evaluate whether predictive process monitoring models trained on
simulated data perform comparably to those trained on real data for downstream
analysis tasks. Empirical results show that our framework not only helps
identify sources of discrepancies but also distinguishes between model accuracy
and data complexity, offering a more meaningful way to assess BPS quality.

</details>


### [122] [A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective](https://arxiv.org/abs/2505.22322)
*Zhengyu Fang,Zhimeng Jiang,Huiyuan Chen,Xiaoge Zhang,Kaiyu Tang,Xiao Li,Jing Li*

Main category: cs.LG

TL;DR: Diffusion models can generate high-quality tabular data but risk reproducing training samples. This study explores memorization dynamics, identifying a small subset of samples causing disproportionate leakage. Based on this, they propose DynamicCut, a mitigation method that reduces memorization across various datasets and models without significantly impacting data diversity or performance.


<details>
  <summary>Details</summary>
Motivation: To address the privacy risks associated with diffusion models reproducing exact training samples when generating high-quality tabular data. Previous work has focused on dataset-level augmentation, but there is limited understanding of which individual samples contribute most to memorization.

Method: The authors quantify memorization for each real sample using a relative distance ratio and identify a heavy-tailed distribution of memorization counts. They divide samples into top- and non-top-memorized groups and analyze their training-time behaviors. Based on these insights, they propose DynamicCut, a two-stage model-agnostic mitigation method involving ranking samples by epoch-wise intensity, pruning a tunable top fraction, and retraining on the filtered dataset.

Result: DynamicCut effectively reduces memorization across multiple tabular datasets and models while having minimal impact on data diversity and downstream performance. It complements augmentation-based defenses and enables cross-model transferability.

Conclusion: DynamicCut offers an effective approach to mitigating memorization in tabular diffusion models, reducing privacy risks while preserving data quality and performance.

Abstract: Diffusion models have shown strong performance in generating high-quality
tabular data, but they carry privacy risks by reproducing exact training
samples. While prior work focuses on dataset-level augmentation to reduce
memorization, little is known about which individual samples contribute most.
We present the first data-centric study of memorization dynamics in tabular
diffusion models. We quantify memorization for each real sample based on how
many generated samples are flagged as replicas, using a relative distance
ratio. Our empirical analysis reveals a heavy-tailed distribution of
memorization counts: a small subset of samples contributes disproportionately
to leakage, confirmed via sample-removal experiments. To understand this, we
divide real samples into top- and non-top-memorized groups and analyze their
training-time behaviors. We track when each sample is first memorized and
monitor per-epoch memorization intensity (AUC). Memorized samples are memorized
slightly earlier and show stronger signals in early training. Based on these
insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method:
(a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and
(c) retrain on the filtered dataset. Across multiple tabular datasets and
models, DynamicCut reduces memorization with minimal impact on data diversity
and downstream performance. It also complements augmentation-based defenses.
Furthermore, DynamicCut enables cross-model transferability: high-ranked
samples identified from one model (e.g., a diffusion model) are also effective
for reducing memorization when removed from others, such as GANs and VAEs.

</details>


### [123] [Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning](https://arxiv.org/abs/2505.22355)
*Yongkang Liu,Xingle Xu,Ercong Nie,Zijing Wang,Shi Feng,Daling Wang,Qian Li,Hinrich Schütze*

Main category: cs.LG

TL;DR: 尽管PEFT方法在计算资源需求上较少，但在复杂任务上的表现不如FFT。本文通过优化理论分析了PEFT和FFT的特性差异，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 研究者们发现PEFT在一些基准测试中可以达到与FFT相当的结果，但在复杂任务（如推理和指令微调）中性能不足。因此，需要从理论上对比PEFT和FFT在表示能力和鲁棒性方面的特性。

Method: 基于优化理论，比较PEFT和FFT在表示容量和鲁棒性方面的特性，证明PEFT是FFT的一个严格子集，并给出PEFT的理论上限，说明参数空间限制了模型的表示能力。

Result: 通过在15个数据集和11个对抗测试集上的实验，验证了PEFT受限于参数空间，表示能力较弱且更容易受到扰动影响。

Conclusion: PEFT的性能受限于其参数空间，在复杂任务中的表现不如FFT。这些结果希望推动超越现有PEFT方法的研究。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable
to Full Fine-Tuning (FFT) while requiring significantly fewer computing
resources, making it the go-to choice for researchers. We find that although
PEFT can achieve competitive results on some benchmarks, its performance falls
short of FFT in complex tasks, such as reasoning and instruction-based
fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in
terms of representational capacity and robustness based on optimization theory.
We theoretically demonstrate that PEFT is a strict subset of FFT. By providing
theoretical upper bounds for PEFT, we show that the limited parameter space
constrains the model's representational ability, making it more susceptible to
perturbations. Experiments on 15 datasets encompassing classification,
generation, reasoning, instruction fine-tuning tasks and 11 adversarial test
sets validate our theories. We hope that these results spark further research
beyond the realms of well established PEFT. The source code is in the anonymous
Github repository\footnote{https://github.com/misonsky/PEFTEval}.

</details>


### [124] [Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings](https://arxiv.org/abs/2505.22356)
*Angéline Pouget,Mohammad Yaghini,Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: In safety-critical domains, deploying machine learning models requires ensuring reliable performance without ground truth labels. This paper introduces the suitability filter, a framework detecting performance drops by comparing test and user data distributions using statistical methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of ensuring model reliability in safety-critical applications where direct validation via ground truth labels is not accessible.

Method: Propose a suitability filter that detects performance degradation by utilizing suitability signals, which are model output features sensitive to covariate shifts. It compares empirical distributions of suitability signals from test and user data through statistical hypothesis testing.

Result: Empirical evaluations show that the suitability filter reliably detects performance deviations caused by covariate shift across different classification tasks.

Conclusion: The suitability filter enables proactive mitigation of potential failures in high-stakes applications by reliably detecting performance deterioration.

Abstract: Deploying machine learning models in safety-critical domains poses a key
challenge: ensuring reliable model performance on downstream user data without
access to ground truth labels for direct validation. We propose the suitability
filter, a novel framework designed to detect performance deterioration by
utilizing suitability signals -- model output features that are sensitive to
covariate shifts and indicative of potential prediction errors. The suitability
filter evaluates whether classifier accuracy on unlabeled user data shows
significant degradation compared to the accuracy measured on the labeled test
dataset. Specifically, it ensures that this degradation does not exceed a
pre-specified margin, which represents the maximum acceptable drop in accuracy.
To achieve reliable performance evaluation, we aggregate suitability signals
for both test and user data and compare these empirical distributions using
statistical hypothesis testing, thus providing insights into decision
uncertainty. Our modular method adapts to various models and domains. Empirical
evaluations across different classification tasks demonstrate that the
suitability filter reliably detects performance deviations due to covariate
shift. This enables proactive mitigation of potential failures in high-stakes
applications.

</details>


### [125] [Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs](https://arxiv.org/abs/2505.22358)
*Zhiyi Wan,Wanrou Du,Liang Li,Miao Pan,Xiaoqi Qin*

Main category: cs.LG

TL;DR: OA-Adapter是一种新的参数高效方法，用于大型语言模型的持续学习，通过统一动态预算适应与正交子空间学习，在单一端到端训练阶段中解决了灾难性遗忘问题。实验表明，该方法在准确性和参数效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在持续学习场景中常遭受灾难性遗忘问题，即在训练新任务时，之前学得的任务性能会严重退化。尽管现有的持续学习方法可以缓解任务干扰，但它们通常采用固定的预算分配，忽略了任务和层之间的复杂性差异。此外，最近的自适应预算调整方法存在优化和预算分配脱节的问题，导致潜在的不一致，阻碍了其在持续学习中的实际应用。

Method: 提出了一种名为OA-Adapter的方法，它通过引入动态瓶颈维度适应机制，在单个端到端训练阶段中统一动态预算适应与正交子空间学习。具体而言，OA-Adapter通过施加正交约束来有效地保留先前获得的知识，并与动态预算分配相协调，这些正交约束专门应用于当前任务的参数子空间与历史任务的动态分配参数子空间之间。

Result: 实验结果表明，OA-Adapter在持续学习基准测试中表现优异，不仅在准确性和参数效率方面超越了最先进的方法，而且在标准CL基准上使用更少的参数（减少了58.5%）的情况下实现了更高的平均准确率。

Conclusion: OA-Adapter提供了一种有效的解决方案，解决了大型语言模型在持续学习中的灾难性遗忘问题，同时提高了参数效率。这种方法为未来的研究提供了新的方向，特别是在处理任务复杂性和动态预算分配方面的研究。

Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in
continual learning (CL) scenarios, where performance on previously learned
tasks degrades severely while training on sequentially arriving tasks. Although
pioneering CL approaches using orthogonal subspaces can mitigate task
interference, they typically employ fixed budget allocation, neglecting the
varying complexity across tasks and layers. Besides, recent budget-adaptive
tuning methods for LLMs often adopt multi-stage paradigms that decouple
optimization and budget allocation. Such decoupling results in potential
misalignment, which hinders those approaches' practical application in CL
scenarios. To address these limitations, we propose OA-Adapter, a novel
parameter-efficient approach for continual learning in LLMs that unifies
dynamic budget adaptation with orthogonal subspace learning in a single
end-to-end training stage. Specifically, OA-Adapter introduces a dynamic
bottleneck dimension adaptation mechanism that simultaneously allocates an
efficient parameter budget and optimizes task objectives without misalignment.
To effectively preserve previously acquired knowledge while coordinating with
the dynamic budget allocation, orthogonal constraints are applied specifically
between the parameter subspace of the current task and the dynamically
allocated parameter subspaces of historical tasks. Experimental results on
continual learning benchmarks demonstrate that OA-Adapter outperforms
state-of-the-art methods in both accuracy and parameter efficiency, achieving
higher average accuracy while using 58.5% fewer parameters on the standard CL
benchmark.

</details>


### [126] [Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification](https://arxiv.org/abs/2505.22359)
*Matan Schliserman,Tomer Koren*

Main category: cs.LG

TL;DR: 研究了无正则化梯度方法在可分线性分类中的泛化性能，特别是在多类设置下，风险上界与损失模板的几何结构密切相关。对于指数衰减损失函数，当p=∞时风险对类别数k呈对数依赖，而p=2时风险对k呈线性依赖，并证明了后一种情况下k的多项式依赖不可避免。


<details>
  <summary>Details</summary>
Motivation: 之前的工作大多集中于二分类问题，本文旨在探讨无正则化梯度方法在多类（k类）可分线性分类问题中的泛化性能，特别是损失函数衰减至零时的风险边界。

Method: 分析了梯度下降法在多类线性分类问题中的表现，建立了新的总体风险边界，重点关注损失模板的几何特性对收敛速度的影响。通过引入Rademacher复杂度的新界限，研究了不同范数下的损失函数衰减速率对风险边界的影响。

Result: 发现风险边界主要受损失模板的几何特性影响，而非损失函数本身。对于指数衰减损失函数，当p=∞时，风险对类别数k呈对数依赖；当p=2时，风险对k呈线性依赖，并且证明了在这种情况下k的多项式依赖是不可避免的。

Conclusion: 无正则化梯度方法在多类线性分类问题中的泛化性能受到损失模板几何特性的显著影响，不同范数下的衰减特性会导致不同的风险依赖关系。

Abstract: We study the generalization performance of unregularized gradient methods for
separable linear classification. While previous work mostly deal with the
binary case, we focus on the multiclass setting with $k$ classes and establish
novel population risk bounds for Gradient Descent for loss functions that decay
to zero. In this setting, we show risk bounds that reveal that convergence
rates are crucially influenced by the geometry of the loss template, as
formalized by Wang and Scott (2024), rather than of the loss function itself.
Particularly, we establish risk upper bounds that holds for any decay rate of
the loss whose template is smooth with respect to the $p$-norm. In the case of
exponentially decaying losses, our results indicates a contrast between the
$p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and
$p=2$ where the risk scales linearly with $k$. To establish this separation
formally, we also prove a lower bound in the latter scenario, demonstrating
that the polynomial dependence on $k$ is unavoidable. Central to our analysis
is a novel bound on the Rademacher complexity of low-noise vector-valued linear
predictors with a loss template smooth w.r.t.~general $p$-norms.

</details>


### [127] [Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles](https://arxiv.org/abs/2505.22361)
*Xiangyu Chang,Xi Chen,Yining Wang,Zhiyi Zeng*

Main category: cs.LG

TL;DR: This paper studies a bandit optimization problem with a new pairwise comparison oracle, addresses challenges in decision-making and biased estimation through discretization, polynomial approximation, and tournament successive elimination techniques, achieving optimal regret bounds and improving results in operations management problems.


<details>
  <summary>Details</summary>
Motivation: To maximize a function $f(x)$ over $T$ periods for an unknown strongly concave function $f$, the paper introduces a new pairwise comparison oracle that finds applications in joint pricing and inventory replenishment problems as well as network revenue management.

Method: The method involves using a discretization technique and local polynomial approximation to relate the problem to linear bandits. A tournament successive elimination technique is developed to localize the discretized cell, followed by running an interactive batched version of LinUCB algorithm on cells.

Result: The authors establish regret bounds that are optimal up to poly-logarithmic factors. When applied to two operations management problems, the proposed algorithm and analytical framework yield improved state-of-the-art results.

Conclusion: The study successfully addresses the challenges in the bandit optimization problem with a new pairwise comparison oracle, providing effective solutions for operations management problems and setting new benchmarks with optimal regret bounds.

Abstract: This paper studies a bandit optimization problem where the goal is to
maximize a function $f(x)$ over $T$ periods for some unknown strongly concave
function $f$. We consider a new pairwise comparison oracle, where the
decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of
periods and then obtains an estimate of $f(x)-f(x')$. We show that such a
pairwise comparison oracle finds important applications to joint pricing and
inventory replenishment problems and network revenue management. The challenge
in this bandit optimization is twofold. First, the decision-maker not only
needs to determine a pair of actions $(x, x')$ but also a stopping time $n$
(i.e., the number of queries based on $(x, x')$). Second, motivated by our
inventory application, the estimate of the difference $f(x)-f(x')$ is biased,
which is different from existing oracles in stochastic optimization literature.
To address these challenges, we first introduce a discretization technique and
local polynomial approximation to relate this problem to linear bandits. Then
we developed a tournament successive elimination technique to localize the
discretized cell and run an interactive batched version of LinUCB algorithm on
cells. We establish regret bounds that are optimal up to poly-logarithmic
factors. Furthermore, we apply our proposed algorithm and analytical framework
to the two operations management problems and obtain results that improve
state-of-the-art results in the existing literature.

</details>


### [128] [Directed Homophily-Aware Graph Neural Network](https://arxiv.org/abs/2505.22362)
*Aihu Zhang,Jiaxing Xu,Mengcheng Lan,Shili Xiang,Yiping Ke*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) have been successful in many areas, but they face challenges when dealing with heterophilic neighborhoods and directed graphs. This paper introduces Directed Homophily-aware Graph Neural Network (DHGNN), a new framework that incorporates homophily-aware and direction-sensitive components to overcome these limitations. DHGNN uses a resettable gating mechanism and structure-aware noise-tolerant fusion module to improve performance on node classification and link prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Current GNNs struggle with heterophilic neighborhoods and ignore the directional nature of real-world graphs, leading to suboptimal performance on directed graphs with asymmetric structures.

Method: The proposed method, DHGNN, includes a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions.

Result: Extensive experiments show that DHGNN outperforms state-of-the-art methods in node classification and link prediction on both homophilic and heterophilic directed graph datasets. Specifically, DHGNN improves over the best baseline by up to 15.07% in link prediction.

Conclusion: DHGNN addresses the limitations of current GNNs by incorporating homophily-aware and direction-sensitive components, demonstrating superior performance in various tasks.

Abstract: Graph Neural Networks (GNNs) have achieved significant success in various
learning tasks on graph-structured data. Nevertheless, most GNNs struggle to
generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the
directional nature of real-world graphs, resulting in suboptimal performance on
directed graphs with asymmetric structures. In this work, we propose Directed
Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses
these limitations by incorporating homophily-aware and direction-sensitive
components. DHGNN employs a resettable gating mechanism to adaptively modulate
message contributions based on homophily levels and informativeness, and a
structure-aware noise-tolerant fusion module to effectively integrate node
representations from the original and reverse directions. Extensive experiments
on both homophilic and heterophilic directed graph datasets demonstrate that
DHGNN outperforms state-of-the-art methods in node classification and link
prediction. In particular, DHGNN improves over the best baseline by up to
15.07% in link prediction. Our analysis further shows that the gating mechanism
captures directional homophily gaps and fluctuating homophily across layers,
providing deeper insights into message-passing behavior on complex graph
structures.

</details>


### [129] [SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting](https://arxiv.org/abs/2505.22370)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Weili Guan,Min Zhang,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces SplitLoRA, a novel continual learning approach based on Low-Rank Adaptation that balances stability and plasticity by optimally partitioning the gradient space.


<details>
  <summary>Details</summary>
Motivation: Continual Learning requires maintaining stability (preserving past knowledge) and plasticity (learning new tasks), but existing Gradient Projection methods struggle to appropriately partition the gradient space to achieve this balance.

Method: The authors propose SplitLoRA, which involves a theoretical analysis of subspace partitioning's effects on stability and plasticity, followed by an effective method to derive the optimal partition of the gradient space for previously learned tasks.

Result: Experimental results on multiple datasets demonstrate state-of-the-art performance of the proposed SplitLoRA method in continual learning.

Conclusion: SplitLoRA effectively balances stability and plasticity in continual learning through optimal gradient space partitioning.

Abstract: Continual Learning requires a model to learn multiple tasks in sequence while
maintaining both stability:preserving knowledge from previously learned tasks,
and plasticity:effectively learning new tasks. Gradient projection has emerged
as an effective and popular paradigm in CL, where it partitions the gradient
space of previously learned tasks into two orthogonal subspaces: a primary
subspace and a minor subspace. New tasks are learned effectively within the
minor subspace, thereby reducing interference with previously acquired
knowledge. However, existing Gradient Projection methods struggle to achieve an
optimal balance between plasticity and stability, as it is hard to
appropriately partition the gradient space. In this work, we consider a
continual learning paradigm based on Low-Rank Adaptation, which has gained
considerable attention due to its efficiency and wide applicability, and
propose a novel approach for continual learning, called SplitLoRA. We first
provide a theoretical analysis of how subspace partitioning affects model
stability and plasticity. Informed by this analysis, we then introduce an
effective method that derives the optimal partition of the gradient space for
previously learned tasks. This approach effectively balances stability and
plasticity in continual learning. Experimental results on multiple datasets
demonstrate that the proposed method achieves state-of-the-art performance.

</details>


### [130] [A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation](https://arxiv.org/abs/2505.22381)
*Lukas Kirchdorfer,Konrad Özdemir,Stjepan Kusenic,Han van der Aa,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: In order to improve the accuracy and reliability of Business Process Simulation (BPS), this paper proposes a new method called Auto Time Kernel Density Estimation (AT-KDE). It is more accurate and robust than existing approaches, while maintaining reasonable execution time efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for modeling case-arrival in BPS often rely on oversimplified static distributions which cannot capture the dynamic and temporal complexities inherent in organizational environments. This leads to less accurate and reliable simulation outcomes.

Method: The proposed method, AT-KDE, uses a divide-and-conquer approach to model arrival times of processes by considering global dynamics, day-of-week variations, and intraday distributional changes.

Result: Experiments conducted across 20 diverse processes showed that AT-KDE is significantly more accurate and robust compared to existing methods while keeping sensible execution time efficiency.

Conclusion: AT-KDE improves the precision and scalability of case-arrival modeling in BPS, making it a superior alternative to current approaches.

Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and
improving organizational processes by estimating the impact of process changes.
A key component of BPS is the case-arrival model, which determines the pattern
of new case entries into a process. Although accurate case-arrival modeling is
essential for reliable simulations, as it influences waiting and overall cycle
times, existing approaches often rely on oversimplified static distributions of
inter-arrival times. These approaches fail to capture the dynamic and temporal
complexities inherent in organizational environments, leading to less accurate
and reliable outcomes. To address this limitation, we propose Auto Time Kernel
Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival
times of processes by incorporating global dynamics, day-of-week variations,
and intraday distributional changes, ensuring both precision and scalability.
Experiments conducted across 20 diverse processes demonstrate that AT-KDE is
far more accurate and robust than existing approaches while maintaining
sensible execution time efficiency.

</details>


### [131] [Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning](https://arxiv.org/abs/2505.22389)
*Haomiao Qiu,Miao Zhang,Ziyue Qiao,Liqiang Nie*

Main category: cs.LG

TL;DR: Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks while avoiding catastrophic forgetting. The proposed Perturb-and-Merge (P&M) framework integrates model merging into CL to mitigate forgetting by constructing a new model via convex combination, deriving an analytical solution for the optimal merging coefficient, and devising a stochastic perturbation strategy for effective regularization approximation without additional forward or backward passes. When combined with LoRA, P&M achieves state-of-the-art performance on several continual learning benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing CL methods suffer from catastrophic forgetting as they rely solely on parameters from the most recent task for inference. To address this limitation, there is a need for a novel approach that leverages model merging techniques to better retain learned information across tasks.

Method: The proposed method, Perturb-and-Merge (P&M), constructs a new model after training on each task by forming a convex combination of the previous model and the newly trained task-specific model. An analytical solution for the optimal merging coefficient is derived through theoretical analysis to minimize total loss increase across all tasks. A regularization term composed of the task vector and the Hessian matrix is introduced to alleviate degradation during merging, which can be efficiently approximated using second-order symmetric finite differences. Additionally, a stochastic perturbation strategy along the task vector direction is devised to provide an effective approximation of the regularization term without incurring additional forward or backward passes. Finally, P&M is combined with LoRA for parameter-efficient fine-tuning.

Result: The proposed Perturb-and-Merge (P&M) framework, when combined with LoRA, achieves state-of-the-art performance on several continual learning benchmark datasets.

Conclusion: Perturb-and-Merge (P&M) is a novel continual learning framework that effectively mitigates catastrophic forgetting by integrating model merging techniques. By constructing a new model via convex combination, deriving an analytical solution for the optimal merging coefficient, and devising an efficient regularization approximation strategy, P&M demonstrates superior performance on continual learning benchmarks when combined with LoRA.

Abstract: Continual Learning (CL) aims to enable models to continuously acquire new
knowledge from a sequence of tasks with avoiding the forgetting of learned
information. However, existing CL methods only rely on the parameters of the
most recent task for inference, which makes them susceptible to catastrophic
forgetting. Inspired by the recent success of model merging techniques, we
propose \textbf{Perturb-and-Merge (P\&M)}, a novel continual learning framework
that integrates model merging into the CL paradigm to mitigate forgetting.
Specifically, after training on each task, P\&M constructs a new model by
forming a convex combination of the previous model and the newly trained
task-specific model. Through theoretical analysis, we minimize the total loss
increase across all tasks and derive an analytical solution for the optimal
merging coefficient. To further improve the performance of the merged model, we
observe that the degradation introduced during merging can be alleviated by a
regularization term composed of the task vector and the Hessian matrix of the
loss function. Interestingly, we show that this term can be efficiently
approximated using second-order symmetric finite differences, and a stochastic
perturbation strategy along the task vector direction is accordingly devised
which incurs no additional forward or backward passes while providing an
effective approximation of the regularization term. Finally, we combine P\&M
with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead.
Our proposed approach achieves state-of-the-art performance on several
continual learning benchmark datasets.

</details>


### [132] [Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation](https://arxiv.org/abs/2505.22391)
*Yi Zhang,Difan Zou*

Main category: cs.LG

TL;DR: The paper proposes Physics-Informed Distillation of Diffusion Models (PIDDM), a post-hoc distillation approach that enforces PDE constraints during the distillation stage to improve generative modeling accuracy and PDE satisfaction without significant computational overhead.


<details>
  <summary>Details</summary>
Motivation: Diffusion models used for physical systems governed by PDEs face challenges due to Jensen's Gap when enforcing PDE constraints on the expectation of clean samples rather than directly on the true clean data. This creates a trade-off between constraint enforcement and generative modeling accuracy.

Method: The authors introduce PIDDM, which avoids injecting PDE constraints directly into the diffusion process. Instead, these constraints are enforced in a post-hoc distillation stage. This method supports single-step generation with improved PDE satisfaction and can handle forward and inverse problems as well as reconstruction from partial observations.

Result: Extensive experiments across various PDE benchmarks show that PIDDM significantly improves PDE satisfaction compared to recent baselines like PIDM, DiffusionPDE, and ECI-sampling, while also reducing computational overhead.

Conclusion: PIDDM provides an efficient and effective strategy for incorporating physical constraints into diffusion models, improving both accuracy and PDE satisfaction.

Abstract: Modeling physical systems in a generative manner offers several advantages,
including the ability to handle partial observations, generate diverse
solutions, and address both forward and inverse problems. Recently, diffusion
models have gained increasing attention in the modeling of physical systems,
particularly those governed by partial differential equations (PDEs). However,
diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate
steps, making it infeasible to directly enforce constraints on the clean sample
$\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are
typically applied to the expectation of clean samples
$\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the
learned score network. However, imposing PDE constraints on the expectation
does not strictly represent the one on the true clean data, known as Jensen's
Gap. This gap creates a trade-off: enforcing PDE constraints may come at the
cost of reduced accuracy in generative modeling. To address this, we propose a
simple yet effective post-hoc distillation approach, where PDE constraints are
not injected directly into the diffusion process, but instead enforced during a
post-hoc distillation stage. We term our method as Physics-Informed
Distillation of Diffusion Models (PIDDM). This distillation not only
facilitates single-step generation with improved PDE satisfaction, but also
support both forward and inverse problem solving and reconstruction from
randomly partial observation. Extensive experiments across various PDE
benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over
several recent and competitive baselines, such as PIDM, DiffusionPDE, and
ECI-sampling, with less computation overhead. Our approach can shed light on
more efficient and effective strategies for incorporating physical constraints
into diffusion models.

</details>


### [133] [Mitigating Overthinking in Large Reasoning Models via Manifold Steering](https://arxiv.org/abs/2505.22411)
*Yao Huang,Huanran Chen,Shouwei Ruan,Yichi Zhang,Xingxing Wei,Yinpeng Dong*

Main category: cs.LG

TL;DR: Recent advances in Large Reasoning Models (LRMs) have shown great abilities in complex tasks, but suffer from overthinking. This paper proposes Manifold Steering to mitigate overthinking by projecting the steering direction onto a low-dimensional activation manifold. Experiments show that this method reduces output tokens significantly while maintaining or improving accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the issue of overthinking in LRMs which leads to computational overheads.

Method: Investigate the underlying mechanisms of overthinking from the perspective of mechanistic interpretability and propose Manifold Steering, which projects the steering direction onto the low-dimensional activation manifold.

Result: Reduces output tokens by up to 71% while maintaining and even improving accuracy on mathematical benchmarks, and exhibits robust cross-domain transferability.

Conclusion: Manifold Steering effectively mitigates overthinking in LRMs and can be applied across different domains.

Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in solving complex tasks such as mathematics and coding. However,
these models frequently exhibit a phenomenon known as overthinking during
inference, characterized by excessive validation loops and redundant
deliberation, leading to substantial computational overheads. In this paper, we
aim to mitigate overthinking by investigating the underlying mechanisms from
the perspective of mechanistic interpretability. We first showcase that the
tendency of overthinking can be effectively captured by a single direction in
the model's activation space and the issue can be eased by intervening the
activations along this direction. However, this efficacy soon reaches a plateau
and even deteriorates as the intervention strength increases. We therefore
systematically explore the activation space and find that the overthinking
phenomenon is actually tied to a low-dimensional manifold, which indicates that
the limited effect stems from the noises introduced by the high-dimensional
steering direction. Based on this insight, we propose Manifold Steering, a
novel approach that elegantly projects the steering direction onto the
low-dimensional activation manifold given the theoretical approximation of the
interference noise. Extensive experiments on DeepSeek-R1 distilled models
validate that our method reduces output tokens by up to 71% while maintaining
and even improving the accuracy on several mathematical benchmarks. Our method
also exhibits robust cross-domain transferability, delivering consistent token
reduction performance in code generation and knowledge-based QA tasks. Code is
available at: https://github.com/Aries-iai/Manifold_Steering.

</details>


### [134] [STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals](https://arxiv.org/abs/2505.22422)
*Václav Voráček,Francesco Orabona*

Main category: cs.LG

TL;DR: The paper proposes a betting-based algorithm for constructing confidence intervals that outperforms existing methods and guarantees optimal interval width.


<details>
  <summary>Details</summary>
Motivation: Constructing the tightest possible confidence intervals is crucial when sampling is expensive. Current state-of-the-art betting algorithms are either sub-optimal or lack finite-time guarantees in fixed horizon settings.

Method: A new betting-based algorithm is introduced which computes confidence intervals by using an optimal strategy at each step, rather than choosing a constant strategy in advance. This approach improves upon classical concentration inequalities such as Hoeffding's and Bernstein's.

Result: Empirically, the proposed algorithm outperforms competitors. Theoretically, it is proven that the width of the confidence intervals is optimal up to a diminishing factor.

Conclusion: This work bridges the gap in fixed horizon settings by providing a betting-based algorithm with finite-time guarantees and optimal confidence interval width.

Abstract: The construction of confidence intervals for the mean of a bounded random
variable is a classical problem in statistics with numerous applications in
machine learning and virtually all scientific fields. In particular, obtaining
the tightest possible confidence intervals is vital every time the sampling of
the random variables is expensive. The current state-of-the-art method to
construct confidence intervals is by using betting algorithms. This is a very
successful approach for deriving optimal confidence sequences, even matching
the rate of law of iterated logarithms. However, in the fixed horizon setting,
these approaches are either sub-optimal or based on heuristic solutions with
strong empirical performance but without a finite-time guarantee. Hence, no
betting-based algorithm guaranteeing the optimal
$\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the
confidence intervals are known. This work bridges this gap. We propose a
betting-based algorithm to compute confidence intervals that empirically
outperforms the competitors. Our betting strategy uses the optimal strategy in
every step (in a certain sense), whereas the standard betting methods choose a
constant strategy in advance. Leveraging this fact results in strict
improvements even for classical concentration inequalities, such as the ones of
Hoeffding or Bernstein. Moreover, we also prove that the width of our
confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$.
The code is available
on~https://github.com/vvoracek/STaR-bets-confidence-interval.

</details>


### [135] [Scaling Reasoning without Attention](https://arxiv.org/abs/2505.22425)
*Xueliang Zhao,Wei Wu,Lingpeng Kong*

Main category: cs.LG

TL;DR: 本论文提出了一种新的无注意力语言模型，通过架构和数据驱动的创新解决了Transformer架构效率低下的问题，并通过两阶段课程微调策略提高了复杂推理能力。在基准评估中，该模型超越了相同规模的强大Transformer和混合模型，甚至超过了更大规模的Gemma3-27B。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在复杂推理任务上取得了显著进展，但仍存在架构效率低下（依赖Transformer）以及缺乏针对高难度领域的结构化微调的问题。为了解决这些问题，需要一种新的模型架构和训练方法。

Method: 提出了一个基于Mamba-2的SSD层的无注意力语言模型，消除了自注意力和键值缓存的需求，实现了固定内存和常数时间推理。同时，提出了一种基于PromptCoT合成范式的两阶段课程微调策略，生成结构化问题以提高复杂推理能力。

Result: 在多个基准测试中，OurModel-7B的表现优于相同规模的Transformer和混合模型，并且在AIME 24、AIME 25和Livecodebench上分别超过Gemma3-27B 2.6%、0.6%和3.0%。

Conclusion: 状态空间模型作为一种高效且可扩展的替代方案，具有作为高容量推理的注意力机制架构的潜力。

Abstract: Large language models (LLMs) have made significant advances in complex
reasoning tasks, yet they remain bottlenecked by two core challenges:
architectural inefficiency due to reliance on Transformers, and a lack of
structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an
attention-free language model that addresses both issues through architectural
and data-centric innovations. Built on the state space dual (SSD) layers of
Mamba-2, our model eliminates the need for self-attention and key-value
caching, enabling fixed-memory, constant-time inference. To train it for
complex reasoning, we propose a two-phase curriculum fine-tuning strategy based
on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically
structured problems via abstract concept selection and rationale-guided
generation. On benchmark evaluations, \ourmodel-7B outperforms strong
Transformer and hybrid models of comparable scale, and even surpasses the much
larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on
Livecodebench. These results highlight the potential of state space models as
efficient and scalable alternatives to attention-based architectures for
high-capacity reasoning.

</details>


### [136] [Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models](https://arxiv.org/abs/2505.22440)
*Khan Masood Parvez,Sk Md Abidar Rahaman,Ali Shiri Sichani*

Main category: cs.LG

TL;DR: 本研究展示了一种结合量子行为动态粒子群优化（QDPSO）与ANSYS HFSS仿真的机器学习增强工作流，以加速天线设计。通过优化、预测和验证的完整设计周期仅需12.42分钟，比传统方法快240倍。该框架为6G和物联网应用中的下一代射频系统提供了可扩展的设计范式。


<details>
  <summary>Details</summary>
Motivation: 无线技术的快速发展需要自动化设计框架来解决天线小型化和性能优化的问题，特别是在受限的开发周期内。传统的试错方法耗时且效率低，因此需要更高效的设计方法。

Method: 将QDPSO算法与ANSYS HFSS仿真结合，自动优化天线环尺寸；利用机器学习模型（SVM、随机森林、XGBoost和堆叠集成）基于936个模拟数据集预测共振频率；最后通过ANSYS验证设计结果。

Result: QDPSO算法在11.53秒内将共振频率从1.60 GHz降低到1.4208 GHz（减少12.7%）；机器学习模型中堆叠模型训练精度最高（R2=0.9825），SVM验证性能最佳（R2=0.7197）；完整设计周期仅需12.42分钟，相较于传统方法加速了240倍。

Conclusion: 这种AI驱动的优化与CAD验证相结合的框架减少了工程工作量，确保了可生产的设计，并为6G和物联网应用中的下一代射频系统提供了一个可扩展的设计范式。

Abstract: The rapid evolution of wireless technologies necessitates automated design
frameworks to address antenna miniaturization and performance optimization
within constrained development cycles. This study demonstrates a machine
learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm
Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design.
The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds,
achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared
to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest,
XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds
using 936 simulation datasets, with stacked models showing superior training
accuracy (R2=0.9825) and SVM demonstrating optimal validation performance
(R2=0.7197). The complete design cycle, encompassing optimization, prediction,
and ANSYS validation, required 12.42 minutes on standard desktop hardware
(Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of
PSADEA-based approaches. This 240 times of acceleration eliminates traditional
trial-and-error methods that often extend beyond seven expert-led days. The
system enables precise specifications of performance targets with automated
generation of fabrication-ready parameters, particularly benefiting compact
consumer devices requiring rapid frequency tuning. By bridging AI-driven
optimization with CAD validation, this framework reduces engineering workloads
while ensuring production-ready designs, establishing a scalable paradigm for
next-generation RF systems in 6G and IoT applications.

</details>


### [137] [SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning](https://arxiv.org/abs/2505.22442)
*Mattie Fellows,Clarisse Wibault,Uljad Berdica,Johannes Forkel,Jakob N. Foerster,Michael A. Osborne*

Main category: cs.LG

TL;DR: This paper proposes two algorithms, SOReL and TOReL, to improve offline reinforcement learning by enabling safe and reliable hyperparameter tuning without online interactions.


<details>
  <summary>Details</summary>
Motivation: Sample efficiency is a major obstacle for real-world adoption of reinforcement learning (RL). Offline RL offers a solution but current methods rely on extensive online interactions for hyperparameter tuning and have no reliable bound on their initial online performance.

Method: SOReL uses a Bayesian approach to infer a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. All hyperparameters are tuned fully offline. TOReL extends information rate based offline hyperparameter tuning methods to general offline RL approaches.

Result: Empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting. TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data.

Conclusion: SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world.

Abstract: Sample efficiency remains a major obstacle for real world adoption of
reinforcement learning (RL): success has been limited to settings where
simulators provide access to essentially unlimited environment interactions,
which in reality are typically costly or dangerous to obtain. Offline RL in
principle offers a solution by exploiting offline data to learn a near-optimal
policy before deployment. In practice, however, current offline RL methods rely
on extensive online interactions for hyperparameter tuning, and have no
reliable bound on their initial online performance. To address these two
issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe
offline reinforcement learning. Using only offline data, our Bayesian approach
infers a posterior over environment dynamics to obtain a reliable estimate of
the online performance via the posterior predictive uncertainty. Crucially, all
hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a
tuning for offline reinforcement learning algorithm that extends our
information rate based offline hyperparameter tuning methods to general offline
RL approaches. Our empirical evaluation confirms SOReL's ability to accurately
estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter
tuning achieves competitive performance with the best online hyperparameter
tuning methods using only offline data. Thus, SOReL and TOReL make a
significant step towards safe and reliable offline RL, unlocking the potential
for RL in the real world. Our implementations are publicly available:
https://github.com/CWibault/sorel\_torel.

</details>


### [138] [Position: All Current Generative Fidelity and Diversity Metrics are Flawed](https://arxiv.org/abs/2505.22450)
*Ossi Räisä,Boris van Breugel,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 这篇论文指出当前生成模型的保真度和多样性指标存在缺陷，严重影响了合成数据的实际应用。作者提出了一系列合成数据指标的理想特性及合理性检查，并建议研究界应更加注重发展指标而非单纯关注模型。此外，通过分析现有指标的不足，为从业者提供了使用这些指标的指导。


<details>
  <summary>Details</summary>
Motivation: 生成建模的流行突显了合成数据指标的重要性，然而现有的指标存在许多问题，如缺乏异常值鲁棒性和界限不清等，这些问题限制了方法的发展和实际应用。

Method: 作者提出了一套合成数据指标的理想特性清单，以及一系列合理性检查实验，旨在检测特定且已知的生成建模失败模式。通过这些检查，评估当前指标的表现。

Result: 所有当前的生成保真度和多样性指标都被发现存在缺陷，这大大阻碍了合成数据的实际应用。

Conclusion: 需要更多的努力来开发更可靠的合成数据指标，同时提供了关于如何正确（不）使用现有指标的指南。

Abstract: Any method's development and practical application is limited by our ability
to measure its reliability. The popularity of generative modeling emphasizes
the importance of good synthetic data metrics. Unfortunately, previous works
have found many failure cases in current metrics, for example lack of outlier
robustness and unclear lower and upper bounds. We propose a list of desiderata
for synthetic data metrics, and a suite of sanity checks: carefully chosen
simple experiments that aim to detect specific and known generative modeling
failure modes. Based on these desiderata and the results of our checks, we
arrive at our position: all current generative fidelity and diversity metrics
are flawed. This significantly hinders practical use of synthetic data. Our aim
is to convince the research community to spend more effort in developing
metrics, instead of models. Additionally, through analyzing how current metrics
fail, we provide practitioners with guidelines on how these metrics should
(not) be used.

</details>


### [139] [Pure Exploration with Infinite Answers](https://arxiv.org/abs/2505.22473)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.LG

TL;DR: 研究了正确答案集可能无限的纯探索问题，提出了一个实例依赖的下界，并分析了现有方法在更通用设定下的不足，最后提出了一种新的框架 Sticky-Sequence Track-and-Stop，具有渐进最优性。


<details>
  <summary>Details</summary>
Motivation: 研究无限正确答案集的纯探索问题，理解现有方法在更通用设定下的表现及局限性。

Method: 1. 推导出针对无限正确答案集问题的实例依赖下界。
2. 分析现有方法（如Sticky Track-and-Stop）为何在该通用设定下无法达到渐进最优。
3. 提出新框架Sticky-Sequence Track-and-Stop，该框架整合并扩展了Track-and-Stop和Sticky Track-and-Stop。

Result: 1. 得到了一个适用于无限正确答案集问题的实例依赖下界。
2. 明确了现有方法在更通用设定下的不足。
3. 新框架Sticky-Sequence Track-and-Stop被证明具有渐进最优性，并且由于其通用性，也揭示了一些特殊情况下现有方法的最优性。

Conclusion: 提出了一个新框架Sticky-Sequence Track-and-Stop解决无限正确答案集的纯探索问题，展示了其渐进最优性，并通过分析下界解释了现有方法的局限性。

Abstract: We study pure exploration problems where the set of correct answers is
possibly infinite, e.g., the regression of any continuous function of the means
of the bandit. We derive an instance-dependent lower bound for these problems.
By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop)
for finite answer problems fail at being asymptotically optimal in this more
general setting. Finally, we present a framework, Sticky-Sequence
Track-and-Stop, which generalizes both Track-and-Stop and Sticky
Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality,
our analysis also highlights special cases where existing methods enjoy
optimality.

</details>


### [140] [Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis](https://arxiv.org/abs/2505.22474)
*Amirhossein Sohrabbeig,Omid Ardakanian,Petr Musilek*

Main category: cs.LG

TL;DR: This paper presents a new multivariate time-series forecasting model using Graph Neural Networks (GNNs) to predict urban data such as weather, air pollution, carbon intensity, and energy demand. The model incorporates decomposition-based preprocessing to isolate trend, seasonal, and residual components. Experiments on real-world datasets show the model's effectiveness in various forecasting scenarios, contributing to energy-efficient urban development.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research is the complexity of forecasting multivariate urban data due to intricate dependencies between various urban metrics like weather, air pollution, carbon intensity, and energy demand.

Method: The method involves developing a novel multivariate time-series forecasting model that leverages advanced Graph Neural Networks (GNNs) to capture spatial dependencies among different time-series variables. A decomposition-based preprocessing step isolates trend, seasonal, and residual components to improve forecast accuracy and interpretability.

Result: The experiments conducted on real-world datasets including electricity usage, weather metrics, carbon intensity, and air pollution data demonstrate the effectiveness of the proposed approach across various forecasting scenarios.

Conclusion: The conclusion drawn from the results is that the proposed model has the potential to optimize smart infrastructure systems, leading to energy-efficient urban development and enhanced public well-being.

Abstract: The forecasting of multivariate urban data presents a complex challenge due
to the intricate dependencies between various urban metrics such as weather,
air pollution, carbon intensity, and energy demand. This paper introduces a
novel multivariate time-series forecasting model that utilizes advanced Graph
Neural Networks (GNNs) to capture spatial dependencies among different
time-series variables. The proposed model incorporates a decomposition-based
preprocessing step, isolating trend, seasonal, and residual components to
enhance the accuracy and interpretability of forecasts. By leveraging the
dynamic capabilities of GNNs, the model effectively captures interdependencies
and improves the forecasting performance. Extensive experiments on real-world
datasets, including electricity usage, weather metrics, carbon intensity, and
air pollution data, demonstrate the effectiveness of the proposed approach
across various forecasting scenarios. The results highlight the potential of
the model to optimize smart infrastructure systems, contributing to
energy-efficient urban development and enhanced public well-being.

</details>


### [141] [Non-Asymptotic Analysis of (Sticky) Track-and-Stop](https://arxiv.org/abs/2505.22475)
*Riccardo Poiani,Martino Bernasconi,Andrea Celli*

Main category: cs.LG

TL;DR: In pure exploration problems, the Track-and-Stop algorithm and its extension, Sticky Track-and-Stop, are analyzed for their non-asymptotic guarantees.


<details>
  <summary>Details</summary>
Motivation: To provide non-asymptotic guarantees for the Track-and-Stop and Sticky Track-and-Stop algorithms in pure exploration problems where a statistician sequentially collects information to answer questions about an unknown environment with a maximum risk parameter δ.

Method: Analyzing the Track-and-Stop algorithm which is known for asymptotic optimality sample complexity guarantees when δ→0 under single-valued mappings, and its extension Sticky Track-and-Stop for environments with multiple correct answers.

Result: Provided non-asymptotic guarantees for both Track-and-Stop and Sticky Track-and-Stop algorithms.

Conclusion: The study successfully fills the gap by offering non-asymptotic performance guarantees for both algorithms used in pure exploration problems.

Abstract: In pure exploration problems, a statistician sequentially collects
information to answer a question about some stochastic and unknown environment.
The probability of returning a wrong answer should not exceed a maximum risk
parameter $\delta$ and good algorithms make as few queries to the environment
as possible. The Track-and-Stop algorithm is a pioneering method to solve these
problems. Specifically, it is well-known that it enjoys asymptotic optimality
sample complexity guarantees for $\delta\to 0$ whenever the map from the
environment to its correct answers is single-valued (e.g., best-arm
identification with a unique optimal arm). The Sticky Track-and-Stop algorithm
extends these results to settings where, for each environment, there might
exist multiple correct answers (e.g., $\epsilon$-optimal arm identification).
Although both methods are optimal in the asymptotic regime, their
non-asymptotic guarantees remain unknown. In this work, we fill this gap and
provide non-asymptotic guarantees for both algorithms.

</details>


### [142] [A Closer Look at Multimodal Representation Collapse](https://arxiv.org/abs/2505.22483)
*Abhra Chaudhuri,Anjan Dutta,Tu Bui,Serban Georgescu*

Main category: cs.LG

TL;DR: The paper explores modality collapse in multimodal fusion models, identifies its cause as entanglement of noisy and predictive features, proves cross-modal knowledge distillation can disentangle them, and proposes an algorithm to prevent modality collapse via basis reallocation.


<details>
  <summary>Details</summary>
Motivation: To develop a fundamental understanding of modality collapse, a phenomenon where multimodal fusion models tend to rely only on a subset of modalities while ignoring others.

Method: Show that modality collapse occurs due to entanglement of noisy features from one modality with predictive features from another via shared neurons. Prove that cross-modal knowledge distillation disentangles these representations by freeing rank bottlenecks in the student encoder. Propose an algorithm for preventing modality collapse through explicit basis reallocation.

Result: Extensive experiments on multiple multimodal benchmarks validate the theoretical claims about preventing modality collapse and handling missing modalities.

Conclusion: Modality collapse is caused by feature entanglement, cross-modal knowledge distillation can help disentangle them, and the proposed algorithm effectively prevents modality collapse.

Abstract: We aim to develop a fundamental understanding of modality collapse, a
recently observed empirical phenomenon wherein models trained for multimodal
fusion tend to rely only on a subset of the modalities, ignoring the rest. We
show that modality collapse happens when noisy features from one modality are
entangled, via a shared set of neurons in the fusion head, with predictive
features from another, effectively masking out positive contributions from the
predictive features of the former modality and leading to its collapse. We
further prove that cross-modal knowledge distillation implicitly disentangles
such representations by freeing up rank bottlenecks in the student encoder,
denoising the fusion-head outputs without negatively impacting the predictive
features from either modality. Based on the above findings, we propose an
algorithm that prevents modality collapse through explicit basis reallocation,
with applications in dealing with missing modalities. Extensive experiments on
multiple multimodal benchmarks validate our theoretical claims. Project page:
https://abhrac.github.io/mmcollapse/.

</details>


### [143] [Understanding Adversarial Training with Energy-based Models](https://arxiv.org/abs/2505.22486)
*Mujtaba Hussain Mirza,Maria Rosaria Briglia,Filippo Bartolucci,Senad Beadini,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.LG

TL;DR: This paper explores adversarial training in classifiers using an Energy-based Model (EBM) framework. It introduces the Delta Energy Regularizer (DER) to address issues of Catastrophic Overfitting and Robust Overfitting, and proposes a technique based on local class-wise PCA for improving sample diversity and generation quality.


<details>
  <summary>Details</summary>
Motivation: To better understand adversarial training in classifiers and analyze the intrinsic generative capabilities of robust classifiers using the EBM framework.

Method: Analyze adversarial examples through energy lens, study overfitting phenomena (CO & RO) via energy dynamics, propose DER regularizer, and use local class-wise PCA with energy-based guidance for generative improvements.

Result: DER effectively mitigates CO and RO across benchmarks; proposed generative technique achieves competitive IS and FID scores without explicit generative model training.

Conclusion: Energy-based analysis provides insights into adversarial training and robust classifier's generative limitations; DER and improved generative techniques enhance performance.

Abstract: We aim at using Energy-based Model (EBM) framework to better understand
adversarial training (AT) in classifiers, and additionally to analyze the
intrinsic generative capabilities of robust classifiers. By viewing standard
classifiers through an energy lens, we begin by analyzing how the energies of
adversarial examples, generated by various attacks, differ from those of the
natural samples. The central focus of our work is to understand the critical
phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT
from an energy perspective. We analyze the impact of existing AT approaches on
the energy of samples during training and observe that the behavior of the
``delta energy' -- change in energy between original sample and its adversarial
counterpart -- diverges significantly when CO or RO occurs. After a thorough
analysis of these energy dynamics and their relationship with overfitting, we
propose a novel regularizer, the Delta Energy Regularizer (DER), designed to
smoothen the energy landscape during training. We demonstrate that DER is
effective in mitigating both CO and RO across multiple benchmarks. We further
show that robust classifiers, when being used as generative models, have limits
in handling trade-off between image quality and variability. We propose an
improved technique based on a local class-wise principal component analysis
(PCA) and energy-based guidance for better class-specific initialization and
adaptive stopping, enhancing sample diversity and generation quality.
Considering that we do not explicitly train for generative modeling, we achieve
a competitive Inception Score (IS) and Fr\'echet inception distance (FID)
compared to hybrid discriminative-generative models.

</details>


### [144] [On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling](https://arxiv.org/abs/2505.22491)
*Moritz Haas,Sebastian Bordt,Ulrike von Luxburg,Leena Chennuru Vankadara*

Main category: cs.LG

TL;DR: The paper explores why standard parameterization (SP) works well in practice despite theoretical predictions of instability. It finds that the discrepancy between theory and practice can be resolved by considering the loss function, specifically cross-entropy (CE) loss, which allows for a 'controlled divergence' regime where training remains stable even with large learning rates.


<details>
  <summary>Details</summary>
Motivation: There is a gap between the practical success of standard parameterization (He initialization and a single global learning rate) and its theoretical understanding. Existing infinite-width theory predicts instability under large learning rates and vanishing feature learning under stable learning rates, yet empirically optimal learning rates decay much slower than predicted.

Method: The authors study neural network training dynamics under different loss functions (cross-entropy vs mean squared error). They prove theoretically and validate empirically that under cross-entropy loss, an intermediate 'controlled divergence' regime emerges where logits diverge but loss, gradients, and activations remain stable. This enables persistent feature evolution in all hidden layers.

Result: Neural networks operate in a controlled divergence regime under cross-entropy loss across different optimizers (SGD, Adam), architectures (MLPs, GPT), and data modalities (vision, language). This explains the practical success of standard parameterization. Width-scaling considerations are useful for predicting optimal learning rate exponents.

Conclusion: The analysis clarifies the effectiveness and limitations of layerwise learning rate scalings for standard initialization. The findings highlight the importance of considering the loss function in understanding training dynamics and optimizing learning rates.

Abstract: The dominant paradigm for training large-scale vision and language models is
He initialization and a single global learning rate (\textit{standard
parameterization}, SP). Despite its practical success, standard parametrization
remains poorly understood from a theoretical perspective: Existing
infinite-width theory would predict instability under large learning rates and
vanishing feature learning under stable learning rates. However, empirically
optimal learning rates consistently decay much slower than theoretically
predicted. By carefully studying neural network training dynamics, we
demonstrate that this discrepancy is not fully explained by finite-width
phenomena such as catapult effects or a lack of alignment between weights and
incoming activations. We instead show that the apparent contradiction can be
fundamentally resolved by taking the loss function into account: In contrast to
Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an
intermediate \textit{controlled divergence} regime emerges, where logits
diverge but loss, gradients, and activations remain stable. Stable training
under large learning rates enables persistent feature evolution at scale in all
hidden layers, which is crucial for the practical success of SP. In experiments
across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities
(vision, language), we validate that neural networks operate in this controlled
divergence regime under CE loss but not under MSE loss. Our empirical evidence
suggests that width-scaling considerations are surprisingly useful for
predicting empirically optimal learning rate exponents. Finally, our analysis
clarifies the effectiveness and limitations of recently proposed layerwise
learning rate scalings for standard initialization.

</details>


### [145] [Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation](https://arxiv.org/abs/2505.22492)
*Hongyi Zhou,Josiah P. Hanna,Jin Zhu,Ying Yang,Chengchun Shi*

Main category: cs.LG

TL;DR: 在离线策略评估(OPE)中，历史依赖的行为策略估计虽然增加了有限样本的偏差，但降低了渐近方差，从而使均方误差(MSE)降低。此研究通过偏差-方差分解解释了这一现象，并扩展到了其他OPE估计器。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，即使真实的行为策略是马尔可夫的，使用历史依赖的行为策略估计可以降低均方误差（MSE），但为何使用历史能降低MSE尚不清楚。

Method: 理论上推导了普通重要性采样（IS）估计器的MSE的偏差-方差分解，证明了历史依赖的行为策略估计会降低其渐近方差，同时增加有限样本的偏差。随着估计的行为策略对更长历史的条件作用，展示了方差的一致减少。并将这些发现扩展到一系列其他OPE估计器，包括顺序IS估计器、双重稳健估计器和边缘化IS估计器，行为策略可以参数化或非参数化估计。

Result: 揭示了历史依赖的行为策略估计在降低MSE中的作用机制：尽管有限样本的偏差有所增加，但渐近方差显著降低。并且随着条件于更长的历史，方差持续减少。

Conclusion: 本研究理论解释了在强化学习的离线策略评估中，历史依赖的行为策略估计能够降低MSE的原因，为未来在实际应用中优化OPE提供了理论支持。

Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with
a focus on behavior policy estimation for importance sampling. Prior work has
shown empirically that estimating a history-dependent behavior policy can lead
to lower mean squared error (MSE) even when the true behavior policy is
Markovian. However, the question of why the use of history should lower MSE
remains open. In this paper, we theoretically demystify this paradox by
deriving a bias-variance decomposition of the MSE of ordinary importance
sampling (IS) estimators, demonstrating that history-dependent behavior policy
estimation decreases their asymptotic variances while increasing their
finite-sample biases. Additionally, as the estimated behavior policy conditions
on a longer history, we show a consistent decrease in variance. We extend these
findings to a range of other OPE estimators, including the sequential IS
estimator, the doubly robust estimator and the marginalized IS estimator, with
the behavior policy estimated either parametrically or non-parametrically.

</details>


### [146] [ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods](https://arxiv.org/abs/2505.22494)
*Michal Kmicikiewicz,Vincent Fortuin,Ewa Szczurek*

Main category: cs.LG

TL;DR: ProSpero is an active learning framework that combines a frozen pre-trained generative model with a surrogate updated from oracle feedback to design protein sequences of high fitness and novelty.


<details>
  <summary>Details</summary>
Motivation: Designing protein sequences with both high fitness and novelty is challenging due to the risk of generating biologically implausible sequences or relying on surrogate models that lose fidelity in novel regions.

Method: ProSpero uses a frozen pre-trained generative model guided by a surrogate updated from oracle feedback. It integrates fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling to explore beyond wild-type neighborhoods while preserving biological plausibility.

Result: ProSpero remains effective even when the surrogate is misspecified and consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.

Conclusion: ProSpero provides a robust approach for designing protein sequences with high fitness and novelty, overcoming limitations of previous methods.

Abstract: Designing protein sequences of both high fitness and novelty is a challenging
task in data-efficient protein engineering. Exploration beyond wild-type
neighborhoods often leads to biologically implausible sequences or relies on
surrogate models that lose fidelity in novel regions. Here, we propose
ProSpero, an active learning framework in which a frozen pre-trained generative
model is guided by a surrogate updated from oracle feedback. By integrating
fitness-relevant residue selection with biologically-constrained Sequential
Monte Carlo sampling, our approach enables exploration beyond wild-type
neighborhoods while preserving biological plausibility. We show that our
framework remains effective even when the surrogate is misspecified. ProSpero
consistently outperforms or matches existing methods across diverse protein
engineering tasks, retrieving sequences of both high fitness and novelty.

</details>


### [147] [Geometric GNNs for Charged Particle Tracking at GlueX](https://arxiv.org/abs/2505.22504)
*Ahmed Hossam Mohammed,Kishansingh Rajput,Simon Taylor,Denis Furletov,Sergey Furletov,Malachi Schram*

Main category: cs.LG

TL;DR: The paper explores the application of Graph Neural Networks (GNNs) for track finding in nuclear physics experiments, demonstrating their superior performance and efficiency compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the tracking of charged particles resulting from high-energy collisions, which is critical for reconstructing particle trajectories and determining interactions. Traditional combinatorial approaches scale poorly as the number of hits grows.

Method: Evaluate GNN models for track finding using data from the GlueX experiment. Train on simulation data and test on both simulation and real measurements. Compare GNN-based track finding with traditional methods in terms of efficiency, purity, and inference speed. Also compare GNN implementation on GPU and FPGA.

Result: GNN-based track finding outperforms traditional methods in segment-based efficiency at a fixed purity while providing faster inferences. Significant speedup can be achieved by processing multiple events in batches on GPUs.

Conclusion: GNNs are an intuitive and effective choice for track finding in nuclear physics experiments, offering improved performance and efficiency.

Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building
blocks of matter. The experiments involve high-energy collisions that produce
complex events with many particle trajectories. Tracking charged particles
resulting from collisions in the presence of a strong magnetic field is
critical to enable the reconstruction of particle trajectories and precise
determination of interactions. It is traditionally achieved through
combinatorial approaches that scale worse than linearly as the number of hits
grows. Since particle hit data naturally form a 3-dimensional point cloud and
can be structured as graphs, Graph Neural Networks (GNNs) emerge as an
intuitive and effective choice for this task. In this study, we evaluate the
GNN model for track finding on the data from the GlueX experiment at Jefferson
Lab. We use simulation data to train the model and test on both simulation and
real GlueX measurements. We demonstrate that GNN-based track finding
outperforms the currently used traditional method at GlueX in terms of
segment-based efficiency at a fixed purity while providing faster inferences.
We show that the GNN model can achieve significant speedup by processing
multiple events in batches, which exploits the parallel computation capability
of Graphical Processing Units (GPUs). Finally, we compare the GNN
implementation on GPU and FPGA and describe the trade-off.

</details>


### [148] [Sparsification and Reconstruction from the Perspective of Representation Geometry](https://arxiv.org/abs/2505.22506)
*Wenjie Sun,Bingzhe Wu,Zhile Yang,Chengke Wu*

Main category: cs.LG

TL;DR: Sparse Autoencoders (SAEs) are key in mechanistic interpretability. This paper introduces SAEMA to explore how sparse encoding organizes representations, defines local and global representations, proves a causal relationship between separability and reconstruction performance, and emphasizes the importance of understanding representations.


<details>
  <summary>Details</summary>
Motivation: To understand how sparse encoding organizes representations from language models and its relationship with feature disentanglement and reconstruction performance.

Method: Propose SAEMA which observes variability of rank of SSPD matrix unfolded along latent tensor with noise added to residual stream. Defines local and global representations that amplify inter-feature distinctions.

Result: Proves significant causal relationship between separability of global representation and reconstruction performance.

Conclusion: Explains principles of sparsity from representational geometry perspective, highlights necessity of understanding representations, and provides empirical references for developing new interpretable tools.

Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic
interpretability, aiming to identify interpretable monosemantic features.
However, how does sparse encoding organize the representations of activation
vector from language models? What is the relationship between this
organizational paradigm and feature disentanglement as well as reconstruction
performance? To address these questions, we propose the SAEMA, which validates
the stratified structure of the representation by observing the variability of
the rank of the symmetric semipositive definite (SSPD) matrix corresponding to
the modal tensor unfolded along the latent tensor with the level of noise added
to the residual stream. To systematically investigate how sparse encoding
alters representational structures, we define local and global representations,
demonstrating that they amplify inter-feature distinctions by merging similar
semantic features and introducing additional dimensionality. Furthermore, we
intervene the global representation from an optimization perspective, proving a
significant causal relationship between their separability and the
reconstruction performance. This study explains the principles of sparsity from
the perspective of representational geometry and demonstrates the impact of
changes in representational structure on reconstruction performance.
Particularly emphasizes the necessity of understanding representations and
incorporating representational constraints, providing empirical references for
developing new interpretable tools and improving SAEs. The code is available at
\hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.

</details>


### [149] [Accelerating Optimization via Differentiable Stopping Time](https://arxiv.org/abs/2505.22509)
*Zhonglin Xie,Yiman Fong,Haoran Yuan,Zaiwen Wen*

Main category: cs.LG

TL;DR: This paper proposes a differentiable stopping time to optimize the time to reach a target loss, overcoming previous limitations of non-differentiability. The authors design an efficient algorithm for backpropagation and demonstrate superior performance in various experiments.


<details>
  <summary>Details</summary>
Motivation: Optimization algorithms are crucial in modern machine learning applications. While minimizing loss at a given time is a common formulation, minimizing the time to reach a target loss has been challenging due to its perceived non-differentiability. This limitation restricts optimization to conceptual frameworks or zeroth-order methods.

Method: The authors propose a differentiable stopping time, theoretically justified by differential equations, which allows for an efficient algorithm to backpropagate through it. This new formulation accelerates algorithms and can be applied to tasks like online hyperparameter tuning and learning to optimize.

Result: Comprehensive experiments across various problems show that the proposed methods outperform existing techniques, confirming their effectiveness.

Conclusion: The introduction of differentiable stopping time provides a novel approach to accelerate optimization algorithms. Its successful application in diverse scenarios highlights its potential impact on machine learning optimization.

Abstract: Optimization is an important module of modern machine learning applications.
Tremendous efforts have been made to accelerate optimization algorithms. A
common formulation is achieving a lower loss at a given time. This enables a
differentiable framework with respect to the algorithm hyperparameters. In
contrast, its dual, minimizing the time to reach a target loss, is believed to
be non-differentiable, as the time is not differentiable. As a result, it
usually serves as a conceptual framework or is optimized using zeroth-order
methods. To address this limitation, we propose a differentiable stopping time
and theoretically justify it based on differential equations. An efficient
algorithm is designed to backpropagate through it. As a result, the proposed
differentiable stopping time enables a new differentiable formulation for
accelerating algorithms. We further discuss its applications, such as online
hyperparameter tuning and learning to optimize. Our proposed methods show
superior performance in comprehensive experiments across various problems,
which confirms their effectiveness.

</details>


### [150] [Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data](https://arxiv.org/abs/2505.22521)
*Chao Wang,Chuanhao Nie,Yunbo Liu*

Main category: cs.LG

TL;DR: 本文研究了四个监督学习模型在大规模、高度不平衡的在线交易数据集上的表现，强调了根据具体风险承受能力和运营需求选择模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 欺诈检测在金融和电子商务等高风险领域至关重要，未检测到的欺诈性交易可能导致重大经济损失。因此，需要系统地比较不同模型在欺诈检测任务中的表现。

Method: 研究使用了一个大规模且高度不平衡的在线交易数据集，评估了逻辑回归、随机森林、LightGBM和GRU四种监督学习模型的性能。评估指标包括加权平均值以及每类的精确度、召回率和F1分数。

Result: 随机森林和LightGBM在整体和类别特定指标上表现出色；逻辑回归提供了可靠且可解释的基线；GRU模型对少数类别的欺诈行为显示出较高的召回率，但牺牲了精确度。

Conclusion: 选择欺诈检测系统中的模型时，应根据其特定的风险承受能力和运营需求进行决策。

Abstract: Fraud detection remains a critical task in high-stakes domains such as
finance and e-commerce, where undetected fraudulent transactions can lead to
significant economic losses. In this study, we systematically compare the
performance of four supervised learning models - Logistic Regression, Random
Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit
(GRU) network - on a large-scale, highly imbalanced online transaction dataset.
While ensemble methods such as Random Forest and LightGBM demonstrated superior
performance in both overall and class-specific metrics, Logistic Regression
offered a reliable and interpretable baseline. The GRU model showed strong
recall for the minority fraud class, though at the cost of precision,
highlighting a trade-off relevant for real-world deployment. Our evaluation
emphasizes not only weighted averages but also per-class precision, recall, and
F1-scores, providing a nuanced view of each model's effectiveness in detecting
rare but consequential fraudulent activity. The findings underscore the
importance of choosing models based on the specific risk tolerance and
operational needs of fraud detection systems.

</details>


### [151] [Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo](https://arxiv.org/abs/2505.22524)
*Chinmay Pani,Zijing Ou,Yingzhen Li*

Main category: cs.LG

TL;DR: 提出了一种基于SMC的无需训练的方法，结合Gumbel-Softmax松弛技术，在离散扩散模型中实现与奖励对齐的目标分布采样。


<details>
  <summary>Details</summary>
Motivation: 现有的离散扩散模型虽然在多个领域表现优异，但在实际应用中需要生成过程满足特定约束条件而不进行任务特定的微调。

Method: 提出一种无需训练的方法，基于顺序蒙特卡洛（SMC），使用扭曲SMC和局部最优提议来从奖励对齐的目标分布中采样，同时通过Gumbel-Softmax放松技术解决离散空间中梯度不明确的问题。

Result: 该方法在合成数据集和图像建模上的实证结果验证了其有效性。

Conclusion: 所提出的方法可以在不进行任务特定微调的情况下，有效地满足实际应用中的约束条件。

Abstract: Discrete diffusion models have become highly effective across various
domains. However, real-world applications often require the generative process
to adhere to certain constraints but without task-specific fine-tuning. To this
end, we propose a training-free method based on Sequential Monte Carlo (SMC) to
sample from the reward-aligned target distribution at the test time. Our
approach leverages twisted SMC with an approximate locally optimal proposal,
obtained via a first-order Taylor expansion of the reward function. To address
the challenge of ill-defined gradients in discrete spaces, we incorporate a
Gumbel-Softmax relaxation, enabling efficient gradient-based approximation
within the discrete generative framework. Empirical results on both synthetic
datasets and image modelling validate the effectiveness of our approach.

</details>


### [152] [TabularQGAN: A Quantum Generative Model for Tabular Data](https://arxiv.org/abs/2505.22533)
*Pallavi Bhardwaj,Caitlin Jones,Lasse Dierich,Aleksandar Vučković*

Main category: cs.LG

TL;DR: This paper presents a new quantum generative model for synthesizing tabular data, which outperforms classical models by 8.5% in similarity score while using only 0.072% of their parameters.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of synthesizing high-quality tabular data when real-world data is scarce or private.

Method: A quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz is proposed to effectively model tabular data.

Result: The quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models.

Conclusion: This is one of the first successful demonstrations of a quantum generative model for handling tabular data, suggesting that this task could be well-suited to quantum computers.

Abstract: In this paper, we introduce a novel quantum generative model for synthesizing
tabular data. Synthetic data is valuable in scenarios where real-world data is
scarce or private, it can be used to augment or replace existing datasets.
Real-world enterprise data is predominantly tabular and heterogeneous, often
comprising a mixture of categorical and numerical features, making it highly
relevant across various industries such as healthcare, finance, and software.
We propose a quantum generative adversarial network architecture with flexible
data encoding and a novel quantum circuit ansatz to effectively model tabular
data. The proposed approach is tested on the MIMIC III healthcare and Adult
Census datasets, with extensive benchmarking against leading classical models,
CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model
outperforms classical models by an average of 8.5% with respect to an overall
similarity score from SDMetrics, while using only 0.072% of the parameters of
the classical models. Additionally, we evaluate the generalization capabilities
of the models using two custom-designed metrics that demonstrate the ability of
the proposed quantum model to generate useful and novel samples. To our
knowledge, this is one of the first demonstrations of a successful quantum
generative model for handling tabular data, indicating that this task could be
well-suited to quantum computers.

</details>


### [153] [Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks](https://arxiv.org/abs/2505.22538)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: The paper proposes a framework for uncertainty quantification using measures derived from proper scoring rules, demonstrating its advantages in selective prediction, out-of-distribution detection, and active learning.


<details>
  <summary>Details</summary>
Motivation: To address the problem of uncertainty quantification and create a flexible framework that can be tailored to different use cases by leveraging the decomposition of proper scoring rules.

Method: Propose measures of total, aleatoric, and epistemic uncertainty based on the decomposition of proper scoring rules into divergence and entropy components, allowing flexibility through different losses (scoring rules).

Result: In selective prediction, the scoring rule should ideally match the task loss. For out-of-distribution detection, mutual information performs best. In active learning, the measure based on zero-one-loss outperforms other uncertainty measures.

Conclusion: The proposed framework offers flexibility and effectiveness in various tasks, such as selective prediction, out-of-distribution detection, and active learning.

Abstract: We address the problem of uncertainty quantification and propose measures of
total, aleatoric, and epistemic uncertainty based on a known decomposition of
(strictly) proper scoring rules, a specific type of loss function, into a
divergence and an entropy component. This leads to a flexible framework for
uncertainty quantification that can be instantiated with different losses
(scoring rules), which makes it possible to tailor uncertainty quantification
to the use case at hand. We show that this flexibility is indeed advantageous.
In particular, we analyze the task of selective prediction and show that the
scoring rule should ideally match the task loss. In addition, we perform
experiments on two other common tasks. For out-of-distribution detection, our
results confirm that a widely used measure of epistemic uncertainty, mutual
information, performs best. Moreover, in the setting of active learning, our
measure of epistemic uncertainty based on the zero-one-loss consistently
outperforms other uncertainty measures.

</details>


### [154] [A Human-Centric Approach to Explainable AI for Personalized Education](https://arxiv.org/abs/2505.22541)
*Vinitra Swamy*

Main category: cs.LG

TL;DR: 这篇论文探讨了可解释性人工智能（XAI）在教育领域的应用，提出了四个技术贡献以提高模型的可解释性和信任度，为以人为本的AI系统奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 尽管深度神经网络在许多领域表现出色，但在个性化学习和教学中的实际应用仍然有限，主要原因是模型决策缺乏可解释性，导致学生、家长和教师对其缺乏信任。

Method: 论文提出了四种新的技术贡献：1) 多模态模块化架构（MultiModN）；2) 可解释的专家混合模型（InterpretCC）；3) 对抗训练以提高解释器稳定性；4) 理论驱动的LLM-XAI框架（iLLuMinaTE），用于向学生提供解释。这些方法通过结合现有解释器的经验评估、新颖的架构设计和人类研究来提升AI系统的透明度和信任度。

Result: 研究表明，后验解释器与教育场景的实际需求之间存在系统性分歧，强调了对固有可解释模型架构的需求。提出的四种技术贡献在不同的设置中进行了评估，证明了其在提升模型可解释性和用户信任方面的有效性。

Conclusion: 本研究为以人为本的AI系统奠定了基础，这些系统能够在保持最先进的性能的同时，内置透明性和信任机制，从而推动个性化学习和教学的发展。

Abstract: Deep neural networks form the backbone of artificial intelligence research,
with potential to transform the human experience in areas ranging from
autonomous driving to personal assistants, healthcare to education. However,
their integration into the daily routines of real-world classrooms remains
limited. It is not yet common for a teacher to assign students individualized
homework targeting their specific weaknesses, provide students with instant
feedback, or simulate student responses to a new exam question. While these
models excel in predictive performance, this lack of adoption can be attributed
to a significant weakness: the lack of explainability of model decisions,
leading to a lack of trust from students, parents, and teachers. This thesis
aims to bring human needs to the forefront of eXplainable AI (XAI) research,
grounded in the concrete use case of personalized learning and teaching. We
frame the contributions along two verticals: technical advances in XAI and
their aligned human studies. We investigate explainability in AI for education,
revealing systematic disagreements between post-hoc explainers and identifying
a need for inherently interpretable model architectures. We propose four novel
technical contributions in interpretability with a multimodal modular
architecture (MultiModN), an interpretable mixture-of-experts model
(InterpretCC), adversarial training for explainer stability, and a
theory-driven LLM-XAI framework to present explanations to students
(iLLuMinaTE), which we evaluate in diverse settings with professors, teachers,
learning scientists, and university students. By combining empirical
evaluations of existing explainers with novel architectural designs and human
studies, our work lays a foundation for human-centric AI systems that balance
state-of-the-art performance with built-in transparency and trust.

</details>


### [155] [DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models](https://arxiv.org/abs/2505.22549)
*Alex Iacob,Lorenzo Sani,Mher Safaryan,Paris Giampouras,Samuel Horváth,Andrej Jovanovic,Meghdad Kurmanji,Preslav Aleksandrov,William F. Shen,Xinchi Qiu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: DES-LOC is a family of optimizers that can reduce communication costs while preserving convergence, making it suitable for practical training scenarios prone to system failures.


<details>
  <summary>Details</summary>
Motivation: Existing infrequent communication methods like Local SGD were designed to synchronize only model parameters and cannot be trivially applied to adaptive optimizers due to additional optimizer states. Current approaches extending Local SGD either lack convergence guarantees or require synchronizing all optimizer states, tripling communication costs.

Method: DES-LOC assigns independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence.

Result: Through extensive experiments on language models of up to 1.7B, DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local ADAM.

Conclusion: DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for foundation model training.

Abstract: Scaling foundation model training with Distributed Data Parallel (DDP)
methods is bandwidth-limited. Existing infrequent communication methods like
Local SGD were designed to synchronize only model parameters and cannot be
trivially applied to adaptive optimizers due to additional optimizer states.
Current approaches extending Local SGD either lack convergence guarantees or
require synchronizing all optimizer states, tripling communication costs. We
propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of
optimizers assigning independent synchronization periods to parameters and
momenta, enabling lower communication costs while preserving convergence.
Through extensive experiments on language models of up to 1.7B, we show that
DES-LOC can communicate 170x less than DDP and 2x less than the previous
state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches,
DES-LOC is suited for practical training scenarios prone to system failures.
DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for
foundation model training.

</details>


### [156] [Geometric Hyena Networks for Large-scale Equivariant Learning](https://arxiv.org/abs/2505.22560)
*Artem Moskalev,Mangal Prakash,Junjie Xu,Tianyu Cui,Rui Liao,Tommaso Mansi*

Main category: cs.LG

TL;DR: Geometric Hyena is the first equivariant long-convolutional model for geometric systems, capturing global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. It outperforms existing models in all-atom property prediction of large RNA molecules and full protein molecular dynamics with significantly less memory and compute.


<details>
  <summary>Details</summary>
Motivation: Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems, but standard methods suffer from quadratic complexity or sacrifice global information.

Method: Introduced Geometric Hyena, an equivariant long-convolutional model that captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations.

Result: Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute.

Conclusion: Geometric Hyena processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.

Abstract: Processing global geometric context while preserving equivariance is crucial
when modeling biological, chemical, and physical systems. Yet, this is
challenging due to the computational demands of equivariance and global context
at scale. Standard methods such as equivariant self-attention suffer from
quadratic complexity, while local methods such as distance-based message
passing sacrifice global information. Inspired by the recent success of
state-space and long-convolutional models, we introduce Geometric Hyena, the
first equivariant long-convolutional model for geometric systems. Geometric
Hyena captures global geometric context at sub-quadratic complexity while
maintaining equivariance to rotations and translations. Evaluated on all-atom
property prediction of large RNA molecules and full protein molecular dynamics,
Geometric Hyena outperforms existing equivariant models while requiring
significantly less memory and compute that equivariant self-attention. Notably,
our model processes the geometric context of 30k tokens 20x faster than the
equivariant transformer and allows 72x longer context within the same budget.

</details>


### [157] [FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators](https://arxiv.org/abs/2505.22573)
*Guy Moss,Leah Sophie Muhle,Reinhard Drews,Jakob H. Macke,Cornelius Schröder*

Main category: cs.LG

TL;DR: The paper presents FNOPE, a method for efficient posterior estimation using Fourier Neural Operator architecture with flow matching objective to perform inference of function-valued parameters.


<details>
  <summary>Details</summary>
Motivation: Simulation-based inference (SBI) is effective for low-dimensional parametric models but struggles with function-valued parameters common in spatiotemporal processes.

Method: FNOPE employs a Fourier Neural Operator (FNO) architecture with a flow matching objective for efficient posterior estimation and supports arbitrary discretizations as well as vector-valued parameter estimation simultaneously.

Result: FNOPE can perform inference of function-valued parameters at a fraction of the simulation budget compared to state-of-the-art methods and demonstrates effectiveness on benchmark tasks and a glaciology spatial inference task.

Conclusion: FNOPE extends the applicability of SBI methods to new scientific domains by enabling the inference of function-valued parameters.

Abstract: Simulation-based inference (SBI) is an established approach for performing
Bayesian inference on scientific simulators. SBI so far works best on
low-dimensional parametric models. However, it is difficult to infer
function-valued parameters, which frequently occur in disciplines that model
spatiotemporal processes such as the climate and earth sciences. Here, we
introduce an approach for efficient posterior estimation, using a Fourier
Neural Operator (FNO) architecture with a flow matching objective. We show that
our approach, FNOPE, can perform inference of function-valued parameters at a
fraction of the simulation budget of state of the art methods. In addition,
FNOPE supports posterior evaluation at arbitrary discretizations of the domain,
as well as simultaneous estimation of vector-valued parameters. We demonstrate
the effectiveness of our approach on several benchmark tasks and a challenging
spatial inference task from glaciology. FNOPE extends the applicability of SBI
methods to new scientific domains by enabling the inference of function-valued
parameters.

</details>


### [158] [Benignity of loss landscape with weight decay requires both large overparametrization and initialization](https://arxiv.org/abs/2505.22578)
*Etienne Boursier,Matthew Bowditch,Matthias Englert,Ranko Lazic*

Main category: cs.LG

TL;DR: 这篇论文研究了带有权重衰减的两层ReLU神经网络的损失景观，并发现当网络宽度满足m ≳ min(n^d, 2^n)时，损失景观变得良性（无虚假局部最小值）。这种过度参数化程度是必要且充分的。此外，结果表明大初始化有助于优化，而小初始化可能导致收敛到虚假局部最小值。


<details>
  <summary>Details</summary>
Motivation: 尽管权重衰减在现代训练中很常见，但大多数理论分析集中在未正则化的设置上。本研究旨在从理论上理解权重衰减对神经网络优化的影响。

Method: 作者研究了带有l2正则化的两层ReLU网络的训练损失景观。他们通过分析网络宽度与数据点和输入维度的关系，确定了使损失景观良性的过度参数化条件。同时，还探讨了不同初始化规模对优化过程的影响。

Result: 1. 当网络宽度m ≥ min(n^d, 2^n)时，损失景观变得良性，几乎所有的常数激活区域包含全局最小值且没有虚假局部最小值。
2. 此种过度参数化程度既是充分也是必要的。
3. 大初始化有助于优化过程，而小初始化可能导致收敛到虚假局部最小值。

Conclusion: 权重衰减和过度参数化可以改善神经网络的损失景观，使其更易于优化。然而，这种改善在小初始化情况下可能不成立，这为未来的优化策略提供了理论指导。

Abstract: The optimization of neural networks under weight decay remains poorly
understood from a theoretical standpoint. While weight decay is standard
practice in modern training procedures, most theoretical analyses focus on
unregularized settings. In this work, we investigate the loss landscape of the
$\ell_2$-regularized training loss for two-layer ReLU networks. We show that
the landscape becomes benign -- i.e., free of spurious local minima -- under
large overparametrization, specifically when the network width $m$ satisfies $m
\gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the
input dimension. More precisely in this regime, almost all constant activation
regions contain a global minimum and no spurious local minima. We further show
that this level of overparametrization is not only sufficient but also
necessary via the example of orthogonal data. Finally, we demonstrate that such
loss landscape results primarily hold relevance in the large initialization
regime. In contrast, for small initializations -- corresponding to the feature
learning regime -- optimization can still converge to spurious local minima,
despite the global benignity of the landscape.

</details>


### [159] [Machine Unlearning under Overparameterization](https://arxiv.org/abs/2505.22601)
*Jacob L. Block,Aryan Mokhtari,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: In the overparameterized setting, traditional unlearning methods fail due to vanishing loss gradients. This paper redefines unlearning as finding minimum-complexity interpolators on retained data and proposes a new algorithmic framework that only needs model gradients on this data.


<details>
  <summary>Details</summary>
Motivation: Unlearning algorithms are important for privacy protection and data removal. However, in the overparameterized setting where many models can interpolate the data, simply minimizing loss on retained data is inadequate because original models may already satisfy this condition. Moreover, loss gradients vanish in this regime, rendering prior gradient-based methods ineffective.

Method: The authors redefine unlearning as finding the minimum-complexity interpolator over the retained data. They propose an algorithmic framework that minimizes a regularized objective over perturbations constrained to be orthogonal to model gradients on the retained set at the original solution. This is a first-order relaxation of the interpolation condition.

Result: For different model classes, the authors provide both exact and approximate unlearning guarantees. Their implementation of the proposed framework outperforms existing baselines across various unlearning experiments.

Conclusion: This work addresses the inadequacy of previous unlearning definitions and methods in the overparameterized setting. By redefining unlearning and proposing a new effective algorithmic framework, it advances the field of machine unlearning, particularly for complex models.

Abstract: Machine unlearning algorithms aim to remove the influence of specific
training samples, ideally recovering the model that would have resulted from
training on the remaining data alone. We study unlearning in the
overparameterized setting, where many models interpolate the data, and defining
the unlearning solution as any loss minimizer over the retained
set$\unicode{x2013}$as in prior work in the underparameterized
setting$\unicode{x2013}$is inadequate, since the original model may already
interpolate the retained data and satisfy this condition. In this regime, loss
gradients vanish, rendering prior methods based on gradient perturbations
ineffective, motivating both new unlearning definitions and algorithms. For
this setting, we define the unlearning solution as the minimum-complexity
interpolator over the retained data and propose a new algorithmic framework
that only requires access to model gradients on the retained set at the
original solution. We minimize a regularized objective over perturbations
constrained to be orthogonal to these model gradients, a first-order relaxation
of the interpolation condition. For different model classes, we provide exact
and approximate unlearning guarantees, and we demonstrate that an
implementation of our framework outperforms existing baselines across various
unlearning experiments.

</details>


### [160] [One Rank at a Time: Cascading Error Dynamics in Sequential Learning](https://arxiv.org/abs/2505.22602)
*Mahtab Alizadeh Vandchali,Fangshuo,Liao,Anastasios Kyrillidis*

Main category: cs.LG

TL;DR: 本论文通过低秩线性回归的视角研究了序列学习中的误差传播问题，并提出了一个分析框架，将学习过程分解为一系列秩1估计问题。研究表明，误差以可预测的方式累积，这对算法设计和稳定性保证有重要影响。


<details>
  <summary>Details</summary>
Motivation: 序列学习作为一种将复杂任务分解为简单、分层组件的范式在AI领域中变得越来越重要。本文旨在探讨序列学习过程中误差传播的特性，特别是当顺序学习秩1子空间时。

Method: 作者提出了一种分析框架，将学习过程分解为一系列秩1估计问题，其中每个后续估计都依赖于前一步骤的准确性。通过这一框架，研究了由于计算预算有限和精度有限等原因导致的误差如何影响整体模型的准确性。

Result: 研究结果表明，在序列学习过程中，误差以可预测的方式累积。这为理解序列学习中误差传播提供了理论基础，并对算法设计和稳定性保障提供了新的见解。

Conclusion: 本文通过低秩线性回归的视角，分析了序列学习中的误差传播问题，证明了误差在序列学习过程中的可预测累积特性，为未来算法设计和稳定性保证提供了理论依据。

Abstract: Sequential learning -- where complex tasks are broken down into simpler,
hierarchical components -- has emerged as a paradigm in AI. This paper views
sequential learning through the lens of low-rank linear regression, focusing
specifically on how errors propagate when learning rank-1 subspaces
sequentially. We present an analysis framework that decomposes the learning
process into a series of rank-1 estimation problems, where each subsequent
estimation depends on the accuracy of previous steps. Our contribution is a
characterization of the error propagation in this sequential process,
establishing bounds on how errors -- e.g., due to limited computational budgets
and finite precision -- affect the overall model accuracy. We prove that these
errors compound in predictable ways, with implications for both algorithmic
design and stability guarantees.

</details>


### [161] [The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models](https://arxiv.org/abs/2505.22617)
*Ganqu Cui,Yuchen Zhang,Jiacheng Chen,Lifan Yuan,Zhi Wang,Yuxin Zuo,Haozhan Li,Yuchen Fan,Huayu Chen,Weize Chen,Zhiyuan Liu,Hao Peng,Lei Bai,Wanli Ouyang,Yu Cheng,Bowen Zhou,Ning Ding*

Main category: cs.LG

TL;DR: The paper addresses the issue of policy entropy collapse in RL for reasoning with LLMs, proposes methods (Clip-Cov and KL-Cov) to control entropy by restricting high-covariance token updates, and demonstrates their effectiveness in encouraging exploration and improving performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the obstacle of policy entropy collapse in RL when applied to reasoning with LLMs, which leads to diminished exploratory ability and saturation of policy performance.

Method: The authors establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. They investigate entropy dynamics theoretically and empirically, finding that changes in policy entropy are driven by the covariance between action probability and the change in logits. Based on this understanding, they propose two techniques, Clip-Cov and KL-Cov, to control entropy by restricting updates of high-covariance tokens.

Result: Experiments show that the proposed methods encourage exploration, help policies escape entropy collapse, and achieve better downstream performance.

Conclusion: Entropy management is necessary for continuous exploration in scaling compute for RL. The proposed methods effectively control entropy and improve policy performance.

Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with
LLMs, namely the collapse of policy entropy. Such phenomenon is consistently
observed across vast RL runs without entropy intervention, where the policy
entropy dropped sharply at the early training stage, this diminished
exploratory ability is always accompanied with the saturation of policy
performance. In practice, we establish a transformation equation R=-a*e^H+b
between entropy H and downstream performance R. This empirical law strongly
indicates that, the policy performance is traded from policy entropy, thus
bottlenecked by its exhaustion, and the ceiling is fully predictable H=0,
R=-a+b. Our finding necessitates entropy management for continuous exploration
toward scaling compute for RL. To this end, we investigate entropy dynamics
both theoretically and empirically. Our derivation highlights that, the change
in policy entropy is driven by the covariance between action probability and
the change in logits, which is proportional to its advantage when using Policy
Gradient-like algorithms. Empirical study shows that, the values of covariance
term and entropy differences matched exactly, supporting the theoretical
conclusion. Moreover, the covariance term stays mostly positive throughout
training, further explaining why policy entropy would decrease monotonically.
Through understanding the mechanism behind entropy dynamics, we motivate to
control entropy by restricting the update of high-covariance tokens.
Specifically, we propose two simple yet effective techniques, namely Clip-Cov
and KL-Cov, which clip and apply KL penalty to tokens with high covariances
respectively. Experiments show that these methods encourage exploration, thus
helping policy escape entropy collapse and achieve better downstream
performance.

</details>


### [162] [Understanding (Un)Reliability of Steering Vectors in Language Models](https://arxiv.org/abs/2505.22637)
*Joschka Braun,Carsten Eickhoff,David Krueger,Seyed Ali Bahrainian,Dmitrii Krasheninnikov*

Main category: cs.LG

TL;DR: 尽管转向量在控制语言模型行为方面表现出有希望的性能，但在某些情况下可能是不可靠甚至适得其反。本文研究了提示类型和激活差异的几何形状对转向可靠性的影响。结果表明，所有七种提示类型都产生了净正转向效应，但样本间差异很大，经常产生与预期相反的效果。没有一种提示类型明显优于其他类型。此外，训练集激活差异之间的余弦相似度越高，转向越有效。并且，正负激活分离较好的数据集更容易转向。最终得出结论，当目标行为不能由连贯的方向表示时，向量转向是不可靠的。


<details>
  <summary>Details</summary>
Motivation: 转向量作为控制语言模型行为的一种轻量级方法，虽然展示出有希望的性能，但在某些情况下可能不可靠或适得其反。因此需要研究影响转向可靠性的因素。

Method: 研究了不同提示类型和激活差异的几何形状对转向可靠性的影响，并通过实验分析了各种提示类型的转向效果及激活差异的特征。

Result: 发现所有提示类型都能产生净正转向效应，但效果不稳定且经常与预期相反；提示类型之间无明显优劣；训练集激活差异的余弦相似度与转向效果呈正相关；正负激活分离较好的数据集更容易转向。

Conclusion: 当目标行为无法用连贯方向表示时，向量转向是不可靠的。

Abstract: Steering vectors are a lightweight method to control language model behavior
by adding a learned bias to the activations at inference time. Although
steering demonstrates promising performance, recent work shows that it can be
unreliable or even counterproductive in some cases. This paper studies the
influence of prompt types and the geometry of activation differences on
steering reliability. First, we find that all seven prompt types used in our
experiments produce a net positive steering effect, but exhibit high variance
across samples, and often give an effect opposite of the desired one. No prompt
type clearly outperforms the others, and yet the steering vectors resulting
from the different prompt types often differ directionally (as measured by
cosine similarity). Second, we show that higher cosine similarity between
training set activation differences predicts more effective steering. Finally,
we observe that datasets where positive and negative activations are better
separated are more steerable. Our results suggest that vector steering is
unreliable when the target behavior is not represented by a coherent direction.

</details>


### [163] [Spectral Survival Analysis](https://arxiv.org/abs/2505.22641)
*Chengzhi Shi,Stratis Ioannidis*

Main category: cs.LG

TL;DR: The paper links rank regression with the CoxPH model, adapting spectral methods for survival analysis in large datasets and deep architectures, showing superior performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of scaling Cox Proportional Hazard models to large datasets and deep architectures, especially in high-dimensional settings.

Method: Identify the connection between rank regression and the CoxPH model, then adapt and extend the spectral method from rank regression to survival analysis, making it versatile for several CoxPH variants including deep models.

Result: Empirical verification on multiple real-world high-dimensional datasets shows that the method scales effectively and outperforms legacy methods in both predictive performance and efficiency.

Conclusion: The approach successfully generalizes the CoxPH model to large-scale and deep learning contexts, enhancing both scalability and performance.

Abstract: Survival analysis is widely deployed in a diverse set of fields, including
healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model
is a semi-parametric model often encountered in the literature. Despite its
popularity, wide deployment, and numerous variants, scaling CoxPH to large
datasets and deep architectures poses a challenge, especially in the
high-dimensional regime. We identify a fundamental connection between rank
regression and the CoxPH model: this allows us to adapt and extend the
so-called spectral method for rank regression to survival analysis. Our
approach is versatile, naturally generalizing to several CoxPH variants,
including deep models. We empirically verify our method's scalability on
multiple real-world high-dimensional datasets; our method outperforms legacy
methods w.r.t. predictive performance and efficiency.

</details>


### [164] [On Learning Verifiers for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.22650)
*Maria-Florina Balcan,Avrim Blum,Zhiyuan Li,Dravyansh Sharma*

Main category: cs.LG

TL;DR: The paper explores the development of reliable verifiers for natural language Chain-of-Thought reasoning using a formal PAC-learning framework, providing sample complexity bounds and analysis of various verification goals.


<details>
  <summary>Details</summary>
Motivation: Chain-of-Thought reasoning is powerful but prone to errors through incorrect inferences. Formal mathematical reasoning could address this, but LLMs struggle with fully formal approaches.

Method: The authors propose learning verifiers for natural language Chain-of-Thought reasoning within a PAC-learning framework, analyzing different verification goals and their associated complexities.

Result: They provide upper-bounds on sample complexity for certain verification goals, along with lower-bound and impossibility results for others without additional assumptions.

Conclusion: This work contributes to understanding how to reliably verify Chain-of-Thought reasoning in natural language, advancing towards more robust AI reasoning systems.

Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving
complex mathematical and logical problems. However, it can often veer off track
through incorrect or unsubstantiated inferences. Formal mathematical reasoning,
which can be checked with a formal verifier, is one approach to addressing this
issue. However, currently LLMs are simply not good enough to solve complex
problems in a formal way, and even just formalizing an informal problem
statement can be challenging. Motivated by this fact, in this work we consider
the problem of learning reliable verifiers for natural language
Chain-of-Thought reasoning. That is, given a problem statement and step-by-step
solution in natural language, the aim of the verifier is to output [Yes] if the
reasoning steps in the solution are all valid, and [No] otherwise. In this work
we give a formal PAC-learning framework for studying this problem. We propose
and analyze several natural verification goals, at different levels of
strength, in this framework. We provide sample complexity upper-bounds for
learning verifiers satisfying these goals, as well as lower-bound and
impossibility results for learning other natural verification objectives
without additional assumptions.

</details>


### [165] [Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents](https://arxiv.org/abs/2505.22655)
*Michael Kirchhof,Gjergji Kasneci,Enkelejda Kasneci*

Main category: cs.LG

TL;DR: 大型语言模型（LLMs）和聊天机器人有时会提供错误输出，且无法完全避免。因此，不确定性量化至关重要。本文探讨了传统不确定性分类在交互式LLM环境中的局限性，并提出了三个新的研究方向：未指定不确定性、交互学习和输出不确定性，以期提高LLM代理交互的透明度、可信度和直观性。


<details>
  <summary>Details</summary>
Motivation: 当前对于LLMs和聊天机器人的不确定性量化主要集中在整体数字或分为随机性和认知不确定性两类，但在开放和交互式的LLM环境中，这种传统的二分法过于局限，不能充分描述用户与模型交互时的复杂情况。

Method: 通过文献回顾，发现现有随机性和认知不确定性的定义在交互式LLM场景中存在矛盾并失去意义。基于此，提出三个新的研究方向：1）未指定不确定性，用于处理用户未提供所有信息或任务不明确的情况；2）交互学习，通过提问后续问题减少对当前上下文的不确定性；3）输出不确定性，利用丰富的语言和语音空间表达不确定性而不仅限于数字。

Result: 这三个新方向有望使LLM代理交互更加透明、可信和直观，为未来的不确定性研究提供了指导。

Conclusion: 传统不确定性分类在交互式LLM环境中不再适用，需要探索新的方法来丰富和表达不确定性，从而提升LLM代理与用户的交互质量。

Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong
outputs at times, and it was recently found that this can never be fully
prevented. Hence, uncertainty quantification plays a crucial role, aiming to
quantify the level of ambiguity in either one overall number or two numbers for
aleatoric and epistemic uncertainty. This position paper argues that this
traditional dichotomy of uncertainties is too limited for the open and
interactive setup that LLM agents operate in when communicating with a user,
and that we need to research avenues that enrich uncertainties in this novel
scenario. We review the literature and find that popular definitions of
aleatoric and epistemic uncertainties directly contradict each other and lose
their meaning in interactive LLM agent settings. Hence, we propose three novel
research directions that focus on uncertainties in such human-computer
interactions: Underspecification uncertainties, for when users do not provide
all information or define the exact task at the first go, interactive learning,
to ask follow-up questions and reduce the uncertainty about the current
context, and output uncertainties, to utilize the rich language and speech
space to express uncertainties as more than mere numbers. We expect that these
new ways of dealing with and communicating uncertainties will lead to LLM agent
interactions that are more transparent, trustworthy, and intuitive.

</details>


### [166] [Maximizing Confidence Alone Improves Reasoning](https://arxiv.org/abs/2505.22660)
*Mihir Prabhudesai,Lili Chen,Alex Ippoliti,Katerina Fragkiadaki,Hao Liu,Deepak Pathak*

Main category: cs.LG

TL;DR: The paper introduces RENT, an unsupervised reinforcement learning method using entropy minimization as intrinsic reward to enhance model reasoning without external rewards or ground-truth answers.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of reward engineering in reinforcement learning by developing a method that does not require external rewards or supervision.

Method: RENT uses the model's entropy of its underlying distribution as an intrinsic reward, reinforcing chains of thought that yield high model confidence on generated answers.

Result: Improvements in reasoning ability were demonstrated across various benchmarks (GSM8K, MATH500, AMC, AIME, GPQA) using models from Qwen and Mistral families.

Conclusion: RENT is a general unsupervised learning method applicable in domains with limited or no external supervision.

Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve
significant advances in many fields. Most recently, RL has empowered frontier
language models to solve challenging math, science, and coding problems.
However, central to any RL algorithm is the reward function, and reward
engineering is a notoriously difficult problem in any domain. In this paper, we
propose RENT: Reinforcement Learning via Entropy Minimization -- a fully
unsupervised RL method that requires no external reward or ground-truth
answers, and instead uses the model's entropy of its underlying distribution as
an intrinsic reward. We find that by reinforcing the chains of thought that
yield high model confidence on its generated answers, the model improves its
reasoning ability. In our experiments, we showcase these improvements on an
extensive suite of commonly-used reasoning benchmarks, including GSM8K,
MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and
Mistral families. The generality of our unsupervised learning method lends
itself to applicability in a wide range of domains where external supervision
is limited or unavailable.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [167] [Preventing Adversarial AI Attacks Against Autonomous Situational Awareness: A Maritime Case Study](https://arxiv.org/abs/2505.21609)
*Mathew J. Walter,Aaron Barrett,Kimberly Tam*

Main category: cs.CR

TL;DR: This paper proposes the Data Fusion Cyber Resilience (DFCR) method to enhance resilience against adversarial AI attacks in autonomous systems, particularly maritime vessels. Through real-world demonstrations and quantitative analyses, DFCR significantly reduces losses from various adversarial attacks and improves decision-making even when typical defences are compromised.


<details>
  <summary>Details</summary>
Motivation: Adversarial AI attacks pose significant threats to autonomous transportation systems, such as maritime vessels. Current defences have a limited scope, security metrics are inadequate, and there is a need for resilience beyond model-level defences.

Method: The DFCR method uses multiple inputs and data fusion to create defensive components and introduces an AI security metric. It is evaluated through real-world demonstrations and comprehensive quantitative analyses, comparing it with single-input models and state-of-the-art defences.

Result: DFCR achieves up to a 35% reduction in loss for multi-pronged perturbation attacks, up to 100% reduction in loss for adversarial patch attacks, and up to 100% reduction in loss for spoofing attacks. It also reduces adversarial AI contact confidence and improves system decision-making.

Conclusion: The DFCR method contributes to the development of more secure and resilient AI-driven systems against adversarial attacks.

Abstract: Adversarial artificial intelligence (AI) attacks pose a significant threat to
autonomous transportation, such as maritime vessels, that rely on AI
components. Malicious actors can exploit these systems to deceive and
manipulate AI-driven operations. This paper addresses three critical research
challenges associated with adversarial AI: the limited scope of traditional
defences, inadequate security metrics, and the need to build resilience beyond
model-level defences. To address these challenges, we propose building defences
utilising multiple inputs and data fusion to create defensive components and an
AI security metric as a novel approach toward developing more secure AI
systems. We name this approach the Data Fusion Cyber Resilience (DFCR) method,
and we evaluate it through real-world demonstrations and comprehensive
quantitative analyses, comparing a system built with the DFCR method against
single-input models and models utilising existing state-of-the-art defences.
The findings show that the DFCR approach significantly enhances resilience
against adversarial machine learning attacks in maritime autonomous system
operations, achieving up to a 35\% reduction in loss for successful
multi-pronged perturbation attacks, up to a 100\% reduction in loss for
successful adversarial patch attacks and up to 100\% reduction in loss for
successful spoofing attacks when using these more resilient systems. We
demonstrate how DFCR and DFCR confidence scores can reduce adversarial AI
contact confidence and improve decision-making by the system, even when typical
adversarial defences have been compromised. Ultimately, this work contributes
to the development of more secure and resilient AI-driven systems against
adversarial attacks.

</details>


### [168] [VideoMarkBench: Benchmarking Robustness of Video Watermarking](https://arxiv.org/abs/2505.21620)
*Zhengyuan Jiang,Moyang Guo,Kecen Li,Yuepeng Hu,Yupu Wang,Zhicong Huang,Cheng Hong,Neil Zhenqiang Gong*

Main category: cs.CR

TL;DR: The paper presents VideoMarkBench, a benchmark for evaluating the robustness of video watermarks against various attacks, revealing vulnerabilities in current methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of exploration on the robustness of video watermarking methods against perturbations and attacks.

Method: Introduced VideoMarkBench, a systematic benchmark with a unified dataset from three video generative models across three styles, four watermarking methods, and seven aggregation strategies. Evaluated 12 types of perturbations under different threat models.

Result: Found significant vulnerabilities in existing watermarking approaches.

Conclusion: There is an urgent need for more robust video watermarking solutions.

Abstract: The rapid development of video generative models has led to a surge in highly
realistic synthetic videos, raising ethical concerns related to disinformation
and copyright infringement. Recently, video watermarking has been proposed as a
mitigation strategy by embedding invisible marks into AI-generated videos to
enable subsequent detection. However, the robustness of existing video
watermarking methods against both common and adversarial perturbations remains
underexplored. In this work, we introduce VideoMarkBench, the first systematic
benchmark designed to evaluate the robustness of video watermarks under
watermark removal and watermark forgery attacks. Our study encompasses a
unified dataset generated by three state-of-the-art video generative models,
across three video styles, incorporating four watermarking methods and seven
aggregation strategies used during detection. We comprehensively evaluate 12
types of perturbations under white-box, black-box, and no-box threat models.
Our findings reveal significant vulnerabilities in current watermarking
approaches and highlight the urgent need for more robust solutions. Our code is
available at https://github.com/zhengyuan-jiang/VideoMarkBench.

</details>


### [169] [The Feasibility of Topic-Based Watermarking on Academic Peer Reviews](https://arxiv.org/abs/2505.21636)
*Alexander Nemecek,Yuzhou Jiang,Erman Ayday*

Main category: cs.CR

TL;DR: The paper evaluates topic-based watermarking (TBW) as a method to embed detectable signals into LLM-generated text for use in peer reviews, showing it maintains review quality and is robust to paraphrasing.


<details>
  <summary>Details</summary>
Motivation: To address concerns about confidentiality breaches, hallucinated content, and inconsistent evaluations when using LLMs in peer review, there is a need for reliable attribution mechanisms.

Method: Evaluate topic-based watermarking (TBW) across multiple LLM configurations (base, few-shot, fine-tuned) using authentic peer review data from academic conferences.

Result: TBW maintains review quality relative to non-watermarked outputs and demonstrates strong robustness to paraphrasing-based evasion.

Conclusion: TBW is a minimally intrusive and practical solution for enforcing LLM usage in peer review.

Abstract: Large language models (LLMs) are increasingly integrated into academic
workflows, with many conferences and journals permitting their use for tasks
such as language refinement and literature summarization. However, their use in
peer review remains prohibited due to concerns around confidentiality breaches,
hallucinated content, and inconsistent evaluations. As LLM-generated text
becomes more indistinguishable from human writing, there is a growing need for
reliable attribution mechanisms to preserve the integrity of the review
process. In this work, we evaluate topic-based watermarking (TBW), a
lightweight, semantic-aware technique designed to embed detectable signals into
LLM-generated text. We conduct a comprehensive assessment across multiple LLM
configurations, including base, few-shot, and fine-tuned variants, using
authentic peer review data from academic conferences. Our results show that TBW
maintains review quality relative to non-watermarked outputs, while
demonstrating strong robustness to paraphrasing-based evasion. These findings
highlight the viability of TBW as a minimally intrusive and practical solution
for enforcing LLM usage in peer review.

</details>


### [170] [Reproducible Builds and Insights from an Independent Verifier for Arch Linux](https://arxiv.org/abs/2505.21642)
*Joshua Drexel,Esther Hänggi,Iyán Méndez Veiga*

Main category: cs.CR

TL;DR: 为了应对供应链攻击，本文介绍了可重现构建和引导构建的概念，并分析了过去十年的成果与剩余挑战。通过设置Arch Linux包的重建者和验证者实例，发现Certbot相关包的未被注意到的安全问题，以及fwupd中导致不可重现性的根源并提交补丁。


<details>
  <summary>Details</summary>
Motivation: 供应链攻击成为近年来显著的网络安全威胁，可重现和可引导构建结合独立、全面和定期的源代码审计可以有效减少此类攻击。

Method: 介绍可重现构建和引导构建的概念，分析过去十年的成就，解释剩余挑战，并通过建立rebuilder和verifier实例测试Arch Linux包的可重现性。

Result: 发现了影响16个与Certbot相关的包的未注意到的安全相关包装问题，找出fwupd源代码中不可重现的根本原因并提交上游补丁。

Conclusion: 可重现构建在减少供应链攻击方面具有巨大潜力，但仍存在需要解决的挑战。

Abstract: Supply chain attacks have emerged as a prominent cybersecurity threat in
recent years. Reproducible and bootstrappable builds have the potential to
reduce such attacks significantly. In combination with independent, exhaustive
and periodic source code audits, these measures can effectively eradicate
compromises in the building process. In this paper we introduce both concepts,
we analyze the achievements over the last ten years and explain the remaining
challenges. We contribute to the reproducible builds effort by setting up a
rebuilder and verifier instance to test the reproducibility of Arch Linux
packages. Using the results from this instance, we uncover an unnoticed and
security-relevant packaging issue affecting 16 packages related to Certbot, the
recommended software to install TLS certificates from Let's Encrypt, making
them unreproducible. Additionally, we find the root cause of unreproduciblity
in the source code of fwupd, a critical software used to update device firmware
on Linux devices, and submit an upstream patch to fix it.

</details>


### [171] [A Joint Reconstruction-Triplet Loss Autoencoder Approach Towards Unseen Attack Detection in IoV Networks](https://arxiv.org/abs/2505.21703)
*Julia Boone,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.CR

TL;DR: In this paper, researchers address the security vulnerabilities in Internet of Vehicles (IoV) systems which traditional mechanisms cannot handle effectively due to the complexity and volume of data. They propose an unsupervised autoencoder method trained solely on benign network data for detecting unseen attacks in IoV networks. The model uses a weighted combination of reconstruction and triplet margin loss during training and demonstrates robust performance with high accuracy on both benign and anomaly data. Experiments conducted on recent network intrusion datasets show that the method is effective across different application domains and adaptable via transfer learning.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this research lies in the substantial security vulnerabilities introduced by IoV systems. Traditional security mechanisms are inadequate for accurately detecting sophisticated and evolving cyberattacks in these highly interconnected systems.

Method: The method involves an unsupervised autoencoder trained entirely on benign network data for detecting unseen attacks in IoV networks. A weighted combination of reconstruction and triplet margin loss is used to guide the autoencoder training and develop a diverse representation of the benign training set.

Result: The proposed method performs robustly for all unseen attack types, achieving approximately 99% accuracy on benign data and between 97% to 100% performance on anomaly data. Additionally, the model's adaptability through transfer learning is demonstrated, maintaining high performance while leveraging domain features from one domain to another.

Conclusion: The conclusion is that the unsupervised autoencoder method effectively detects unseen attacks in IoV networks with high accuracy and robustness. The model's adaptability via transfer learning further enhances its applicability across different domains.

Abstract: Internet of Vehicles (IoV) systems, while offering significant advancements
in transportation efficiency and safety, introduce substantial security
vulnerabilities due to their highly interconnected nature. These dynamic
systems produce massive amounts of data between vehicles, infrastructure, and
cloud services and present a highly distributed framework with a wide attack
surface. In considering network-centered attacks on IoV systems, attacks such
as Denial-of-Service (DoS) can prohibit the communication of essential physical
traffic safety information between system elements, illustrating that the
security concerns for these systems go beyond the traditional confidentiality,
integrity, and availability concerns of enterprise systems. Given the
complexity and volume of data generated by IoV systems, traditional security
mechanisms are often inadequate for accurately detecting sophisticated and
evolving cyberattacks. Here, we present an unsupervised autoencoder method
trained entirely on benign network data for the purpose of unseen attack
detection in IoV networks. We leverage a weighted combination of reconstruction
and triplet margin loss to guide the autoencoder training and develop a diverse
representation of the benign training set. We conduct extensive experiments on
recent network intrusion datasets from two different application domains,
industrial IoT and home IoT, that represent the modern IoV task. We show that
our method performs robustly for all unseen attack types, with roughly 99%
accuracy on benign data and between 97% and 100% performance on anomaly data.
We extend these results to show that our model is adaptable through the use of
transfer learning, achieving similarly high results while leveraging domain
features from one domain to another.

</details>


### [172] [Lazarus Group Targets Crypto-Wallets and Financial Data while employing new Tradecrafts](https://arxiv.org/abs/2505.21725)
*Alessio Di Santo*

Main category: cs.CR

TL;DR: This report conducts a thorough analysis of a malware sample's architecture and behavior, links it to known threat actors, and provides actionable intelligence to improve defense.


<details>
  <summary>Details</summary>
Motivation: To understand the capabilities and motivations of the likely threat actor behind a specific malware sample and provide actionable threat intelligence for proactive defense.

Method: Through static and dynamic examination, identifying core functionalities such as persistence mechanisms, command-and-control communication, and data exfiltration routines, then correlating indicators of compromise with known techniques, tactics, and procedures.

Result: The malware sample is situated within contemporary threat campaigns, providing precise detection hypotheses and refined alert logic for real-time monitoring systems.

Conclusion: This structured intelligence enhances predictive risk assessments, informs vulnerability prioritization, and strengthens organizational resilience against advanced persistent threats.

Abstract: This report presents a comprehensive analysis of a malicious software sample,
detailing its architecture, behavioral characteristics, and underlying intent.
Through static and dynamic examination, the malware core functionalities,
including persistence mechanisms, command-and-control communication, and data
exfiltration routines, are identified and its supporting infrastructure is
mapped. By correlating observed indicators of compromise with known techniques,
tactics, and procedures, this analysis situates the sample within the broader
context of contemporary threat campaigns and infers the capabilities and
motivations of its likely threat actor.
  Building on these findings, actionable threat intelligence is provided to
support proactive defenses. Threat hunting teams receive precise detection
hypotheses for uncovering latent adversarial presence, while monitoring systems
can refine alert logic to detect anomalous activity in real time. Finally, the
report discusses how this structured intelligence enhances predictive risk
assessments, informs vulnerability prioritization, and strengthens
organizational resilience against advanced persistent threats. By integrating
detailed technical insights with strategic threat landscape mapping, this
malware analysis report not only reconstructs past adversary actions but also
establishes a robust foundation for anticipating and mitigating future attacks.

</details>


### [173] [VulBinLLM: LLM-powered Vulnerability Detection for Stripped Binaries](https://arxiv.org/abs/2505.22010)
*Nasir Hussain,Haohan Chen,Chanh Tran,Philip Huang,Zhuohao Li,Pravir Chugh,William Chen,Ashish Kundu,Yuan Tian*

Main category: cs.CR

TL;DR: This paper presents Vul-BinLLM, an LLM-based framework for detecting vulnerabilities in stripped binary files by combining decompilation optimization and long-term memory. It achieves state-of-the-art performance in binary vulnerability analysis.


<details>
  <summary>Details</summary>
Motivation: Detecting vulnerabilities within stripped binary files is a significant challenge in software security. While some progress has been made with LLMs in generating human-readable information from decompiled binaries, effectively detecting vulnerabilities remains an open problem.

Method: The Vul-BinLLM framework uses a combined approach of decompilation optimization to make vulnerabilities more prominent and long-term memory for a larger context window. In the decompilation phase, it adds vulnerability and weakness comments without altering the code structure or functionality. For vulnerability reasoning, it combines in-context learning, chain-of-thought prompting, and a memory management agent to enhance accuracy.

Result: Evaluations on the commonly used synthetic dataset Juliet show that Vul-BinLLM is highly effective in detecting vulnerabilities in C/C++ binaries.

Conclusion: Vul-BinLLM demonstrates the potential of LLMs to overcome limitations of traditional analysis methods and advance the field of binary vulnerability detection, leading to more secure software systems.

Abstract: Recognizing vulnerabilities in stripped binary files presents a significant
challenge in software security. Although some progress has been made in
generating human-readable information from decompiled binary files with Large
Language Models (LLMs), effectively and scalably detecting vulnerabilities
within these binary files is still an open problem. This paper explores the
novel application of LLMs to detect vulnerabilities within these binary files.
We demonstrate the feasibility of identifying vulnerable programs through a
combined approach of decompilation optimization to make the vulnerabilities
more prominent and long-term memory for a larger context window, achieving
state-of-the-art performance in binary vulnerability analysis. Our findings
highlight the potential for LLMs to overcome the limitations of traditional
analysis methods and advance the field of binary vulnerability detection,
paving the way for more secure software systems. In this paper, we present
Vul-BinLLM , an LLM-based framework for binary vulnerability detection that
mirrors traditional binary analysis workflows with fine-grained optimizations
in decompilation and vulnerability reasoning with an extended context. In the
decompilation phase, Vul-BinLLM adds vulnerability and weakness comments
without altering the code structure or functionality, providing more contextual
information for vulnerability reasoning later. Then for vulnerability
reasoning, Vul-BinLLM combines in-context learning and chain-of-thought
prompting along with a memory management agent to enhance accuracy. Our
evaluations encompass the commonly used synthetic dataset Juliet to evaluate
the potential feasibility for analysis and vulnerability detection in C/C++
binaries. Our evaluations show that Vul-BinLLM is highly effective in detecting
vulnerabilities on the compiled Juliet dataset.

</details>


### [174] [A Comparative Study of Fuzzers and Static Analysis Tools for Finding Memory Unsafety in C and C++](https://arxiv.org/abs/2505.22052)
*Keno Hassler,Philipp Görz,Stephan Lipp,Thorsten Holz,Marcel Böhme*

Main category: cs.CR

TL;DR: Even today, over 70% of security vulnerabilities result from memory safety violations. This paper presents an empirical analysis of five static analyzers and 13 fuzzers applied to over 100 known security vulnerabilities in C/C++ programs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of memory safety violations which cause over 70% of security vulnerabilities in critical software systems.

Method: Empirical analysis of five static analyzers and 13 fuzzers applied to over 100 known security vulnerabilities in C/C++ programs. Measurement of bug reports generated for each vulnerability, manual analysis of sampled bug-containing functions, quantification of false-positive rates, and assessment of bug discovery limits, ease of use, resource requirements, and integration into development process.

Result: Both techniques discover different types of bugs with clear winners for each. Static analysis and fuzzing have different strengths and limitations.

Conclusion: Developers should choose tools based on specific workflow and usability requirements. Future research should foster collaboration between static analysis and fuzzing domains.

Abstract: Even today, over 70% of security vulnerabilities in critical software systems
result from memory safety violations. To address this challenge, fuzzing and
static analysis are widely used automated methods to discover such
vulnerabilities. Fuzzing generates random program inputs to identify faults,
while static analysis examines source code to detect potential vulnerabilities.
Although these techniques share a common goal, they take fundamentally
different approaches and have evolved largely independently.
  In this paper, we present an empirical analysis of five static analyzers and
13 fuzzers, applied to over 100 known security vulnerabilities in C/C++
programs. We measure the number of bug reports generated for each vulnerability
to evaluate how the approaches differ and complement each other. Moreover, we
randomly sample eight bug-containing functions, manually analyze all bug
reports therein, and quantify false-positive rates. We also assess limits to
bug discovery, ease of use, resource requirements, and integration into the
development process. We find that both techniques discover different types of
bugs, but there are clear winners for each. Developers should consider these
tools depending on their specific workflow and usability requirements. Based on
our findings, we propose future directions to foster collaboration between
these research domains.

</details>


### [175] [Accountable, Scalable and DoS-resilient Secure Vehicular Communication](https://arxiv.org/abs/2505.22162)
*Hongyu Jin,Panos Papadimitratos*

Main category: cs.CR

TL;DR: 提出了一种新的消息验证机制，通过优先处理有效消息、批量验证以及行为检测等手段，增强车辆通信中基于标准化VC匿名认证的DoS攻击抵御能力，同时保持低延迟和适度通信开销。


<details>
  <summary>Details</summary>
Motivation: 广播的CAM和DENM消息需要在过期期限内完成验证，但容易受到伪造消息引发的DoS攻击，导致系统资源耗尽，无法及时验证正常消息，从而可能造成灾难性后果。

Method: 设计了高效的消息验证辅助构造（message verification facilitators），用于优先验证潜在的有效消息，并实现基于单一签名验证的多消息验证；结合概率签名验证和跨多个辅助构造的交叉检查进行行为检测，同时保持低验证延迟并引入适度通信开销。

Result: 所提出的机制能够在遭受攻击时维持较低的验证延迟，同时实现高效的事件驱动消息发现与验证，包括用于方案中的不当行为证据的验证。

Conclusion: 该方法在不牺牲不可否认性的前提下，增强了标准化VC匿名认证对DoS攻击的抵抗能力，为车辆安全提供了更可靠的保障。

Abstract: Paramount to vehicle safety, broadcasted Cooperative Awareness Messages
(CAMs) and Decentralized Environmental Notification Messages (DENMs) are
pseudonymously authenticated for security and privacy protection, with each
node needing to have all incoming messages validated within an expiration
deadline. This creates an asymmetry that can be easily exploited by external
adversaries to launch a clogging Denial of Service (DoS) attack: each forged VC
message forces all neighboring nodes to cryptographically validate it; at
increasing rates, easy to generate forged messages gradually exhaust processing
resources and severely degrade or deny timely validation of benign CAMs/DENMs.
The result can be catastrophic when awareness of neighbor vehicle positions or
critical reports are missed. We address this problem making the standardized VC
pseudonymous authentication DoS-resilient. We propose efficient cryptographic
constructs, which we term message verification facilitators, to prioritize
processing resources for verification of potentially valid messages among bogus
messages and verify multiple messages based on one signature verification. Any
message acceptance is strictly based on public-key based message
authentication/verification for accountability, i.e., non-repudiation is not
sacrificed, unlike symmetric key based approaches. This further enables drastic
misbehavior detection, also exploiting the newly introduced facilitators, based
on probabilistic signature verification and cross-checking over multiple
facilitators verifying the same message; while maintaining verification latency
low even when under attack, trading off modest communication overhead. Our
facilitators can also be used for efficient discovery and verification of DENM
or any event-driven message, including misbehavior evidence used for our
scheme.

</details>


### [176] [Domainator: Detecting and Identifying DNS-Tunneling Malware Using Metadata Sequences](https://arxiv.org/abs/2505.22220)
*Denis Petrov,Pascal Ruffing,Sebastian Zillien,Steffen Wendzel*

Main category: cs.CR

TL;DR: This paper presents Domainator, an approach to detect and differentiate state-of-the-art malware and DNS tunneling tools by analyzing sequential patterns in DNS traffic without relying on trivial features. Evaluated with 7 samples, it can identify specific malware and infer their behavior.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenges in detecting and differentiating malware based on its DNS tunneling features, as well as gaining knowledge about the current actions taken by the malware through its DNS tunneling traffic.

Method: The method used in this paper is to apply an analysis of sequential patterns to identify specific types of malware, rather than relying on trivial features such as 'magic bytes' embedded into subdomains.

Result: The result is that they can identify particular malware based on its DNS traffic and infer the rough behavior of the malware through its DNS tunneling artifacts.

Conclusion: In conclusion, the paper presents Domainator, a novel approach for detecting and differentiating malware and DNS tunneling tools, and compares it with related methods.

Abstract: In recent years, malware with tunneling (or: covert channel) capabilities is
on the rise. While malware research led to several methods and innovations, the
detection and differentiation of malware solely based on its DNS tunneling
features is still in its infancy. Moreover, no work so far has used the DNS
tunneling traffic to gain knowledge over the current actions taken by the
malware. In this paper, we present Domainator, an approach to detect and
differentiate state-of-the-art malware and DNS tunneling tools without relying
on trivial (but quickly altered) features such as "magic bytes" that are
embedded into subdomains. Instead, we apply an analysis of sequential patterns
to identify specific types of malware. We evaluate our approach with 7
different malware samples and tunneling tools and can identify the particular
malware based on its DNS traffic. We further infer the rough behavior of the
particular malware through its DNS tunneling artifacts. Finally, we compare our
Domainator with related methods.

</details>


### [177] [Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models](https://arxiv.org/abs/2505.22271)
*Yongcan Yu,Yanbo Wang,Ran He,Jian Liang*

Main category: cs.CR

TL;DR: A universal defense framework called Test-time IMmunization (TIM) is proposed to defend against various jailbreak attacks on large language models.


<details>
  <summary>Details</summary>
Motivation: Large language models are vulnerable to jailbreak attacks and current defense methods are often tailored to specific types of attacks, limiting their effectiveness against diverse adversarial strategies.

Method: The TIM framework initially trains a gist token for efficient detection of jailbreak activities. When jailbreak attempts are identified, safety fine-tuning is implemented using the detected jailbreak instructions paired with refusal answers. To mitigate potential performance degradation in the detector, the fine-tuning process is decoupled from the detection module.

Result: Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy of TIM.

Conclusion: TIM can adaptively defend against various jailbreak attacks in a self-evolving way.

Abstract: While (multimodal) large language models (LLMs) have attracted widespread
attention due to their exceptional capabilities, they remain vulnerable to
jailbreak attacks. Various defense methods are proposed to defend against
jailbreak attacks, however, they are often tailored to specific types of
jailbreak attacks, limiting their effectiveness against diverse adversarial
strategies. For instance, rephrasing-based defenses are effective against text
adversarial jailbreaks but fail to counteract image-based attacks. To overcome
these limitations, we propose a universal defense framework, termed Test-time
IMmunization (TIM), which can adaptively defend against various jailbreak
attacks in a self-evolving way. Specifically, TIM initially trains a gist token
for efficient detection, which it subsequently applies to detect jailbreak
activities during inference. When jailbreak attempts are identified, TIM
implements safety fine-tuning using the detected jailbreak instructions paired
with refusal answers. Furthermore, to mitigate potential performance
degradation in the detector caused by parameter updates during safety
fine-tuning, we decouple the fine-tuning process from the detection module.
Extensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy
of TIM.

</details>


### [178] [Does Johnny Get the Message? Evaluating Cybersecurity Notifications for Everyday Users](https://arxiv.org/abs/2505.22435)
*Victor Jüttner,Erik Buchmann*

Main category: cs.CR

TL;DR: The paper introduces HCSAEF, a framework for evaluating LLM-generated cybersecurity alerts aimed at end users. It assesses the intuitiveness, urgency, and correctness of these notifications through three use cases.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of ensuring that security alerts generated by LLMs are effectively communicated to everyday users in an intuitive and accurate manner.

Method: Development of HCSAEF, which evaluates LLM-generated cybersecurity notifications across dimensions like intuitiveness, urgency, and correctness. Demonstrated through three use cases focusing on prompt design, model selection, and output consistency.

Result: HCSAEF successfully differentiates and quantifies the quality of generated notifications, providing insights into improving LLM-generated alerts.

Conclusion: HCSAEF is an effective tool for researchers to compare, improve, and analyze LLM-generated cybersecurity notifications for end users.

Abstract: Due to the increasing presence of networked devices in everyday life, not
only cybersecurity specialists but also end users benefit from security
applications such as firewalls, vulnerability scanners, and intrusion detection
systems. Recent approaches use large language models (LLMs) to rewrite brief,
technical security alerts into intuitive language and suggest actionable
measures, helping everyday users understand and respond appropriately to
security risks. However, it remains an open question how well such alerts are
explained to users. LLM outputs can also be hallucinated, inconsistent, or
misleading. In this work, we introduce the Human-Centered Security Alert
Evaluation Framework (HCSAEF). HCSAEF assesses LLM-generated cybersecurity
notifications to support researchers who want to compare notifications
generated for everyday users, improve them, or analyze the capabilities of
different LLMs in explaining cybersecurity issues. We demonstrate HCSAEF
through three use cases, which allow us to quantify the impact of prompt
design, model selection, and output consistency. Our findings indicate that
HCSAEF effectively differentiates generated notifications along dimensions such
as intuitiveness, urgency, and correctness.

</details>


### [179] [Privacy-preserving Prompt Personalization in Federated Learning for Multimodal Large Language Models](https://arxiv.org/abs/2505.22447)
*Sizai Hou,Songze Li,Baturalp Buyukates*

Main category: cs.CR

TL;DR: An abstract about SecFPP, a secure federated prompt personalization protocol that balances generalization, personalization, and privacy.


<details>
  <summary>Details</summary>
Motivation: Prompt learning is essential for adapting pre-trained multimodal language models to user tasks. However, existing methods like Federated prompt personalization (FPP) expose personalized prompts to privacy risks such as prompt stealing or membership inference attacks while techniques like differential privacy degrade personalization performance.

Method: The paper proposes SecFPP which employs hierarchical prompt adaptation with domain-level and class-level components to handle multi-granular data imbalance. For privacy, it uses a novel secret-sharing-based adaptive clustering algorithm for domain-level adaptation while keeping class-level components private.

Result: SecFPP achieves state-of-the-art accuracy under severe heterogeneity in data distribution. Extensive experiments show it significantly outperforms both non-private and privacy-preserving baselines, offering a superior privacy-performance trade-off.

Conclusion: SecFPP is a secure FPP protocol that harmonizes generalization, personalization, and privacy guarantees.

Abstract: Prompt learning is a crucial technique for adapting pre-trained multimodal
language models (MLLMs) to user tasks. Federated prompt personalization (FPP)
is further developed to address data heterogeneity and local overfitting,
however, it exposes personalized prompts - valuable intellectual assets - to
privacy risks like prompt stealing or membership inference attacks.
Widely-adopted techniques like differential privacy add noise to prompts,
whereas degrading personalization performance. We propose SecFPP, a secure FPP
protocol harmonizing generalization, personalization, and privacy guarantees.
SecFPP employs hierarchical prompt adaptation with domain-level and class-level
components to handle multi-granular data imbalance. For privacy, it uses a
novel secret-sharing-based adaptive clustering algorithm for domain-level
adaptation while keeping class-level components private. While theoretically
and empirically secure, SecFPP achieves state-of-the-art accuracy under severe
heterogeneity in data distribution. Extensive experiments show it significantly
outperforms both non-private and privacy-preserving baselines, offering a
superior privacy-performance trade-off.

</details>


### [180] [Private Lossless Multiple Release](https://arxiv.org/abs/2505.22449)
*Joel Daniel Andersson,Lukas Retschmeier,Boel Nelson,Rasmus Pagh*

Main category: cs.CR

TL;DR: 本论文研究了在不同访问/信任级别下，分析师持有具有不同隐私参数的私有发布之更通用的多发布设置。主要结果表明，对于一大类加性噪声机制，无损多发布是可行的，并对高斯、拉普拉斯和泊松机制提供了无损多发布方法。此外，还提出了一种与维度数量无关的稀疏直方图渐进发布的有效机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究已经展示了基于拉普拉斯噪声和高斯噪声的差分隐私机制的渐进发布技术，但未充分探索在不同访问/信任级别下的多发布场景及其无损特性。

Method: 1. 定义无损多发布：确保子集S的发布具有与S中最少隐私发布相同的隐私保证，并且每个发布的分布等同于单个具有相同隐私参数的发布。
2. 对高斯机制提供简单的方法实现无损多发布，并进行独立分析。
3. 提供拉普拉斯和泊松机制的无损多发布方法。
4. 探讨稀疏直方图的高效渐进发布机制，提出一种运行时间与维度数量无关的机制。

Result: - 证明无损多发布对于一大类加性噪声机制是可行的。
- 成功实现了高斯、拉普拉斯和泊松机制的无损多发布。
- 提出了一个高效的稀疏直方图渐进发布机制，其运行时间与维度数量无关。

Conclusion: 本文通过定义并实现无损多发布，扩展了差分隐私机制的应用范围，特别是在不同访问/信任级别下的场景。同时，为高斯、拉普拉斯和泊松机制提供了具体的无损多发布方法，并解决了稀疏直方图渐进发布的问题。

Abstract: Koufogiannis et al. (2016) showed a $\textit{gradual release}$ result for
Laplace noise-based differentially private mechanisms: given an
$\varepsilon$-DP release, a new release with privacy parameter $\varepsilon' >
\varepsilon$ can be computed such that the combined privacy loss of both
releases is at most $\varepsilon'$ and the distribution of the latter is the
same as a single release with parameter $\varepsilon'$. They also showed
gradual release techniques for Gaussian noise, later also explored by
Whitehouse et al. (2022).
  In this paper, we consider a more general $\textit{multiple release}$ setting
in which analysts hold private releases with different privacy parameters
corresponding to different access/trust levels. These releases are determined
one by one, with privacy parameters in arbitrary order. A multiple release is
$\textit{lossless}$ if having access to a subset $S$ of the releases has the
same privacy guarantee as the least private release in $S$, and each release
has the same distribution as a single release with the same privacy parameter.
Our main result is that lossless multiple release is possible for a large class
of additive noise mechanisms. For the Gaussian mechanism we give a simple
method for lossless multiple release with a short, self-contained analysis that
does not require knowledge of the mathematics of Brownian motion. We also
present lossless multiple release for the Laplace and Poisson mechanisms.
Finally, we consider how to efficiently do gradual release of sparse
histograms, and present a mechanism with running time independent of the number
of dimensions.

</details>


### [181] [Transformers for Secure Hardware Systems: Applications, Challenges, and Outlook](https://arxiv.org/abs/2505.22605)
*Banafsheh Saber Latibari,Najmeh Nazari,Avesta Sasan,Houman Homayoun,Pratik Satam,Soheil Salehi,Hossein Sayadi*

Main category: cs.CR

TL;DR: Transformers are gaining traction in hardware security due to their ability to model complex dependencies, offering enhanced capabilities in identifying vulnerabilities, detecting anomalies, and reinforcing system integrity.


<details>
  <summary>Details</summary>
Motivation: The rise of hardware-level security threats such as side-channel attacks, hardware Trojans, and firmware vulnerabilities demands advanced detection mechanisms that are more intelligent and adaptive.

Method: This survey provides a comprehensive review of recent advancements on the use of Transformers in hardware security, examining their application across key areas such as side-channel analysis, hardware Trojan detection, vulnerability classification, device fingerprinting, and firmware security.

Result: The practical challenges of applying Transformers to secure hardware systems are discussed, and opportunities and future research directions are highlighted.

Conclusion: These insights pave the way for deeper integration of AI-driven techniques into hardware security frameworks, enabling more resilient and intelligent defenses.

Abstract: The rise of hardware-level security threats, such as side-channel attacks,
hardware Trojans, and firmware vulnerabilities, demands advanced detection
mechanisms that are more intelligent and adaptive. Traditional methods often
fall short in addressing the complexity and evasiveness of modern attacks,
driving increased interest in machine learning-based solutions. Among these,
Transformer models, widely recognized for their success in natural language
processing and computer vision, have gained traction in the security domain due
to their ability to model complex dependencies, offering enhanced capabilities
in identifying vulnerabilities, detecting anomalies, and reinforcing system
integrity. This survey provides a comprehensive review of recent advancements
on the use of Transformers in hardware security, examining their application
across key areas such as side-channel analysis, hardware Trojan detection,
vulnerability classification, device fingerprinting, and firmware security.
Furthermore, we discuss the practical challenges of applying Transformers to
secure hardware systems, and highlight opportunities and future research
directions that position them as a foundation for next-generation
hardware-assisted security. These insights pave the way for deeper integration
of AI-driven techniques into hardware security frameworks, enabling more
resilient and intelligent defenses.

</details>


### [182] [SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes](https://arxiv.org/abs/2505.22638)
*Denis Donadel,Gabriele Crestanello,Giulio Morandini,Daniele Antonioli,Mauro Conti,Massimo Merro*

Main category: cs.CR

TL;DR: 工业控制系统（ICS）对关键基础设施至关重要，但容易受到网络攻击。本文提出SimProcess框架，通过评估模拟与真实带噪声物理过程的接近程度来提高ICS仿真保真度，从而改进蜜罐系统的防御能力。


<details>
  <summary>Details</summary>
Motivation: 现有的ICS蜜罐系统难以准确复制ICS物理过程中的噪声，导致容易被检测到。因此需要一种能够评估ICS仿真保真度的方法，以帮助改进防御机制。

Method: 提出SimProcess框架，利用机器学习模型（如随机森林）估计噪声分布，通过测量序列数据评估仿真与真实系统的距离。该方法无需详细数学模型，适用于复杂动态系统。

Result: 通过使用EPIC测试平台的真实电力数据进行案例研究，证明了SimProcess的有效性。模型能以高达1.0的召回率正确分类真实样本，并确定高斯分布和高斯混合分布为最佳噪声模拟方法，同时推荐使用自动编码器生成解决方案。

Conclusion: SimProcess框架可有效评估ICS仿真的保真度，帮助改进蜜罐系统。代码已公开，便于进一步研究和应用。

Abstract: Industrial Control Systems (ICS) manage critical infrastructures like power
grids and water treatment plants. Cyberattacks on ICSs can disrupt operations,
causing severe economic, environmental, and safety issues. For example,
undetected pollution in a water plant can put the lives of thousands at stake.
ICS researchers have increasingly turned to honeypots -- decoy systems designed
to attract attackers, study their behaviors, and eventually improve defensive
mechanisms. However, existing ICS honeypots struggle to replicate the ICS
physical process, making them susceptible to detection. Accurately simulating
the noise in ICS physical processes is challenging because different factors
produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity
of ICS simulations by evaluating how closely they resemble real-world and noisy
physical processes. It measures the simulation distance from a target system by
estimating the noise distribution with machine learning models like Random
Forest. Unlike existing solutions that require detailed mathematical models or
are limited to simple systems, SimProcess operates with only a timeseries of
measurements from the real system, making it applicable to a broader range of
complex dynamic systems. We demonstrate the framework's effectiveness through a
case study using real-world power grid data from the EPIC testbed. We compare
the performance of various simulation methods, including static and generative
noise techniques. Our model correctly classifies real samples with a recall of
up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best
distribution to simulate our power systems, together with a generative solution
provided by an autoencoder, thereby helping developers to improve honeypot
fidelity. Additionally, we make our code publicly available.

</details>


### [183] [On the Intractability of Chaotic Symbolic Walks: Toward a Non-Algebraic Post-Quantum Hardness Assumption](https://arxiv.org/abs/2505.22644)
*Mohamed Aly Bouke*

Main category: cs.CR

TL;DR: The paper introduces SPIP, a new non-algebraic computational hardness assumption for post-quantum cryptography, proving its PSPACE-hard and #P-hard nature and demonstrating resistance to quantum attacks.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of classical and post-quantum cryptographic assumptions to quantum algorithms and advanced algebraic attacks, there is a need for structure-free alternatives.

Method: Introduced Symbolic Path Inversion Problem (SPIP) based on symbolic trajectories generated by contractive affine maps with bounded noise over Z2. Proved that SPIP is PSPACE-hard and #P-hard, and demonstrated through empirical simulation.

Result: Even short symbolic sequences can produce a large number of valid trajectories with exponential growth. Quantum security analysis shows resistance to Grover-style search due to oracle ambiguity and verification instability.

Conclusion: SPIP is positioned as a viable foundation for post-quantum cryptography avoiding algebraic symmetry vulnerabilities while offering scalability, unpredictability, and resistance to both classical and quantum inversion.

Abstract: Most classical and post-quantum cryptographic assumptions, including integer
factorization, discrete logarithms, and Learning with Errors (LWE), rely on
algebraic structures such as rings or vector spaces. While mathematically
powerful, these structures can be exploited by quantum algorithms or advanced
algebraic attacks, raising a pressing need for structure-free alternatives. To
address this gap, we introduce the Symbolic Path Inversion Problem (SPIP), a
new computational hardness assumption based on symbolic trajectories generated
by contractive affine maps with bounded noise over Z2. Unlike traditional
systems, SPIP is inherently non-algebraic and relies on chaotic symbolic
evolution and rounding-induced non-injectivity to render inversion
computationally infeasible. We prove that SPIP is PSPACE-hard and #P-hard, and
demonstrate through empirical simulation that even short symbolic sequences
(e.g., n = 3, m = 2) can produce over 500 valid trajectories for a single
endpoint, with exponential growth reaching 2256 paths for moderate parameters.
A quantum security analysis further shows that Grover-style search offers no
practical advantage due to oracle ambiguity and verification instability. These
results position SPIP as a viable foundation for post-quantum cryptography that
avoids the vulnerabilities of algebraic symmetry while offering scalability,
unpredictability, and resistance to both classical and quantum inversion.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [184] [VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining](https://arxiv.org/abs/2505.21527)
*Jianheng Zhuo,Yifan Yang,Yiwen Shao,Yong Xu,Dong Yu,Kai Yu,Xie Chen*

Main category: eess.AS

TL;DR: VietASR is a new ASR training pipeline designed for low-resource languages like Vietnamese, utilizing large unlabeled data and limited labeled data via multi-iteration self-supervised learning. It surpasses Whisper Large-v3 and commercial systems in performance while being cost-effective.


<details>
  <summary>Details</summary>
Motivation: Current ASR systems struggle with high training costs, latency, and accessibility issues when dealing with low-resource languages such as Vietnamese.

Method: Propose VietASR, an ASR training pipeline that employs multi-iteration ASR-biased self-supervised learning on large-scale unlabeled data combined with a small set of labeled data for fine-tuning.

Result: Pre-training on 70,000-hour unlabeled data and fine-tuning on 50-hour labeled data results in a lightweight yet powerful ASR model that outperforms Whisper Large-v3 and commercial systems on real-world datasets.

Conclusion: VietASR offers a practical and cost-effective solution to enhance ASR performance for low-resource languages, and the code and models will be open-sourced.

Abstract: Automatic speech recognition (ASR) has made remarkable progress but heavily
relies on large-scale labeled data, which is scarce for low-resource languages
like Vietnamese. While existing systems such as Whisper, USM, and MMS achieve
promising performance, their efficacy remains inadequate in terms of training
costs, latency, and accessibility. To address these issues, we propose VietASR,
a novel ASR training pipeline that leverages vast amounts of unlabeled data and
a small set of labeled data. Through multi-iteration ASR-biased self-supervised
learning on a large-scale unlabeled dataset, VietASR offers a cost-effective
and practical solution for enhancing ASR performance. Experiments demonstrate
that pre-training on 70,000-hour unlabeled data and fine-tuning on merely
50-hour labeled data yield a lightweight but powerful ASR model. It outperforms
Whisper Large-v3 and commercial ASR systems on real-world data. Our code and
models will be open-sourced to facilitate research in low-resource ASR.

</details>


### [185] [WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper](https://arxiv.org/abs/2505.21551)
*Emmanuel Akinrintoyo,Nadine Abdelhalim,Nicole Salomons*

Main category: eess.AS

TL;DR: 通过使用开源失智症语音数据集(DementiaBank)和内部数据集微调Whisper模型，研究显著提升了其在处理失智症语音时的词错率（WER）表现。中等规模的模型实现了0.24的WER，并且在未见数据和语音模式上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 由于失智症患者的语音常包含不规则模式和不流畅现象（如停顿、重复和破碎句式），现有的语音转录技术（如Whisper）无法准确转录这些语音。而准确的语音转录对失智症的经济诊断及辅助技术的发展至关重要。

Method: 研究者采用开源数据集DementiaBank以及内部数据集对Whisper进行微调，同时加入填充词以评估填充词包含率（FIR）和F1分数。

Result: 微调后的模型在词错率（WER）方面显著优于现成模型，其中中等规模模型达到了0.24的WER，并且在未见数据和语音模式上展现出显著的泛化能力。

Conclusion: 微调过的Whisper模型在处理失智症语音方面表现更佳，具备更好的准确性和泛化性能。

Abstract: Whisper fails to correctly transcribe dementia speech because persons with
dementia (PwDs) often exhibit irregular speech patterns and disfluencies such
as pauses, repetitions, and fragmented sentences. It was trained on standard
speech and may have had little or no exposure to dementia-affected speech.
However, correct transcription is vital for dementia speech for cost-effective
diagnosis and the development of assistive technology. In this work, we
fine-tune Whisper with the open-source dementia speech dataset (DementiaBank)
and our in-house dataset to improve its word error rate (WER). The fine-tuning
also includes filler words to ascertain the filler inclusion rate (FIR) and F1
score. The fine-tuned models significantly outperformed the off-the-shelf
models. The medium-sized model achieved a WER of 0.24, outperforming previous
work. Similarly, there was a notable generalisability to unseen data and speech
patterns.

</details>


### [186] [Analysis and Evaluation of Synthetic Data Generation in Speech Dysfluency Detection](https://arxiv.org/abs/2505.22029)
*Jinming Zhang,Xuanru Zhou,Jiachen Lian,Shuhe Li,William Li,Zoe Ezzes,Rian Bogley,Lisa Wauters,Zachary Miller,Jet Vonk,Brittany Morin,Maria Gorno-Tempini,Gopala Anumanchipalli*

Main category: eess.AS

TL;DR: The paper presents LLM-Dys, a comprehensive dysfluent speech corpus using LLM-enhanced dysfluency simulation across 11 categories, and an improved end-to-end detection framework achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Speech dysfluency detection is critical for clinical diagnosis and language assessment, but current methods are constrained by limited high-quality annotated data. Synthetic datasets exist but have issues with unnatural prosody and lack of contextual diversity.

Method: Propose LLM-Dys, the most extensive dysfluent speech corpus with LLM-enhanced dysfluency simulation covering 11 categories at word and phoneme levels. Utilize this dataset to improve an end-to-end dysfluency detection framework.

Result: Experimental validation shows state-of-the-art performance in dysfluency detection.

Conclusion: LLM-Dys addresses limitations in existing datasets and improves dysfluency detection. All resources are open-sourced.

Abstract: Speech dysfluency detection is crucial for clinical diagnosis and language
assessment, but existing methods are limited by the scarcity of high-quality
annotated data. Although recent advances in TTS model have enabled synthetic
dysfluency generation, existing synthetic datasets suffer from unnatural
prosody and limited contextual diversity. To address these limitations, we
propose LLM-Dys -- the most comprehensive dysfluent speech corpus with
LLM-enhanced dysfluency simulation. This dataset captures 11 dysfluency
categories spanning both word and phoneme levels. Building upon this resource,
we improve an end-to-end dysfluency detection framework. Experimental
validation demonstrates state-of-the-art performance. All data, models, and
code are open-sourced at https://github.com/Berkeley-Speech-Group/LLM-Dys.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [187] [iDSE: Navigating Design Space Exploration in High-Level Synthesis Using LLMs](https://arxiv.org/abs/2505.22086)
*Runkai Li,Jia Xiong,Xi Wang*

Main category: cs.AR

TL;DR: High-Level Synthesis (HLS) simplifies circuit design but has a complex design space. Traditional methods for exploring this space are inefficient. This paper introduces iDSE, an LLM-aided framework that efficiently explores the HLS design space by intelligently pruning it and leveraging LLMs' thinking patterns, leading to significantly better performance than traditional methods with far fewer explored designs.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and suboptimal results of traditional design space exploration methods in High-Level Synthesis, which struggles with the combinatorial explosion of directive configurations.

Method: iDSE is the first LLM-aided DSE framework that prunes the design space intelligently to guide LLMs in calibrating initial sampling designs, expediting convergence toward the Pareto front while achieving multi-path refinement through LLMs' convergent and divergent thinking patterns.

Result: Extensive experiments show that iDSE outperforms heuristic-based DSE methods by 5.1$	imes$$sim$16.6$	imes$ in proximity to the reference Pareto front, achieving similar results to NSGA-II with only 4.6% of the explored designs.

Conclusion: iDSE demonstrates the potential of LLMs in efficient and scalable HLS design optimization, providing new insights into overcoming multiobjective optimization challenges.

Abstract: High-Level Synthesis (HLS) serves as an agile hardware development tool that
streamlines the circuit design by abstracting the register transfer level into
behavioral descriptions, while allowing designers to customize the generated
microarchitectures through optimization directives. However, the combinatorial
explosion of possible directive configurations yields an intractable design
space. Traditional design space exploration (DSE) methods, despite adopting
heuristics or constructing predictive models to accelerate Pareto-optimal
design acquisition, still suffer from prohibitive exploration costs and
suboptimal results. Addressing these concerns, we introduce iDSE, the first
LLM-aided DSE framework that leverages HLS design quality perception to
effectively navigate the design space. iDSE intelligently pruns the design
space to guide LLMs in calibrating representative initial sampling designs,
expediting convergence toward the Pareto front. By exploiting the convergent
and divergent thinking patterns inherent in LLMs for hardware optimization,
iDSE achieves multi-path refinement of the design quality and diversity.
Extensive experiments demonstrate that iDSE outperforms heuristic-based DSE
methods by 5.1$\times$$\sim$16.6$\times$ in proximity to the reference Pareto
front, matching NSGA-II with only 4.6% of the explored designs. Our work
demonstrates the transformative potential of LLMs in scalable and efficient HLS
design optimization, offering new insights into multiobjective optimization
challenges.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [188] [Synonymous Variational Inference for Perceptual Image Compression](https://arxiv.org/abs/2505.22438)
*Zijian Liang,Kai Niu,Changshuo Wang,Jin Xu,Ping Zhang*

Main category: cs.IT

TL;DR: This paper proposes a synonymous variational inference (SVI) method for perceptual image compression, introducing synonymous image compression (SIC) with experimental results demonstrating its effectiveness.


<details>
  <summary>Details</summary>
Motivation: To re-analyze the perceptual image compression problem by leveraging the set-element relationship between semantic and syntactic information as synonymous relationships.

Method: Propose SVI method based on synonymity viewpoint, using perceptual similarity as synonymous criterion to build an ideal Synset and approximate posterior of latent synonymous representation. Introduce SIC scheme and implement progressive SIC codec.

Result: Experimental results show comparable rate-distortion-perception performance with a single progressive SIC codec.

Conclusion: The proposed SVI analysis method is effective in perceptual image compression, revealing a triple tradeoff in optimization.

Abstract: Recent contributions of semantic information theory reveal the set-element
relationship between semantic and syntactic information, represented as
synonymous relationships. In this paper, we propose a synonymous variational
inference (SVI) method based on this synonymity viewpoint to re-analyze the
perceptual image compression problem. It takes perceptual similarity as a
typical synonymous criterion to build an ideal synonymous set (Synset), and
approximate the posterior of its latent synonymous representation with a
parametric density by minimizing a partial semantic KL divergence. This
analysis theoretically proves that the optimization direction of perception
image compression follows a triple tradeoff that can cover the existing
rate-distortion-perception schemes. Additionally, we introduce synonymous image
compression (SIC), a new image compression scheme that corresponds to the
analytical process of SVI, and implement a progressive SIC codec to fully
leverage the model's capabilities. Experimental results demonstrate comparable
rate-distortion-perception performance using a single progressive SIC codec,
thus verifying the effectiveness of our proposed analysis method.

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [189] [Symbolic Foundation Regressor on Complex Networks](https://arxiv.org/abs/2505.21879)
*Weiting Liu,Jiaxu Cui,Jiao Hu,En Wang,Bo Yang*

Main category: cs.SC

TL;DR: The paper introduces a pre-trained symbolic foundation regressor that compresses complex data and generates interpretable physical representations, showing significant improvements in equation inference efficiency across various scientific domains.


<details>
  <summary>Details</summary>
Motivation: To streamline the traditional manual process of discovering scientific laws by leveraging data-driven machine learning technology, aiding in understanding the interpretable underlying models behind predictions.

Method: A pre-trained symbolic foundation regressor model is introduced which can compress complex data with interacting variables and produce interpretable physical representations. The model is tested on non-network symbolic regression, symbolic regression on complex networks, and network dynamics inference.

Result: The model shows a threefold improvement in equation inference efficiency compared to baseline approaches while maintaining accurate predictions. It also successfully uncovers intuitive laws of interaction transmission from global epidemic outbreak data.

Conclusion: This model extends the application of pre-trained symbolic regression models to complex networks, providing a foundational solution for revealing hidden mechanisms in complex phenomena, improving interpretability, and inspiring further scientific discoveries.

Abstract: In science, we are interested not only in forecasting but also in
understanding how predictions are made, specifically what the interpretable
underlying model looks like. Data-driven machine learning technology can
significantly streamline the complex and time-consuming traditional manual
process of discovering scientific laws, helping us gain insights into
fundamental issues in modern science. In this work, we introduce a pre-trained
symbolic foundation regressor that can effectively compress complex data with
numerous interacting variables while producing interpretable physical
representations. Our model has been rigorously tested on non-network symbolic
regression, symbolic regression on complex networks, and the inference of
network dynamics across various domains, including physics, biochemistry,
ecology, and epidemiology. The results indicate a remarkable improvement in
equation inference efficiency, being three times more effective than baseline
approaches while maintaining accurate predictions. Furthermore, we apply our
model to uncover more intuitive laws of interaction transmission from global
epidemic outbreak data, achieving optimal data fitting. This model extends the
application boundary of pre-trained symbolic regression models to complex
networks, and we believe it provides a foundational solution for revealing the
hidden mechanisms behind changes in complex phenomena, enhancing
interpretability, and inspiring further scientific discoveries.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [190] [RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination](https://arxiv.org/abs/2505.21925)
*Chong Zeng,Yue Dong,Pieter Peers,Hongzhi Wu,Xin Tong*

Main category: cs.GR

TL;DR: RenderFormer是一种无需每场景训练或微调，可直接从基于三角形的场景表示渲染图像并包含完整全局光照效果的神经渲染管道。它将渲染视为序列到序列转换，使用两个阶段的transformer架构来建模和生成图像。


<details>
  <summary>Details</summary>
Motivation: 现有的神经渲染方法可能需要针对每个场景进行训练或微调，并且在处理全局光照效果时可能存在局限性。为了克服这些限制，提出了一种新的神经渲染方法RenderFormer。

Method: RenderFormer采用两阶段pipeline：第一阶段是与视图无关的阶段，使用transformer架构对三角形之间的光传输进行建模；第二阶段是视图相关的阶段，同样基于transformer架构，将光线束转化为像素值。整个过程不需要每场景的训练或微调，仅使用最小的先验约束。

Result: RenderFormer在不同形状和光传输复杂度的场景中进行了演示和评估，表明其能够有效地渲染具有全局光照效果的图像。

Conclusion: RenderFormer提供了一种新的神经渲染方法，通过序列到序列转换实现高效的渲染，同时避免了每场景的单独训练需求，展示了良好的适应性和性能。

Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an
image from a triangle-based representation of a scene with full global
illumination effects and that does not require per-scene training or
fine-tuning. Instead of taking a physics-centric approach to rendering, we
formulate rendering as a sequence-to-sequence transformation where a sequence
of tokens representing triangles with reflectance properties is converted to a
sequence of output tokens representing small patches of pixels. RenderFormer
follows a two stage pipeline: a view-independent stage that models
triangle-to-triangle light transport, and a view-dependent stage that
transforms a token representing a bundle of rays to the corresponding pixel
values guided by the triangle-sequence from the view-independent stage. Both
stages are based on the transformer architecture and are learned with minimal
prior constraints. We demonstrate and evaluate RenderFormer on scenes with
varying complexity in shape and light transport.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [191] [Nonadaptive Output Regulation of Second-Order Nonlinear Uncertain Systems](https://arxiv.org/abs/2505.21838)
*Maobin Lu,Martin Guay,Telema Harry,Shimin Wang,Jordan Cooper*

Main category: eess.SY

TL;DR: This paper investigates the robust output regulation problem of second-order nonlinear uncertain systems with an unknown exosystem.


<details>
  <summary>Details</summary>
Motivation: To solve the robust output regulation problem without using adaptive control approach, but resorting to a robust control methodology to avoid bursting phenomenon.

Method: Construct generic internal models for steady-state state and input variables. Convert the robust output regulation problem into a nonadaptive stabilization problem through coordinate transformation. Design stabilization control law and construct strict Lyapunov function.

Result: The output zeroing manifold of the augmented system can be made attractive by the proposed nonadaptive control law.

Conclusion: The effectiveness of the proposed nonadaptive internal model approach is demonstrated by its application to the control of the Duffing system.

Abstract: This paper investigates the robust output regulation problem of second-order
nonlinear uncertain systems with an unknown exosystem. Instead of the adaptive
control approach, this paper resorts to a robust control methodology to solve
the problem and thus avoid the bursting phenomenon. In particular, this paper
constructs generic internal models for the steady-state state and input
variables of the system. By introducing a coordinate transformation, this paper
converts the robust output regulation problem into a nonadaptive stabilization
problem of an augmented system composed of the second-order nonlinear uncertain
system and the generic internal models. Then, we design the stabilization
control law and construct a strict Lyapunov function that guarantees the
robustness with respect to unmodeled disturbances. The analysis shows that the
output zeroing manifold of the augmented system can be made attractive by the
proposed nonadaptive control law, which solves the robust output regulation
problem. Finally, we demonstrate the effectiveness of the proposed nonadaptive
internal model approach by its application to the control of the Duffing
system.

</details>


### [192] [A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem](https://arxiv.org/abs/2505.21842)
*Filippos Fotiadis,Kyriakos G. Vamvoudakis*

Main category: eess.SY

TL;DR: 提出了一种基于物理信息神经网络（PINNs）的框架，用于解决非线性系统的无限时域最优控制问题。通过求解与之相关的稳态Hamilton-Jacobi-Bellman（HJB）方程，利用PINNs学习无限时域最优控制问题的价值函数。为避免稳态HJB方程多解问题，提出了将其应用于具有唯一解的有限时域变体，并提供了验证所选时域是否足够大的算法以及在不足够大时进行扩展的方法。该方法无需迭代策略评估、先验稳定控制器知识，并适用于非多项式基函数。仿真结果验证并阐明了理论发现。


<details>
  <summary>Details</summary>
Motivation: 现有的许多方法需要迭代策略评估、先验稳定控制器知识或仅适用于多项式基函数。为了克服这些限制，本文提出了一种基于PINNs的新方法，以更高效和通用的方式解决非线性系统的无限时域最优控制问题。

Method: 1. 使用PINNs框架解决非线性系统无限时域最优控制问题中的稳态HJB方程。2. 针对稳态HJB方程可能有多解的问题，采用其有限时域变体，确保唯一解。3. 提供验证所选时域是否足够大的算法，以及在不足够大时进行扩展的方法，减少计算量并增强鲁棒性。4. 该方法无需迭代策略评估、先验稳定控制器知识，并适用于非多项式基函数。

Result: 1. 成功解决了非线性系统的无限时域最优控制问题。2. 方法在不需要迭代策略评估、先验稳定控制器知识的情况下表现良好。3. 仿真结果验证并阐明了理论发现，证明了方法的有效性和可行性。

Conclusion: 所提出的基于PINNs的框架能够有效解决非线性系统的无限时域最优控制问题，尤其在处理非多项式基函数、无需先验稳定控制器知识和不进行迭代策略评估方面表现出色。这为未来研究提供了新的方向和工具。

Abstract: We propose a physics-informed neural networks (PINNs) framework to solve the
infinite-horizon optimal control problem of nonlinear systems. In particular,
since PINNs are generally able to solve a class of partial differential
equations (PDEs), they can be employed to learn the value function of the
infinite-horizon optimal control problem via solving the associated
steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is
that the steady-state HJB equation generally yields multiple solutions; hence
if PINNs are directly employed to it, they may end up approximating a solution
that is different from the optimal value function of the problem. We tackle
this by instead applying PINNs to a finite-horizon variant of the steady-state
HJB that has a unique solution, and which uniformly approximates the optimal
value function as the horizon increases. An algorithm to verify if the chosen
horizon is large enough is also given, as well as a method to extend it -- with
reduced computations and robustness to approximation errors -- in case it is
not. Unlike many existing methods, the proposed technique works well with
non-polynomial basis functions, does not require prior knowledge of a
stabilizing controller, and does not perform iterative policy evaluations.
Simulations are performed, which verify and clarify theoretical findings.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [193] [Conformance Checking for Less: Efficient Conformance Checking for Long Event Sequences](https://arxiv.org/abs/2505.21506)
*Eli Bogdanov,Izack Cohen,Avigdor Gal*

Main category: cs.DB

TL;DR: In order to solve the computational infeasibility of conformance checking for long event sequences, this paper proposes ConLES. It partitions traces into subtraces and uses global information for alignment, significantly reducing the search space while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Conformance checking for long event sequences and large data logs can be computationally infeasible due to the exponential complexity of finding an optimal alignment.

Method: Propose ConLES, a sliding-window conformance checking approach that partitions traces into manageable subtraces and iteratively aligns each against the expected behavior using global information capturing structural properties of both the trace and the process model.

Result: ConLES outperforms leading optimal and heuristic algorithms for long traces, achieving optimal or near-optimal solutions while significantly reducing the search space.

Conclusion: ConLES is a viable and leading option for conformance checking of long event sequences as it scales efficiently and supports both predefined and discovered process models.

Abstract: Long event sequences (termed traces) and large data logs that originate from
sensors and prediction models are becoming increasingly common in our data-rich
world. In such scenarios, conformance checking-validating a data log against an
expected system behavior (the process model) can become computationally
infeasible due to the exponential complexity of finding an optimal alignment.
To alleviate scalability challenges for this task, we propose ConLES, a
sliding-window conformance checking approach for long event sequences that
preserves the interpretability of alignment-based methods. ConLES partitions
traces into manageable subtraces and iteratively aligns each against the
expected behavior, leading to significant reduction of the search space while
maintaining overall accuracy. We use global information that captures
structural properties of both the trace and the process model, enabling
informed alignment decisions and discarding unpromising alignments, even if
they appear locally optimal. Performance evaluations across multiple datasets
highlight that ConLES outperforms the leading optimal and heuristic algorithms
for long traces, consistently achieving the optimal or near-optimal solution.
Unlike other conformance methods that struggle with long event sequences,
ConLES significantly reduces the search space, scales efficiently, and uniquely
supports both predefined and discovered process models, making it a viable and
leading option for conformance checking of long event sequences.

</details>


### [194] [StreamLink: Large-Language-Model Driven Distributed Data Engineering System](https://arxiv.org/abs/2505.21575)
*Dawei Feng,Di Mei,Huiri Tan,Lei Ren,Xianying Lou,Zhangxi Tan*

Main category: cs.DB

TL;DR: StreamLink is an LLM-driven distributed data system designed to improve data engineering tasks' efficiency and accessibility. It uses local fine-tuned LLMs for respecting user data privacy, incorporates domain-adapted LLMs to enhance natural language query understanding, simplifies SQL generation, and integrates LLM-based syntax and security checkers.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and accessibility of data engineering tasks while ensuring user data privacy.

Method: Building StreamLink on top of distributed frameworks such as Apache Spark and Hadoop, utilizing local fine-tuned LLMs instead of public AI services, incorporating domain-adapted LLMs to improve natural language query understanding, and integrating LLM-based syntax and security checkers.

Result: SQL generation reaches over 10% of execution accuracy compared to baseline methods, and users can find concerned items from hundreds of millions within a few seconds using natural language.

Conclusion: StreamLink demonstrates the potential of combining generative LLMs with distributed data processing for comprehensive and user-centric data engineering.

Abstract: Large Language Models (LLMs) have shown remarkable proficiency in natural
language understanding (NLU), opening doors for innovative applications. We
introduce StreamLink - an LLM-driven distributed data system designed to
improve the efficiency and accessibility of data engineering tasks. We build
StreamLink on top of distributed frameworks such as Apache Spark and Hadoop to
handle large data at scale. One of the important design philosophies of
StreamLink is to respect user data privacy by utilizing local fine-tuned LLMs
instead of a public AI service like ChatGPT. With help from domain-adapted
LLMs, we can improve our system's understanding of natural language queries
from users in various scenarios and simplify the procedure of generating
database queries like the Structured Query Language (SQL) for information
processing. We also incorporate LLM-based syntax and security checkers to
guarantee the reliability and safety of each generated query. StreamLink
illustrates the potential of merging generative LLMs with distributed data
processing for comprehensive and user-centric data engineering. With this
architecture, we allow users to interact with complex database systems at
different scales in a user-friendly and security-ensured manner, where the SQL
generation reaches over 10\% of execution accuracy compared to baseline
methods, and allow users to find the most concerned item from hundreds of
millions of items within a few seconds using natural language.

</details>


### [195] [ChatPD: An LLM-driven Paper-Dataset Networking System](https://arxiv.org/abs/2505.22349)
*Anjie Xu,Ruiqing Ding,Leye Wang*

Main category: cs.DB

TL;DR: The paper introduces ChatPD, a system using Large Language Models (LLMs) to automate dataset information extraction from academic papers and create structured paper-dataset networks. It outperforms PapersWithCode in dataset usage extraction with about 90% precision and recall in entity resolution tasks.


<details>
  <summary>Details</summary>
Motivation: Scientific research requires suitable datasets for method validation, but current platforms like PapersWithCode have inefficiencies due to manual workflows.

Method: ChatPD system consists of three key modules: paper collection, dataset information extraction, and dataset entity resolution. A Graph Completion and Inference strategy is proposed to map dataset descriptions to their corresponding entities.

Result: Through extensive experiments, ChatPD outperforms PapersWithCode in dataset usage extraction and achieves about 90% precision and recall in entity resolution tasks.

Conclusion: ChatPD is deployed to continuously extract dataset usage in papers and provide dataset discovery services. The system and the current paper-dataset network are open sourced on GitHub.

Abstract: Scientific research heavily depends on suitable datasets for method
validation, but existing academic platforms with dataset management like
PapersWithCode suffer from inefficiencies in their manual workflow. To overcome
this bottleneck, we present a system, called ChatPD, that utilizes Large
Language Models (LLMs) to automate dataset information extraction from academic
papers and construct a structured paper-dataset network. Our system consists of
three key modules: \textit{paper collection}, \textit{dataset information
extraction}, and \textit{dataset entity resolution} to construct paper-dataset
networks. Specifically, we propose a \textit{Graph Completion and Inference}
strategy to map dataset descriptions to their corresponding entities. Through
extensive experiments, we demonstrate that ChatPD not only outperforms the
existing platform PapersWithCode in dataset usage extraction but also achieves
about 90\% precision and recall in entity resolution tasks. Moreover, we have
deployed ChatPD to continuously extract which datasets are used in papers, and
provide a dataset discovery service, such as task-specific dataset queries and
similar dataset recommendations. We open source ChatPD and the current
paper-dataset network on this [GitHub
repository]{https://github.com/ChatPD-web/ChatPD}.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [196] [On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models](https://arxiv.org/abs/2505.22598)
*Luca Maria Del Bono,Federico Ricci-Tersenghi,Francesco Zamponi*

Main category: cond-mat.dis-nn

TL;DR: This paper conducts a comprehensive analytic study of the Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model, providing theoretical predictions on the best procedure and establishing a clear theoretical basis for integrating machine learning techniques into Monte Carlo sampling and optimization.


<details>
  <summary>Details</summary>
Motivation: To address the lack of wide theoretical understanding in applying machine learning techniques to aid the simulation of hard-to-sample systems, which may lead to suboptimal implementations.

Method: Complete analytic study of the Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model, including description of optimal weights and training under Gradient Descent optimization, and comparison with and without local Metropolis Monte Carlo steps.

Result: Theoretical predictions on the best procedure to apply in this case are provided, contributing to a clear theoretical basis for integrating machine learning techniques into Monte Carlo sampling and optimization.

Conclusion: This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization.

Abstract: Recent years have seen a rise in the application of machine learning
techniques to aid the simulation of hard-to-sample systems that cannot be
studied using traditional methods. Despite the introduction of many different
architectures and procedures, a wide theoretical understanding is still
lacking, with the risk of suboptimal implementations. As a first step to
address this gap, we provide here a complete analytic study of the widely-used
Sequential Tempering procedure applied to a shallow MADE architecture for the
Curie-Weiss model. The contribution of this work is twofold: firstly, we give a
description of the optimal weights and of the training under Gradient Descent
optimization. Secondly, we compare what happens in Sequential Tempering with
and without the addition of local Metropolis Monte Carlo steps. We are thus
able to give theoretical predictions on the best procedure to apply in this
case. This work establishes a clear theoretical basis for the integration of
machine learning techniques into Monte Carlo sampling and optimization.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [197] [What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization](https://arxiv.org/abs/2505.21692)
*Omar Bennouna,Amine Bennouna,Saurabh Amin,Asuman Ozdaglar*

Main category: math.OC

TL;DR: 研究了数据集在解决特定决策任务中的信息量问题，特别是在线性规划中，通过不确定性集合来判断数据集是否足以得出最优决策，并提出了一种几何特征方法和实际算法以构建最小成本的充分数据集。


<details>
  <summary>Details</summary>
Motivation: 评估数据集对于给定决策任务的信息量及有效性，特别是在存在未知参数影响任务结果的情况下。

Method: 专注于线性规划问题，通过几何特征分析成本向量方向与任务约束和不确定性集合的关系，提出一种算法构建最小或最低成本的充分数据集。

Result: 揭示了精心挑选的小型数据集往往能够完全决定最优决策，为任务感知的数据选择提供了理论基础。

Conclusion: 提出了一个尖锐的几何特征用于判断数据集是否充分支持最优决策，并展示了如何构建最小成本的充分数据集。

Abstract: We study the fundamental question of how informative a dataset is for solving
a given decision-making task. In our setting, the dataset provides partial
information about unknown parameters that influence task outcomes. Focusing on
linear programs, we characterize when a dataset is sufficient to recover an
optimal decision, given an uncertainty set on the cost vector. Our main
contribution is a sharp geometric characterization that identifies the
directions of the cost vector that matter for optimality, relative to the task
constraints and uncertainty set. We further develop a practical algorithm that,
for a given task, constructs a minimal or least-costly sufficient dataset. Our
results reveal that small, well-chosen datasets can often fully determine
optimal decisions -- offering a principled foundation for task-aware data
selection.

</details>


### [198] [PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective](https://arxiv.org/abs/2505.21799)
*Tim Tsz-Kit Lau,Qi Long,Weijie Su*

Main category: math.OC

TL;DR: The paper presents PolarGrad, a new optimization method based on the polar decomposition of matrix-valued gradients, which outperforms Adam and Muon in language model pre-training tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient optimization methods in deep learning models and explore structure-aware preconditioned optimizers that demonstrate faster convergence.

Method: A unifying framework is introduced to analyze 'matrix-aware' preconditioned methods, distinguishing between vector-based and matrix-structured preconditioning strategies. Based on this framework, PolarGrad is developed, leveraging polar decomposition algorithms for enhanced convergence.

Result: PolarGrad outperforms both Adam and Muon across diverse matrix optimization problems and language model pre-training tasks.

Conclusion: PolarGrad represents an advancement in preconditioned optimization methods, offering improved performance through its consideration of gradient matrix structures.

Abstract: The ever-growing scale of deep learning models and datasets underscores the
critical importance of efficient optimization methods. While preconditioned
gradient methods such as Adam and AdamW are the de facto optimizers for
training neural networks and large language models, structure-aware
preconditioned optimizers like Shampoo and Muon, which utilize the matrix
structure of gradients, have demonstrated promising evidence of faster
convergence. In this paper, we introduce a unifying framework for analyzing
"matrix-aware" preconditioned methods, which not only sheds light on the
effectiveness of Muon and related optimizers but also leads to a class of new
structure-aware preconditioned methods. A key contribution of this framework is
its precise distinction between preconditioning strategies that treat neural
network weights as vectors (addressing curvature anisotropy) versus those that
consider their matrix structure (addressing gradient anisotropy). This
perspective provides new insights into several empirical phenomena in language
model pre-training, including Adam's training instabilities, Muon's accelerated
convergence, and the necessity of learning rate warmup for Adam. Building upon
this framework, we introduce PolarGrad, a new class of preconditioned
optimization methods based on the polar decomposition of matrix-valued
gradients. As a special instance, PolarGrad includes Muon with updates scaled
by the nuclear norm of the gradients. We provide numerical implementations of
these methods, leveraging efficient numerical polar decomposition algorithms
for enhanced convergence. Our extensive evaluations across diverse matrix
optimization problems and language model pre-training tasks demonstrate that
PolarGrad outperforms both Adam and Muon.

</details>


### [199] [PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning](https://arxiv.org/abs/2505.22085)
*Arnulf Jentzen,Julian Kranz,Adrian Riekert*

Main category: math.OC

TL;DR: The paper introduces Parallel Averaged ADAM (PADAM), an averaging approach that computes different averaged variants of the ADAM optimizer in parallel and dynamically selects the variant with the smallest optimization error during training. PADAM achieves comparable or better performance than existing optimizers in various stochastic optimization and deep neural network learning problems, particularly in scientific machine learning applications.


<details>
  <summary>Details</summary>
Motivation: Existing averaging techniques for accelerating stochastic gradient descent (SGD) methods require parameter tuning based on specific optimization problems to achieve the smallest optimization error. The authors aim to develop a more adaptive approach that can automatically select the best averaging variant without additional gradient evaluations.

Method: The proposed method, PADAM, computes multiple averaged versions of the ADAM optimizer in parallel using the same underlying gradients as the standard ADAM optimizer. During training, it dynamically selects the variant with the smallest optimization error. This avoids the need for additional gradient evaluations compared to standard ADAM.

Result: PADAM was tested on 13 stochastic optimization and DNN learning problems, including physics-informed neural networks, deep Galerkin methods, and optimal control problems. In nearly all cases, PADAM achieved the smallest optimization error, either alone or alongside other optimizers.

Conclusion: PADAM is a promising optimizer for scientific machine learning problems due to its ability to achieve small optimization errors without requiring additional gradient computations. The results motivate further research into adaptive averaging procedures for training deep neural networks.

Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential
movering averaging (EMA) are powerful approaches to accelerate optimization
procedures of stochastic gradient descent (SGD) optimization methods such as
the popular ADAM optimizer. However, depending on the specific optimization
problem under consideration, the type and the parameters for the averaging need
to be adjusted to achieve the smallest optimization error. In this work we
propose an averaging approach, which we refer to as parallel averaged ADAM
(PADAM), in which we compute parallely different averaged variants of ADAM and
during the training process dynamically select the variant with the smallest
optimization error. A central feature of this approach is that this procedure
requires no more gradient evaluations than the usual ADAM optimizer as each of
the averaged trajectories relies on the same underlying ADAM trajectory and
thus on the same underlying gradients. We test the proposed PADAM optimizer in
13 stochastic optimization and deep neural network (DNN) learning problems and
compare its performance with known optimizers from the literature such as
standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In
particular, we apply the compared optimizers to physics-informed neural
network, deep Galerkin, deep backward stochastic differential equation and deep
Kolmogorov approximations for boundary value partial differential equation
problems from scientific machine learning, as well as to DNN approximations for
optimal control and optimal stopping problems. In nearly all of the considered
examples PADAM achieves, sometimes among others and sometimes exclusively,
essentially the smallest optimization error. This work thus strongly suggest to
consider PADAM for scientific machine learning problems and also motivates
further research for adaptive averaging procedures within the training of DNNs.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [200] [Network classification through random walks](https://arxiv.org/abs/2505.21706)
*Gonzalo Travieso,Joao Merenda,Odemir M. Bruno*

Main category: cs.SI

TL;DR: This paper proposes a new method for network classification using random walk statistics, which performs well in many cases but has some limitations.


<details>
  <summary>Details</summary>
Motivation: To address the problem of inferring the type of system represented by a network based on its structure.

Method: The study introduces an approach to characterize networks using statistics from random walks and compares it with other state-of-the-art feature extraction methods.

Result: The proposed method is effective in many cases and often outperforms existing approaches, although some limitations are observed across certain datasets.

Conclusion: The novel approach using random walk statistics can be a valuable tool for network classification.

Abstract: Network models have been widely used to study diverse systems and analyze
their dynamic behaviors. Given the structural variability of networks, an
intriguing question arises: Can we infer the type of system represented by a
network based on its structure? This classification problem involves extracting
relevant features from the network. Existing literature has proposed various
methods that combine structural measurements and dynamical processes for
feature extraction. In this study, we introduce a novel approach to
characterize networks using statistics from random walks, which can be
particularly informative about network properties. We present the employed
statistical metrics and compare their performance on multiple datasets with
other state-of-the-art feature extraction methods. Our results demonstrate that
the proposed method is effective in many cases, often outperforming existing
approaches, although some limitations are observed across certain datasets.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [201] [Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size](https://arxiv.org/abs/2505.22384)
*Foivos Fioravantes,Harmender Gahlawat,Nikolaos Melissinos*

Main category: cs.DS

TL;DR: The paper explores a version of the Coalition Formation problem where teams have bounded sizes, providing intractability results and efficient algorithms that could be practical for large inputs. The main contribution is an algorithm optimized for tree-like structures with small teams, proven to be asymptotically optimal.


<details>
  <summary>Details</summary>
Motivation: To split a group of agents into teams efficiently considering each agent's preferences about their teammates, while ensuring each team has a bounded size.

Method: Systematic algorithmic study offering multiple exact algorithms that scale well with input growth (FPT). Developed an algorithm optimized for tree-like structures (bounded treewidth) with small teams.

Result: An algorithm dealing efficiently with tree-like structures for small teams was developed and proven to be asymptotically optimal. No other algorithm can significantly outperform it under reasonable theoretical assumptions, even for star-like structures (bounded vertex cover number).

Conclusion: A systematic study on a bounded size team Coalition Formation problem was conducted, resulting in intractability insights and practical algorithms, with a key algorithm proven to be asymptotically optimal.

Abstract: Imagine we want to split a group of agents into teams in the most
\emph{efficient} way, considering that each agent has their own preferences
about their teammates. This scenario is modeled by the extensively studied
\textsc{Coalition Formation} problem. Here, we study a version of this problem
where each team must additionally be of bounded size.
  We conduct a systematic algorithmic study, providing several intractability
results as well as multiple exact algorithms that scale well as the input grows
(FPT), which could prove useful in practice.
  Our main contribution is an algorithm that deals efficiently with tree-like
structures (bounded \emph{treewidth}) for ``small'' teams. We complement this
result by proving that our algorithm is asymptotically optimal. Particularly,
there can be no algorithm that vastly outperforms the one we present, under
reasonable theoretical assumptions, even when considering star-like structures
(bounded \emph{vertex cover number}).

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [202] [Uncovering Bottlenecks and Optimizing Scientific Lab Workflows with Cycle Time Reduction Agents](https://arxiv.org/abs/2505.21534)
*Yao Fehlis*

Main category: cs.MA

TL;DR: The paper introduces Cycle Time Reduction Agents (CTRA), a LangGraph-based agentic workflow for automating lab operational metrics analysis. CTRA includes Question Creation Agent, Operational Metrics Agents, and Insights Agents to identify bottlenecks in lab processes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of optimizing workflows in scientific laboratories due to the complexity and volume of tasks such as compound screening and assay execution.

Method: CTRA is composed of three main components: Question Creation Agent for initiating analysis, Operational Metrics Agents for data extraction and validation, and Insights Agents for reporting and visualization.

Result: CTRA's performance is evaluated on a lab dataset, demonstrating its potential to accelerate pharmaceutical and biotechnological development.

Conclusion: CTRA provides a scalable framework for reducing cycle times in scientific labs.

Abstract: Scientific laboratories, particularly those in pharmaceutical and
biotechnology companies, encounter significant challenges in optimizing
workflows due to the complexity and volume of tasks such as compound screening
and assay execution. We introduce Cycle Time Reduction Agents (CTRA), a
LangGraph-based agentic workflow designed to automate the analysis of lab
operational metrics. CTRA comprises three main components: the Question
Creation Agent for initiating analysis, Operational Metrics Agents for data
extraction and validation, and Insights Agents for reporting and visualization,
identifying bottlenecks in lab processes. This paper details CTRA's
architecture, evaluates its performance on a lab dataset, and discusses its
potential to accelerate pharmaceutical and biotechnological development. CTRA
offers a scalable framework for reducing cycle times in scientific labs.

</details>


### [203] [Streamlining Resilient Kubernetes Autoscaling with Multi-Agent Systems via an Automated Online Design Framework](https://arxiv.org/abs/2505.21559)
*Julien Soulé,Jean-Paul Jamont,Michel Occello,Louis-Marie Traonouez,Paul Théron*

Main category: cs.MA

TL;DR: In cloud-native systems, Kubernetes clusters face operational resilience challenges. Conventional HPAs and RL-based methods have limitations. This paper proposes an HPA Multi-Agent System (MAS) with a four-phase framework to enhance resilience.


<details>
  <summary>Details</summary>
Motivation: Kubernetes clusters in cloud-native systems often encounter operational resilience issues due to poor workload management, which are exacerbated in adversarial conditions like DDoS attacks. Existing solutions, such as conventional HPAs and reinforcement learning-based methods, fail to address these challenges comprehensively.

Method: The method involves decomposing the goal of maintaining operational resilience into failure-specific sub-goals managed by collaborative agents forming an HPA MAS. An automated, four-phase online framework is introduced: 1) modeling a digital twin from cluster traces; 2) training agents in simulation for specific failure contexts; 3) analyzing agent behaviors for explainability; and 4) transferring learned policies to the real cluster.

Result: Experimental results indicate that the proposed HPA MASs surpass three state-of-the-art HPA systems in maintaining operational resilience under various adversarial conditions in a complex cluster.

Conclusion: The proposed HPA MAS with its four-phase framework effectively enhances operational resilience in Kubernetes clusters under adversarial conditions, outperforming existing state-of-the-art systems.

Abstract: In cloud-native systems, Kubernetes clusters with interdependent services
often face challenges to their operational resilience due to poor workload
management issues such as resource blocking, bottlenecks, or continuous pod
crashes. These vulnerabilities are further amplified in adversarial scenarios,
such as Distributed Denial-of-Service attacks (DDoS). Conventional Horizontal
Pod Autoscaling (HPA) approaches struggle to address such dynamic conditions,
while reinforcement learning-based methods, though more adaptable, typically
optimize single goals like latency or resource usage, neglecting broader
failure scenarios. We propose decomposing the overarching goal of maintaining
operational resilience into failure-specific sub-goals delegated to
collaborative agents, collectively forming an HPA Multi-Agent System (MAS). We
introduce an automated, four-phase online framework for HPA MAS design: 1)
modeling a digital twin built from cluster traces; 2) training agents in
simulation using roles and missions tailored to failure contexts; 3) analyzing
agent behaviors for explainability; and 4) transferring learned policies to the
real cluster. Experimental results demonstrate that the generated HPA MASs
outperform three state-of-the-art HPA systems in sustaining operational
resilience under various adversarial conditions in a proposed complex cluster.

</details>


### [204] [Herd Behavior: Investigating Peer Influence in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.21588)
*Young-Min Cho,Sharath Chandra Guntuku,Lyle Ungar*

Main category: cs.MA

TL;DR: The paper explores herd behavior in LLM-based multi-agent systems through controlled experiments, revealing factors like confidence gaps and peer information format that impact conformism, and demonstrating the potential to control herd tendencies for better collaboration.


<details>
  <summary>Details</summary>
Motivation: To understand the dynamics of peer influence and herd behavior within LLM-based multi-agent interactions, as this area remains underexplored despite extensive study on individual model behavior.

Method: Conducting a series of controlled experiments examining how herd behaviors are shaped by various factors including self-confidence vs. perceived peer confidence, presentation format of peer information, and the degree of herd behavior.

Result: Discovering significant impacts from confidence gaps and peer information formats on herd behavior, along with the ability to systematically control herd tendencies to improve collaborative outcomes.

Conclusion: The findings provide new insights into social dynamics in LLM-based systems and suggest pathways for designing more effective and adaptive multi-agent collaboration frameworks.

Abstract: Recent advancements in Large Language Models (LLMs) have enabled the
emergence of multi-agent systems where LLMs interact, collaborate, and make
decisions in shared environments. While individual model behavior has been
extensively studied, the dynamics of peer influence in such systems remain
underexplored. In this paper, we investigate herd behavior, the tendency of
agents to align their outputs with those of their peers, within LLM-based
multi-agent interactions. We present a series of controlled experiments that
reveal how herd behaviors are shaped by multiple factors. First, we show that
the gap between self-confidence and perceived confidence in peers significantly
impacts an agent's likelihood to conform. Second, we find that the format in
which peer information is presented plays a critical role in modulating the
strength of herd behavior. Finally, we demonstrate that the degree of herd
behavior can be systematically controlled, and that appropriately calibrated
herd tendencies can enhance collaborative outcomes. These findings offer new
insights into the social dynamics of LLM-based systems and open pathways for
designing more effective and adaptive multi-agent collaboration frameworks.

</details>


### [205] [Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation](https://arxiv.org/abs/2505.21880)
*Yu-Lun Song,Chung-En Tsern,Che-Cheng Wu,Yu-Ming Chang,Syuan-Bo Huang,Wei-Chu Chen,Michael Chia-Liang Lin,Yu-Ta Lin*

Main category: cs.MA

TL;DR: This study integrates LLM with ABM for urban mobility simulation, enhancing agent diversity and realism through synthetic population profiles, location allocation, and personalized routes. It models individual behaviors and large-scale mobility patterns in Taipei City, providing insights for urban planning.


<details>
  <summary>Details</summary>
Motivation: To improve the realism and diversity of agents in urban mobility simulations beyond traditional rule-based ABM.

Method: Integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM) to generate synthetic population profiles, allocate locations, and simulate personalized routes.

Result: The simulation successfully models individual behaviors and large-scale mobility patterns in Taipei City, offering actionable insights like route heat maps and mode-specific indicators for urban planners.

Conclusion: The proposed framework shows promise in enhancing urban mobility simulations; future work will focus on robust validation frameworks for accuracy and reliability in urban planning applications.

Abstract: This study presents an innovative approach to urban mobility simulation by
integrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).
Unlike traditional rule-based ABM, the proposed framework leverages LLM to
enhance agent diversity and realism by generating synthetic population
profiles, allocating routine and occasional locations, and simulating
personalized routes. Using real-world data, the simulation models individual
behaviors and large-scale mobility patterns in Taipei City. Key insights, such
as route heat maps and mode-specific indicators, provide urban planners with
actionable information for policy-making. Future work focuses on establishing
robust validation frameworks to ensure accuracy and reliability in urban
planning applications.

</details>


### [206] [Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2505.21985)
*Naoto Yoshida,Tadahiro Taniguchi*

Main category: cs.MA

TL;DR: 本论文提出了MARL-CPC框架，通过集体预测编码（CPC）消息学习模型，实现去中心化独立代理之间的通信，无需参数共享。与传统方法不同，MARL-CPC将消息与状态推理联系起来，支持非合作、奖励独立环境下的通信。论文引入了Bandit-CPC和IPPO-CPC两种算法，并在非合作多代理强化学习任务中进行了评估。实验结果表明，这两种算法在消息无直接发送者利益的情况下，仍然能够建立有效的通信机制，展示了MARL-CPC在复杂去中心化环境中促进协调的潜力。


<details>
  <summary>Details</summary>
Motivation: 多代理强化学习（MARL）中，有效的通信可以提升代理的表现，尤其是在部分可观测环境下。然而，现有的通信方法通常依赖于参数共享或假设合作环境，限制了其在去中心化和非合作场景中的应用。因此，需要一种新的框架来解决这些问题。

Method: 论文提出了一种名为MARL-CPC的框架，该框架基于集体预测编码（CPC）的消息学习模型，允许完全去中心化的独立代理之间进行通信，而无需参数共享。此外，论文还引入了两种算法：Bandit-CPC和IPPO-CPC，分别用于非合作MARL任务的评估。这些算法将消息与状态推理联系起来，从而支持非合作和奖励独立环境下的通信。

Result: 实验结果表明，Bandit-CPC和IPPO-CPC在非合作MARL任务中均优于标准的消息作为动作的方法。即使在消息对发送者没有直接利益的情况下，MARL-CPC也能建立有效的通信机制。

Conclusion: MARL-CPC框架成功地实现了去中心化独立代理之间的通信，无需参数共享，并且能够在非合作、奖励独立的环境中工作。这为复杂、去中心化环境中的协调提供了新的可能性。

Abstract: In multi-agent reinforcement learning (MARL), effective communication
improves agent performance, particularly under partial observability. We
propose MARL-CPC, a framework that enables communication among fully
decentralized, independent agents without parameter sharing. MARL-CPC
incorporates a message learning model based on collective predictive coding
(CPC) from emergent communication research. Unlike conventional methods that
treat messages as part of the action space and assume cooperation, MARL-CPC
links messages to state inference, supporting communication in non-cooperative,
reward-independent settings. We introduce two algorithms -Bandit-CPC and
IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that
both outperform standard message-as-action approaches, establishing effective
communication even when messages offer no direct benefit to the sender. These
results highlight MARL-CPC's potential for enabling coordination in complex,
decentralized environments.

</details>


### [207] [Sentiment Simulation using Generative AI Agents](https://arxiv.org/abs/2505.22125)
*Melrose Tia,Jezreel Sophia Lanuzo,Lei Rigi Baltazar,Marie Joy Lopez-Relente,Diwa Malaya Quiñones,Jason Albia*

Main category: cs.MA

TL;DR: The paper presents a sentiment simulation framework using generative AI agents with psychological profiles, achieving high accuracy in replicating human responses and sentiment ratings.


<details>
  <summary>Details</summary>
Motivation: Traditional sentiment analysis methods are limited by their reliance on surface-level linguistic patterns and retrospective data, which restricts their ability to capture the psychological and contextual drivers of human sentiment.

Method: The framework involves three stages: (1) agent embodiment through categorical or contextualized encodings, (2) exposure to real-world political and economic scenarios, and (3) generation of sentiment ratings with explanatory rationales. Agents are instantiated from a survey of 2,485 Filipino respondents, incorporating sociodemographic information and psychological constructs.

Result: Contextualized encoding achieved 92% alignment with original survey responses. In sentiment simulation tasks, agents reached 81%-86% accuracy compared to ground truth sentiment, with contextualized profile encodings significantly outperforming categorical ones. The simulation results were consistent across repeated trials and resilient to variations in scenario framing.

Conclusion: The study establishes a scalable framework for sentiment modeling using AI agents grounded in psychological profiles, marking a shift from retrospective classification to prospective and dynamic simulation.

Abstract: Traditional sentiment analysis relies on surface-level linguistic patterns
and retrospective data, limiting its ability to capture the psychological and
contextual drivers of human sentiment. These limitations constrain its
effectiveness in applications that require predictive insight, such as policy
testing, narrative framing, and behavioral forecasting. We present a robust
framework for sentiment simulation using generative AI agents embedded with
psychologically rich profiles. Agents are instantiated from a nationally
representative survey of 2,485 Filipino respondents, combining sociodemographic
information with validated constructs of personality traits, values, beliefs,
and socio-political attitudes. The framework includes three stages: (1) agent
embodiment via categorical or contextualized encodings, (2) exposure to
real-world political and economic scenarios, and (3) generation of sentiment
ratings accompanied by explanatory rationales. Using Quadratic Weighted
Accuracy (QWA), we evaluated alignment between agent-generated and human
responses. Contextualized encoding achieved 92% alignment in replicating
original survey responses. In sentiment simulation tasks, agents reached
81%--86% accuracy against ground truth sentiment, with contextualized profile
encodings significantly outperforming categorical (p < 0.0001, Cohen's d =
0.70). Simulation results remained consistent across repeated trials
(+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676,
Cohen's d = 0.02). Our findings establish a scalable framework for sentiment
modeling through psychographically grounded AI agents. This work signals a
paradigm shift in sentiment analysis from retrospective classification to
prospective and dynamic simulation grounded in psychology of sentiment
formation.

</details>


### [208] [Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2505.22467)
*Jiaxi Yang,Mengqi Zhang,Yiqiao Jin,Hao Chen,Qingsong Wen,Lu Lin,Yi He,Weijie Xu,James Evans,Jindong Wang*

Main category: cs.MA

TL;DR: Large Language Model-based Multi-Agent Systems (MASs) require optimal structural organization for effective cooperation, which is currently underexplored. This paper proposes a three-stage framework (agent selection, structure profiling, topology synthesis) to develop topology-aware MASs, highlighting new research opportunities and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored aspect of how agents in MASs should be structurally organized for optimal cooperation.

Method: Propose a three-stage framework consisting of agent selection, structure profiling, and topology synthesis to develop topology-aware MASs.

Result: Redirects focus on structural organization in MASs, offering new research opportunities in language models, reinforcement learning, graph learning, and generative modeling.

Conclusion: The perspective and framework provided could offer critical new insights into the era of agentic AI.

Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a
powerful paradigm for tackling complex tasks through collaborative
intelligence. Nevertheless, the question of how agents should be structurally
organized for optimal cooperation remains largely unexplored. In this position
paper, we aim to gently redirect the focus of the MAS research community toward
this critical dimension: develop topology-aware MASs for specific tasks.
Specifically, the system consists of three core components - agents,
communication links, and communication patterns - that collectively shape its
coordination performance and efficiency. To this end, we introduce a
systematic, three-stage framework: agent selection, structure profiling, and
topology synthesis. Each stage would trigger new research opportunities in
areas such as language models, reinforcement learning, graph learning, and
generative modeling; together, they could unleash the full potential of MASs in
complicated real-world applications. Then, we discuss the potential challenges
and opportunities in the evaluation of multiple systems. We hope our
perspective and framework can offer critical new insights in the era of agentic
AI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [209] [Enhancing Vision Transformer Explainability Using Artificial Astrocytes](https://arxiv.org/abs/2505.21513)
*Nicolas Echevarrieta-Catalan,Ana Ribas-Rodriguez,Francisco Cedron,Odelia Schwartz,Vanessa Aguiar-Pulido*

Main category: cs.CV

TL;DR: The paper introduces Vision Transformer with artificial Astrocytes (ViTA), a training-free method inspired by neuroscience, which enhances pretrained neural networks' reasoning for more human-aligned explanations. Using Grad-CAM and Grad-CAM++ on the ClickMe dataset, it shows significant improvements in aligning model explanations with human perception.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explainability in complex machine learning models and the limitations of current XAI techniques or explainability constraints.

Method: Propose ViTA, a training-free approach incorporating artificial astrocytes into pretrained deep neural networks to improve their reasoning and generate more human-aligned explanations. Evaluated using Grad-CAM and Grad-CAM++ techniques and compared against standard Vision Transformer.

Result: ViTA leads to statistically significant improvements in aligning model explanations with human perception across all XAI techniques and metrics tested on the ClickMe dataset.

Conclusion: Incorporating artificial astrocytes enhances the alignment of model explanations with human perception, offering a promising direction for improving explainability in machine learning models.

Abstract: Machine learning models achieve high precision, but their decision-making
processes often lack explainability. Furthermore, as model complexity
increases, explainability typically decreases. Existing efforts to improve
explainability primarily involve developing new eXplainable artificial
intelligence (XAI) techniques or incorporating explainability constraints
during training. While these approaches yield specific improvements, their
applicability remains limited. In this work, we propose the Vision Transformer
with artificial Astrocytes (ViTA). This training-free approach is inspired by
neuroscience and enhances the reasoning of a pretrained deep neural network to
generate more human-aligned explanations. We evaluated our approach employing
two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a
standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the
similarity between the heatmaps produced by the XAI techniques and a
(human-aligned) ground truth. Our results consistently demonstrate that
incorporating artificial astrocytes enhances the alignment of model
explanations with human perception, leading to statistically significant
improvements across all XAI techniques and metrics utilized.

</details>


### [210] [Do DeepFake Attribution Models Generalize?](https://arxiv.org/abs/2505.21520)
*Spiros Baxavanakis,Manos Schinas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: Recent advancements in DeepFake generation have lowered the barrier for creating synthetic media, posing a threat to online information integrity. State-of-the-art research on DeepFake detection primarily focuses on binary detection models, which treat all manipulation techniques as equivalent. This work leverages five state-of-the-art backbone models and conducts extensive experiments across six DeepFake datasets, comparing binary and multi-class models in terms of cross-dataset generalization, examining the accuracy of attribution models, and assessing the effectiveness of contrastive methods.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the limitation of current DeepFake detection models that treat all manipulation techniques equivalently, despite different methods introducing distinct artifacts and visual cues. The authors aim to explore DeepFake attribution models, which provide the specific manipulation method employed, enhancing both the perceived trustworthiness and explainability for end users.

Method: The authors leverage five state-of-the-art backbone models and conduct extensive experiments across six DeepFake datasets. They compare binary and multi-class models in terms of cross-dataset generalization, examine the accuracy of attribution models in detecting seen manipulation methods in unknown datasets, and assess the effectiveness of contrastive methods in improving cross-dataset generalization performance.

Result: The findings indicate that binary models demonstrate better generalization abilities. Larger models, contrastive methods, and higher data quality can lead to performance improvements in attribution models.

Conclusion: This study highlights the importance of DeepFake attribution models and provides insights into their performance and potential improvements. The code for this work is available on GitHub.

Abstract: Recent advancements in DeepFake generation, along with the proliferation of
open-source tools, have significantly lowered the barrier for creating
synthetic media. This trend poses a serious threat to the integrity and
authenticity of online information, undermining public trust in institutions
and media. State-of-the-art research on DeepFake detection has primarily
focused on binary detection models. A key limitation of these models is that
they treat all manipulation techniques as equivalent, despite the fact that
different methods introduce distinct artifacts and visual cues. Only a limited
number of studies explore DeepFake attribution models, although such models are
crucial in practical settings. By providing the specific manipulation method
employed, these models could enhance both the perceived trustworthiness and
explainability for end users. In this work, we leverage five state-of-the-art
backbone models and conduct extensive experiments across six DeepFake datasets.
First, we compare binary and multi-class models in terms of cross-dataset
generalization. Second, we examine the accuracy of attribution models in
detecting seen manipulation methods in unknown datasets, hence uncovering data
distribution shifts on the same DeepFake manipulations. Last, we assess the
effectiveness of contrastive methods in improving cross-dataset generalization
performance. Our findings indicate that while binary models demonstrate better
generalization abilities, larger models, contrastive methods, and higher data
quality can lead to performance improvements in attribution models. The code of
this work is available on GitHub.

</details>


### [211] [CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures](https://arxiv.org/abs/2505.21522)
*Shan Gao,Zhiqiang Wu,Yawen Niu,Xiaotao Li,Qingqing Xu*

Main category: cs.CV

TL;DR: This paper proposes a hardware-algorithm co-design framework for video denoising, including CIM-NET and CIM-CONV, which significantly reduces MVM operations on CIM chips while maintaining competitive performance.


<details>
  <summary>Details</summary>
Motivation: Deep neural network-based video denoising shows great performance, but deploying state-of-the-art models on edge devices is challenging due to real-time and energy efficiency requirements. Existing DNN models are not optimized for CIM architectural constraints, limiting their acceleration potential during inference.

Method: The authors propose CIM-NET, a CIM-Aware Architecture optimized for large receptive field operation and CIM's crossbar-based MVM acceleration, and CIM-CONV, a pseudo-convolutional operator that integrates slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction.

Result: Compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with only a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original while maintaining competitive PSNR (35.11 dB vs. 35.56 dB).

Conclusion: The proposed hardware-algorithm co-design framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance.

Abstract: While deep neural network (DNN)-based video denoising has demonstrated
significant performance, deploying state-of-the-art models on edge devices
remains challenging due to stringent real-time and energy efficiency
requirements. Computing-in-Memory (CIM) chips offer a promising solution by
integrating computation within memory cells, enabling rapid matrix-vector
multiplication (MVM). However, existing DNN models are often designed without
considering CIM architectural constraints, thus limiting their acceleration
potential during inference. To address this, we propose a hardware-algorithm
co-design framework incorporating two innovations: (1) a CIM-Aware
Architecture, CIM-NET, optimized for large receptive field operation and CIM's
crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator,
CIM-CONV, used within CIM-NET to integrate slide-based processing with fully
connected transformations for high-quality feature extraction and
reconstruction. This framework significantly reduces the number of MVM
operations, improving inference speed on CIM chips while maintaining
competitive performance. Experimental results indicate that, compared to the
conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM
operations with a slight decrease in denoising performance. With a stride value
of 8, CIM-NET reduces MVM operations to 1/77th of the original, while
maintaining competitive PSNR (35.11 dB vs. 35.56 dB

</details>


### [212] [UniDB++: Fast Sampling of Unified Diffusion Bridge](https://arxiv.org/abs/2505.21528)
*Mokai Pan,Kaizhen Zhu,Yuexin Ma,Yanwei Fu,Jingyi Yu,Jingya Wang,Ye Shi*

Main category: cs.CV

TL;DR: UniDB++是一种无需训练的采样算法，通过推导UniDB反时间SDE的精确闭式解，显著减少误差积累，提高生成质量，并将采样步骤减少20倍。此外，采用更稳定的数据预测模型和SDE-Corrector机制，在低步长条件下保持感知质量。实验表明，UniDB++在图像恢复任务中表现出色，超越了基于欧拉方法的模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Unified Diffusion Bridge（UniDB）框架能够实现高保真图像生成，但其依赖迭代的Euler采样方法导致推理速度慢且计算成本高。现有的加速技术无法解决其独特的挑战，如缺失的终端均值约束和SOC特定的惩罚系数。

Method: 提出了一种无需训练的采样算法UniDB++，通过推导UniDB反时间SDE的精确闭式解来减少误差积累，从而实现高质量生成。还用更稳定的数据预测模型取代传统的噪声预测，并引入SDE-Corrector机制以在低步长条件下维持感知质量。

Result: 实验表明，UniDB++在图像恢复任务中表现出色，相较于Euler-based方法，在保真度和速度上均有提升，并显著减少了推理时间。此外，UniDB++与现有的扩散桥加速方法相一致，并能在某些理论条件下恢复DBIMs作为特殊情况。

Conclusion: UniDB++弥合了SOC驱动的扩散桥模型在理论普遍性和实际效率之间的差距，为图像生成任务提供了更快、更高效的解决方案。

Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with
the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image
generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's
reliance on iterative Euler sampling methods results in slow, computationally
expensive inference, while existing acceleration techniques for diffusion or
diffusion bridge models fail to address its unique challenges: missing terminal
mean constraints and SOC-specific penalty coefficients in its SDEs. We present
UniDB++, a training-free sampling algorithm that significantly improves upon
these limitations. The method's key advancement comes from deriving exact
closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the
error accumulation inherent in Euler approximations and enabling high-quality
generation with up to 20$\times$ fewer sampling steps. This method is further
complemented by replacing conventional noise prediction with a more stable data
prediction model, along with an SDE-Corrector mechanism that maintains
perceptual quality for low-step regimes (5-10 steps). Additionally, we
demonstrate that UniDB++ aligns with existing diffusion bridge acceleration
methods by evaluating their update rules, and UniDB++ can recover DBIMs as
special cases under some theoretical conditions. Experiments demonstrate
UniDB++'s state-of-the-art performance in image restoration tasks,
outperforming Euler-based methods in fidelity and speed while reducing
inference time significantly. This work bridges the gap between theoretical
generality and practical efficiency in SOC-driven diffusion bridge models. Our
code is available at https://github.com/2769433owo/UniDB-plusplus.

</details>


### [213] [How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control](https://arxiv.org/abs/2505.21531)
*Kunhang Li,Jason Naradowsky,Yansong Feng,Yusuke Miyao*

Main category: cs.CV

TL;DR: Large Language Models (LLMs) are explored for their ability to generate human motion plans and avatar animations from instructions, revealing strengths in high-level planning but weaknesses in precise body part positioning.


<details>
  <summary>Details</summary>
Motivation: To investigate the extent of Large Language Models' understanding and generation capabilities regarding human motion knowledge through 3D avatar control.

Method: Given a motion instruction, LLMs generate a high-level movement plan followed by specifying body part positions in each step. These positions are linearly interpolated into avatar animations which serve as a verification tool for human evaluators.

Result: LLMs excel at interpreting high-level body movements but struggle with precise body part positioning. They provide reasonable approximations for general spatial descriptions yet fail in handling precise spatial specifications and parameters needed for avatar control. However, they show promise in conceptualizing creative motions and distinguishing culturally-specific motion patterns.

Conclusion: LLMs demonstrate potential in generating high-level human motion plans but face challenges in precise low-level planning, indicating their current limitations in detailed spatial-temporal parameter handling.

Abstract: We explore Large Language Models (LLMs)' human motion knowledge through 3D
avatar control. Given a motion instruction, we prompt LLMs to first generate a
high-level movement plan with consecutive steps (High-level Planning), then
specify body part positions in each step (Low-level Planning), which we
linearly interpolate into avatar animations as a clear verification lens for
human evaluators. Through carefully designed 20 representative motion
instructions with full coverage of basic movement primitives and balanced body
part usage, we conduct comprehensive evaluations including human assessment of
both generated animations and high-level movement plans, as well as automatic
comparison with oracle positions in low-level planning. We find that LLMs are
strong at interpreting the high-level body movements but struggle with precise
body part positioning. While breaking down motion queries into atomic
components improves planning performance, LLMs have difficulty with multi-step
movements involving high-degree-of-freedom body parts. Furthermore, LLMs
provide reasonable approximation for general spatial descriptions, but fail to
handle precise spatial specifications in text, and the precise spatial-temporal
parameters needed for avatar control. Notably, LLMs show promise in
conceptualizing creative motions and distinguishing culturally-specific motion
patterns.

</details>


### [214] [EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media](https://arxiv.org/abs/2505.21532)
*Ismail Erbas,Ferhat Demirkiran,Karthik Swaminathan,Naigang Wang,Navid Ibtehaj Nizam,Stefan T. Radev,Kaoutar El Maghraoui,Xavier Intes,Vikas Pandey*

Main category: cs.CV

TL;DR: This paper proposes a Physics-Guided Mixture-of-Experts (MoE) framework with Evidence-Based Dirichlet Critics (EDCs) for Fluorescence LiDAR (FLiDAR) data analysis in scattering media. The method improves the estimation of photon time-of-flight and fluorescence lifetime, achieving NRMSE values of 0.030 for depth and 0.074 for lifetime.


<details>
  <summary>Details</summary>
Motivation: The computational challenges faced by FLiDAR technology in scattering media motivate the need for more effective methodologies to isolate photon time-of-flight and intrinsic fluorescence lifetime.

Method: The authors develop a Physics-Guided MoE framework incorporating EDCs that assess expert model outputs based on physical principles such as the radiative transport equation. A Decider Network uses this information to adaptively fuse expert predictions into robust final estimates.

Result: Validated using simulated FLiDAR data for non-invasive cancer cell depth detection, the proposed framework achieves an NRMSE of 0.030 for depth estimation and 0.074 for fluorescence lifetime.

Conclusion: The Physics-Guided MoE framework demonstrates strong performance in analyzing FLiDAR data, providing accurate estimations of depth and fluorescence lifetime in scattering media.

Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology
employed for distance and depth estimation across medical, automotive, and
other fields, encounters significant computational challenges in scattering
media. The complex nature of the acquired FLiDAR signal, particularly in such
environments, makes isolating photon time-of-flight (related to target depth)
and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the
effectiveness of current analytical and computational methodologies. To
overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE)
framework tailored for specialized modeling of diverse temporal components. In
contrast to the conventional MoE approaches our expert models are informed by
underlying physics, such as the radiative transport equation governing photon
propagation in scattering media. Central to our approach is EvidenceMoE, which
integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess
the reliability of each expert's output by providing per-expert quality scores
and corrective feedback. A Decider Network then leverages this information to
fuse expert predictions into a robust final estimate adaptively. We validate
our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for
non-invasive cancer cell depth detection generated from photon transport models
in tissue. Our framework demonstrates strong performance, achieving a
normalized root mean squared error (NRMSE) of 0.030 for depth estimation and
0.074 for fluorescence lifetime.

</details>


### [215] [Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement](https://arxiv.org/abs/2505.21535)
*Yuxin Ren,Maxwell D Collins,Miao Hu,Huanrui Yang*

Main category: cs.CV

TL;DR: 在边缘设备上使用Transformer时，尽管其在视觉和语言预训练任务中表现出色，但注意力机制对其推理效率构成了挑战。本文提出了FAR框架，用LSTM等可学习的序列到序列模块替换Transformer中的所有注意力模块，从而生成一系列高效的基于LSTM的模型，同时保留了原始模型的准确性和语义关系。


<details>
  <summary>Details</summary>
Motivation: Transformer在推理阶段对注意力机制的依赖导致在边缘和嵌入式加速器上存在效率问题，因为这些设备的并行性和内存带宽有限。观察到推理时注意力存在冗余，研究者假设每个注意力层的序列到序列映射可以通过更简单的函数表示。

Method: 提出了一种名为FAR（Function-preserving Attention Replacement）的框架，该框架通过块级蒸馏目标和全局结构剪枝框架优化多头LSTM架构，用LSTM等可学习的序列到序列模块替换预训练Transformer中的所有注意力模块。

Result: 在DeiT视觉Transformer系列上验证了FAR，结果表明，FAR能够在减少参数和延迟的情况下，与原始模型在ImageNet和多个下游任务上的准确率相匹配。进一步分析表明，FAR保留了Transformer注意力模块中学到的语义令牌关系和令牌间相关性。

Conclusion: FAR框架能够有效地将预训练Transformer转换为基于LSTM的高效模型，同时保持性能和学到的语义关系，适用于资源受限的设备。

Abstract: While transformers excel across vision and language pretraining tasks, their
reliance on attention mechanisms poses challenges for inference efficiency,
especially on edge and embedded accelerators with limited parallelism and
memory bandwidth. Hinted by the observed redundancy of attention at inference
time, we hypothesize that though the model learns complicated token dependency
through pretraining, the inference-time sequence-to-sequence mapping in each
attention layer is actually ''simple'' enough to be represented with a much
cheaper function. In this work, we explore FAR, a Function-preserving Attention
Replacement framework that replaces all attention blocks in pretrained
transformers with learnable sequence-to-sequence modules, exemplified by an
LSTM. FAR optimize a multi-head LSTM architecture with a block-wise
distillation objective and a global structural pruning framework to achieve a
family of efficient LSTM-based models from pretrained transformers. We validate
FAR on the DeiT vision transformer family and demonstrate that it matches the
accuracy of the original models on ImageNet and multiple downstream tasks with
reduced parameters and latency. Further analysis shows that FAR preserves the
semantic token relationships and the token-to-token correlation learned in the
transformer's attention module.

</details>


### [216] [Caption This, Reason That: VLMs Caught in the Middle](https://arxiv.org/abs/2505.21538)
*Zihan Weng,Lucas Gomez,Taylor Whittington Webb,Pouya Bashivan*

Main category: cs.CV

TL;DR: 视觉-语言模型(VLMs)在视觉理解方面取得了显著进展，但在特定任务如计数或关系推理上仍不及人类。通过认知科学方法分析VLMs在感知、注意力和记忆上的表现，发现尽管在某些任务（如类别识别）接近天花板性能，但在需要空间理解和选择性注意的任务上存在显著差距。研究显示，对自身生成文本进行推理能显著改善模型的直接视觉推理能力，强调了改进VLM思维链(CoT)能力的重要性。针对复合视觉推理任务的精细调整可显著提升小型VLM的核心认知能力，但对挑战性的、分布外基准测试的提升有限。


<details>
  <summary>Details</summary>
Motivation: 近年来，视觉-语言模型(VLMs)在视觉理解方面取得了显著进展，但仍需深入了解其在特定视觉任务中的局限性，以进一步提升其能力。

Method: 采用认知科学的方法论，沿着核心认知轴(感知、注意力和记忆)分析VLMs的表现。使用一系列任务评估这些能力，并对最先进的VLMs进行测试。通过视觉-文本解耦分析研究失败原因及改进建议，探索目标精细调整在复合视觉推理任务中的潜力。

Result: 发现VLMs在某些任务上接近天花板性能，但在需要空间理解和选择性注意的任务上存在显著差距。通过对其生成的文本进行推理，可以显著提高模型的直接视觉推理能力。针对复合视觉推理任务的精细调整可显著改善小型VLM的核心认知能力，但对挑战性的、分布外基准测试的提升有限。

Conclusion: 本研究详细分析了VLMs的认知优势和劣势，确定了同时进行感知和推理的关键瓶颈，并提供了一种有效且简单的解决方案。

Abstract: Vision-Language Models (VLMs) have shown remarkable progress in visual
understanding in recent years. Yet, they still lag behind human capabilities in
specific visual tasks such as counting or relational reasoning. To understand
the underlying limitations, we adopt methodologies from cognitive science,
analyzing VLM performance along core cognitive axes: Perception, Attention, and
Memory. Using a suite of tasks targeting these abilities, we evaluate
state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct
cognitive profiles: while advanced models approach ceiling performance on some
tasks (e.g. category identification), a significant gap persists, particularly
in tasks requiring spatial understanding or selective attention. Investigating
the source of these failures and potential methods for improvement, we employ a
vision-text decoupling analysis, finding that models struggling with direct
visual reasoning show marked improvement when reasoning over their own
generated text captions. These experiments reveal a strong need for improved
VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed
human performance. Furthermore, we demonstrate the potential of targeted
fine-tuning on composite visual reasoning tasks and show that fine-tuning
smaller VLMs substantially improves core cognitive abilities. While this
improvement does not translate to large enhancements on challenging,
out-of-distribution benchmarks, we show broadly that VLM performance on our
datasets strongly correlates with performance on these other benchmarks. Our
work provides a detailed analysis of VLM cognitive strengths and weaknesses and
identifies key bottlenecks in simultaneous perception and reasoning while also
providing an effective and simple solution.

</details>


### [217] [Equivariant Flow Matching for Point Cloud Assembly](https://arxiv.org/abs/2505.21539)
*Ziming Wang,Nan Xue,Rebecka Jörnsten*

Main category: cs.CV

TL;DR: This paper introduces Eda, a novel equivariant solver for point cloud assembly tasks that is based on flow matching models and demonstrates high competitiveness on practical datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to reconstruct complete 3D shapes by aligning multiple point cloud pieces using a new method that improves upon existing techniques.

Method: The authors theoretically show the importance of learning related vector fields for flow matching. They then propose Eda, an assembly model which learns these vector fields conditioned on input pieces and construct an equivariant path for Eda to ensure efficient training.

Result: Eda performs highly competitively on practical datasets and can handle challenging non-overlapped input pieces.

Conclusion: Eda presents a promising approach for point cloud assembly with strong performance and data efficiency.

Abstract: The goal of point cloud assembly is to reconstruct a complete 3D shape by
aligning multiple point cloud pieces. This work presents a novel equivariant
solver for assembly tasks based on flow matching models. We first theoretically
show that the key to learning equivariant distributions via flow matching is to
learn related vector fields. Based on this result, we propose an assembly
model, called equivariant diffusion assembly (Eda), which learns related vector
fields conditioned on the input pieces. We further construct an equivariant
path for Eda, which guarantees high data efficiency of the training process.
Our numerical results show that Eda is highly competitive on practical
datasets, and it can even handle the challenging situation where the input
pieces are non-overlapped.

</details>


### [218] [DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers](https://arxiv.org/abs/2505.21541)
*Zitong Wang,Hang Zhao,Qianyu Zhou,Xuequan Lu,Xiangtai Li,Yiren Song*

Main category: cs.CV

TL;DR: The paper introduces AlphaBlend, a new large-scale dataset for transparent/semi-transparent layer decomposition, and DiffDecompose, a diffusion Transformer-based framework that addresses layer ambiguity, generalization, and data scarcity issues in image decomposition tasks.


<details>
  <summary>Details</summary>
Motivation: Existing image decomposition methods struggle with disentangling semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and lack of datasets.

Method: 1. Introduced AlphaBlend - a large-scale dataset for transparent and semi-transparent layer decomposition.
2. Proposed DiffDecompose - a diffusion Transformer-based framework that learns posterior over possible layer decompositions conditioned on input image, semantic prompts, and blending type. It performs In-Context Decomposition and uses Layer Position Encoding Cloning.

Result: Extensive experiments on AlphaBlend and public LOGO dataset verify the effectiveness of DiffDecompose.

Conclusion: DiffDecompose effectively addresses challenges in layer ambiguity, generalization, and data scarcity in image decomposition tasks.

Abstract: Diffusion models have recently motivated great success in many generation
tasks like object removal. Nevertheless, existing image decomposition methods
struggle to disentangle semi-transparent or transparent layer occlusions due to
mask prior dependencies, static object assumptions, and the lack of datasets.
In this paper, we delve into a novel task: Layer-Wise Decomposition of
Alpha-Composited Images, aiming to recover constituent layers from single
overlapped images under the condition of semi-transparent/transparent alpha
layer non-linear occlusion. To address challenges in layer ambiguity,
generalization, and data scarcity, we first introduce AlphaBlend, the first
large-scale and high-quality dataset for transparent and semi-transparent layer
decomposition, supporting six real-world subtasks (e.g., translucent flare
removal, semi-transparent cell decomposition, glassware decomposition).
Building on this dataset, we present DiffDecompose, a diffusion
Transformer-based framework that learns the posterior over possible layer
decompositions conditioned on the input image, semantic prompts, and blending
type. Rather than regressing alpha mattes directly, DiffDecompose performs
In-Context Decomposition, enabling the model to predict one or multiple layers
without per-layer supervision, and introduces Layer Position Encoding Cloning
to maintain pixel-level correspondence across layers. Extensive experiments on
the proposed AlphaBlend dataset and public LOGO dataset verify the
effectiveness of DiffDecompose. The code and dataset will be available upon
paper acceptance. Our code will be available at:
https://github.com/Wangzt1121/DiffDecompose.

</details>


### [219] [Image Tokens Matter: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing](https://arxiv.org/abs/2505.21547)
*Weixing Wang,Zifeng Ding,Jindong Gu,Rui Cao,Christoph Meinel,Gerard de Melo,Haojin Yang*

Main category: cs.CV

TL;DR: Large Vision-Language Models (LVLMs) can hallucinate non-existent objects due to strong associations between co-occuring image tokens. This paper proposes a method using Graph Neural Networks and clustering to identify and mitigate these hallucinations by modifying latent image embeddings.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the issue of hallucination in Large Vision-Language Models (LVLMs), where the models generate non-existent objects in their outputs. The authors hypothesize that this may be due to visual priors formed during training, leading to strong associations between certain image tokens that frequently co-occur.

Method: To test their hypothesis, the authors construct a co-occurrence graph of image tokens using a segmentation dataset. They then use a Graph Neural Network (GNN) with contrastive learning and a clustering method to group tokens that often co-occur in similar visual contexts. Based on the observation that hallucinations correspond to clusters dominated by visually absent tokens, they propose a hallucination mitigation method that modifies latent image embeddings to suppress the influence of these absent tokens during generation.

Result: Experiments show that the proposed method reduces hallucinations while preserving the expressivity of the model.

Conclusion: In conclusion, the paper demonstrates that hallucinations in LVLMs are linked to clusters of co-occurring image tokens and presents an effective method to mitigate such hallucinations by adjusting latent image embeddings.

Abstract: Large Vision-Language Models (LVLMs) with discrete image tokenizers unify
multimodal representations by encoding visual inputs into a finite set of
tokens. Despite their effectiveness, we find that these models still
hallucinate non-existent objects. We hypothesize that this may be due to visual
priors induced during training: When certain image tokens frequently co-occur
in the same spatial regions and represent shared objects, they become strongly
associated with the verbalizations of those objects. As a result, the model may
hallucinate by evoking visually absent tokens that often co-occur with present
ones. To test this assumption, we construct a co-occurrence graph of image
tokens using a segmentation dataset and employ a Graph Neural Network (GNN)
with contrastive learning followed by a clustering method to group tokens that
frequently co-occur in similar visual contexts. We find that hallucinations
predominantly correspond to clusters whose tokens dominate the input, and more
specifically, that the visually absent tokens in those clusters show much
higher correlation with hallucinated objects compared to tokens present in the
image. Based on this observation, we propose a hallucination mitigation method
that suppresses the influence of visually absent tokens by modifying latent
image embeddings during generation. Experiments show our method reduces
hallucinations while preserving expressivity. Code is available at
https://github.com/weixingW/CGC-VTD/tree/main

</details>


### [220] [Benign-to-Toxic Jailbreaking: Inducing Harmful Responses from Harmless Prompts](https://arxiv.org/abs/2505.21556)
*Hee-Seon Kim,Minbeom Kim,Wonjun Lee,Kihyun Kim,Changick Kim*

Main category: cs.CV

TL;DR: The paper introduces Benign-to-Toxic (B2T) jailbreak, a new method to optimize adversarial images inducing toxic outputs from benign conditioning in large vision-language models (LVLMs), revealing a vulnerability in multimodal alignment.


<details>
  <summary>Details</summary>
Motivation: Existing Toxic-Continuation setting is effective for continuing already-toxic inputs but struggles to induce safety misalignment when explicit toxic signals are absent.

Method: Propose the Benign-to-Toxic (B2T) jailbreak paradigm where adversarial images are optimized to induce toxic outputs from benign conditioning without any safety violations.

Result: The B2T method outperforms prior approaches, transfers in black-box settings, and complements text-based jailbreaks.

Conclusion: This work reveals an underexplored vulnerability in multimodal alignment and introduces a fundamentally new direction for jailbreak approaches.

Abstract: Optimization-based jailbreaks typically adopt the Toxic-Continuation setting
in large vision-language models (LVLMs), following the standard next-token
prediction objective. In this setting, an adversarial image is optimized to
make the model predict the next token of a toxic prompt. However, we find that
the Toxic-Continuation paradigm is effective at continuing already-toxic
inputs, but struggles to induce safety misalignment when explicit toxic signals
are absent. We propose a new paradigm: Benign-to-Toxic (B2T) jailbreak. Unlike
prior work, we optimize adversarial images to induce toxic outputs from benign
conditioning. Since benign conditioning contains no safety violations, the
image alone must break the model's safety mechanisms. Our method outperforms
prior approaches, transfers in black-box settings, and complements text-based
jailbreaks. These results reveal an underexplored vulnerability in multimodal
alignment and introduce a fundamentally new direction for jailbreak approaches.

</details>


### [221] [Analytical Calculation of Weights Convolutional Neural Network](https://arxiv.org/abs/2505.21557)
*Polad Geidarov*

Main category: cs.CV

TL;DR: The paper introduces an algorithm for calculating CNN weights and thresholds analytically without standard training, using only 10 MNIST images. It shows that such a CNN can classify over half of 1000 handwritten digit images instantly without training.


<details>
  <summary>Details</summary>
Motivation: To explore the possibility of constructing CNNs without undergoing traditional training procedures by analytically determining their weights and thresholds.

Method: An algorithm is presented to analytically calculate the weights and thresholds of CNNs using just 10 selected images from the MNIST dataset, each representing a digit from 0 to 9. The number of channels in CNN layers is also derived analytically.

Result: The analytically computed CNN can recognize over half of 1000 handwritten digit images without any training, achieving inference in fractions of a second.

Conclusion: CNNs can be constructed and applied directly for classification tasks without training, using purely analytical computation of weights.

Abstract: This paper presents an algorithm for analytically calculating the weights and
thresholds of convolutional neural networks (CNNs) without using standard
training procedures. The algorithm enables the determination of CNN parameters
based on just 10 selected images from the MNIST dataset, each representing a
digit from 0 to 9. As part of the method, the number of channels in CNN layers
is also derived analytically. A software module was implemented in C++ Builder,
and a series of experiments were conducted using the MNIST dataset. Results
demonstrate that the analytically computed CNN can recognize over half of 1000
handwritten digit images without any training, achieving inference in fractions
of a second. These findings suggest that CNNs can be constructed and applied
directly for classification tasks without training, using purely analytical
computation of weights.

</details>


### [222] [A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification](https://arxiv.org/abs/2505.21558)
*Elhoucine Elfatimia,Recep Eryigitb,Lahcen Elfatimi*

Main category: cs.CV

TL;DR: Recent agricultural research has seen growth, but farmers lack time for on-farm research. Seed classification aids in quality control and efficiency. Early seed type identification reduces cost and risk. Seed sampling helps growers monitor quality and improve precision. This study proposes a CNN-based framework to classify ten Brassica seed types, addressing texture similarity challenges with a custom CNN architecture. Evaluated against pre-trained models, the proposed model achieved 93% accuracy.


<details>
  <summary>Details</summary>
Motivation: Farmers often lack the time and resources for on-farm research due to crop production demands. Seed classification is important for quality control, production efficiency, and impurity detection.

Method: A novel CNN-based framework was developed for efficient classification of ten common Brassica seed types, using a custom-designed CNN architecture to address texture similarity challenges in seed images.

Result: The proposed model achieved a high accuracy rate of 93 percent when evaluated using the collected Brassica seed dataset.

Conclusion: The study's CNN-based framework effectively classifies Brassica seed types, offering a valuable tool for improving seed quality control and production efficiency.

Abstract: Agricultural research has accelerated in recent years, yet farmers often lack
the time and resources for on-farm research due to the demands of crop
production and farm operations. Seed classification offers valuable insights
into quality control, production efficiency, and impurity detection. Early
identification of seed types is critical to reducing the cost and risk
associated with field emergence, which can lead to yield losses or disruptions
in downstream processes like harvesting. Seed sampling supports growers in
monitoring and managing seed quality, improving precision in determining seed
purity levels, guiding management adjustments, and enhancing yield estimations.
This study proposes a novel convolutional neural network (CNN)-based framework
for the efficient classification of ten common Brassica seed types. The
approach addresses the inherent challenge of texture similarity in seed images
using a custom-designed CNN architecture. The model's performance was evaluated
against several pre-trained state-of-the-art architectures, with adjustments to
layer configurations for optimized classification. Experimental results using
our collected Brassica seed dataset demonstrate that the proposed model
achieved a high accuracy rate of 93 percent.

</details>


### [223] [Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks](https://arxiv.org/abs/2505.21572)
*Sungwon Kim,Namkyeong Lee,Yunyoung Doh,Seungmin Shin,Guimok Cho,Seung-Won Jeon,Sangkook Kim,Chanyoung Park*

Main category: cs.CV

TL;DR: 提出了一种新的框架T-EMNN，能够有效整合3D物体厚度信息，同时保持表面网格计算效率，并在实际工业数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于网格的3D静态分析方法虽然高效，但主要关注表面拓扑和几何，忽略了真实3D物体固有的厚度信息及其相关行为。

Method: 提出了Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN)框架，通过数据驱动坐标编码空间信息，保留E(3)-等变或不变性属性，从而整合物体厚度并保持计算效率。

Result: 在实际工业数据集上的评估表明，T-EMNN在准确预测节点级3D变形方面表现出优越性能，能有效捕捉厚度效应。

Conclusion: T-EMNN框架成功地将物体厚度纳入分析，同时保持了表面网格的计算效率，为3D静态分析提供了更准确的方法。

Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient
alternatives to traditional computational numerical solvers, significantly
reducing computational costs and runtime for various physics-based analyses.
However, these methods primarily focus on surface topology and geometry, often
overlooking the inherent thickness of real-world 3D objects, which exhibits
high correlations and similar behavior between opposing surfaces. This
limitation arises from the disconnected nature of these surfaces and the
absence of internal edge connections within the mesh. In this work, we propose
a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network
(T-EMNN), that effectively integrates the thickness of 3D objects while
maintaining the computational efficiency of surface meshes. Additionally, we
introduce data-driven coordinates that encode spatial information while
preserving E(3)-equivariance or invariance properties, ensuring consistent and
robust analysis. Evaluations on a real-world industrial dataset demonstrate the
superior performance of T-EMNN in accurately predicting node-level 3D
deformations, effectively capturing thickness effects while maintaining
computational efficiency.

</details>


### [224] [Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI](https://arxiv.org/abs/2505.21589)
*Carina Newen,Luca Hinkamp,Maria Ntonti,Emmanuel Müller*

Main category: cs.CV

TL;DR: This paper introduces a new dataset of optical illusions with animal pairs to study perceptual ambiguity, revealing the impact of gaze direction and eye cues on model accuracy, aiming to bridge the gap between human and machine vision.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of optical illusion datasets and investigate the role of ambiguous data in machine learning, particularly in safety-critical domains.

Method: Creating a novel dataset featuring intermingled animal pairs that evoke perceptual ambiguity, identifying key visual concepts like gaze direction and eye cues.

Result: The dataset highlights the influence of subtle visual features on model accuracy and provides insights into bias and alignment in human and machine vision.

Conclusion: Optical illusions offer valuable perspectives for understanding limitations in perception; this dataset serves as a tool for studying and mitigating biases in visual learning.

Abstract: From uncertainty quantification to real-world object detection, we recognize
the importance of machine learning algorithms, particularly in safety-critical
domains such as autonomous driving or medical diagnostics. In machine learning,
ambiguous data plays an important role in various machine learning domains.
Optical illusions present a compelling area of study in this context, as they
offer insight into the limitations of both human and machine perception.
Despite this relevance, optical illusion datasets remain scarce. In this work,
we introduce a novel dataset of optical illusions featuring intermingled animal
pairs designed to evoke perceptual ambiguity. We identify generalizable visual
concepts, particularly gaze direction and eye cues, as subtle yet impactful
features that significantly influence model accuracy. By confronting models
with perceptual ambiguity, our findings underscore the importance of concepts
in visual learning and provide a foundation for studying bias and alignment
between human and machine vision. To make this dataset useful for general
purposes, we generate optical illusions systematically with different concepts
discussed in our bias mitigation section. The dataset is accessible in Kaggle
via
https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333.
Our source code can be found at
https://github.com/KDD-OpenSource/Ambivision.git.

</details>


### [225] [Any-to-Bokeh: One-Step Video Bokeh via Multi-Plane Image Guided Diffusion](https://arxiv.org/abs/2505.21593)
*Yang Yang,Siming Zheng,Jinwei Chen,Boxi Wu,Xiaofei He,Deng Cai,Bo Li,Peng-Tao Jiang*

Main category: cs.CV

TL;DR: Recent advances in diffusion based editing models have enabled realistic camera simulation and image-based bokeh, but video bokeh remains largely unexplored. To address the challenges, we propose a novel one-step video bokeh framework that converts arbitrary input videos into temporally coherent, depth-aware bokeh effects.


<details>
  <summary>Details</summary>
Motivation: Existing video editing models cannot explicitly control focus planes or adjust bokeh intensity, limiting their applicability for controllable optical effects. Moreover, naively extending image-based bokeh methods to video often results in temporal flickering and unsatisfactory edge blur transitions due to the lack of temporal modeling and generalization capability.

Method: The proposed method leverages a multi-plane image (MPI) representation constructed through a progressively widening depth sampling function, providing explicit geometric guidance for depth-dependent blur synthesis. By conditioning a single-step video diffusion model on MPI layers and utilizing the strong 3D priors from pre-trained models such as Stable Video Diffusion.

Result: Extensive experiments demonstrate that our method produces high-quality, controllable bokeh effects and achieves state-of-the-art performance on multiple evaluation benchmarks.

Conclusion: We propose a novel one-step video bokeh framework that converts arbitrary input videos into temporally coherent, depth-aware bokeh effects.

Abstract: Recent advances in diffusion based editing models have enabled realistic
camera simulation and image-based bokeh, but video bokeh remains largely
unexplored. Existing video editing models cannot explicitly control focus
planes or adjust bokeh intensity, limiting their applicability for controllable
optical effects. Moreover, naively extending image-based bokeh methods to video
often results in temporal flickering and unsatisfactory edge blur transitions
due to the lack of temporal modeling and generalization capability. To address
these challenges, we propose a novel one-step video bokeh framework that
converts arbitrary input videos into temporally coherent, depth-aware bokeh
effects. Our method leverages a multi-plane image (MPI) representation
constructed through a progressively widening depth sampling function, providing
explicit geometric guidance for depth-dependent blur synthesis. By conditioning
a single-step video diffusion model on MPI layers and utilizing the strong 3D
priors from pre-trained models such as Stable Video Diffusion, our approach
achieves realistic and consistent bokeh effects across diverse scenes.
Additionally, we introduce a progressive training strategy to enhance temporal
consistency, depth robustness, and detail preservation. Extensive experiments
demonstrate that our method produces high-quality, controllable bokeh effects
and achieves state-of-the-art performance on multiple evaluation benchmarks.

</details>


### [226] [OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions](https://arxiv.org/abs/2505.21724)
*Cheng Luo,Jianghui Wang,Bing Li,Siyang Song,Bernard Ghanem*

Main category: cs.CV

TL;DR: This paper introduces Online Multimodal Conversational Response Generation (OMCRG), a new task for generating synchronized verbal and non-verbal listener feedback based on speaker's multimodal input. To achieve this, they propose OmniResponse, a model leveraging pretrained LLM with Chrono-Text and TempoVoice modules, and introduce ResponseNet, a dataset supporting OMCRG research.


<details>
  <summary>Details</summary>
Motivation: To create a system capable of generating synchronized verbal and non-verbal listener feedback in natural dyadic interactions, addressing the challenge of audio and facial response synchronization.

Method: Introduced text as an intermediate modality to bridge audio and facial responses; proposed OmniResponse, a Multimodal Large Language Model that autoregressively generates high-quality multi-modal listener responses using Chrono-Text and TempoVoice components; created ResponseNet, a dataset with synchronized dyadic interactions.

Result: OmniResponse significantly outperforms baseline models in semantic speech content, audio-visual synchronization, and generation quality when evaluated on the ResponseNet dataset.

Conclusion: The introduction of OMCRG, OmniResponse, and ResponseNet advances the state-of-the-art in multimodal conversational response generation, particularly in achieving synchronization between generated audio and facial responses.

Abstract: In this paper, we introduce Online Multimodal Conversational Response
Generation (OMCRG), a novel task that aims to online generate synchronized
verbal and non-verbal listener feedback, conditioned on the speaker's
multimodal input. OMCRG reflects natural dyadic interactions and poses new
challenges in achieving synchronization between the generated audio and facial
responses of the listener. To address these challenges, we innovatively
introduce text as an intermediate modality to bridge the audio and facial
responses. We hence propose OmniResponse, a Multimodal Large Language Model
(MLLM) that autoregressively generates high-quality multi-modal listener
responses. OmniResponse leverages a pretrained LLM enhanced with two novel
components: Chrono-Text, which temporally anchors generated text tokens, and
TempoVoice, a controllable online TTS module that produces speech synchronized
with facial reactions. To support further OMCRG research, we present
ResponseNet, a new dataset comprising 696 high-quality dyadic interactions
featuring synchronized split-screen videos, multichannel audio, transcripts,
and facial behavior annotations. Comprehensive evaluations conducted on
ResponseNet demonstrate that OmniResponse significantly outperforms baseline
models in terms of semantic speech content, audio-visual synchronization, and
generation quality.

</details>


### [227] [Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture](https://arxiv.org/abs/2505.21746)
*Arif Masrur,Peder A. Olsen,Paul R. Adler,Carlan Jackson,Matthew W. Myers,Nathan Sedghi,Ray R. Weil*

Main category: cs.CV

TL;DR: The paper presents a novel framework that fuses satellite and UAS imagery using super-resolution methods to improve biomass and nitrogen estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the trade-offs between satellite data (broad coverage but lack of resolution) and UAS data (high resolution but limited coverage and costly for hyperspectral data).

Method: Fusing satellite and UAS imagery using super-resolution methods, spectrally extending UAS RGB data to vegetation red edge and near-infrared regions.

Result: Improved biomass and nitrogen estimation accuracy by 18% and 31% respectively. The SRCNN-based spectral extension model shows promise for model transferability and better predictions than models built on raw UAS RGB images.

Conclusion: A lightweight and scalable system for affordable on-farm use is introduced.

Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for
precision agriculture, yet each presents trade-offs. Satellite data offer broad
spatial, temporal, and spectral coverage but lack the resolution needed for
many precision farming applications, while UAS provide high spatial detail but
are limited by coverage and cost, especially for hyperspectral data. This study
presents a novel framework that fuses satellite and UAS imagery using
super-resolution methods. By integrating data across spatial, spectral, and
temporal domains, we leverage the strengths of both platforms cost-effectively.
We use estimation of cover crop biomass and nitrogen (N) as a case study to
evaluate our approach. By spectrally extending UAS RGB data to the vegetation
red edge and near-infrared regions, we generate high-resolution Sentinel-2
imagery and improve biomass and N estimation accuracy by 18% and 31%,
respectively. Our results show that UAS data need only be collected from a
subset of fields and time points. Farmers can then 1) enhance the spectral
detail of UAS RGB imagery; 2) increase the spatial resolution by using
satellite data; and 3) extend these enhancements spatially and across the
growing season at the frequency of the satellite flights. Our SRCNN-based
spectral extension model shows considerable promise for model transferability
over other cropping systems in the Upper and Lower Chesapeake Bay regions.
Additionally, it remains effective even when cloud-free satellite data are
unavailable, relying solely on the UAS RGB input. The spatial extension model
produces better biomass and N predictions than models built on raw UAS RGB
images. Once trained with targeted UAS RGB data, the spatial extension model
allows farmers to stop repeated UAS flights. While we introduce
super-resolution advances, the core contribution is a lightweight and scalable
system for affordable on-farm use.

</details>


### [228] [FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering](https://arxiv.org/abs/2505.21755)
*Chengyue Huang,Brisa Maneechotesuwan,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 视觉问答（VQA）系统在适应现实世界的数据变化时面临重大挑战，特别是在多模态环境下。本文提出了一种新的基准FRAMES-VQA，用于评估VQA任务中的鲁棒微调方法。通过使用十个现有的VQA基准，并将它们分类为同分布、近似和远距离的异分布数据集，涵盖了单模态、多模态和对抗性分布变化。首先对现有的鲁棒微调方法进行了全面比较，然后通过计算单模态和多模态嵌入的马氏距离来量化分布变化，并分析了单模态与多模态变化之间的相互作用及模态重要性。


<details>
  <summary>Details</summary>
Motivation: 当前的评估设置主要是单模态或特定类型的异分布，对于多模态环境下的复杂性提供了有限的见解。因此，需要一种新的基准来更好地评估和理解VQA系统在多模态环境下的表现。

Method: 1. 提出FRAMES-VQA基准，整合十个现有VQA基准并分类为ID、近OOO和远OOO数据集。
2. 对现有鲁棒微调方法进行综合比较。
3. 使用单模态和多模态嵌入计算马氏距离以量化分布变化。
4. 分析单模态与多模态变化之间的相互作用及模态重要性。

Result: 提供了关于如何开发更鲁棒的微调方法以处理多模态分布变化的宝贵指导。

Conclusion: FRAMES-VQA为评估VQA任务中的鲁棒微调方法提供了一个新基准，有助于理解和改进VQA系统在多模态环境下的表现。

Abstract: Visual question answering (VQA) systems face significant challenges when
adapting to real-world data shifts, especially in multi-modal contexts. While
robust fine-tuning strategies are essential for maintaining performance across
in-distribution (ID) and out-of-distribution (OOD) scenarios, current
evaluation settings are primarily unimodal or particular to some types of OOD,
offering limited insight into the complexities of multi-modal contexts. In this
work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across
Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We
utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA
and others, and categorize them into ID, near and far OOD datasets covering
uni-modal, multi-modal and adversarial distribution shifts. We first conduct a
comprehensive comparison of existing robust fine-tuning methods. We then
quantify the distribution shifts by calculating the Mahalanobis distance using
uni-modal and multi-modal embeddings extracted from various models. Further, we
perform an extensive analysis to explore the interactions between uni- and
multi-modal shifts as well as modality importance for ID and OOD samples. These
analyses offer valuable guidance on developing more robust fine-tuning methods
to handle multi-modal distribution shifts. The code is available at
https://github.com/chengyuehuang511/FRAMES-VQA .

</details>


### [229] [MMTBENCH: A Unified Benchmark for Complex Multimodal Table Reasoning](https://arxiv.org/abs/2505.21771)
*Prasham Yatinkumar Titiya,Jainil Trivedi,Chitta Baral,Vivek Gupta*

Main category: cs.CV

TL;DR: Multimodal tables, integrating semi-structured data with visual elements, challenge current vision-language models (VLMs). While large language models (LLMs) and VLMs excel in text and image understanding individually, their performance on complex multimodal table reasoning remains unexplored. To address this, MMTBENCH, a benchmark consisting of 500 real-world multimodal tables and 4021 question-answer pairs, is introduced. Evaluations reveal significant performance gaps, especially in visual-based reasoning and multi-step inference, highlighting the need for improved vision-language integration.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to bridge the gap in understanding and processing multimodal tables that combine semi-structured data with visual elements like charts and maps. Current models perform well on individual tasks but struggle with the complexity of multimodal table reasoning.

Method: The authors developed MMTBENCH, a benchmark containing 500 real-world multimodal tables and 4021 question-answer pairs. This benchmark covers various question types, reasoning types, and table types. State-of-the-art models were evaluated across all these categories.

Result: Evaluations showed substantial performance gaps, particularly in questions requiring visual-based reasoning and multi-step inference. This indicates the current limitations of models in tightly integrating vision and language processing.

Conclusion: MMTBENCH serves as a valuable resource for future research on multimodal tables by providing a challenging, high-quality dataset that reflects the complexity of real-world tasks. The findings emphasize the need for better architectures that more effectively integrate vision and language.

Abstract: Multimodal tables those that integrate semi structured data with visual
elements such as charts and maps are ubiquitous across real world domains, yet
they pose a formidable challenge to current vision language models (VLMs).
While Large Language models (LLMs) and VLMs have demonstrated strong
capabilities in text and image understanding, their performance on complex,
real world multimodal table reasoning remains unexplored. To bridge this gap,
we introduce MMTBENCH (Multimodal Table Benchmark), a benchmark consisting of
500 real world multimodal tables drawn from diverse real world sources, with a
total of 4021 question answer pairs. MMTBENCH questions cover four question
types (Explicit, Implicit, Answer Mention, and Visual Based), five reasoning
types (Mathematical, Extrema Identification, Fact Verification, Vision Based,
and Others), and eight table types (Single/Multiple Entity, Maps and Charts
with Entities, Single/Multiple Charts, Maps, and Visualizations). Extensive
evaluation of state of the art models on all types reveals substantial
performance gaps, particularly on questions requiring visual-based reasoning
and multi-step inference. These findings show the urgent need for improved
architectures that more tightly integrate vision and language processing. By
providing a challenging, high-quality resource that mirrors the complexity of
real-world tasks, MMTBENCH underscores its value as a resource for future
research on multimodal tables.

</details>


### [230] [RePaViT: Scalable Vision Transformer Acceleration via Structural Reparameterization on Feedforward Network Layers](https://arxiv.org/abs/2505.21847)
*Xuwei Xu,Yang Li,Yudong Chen,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: 通过引入通道空闲机制，提出了一种可重新参数化的视觉变换器（RePaViT），在保持较高精度的同时显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 发现前馈网络(FFN)层而非注意力层是导致视觉变换器(ViT)推理延迟的主要原因，且随着模型规模增大影响更显著，这为优化大规模ViT的效率提供了关键机会。

Method: 提出一种通道空闲机制，在测试时对FFN层进行后训练结构重新参数化。具体来说，部分特征通道保持空闲并跳过非线性激活函数，形成线性路径以支持推理时的结构重新参数化。

Result: 该方法在不同规模的ViT上实现了显著的延迟降低，并在较大模型上表现出更高的速度提升和逐渐缩小的精度差距，甚至更高的精度。例如，RePa-ViT-Large和RePa-ViT-Huge分别提升了66.8%和68.7%的速度，同时top-1精度分别提高了1.7%和1.1%。

Conclusion: RePaViT是首个利用结构重新参数化加速ViT的方法，展示了高效ViT的一个有希望的方向。

Abstract: We reveal that feedforward network (FFN) layers, rather than attention
layers, are the primary contributors to Vision Transformer (ViT) inference
latency, with their impact signifying as model size increases. This finding
highlights a critical opportunity for optimizing the efficiency of large-scale
ViTs by focusing on FFN layers. In this work, we propose a novel channel idle
mechanism that facilitates post-training structural reparameterization for
efficient FFN layers during testing. Specifically, a set of feature channels
remains idle and bypasses the nonlinear activation function in each FFN layer,
thereby forming a linear pathway that enables structural reparameterization
during inference. This mechanism results in a family of ReParameterizable
Vision Transformers (RePaViTs), which achieve remarkable latency reductions
with acceptable sacrifices (sometimes gains) in accuracy across various ViTs.
The benefits of our method scale consistently with model sizes, demonstrating
greater speed improvements and progressively narrowing accuracy gaps or even
higher accuracies on larger models. In particular, RePa-ViT-Large and
RePa-ViT-Huge enjoy 66.8% and 68.7% speed-ups with +1.7% and +1.1% higher top-1
accuracies under the same training strategy, respectively. RePaViT is the first
to employ structural reparameterization on FFN layers to expedite ViTs to our
best knowledge, and we believe that it represents an auspicious direction for
efficient ViTs. Source code is available at
https://github.com/Ackesnal/RePaViT.

</details>


### [231] [Beyond Perception: Evaluating Abstract Visual Reasoning through Multi-Stage Task](https://arxiv.org/abs/2505.21850)
*Yanbei Jiang,Yihao Ding,Chao Lei,Jiayang Ao,Jey Han Lau,Krista A. Ehinger*

Main category: cs.CV

TL;DR: 当前多模态大语言模型（MLLMs）在通用视觉推理方面表现出色，但在抽象视觉推理（AVR）方面仍待深入研究。为了评估模型在多阶段推理过程中的表现，本文提出了MultiStAR基准和MSEval指标，并通过实验揭示了现有模型在复杂规则检测中的不足。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的多模态大语言模型在一般视觉推理任务中表现出色，但在需要高层次推理的抽象视觉推理（AVR）领域尚未得到充分探索。此外，现有的AVR基准仅关注单步推理，忽略了推理过程的多阶段性，且评价指标如准确率仅关注最终结果，无法反映中间步骤的正确性。因此，研究者希望填补这一空白，提出新的基准和指标来全面评估模型的推理能力。

Method: 1. 提出了一个基于RAVEN的多阶段抽象视觉推理基准（MultiStAR），用于评估模型在不同复杂度水平上的推理能力。
2. 设计了一个新型评价指标MSEval，不仅考虑最终结果的准确性，还评估推理过程中间步骤的正确性。
3. 在MultiStAR上对17个具有代表性的闭源和开源多模态大语言模型进行了全面实验。

Result: 实验结果表明，现有的多模态大语言模型在基础感知任务上表现良好，但在更复杂的规则检测阶段仍面临挑战。这说明模型在高层次推理能力方面仍有不足。

Conclusion: 本文通过提出MultiStAR基准和MSEval指标，揭示了现有多模态大语言模型在抽象视觉推理领域的局限性，为未来研究提供了方向。改进模型的多阶段推理能力和设计更全面的评价指标将是提升其抽象推理性能的关键。

Abstract: Current Multimodal Large Language Models (MLLMs) excel in general visual
reasoning but remain underexplored in Abstract Visual Reasoning (AVR), which
demands higher-order reasoning to identify abstract rules beyond simple
perception. Existing AVR benchmarks focus on single-step reasoning, emphasizing
the end result but neglecting the multi-stage nature of reasoning process. Past
studies found MLLMs struggle with these benchmarks, but it doesn't explain how
they fail. To address this gap, we introduce MultiStAR, a Multi-Stage AVR
benchmark, based on RAVEN, designed to assess reasoning across varying levels
of complexity. Additionally, existing metrics like accuracy only focus on the
final outcomes while do not account for the correctness of intermediate steps.
Therefore, we propose a novel metric, MSEval, which considers the correctness
of intermediate steps in addition to the final outcomes. We conduct
comprehensive experiments on MultiStAR using 17 representative close-source and
open-source MLLMs. The results reveal that while existing MLLMs perform
adequately on basic perception tasks, they continue to face challenges in more
complex rule detection stages.

</details>


### [232] [Rethinking Gradient-based Adversarial Attacks on Point Cloud Classification](https://arxiv.org/abs/2505.21854)
*Jun Chen,Xinke Li,Mingyue Xu,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: The paper proposes WAAttack and SubAttack to improve gradient-based adversarial attacks on 3D point cloud classification by considering the heterogeneous nature of point clouds.


<details>
  <summary>Details</summary>
Motivation: Gradient-based adversarial attacks are widely used for evaluating the robustness of point cloud classification models, but existing methods have limitations in considering the heterogeneous nature of point clouds, leading to perceptible perturbations.

Method: The paper introduces WAAttack which uses weighted gradients and adaptive step-size strategy, and SubAttack which focuses on structurally critical regions of the point cloud.

Result: Extensive experiments show that the proposed approach outperforms state-of-the-art baselines in generating highly imperceptible adversarial examples.

Conclusion: WAAttack and SubAttack provide a principled rethinking of gradient-based adversarial attacks for 3D point cloud classification.

Abstract: Gradient-based adversarial attacks have become a dominant approach for
evaluating the robustness of point cloud classification models. However,
existing methods often rely on uniform update rules that fail to consider the
heterogeneous nature of point clouds, resulting in excessive and perceptible
perturbations. In this paper, we rethink the design of gradient-based attacks
by analyzing the limitations of conventional gradient update mechanisms and
propose two new strategies to improve both attack effectiveness and
imperceptibility. First, we introduce WAAttack, a novel framework that
incorporates weighted gradients and an adaptive step-size strategy to account
for the non-uniform contribution of points during optimization. This approach
enables more targeted and subtle perturbations by dynamically adjusting updates
according to the local structure and sensitivity of each point. Second, we
propose SubAttack, a complementary strategy that decomposes the point cloud
into subsets and focuses perturbation efforts on structurally critical regions.
Together, these methods represent a principled rethinking of gradient-based
adversarial attacks for 3D point cloud classification. Extensive experiments
demonstrate that our approach outperforms state-of-the-art baselines in
generating highly imperceptible adversarial examples. Code will be released
upon paper acceptance.

</details>


### [233] [EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video Guidance](https://arxiv.org/abs/2505.21876)
*Zun Wang,Jaemin Cho,Jialu Li,Han Lin,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: The paper presents EPiC, a framework for efficient and precise 3D camera control in video diffusion models that creates high-quality anchor videos without needing expensive camera trajectory annotations. It introduces Anchor-ControlNet, which integrates anchor video guidance with pretrained VDMs using minimal parameters. EPiC achieves state-of-the-art performance on I2V camera control tasks.


<details>
  <summary>Details</summary>
Motivation: Recent methods for 3D camera control in video diffusion models often suffer from inaccuracies due to point cloud estimation errors and require extensive camera trajectory annotations, increasing resource demands.

Method: EPiC constructs high-quality anchor videos by masking source videos based on first-frame visibility, eliminating the need for camera trajectory annotations. The Anchor-ControlNet module is introduced to integrate anchor video guidance into pretrained VDMs with minimal parameters.

Result: EPiC achieves state-of-the-art performance on RealEstate10K and MiraData for image-to-video camera control tasks, demonstrating precise and robust camera control both quantitatively and qualitatively. It also exhibits strong zero-shot generalization to video-to-video scenarios.

Conclusion: EPiC provides an efficient and precise method for 3D camera control in video diffusion models, reducing the need for extensive annotations and achieving superior performance.

Abstract: Recent approaches on 3D camera control in video diffusion models (VDMs) often
create anchor videos to guide diffusion models as a structured prior by
rendering from estimated point clouds following annotated camera trajectories.
However, errors inherent in point cloud estimation often lead to inaccurate
anchor videos. Moreover, the requirement for extensive camera trajectory
annotations further increases resource demands. To address these limitations,
we introduce EPiC, an efficient and precise camera control learning framework
that automatically constructs high-quality anchor videos without expensive
camera trajectory annotations. Concretely, we create highly precise anchor
videos for training by masking source videos based on first-frame visibility.
This approach ensures high alignment, eliminates the need for camera trajectory
annotations, and thus can be readily applied to any in-the-wild video to
generate image-to-video (I2V) training pairs. Furthermore, we introduce
Anchor-ControlNet, a lightweight conditioning module that integrates anchor
video guidance in visible regions to pretrained VDMs, with less than 1% of
backbone model parameters. By combining the proposed anchor video data and
ControlNet module, EPiC achieves efficient training with substantially fewer
parameters, training steps, and less data, without requiring modifications to
the diffusion model backbone typically needed to mitigate rendering
misalignments. Although being trained on masking-based anchor videos, our
method generalizes robustly to anchor videos made with point clouds during
inference, enabling precise 3D-informed camera control. EPiC achieves SOTA
performance on RealEstate10K and MiraData for I2V camera control task,
demonstrating precise and robust camera control ability both quantitatively and
qualitatively. Notably, EPiC also exhibits strong zero-shot generalization to
video-to-video scenarios.

</details>


### [234] [CAST: Contrastive Adaptation and Distillation for Semi-Supervised Instance Segmentation](https://arxiv.org/abs/2505.21904)
*Pardis Taghavi,Tian Liu,Renjie Li,Reza Langari,Zhengzhong Tu*

Main category: cs.CV

TL;DR: CAST is a semi-supervised knowledge distillation framework that compresses pretrained vision foundation models into compact experts using limited labeled and abundant unlabeled data, with a novel instance-aware pixel-wise contrastive loss.


<details>
  <summary>Details</summary>
Motivation: Instance segmentation requires costly per-pixel annotations and large models, so there's a need for methods that can use less labeled data and produce smaller models.

Method: The CAST framework consists of three stages: domain adaptation of the VFM teacher(s) via self-training with contrastive pixel calibration, distillation into a compact student via a unified multi-objective loss, and fine-tuning on labeled data to remove residual pseudo-label bias. It also uses an instance-aware pixel-wise contrastive loss.

Result: On Cityscapes and ADE20K, the ~11X smaller student model surpasses its adapted VFM teacher(s) by +3.4 AP and +1.5 AP respectively and outperforms state-of-the-art semi-supervised approaches.

Conclusion: CAST effectively leverages unlabeled data and a novel contrastive loss to create smaller, more efficient models for instance segmentation that outperform their larger counterparts.

Abstract: Instance segmentation demands costly per-pixel annotations and large models.
We introduce CAST, a semi-supervised knowledge distillation (SSKD) framework
that compresses pretrained vision foundation models (VFM) into compact experts
using limited labeled and abundant unlabeled data. CAST unfolds in three
stages: (1) domain adaptation of the VFM teacher(s) via self-training with
contrastive pixel calibration, (2) distillation into a compact student via a
unified multi-objective loss that couples standard supervision and
pseudo-labels with our instance-aware pixel-wise contrastive term, and (3)
fine-tuning on labeled data to remove residual pseudo-label bias. Central to
CAST is an \emph{instance-aware pixel-wise contrastive loss} that fuses mask
and class scores to mine informative negatives and enforce clear inter-instance
margins. By maintaining this contrastive signal across both adaptation and
distillation, we align teacher and student embeddings and fully leverage
unlabeled images. On Cityscapes and ADE20K, our ~11X smaller student surpasses
its adapted VFM teacher(s) by +3.4 AP (33.9 vs. 30.5) and +1.5 AP (16.7 vs.
15.2) and outperforms state-of-the-art semi-supervised approaches.

</details>


### [235] [UniTalk: Towards Universal Active Speaker Detection in Real World Scenarios](https://arxiv.org/abs/2505.21954)
*Le Thien Phuc Nguyen,Zhuoran Yu,Khoa Quang Nhat Cao,Yuwei Guo,Tu Ho Manh Pham,Tuan Tai Nguyen,Toan Ngo Duc Vo,Lucas Poon,Soochahn Lee,Yong Jae Lee*

Main category: cs.CV

TL;DR: The paper introduces UniTalk, a new dataset for active speaker detection (ASD) that addresses limitations of previous datasets by focusing on diverse real-world conditions. It contains over 44.5 hours of video with frame-level annotations across 48,693 speaking identities. State-of-the-art models perform well on older benchmarks like AVA but struggle with UniTalk, highlighting the need for improved ASD methods. Models trained on UniTalk generalize better to modern datasets.


<details>
  <summary>Details</summary>
Motivation: Existing ASD benchmarks, such as AVA, have significant domain gaps due to their focus on old movies. There is a need for a dataset that reflects diverse and challenging real-world conditions to enhance model generalization.

Method: UniTalk is a dataset containing over 44.5 hours of video with frame-level active speaker annotations across 48,693 speaking identities. It includes underrepresented languages, noisy backgrounds, and crowded scenes with overlapping speakers. The dataset spans a broad range of video types reflecting real-world conditions.

Result: State-of-the-art models achieve nearly perfect scores on AVA but fail to reach saturation on UniTalk, indicating the challenges of realistic conditions. Models trained on UniTalk demonstrate stronger generalization to modern datasets like Talkies and ASW, as well as to AVA.

Conclusion: UniTalk establishes a new benchmark for active speaker detection, offering researchers a valuable resource to develop and evaluate more versatile and resilient models.

Abstract: We present UniTalk, a novel dataset specifically designed for the task of
active speaker detection, emphasizing challenging scenarios to enhance model
generalization. Unlike previously established benchmarks such as AVA, which
predominantly features old movies and thus exhibits significant domain gaps,
UniTalk focuses explicitly on diverse and difficult real-world conditions.
These include underrepresented languages, noisy backgrounds, and crowded scenes
- such as multiple visible speakers speaking concurrently or in overlapping
turns. It contains over 44.5 hours of video with frame-level active speaker
annotations across 48,693 speaking identities, and spans a broad range of video
types that reflect real-world conditions. Through rigorous evaluation, we show
that state-of-the-art models, while achieving nearly perfect scores on AVA,
fail to reach saturation on UniTalk, suggesting that the ASD task remains far
from solved under realistic conditions. Nevertheless, models trained on UniTalk
demonstrate stronger generalization to modern "in-the-wild" datasets like
Talkies and ASW, as well as to AVA. UniTalk thus establishes a new benchmark
for active speaker detection, providing researchers with a valuable resource
for developing and evaluating versatile and resilient models.
  Dataset: https://huggingface.co/datasets/plnguyen2908/UniTalk-ASD
  Code: https://github.com/plnguyen2908/UniTalk-ASD-code

</details>


### [236] [Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs](https://arxiv.org/abs/2505.21955)
*Insu Lee,Wooje Park,Jaeyun Jang,Minyoung Noh,Kyuhong Shim,Byonghyo Shim*

Main category: cs.CV

TL;DR: 通过引入E3VQA基准和M3CoT技术，本研究增强了大型视觉-语言模型在多视角推理任务中的表现，特别是在结合第一人称和第三人称视角信息时。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉-语言模型在处理需要空间或上下文要求较高的查询时存在失败情况，尤其是在使用头戴式相机捕获的第一人称视图作为输入时。这主要是由于其视野狭窄且缺乏全局上下文所致。

Method: 提出了一种框架，将第一人称视图与第三人称视图相结合，为大型视觉-语言模型提供补充信息，如全局场景布局和对象可见性。同时，推出了E3VQA基准，包含4K高质量问答对，并提出了无需训练的提示技术M3CoT，通过整合三个互补视角的场景图构建统一的场景表示。

Result: M3CoT技术使大型视觉-语言模型在多视角推理中表现出显著性能提升（GPT-4o提升4.84%，Gemini 2.0 Flash提升5.94%），超过最近的CoT基线。

Conclusion: 研究表明，结合第一人称和第三人称输入可以有效增强大型视觉-语言模型在多视角推理中的能力，揭示了其关键优势和局限性。

Abstract: Large vision-language models (LVLMs) are increasingly deployed in interactive
applications such as virtual and augmented reality, where first-person
(egocentric) view captured by head-mounted cameras serves as key input. While
this view offers fine-grained cues about user attention and hand-object
interactions, their narrow field of view and lack of global context often lead
to failures on spatially or contextually demanding queries. To address this, we
introduce a framework that augments egocentric inputs with third-person
(exocentric) views, providing complementary information such as global scene
layout and object visibility to LVLMs. We present E3VQA, the first benchmark
for multi-view question answering with 4K high-quality question-answer pairs
grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a
training-free prompting technique that constructs a unified scene
representation by integrating scene graphs from three complementary
perspectives. M3CoT enables LVLMs to reason more effectively across views,
yielding consistent performance gains (4.84% for GPT-4o and 5.94% for Gemini
2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key
strengths and limitations of LVLMs in multi-view reasoning and highlights the
value of leveraging both egocentric and exocentric inputs.

</details>


### [237] [Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation](https://arxiv.org/abs/2505.21956)
*Mengdan Zhu,Senhao Cheng,Guangji Bai,Yifei Zhang,Liang Zhao*

Main category: cs.CV

TL;DR: This paper presents Cross-modal RAG, a new framework that breaks down complex text-to-image generation queries and images into smaller parts for better retrieval and creation. It uses a hybrid retrieval method to find the best matching images and guides a multimodal model during generation, leading to superior results.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models need more specific, up-to-date knowledge than what pretrained models can offer. Current RAG methods fall short when a single image doesn't contain all elements from a detailed user request.

Method: The Cross-modal RAG framework splits queries and images into sub-components. A hybrid retrieval approach combines sparse and dense retrievers to select Pareto-optimal images covering different aspects of the query. During image synthesis, a large language model is directed to focus on visual features corresponding to specific subqueries.

Result: Cross-modal RAG surpasses existing methods in retrieval and generation quality across multiple datasets (MS-COCO, Flickr30K, WikiArt, CUB, ImageNet-LT), while staying efficient.

Conclusion: Cross-modal RAG offers an effective solution for complex text-to-image tasks by enabling subquery-aware retrieval and generation.

Abstract: Text-to-image generation increasingly demands access to domain-specific,
fine-grained, and rapidly evolving knowledge that pretrained models cannot
fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to
address this by retrieving globally relevant images, but they fail when no
single image contains all desired elements from a complex user query. We
propose Cross-modal RAG, a novel framework that decomposes both queries and
images into sub-dimensional components, enabling subquery-aware retrieval and
generation. Our method introduces a hybrid retrieval strategy - combining a
sub-dimensional sparse retriever with a dense retriever - to identify a
Pareto-optimal set of images, each contributing complementary aspects of the
query. During generation, a multimodal large language model is guided to
selectively condition on relevant visual features aligned to specific
subqueries, ensuring subquery-aware image synthesis. Extensive experiments on
MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal
RAG significantly outperforms existing baselines in both retrieval and
generation quality, while maintaining high efficiency.

</details>


### [238] [Learning Shared Representations from Unpaired Data](https://arxiv.org/abs/2505.21524)
*Amitai Yacobi,Nir Ben-Ari,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: The paper presents a method for learning shared representations from unpaired data in multimodal representation learning, demonstrating its potential and effectiveness in various tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods for learning shared representations rely heavily on paired samples which are harder to obtain than unpaired ones. The authors aim to demonstrate that shared representations can be learned almost exclusively from unpaired data.

Method: The arguments are grounded in the spectral embeddings of the random walk matrices constructed independently from each unimodal representation. This approach leverages unpaired data to capture meaningful cross-modal relations.

Result: Empirical results in computer vision and natural language processing domains reveal the effectiveness of unpaired data in capturing cross-modal relations, showing high capabilities in retrieval tasks, generation, arithmetics, zero-shot, and cross-domain classification.

Conclusion: This work is the first to demonstrate these capabilities almost exclusively from unpaired samples, leading to a cross-modal embedding that could be viewed as universal, independent of the specific modalities of the data.

Abstract: Learning shared representations is a primary area of multimodal
representation learning. The current approaches to achieve a shared embedding
space rely heavily on paired samples from each modality, which are
significantly harder to obtain than unpaired ones. In this work, we demonstrate
that shared representations can be learned almost exclusively from unpaired
data. Our arguments are grounded in the spectral embeddings of the random walk
matrices constructed independently from each unimodal representation. Empirical
results in computer vision and natural language processing domains support its
potential, revealing the effectiveness of unpaired data in capturing meaningful
cross-modal relations, demonstrating high capabilities in retrieval tasks,
generation, arithmetics, zero-shot, and cross-domain classification. This work,
to the best of our knowledge, is the first to demonstrate these capabilities
almost exclusively from unpaired samples, giving rise to a cross-modal
embedding that could be viewed as universal, i.e., independent of the specific
modalities of the data. Our code IS publicly available at
https://github.com/shaham-lab/SUE.

</details>


### [239] [Learning World Models for Interactive Video Generation](https://arxiv.org/abs/2505.21996)
*Taiye Chen,Xun Hu,Zihan Ding,Chi Jin*

Main category: cs.CV

TL;DR: Foundational world models require interactive and spatiotemporally coherent for future planning. Current video generation models face challenges of compounding errors and insufficient memory mechanisms. This study proposes VRAG with explicit global state conditioning to reduce long-term compounding errors and enhance spatiotemporal consistency, providing a benchmark for improving video generation models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in current video generation models that hinder effective future planning due to compounding errors and insufficient memory mechanisms.

Method: Propose VRAG with explicit global state conditioning to significantly reduce long-term compounding errors and increase spatiotemporal consistency of world models.

Result: VRAG proves more effective than naive autoregressive generation and retrieval-augmented generation primarily because of its better handling of long-term compounding errors and spatiotemporal consistency.

Conclusion: This work highlights fundamental challenges in video world models and sets a benchmark for enhancing video generation models with internal world modeling capabilities.

Abstract: Foundational world models must be both interactive and preserve
spatiotemporal coherence for effective future planning with action choices.
However, present models for long video generation have limited inherent world
modeling capabilities due to two main challenges: compounding errors and
insufficient memory mechanisms. We enhance image-to-video models with
interactive capabilities through additional action conditioning and
autoregressive framework, and reveal that compounding error is inherently
irreducible in autoregressive video generation, while insufficient memory
mechanism leads to incoherence of world models. We propose video retrieval
augmented generation (VRAG) with explicit global state conditioning, which
significantly reduces long-term compounding errors and increases spatiotemporal
consistency of world models. In contrast, naive autoregressive generation with
extended context windows and retrieval-augmented generation prove less
effective for video generation, primarily due to the limited in-context
learning capabilities of current video models. Our work illuminates the
fundamental challenges in video world models and establishes a comprehensive
benchmark for improving video generation models with internal world modeling
capabilities.

</details>


### [240] [Self-Organizing Visual Prototypes for Non-Parametric Representation Learning](https://arxiv.org/abs/2505.21533)
*Thalles Silva,Helio Pedrini,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: The paper presents Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning, which uses semantically similar representations to better characterize data clusters. It also introduces SOP Masked Image Modeling (SOP-MIM) and shows state-of-the-art performance in retrieval benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing prototypical self-supervised learning methods rely on a single prototype to encode all relevant features of a hidden cluster in the data, which may not be sufficient.

Method: The SOP strategy represents a prototype by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features. Non-parametric SSL is reaffirmed by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. The SOP Masked Image Modeling (SOP-MIM) task reconstructs masked representations from the perspective of multiple non-parametric local SEs.

Result: The pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.

Conclusion: The SOP strategy is effective in unsupervised visual feature learning and can lead to superior performance in various benchmarks.

Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique
for unsupervised visual feature learning. Unlike existing prototypical
self-supervised learning (SSL) methods that rely on a single prototype to
encode all relevant features of a hidden cluster in the data, we propose the
SOP strategy. In this strategy, a prototype is represented by many semantically
similar representations, or support embeddings (SEs), each containing a
complementary set of features that together better characterize their region in
space and maximize training performance. We reaffirm the feasibility of
non-parametric SSL by introducing novel non-parametric adaptations of two loss
functions that implement the SOP strategy. Notably, we introduce the SOP Masked
Image Modeling (SOP-MIM) task, where masked representations are reconstructed
from the perspective of multiple non-parametric local SEs. We comprehensively
evaluate the representations learned using the SOP strategy on a range of
benchmarks, including retrieval, linear evaluation, fine-tuning, and object
detection. Our pre-trained encoders achieve state-of-the-art performance on
many retrieval benchmarks and demonstrate increasing performance gains with
more complex encoders.

</details>


### [241] [GL-PGENet: A Parameterized Generation Framework for Robust Document Image Enhancement](https://arxiv.org/abs/2505.22021)
*Zhihong Tang,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种新的文档图像增强网络GL-PGENet，针对多退化彩色文档图像，通过全局与局部增强框架、双分支局部精化网络以及改进的NestUNet架构，结合两阶段训练策略，在多个指标上达到SOTA，并展现出优秀的跨域适应性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多局限于单一退化恢复或灰度图像处理，无法有效应对多退化彩色文档图像的需求。

Method: 设计了GL-PGENet，包含：1) 层次化增强框架，结合全局外观校正和局部精细化；2) 双分支局部精化网络，采用参数生成机制替代直接预测；3) 改进的NestUNet架构，融合低级像素特征和高级语义特征。此外，采用两阶段训练策略（大规模预训练+任务特定微调）。

Result: 在DocUNet和RealDAE数据集上分别达到0.7721和0.9480的SSIM分数，展现跨域适应性及高分辨率图像上的高效性，无性能下降。

Conclusion: GL-PGENet在多退化彩色文档图像增强任务中表现出色，具备实际应用价值。

Abstract: Document Image Enhancement (DIE) serves as a critical component in Document
AI systems, where its performance substantially determines the effectiveness of
downstream tasks. To address the limitations of existing methods confined to
single-degradation restoration or grayscale image processing, we present Global
with Local Parametric Generation Enhancement Network (GL-PGENet), a novel
architecture designed for multi-degraded color document images, ensuring both
efficiency and robustness in real-world scenarios. Our solution incorporates
three key innovations: First, a hierarchical enhancement framework that
integrates global appearance correction with local refinement, enabling
coarse-to-fine quality improvement. Second, a Dual-Branch Local-Refine Network
with parametric generation mechanisms that replaces conventional direct
prediction, producing enhanced outputs through learned intermediate parametric
representations rather than pixel-wise mapping. This approach enhances local
consistency while improving model generalization. Finally, a modified NestUNet
architecture incorporating dense block to effectively fuse low-level pixel
features and high-level semantic features, specifically adapted for document
image characteristics. In addition, to enhance generalization performance, we
adopt a two-stage training strategy: large-scale pretraining on a synthetic
dataset of 500,000+ samples followed by task-specific fine-tuning. Extensive
experiments demonstrate the superiority of GL-PGENet, achieving
state-of-the-art SSIM scores of 0.7721 on DocUNet and 0.9480 on RealDAE. The
model also exhibits remarkable cross-domain adaptability and maintains
computational efficiency for high-resolution images without performance
degradation, confirming its practical utility in real-world scenarios.

</details>


### [242] [Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation](https://arxiv.org/abs/2505.21545)
*Chika Maduabuchi,Hao Chen,Yujin Han,Jindong Wang*

Main category: cs.CV

TL;DR: CAT-LVDM is the first corruption-aware training framework for LVDMs, which improves robustness through structured, data-aligned noise injection. BCNI and SACN are proposed to preserve temporal consistency and improve low-frequency smoothness respectively.


<details>
  <summary>Details</summary>
Motivation: LVDMs achieve high-quality generation but are sensitive to imperfect conditioning, which causes semantic drift and temporal incoherence on noisy, web-scale video-text datasets.

Method: The method includes Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal consistency, and Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral directions to improve low-frequency smoothness.

Result: On average, BCNI reduces FVD by 31.9% across WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.

Conclusion: CAT-LVDM establishes a principled, scalable training approach for robust video diffusion under multimodal noise.

Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are
sensitive to imperfect conditioning, which causes semantic drift and temporal
incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the
first corruption-aware training framework for LVDMs that improves robustness
through structured, data-aligned noise injection. Our method includes
Batch-Centered Noise Injection (BCNI), which perturbs embeddings along
intra-batch semantic directions to preserve temporal consistency. BCNI is
especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and
MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects
noise along dominant spectral directions to improve low-frequency smoothness,
showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across
WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101.
Ablation studies confirm the benefit of low-rank, data-aligned noise. Our
theoretical analysis further explains how such perturbations tighten entropy,
Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM
establishes a principled, scalable training approach for robust video diffusion
under multimodal noise. Code and models: https://github.com/chikap421/catlvdm

</details>


### [243] [Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization](https://arxiv.org/abs/2505.22038)
*Kaiyuan Li,Xiaoyue Chen,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: 提出了一种名为Balanced Token Pruning(BTP)的方法，通过多阶段策略对视觉标记进行剪枝，在早期阶段关注剪枝对后续层的影响，而在较深阶段则侧重于保持局部输出的一致性。该方法在多个LVLM基准测试中表现出广泛的有效性，达到了78%的压缩率，同时保留了原始模型性能的96.7%。


<details>
  <summary>Details</summary>
Motivation: 现有的图像标记剪枝方法通常忽略了剪枝对当前层输出（局部）和后续层输出（全局）的联合影响，导致次优的剪枝决策。

Method: 提出了一种名为Balanced Token Pruning(BTP)的方法，利用小校准集将剪枝过程分为多个阶段。早期阶段强调剪枝对后续层的影响，较深阶段则侧重于保持局部输出的一致性。

Result: 实验表明，该方法在多个LVLM基准测试中具有广泛的有效性，能够达到78%的压缩率，同时保留96.7%的原始模型性能。

Conclusion: BTP是一种有效的插件式方法，可以在显著减少计算开销的同时，保持模型性能。

Abstract: Large Vision-Language Models (LVLMs) have shown impressive performance across
multi-modal tasks by encoding images into thousands of tokens. However, the
large number of image tokens results in significant computational overhead, and
the use of dynamic high-resolution inputs further increases this burden.
Previous approaches have attempted to reduce the number of image tokens through
token pruning, typically by selecting tokens based on attention scores or image
token diversity. Through empirical studies, we observe that existing methods
often overlook the joint impact of pruning on both the current layer's output
(local) and the outputs of subsequent layers (global), leading to suboptimal
pruning decisions. To address this challenge, we propose Balanced Token Pruning
(BTP), a plug-and-play method for pruning vision tokens. Specifically, our
method utilizes a small calibration set to divide the pruning process into
multiple stages. In the early stages, our method emphasizes the impact of
pruning on subsequent layers, whereas in the deeper stages, the focus shifts
toward preserving the consistency of local outputs. Extensive experiments
across various LVLMs demonstrate the broad effectiveness of our approach on
multiple benchmarks. Our method achieves a 78% compression rate while
preserving 96.7% of the original models' performance on average.

</details>


### [244] [From Failures to Fixes: LLM-Driven Scenario Repair for Self-Evolving Autonomous Driving](https://arxiv.org/abs/2505.22067)
*Xinyu Xia,Xingjun Ma,Yunfeng Hu,Ting Qu,Hong Chen,Xun Gong*

Main category: cs.CV

TL;DR: SERA is an LLM-powered framework designed to enhance autonomous driving systems by identifying and repairing failure cases through targeted scenario recommendations, which improves key metrics in safety-critical conditions.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating and selecting scenarios often lack adaptivity and semantic relevance, thus limiting their impact on improving the performance of autonomous driving systems.

Method: The SERA framework analyzes performance logs to identify failure patterns, dynamically retrieves semantically aligned scenarios from a structured bank, and refines these recommendations using an LLM-based reflection mechanism. Selected scenarios are used for few-shot fine-tuning to enable targeted adaptation with minimal data.

Result: Experiments demonstrate that SERA consistently improves key metrics across multiple autonomous driving baselines, showing its effectiveness and generalizability under safety-critical conditions.

Conclusion: SERA enables autonomous driving systems to self-evolve by efficiently repairing failure cases through targeted scenario recommendation.

Abstract: Ensuring robust and generalizable autonomous driving requires not only broad
scenario coverage but also efficient repair of failure cases, particularly
those related to challenging and safety-critical scenarios. However, existing
scenario generation and selection methods often lack adaptivity and semantic
relevance, limiting their impact on performance improvement. In this paper, we
propose \textbf{SERA}, an LLM-powered framework that enables autonomous driving
systems to self-evolve by repairing failure cases through targeted scenario
recommendation. By analyzing performance logs, SERA identifies failure patterns
and dynamically retrieves semantically aligned scenarios from a structured
bank. An LLM-based reflection mechanism further refines these recommendations
to maximize relevance and diversity. The selected scenarios are used for
few-shot fine-tuning, enabling targeted adaptation with minimal data.
Experiments on the benchmark show that SERA consistently improves key metrics
across multiple autonomous driving baselines, demonstrating its effectiveness
and generalizability under safety-critical conditions.

</details>


### [245] [Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment](https://arxiv.org/abs/2505.21561)
*Omid Halimi Milani,Amanda Nikho,Marouane Tliba,Lauren Mills,Ahmet Enis Cetin,Mohammed H Elnagar*

Main category: cs.CV

TL;DR: The paper presents a new deep learning framework for automatically staging spheno-occipital synchondrosis (SOS) fusion, using a dual-model architecture with knowledge distillation to achieve high diagnostic accuracy without needing extra pre-processing tools.


<details>
  <summary>Details</summary>
Motivation: To create an automated and efficient method for staging SOS fusion, which is important in orthodontics and forensic anthropology, while avoiding reliance on external cropping or segmentation methods.

Method: A dual-model architecture where a teacher model trained on manually cropped images transfers its knowledge to a student model operating on full images through a novel loss function incorporating gradient-based attention spatial mapping.

Result: Achieves robust diagnostic accuracy and creates a clinically viable end-to-end pipeline for skeletal maturation assessment.

Conclusion: This approach eliminates the need for additional pre-processing tools, enhances efficiency and consistency, and accelerates deployment in clinical settings.

Abstract: We introduce a novel deep learning framework for the automated staging of
spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in
both orthodontics and forensic anthropology. Our approach leverages a
dual-model architecture wherein a teacher model, trained on manually cropped
images, transfers its precise spatial understanding to a student model that
operates on full, uncropped images. This knowledge distillation is facilitated
by a newly formulated loss function that aligns spatial logits as well as
incorporates gradient-based attention spatial mapping, ensuring that the
student model internalizes the anatomically relevant features without relying
on external cropping or YOLO-based segmentation. By leveraging expert-curated
data and feedback at each step, our framework attains robust diagnostic
accuracy, culminating in a clinically viable end-to-end pipeline. This
streamlined approach obviates the need for additional pre-processing tools and
accelerates deployment, thereby enhancing both the efficiency and consistency
of skeletal maturation assessment in diverse clinical settings.

</details>


### [246] [Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model](https://arxiv.org/abs/2505.21564)
*Koki Matsuishi,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: In deep multi-instance learning, especially for brain hematoma CT images, learning becomes difficult when the number of instances increases. This paper proposes a pre-trained model with self-supervised learning to overcome this problem.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty in learning when the number of instances in a bag increases to 256 in brain hematoma CT images.

Method: The proposed method uses a pre-trained model with self-supervised learning for the multi-instance learner as a downstream task.

Result: This method shows improvements of 5% to 13% in accuracy and 40% to 55% in the F1 measure for the hypodensity marker classification of brain hematoma CT.

Conclusion: Using a pre-trained model with self-supervised learning can significantly improve the performance of multi-instance learning in brain hematoma CT images.

Abstract: In deep multi-instance learning, the number of applicable instances depends
on the data set. In histopathology images, deep learning multi-instance
learners usually assume there are hundreds to thousands instances in a bag.
However, when the number of instances in a bag increases to 256 in brain
hematoma CT, learning becomes extremely difficult. In this paper, we address
this drawback. To overcome this problem, we propose using a pre-trained model
with self-supervised learning for the multi-instance learner as a downstream
task. With this method, even when the original target task suffers from the
spurious correlation problem, we show improvements of 5% to 13% in accuracy and
40% to 55% in the F1 measure for the hypodensity marker classification of brain
hematoma CT.

</details>


### [247] [Diffusion Model-based Activity Completion for AI Motion Capture from Videos](https://arxiv.org/abs/2505.21566)
*Gao Huayu,Huang Tengjiu,Ye Xiaolong,Tsuyoshi Okita*

Main category: cs.CV

TL;DR: AI-based motion capture technology using a diffusion-model-based action completion technique for virtual humans, with competitive results on Human3.6M dataset.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of current AI motion capture methods which rely entirely on observed video sequences and cannot generate movements outside these sequences.

Method: Propose a diffusion-model-based action completion technique with a gate module and a position-time embedding module to generate smooth and continuous human motion sequences beyond observed actions.

Result: MDC-Net outperforms existing methods in ADE, FDE, and MMADE metrics but is slightly less accurate in MMFDE; it has a smaller model size compared to HumanMAC and generates more natural motion sequences. Also, a method for extracting sensor data from human motion sequences is proposed.

Conclusion: The proposed MDC-Net achieves competitive performance in generating flexible and coherent human motions, offering a cost-effective alternative to traditional motion capture systems.

Abstract: AI-based motion capture is an emerging technology that offers a
cost-effective alternative to traditional motion capture systems. However,
current AI motion capture methods rely entirely on observed video sequences,
similar to conventional motion capture. This means that all human actions must
be predefined, and movements outside the observed sequences are not possible.
To address this limitation, we aim to apply AI motion capture to virtual
humans, where flexible actions beyond the observed sequences are required. We
assume that while many action fragments exist in the training data, the
transitions between them may be missing. To bridge these gaps, we propose a
diffusion-model-based action completion technique that generates complementary
human motion sequences, ensuring smooth and continuous movements. By
introducing a gate module and a position-time embedding module, our approach
achieves competitive results on the Human3.6M dataset. Our experimental results
show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but
is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size
(16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural
and coherent motion sequences. Additionally, we propose a method for extracting
sensor data, including acceleration and angular velocity, from human motion
sequences.

</details>


### [248] [EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models](https://arxiv.org/abs/2505.21567)
*Feng Jiang,Zihao Zheng,Xiuping Cui,Maoliang Li,JIayu Chen,Xiang Chen*

Main category: cs.CV

TL;DR: This paper addresses the challenge of applying quantization to Vision-Language-Action (VLA) models, which are used in embodied artificial intelligence. The authors propose EaqVLA, a framework that uses encoding-aligned quantization for VLA models. This includes a method to identify misalignment and a mixed precision quantization technique. Experiments show that EaqVLA performs better than existing methods, with minimal quantization loss and significant acceleration.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to optimize the expensive computing and storage costs associated with existing Vision-Language-Action (VLA) models, which are prevalent in embodied artificial intelligence. Quantization is seen as an effective solution, but the token alignment in VLA models poses a challenge for its application.

Method: The authors propose EaqVLA, an optimized framework that applies encoding-aligned quantization to VLA models. They introduce a complete analysis method to detect misalignment at various granularities within the VLA models. Based on these findings, they implement a mixed precision quantization approach that considers encoding alignment.

Result: Experiments demonstrate that EaqVLA achieves superior quantization performance compared to existing methods, characterized by minimal quantization loss for end-to-end action control and a significant acceleration factor (xxx times).

Conclusion: EaqVLA, with its encoding-aligned quantization strategy, successfully overcomes the challenges posed by token alignment in VLA models. It provides an effective means to reduce memory costs and accelerate computations in VLA models, outperforming current quantization techniques.

Abstract: With the development of Embodied Artificial intelligence, the end-to-end
control policy such as Vision-Language-Action (VLA) model has become the
mainstream. Existing VLA models faces expensive computing/storage cost, which
need to be optimized. Quantization is considered as the most effective method
which can not only reduce the memory cost but also achieve computation
acceleration. However, we find the token alignment of VLA models hinders the
application of existing quantization methods. To address this, we proposed an
optimized framework called EaqVLA, which apply encoding-aligned quantization to
VLA models. Specifically, we propose an complete analysis method to find the
misalignment in various granularity. Based on the analysis results, we propose
a mixed precision quantization with the awareness of encoding alignment.
Experiments shows that the porposed EaqVLA achieves better quantization
performance (with the minimal quantization loss for end-to-end action control
and xxx times acceleration) than existing quantization methods.

</details>


### [249] [Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models](https://arxiv.org/abs/2505.21574)
*Dang Nguyen,Jiping Li,Jinghao Zheng,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: 通过仅增强未在训练初期被学习的部分数据，而非整个数据集，可以改善图像分类器的泛化性能。方法在多种模型和数据集上测试，提升最高达2.8%，并能与现有增广策略结合进一步提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有的使用扩散模型合成增强训练数据集的方法虽然有效，但在保证生成多样性方面存在困难，并且需要将数据量增加10-30倍以提高分布内性能。因此，研究者探索了一种新的数据增强策略，即仅对训练初期未被快速学习的数据部分进行增强。

Method: 研究者分析了两层CNN，证明仅增强部分数据（约30%-40%）而非整个数据集，能够通过促进特征学习速度的同质性来改善泛化性能，同时不会放大噪声。此方法适用于多种模型（如ResNet、ViT、DenseNet）和数据集（如CIFAR-10、CIFAR-100、TinyImageNet），以及不同的优化器（如SGD、SAM）。

Result: 实验结果表明，在不同场景下，该方法可将性能提升高达2.8%。特别是在CIFAR-100和TinyImageNet数据集上，使用SGD优化器时，其表现优于最先进的优化器SAM。此外，该方法可以轻松与现有的弱或强数据增强策略叠加，进一步提升性能。

Conclusion: 通过选择性地增强训练初期未被快速学习的数据部分，可以更有效地改善图像分类器的泛化性能，而无需大幅增加数据量。此方法简单易用，适用于多种模型、数据集和优化器，并能与现有增强策略结合，进一步提升性能。

Abstract: Synthetically augmenting training datasets with diffusion models has been an
effective strategy for improving generalization of image classifiers. However,
existing techniques struggle to ensure the diversity of generation and increase
the size of the data by up to 10-30x to improve the in-distribution
performance. In this work, we show that synthetically augmenting part of the
data that is not learned early in training outperforms augmenting the entire
dataset. By analyzing a two-layer CNN, we prove that this strategy improves
generalization by promoting homogeneity in feature learning speed without
amplifying noise. Our extensive experiments show that by augmenting only
30%-40% of the data, our method boosts the performance by up to 2.8% in a
variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10,
CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM.
Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on
CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and
strong augmentation strategies to further boost the performance.

</details>


### [250] [SridBench: Benchmark of Scientific Research Illustration Drawing of Image Generation Model](https://arxiv.org/abs/2505.22126)
*Yifan Chang,Yukang Feng,Jianwen Sun,Jiaxin Ai,Chuanhao Li,S. Kevin Zhou,Kaipeng Zhang*

Main category: cs.CV

TL;DR: The paper introduces SridBench, the first benchmark for scientific figure generation, revealing that even advanced models like GPT-4o-image fall short of human performance in aspects such as text/visual clarity and scientific correctness.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a benchmark for evaluating AI's ability in scientific illustration generation, which is more knowledge-intensive and laborious compared to general image synthesis.

Method: Introduced SridBench, a benchmark consisting of 1,120 instances from leading scientific papers across 13 disciplines, with samples evaluated along six dimensions including semantic fidelity and structural accuracy.

Result: Experimental results showed that top-tier models like GPT-4o-image have shortcomings in text/visual clarity and scientific correctness compared to human performance.

Conclusion: There is a need for more advanced reasoning-driven visual generation capabilities to improve scientific illustration generation.

Abstract: Recent years have seen rapid advances in AI-driven image generation. Early
diffusion models emphasized perceptual quality, while newer multimodal models
like GPT-4o-image integrate high-level reasoning, improving semantic
understanding and structural composition. Scientific illustration generation
exemplifies this evolution: unlike general image synthesis, it demands accurate
interpretation of technical content and transformation of abstract ideas into
clear, standardized visuals. This task is significantly more
knowledge-intensive and laborious, often requiring hours of manual work and
specialized tools. Automating it in a controllable, intelligent manner would
provide substantial practical value. Yet, no benchmark currently exists to
evaluate AI on this front. To fill this gap, we introduce SridBench, the first
benchmark for scientific figure generation. It comprises 1,120 instances
curated from leading scientific papers across 13 natural and computer science
disciplines, collected via human experts and MLLMs. Each sample is evaluated
along six dimensions, including semantic fidelity and structural accuracy.
Experimental results reveal that even top-tier models like GPT-4o-image lag
behind human performance, with common issues in text/visual clarity and
scientific correctness. These findings highlight the need for more advanced
reasoning-driven visual generation capabilities.

</details>


### [251] [Real-Time Blind Defocus Deblurring for Earth Observation: The IMAGIN-e Mission Approach](https://arxiv.org/abs/2505.22128)
*Alejandro D. Mousist*

Main category: cs.CV

TL;DR: This paper proposes a blind deblurring approach for mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, which leverages Sentinel-2 data and operates within a GAN framework without reference images. It demonstrates substantial improvements in image quality metrics and is currently deployed aboard the IMAGIN-e mission.


<details>
  <summary>Details</summary>
Motivation: To address mechanical defocus in Earth observation images from the IMAGIN-e mission aboard the ISS, particularly under space-based edge computing constraints.

Method: The method estimates the defocus kernel and trains a restoration model within a GAN framework using Sentinel-2 data, operating without reference images.

Result: On synthetic degradation of Sentinel-2 images, SSIM improved by 72.47% and PSNR by 25.00%. On IMAGIN-e images without reference, NIQE improved by 60.66% and BRISQUE by 48.38%.

Conclusion: The proposed blind deblurring approach successfully enhances image quality under edge computing constraints and is practically applicable in an operational space environment.

Abstract: This work addresses mechanical defocus in Earth observation images from the
IMAGIN-e mission aboard the ISS, proposing a blind deblurring approach adapted
to space-based edge computing constraints. Leveraging Sentinel-2 data, our
method estimates the defocus kernel and trains a restoration model within a GAN
framework, effectively operating without reference images.
  On Sentinel-2 images with synthetic degradation, SSIM improved by 72.47% and
PSNR by 25.00%, confirming the model's ability to recover lost details when the
original clean image is known. On IMAGIN-e, where no reference images exist,
perceptual quality metrics indicate a substantial enhancement, with NIQE
improving by 60.66% and BRISQUE by 48.38%, validating real-world onboard
restoration. The approach is currently deployed aboard the IMAGIN-e mission,
demonstrating its practical application in an operational space environment.
  By efficiently handling high-resolution images under edge computing
constraints, the method enables applications such as water body segmentation
and contour detection while maintaining processing viability despite resource
limitations.

</details>


### [252] [FaceEditTalker: Interactive Talking Head Generation with Facial Attribute Editing](https://arxiv.org/abs/2505.22141)
*Guanwen Feng,Zhiyuan Ma,Yunan Li,Junwei Jing,Jiahao Yang,Qiguang Miao*

Main category: cs.CV

TL;DR: Recent advances in audio-driven talking head generation have achieved impressive results but overlook facial attribute editing. This paper presents FaceEditTalker, a framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos.


<details>
  <summary>Details</summary>
Motivation: Facial attribute editing is crucial for achieving deep personalization and expanding the range of practical applications, including user-tailored digital avatars, engaging online education content, and brand-specific digital customer service.

Method: FaceEditTalker consists of two key components: an image feature space editing module, which extracts semantic and detail features and allows flexible control over attributes like expression, hairstyle, and accessories; and an audio-driven video generation module, which fuses these edited features with audio-guided facial landmarks to drive a diffusion-based generator.

Result: Extensive experiments on public datasets demonstrate that our method outperforms state-of-the-art approaches in lip-sync accuracy, video quality, and attribute controllability.

Conclusion: FaceEditTalker is a unified framework that enables controllable facial attribute manipulation while generating high-quality, audio-synchronized talking head videos.

Abstract: Recent advances in audio-driven talking head generation have achieved
impressive results in lip synchronization and emotional expression. However,
they largely overlook the crucial task of facial attribute editing. This
capability is crucial for achieving deep personalization and expanding the
range of practical applications, including user-tailored digital avatars,
engaging online education content, and brand-specific digital customer service.
In these key domains, the flexible adjustment of visual attributes-such as
hairstyle, accessories, and subtle facial features is essential for aligning
with user preferences, reflecting diverse brand identities, and adapting to
varying contextual demands. In this paper, we present FaceEditTalker, a unified
framework that enables controllable facial attribute manipulation while
generating high-quality, audio-synchronized talking head videos. Our method
consists of two key components: an image feature space editing module, which
extracts semantic and detail features and allows flexible control over
attributes like expression, hairstyle, and accessories; and an audio-driven
video generation module, which fuses these edited features with audio-guided
facial landmarks to drive a diffusion-based generator. This design ensures
temporal coherence, visual fidelity, and identity preservation across frames.
Extensive experiments on public datasets demonstrate that our method
outperforms state-of-the-art approaches in lip-sync accuracy, video quality,
and attribute controllability. Project page:
https://peterfanfan.github.io/FaceEditTalker/

</details>


### [253] [Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language](https://arxiv.org/abs/2505.22146)
*Guangfu Hao,Haojie Wen,Liangxuna Guo,Yang Chen,Yanchao Bi,Shan Yu*

Main category: cs.CV

TL;DR: The paper presents a framework using low-dimensional attribute representations to connect visual tool perception and linguistic task understanding, achieving 74% accuracy in tool selection tasks with parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the underdevelopment of computational models that capture human's complex cognitive ability in flexible tool selection which distinguishes humans from other species.

Method: Developed a framework using low-dimensional attribute representations, constructed a dataset (ToolNet) with 115 common tools labeled by 13 attributes, used visual encoders (ResNet or ViT) to extract attributes from tool images and fine-tuned language models (GPT-2, LLaMA, DeepSeek) to derive required attributes from task descriptions.

Result: Achieved 74% accuracy in tool selection tasks, outperforming direct tool matching (20%) and smaller multimodal models (21%-58%), while approaching the performance of much larger models like GPT-4o (73%) with fewer parameters. Manipulation-related attributes were found to be most critical across modalities.

Conclusion: This work provides a parameter-efficient, interpretable solution that mimics human-like tool cognition, advancing both cognitive science understanding and practical applications in tool selection tasks.

Abstract: Flexible tool selection reflects a complex cognitive ability that
distinguishes humans from other species, yet computational models that capture
this ability remain underdeveloped. We developed a framework using
low-dimensional attribute representations to bridge visual tool perception and
linguistic task understanding. We constructed a comprehensive dataset (ToolNet)
containing 115 common tools labeled with 13 carefully designed attributes
spanning physical, functional, and psychological properties, paired with
natural language scenarios describing tool usage. Visual encoders (ResNet or
ViT) extract attributes from tool images while fine-tuned language models
(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our
approach achieves 74% accuracy in tool selection tasks-significantly
outperforming direct tool matching (20%) and smaller multimodal models
(21%-58%), while approaching performance of much larger models like GPT-4o
(73%) with substantially fewer parameters. Ablation studies revealed that
manipulation-related attributes (graspability, hand-relatedness, elongation)
consistently prove most critical across modalities. This work provides a
parameter-efficient, interpretable solution that mimics human-like tool
cognition, advancing both cognitive science understanding and practical
applications in tool selection tasks.

</details>


### [254] [QuARI: Query Adaptive Retrieval Improvement](https://arxiv.org/abs/2505.21647)
*Eric Xing,Abby Stylianou,Robert Pless,Nathan Jacobs*

Main category: cs.CV

TL;DR: 通过将查询映射到特定于查询的特征空间转换，提出了一种专为实例检索优化的方法，该方法计算成本低且在大规模检索或重新排序中表现优异，显著超越现有技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模预训练的视觉-语言模型在跨领域图像和文本检索任务中表现出色，但在处理具有挑战性的检索任务（如超大规模图像集合中的实例检索）时效果不佳。已有研究表明，通过对VLM特征进行线性变换以强调与目标领域相关的子空间可以提升性能。

Method: 学习将给定查询映射到特定于该查询的特征空间转换。由于这种转换是线性的，因此可以以极小的计算成本将其应用于数百万个图像嵌入，从而实现高效的大规模检索或重新排序。

Result: 实验结果表明，所提出的方法始终优于现有的最先进技术，包括那些在查询时需要多数量级更多计算的方法。

Conclusion: 本文提出的方法通过引入特定于查询的特征空间转换，在保持低计算成本的同时提高了大规模实例检索的性能，为相关领域提供了新的解决方案。

Abstract: Massive-scale pretraining has made vision-language models increasingly
popular for image-to-image and text-to-image retrieval across a broad
collection of domains. However, these models do not perform well when used for
challenging retrieval tasks, such as instance retrieval in very large-scale
image collections. Recent work has shown that linear transformations of VLM
features trained for instance retrieval can improve performance by emphasizing
subspaces that relate to the domain of interest. In this paper, we explore a
more extreme version of this specialization by learning to map a given query to
a query-specific feature space transformation. Because this transformation is
linear, it can be applied with minimal computational cost to millions of image
embeddings, making it effective for large-scale retrieval or re-ranking.
Results show that this method consistently outperforms state-of-the-art
alternatives, including those that require many orders of magnitude more
computation at query time.

</details>


### [255] [Investigating Mechanisms for In-Context Vision Language Binding](https://arxiv.org/abs/2505.22200)
*Darshana Saravanan,Makarand Tapaswi,Vineet Gandhi*

Main category: cs.CV

TL;DR: This paper explores how Vision-Language models (VLMs) use Binding IDs to associate images with text using a synthetic dataset.


<details>
  <summary>Details</summary>
Motivation: To understand how VLMs bind images and text together, specifically associating 3D objects in images with their textual descriptions.

Method: Using a synthetic dataset, the researchers conduct experiments to observe if VLMs assign distinct Binding IDs to objects' image tokens and their textual references.

Result: The experiments show that VLMs do assign distinct Binding IDs, allowing for in-context association between image and text.

Conclusion: Binding IDs play a crucial role in enabling VLMs to associate images with text effectively.

Abstract: To understand a prompt, Vision-Language models (VLMs) must perceive the
image, comprehend the text, and build associations within and across both
modalities. For instance, given an 'image of a red toy car', the model should
associate this image to phrases like 'car', 'red toy', 'red object', etc. Feng
and Steinhardt propose the Binding ID mechanism in LLMs, suggesting that the
entity and its corresponding attribute tokens share a Binding ID in the model
activations. We investigate this for image-text binding in VLMs using a
synthetic dataset and task that requires models to associate 3D objects in an
image with their descriptions in the text. Our experiments demonstrate that
VLMs assign a distinct Binding ID to an object's image tokens and its textual
references, enabling in-context association.

</details>


### [256] [Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks](https://arxiv.org/abs/2505.21736)
*Zachary Schlamowitz,Andrew Bennecke,Daniel J. Tward*

Main category: cs.CV

TL;DR: The paper presents a new approach to achieve equivariance in neural networks using 'moment kernels', which simplifies the exploitation of symmetries such as rotations and reflections for biomedical image analysis.


<details>
  <summary>Details</summary>
Motivation: Existing methods that exploit symmetries like rotations and reflections rely on complex mathematical concepts, limiting their adoption. The authors aim to simplify these methods to make them more accessible and applicable.

Method: The authors introduce 'moment kernels' as a simple form of convolution kernels that can achieve equivariance. These kernels consist of radially symmetric functions multiplied by powers of spatial position components or the identity matrix. They implement equivariant neural networks with standard convolution modules for various tasks.

Result: The authors successfully implemented architectures for classification, 3D image registration, and cell segmentation tasks, demonstrating the effectiveness of moment kernels in achieving equivariance.

Conclusion: Moment kernels provide a simpler way to achieve equivariance in neural networks, potentially leading to wider adoption of symmetry-exploiting techniques in biomedical image analysis.

Abstract: The principle of translation equivariance (if an input image is translated an
output image should be translated by the same amount), led to the development
of convolutional neural networks that revolutionized machine vision. Other
symmetries, like rotations and reflections, play a similarly critical role,
especially in biomedical image analysis, but exploiting these symmetries has
not seen wide adoption. We hypothesize that this is partially due to the
mathematical complexity of methods used to exploit these symmetries, which
often rely on representation theory, a bespoke concept in differential geometry
and group theory. In this work, we show that the same equivariance can be
achieved using a simple form of convolution kernels that we call ``moment
kernels,'' and prove that all equivariant kernels must take this form. These
are a set of radially symmetric functions of a spatial position $x$, multiplied
by powers of the components of $x$ or the identity matrix. We implement
equivariant neural networks using standard convolution modules, and provide
architectures to execute several biomedical image analysis tasks that depend on
equivariance principles: classification (outputs are invariant under orthogonal
transforms), 3D image registration (outputs transform like a vector), and cell
segmentation (quadratic forms defining ellipses transform like a matrix).

</details>


### [257] [What is Adversarial Training for Diffusion Models?](https://arxiv.org/abs/2505.21742)
*Briglia Maria Rosaria,Mujtaba Hussain Mirza,Giuseppe Lisanti,Iacopo Masi*

Main category: cs.CV

TL;DR: This paper explores adversarial training (AT) for diffusion models (DMs), showing it fundamentally differs from classifiers. AT in DMs requires equivariance to align the diffusion process with the data distribution, enforcing smoothness in the diffusion flow and improving robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to investigate how adversarial training can be applied to diffusion models, highlighting its fundamental differences from classifier-based adversarial training.

Method: The method involves integrating adversarial training into diffusion models by adding random or adversarial noise during training. This approach enforces equivariance and smoothness in the diffusion flow without making assumptions about the noise model.

Result: The results demonstrate strong performance under severe noise, data corruption, and iterative adversarial attacks when evaluated on proof-of-concept datasets with known distributions as well as standard benchmarks such as CIFAR-10, CelebA, and LSUN Bedroom.

Conclusion: Adversarial training for diffusion models improves robustness to outliers and corrupted data by enforcing smoothness in the diffusion flow through equivariance.

Abstract: We answer the question in the title, showing that adversarial training (AT)
for diffusion models (DMs) fundamentally differs from classifiers: while AT in
classifiers enforces output invariance, AT in DMs requires equivariance to keep
the diffusion process aligned with the data distribution. AT is a way to
enforce smoothness in the diffusion flow, improving robustness to outliers and
corrupted data. Unlike prior art, our method makes no assumptions about the
noise model and integrates seamlessly into diffusion training by adding random
noise, similar to randomized smoothing, or adversarial noise, akin to AT. This
enables intrinsic capabilities such as handling noisy data, dealing with
extreme variability such as outliers, preventing memorization, and improving
robustness. We rigorously evaluate our approach with proof-of-concept datasets
with known distributions in low- and high-dimensional space, thereby taking a
perfect measure of errors; we further evaluate on standard benchmarks such as
CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe
noise, data corruption, and iterative adversarial attacks.

</details>


### [258] [Neural Restoration of Greening Defects in Historical Autochrome Photographs Based on Purely Synthetic Data](https://arxiv.org/abs/2505.22291)
*Saptarshi Neil Sinha,P. Julius Kuehn,Johannes Koppe,Arjan Kuijper,Michael Weinmann*

Main category: cs.CV

TL;DR: The paper introduces an automatic method to remove greening color defects in digitized autochrome photographs using synthetic dataset generation and generative AI.


<details>
  <summary>Details</summary>
Motivation: To preserve early visual arts, particularly color photographs which are challenged by deterioration due to aging and improper storage leading to issues such as blurring, scratches, color bleeding, and fading defects.

Method: The method is based on synthetic dataset generation and the use of generative AI with a carefully designed loss function for the restoration of visual arts. They introduce a novel approach for accurately simulating greening defects in synthetic data and propose a modified weighted loss function for the ChaIR method to account for color imbalances between defected and non-defected areas.

Result: Existing methods struggle with accurately reproducing original colors and may require significant manual effort, whereas this new method allows for efficient restoration with reduced time requirements.

Conclusion: This is the first approach for the automatic removal of greening color defects in digitized autochrome photographs.

Abstract: The preservation of early visual arts, particularly color photographs, is
challenged by deterioration caused by aging and improper storage, leading to
issues like blurring, scratches, color bleeding, and fading defects. In this
paper, we present the first approach for the automatic removal of greening
color defects in digitized autochrome photographs. Our main contributions
include a method based on synthetic dataset generation and the use of
generative AI with a carefully designed loss function for the restoration of
visual arts. To address the lack of suitable training datasets for analyzing
greening defects in damaged autochromes, we introduce a novel approach for
accurately simulating such defects in synthetic data. We also propose a
modified weighted loss function for the ChaIR method to account for color
imbalances between defected and non-defected areas. While existing methods
struggle with accurately reproducing original colors and may require
significant manual effort, our method allows for efficient restoration with
reduced time requirements.

</details>


### [259] [UniMoGen: Universal Motion Generation](https://arxiv.org/abs/2505.21837)
*Aliasghar Khani,Arianna Rampini,Evan Atherton,Bruno Roy*

Main category: cs.CV

TL;DR: UniMoGen is a UNet-based diffusion model for skeleton-agnostic motion generation, capable of handling diverse characters and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation methods are limited by reliance on specific skeletal structures, restricting versatility across different characters.

Method: Introduced UniMoGen, a UNet-based diffusion model that can be trained on motion data from diverse characters without predefined joint limits, achieving skeleton agnosticism and computational efficiency. It includes controllability via style and trajectory inputs and the ability to continue motions from past frames.

Result: UniMoGen outperforms state-of-the-art methods in diverse character motion generation on the 100style dataset and achieves high performance and improved efficiency when trained on both 100style and LAFAN1 datasets.

Conclusion: UniMoGen provides a flexible, efficient, and controllable solution for a wide range of character animations, advancing motion generation.

Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming,
and robotics, enabling the creation of realistic and varied character
movements. A significant limitation of existing methods is their reliance on
specific skeletal structures, which restricts their versatility across
different characters. To overcome this, we introduce UniMoGen, a novel
UNet-based diffusion model designed for skeleton-agnostic motion generation.
UniMoGen can be trained on motion data from diverse characters, such as humans
and animals, without the need for a predefined maximum number of joints. By
dynamically processing only the necessary joints for each character, our model
achieves both skeleton agnosticism and computational efficiency. Key features
of UniMoGen include controllability via style and trajectory inputs, and the
ability to continue motions from past frames. We demonstrate UniMoGen's
effectiveness on the 100style dataset, where it outperforms state-of-the-art
methods in diverse character motion generation. Furthermore, when trained on
both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen
achieves high performance and improved efficiency across both skeletons. These
results highlight UniMoGen's potential to advance motion generation by
providing a flexible, efficient, and controllable solution for a wide range of
character animations.

</details>


### [260] [VME: A Satellite Imagery Dataset and Benchmark for Detecting Vehicles in the Middle East and Beyond](https://arxiv.org/abs/2505.22353)
*Noora Al-Emadi,Ingmar Weber,Yin Yang,Ferda Ofli*

Main category: cs.CV

TL;DR: Detecting vehicles in satellite images is important for various applications but current models lack diversity. The paper introduces VME dataset for Middle Eastern countries and CDSI benchmark which improve vehicle detection accuracy in these regions and globally.


<details>
  <summary>Details</summary>
Motivation: Current vehicle detection models struggle with real-world diversity, especially across different geographic regions due to bias in existing datasets.

Method: Introduced VME dataset with high-resolution images from Middle Eastern countries and CDSI benchmark combining images from multiple sources. Both datasets are used to train and evaluate detection models.

Result: Models trained on existing datasets perform poorly on Middle Eastern images while VME dataset improves detection accuracy in this region. CDSI-trained models achieve better global car detection.

Conclusion: VME and CDSI datasets fill critical gaps in vehicle detection, improving accuracy in Middle Eastern regions and globally.

Abstract: Detecting vehicles in satellite images is crucial for traffic management,
urban planning, and disaster response. However, current models struggle with
real-world diversity, particularly across different regions. This challenge is
amplified by geographic bias in existing datasets, which often focus on
specific areas and overlook regions like the Middle East. To address this gap,
we present the Vehicles in the Middle East (VME) dataset, designed explicitly
for vehicle detection in high-resolution satellite images from Middle Eastern
countries. Sourced from Maxar, the VME dataset spans 54 cities across 12
countries, comprising over 4,000 image tiles and more than 100,000 vehicles,
annotated using both manual and semi-automated methods. Additionally, we
introduce the largest benchmark dataset for Car Detection in Satellite Imagery
(CDSI), combining images from multiple sources to enhance global car detection.
Our experiments demonstrate that models trained on existing datasets perform
poorly on Middle Eastern images, while the VME dataset significantly improves
detection accuracy in this region. Moreover, state-of-the-art models trained on
CDSI achieve substantial improvements in global car detection.

</details>


### [261] [DAM: Domain-Aware Module for Multi-Domain Dataset Condensation](https://arxiv.org/abs/2505.22387)
*Jaehyun Choi,Gyojin Han,Dong-Jae Lee,Sunghyun Baek,Junmo Kim*

Main category: cs.CV

TL;DR: The paper presents Multi-Domain Dataset Condensation (MDDC) with Domain-Aware Module (DAM) to enhance dataset condensation across single and multi-domain settings, improving in-domain, out-of-domain, and cross-architecture performance without changing images per class.


<details>
  <summary>Details</summary>
Motivation: Existing Dataset Condensation methods fail to consider the multi-domain nature of modern datasets that consist of heterogeneous images from multiple domains.

Method: The authors introduce MDDC incorporating DAM, a training-time module embedding domain-related features into synthetic images via learnable spatial masks. Frequency-based pseudo-domain labeling is used for real-world datasets lacking explicit domain labels.

Result: Experiments demonstrate consistent improvements in in-domain, out-of-domain, and cross-architecture performance compared to baseline dataset condensation methods.

Conclusion: MDDC with DAM offers an effective solution for dataset condensation across both single and multi-domain settings.

Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the
computational and storage burdens associated with training deep learning
models. However, existing DC methods largely overlook the multi-domain nature
of modern datasets, which are increasingly composed of heterogeneous images
spanning multiple domains. In this paper, we extend DC and introduce
Multi-Domain Dataset Condensation (MDDC), which aims to condense data that
generalizes across both single-domain and multi-domain settings. To this end,
we propose the Domain-Aware Module (DAM), a training-time module that embeds
domain-related features into each synthetic image via learnable spatial masks.
As explicit domain labels are mostly unavailable in real-world datasets, we
employ frequency-based pseudo-domain labeling, which leverages low-frequency
amplitude statistics. DAM is only active during the condensation process, thus
preserving the same images per class (IPC) with prior methods. Experiments show
that DAM consistently improves in-domain, out-of-domain, and cross-architecture
performance over baseline dataset condensation methods.

</details>


### [262] [Can NeRFs See without Cameras?](https://arxiv.org/abs/2505.22441)
*Chaitanya Amballa,Sattwik Basu,Yu-Lin Wei,Zhijian Yang,Mehmet Ergezer,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: NeRFs can be redesigned to learn from multipath RF/audio signals, enabling inference of indoor floorplans from sparse WiFi measurements.


<details>
  <summary>Details</summary>
Motivation: To explore the possibility of inferring environmental information using multipath RF/audio signals, similar to how NeRFs synthesize novel views of 3D scenes.

Method: Redesign NeRFs to process and learn from multipath signals received by RF/audio sensors, applying this to infer indoor floorplans from sparse WiFi measurements at various locations within a home.

Result: The implicitly learned floorplans are promising, allowing for forward applications like indoor signal prediction and basic ray tracing.

Conclusion: Redesigned NeRFs can successfully interpret multipath signals to visualize environments, offering potential in areas such as indoor signal analysis and basic ray tracing.

Abstract: Neural Radiance Fields (NeRFs) have been remarkably successful at
synthesizing novel views of 3D scenes by optimizing a volumetric scene
function. This scene function models how optical rays bring color information
from a 3D object to the camera pixels. Radio frequency (RF) or audio signals
can also be viewed as a vehicle for delivering information about the
environment to a sensor. However, unlike camera pixels, an RF/audio sensor
receives a mixture of signals that contain many environmental reflections (also
called "multipath"). Is it still possible to infer the environment using such
multipath signals? We show that with redesign, NeRFs can be taught to learn
from multipath signals, and thereby "see" the environment. As a grounding
application, we aim to infer the indoor floorplan of a home from sparse WiFi
measurements made at multiple locations inside the home. Although a difficult
inverse problem, our implicitly learnt floorplans look promising, and enables
forward applications, such as indoor signal prediction and basic ray tracing.

</details>


### [263] [NFR: Neural Feature-Guided Non-Rigid Shape Registration](https://arxiv.org/abs/2505.22445)
*Puhua Jiang,Zhangquan Chen,Mingze Sun,Ruqi Huang*

Main category: cs.CV

TL;DR: 提出了一种新的基于学习的3D形状配准框架，该框架克服了输入形状之间的显著非刚性变形和局部性挑战，并且在训练期间不需要对应注释。通过将深度学习形状匹配网络学习的神经特征纳入迭代几何形状配准管道中，提供更准确和语义上有意义的对应估计，并根据中间配准动态更新对应关系以提高整体管道的鲁棒性。实验结果表明，在少量可变性有限的训练形状下，该方法在多个基准测试中实现了最先进的结果，并在处理未见过的具有显著外在和内在变形的形状对时表现出色。


<details>
  <summary>Details</summary>
Motivation: 目前的形状配准方法难以处理显著的非刚性变形和部分形状问题，并且通常需要大量的对应注释数据进行训练。为了克服这些问题，本文提出了一种无需对应注释的新框架。

Method: 1. 将深度学习形状匹配网络学习的神经特征融入到迭代几何形状配准流程中。
2. 使用神经特征代替空间特征（如坐标）来提供更准确和语义上有意义的对应估计。
3. 根据中间配准动态更新对应关系，并通过一致性先验过滤不稳定的对应关系，从而增强整体管道的鲁棒性。

Result: 1. 在仅使用几十个变化有限的训练形状的情况下，该方法在多个非刚性点云匹配和部分形状匹配基准上达到了最先进的性能。
2. 对于经历显著外在和内在变形的未见形状对，该方法能够提供高质量的对应关系，而传统方法和内在方法在这种情况下无法有效工作。

Conclusion: 所提出的框架通过结合神经特征和动态更新策略，显著提高了非刚性形状配准和部分形状匹配的效果，同时避免了对大量标注数据的需求，展现了良好的泛化能力和鲁棒性。

Abstract: In this paper, we propose a novel learning-based framework for 3D shape
registration, which overcomes the challenges of significant non-rigid
deformation and partiality undergoing among input shapes, and, remarkably,
requires no correspondence annotation during training. Our key insight is to
incorporate neural features learned by deep learning-based shape matching
networks into an iterative, geometric shape registration pipeline. The
advantage of our approach is two-fold -- On one hand, neural features provide
more accurate and semantically meaningful correspondence estimation than
spatial features (e.g., coordinates), which is critical in the presence of
large non-rigid deformations; On the other hand, the correspondences are
dynamically updated according to the intermediate registrations and filtered by
consistency prior, which prominently robustify the overall pipeline. Empirical
results show that, with as few as dozens of training shapes of limited
variability, our pipeline achieves state-of-the-art results on several
benchmarks of non-rigid point cloud matching and partial shape matching across
varying settings, but also delivers high-quality correspondences between unseen
challenging shape pairs that undergo both significant extrinsic and intrinsic
deformations, in which case neither traditional registration methods nor
intrinsic methods work.

</details>


### [264] [Fostering Video Reasoning via Next-Event Prediction](https://arxiv.org/abs/2505.22457)
*Haonan Wang,Hongfu Liu,Xiangyan Liu,Chao Du,Kenji Kawaguchi,Ye Wang,Tianyu Pang*

Main category: cs.CV

TL;DR: Next-event prediction (NEP) is proposed as a learning task that uses future video segments to promote temporal reasoning in MLLMs. A dataset V1-33K and evaluation benchmark FutureBench are introduced.


<details>
  <summary>Details</summary>
Motivation: Existing tasks for temporal reasoning in video inputs either rely on human/stronger MLLM annotations or entangle temporal reasoning with spatial information.

Method: Propose NEP where MLLM predicts a summary of events from future video frames given past frames as input, along with a dataset V1-33K and instruction-tuning strategies.

Result: Experiments show NEP is a scalable and effective training paradigm for fostering temporal reasoning in MLLMs.

Conclusion: NEP, supported by V1-33K and FutureBench, provides a promising direction for enhancing temporal reasoning capabilities in MLLMs.

Abstract: Next-token prediction serves as the foundational learning task enabling
reasoning in LLMs. But what should the learning task be when aiming to equip
MLLMs with temporal reasoning capabilities over video inputs? Existing tasks
such as video question answering often rely on annotations from humans or much
stronger MLLMs, while video captioning tends to entangle temporal reasoning
with spatial information. To address this gap, we propose next-event prediction
(NEP), a learning task that harnesses future video segments as a rich,
self-supervised signal to foster temporal reasoning. We segment each video into
past and future frames: the MLLM takes the past frames as input and predicts a
summary of events derived from the future frames, thereby encouraging the model
to reason temporally in order to complete the task. To support this task, we
curate V1-33K, a dataset comprising 33,000 automatically extracted video
segments spanning diverse real-world scenarios. We further explore a range of
video instruction-tuning strategies to study their effects on temporal
reasoning. To evaluate progress, we introduce FutureBench to assess coherence
in predicting unseen future events. Experiments validate that NEP offers a
scalable and effective training paradigm for fostering temporal reasoning in
MLLMs.

</details>


### [265] [On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation](https://arxiv.org/abs/2505.22099)
*Wenwen Qiang,Ziyin Gu,Lingyu Si,Jiangmeng Li,Changwen Zheng,Fuchun Sun,Hui Xiong*

Main category: cs.CV

TL;DR: In this paper, the authors propose a novel adversarial-based Unsupervised Domain Adaptation (UDA) framework called RLGLC. It integrates domain alignment with discriminability-enhancing constraints and uses AR-WWD to address class imbalance and semantic dimension weighting. Experiments show that RLGLC surpasses state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The standard adversarial-based framework in UDA only focuses on distribution alignment and source-domain empirical risk minimization, neglecting the discriminability of target-domain features which leads to suboptimal performance.

Method: The authors define 'good representation learning' as guaranteeing both transferability and discriminability, then they prove that an additional loss term for target-domain discriminability is necessary. They propose RLGLC method which leverages AR-WWD and a local consistency mechanism.

Result: Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently outperforms state-of-the-art methods.

Conclusion: The proposed RLGLC method confirms the value of the theoretical perspective and emphasizes the necessity of enforcing both transferability and discriminability in adversarial-based UDA.

Abstract: In this paper, we addressed the limitation of relying solely on distribution
alignment and source-domain empirical risk minimization in Unsupervised Domain
Adaptation (UDA). Our information-theoretic analysis showed that this standard
adversarial-based framework neglects the discriminability of target-domain
features, leading to suboptimal performance. To bridge this
theoretical-practical gap, we defined "good representation learning" as
guaranteeing both transferability and discriminability, and proved that an
additional loss term targeting target-domain discriminability is necessary.
Building on these insights, we proposed a novel adversarial-based UDA framework
that explicitly integrates a domain alignment objective with a
discriminability-enhancing constraint. Instantiated as Domain-Invariant
Representation Learning with Global and Local Consistency (RLGLC), our method
leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD)
to address class imbalance and semantic dimension weighting, and employs a
local consistency mechanism to preserve fine-grained target-domain
discriminative information. Extensive experiments across multiple benchmark
datasets demonstrate that RLGLC consistently surpasses state-of-the-art
methods, confirming the value of our theoretical perspective and underscoring
the necessity of enforcing both transferability and discriminability in
adversarial-based UDA.

</details>


### [266] [Thinking with Generated Images](https://arxiv.org/abs/2505.22525)
*Ethan Chern,Zhulin Hu,Steffi Chern,Siqi Kou,Jiadi Su,Yan Ma,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为'用生成图像思考'的新范式，使大型多模态模型能够通过自动生成中间视觉思维步骤，在文本和视觉模态之间进行原生的思考。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉推理受限于处理固定用户提供的图像或仅通过基于文本的链式思维进行推理，缺乏主动构建中间视觉思维的能力。

Method: 通过两种互补机制实现：(1) 带中间视觉子目标的视觉生成，将复杂任务分解为逐步生成和集成的部分；(2) 带自我批评的视觉生成，生成初始假设并通过文本推理分析其不足并改进。

Result: 实验表明，在视觉生成基准上比基线方法有显著改进，特别是在处理复杂多对象场景时，相对改进高达50%（从38%提升到57%）。

Conclusion: 该方法允许AI模型像人类一样进行视觉想象和迭代改进，适用于多种领域如生物化学、建筑、法医学和篮球策略等，并且代码已开源。

Abstract: We present Thinking with Generated Images, a novel paradigm that
fundamentally transforms how large multimodal models (LMMs) engage with visual
reasoning by enabling them to natively think across text and vision modalities
through spontaneous generation of intermediate visual thinking steps. Current
visual reasoning with LMMs is constrained to either processing fixed
user-provided images or reasoning solely through text-based chain-of-thought
(CoT). Thinking with Generated Images unlocks a new dimension of cognitive
capability where models can actively construct intermediate visual thoughts,
critique their own visual hypotheses, and refine them as integral components of
their reasoning process. We demonstrate the effectiveness of our approach
through two complementary mechanisms: (1) vision generation with intermediate
visual subgoals, where models decompose complex visual tasks into manageable
components that are generated and integrated progressively, and (2) vision
generation with self-critique, where models generate an initial visual
hypothesis, analyze its shortcomings through textual reasoning, and produce
refined outputs based on their own critiques. Our experiments on vision
generation benchmarks show substantial improvements over baseline approaches,
with our models achieving up to 50% (from 38% to 57%) relative improvement in
handling complex multi-object scenarios. From biochemists exploring novel
protein structures, and architects iterating on spatial designs, to forensic
analysts reconstructing crime scenes, and basketball players envisioning
strategic plays, our approach enables AI models to engage in the kind of visual
imagination and iterative refinement that characterizes human creative,
analytical, and strategic thinking. We release our open-source suite at
https://github.com/GAIR-NLP/thinking-with-generated-images.

</details>


### [267] [Scaling-up Perceptual Video Quality Assessment](https://arxiv.org/abs/2505.22543)
*Ziheng Jia,Zicheng Zhang,Zeyu Zhang,Yingji Liang,Xiaorong Zhu,Chunyi Li,Jinliang Han,Haoning Wu,Bin Wang,Haoran Zhang,Guanyu Zhu,Qiyong Zhao,Xiaohong Liu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: The paper introduces OmniVQA, a framework for building high-quality VQA MIDBs, and creates the largest such database (OmniVQA-Chat-400K). They also build OmniVQA-MOS-20K to improve quality rating, propose a complementary training strategy, and introduce OmniVQA-FG-Benchmark for evaluation. The models achieve state-of-the-art performance in both quality understanding and rating tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of labeled resources and insufficient dataset scale in perceptual video quality assessment (VQA), which limits the potential of data scaling law in enhancing model performance.

Method: 1. Propose OmniVQA - an efficient framework for building high-quality human-in-the-loop VQA multi-modal instruction databases (MIDBs).
2. Scale up to create OmniVQA-Chat-400K, the largest MIDB in the VQA field.
3. Build OmniVQA-MOS-20K dataset to enhance the model's quantitative quality rating capabilities.
4. Introduce a complementary training strategy that leverages knowledge from datasets for quality understanding and rating tasks.
5. Propose OmniVQA-FG-Benchmark to evaluate the fine-grained performance of models.

Result: Models trained using the proposed framework and datasets achieve state-of-the-art performance in both quality understanding and rating tasks.

Conclusion: OmniVQA framework and associated datasets effectively address the challenges of limited labeled resources and dataset scale in VQA, demonstrating significant improvements in model performance.

Abstract: The data scaling law has been shown to significantly enhance the performance
of large multi-modal models (LMMs) across various downstream tasks. However, in
the domain of perceptual video quality assessment (VQA), the potential of
scaling law remains unprecedented due to the scarcity of labeled resources and
the insufficient scale of datasets. To address this, we propose
\textbf{OmniVQA}, an efficient framework designed to efficiently build
high-quality, human-in-the-loop VQA multi-modal instruction databases (MIDBs).
We then scale up to create \textbf{OmniVQA-Chat-400K}, the largest MIDB in the
VQA field concurrently. Our focus is on the technical and aesthetic quality
dimensions, with abundant in-context instruction data to provide fine-grained
VQA knowledge. Additionally, we have built the \textbf{OmniVQA-MOS-20K} dataset
to enhance the model's quantitative quality rating capabilities. We then
introduce a \textbf{complementary} training strategy that effectively leverages
the knowledge from datasets for quality understanding and quality rating tasks.
Furthermore, we propose the \textbf{OmniVQA-FG (fine-grain)-Benchmark} to
evaluate the fine-grained performance of the models. Our results demonstrate
that our models achieve state-of-the-art performance in both quality
understanding and rating tasks.

</details>


### [268] [PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion](https://arxiv.org/abs/2505.22564)
*Jaehyun Choi,Jiwan Hur,Gyojin Han,Jaemyung Yu,Junmo Kim*

Main category: cs.CV

TL;DR: PRISM is a new method for video dataset condensation that integrates static content and dynamic motion, offering better performance with reduced storage needs.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of video data condensation which involves both spatial content and temporal dynamics, unlike the simpler image data.

Method: PRISM progressively refines and inserts frames to capture motion in actions while considering frame gradients, preserving the interdependence of static content and dynamic motion.

Result: Experiments on standard video action recognition benchmarks show PRISM outperforms disentangled approaches with compact representations ideal for resource-limited settings.

Conclusion: PRISM provides an improved approach to video dataset condensation by integrating static and dynamic elements, leading to better performance and efficient storage.

Abstract: Video dataset condensation has emerged as a critical technique for addressing
the computational challenges associated with large-scale video data processing
in deep learning applications. While significant progress has been made in
image dataset condensation, the video domain presents unique challenges due to
the complex interplay between spatial content and temporal dynamics. This paper
introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for
video dataset condensation, a novel approach that fundamentally reconsiders how
video data should be condensed. Unlike the previous method that separates
static content from dynamic motion, our method preserves the essential
interdependence between these elements. Our approach progressively refines and
inserts frames to fully accommodate the motion in an action while achieving
better performance but less storage, considering the relation of gradients for
each frame. Extensive experiments across standard video action recognition
benchmarks demonstrate that PRISM outperforms existing disentangled approaches
while maintaining compact representations suitable for resource-constrained
environments.

</details>


### [269] [Universal Visuo-Tactile Video Understanding for Embodied Interaction](https://arxiv.org/abs/2505.22566)
*Yifan Xie,Mingyang Li,Shoujie Li,Xingting Li,Guangyu Chen,Fei Ma,Fei Richard Yu,Wenbo Ding*

Main category: cs.CV

TL;DR: VTV-LLM is the first multi-modal large language model for universal Visuo-Tactile Video understanding, incorporating tactile information with natural language. It introduces VTV150K dataset and a three-stage training paradigm, achieving superior performance in tactile video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail to effectively incorporate tactile information which is crucial for real-world interaction.

Method: Developed VTV-LLM with a three-stage training paradigm including VTV enhancement, VTV-text alignment, and text prompt finetuning. Introduced VTV150K dataset with 150,000 video frames from 100 objects across three tactile sensors.

Result: Achieves superior performance in tactile video understanding tasks, enabling sophisticated tactile reasoning capabilities.

Conclusion: Establishes a foundation for more intuitive human-machine interaction in tactile domains.

Abstract: Tactile perception is essential for embodied agents to understand physical
attributes of objects that cannot be determined through visual inspection
alone. While existing approaches have made progress in visual and language
modalities for physical understanding, they fail to effectively incorporate
tactile information that provides crucial haptic feedback for real-world
interaction. In this paper, we present VTV-LLM, the first multi-modal large
language model for universal Visuo-Tactile Video (VTV) understanding that
bridges the gap between tactile perception and natural language. To address the
challenges of cross-sensor and cross-modal integration, we contribute VTV150K,
a comprehensive dataset comprising 150,000 video frames from 100 diverse
objects captured across three different tactile sensors (GelSight Mini, DIGIT,
and Tac3D), annotated with four fundamental tactile attributes (hardness,
protrusion, elasticity, and friction). We develop a novel three-stage training
paradigm that includes VTV enhancement for robust visuo-tactile representation,
VTV-text alignment for cross-modal correspondence, and text prompt finetuning
for natural language generation. Our framework enables sophisticated tactile
reasoning capabilities including feature assessment, comparative analysis,
scenario-based decision making and so on. Experimental evaluations demonstrate
that VTV-LLM achieves superior performance in tactile video understanding
tasks, establishing a foundation for more intuitive human-machine interaction
in tactile domains.

</details>


### [270] [Tell me Habibi, is it Real or Fake?](https://arxiv.org/abs/2505.22581)
*Kartik Kuckreja,Parul Gupta,Injy Hamed,Thamar Solorio,Muhammad Haris Khan,Abhinav Dhall*

Main category: cs.CV

TL;DR: The paper presents ArEnAV, a large-scale Arabic-English audio-visual deepfake dataset with code-switching and dialectal variation features, containing 387k videos. It aims to address the challenges in multilingual deepfake detection and provides benchmarks for advancing related research.


<details>
  <summary>Details</summary>
Motivation: Deepfake generation methods are rapidly evolving, making fake media harder to detect and raising societal concerns. Current deepfake detection research mainly focuses on monolingual content, neglecting the complexities of multilingual and code-switched speech, especially between Arabic and English which is common in the Arab world. This creates a need for datasets that can better represent these linguistic challenges.

Method: The authors developed ArEnAV, the first large-scale Arabic-English audio-visual deepfake dataset incorporating intra-utterance code-switching, dialectal variation, and monolingual Arabic content. The dataset contains 387k videos and over 765 hours of real and fake videos. It was created using a novel pipeline integrating four Text-To-Speech models and two lip-sync models to enable comprehensive analysis of multilingual multimodal deepfake detection.

Result: ArEnAV has been benchmarked against existing monolingual and multilingual datasets, state-of-the-art deepfake detection models, and human evaluations. The results show its potential to advance deepfake detection research by addressing the specific challenges of multilingual and code-switched content.

Conclusion: ArEnAV is an important contribution to the field of deepfake detection as it addresses the underrepresented area of multilingual and code-switched speech in deepfake research. By providing a large-scale dataset with diverse linguistic features, it offers new opportunities for developing more robust deepfake detection models.

Abstract: Deepfake generation methods are evolving fast, making fake media harder to
detect and raising serious societal concerns. Most deepfake detection and
dataset creation research focuses on monolingual content, often overlooking the
challenges of multilingual and code-switched speech, where multiple languages
are mixed within the same discourse. Code-switching, especially between Arabic
and English, is common in the Arab world and is widely used in digital
communication. This linguistic mixing poses extra challenges for deepfake
detection, as it can confuse models trained mostly on monolingual data. To
address this, we introduce \textbf{ArEnAV}, the first large-scale
Arabic-English audio-visual deepfake dataset featuring intra-utterance
code-switching, dialectal variation, and monolingual Arabic content. It
\textbf{contains 387k videos and over 765 hours of real and fake videos}. Our
dataset is generated using a novel pipeline integrating four Text-To-Speech and
two lip-sync models, enabling comprehensive analysis of multilingual multimodal
deepfake detection. We benchmark our dataset against existing monolingual and
multilingual datasets, state-of-the-art deepfake detection models, and a human
evaluation, highlighting its potential to advance deepfake research. The
dataset can be accessed
\href{https://huggingface.co/datasets/kartik060702/ArEnAV-Full}{here}.

</details>


### [271] [Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training](https://arxiv.org/abs/2505.22342)
*Shriram M S,Xinyue Hao,Shihao Hou,Yang Lu,Laura Sevilla-Lara,Anurag Arnab,Shreyank N Gowda*

Main category: cs.CV

TL;DR: The paper proposes Progressive Data Dropout, a new training paradigm that reduces effective epochs to 12.4% of baseline without sacrificing accuracy and even improving it by up to 4.82%. It requires no changes to model architecture or optimizer.


<details>
  <summary>Details</summary>
Motivation: To address the high cost associated with training large machine learning models due to the size of models and datasets, while existing research mainly focuses on reducing model size, this work explores alternative training methods to improve efficiency in dataset usage.

Method: Progressive Data Dropout is introduced which integrates insights from hard-data-mining and dropout. This method progressively reduces the number of data samples used for training, thereby decreasing the number of effective epochs required.

Result: Reduces the number of effective epochs to as little as 12.4% of the baseline without any loss in accuracy. In fact, it improves accuracy by up to 4.82%. The approach is easy to implement and can be applied across standard training pipelines.

Conclusion: Progressive Data Dropout offers an efficient alternative to standard training methods without compromising accuracy, making it a promising candidate for wide adoption in various machine learning training pipelines.

Abstract: The success of the machine learning field has reliably depended on training
on large datasets. While effective, this trend comes at an extraordinary cost.
This is due to two deeply intertwined factors: the size of models and the size
of datasets. While promising research efforts focus on reducing the size of
models, the other half of the equation remains fairly mysterious. Indeed, it is
surprising that the standard approach to training remains to iterate over and
over, uniformly sampling the training dataset. In this paper we explore a
series of alternative training paradigms that leverage insights from
hard-data-mining and dropout, simple enough to implement and use that can
become the new training standard. The proposed Progressive Data Dropout reduces
the number of effective epochs to as little as 12.4% of the baseline. This
savings actually do not come at any cost for accuracy. Surprisingly, the
proposed method improves accuracy by up to 4.82%. Our approach requires no
changes to model architecture or optimizer, and can be applied across standard
training pipelines, thus posing an excellent opportunity for wide adoption.
Code can be found here: https://github.com/bazyagami/LearningWithRevision

</details>


### [272] [RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction](https://arxiv.org/abs/2505.22613)
*Yuchi Wang,Yishuo Cai,Shuhuai Ren,Sihan Yang,Linli Yao,Yuanxin Liu,Yuanxing Zhang,Pengfei Wan,Xu Sun*

Main category: cs.CV

TL;DR: Image recaptioning method RICO refines captions through visual reconstruction using text-to-image models and MLLMs, reducing inaccuracies and improving caption quality. RICO-Flash reduces computational cost. Significant improvements on CapsBench and CompreCap.


<details>
  <summary>Details</summary>
Motivation: Existing image recaptioning methods rely on powerful multimodal large language models (MLLMs) to enhance textual descriptions but suffer from inaccuracies due to hallucinations and incompleteness caused by missing fine-grained details.

Method: Propose RICO, a framework that refines captions through visual reconstruction using a text-to-image model to reconstruct a caption into a reference image and prompting an MLLM to identify discrepancies between the original and reconstructed images to refine the caption. Introduce RICO-Flash, which learns to generate captions like RICO using DPO to mitigate additional computational cost.

Result: Extensive experiments demonstrate significant improvements in caption accuracy and completeness, outperforming most baselines by approximately 10% on both CapsBench and CompreCap.

Conclusion: RICO and RICO-Flash effectively improve caption accuracy and completeness while addressing limitations of existing methods.

Abstract: Image recaptioning is widely used to generate training datasets with enhanced
quality for various multimodal tasks. Existing recaptioning methods typically
rely on powerful multimodal large language models (MLLMs) to enhance textual
descriptions, but often suffer from inaccuracies due to hallucinations and
incompleteness caused by missing fine-grained details. To address these
limitations, we propose RICO, a novel framework that refines captions through
visual reconstruction. Specifically, we leverage a text-to-image model to
reconstruct a caption into a reference image, and prompt an MLLM to identify
discrepancies between the original and reconstructed images to refine the
caption. This process is performed iteratively, further progressively promoting
the generation of more faithful and comprehensive descriptions. To mitigate the
additional computational cost induced by the iterative process, we introduce
RICO-Flash, which learns to generate captions like RICO using DPO. Extensive
experiments demonstrate that our approach significantly improves caption
accuracy and completeness, outperforms most baselines by approximately 10% on
both CapsBench and CompreCap. Code released at
https://github.com/wangyuchi369/RICO.

</details>


### [273] [3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model](https://arxiv.org/abs/2505.22657)
*Wenbo Hu,Yining Hong,Yanjun Wang,Leison Gao,Zibu Wei,Xingcheng Yao,Nanyun Peng,Yonatan Bitton,Idan Szpektor,Kai-Wei Chang*

Main category: cs.CV

TL;DR: The paper presents 3DMem-Bench, a benchmark for evaluating long-term memory in 3D environments, and proposes 3DLLM-Mem, a model enabling efficient spatial-temporal reasoning and actions in LLMs.


<details>
  <summary>Details</summary>
Motivation: Humans effectively use long-term memory across temporal and spatial experiences to perform complex tasks, whereas current LLMs have difficulty planning and acting in dynamic, multi-room 3D environments due to the lack of proper 3D spatial-temporal memory modeling.

Method: Introduced 3DMem-Bench, a comprehensive benchmark with over 26,000 trajectories and 2,892 embodied tasks for evaluation. Proposed 3DLLM-Mem, a dynamic memory management and fusion model using working memory tokens as queries to selectively attend to and fuse spatial and temporal features from episodic memory.

Result: 3DLLM-Mem achieves state-of-the-art performance across various tasks, surpassing the strongest baselines by 16.5% in success rate on the most challenging in-the-wild embodied tasks in 3DMem-Bench.

Conclusion: 3DLLM-Mem enables agents to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments.

Abstract: Humans excel at performing complex tasks by leveraging long-term memory
across temporal and spatial experiences. In contrast, current Large Language
Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D
environments. We posit that part of this limitation is due to the lack of
proper 3D spatial-temporal memory modeling in LLMs. To address this, we first
introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000
trajectories and 2,892 embodied tasks, question-answering and captioning,
designed to evaluate an agent's ability to reason over long-term memory in 3D
environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management
and fusion model for embodied spatial-temporal reasoning and actions in LLMs.
Our model uses working memory tokens, which represents current observations, as
queries to selectively attend to and fuse the most useful spatial and temporal
features from episodic memory, which stores past observations and interactions.
Our approach allows the agent to focus on task-relevant information while
maintaining memory efficiency in complex, long-horizon environments.
Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art
performance across various tasks, outperforming the strongest baselines by
16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied
tasks.

</details>


### [274] [RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting](https://arxiv.org/abs/2505.22535)
*Mohamad Hakam Shams Eddin,Yikui Zhang,Stefan Kollet,Juergen Gall*

Main category: cs.CV

TL;DR: The paper introduces RiverMamba, a new deep learning model pretrained with long-term reanalysis data to forecast global river discharge and floods up to 7 days in advance. It uses Mamba blocks for capturing channel network routing and integrates ECMWF HRES forecasts while addressing inaccuracies through spatio-temporal modeling. The model outperforms both operational AI- and physics-based models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning approaches for hydrology are mostly local-scale and do not leverage the spatial connections of water bodies. There is a need for methodologies capable of modeling spatio-temporal relations for better river discharge and flood forecasting.

Method: RiverMamba leverages Mamba blocks to capture global-scale channel network routing and enhance forecast capability for longer lead times. Forecast blocks integrate ECMWF HRES meteorological forecasts while accounting for inaccuracies through spatio-temporal modeling.

Result: RiverMamba delivers reliable predictions of river discharge, including extreme floods across return periods and lead times, surpassing both operational AI- and physics-based models.

Conclusion: RiverMamba presents a significant advancement in global river discharge and flood forecasting by effectively modeling spatio-temporal relations and leveraging long-term reanalysis data.

Abstract: Recent deep learning approaches for river discharge forecasting have improved
the accuracy and efficiency in flood forecasting, enabling more reliable early
warning systems for risk management. Nevertheless, existing deep learning
approaches in hydrology remain largely confined to local-scale applications and
do not leverage the inherent spatial connections of bodies of water. Thus,
there is a strong need for new deep learning methodologies that are capable of
modeling spatio-temporal relations to improve river discharge and flood
forecasting for scientific and operational applications. To address this, we
present RiverMamba, a novel deep learning model that is pretrained with
long-term reanalysis data and that can forecast global river discharge and
floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high
relevance in early warning. To achieve this, RiverMamba leverages efficient
Mamba blocks that enable the model to capture global-scale channel network
routing and enhance its forecast capability for longer lead times. The forecast
blocks integrate ECMWF HRES meteorological forecasts, while accounting for
their inaccuracies through spatio-temporal modeling. Our analysis demonstrates
that RiverMamba delivers reliable predictions of river discharge, including
extreme floods across return periods and lead times, surpassing both
operational AI- and physics-based models.

</details>


### [275] [Sherlock: Self-Correcting Reasoning in Vision-Language Models](https://arxiv.org/abs/2505.22651)
*Yi Ding,Ruqi Zhang*

Main category: cs.CV

TL;DR: Sherlock is a self-correction and self-improvement training framework for reasoning Vision-Language Models (VLMs). It uses trajectory-level self-correction, preference data construction via visual perturbation, and dynamic preference tuning to enhance VLM performance. Built on Llama3.2-Vision-11B, Sherlock achieves an average accuracy of 64.1 with direct generation and 65.4 after self-correction across eight benchmarks.


<details>
  <summary>Details</summary>
Motivation: Reasoning VLMs show promise but are sensitive to errors, require extensive annotated data or verifiers, and struggle to generalize beyond specific domains. This calls for strategies that can enhance their robustness and generalization capabilities without relying heavily on external supervision or large datasets.

Method: The researchers first analyze the self-correction abilities of reasoning VLMs and identify gaps. They then introduce Sherlock, which incorporates: 1) a trajectory-level self-correction objective; 2) a method for constructing preference data using visual perturbations; and 3) a dynamic beta for preference tuning. The model initially learns from 20k randomly sampled annotated data and subsequently continues to self-improve without further external supervision.

Result: Sherlock outperforms other models such as LLaVA-CoT, Mulberry, and LlamaV-o1 in terms of accuracy across eight benchmarks while utilizing less than 20% of the annotated data required by these competing models.

Conclusion: Sherlock demonstrates the potential of self-correction and self-improvement strategies to significantly enhance the performance of reasoning VLMs, achieving higher accuracy with far fewer annotated data.

Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on
complex multimodal tasks. However, they still face significant challenges: they
are highly sensitive to reasoning errors, require large volumes of annotated
data or accurate verifiers, and struggle to generalize beyond specific domains.
To address these limitations, we explore self-correction as a strategy to
enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning
VLMs' self-correction abilities and identify key gaps. Based on our findings,
we introduce Sherlock, a self-correction and self-improvement training
framework. Sherlock introduces a trajectory-level self-correction objective, a
preference data construction method based on visual perturbation, and a dynamic
$\beta$ for preference tuning. Once the model acquires self-correction
capabilities using only 20k randomly sampled annotated data, it continues to
self-improve without external supervision. Built on the Llama3.2-Vision-11B
model, Sherlock achieves remarkable results across eight benchmarks, reaching
an average accuracy of 64.1 with direct generation and 65.4 after
self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and
LlamaV-o1 (63.4) while using less than 20% of the annotated data.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [276] [MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing](https://arxiv.org/abs/2505.21966)
*Aditya Gunturu,Ben Pearman,Keiichi Ihara,Morteza Faraji,Bryan Wang,Rubaiat Habib Kazi,Ryo Suzuki*

Main category: cs.HC

TL;DR: MapStory is an LLM-powered tool that turns text into editable map animations, using a system informed by animator interviews and analysis of existing videos. Evaluation shows it simplifies map animation creation, speeds up iteration, fosters creativity, and reduces barriers.


<details>
  <summary>Details</summary>
Motivation: To create a tool that simplifies the process of creating map animations from natural language text, making it easier for users to produce and edit map-centric stories.

Method: MapStory uses an agentic architecture to automatically generate a scene breakdown from user-written scripts, decomposing them into key animation elements like camera movements and visual highlights. It includes a researcher component leveraging an LLM with web search to query geospatial information accurately, allowing automatic extraction of relevant data while providing editing capabilities. Users can also fine-tune parameters via an interactive timeline editor.

Result: Evaluation through expert interviews (N=5) and a usability study (N=12) indicates that MapStory allows users to create map animations easily, facilitates faster iteration cycles, encourages creative exploration, and lowers the barrier to entry for creating map-focused narratives.

Conclusion: MapStory successfully enables users to create map animations more effortlessly, speeds up the iteration process, promotes creative exploration, and makes it easier for users to engage in map-centered storytelling.

Abstract: We introduce MapStory, an LLM-powered animation authoring tool that generates
editable map animation sequences directly from natural language text. Given a
user-written script, MapStory leverages an agentic architecture to
automatically produce a scene breakdown, which decomposes the script into key
animation building blocks such as camera movements, visual highlights, and
animated elements. Our system includes a researcher component that accurately
queries geospatial information by leveraging an LLM with web search, enabling
the automatic extraction of relevant regions, paths, and coordinates while
allowing users to edit and query for changes or additional information to
refine the results. Additionally, users can fine-tune parameters of these
blocks through an interactive timeline editor. We detail the system's design
and architecture, informed by formative interviews with professional animators
and an analysis of 200 existing map animation videos. Our evaluation, which
includes expert interviews (N=5) and a usability study (N=12), demonstrates
that MapStory enables users to create map animations with ease, facilitates
faster iteration, encourages creative exploration, and lowers barriers to
creating map-centric stories.

</details>


### [277] [Voice CMS: updating the knowledge base of a digital assistant through conversation](https://arxiv.org/abs/2505.22303)
*Grzegorz Wolny,Michał Szczerbak*

Main category: cs.HC

TL;DR: 本研究提出了一种基于多代理LLM架构和语音用户界面（VUI）的解决方案，用于更新数字助手的知识库。与传统图形内容管理系统相比，评估了其可用性，重点关注用户偏好与所提供信息复杂性的关系。结果表明，尽管VUI的整体可用性评分低于图形界面，但在处理较简单的任务时更受用户青睐。通过VUI输入的内容质量与图形界面相当，即使在处理高度复杂任务时也是如此。定性结果建议，结合两者优势的混合界面可以解决实验中发现的关键挑战，如通过图形反馈减轻认知负荷，同时保持语音交互的直观性。这项工作强调了特定商业环境中对话界面作为知识管理有效方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索语音用户界面（VUI）在知识管理中的应用潜力，并理解用户偏好与信息复杂性之间的关系。

Method: 采用多代理LLM架构和语音用户界面（VUI），并与传统的图形内容管理系统（CMS）进行比较，评估两者的可用性及用户偏好。

Result: VUI的整体可用性评分低于图形界面，但在简单任务上更受欢迎；输入内容的质量在两种界面之间没有显著差异，即使是复杂任务也是如此。定性分析显示，混合界面可能解决关键挑战。

Conclusion: 对话界面在特定商业环境中具有作为有效知识管理方法的潜力，结合图形和语音交互的混合界面可能是未来发展方向。

Abstract: In this study, we propose a solution based on a multi-agent LLM architecture
and a voice user interface (VUI) designed to update the knowledge base of a
digital assistant. Its usability is evaluated in comparison to a more
traditional graphical content management system (CMS), with a focus on
understanding the relationship between user preferences and the complexity of
the information being provided. The findings demonstrate that, while the
overall usability of the VUI is rated lower than the graphical interface, it is
already preferred by users for less complex tasks. Furthermore, the quality of
content entered through the VUI is comparable to that achieved with the
graphical interface, even for highly complex tasks. Obtained qualitative
results suggest that a hybrid interface combining the strengths of both
approaches could address the key challenges identified during the experiment,
such as reducing cognitive load through graphical feedback while maintaining
the intuitive nature of voice-based interactions. This work highlights the
potential of conversational interfaces as a viable and effective method for
knowledge management in specific business contexts.

</details>


### [278] [Human-Centered Human-AI Collaboration (HCHAC)](https://arxiv.org/abs/2505.22477)
*Qi Gao,Wei Xu,Hanxi Pan,Mowei Shen,Zaifeng Gao*

Main category: cs.HC

TL;DR: In the intelligent era, Human-AI Collaboration (HAC) forms a new human-machine relationship. Autonomous AI agents act as active teammates in collaboration with humans. Human-centered AI (HCAI) highlights human leadership in this collaboration. This chapter outlines HAC's core concepts and features from the HCAI perspective, reviews research methodologies and agenda, proposes a framework for human-centered HAC (HCHAC), provides a case study on autonomous vehicles, and identifies future research directions.


<details>
  <summary>Details</summary>
Motivation: The interaction between humans and intelligent systems evolves towards collaboration with autonomous intelligent agents, requiring innovative research perspectives to address unique challenges posed by HAC.

Method: The chapter reviews current research methodologies and agenda within the HAC field from the HCAI perspective, integrates these reviews into a proposed framework for human-centered HAC (HCHAC), and provides a case study of HAC in autonomous vehicles.

Result: A framework for human-centered HAC (HCHAC) is proposed, illustrating practical applications through a case study in autonomous vehicles, and identifying potential future research directions.

Conclusion: Future research should aim at enhancing the effectiveness, reliability, and ethical integration of human-centered HAC systems across various domains.

Abstract: In the intelligent era, the interaction between humans and intelligent
systems fundamentally involves collaboration with autonomous intelligent
agents. Human-AI Collaboration (HAC) represents a novel type of human-machine
relationship facilitated by autonomous intelligent machines equipped with AI
technologies. In this paradigm, AI agents serve not only as auxiliary tools but
also as active teammates, partnering with humans to accomplish tasks
collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical
leadership roles in the collaboration. This human-led collaboration imparts new
dimensions to the human-machine relationship, necessitating innovative research
perspectives, paradigms, and agenda to address the unique challenges posed by
HAC. This chapter delves into the essence of HAC from the human-centered
perspective, outlining its core concepts and distinguishing features. It
reviews the current research methodologies and research agenda within the HAC
field from the HCAI perspective, highlighting advancements and ongoing studies.
Furthermore, a framework for human-centered HAC (HCHAC) is proposed by
integrating these reviews and analyses. A case study of HAC in the context of
autonomous vehicles is provided, illustrating practical applications and the
synergistic interactions between humans and AI agents. Finally, it identifies
potential future research directions aimed at enhancing the effectiveness,
reliability, and ethical integration of human-centered HAC systems in diverse
domains.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [279] [Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations](https://arxiv.org/abs/2505.21994)
*Josef Dick,Seungchan Ko,Kassem Mustapha,Sanghyeon Park*

Main category: math.NA

TL;DR: This paper proposes a machine-learning-driven method using Physics-Informed Neural Networks (PINNs) to robustly solve nearly incompressible elasticity equations, which traditional finite element methods struggle with as material incompressibility increases.


<details>
  <summary>Details</summary>
Motivation: Traditional low-order conforming finite element methods face accuracy deterioration when solving nearly incompressible homogeneous elasticity equations due to divergence instability, especially as the Lamé coefficient approaches infinity or the Poisson ratio approaches 1/2. This locking issue is not yet fully understood and remains an open problem.

Method: The authors employ Physics-Informed Neural Networks (PINNs) to develop a new approach for solving linear elasticity equations of nearly incompressible materials. They decompose the original equations to balance coefficients and simultaneously solve both forward and inverse problems to recover solutions and external conditions.

Result: Numerical experiments demonstrate the effectiveness of this method across scenarios with constant, variable, and parametric Lamé coefficients, showing that the proposed methodology can efficiently handle nearly incompressible elasticity problems.

Conclusion: The machine-learning-driven method based on PINNs provides a robust alternative to traditional finite element methods for solving nearly incompressible elasticity equations, alleviating issues related to locking.

Abstract: Due to divergence instability, the accuracy of low-order conforming finite
element methods for nearly incompressible homogeneous elasticity equations
deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as
the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or
non-robustness, remains not fully understood despite extensive investigation.
In this paper, we propose a robust method based on a fundamentally different,
machine-learning-driven approach. Leveraging recently developed
Physics-Informed Neural Networks (PINNs), we address the numerical solution of
linear elasticity equations governing nearly incompressible materials. The core
idea of our method is to appropriately decompose the given equations to
alleviate the extreme imbalance in the coefficients, while simultaneously
solving both the forward and inverse problems to recover the solutions of the
decomposed systems as well as the associated external conditions. Through
various numerical experiments, including constant, variable and parametric
Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [280] [High-Fidelity Functional Ultrasound Reconstruction via A Visual Auto-Regressive Framework](https://arxiv.org/abs/2505.21530)
*Xuhang Chen,Zhuo Li,Yanyan Shen,Mufti Mahmud,Hieu Pham,Chi-Man Pun,Shuqiang Wang*

Main category: eess.IV

TL;DR: fUS imaging has great potential but is limited by data scarcity and signal degradation, affecting the diversity of datasets and fairness of machine learning models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in practical application of fUS imaging such as data scarcity and signal degradation through the cranium.

Method: Not specified in the abstract.

Result: Not specified in the abstract.

Conclusion: The issues of data scarcity and signal degradation need to be resolved for better application of fUS imaging.

Abstract: Functional ultrasound (fUS) imaging provides exceptional spatiotemporal
resolution for neurovascular mapping, yet its practical application is
significantly hampered by critical challenges. Foremost among these are data
scarcity, arising from ethical considerations and signal degradation through
the cranium, which collectively limit dataset diversity and compromise the
fairness of downstream machine learning models.

</details>


### [281] [STA-Risk: A Deep Dive of Spatio-Temporal Asymmetries for Breast Cancer Risk Prediction](https://arxiv.org/abs/2505.21699)
*Zhengbo Zhou,Dooman Arefan,Margarita Zuley,Jules Sumkin,Shandong Wu*

Main category: eess.IV

TL;DR: This paper proposes STA-Risk, a Transformer-based model for breast cancer risk prediction using mammogram images, which captures spatial and temporal asymmetries with side and temporal encoding. It outperforms four SOTA models in experiments with two independent datasets.


<details>
  <summary>Details</summary>
Motivation: Current breast cancer risk prediction models have limited performance when using single exams or overlook detailed longitudinal imaging evolvement.

Method: STA-Risk is a novel Transformer-based model that uses side encoding and temporal encoding to learn spatial-temporal asymmetries from bilateral and longitudinal mammographic images, regulated by an asymmetry loss.

Result: STA-Risk achieved superior performance compared to four representative SOTA models for 1- to 5-year future risk prediction in extensive experiments with two independent mammogram datasets.

Conclusion: STA-Risk effectively captures fine-grained mammographic imaging evolution for breast cancer risk prediction and outperforms existing models.

Abstract: Predicting the risk of developing breast cancer is an important clinical tool
to guide early intervention and tailoring personalized screening strategies.
Early risk models have limited performance and recently machine learning-based
analysis of mammogram images showed encouraging risk prediction effects. These
models however are limited to the use of a single exam or tend to overlook
nuanced breast tissue evolvement in spatial and temporal details of
longitudinal imaging exams that are indicative of breast cancer risk. In this
paper, we propose STA-Risk (Spatial and Temporal Asymmetry-based Risk
Prediction), a novel Transformer-based model that captures fine-grained
mammographic imaging evolution simultaneously from bilateral and longitudinal
asymmetries for breast cancer risk prediction. STA-Risk is innovative by the
side encoding and temporal encoding to learn spatial-temporal asymmetries,
regulated by a customized asymmetry loss. We performed extensive experiments
with two independent mammogram datasets and achieved superior performance than
four representative SOTA models for 1- to 5-year future risk prediction. Source
codes will be released upon publishing of the paper.

</details>


### [282] [Privacy-Preserving Chest X-ray Report Generation via Multimodal Federated Learning with ViT and GPT-2](https://arxiv.org/abs/2505.21715)
*Md. Zahid Hossain,Mustofa Ahmed,Most. Sharmin Sultana Samu,Md. Rakibul Islam*

Main category: eess.IV

TL;DR: The study proposes a Multimodal Federated Learning framework for generating chest X-ray reports using IU-Xray dataset, Vision Transformer and GPT-2. Among the three FL aggregation strategies evaluated, Krum Aggregation performed best in lexical and semantic metrics. The results indicate that FL can match or outperform centralized models while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: To overcome the privacy concerns of traditional centralized approaches which require sensitive data transfer, this paper aims to develop a framework for radiology report generation from chest X-ray images using federated learning.

Method: A Multimodal Federated Learning framework was proposed where Vision Transformer (ViT) is used as encoder and GPT-2 as report generator. Three FL aggregation strategies were evaluated: FedAvg, Krum Aggregation and Loss-aware Federated Averaging (L-FedAvg).

Result: Krum Aggregation showed superior performance across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore and RaTEScore. The results suggest that FL can match or surpass centralized models in generating clinically relevant reports.

Conclusion: This lightweight and privacy-preserving framework allows collaborative medical AI development without compromising data confidentiality.

Abstract: The automated generation of radiology reports from chest X-ray images holds
significant promise in enhancing diagnostic workflows while preserving patient
privacy. Traditional centralized approaches often require sensitive data
transfer, posing privacy concerns. To address this, the study proposes a
Multimodal Federated Learning framework for chest X-ray report generation using
the IU-Xray dataset. The system utilizes a Vision Transformer (ViT) as the
encoder and GPT-2 as the report generator, enabling decentralized training
without sharing raw data. Three Federated Learning (FL) aggregation strategies:
FedAvg, Krum Aggregation and a novel Loss-aware Federated Averaging (L-FedAvg)
were evaluated. Among these, Krum Aggregation demonstrated superior performance
across lexical and semantic evaluation metrics such as ROUGE, BLEU, BERTScore
and RaTEScore. The results show that FL can match or surpass centralized models
in generating clinically relevant and semantically rich radiology reports. This
lightweight and privacy-preserving framework paves the way for collaborative
medical AI development without compromising data confidentiality.

</details>


### [283] [Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology](https://arxiv.org/abs/2505.21928)
*Lianghui Zhu,Xitong Ling,Minxi Ouyang,Xiaoping Liu,Mingxi Fu,Tian Guan,Fanglei Fu,Xuanyu Wang,Maomao Zeng,Mingxi Zhu,Yibo Jin,Liming Liu,Song Duan,Qiming He,Yizhi Wang,Luxi Xie,Houqiang Li,Yonghong He,Sufang Tian*

Main category: eess.IV

TL;DR: The paper presents Digepath, a specialized foundation model for GI pathology that uses dual-phase iterative optimization and pretraining on millions of image patches to improve diagnostic accuracy and reproducibility. It achieves state-of-the-art performance in 33 out of 34 GI pathology tasks and near-perfect sensitivity in early cancer detection across multiple institutions.


<details>
  <summary>Details</summary>
Motivation: Gastrointestinal diseases pose a significant clinical burden with conventional histopathological diagnosis suffering from limited reproducibility and variability due to subjective interpretation by pathologists.

Method: Developed Digepath, a specialized foundation model for GI pathology using a dual-phase iterative optimization strategy combining pretraining with fine-screening. Pretrained on over 353 million image patches from more than 200,000 slides.

Result: Achieved state-of-the-art performance in 33 out of 34 GI pathology tasks, including pathological diagnosis, molecular prediction, gene mutation prediction, and prognosis evaluation. Demonstrated near-perfect 99.6% sensitivity in early cancer detection across 9 medical institutions.

Conclusion: Digepath has the potential to bridge critical gaps in histopathological practice for GI diseases, advancing AI-driven precision pathology and establishing a transferable paradigm for other pathology subspecialties.

Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden,
necessitating precise diagnostic approaches to optimize patient outcomes.
Conventional histopathological diagnosis, heavily reliant on the subjective
interpretation of pathologists, suffers from limited reproducibility and
diagnostic variability. To overcome these limitations and address the lack of
pathology-specific foundation models for GI diseases, we develop Digepath, a
specialized foundation model for GI pathology. Our framework introduces a
dual-phase iterative optimization strategy combining pretraining with
fine-screening, specifically designed to address the detection of sparsely
distributed lesion areas in whole-slide images. Digepath is pretrained on more
than 353 million image patches from over 200,000 hematoxylin and eosin-stained
slides of GI diseases. It attains state-of-the-art performance on 33 out of 34
tasks related to GI pathology, including pathological diagnosis, molecular
prediction, gene mutation prediction, and prognosis evaluation, particularly in
diagnostically ambiguous cases and resolution-agnostic tissue classification.We
further translate the intelligent screening module for early GI cancer and
achieve near-perfect 99.6% sensitivity across 9 independent medical
institutions nationwide. The outstanding performance of Digepath highlights its
potential to bridge critical gaps in histopathological practice. This work not
only advances AI-driven precision pathology for GI diseases but also
establishes a transferable paradigm for other pathology subspecialties.

</details>


### [284] [Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment](https://arxiv.org/abs/2505.21592)
*Ze Chen,Shaode Yu*

Main category: eess.IV

TL;DR: TaylorKAN, an enhancement of Kolmogorov-Arnold Network (KAN), uses Taylor expansions as learnable activation functions for better local approximation in high-dimensional score regression. It integrates network depth reduction and feature dimensionality compression to improve computational efficiency. Experiments show TaylorKAN outperforms other KAN-related models on five databases with authentic distortions.


<details>
  <summary>Details</summary>
Motivation: KAN and its variants have been used in score regression for BIQA but face challenges when processing high-dimensional features, which limits performance gains and increases computational cost.

Method: TaylorKAN leverages Taylor expansions as learnable activation functions to enhance local approximation capability. It also integrates network depth reduction and feature dimensionality compression into the score regression pipeline to improve computational efficiency.

Result: TaylorKAN consistently outperforms other KAN-related models on five databases with authentic distortions. Its local approximation via Taylor expansions is more effective than global approximation using orthogonal functions. Its generalization capacity is validated through inter-database experiments.

Conclusion: TaylorKAN is an efficient and robust model for high-dimensional score regression.

Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong
function approximation capability. In our previous work, KAN and its variants
were explored in score regression for blind image quality assessment (BIQA).
However, these models encounter challenges when processing high-dimensional
features, leading to limited performance gains and increased computational
cost. To address these issues, we propose TaylorKAN that leverages the Taylor
expansions as learnable activation functions to enhance local approximation
capability. To improve the computational efficiency, network depth reduction
and feature dimensionality compression are integrated into the TaylorKAN-based
score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and
FLIVE) with authentic distortions, extensive experiments demonstrate that
TaylorKAN consistently outperforms the other KAN-related models, indicating
that the local approximation via Taylor expansions is more effective than
global approximation using orthogonal functions. Its generalization capacity is
validated through inter-database experiments. The findings highlight the
potential of TaylorKAN as an efficient and robust model for high-dimensional
score regression.

</details>


### [285] [Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off](https://arxiv.org/abs/2505.21597)
*Abdullah Al Mamun,Pollob Chandra Ray,Md Rahat Ul Nasib,Akash Das,Jia Uddin,Md Nurul Absur*

Main category: eess.IV

TL;DR: In this paper, the authors propose a lightweight custom CNN model for skin cancer classification that significantly reduces the number of parameters and computational costs compared to transfer learning models like ResNet50, while maintaining similar accuracy.


<details>
  <summary>Details</summary>
Motivation: Despite high accuracy, current state-of-the-art models such as ResNet50 have significant computational overhead, making them impractical for resource-constrained environments.

Method: The authors developed a custom CNN model with fewer parameters and FLOPs. This model was tested on the HAM10000 dataset and compared to ResNet50 in terms of accuracy, computational cost, and inference latency.

Result: The custom CNN achieved a 96.7% reduction in parameters and a 99.98% reduction in FLOPs compared to ResNet50, with only a 0.022% deviation in classification accuracy.

Conclusion: The optimized CNN model offers a practical solution for mobile and edge-based skin cancer diagnostics by balancing model complexity with real-world feasibility.

Abstract: The rapid advancement of deep learning in medical image analysis has greatly
enhanced the accuracy of skin cancer classification. However, current
state-of-the-art models, especially those based on transfer learning like
ResNet50, come with significant computational overhead, rendering them
impractical for deployment in resource-constrained environments. This study
proposes a custom CNN model that achieves a 96.7\% reduction in parameters
(from 23.9 million in ResNet50 to 692,000) while maintaining a classification
accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000
dataset reveals that although transfer learning models provide a marginal
accuracy improvement of approximately 0.022\%, they result in a staggering
13,216.76\% increase in FLOPs, considerably raising computational costs and
inference latency. In contrast, our lightweight CNN architecture, which
encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion,
significantly reduces energy consumption, memory footprint, and inference time.
These findings underscore the trade-off between the complexity of deep models
and their real-world feasibility, positioning our optimized CNN as a practical
solution for mobile and edge-based skin cancer diagnostics.

</details>


### [286] [Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction](https://arxiv.org/abs/2505.21767)
*Xiaoyan Li,Shixin Xu,Faisal Habib,Arvind Gupta,Huaxiong Huang*

Main category: eess.IV

TL;DR: The paper proposes a novel PPG-to-ECG reconstruction method using Vision Transformer and four-channel signal image representation, which significantly improves ECG reconstruction accuracy and introduces new clinical evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: Reconstructing ECG from PPG has potential but faces challenges in capturing fine-grained waveform features.

Method: Employ a Vision Transformer with a four-channel signal image representation including original PPG, its first-order difference, second-order difference, and area under the curve to enrich feature extraction and capture both inter-beat and intra-beat dependencies.

Result: Outperforms existing 1D convolution-based approaches with up to 29% reduction in PRD and 15% reduction in RMSE, and shows improvements in other evaluation metrics.

Conclusion: Integrating a four-channel signal image representation with ViT's self-attention mechanism enhances PPG feature extraction and modeling of beat-to-beat variations for PPG-to-ECG mapping.

Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent
advancements in generative models have significantly improved ECG
reconstruction, accurately capturing fine-grained waveform features remains a
key challenge. To address this, we propose a novel PPG-to-ECG reconstruction
method that leverages a Vision Transformer (ViT) as the core network. Unlike
conventional approaches that rely on single-channel PPG, our method employs a
four-channel signal image representation, incorporating the original PPG, its
first-order difference, second-order difference, and area under the curve. This
multi-channel design enriches feature extraction by preserving both temporal
and physiological variations within the PPG. By leveraging the self-attention
mechanism in ViT, our approach effectively captures both inter-beat and
intra-beat dependencies, leading to more robust and accurate ECG
reconstruction. Experimental results demonstrate that our method consistently
outperforms existing 1D convolution-based approaches, achieving up to 29%
reduction in PRD and 15% reduction in RMSE. The proposed approach also produces
improvements in other evaluation metrics, highlighting its robustness and
effectiveness in reconstructing ECG signals. Furthermore, to ensure a
clinically relevant evaluation, we introduce new performance metrics, including
QRS area error, PR interval error, RT interval error, and RT amplitude
difference error. Our findings suggest that integrating a four-channel signal
image representation with the self-attention mechanism of ViT enables more
effective extraction of informative PPG features and improved modeling of
beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the
potential of PPG as a viable alternative for heart activity monitoring, our
approach opens new avenues for cyclic signal analysis and prediction.

</details>


### [287] [Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images](https://arxiv.org/abs/2505.21872)
*George R. Nahass,Zhu Wang,Homa Rashidisabet,Won Hwa Kim,Sasha Hubschman,Jeffrey C. Peterson,Ghasem Yazdanpanah,Chad A. Purnell,Pete Setabutr,Ann Q. Tran,Darvin Yi,Sathya N. Ravi*

Main category: eess.IV

TL;DR: 本研究将机器遗忘技术重新定义为一种通用的模型修订工具，特别是在临床环境中。提出了一种基于边界的双层优化遗忘方法，并提供了收敛性保证和可调损失设计来平衡遗忘与保留之间的权衡。该方法在基准和实际临床数据集上优于基线方法，为临床应用中的模型维护提供了一个模块化、实用的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注隐私驱动的场景，而本文将机器遗忘视为一种通用工具，用于部署后模型修订，特别是应对临床环境中的数据变化、设备淘汰和政策变更等问题。

Method: 提出了一种基于边界的双层优化遗忘方法，可以通过迭代算法解决，并引入了可调损失设计以控制遗忘与保留的权衡，还支持新的模型组合策略，结合不同遗忘运行的优势。

Result: 在基准和实际临床成像数据集上的实验表明，该方法在遗忘和保留指标上均优于基线方法，包括涉及成像设备和解剖异常值的场景。

Conclusion: 机器遗忘可以作为一种模块化、实用的重训练替代方案，适用于现实世界中的临床模型维护任务。

Abstract: Machine unlearning aims to remove the influence of specific training samples
from a trained model without full retraining. While prior work has largely
focused on privacy-motivated settings, we recast unlearning as a
general-purpose tool for post-deployment model revision. Specifically, we focus
on utilizing unlearning in clinical contexts where data shifts, device
deprecation, and policy changes are common. To this end, we propose a bilevel
optimization formulation of boundary-based unlearning that can be solved using
iterative algorithms. We provide convergence guarantees when first-order
algorithms are used to unlearn. Our method introduces tunable loss design for
controlling the forgetting-retention tradeoff and supports novel model
composition strategies that merge the strengths of distinct unlearning runs.
Across benchmark and real-world clinical imaging datasets, our approach
outperforms baselines on both forgetting and retention metrics, including
scenarios involving imaging devices and anatomical outliers. This work
establishes machine unlearning as a modular, practical alternative to
retraining for real-world model maintenance in clinical applications.

</details>


### [288] [High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models](https://arxiv.org/abs/2505.22090)
*Tristan S. W. Stevens,Oisín Nolan,Oudom Somphone,Jean-Luc Robert,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: This paper presents a new method using diffusion models for 3D ultrasound reconstruction from fewer elevation planes, which improves image quality and downstream task performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of achieving both high volume rates and high image quality in 3D ultrasound imaging.

Method: Employing diffusion models for reconstructing 3D ultrasound images from a reduced set of elevation planes; comparing with traditional and deep learning-based interpolation methods; leveraging temporal consistency for accelerated inference; quantifying reconstruction uncertainty via probabilistic sampling.

Result: Diffusion model-based reconstruction outperforms baseline methods in image quality and downstream tasks; demonstrates robustness on out-of-distribution data with synthetic anomalies under strong subsampling.

Conclusion: The proposed diffusion model approach enhances spatial and temporal resolution in 3D ultrasound imaging, providing superior image quality and robustness.

Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of
anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the
reliance on precise probe orientation, potentially making ultrasound more
accessible to clinicians with varying levels of experience and improving
automated measurements and post-exam analysis. However, achieving both high
volume rates and high image quality remains a significant challenge. While 3D
diverging waves can provide high volume rates, they suffer from limited tissue
harmonic generation and increased multipath effects, which degrade image
quality. One compromise is to retain the focusing in elevation while leveraging
unfocused diverging waves in the lateral direction to reduce the number of
transmissions per elevation plane. Reaching the volume rates achieved by full
3D diverging waves, however, requires dramatically undersampling the number of
elevation planes. Subsequently, to render the full volume, simple interpolation
techniques are applied. This paper introduces a novel approach to 3D ultrasound
reconstruction from a reduced set of elevation planes by employing diffusion
models (DMs) to achieve increased spatial and temporal resolution. We compare
both traditional and supervised deep learning-based interpolation methods on a
3D cardiac ultrasound dataset. Our results show that DM-based reconstruction
consistently outperforms the baselines in image quality and downstream task
performance. Additionally, we accelerate inference by leveraging the temporal
consistency inherent to ultrasound sequences. Finally, we explore the
robustness of the proposed method by exploiting the probabilistic nature of
diffusion posterior sampling to quantify reconstruction uncertainty and
demonstrate improved recall on out-of-distribution data with synthetic
anomalies under strong subsampling.

</details>


### [289] [Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method](https://arxiv.org/abs/2505.22609)
*Alanna Hazlett,Naomi Ohashi,Timothy Rodriguez,Sodiq Adewole*

Main category: eess.IV

TL;DR: This paper explores the use of transfer learning with pre-trained CNNs to classify chest X-ray images into four categories (COVID-19, pneumonia, tuberculosis, and normal) achieving promising results in accuracy and key classification metrics. Grad-CAM is used for model interpretability.


<details>
  <summary>Details</summary>
Motivation: To effectively classify chest X-ray images into specific health condition categories using advanced machine learning techniques, enhancing diagnostic capabilities in clinical settings.

Method: Employed transfer learning with state-of-the-art pre-trained CNN models, fine-tuned on labeled medical X-ray images. Used Gradient-weighted Class Activation Mapping (Grad-CAM) for interpreting model decisions.

Result: The models demonstrated high accuracy and strong performance across key classification metrics like precision, recall, and F1 score.

Conclusion: Transfer learning with pre-trained CNNs shows great potential in classifying chest X-rays for different diseases, and Grad-CAM enhances the interpretability and trustworthiness of these models.

Abstract: In this work, we investigate the performance across multiple classification
models to classify chest X-ray images into four categories of COVID-19,
pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning
techniques with state-of-the-art pre-trained Convolutional Neural Networks
(CNNs) models. We fine-tuned these pre-trained architectures on a labeled
medical x-ray images. The initial results are promising with high accuracy and
strong performance in key classification metrics such as precision, recall, and
F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for
model interpretability to provide visual explanations for classification
decisions, improving trust and transparency in clinical applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [290] [Towards Human-Like Trajectory Prediction for Autonomous Driving: A Behavior-Centric Approach](https://arxiv.org/abs/2505.21565)
*Haicheng Liao,Zhenning Li,Guohui Zhang,Keqiang Li,Chengzhong Xu*

Main category: cs.RO

TL;DR: This paper presents HiT, a new model for predicting vehicle trajectories in complex traffic scenarios. It outperforms existing models, especially with aggressive driving behaviors.


<details>
  <summary>Details</summary>
Motivation: The need for more accurate and human-like trajectory prediction in autonomous driving systems to handle complex and dynamic traffic environments.

Method: HiT incorporates behavior-aware modules and dynamic centrality measures within a dynamic framework that considers both direct and indirect interactions among traffic participants.

Result: HiT outperforms other top models across multiple metrics in extensive experiments using diverse real-world datasets, particularly excelling in scenarios involving aggressive driving behaviors.

Conclusion: HiT represents a significant advancement in trajectory prediction, providing a more reliable and interpretable approach for enhancing the safety and efficiency of fully autonomous driving systems.

Abstract: Predicting the trajectories of vehicles is crucial for the development of
autonomous driving (AD) systems, particularly in complex and dynamic traffic
environments. In this study, we introduce HiT (Human-like Trajectory
Prediction), a novel model designed to enhance trajectory prediction by
incorporating behavior-aware modules and dynamic centrality measures. Unlike
traditional methods that primarily rely on static graph structures, HiT
leverages a dynamic framework that accounts for both direct and indirect
interactions among traffic participants. This allows the model to capture the
subtle yet significant influences of surrounding vehicles, enabling more
accurate and human-like predictions. To evaluate HiT's performance, we
conducted extensive experiments using diverse and challenging real-world
datasets, including NGSIM, HighD, RounD, ApolloScape, and MoCAD++. The results
demonstrate that HiT consistently outperforms other top models across multiple
metrics, particularly excelling in scenarios involving aggressive driving
behaviors. This research presents a significant step forward in trajectory
prediction, offering a more reliable and interpretable approach for enhancing
the safety and efficiency of fully autonomous driving systems.

</details>


### [291] [Fast and Cost-effective Speculative Edge-Cloud Decoding with Early Exits](https://arxiv.org/abs/2505.21594)
*Yeshwanth Venkatesha,Souvik Kundu,Priyadarshini Panda*

Main category: cs.RO

TL;DR: The paper proposes a speculative edge-cloud decoding framework that combines a large server-side model with a small device-side model to reduce latency and improve efficiency for LLMs on edge devices.


<details>
  <summary>Details</summary>
Motivation: To address the high operational costs and sustainability concerns of deploying Large Language Models (LLMs) via cloud-based APIs, as well as the constraints posed by limited computing resources on edge devices.

Method: A fast and cost-effective speculative edge-cloud decoding framework is proposed. It uses a large target model on the server and a small draft model on the device. Early exits in the target model allow tokens to be generated mid-verification, letting the client preemptively draft subsequent tokens before final verification.

Result: The method achieves up to a 35% reduction in latency compared to cloud-based autoregressive decoding, with an additional 11% improvement from preemptive drafting. On a real-world deployment using a quadruped robot, it achieved a 21% speedup over traditional cloud-based autoregressive decoding.

Conclusion: This framework shows potential for real-time LLM and Vision-Language Model (VLM) applications on resource-constrained edge devices.

Abstract: Large Language Models (LLMs) enable various applications on edge devices such
as smartphones, wearables, and embodied robots. However, their deployment often
depends on expensive cloud-based APIs, creating high operational costs, which
limit access for smaller organizations and raise sustainability concerns.
Certain LLMs can be deployed on-device, offering a cost-effective solution with
reduced latency and improved privacy. Yet, limited computing resources
constrain the size and accuracy of models that can be deployed, necessitating a
collaborative design between edge and cloud. We propose a fast and
cost-effective speculative edge-cloud decoding framework with a large target
model on the server and a small draft model on the device. By introducing early
exits in the target model, tokens are generated mid-verification, allowing the
client to preemptively draft subsequent tokens before final verification, thus
utilizing idle time and enhancing parallelism between edge and cloud. Using an
NVIDIA Jetson Nano (client) and an A100 GPU (server) with Vicuna-68M (draft)
and Llama2-7B (target) models, our method achieves up to a 35% reduction in
latency compared to cloud-based autoregressive decoding, with an additional 11%
improvement from preemptive drafting. To demonstrate real-world applicability,
we deploy our method on the Unitree Go2 quadruped robot using Vision-Language
Model (VLM) based control, achieving a 21% speedup over traditional cloud-based
autoregressive decoding. These results demonstrate the potential of our
framework for real-time LLM and VLM applications on resource-constrained edge
devices.

</details>


### [292] [PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation](https://arxiv.org/abs/2505.21652)
*Yifan Yin,Zhengtao Han,Shivam Aarya,Jianxin Wang,Shuhang Xu,Jiawei Peng,Angtian Wang,Alan Yuille,Tianmin Shu*

Main category: cs.RO

TL;DR: 本研究提出了PartInstruct，这是一个大规模基准测试，用于训练和评估使用部件级指令的精细机器人操作模型。它包括513个对象实例、14个类别以及1302个精细操作任务。通过超过10,000个专家演示数据进行训练，并设计了全面的测试套件来评估学习策略的泛化能力。实验结果表明，现有模型在长时域任务中对部件概念的稳健接地和3D动作预测面临挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管近年来在基于语言指令的通用机器人操作策略训练方面取得了进展，但缺乏大规模的数据集来支持具有部件级指令和多样化3D对象实例的精细操作任务。

Method: 引入了PartInstruct数据集，包含513个对象实例和1302个精细操作任务。利用3D模拟器生成超过10,000个专家演示数据进行训练，并设计了综合测试套件以评估模型的泛化能力。

Result: 实验结果显示，当前最先进的机器人操作方法在稳健接地部件概念、预测3D空间中的动作以及在长时域任务中操纵对象部件方面存在困难。

Conclusion: 需要进一步改进模型以更好地处理部件级概念和复杂任务中的3D动作预测。

Abstract: Fine-grained robot manipulation, such as lifting and rotating a bottle to
display the label on the cap, requires robust reasoning about object parts and
their relationships with intended tasks. Despite recent advances in training
general-purpose robot manipulation policies guided by language instructions,
there is a notable lack of large-scale datasets for fine-grained manipulation
tasks with part-level instructions and diverse 3D object instances annotated
with part-level labels. In this work, we introduce PartInstruct, the first
large-scale benchmark for training and evaluating fine-grained robot
manipulation models using part-level instructions. PartInstruct comprises 513
object instances across 14 categories, each annotated with part-level
information, and 1302 fine-grained manipulation tasks organized into 16 task
classes. Our training set consists of over 10,000 expert demonstrations
synthesized in a 3D simulator, where each demonstration is paired with a
high-level task instruction, a chain of base part-based skill instructions, and
ground-truth 3D information about the object and its parts. Additionally, we
designed a comprehensive test suite to evaluate the generalizability of learned
policies across new states, objects, and tasks. We evaluated several
state-of-the-art robot manipulation approaches, including end-to-end
vision-language policy learning and bi-level planning models for robot
manipulation on our benchmark. The experimental results reveal that current
models struggle to robustly ground part concepts and predict actions in 3D
space, and face challenges when manipulating object parts in long-horizon
tasks.

</details>


### [293] [Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories](https://arxiv.org/abs/2505.21851)
*Sunshine Jiang,Xiaolin Fang,Nicholas Roy,Tomás Lozano-Pérez,Leslie Pack Kaelbling,Siddharth Ancha*

Main category: cs.RO

TL;DR: A new method called streaming flow policy simplifies diffusion/flow policies by treating action trajectories as flow trajectories, enabling faster policy execution and tighter sensorimotor loops for learning-based robot control.


<details>
  <summary>Details</summary>
Motivation: Recent advances in diffusion/flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, these methods are computationally expensive due to their sampling process.

Method: The algorithm samples from a narrow Gaussian around the last action and incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a single trajectory. This allows actions to be streamed to the robot on-the-fly during the flow sampling process.

Result: Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control.

Conclusion: This method retains the ability to model multi-modal behavior and trains flows that stabilize around demonstration trajectories to reduce distribution shift and improve imitation learning performance.

Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation
learning of complex, multi-modal action trajectories. However, they are
computationally expensive because they sample a trajectory of trajectories: a
diffusion$/$flow trajectory of action trajectories. They discard intermediate
action trajectories, and must wait for the sampling process to complete before
any actions can be executed on the robot. We simplify diffusion$/$flow policies
by treating action trajectories as flow trajectories. Instead of starting from
pure noise, our algorithm samples from a narrow Gaussian around the last
action. Then, it incrementally integrates a velocity field learned via flow
matching to produce a sequence of actions that constitute a single trajectory.
This enables actions to be streamed to the robot on-the-fly during the flow
sampling process, and is well-suited for receding horizon policy execution.
Despite streaming, our method retains the ability to model multi-modal
behavior. We train flows that stabilize around demonstration trajectories to
reduce distribution shift and improve imitation learning performance. Streaming
flow policy outperforms prior methods while enabling faster policy execution
and tighter sensorimotor loops for learning-based robot control. Project
website: https://streaming-flow-policy.github.io/

</details>


### [294] [Vision-Language-Action Model with Open-World Embodied Reasoning from Pretrained Knowledge](https://arxiv.org/abs/2505.21906)
*Zhongyi Zhou,Yichen Zhu,Junjie Wen,Chaomin Shen,Yi Xu*

Main category: cs.RO

TL;DR: ChatVLA-2是一种新型的混合专家视觉语言动作(VLA)模型，通过专门的三阶段训练管道，在保留视觉语言模型(VLM)原有优势的同时，增强了机器人可执行推理能力。实验表明，该模型在数学匹配任务中表现出卓越的数学推理和OCR能力，并具备强大的空间推理技能，显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端VLA系统在微调过程中往往会失去关键能力，无法充分保留和扩展VLM的核心竞争力。因此，需要一种新的模型和训练方法来解决这一问题。

Method: 引入了ChatVLA-2，一种新型的混合专家VLA模型，结合专门设计的三阶段训练管道，旨在保留VLM的原始优势并实现可执行推理。设计了一个数学匹配任务来验证方法的有效性。

Result: ChatVLA-2在未明确训练数学推理和OCR能力的情况下，展现出卓越的数学推理和OCR能力，以及强大的空间推理技能。

Conclusion: ChatVLA-2代表了向开发真正可泛化的机器人基础模型迈进的重要一步，具有强大的推理能力。

Abstract: Vision-language-action (VLA) models have emerged as the next generation of
models in robotics. However, despite leveraging powerful pre-trained
Vision-Language Models (VLMs), existing end-to-end VLA systems often lose key
capabilities during fine-tuning as the model adapts to specific robotic tasks.
We argue that a generalizable VLA model should retain and expand upon the VLM's
core competencies: 1) Open-world embodied reasoning - the VLA should inherit
the knowledge from VLM, i.e., recognize anything that the VLM can recognize,
capable of solving math problems, possessing visual-spatial intelligence, 2)
Reasoning following - effectively translating the open-world reasoning into
actionable steps for the robot. In this work, we introduce ChatVLA-2, a novel
mixture-of-expert VLA model coupled with a specialized three-stage training
pipeline designed to preserve the VLM's original strengths while enabling
actionable reasoning. To validate our approach, we design a math-matching task
wherein a robot interprets math problems written on a whiteboard and picks
corresponding number cards from a table to solve equations. Remarkably, our
method exhibits exceptional mathematical reasoning and OCR capabilities,
despite these abilities not being explicitly trained within the VLA.
Furthermore, we demonstrate that the VLA possesses strong spatial reasoning
skills, enabling it to interpret novel directional instructions involving
previously unseen objects. Overall, our method showcases reasoning and
comprehension abilities that significantly surpass state-of-the-art imitation
learning methods such as OpenVLA, DexVLA, and pi-zero. This work represents a
substantial advancement toward developing truly generalizable robotic
foundation models endowed with robust reasoning capacities.

</details>


### [295] [DORAEMON: Decentralized Ontology-aware Reliable Agent with Enhanced Memory Oriented Navigation](https://arxiv.org/abs/2505.21969)
*Tianjun Gu,Linfeng Li,Xuhong Wang,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.RO

TL;DR: DORAEMON is a new framework for robot navigation that improves upon existing methods by addressing spatiotemporal discontinuities, unstructured memory, and task understanding issues. It achieves top performance in success rate and efficiency metrics without needing prior maps or pre-training.


<details>
  <summary>Details</summary>
Motivation: Current zero-shot approaches for robot navigation have limitations such as spatiotemporal discontinuity from discrete observations, unstructured memory representations, and insufficient task understanding which lead to navigation failures.

Method: DORAEMON consists of Ventral and Dorsal Streams inspired by human navigation capabilities. The Dorsal Stream handles spatiotemporal discontinuities through Hierarchical Semantic-Spatial Fusion and Topology Map, while the Ventral Stream enhances decision-making using RAG-VLM and Policy-VLM. Nav-Ensurance ensures safety and efficiency in navigation.

Result: DORAEMON outperforms existing methods on HM3D, MP3D, and GOAT datasets in terms of success rate (SR) and success weighted by path length (SPL). A new evaluation metric (AORI) is introduced to better assess navigation intelligence.

Conclusion: DORAEMON effectively enables zero-shot autonomous navigation without requiring prior map building or pre-training, demonstrating its potential in household service robots.

Abstract: Adaptive navigation in unfamiliar environments is crucial for household
service robots but remains challenging due to the need for both low-level path
planning and high-level scene understanding. While recent vision-language model
(VLM) based zero-shot approaches reduce dependence on prior maps and
scene-specific training data, they face significant limitations: spatiotemporal
discontinuity from discrete observations, unstructured memory representations,
and insufficient task understanding leading to navigation failures. We propose
DORAEMON (Decentralized Ontology-aware Reliable Agent with Enhanced Memory
Oriented Navigation), a novel cognitive-inspired framework consisting of
Ventral and Dorsal Streams that mimics human navigation capabilities. The
Dorsal Stream implements the Hierarchical Semantic-Spatial Fusion and Topology
Map to handle spatiotemporal discontinuities, while the Ventral Stream combines
RAG-VLM and Policy-VLM to improve decision-making. Our approach also develops
Nav-Ensurance to ensure navigation safety and efficiency. We evaluate DORAEMON
on the HM3D, MP3D, and GOAT datasets, where it achieves state-of-the-art
performance on both success rate (SR) and success weighted by path length (SPL)
metrics, significantly outperforming existing methods. We also introduce a new
evaluation metric (AORI) to assess navigation intelligence better.
Comprehensive experiments demonstrate DORAEMON's effectiveness in zero-shot
autonomous navigation without requiring prior map building or pre-training.

</details>


### [296] [Learning Compositional Behaviors from Demonstration and Language](https://arxiv.org/abs/2505.21981)
*Weiyu Liu,Neil Nie,Ruohan Zhang,Jiayuan Mao,Jiajun Wu*

Main category: cs.RO

TL;DR: BLADE is a framework that combines imitation learning and model-based planning to enable long-horizon robotic manipulation using language-annotated demonstrations and LLMs for abstract action knowledge.


<details>
  <summary>Details</summary>
Motivation: To create a system capable of generalizing to novel situations in long-horizon robotic manipulation tasks by integrating imitation learning with model-based planning.

Method: The BLADE framework leverages language-annotated demonstrations, extracts abstract action knowledge from large language models (LLMs), and constructs a library of structured, high-level action representations including preconditions and effects grounded in visual perception and corresponding controllers implemented as neural network-based policies.

Result: BLADE shows significant capabilities in generalizing to novel situations, such as novel initial states, external state perturbations, and novel goals. It has been validated both in simulation and on real robots with a diverse set of objects.

Conclusion: BLADE demonstrates the potential of combining imitation learning and model-based planning for long-horizon robotic manipulation tasks.

Abstract: We introduce Behavior from Language and Demonstration (BLADE), a framework
for long-horizon robotic manipulation by integrating imitation learning and
model-based planning. BLADE leverages language-annotated demonstrations,
extracts abstract action knowledge from large language models (LLMs), and
constructs a library of structured, high-level action representations. These
representations include preconditions and effects grounded in visual perception
for each high-level action, along with corresponding controllers implemented as
neural network-based policies. BLADE can recover such structured
representations automatically, without manually labeled states or symbolic
definitions. BLADE shows significant capabilities in generalizing to novel
situations, including novel initial states, external state perturbations, and
novel goals. We validate the effectiveness of our approach both in simulation
and on real robots with a diverse set of objects with articulated parts,
partial observability, and geometric constraints.

</details>


### [297] [MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation](https://arxiv.org/abs/2505.21734)
*Felix Jahncke,Johannes Betz*

Main category: cs.RO

TL;DR: The paper introduces MIND-Stack, a modular software stack combining a localization network and a Stanley Controller for navigation. It features human interpretable states and end-to-end differentiability, improving control performance and offering sim-to-real capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a robust and efficient navigation system that combines the interpretability of rule-based methods with the learning capabilities of neural networks, addressing the limitations of both.

Method: MIND-Stack is a modular software stack comprising a localization network and a Stanley Controller. It uses intermediate human interpretable state representations and is end-to-end differentiable, allowing the localization module to reduce downstream control error beyond state estimation.

Result: Experiments show that the localization module reduces downstream control loss and performs better than state-of-the-art algorithms. The system demonstrates sim-to-real capabilities on a real-world platform with limited computation power, enabling simultaneous training of localization and controller modules.

Conclusion: MIND-Stack successfully integrates interpretability and learning capabilities, offering improved performance and stability. Future work will incorporate additional modules from the autonomous navigation pipeline for even greater enhancements.

Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based
methods offer interpretability and modularity but struggle with learning from
large datasets, while end-to-end neural networks excel in learning but lack
transparency and modularity. In this paper, we present MIND-Stack, a modular
software stack consisting of a localization network and a Stanley Controller
with intermediate human interpretable state representations and end-to-end
differentiability. Our approach enables the upstream localization module to
reduce the downstream control error, extending its role beyond state
estimation. Unlike existing research on differentiable algorithms that either
lack modules of the autonomous stack to span from sensor input to actuator
output or real-world implementation, MIND-Stack offers both capabilities. We
conduct experiments that demonstrate the ability of the localization module to
reduce the downstream control loss through its end-to-end differentiability
while offering better performance than state-of-the-art algorithms. We showcase
sim-to-real capabilities by deploying the algorithm on a real-world embedded
autonomous platform with limited computation power and demonstrate simultaneous
training of both the localization and controller towards one goal. While
MIND-Stack shows good results, we discuss the incorporation of additional
modules from the autonomous navigation pipeline in the future, promising even
greater stability and performance in the next iterations of the framework.

</details>


### [298] [ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning](https://arxiv.org/abs/2505.22094)
*Tonghe Zhang,Yu Chao,Sicang Su,Yu Wang*

Main category: cs.RO

TL;DR: The paper proposes ReinFlow, an online reinforcement learning framework that fine-tunes flow matching policies for continuous robotic control. It injects learnable noise into the deterministic path of a flow policy to convert it into a discrete-time Markov Process. This conversion facilitates exploration and ensures training stability. The method is benchmarked in locomotion and manipulation tasks, showing significant improvements in episode reward and success rate while saving computation time.


<details>
  <summary>Details</summary>
Motivation: To develop an effective online reinforcement learning framework that can fine-tune diverse flow model variants for continuous robotic control, improving both performance and computational efficiency.

Method: ReinFlow introduces learnable noise into the deterministic path of a flow policy, converting it into a discrete-time Markov Process. This allows for exact likelihood computation, enhancing exploration and ensuring training stability. The framework can fine-tune various flow model variants, including Rectified Flow and Shortcut Models, even with very few denoising steps.

Result: In challenging legged locomotion tasks, the episode reward of Rectified Flow policies increased by an average of 135.36% after fine-tuning with ReinFlow, saving denoising steps and 82.63% of wall time compared to DPPO. In state and visual manipulation tasks, the success rate of Shortcut Model policies improved by an average of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, with performance comparable to DDIM policies while saving an average of 23.20% computation time.

Conclusion: ReinFlow effectively fine-tunes flow matching policies for continuous robotic control, achieving significant improvements in performance and computational efficiency in various locomotion and manipulation tasks.

Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning
(RL) framework that fine-tunes a family of flow matching policies for
continuous robotic control. Derived from rigorous RL theory, ReinFlow injects
learnable noise into a flow policy's deterministic path, converting the flow
into a discrete-time Markov Process for exact and straightforward likelihood
computation. This conversion facilitates exploration and ensures training
stability, enabling ReinFlow to fine-tune diverse flow model variants,
including Rectified Flow [35] and Shortcut Models [19], particularly at very
few or even one denoising step. We benchmark ReinFlow in representative
locomotion and manipulation tasks, including long-horizon planning with visual
input and sparse reward. The episode reward of Rectified Flow policies obtained
an average net growth of 135.36% after fine-tuning in challenging legged
locomotion tasks while saving denoising steps and 82.63% of wall time compared
to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate
of the Shortcut Model policies in state and visual manipulation tasks achieved
an average net increase of 40.34% after fine-tuning with ReinFlow at four or
even one denoising step, whose performance is comparable to fine-tuned DDIM
policies while saving computation time for an average of 23.20%. Project
Webpage: https://reinflow.github.io/

</details>


### [299] [From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation](https://arxiv.org/abs/2505.22503)
*Yuanfei Wang,Xinju Huang,Fangwei Zhong,Yaodong Yang,Yizhou Wang,Yuanpei Chen,Hao Dong*

Main category: cs.RO

TL;DR: Embodied agents need to not only execute tasks but also collaborate with human users having vague and implicit goals. This work develops HA-Desire, a home assistance simulation environment integrating an LLM-driven human user agent. A novel framework FAMER is presented for fast desire alignment through mental reasoning, reflection-based communication, and memory persistence.


<details>
  <summary>Details</summary>
Motivation: Real-world applications require embodied agents to go beyond task execution and collaborate effectively with unfamiliar agents and human users who have vague and implicit goals. Thus, there is a need for agents to interpret ambiguous instructions and uncover underlying desires for effective assistance.

Method: The method involves developing HA-Desire, a home assistance simulation environment with an LLM-driven human user agent that exhibits realistic value-driven goal selection and communication. The ego agent interacts with this proxy user using the FAMER framework, which includes a desire-based mental reasoning mechanism, a reflection-based communication module, and goal-relevant information extraction with memory persistence.

Result: Extensive experiments show that the FAMER framework significantly improves both task execution and communication efficiency, allowing embodied agents to quickly adapt to user-specific desires in complex environments.

Conclusion: Fast and accurate desire alignment is crucial for embodied agents. The proposed FAMER framework enhances the ability of embodied agents to interact effectively with human users in complex environments by improving task execution and communication efficiency.

Abstract: While embodied agents have made significant progress in performing complex
physical tasks, real-world applications demand more than pure task execution.
The agents must collaborate with unfamiliar agents and human users, whose goals
are often vague and implicit. In such settings, interpreting ambiguous
instructions and uncovering underlying desires is essential for effective
assistance. Therefore, fast and accurate desire alignment becomes a critical
capability for embodied agents. In this work, we first develop a home
assistance simulation environment HA-Desire that integrates an LLM-driven human
user agent exhibiting realistic value-driven goal selection and communication.
The ego agent must interact with this proxy user to infer and adapt to the
user's latent desires. To achieve this, we present a novel framework FAMER for
fast desire alignment, which introduces a desire-based mental reasoning
mechanism to identify user intent and filter desire-irrelevant actions. We
further design a reflection-based communication module that reduces redundant
inquiries, and incorporate goal-relevant information extraction with memory
persistence to improve information reuse and reduce unnecessary exploration.
Extensive experiments demonstrate that our framework significantly enhances
both task execution and communication efficiency, enabling embodied agents to
quickly adapt to user-specific desires in complex embodied environments.

</details>


### [300] [LiDAR Based Semantic Perception for Forklifts in Outdoor Environments](https://arxiv.org/abs/2505.22258)
*Benjamin Serfling,Hannes Reichert,Lorenzo Bayerlein,Konrad Doll,Kati Radkhah-Lens*

Main category: cs.RO

TL;DR: This paper presents a new LiDAR-based semantic segmentation framework for autonomous forklifts in complex outdoor settings, integrating a dual LiDAR system to improve obstacle detection and segmentation with high accuracy and runtime efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance the safety and autonomy of forklifts operating in dynamic industrial environments by improving the detection and segmentation of obstacles using LiDAR technology.

Method: The method involves a dual LiDAR system combining forward-facing and downward-angled sensors to capture high-resolution 3D point clouds. A lightweight yet robust approach is then used to segment these point clouds into safety-critical instance classes (e.g., pedestrians, vehicles) and environmental classes (e.g., ground, lanes).

Result: Experimental results show that the approach achieves high segmentation accuracy while meeting strict runtime requirements, proving its effectiveness for autonomous forklift navigation.

Conclusion: The presented framework successfully enables safety-aware, fully autonomous forklift navigation in complex outdoor environments, demonstrating its potential for industrial material handling tasks.

Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework
tailored for autonomous forklifts operating in complex outdoor environments.
Central to our approach is the integration of a dual LiDAR system, which
combines forward-facing and downward-angled LiDAR sensors to enable
comprehensive scene understanding, specifically tailored for industrial
material handling tasks. The dual configuration improves the detection and
segmentation of dynamic and static obstacles with high spatial precision. Using
high-resolution 3D point clouds captured from two sensors, our method employs a
lightweight yet robust approach that segments the point clouds into
safety-critical instance classes such as pedestrians, vehicles, and forklifts,
as well as environmental classes such as driveable ground, lanes, and
buildings. Experimental validation demonstrates that our approach achieves high
segmentation accuracy while satisfying strict runtime requirements,
establishing its viability for safety-aware, fully autonomous forklift
navigation in dynamic warehouse and yard environments.

</details>


### [301] [SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning](https://arxiv.org/abs/2505.22626)
*Yu Zhang,Yuqi Xie,Huihan Liu,Rutav Shah,Michael Wan,Linxi Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: SCIZOR is a self-supervised data curation framework that filters out low-quality state-action pairs to enhance imitation learning performance.


<details>
  <summary>Details</summary>
Motivation: Imitation learning in robotics faces challenges due to variability in dataset quality, necessitating methods to filter low-quality samples and improve policy training outcomes.

Method: SCIZOR uses a self-supervised task progress predictor to identify and remove suboptimal data lacking task progression, and employs a deduplication module to eliminate redundant data patterns based on joint state-action representations.

Result: Empirical results demonstrate that SCIZOR improves imitation learning policy performance by an average of 15.4% across multiple benchmarks while using less data.

Conclusion: SCIZOR effectively curates datasets at the state-action pair level, enhancing the quality and efficiency of imitation learning policies.

Abstract: Imitation learning advances robot capabilities by enabling the acquisition of
diverse behaviors from human demonstrations. However, large-scale datasets used
for policy training often introduce substantial variability in quality, which
can negatively impact performance. As a result, automatically curating datasets
by filtering low-quality samples to improve quality becomes essential. Existing
robotic curation approaches rely on costly manual annotations and perform
curation at a coarse granularity, such as the dataset or trajectory level,
failing to account for the quality of individual state-action pairs. To address
this, we introduce SCIZOR, a self-supervised data curation framework that
filters out low-quality state-action pairs to improve the performance of
imitation learning policies. SCIZOR targets two complementary sources of
low-quality data: suboptimal data, which hinders learning with undesirable
actions, and redundant data, which dilutes training with repetitive patterns.
SCIZOR leverages a self-supervised task progress predictor for suboptimal data
to remove samples lacking task progression, and a deduplication module
operating on joint state-action representation for samples with redundant
patterns. Empirically, we show that SCIZOR enables imitation learning policies
to achieve higher performance with less data, yielding an average improvement
of 15.4% across multiple benchmarks. More information is available at:
https://ut-austin-rpl.github.io/SCIZOR/

</details>


### [302] [FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control](https://arxiv.org/abs/2505.22642)
*Younggyo Seo,Carmelo Sferrazza,Haoran Geng,Michal Nauman,Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: FastTD3 is a new RL algorithm that accelerates training for humanoid robots.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning's complexity and long training times are major bottlenecks in robotics.

Method: Train an off-policy TD3 agent with modifications including parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters.

Result: FastTD3 solves HumanoidBench tasks in under 3 hours on a single A100 GPU while remaining stable during training.

Conclusion: A lightweight and easy-to-use implementation of FastTD3 is provided to accelerate RL research in robotics.

Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but
its complexity and long training times remain major bottlenecks. In this
report, we introduce FastTD3, a simple, fast, and capable RL algorithm that
significantly speeds up training for humanoid robots in popular suites such as
HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably
simple: we train an off-policy TD3 agent with several modifications -- parallel
simulation, large-batch updates, a distributional critic, and carefully tuned
hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours
on a single A100 GPU, while remaining stable during training. We also provide a
lightweight and easy-to-use implementation of FastTD3 to accelerate RL research
in robotics.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [303] [tenSVD algorithm for compression](https://arxiv.org/abs/2505.21686)
*Michele Gallo*

Main category: stat.CO

TL;DR: The paper proposes an efficient image storage method using tensors to reduce memory, bandwidth and energy consumption. It uses the Tucker model for compression and compares it with a baseline algorithm in R.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient image storage approach that minimizes memory, bandwidth and energy consumption.

Method: Organize original data into a higher-order tensor and apply the Tucker model for compression. Compare this method with a baseline algorithm implemented in R.

Result: Evaluation focuses on computational time and quality of information preserved, using both simulated and real datasets. Detailed analysis conducted with established quantitative metrics, paying attention to sustainability in terms of energy consumption.

Conclusion: The proposed tensor-based image storage method is efficient in reducing memory, bandwidth and energy consumption while maintaining good quality of information.

Abstract: Tensors provide a robust framework for managing high-dimensional data.
Consequently, tensor analysis has emerged as an active research area in various
domains, including machine learning, signal processing, computer vision, graph
analysis, and data mining. This study introduces an efficient image storage
approach utilizing tensors, aiming to minimize memory to store, bandwidth to
transmit and energy to processing. The proposed method organizes original data
into a higher-order tensor and applies the Tucker model for compression.
Implemented in R, this method is compared to a baseline algorithm. The
evaluation focuses on efficient of algorithm measured in term of computational
time and the quality of information preserved, using both simulated and real
datasets. A detailed analysis of the results is conducted, employing
established quantitative metrics, with significant attention paid to
sustainability in terms of energy consumption across algorithms.

</details>


### [304] [Are Statistical Methods Obsolete in the Era of Deep Learning?](https://arxiv.org/abs/2505.21723)
*Skyler Wu,Shihao Yang,S. C. Kou*

Main category: stat.CO

TL;DR: 在AI时代，尽管深度学习模型如PINN很流行，但统计方法如MAGI在处理稀疏和噪声数据时表现更佳，尤其是在参数推理、轨迹重建和样本外预测方面。这些方法偏差和方差更低、参数更少、对数值不精确的积累更具鲁棒性，并能更忠实于真实的ODEs。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在建模、推理和预测中的广泛应用，人们开始质疑较简单的统计方法是否仍然具有相关性。

Method: 使用了物理信息神经网络（PINN）作为深度学习范式的代表和流形约束高斯过程推断（MAGI）作为统计原则方法的代表，通过SEIR模型和Lorenz模型进行案例研究。

Result: 统计方法在参数推理和轨迹重建任务中实现更低的偏差和方差，使用更少的参数和超参数调整。在样本外预测方面也明显优于深度学习模型。此外，统计方法对数值不精确的积累更具有鲁棒性，并能更忠实地表示基础系统。

Conclusion: 统计学方法远未过时，在处理稀疏和嘈杂的数据时尤其有效，其结果更准确，计算成本更低且对未来预测更为可靠。

Abstract: In the era of AI, neural networks have become increasingly popular for
modeling, inference, and prediction, largely due to their potential for
universal approximation. With the proliferation of such deep learning models, a
question arises: are leaner statistical methods still relevant? To shed insight
on this question, we employ the mechanistic nonlinear ordinary differential
equation (ODE) inverse problem as a testbed, using physics-informed neural
network (PINN) as a representative of the deep learning paradigm and
manifold-constrained Gaussian process inference (MAGI) as a representative of
statistically principled methods. Through case studies involving the SEIR model
from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate
that statistical methods are far from obsolete, especially when working with
sparse and noisy observations. On tasks such as parameter inference and
trajectory reconstruction, statistically principled methods consistently
achieve lower bias and variance, while using far fewer parameters and requiring
less hyperparameter tuning. Statistical methods can also decisively outperform
deep learning models on out-of-sample future prediction, where the absence of
relevant data often leads overparameterized models astray. Additionally, we
find that statistically principled approaches are more robust to accumulation
of numerical imprecision and can represent the underlying system more faithful
to the true governing ODEs.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [305] [CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs](https://arxiv.org/abs/2505.22469)
*Mohamed R. Elshamy,Mehdi Elahi,Ahmad Patooghy,Abdel-Hameed A. Badawy*

Main category: cs.PF

TL;DR: The paper conducts the first empirical validation of ABPI on commercial hardware, finding accuracy deficiencies in real-world scenarios. It proposes CPINN-ABPI, integrating physics-informed neural networks with ABPI's thermal model to significantly improve power consumption estimation accuracy while maintaining real-time performance.


<details>
  <summary>Details</summary>
Motivation: Efficient thermal and power management in MPSoCs requires accurate power consumption estimation. While ABPI theoretically eliminates dependence on steady-state temperatures, its actual performance has not been verified on real hardware.

Method: Empirical validation of ABPI on NVIDIA Jetson Xavier AGX platform revealed accuracy issues. To address this, a novel approach combining Custom Physics-Informed Neural Networks (CPINNs) with ABPI's thermal model was introduced. This includes a specialized loss function and multi-objective genetic algorithm optimization.

Result: CPINN-ABPI reduces mean absolute error by 84.7% for CPU and 73.9% for GPU compared to ABPI. Weighted mean absolute percentage error improves from 47%-81% to ~12%. The method maintains real-time performance with inference time of 195.3 μs and shows similar accuracy gains across different SoCs.

Conclusion: CPINN-ABPI significantly improves power consumption estimation accuracy over ABPI while maintaining computational efficiency, making it suitable for real-time thermal and power management in modern MPSoCs.

Abstract: Efficient thermal and power management in modern multiprocessor
systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of
the state-of-the-art approaches, Alternative Blind Power Identification (ABPI),
theoretically eliminates the dependence on steady-state temperatures,
addressing a major shortcoming of previous approaches. However, ABPI
performance has remained unverified in actual hardware implementations. In this
study, we conduct the first empirical validation of ABPI on commercial hardware
using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while
ABPI provides computational efficiency and independence from steady-state
temperature, it exhibits considerable accuracy deficiencies in real-world
scenarios. To overcome these limitations, we introduce a novel approach that
integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying
thermal model of ABPI. Our approach employs a specialized loss function that
harmonizes physical principles with data-driven learning, complemented by
multi-objective genetic algorithm optimization to balance estimation accuracy
and computational cost. In experimental validation, CPINN-ABPI achieves a
reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE)
relative to ABPI, with the weighted mean absolute percentage error (WMAPE)
improving from 47\%--81\% to $\sim$12\%. The method maintains real-time
performance with 195.3~$\mu$s of inference time, with similar 85\%--99\%
accuracy gains across heterogeneous SoCs.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [306] [Multi-photon QKD for Practical Quantum Networks](https://arxiv.org/abs/2505.21726)
*Nitin Jha,Abhishek Parakh,Mahadevan Subramaniam*

Main category: quant-ph

TL;DR: The paper explores the 3-stage QKD protocol, comparing it with conventional protocols and analyzing its efficiency across different network topologies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current QKD protocols due to lack of single-photon emitters and noisy hardware, as well as improving transmission distances and integration into classical networks.

Method: Compare and contrast the 3-stage QKD protocol with conventional ones; analyze their performance in various network conditions and topologies; establish a mathematical relationship for key rates to extend transmission distances.

Result: Provides insights on how the 3-stage QKD protocol can be more effective in certain topologies and conditions, allowing for longer transmission distances without relying on single-photon transmissions.

Conclusion: The 3-stage QKD protocol shows potential for practical implementation in future quantum networks by overcoming current limitations.

Abstract: Quantum key distribution (QKD) will most likely be an integral part of any
practical quantum network in the future. However, not all QKD protocols can be
used in today's networks because of the lack of single-photon emitters and
noisy intermediate quantum hardware. Attenuated-photon transmission, typically
used to simulate single-photon emitters, severely limits the achievable
transmission distances and makes the integration of the QKD into existing
classical networks, that use tens of thousands of photons per bit of
transmission, difficult. Furthermore, it has been found that protocol
performance varies with topology. In order to remove the reliance of QKD on
single-photon emitters and increase transmission distances, it is worthwhile to
explore QKD protocols that do not rely on single-photon transmissions for
security, such as the 3-stage QKD protocol, which can tolerate multiple photons
in each burst without information leakage. This paper compares and contrasts
the 3-stage QKD protocol with conventional QKD protocols and its efficiency in
different network topologies and conditions. Furthermore, we establish a
mathematical relationship between achievable key rates to increase transmission
distances in various topologies.

</details>


### [307] [Online Voting using Point to MultiPoint Quantum Key Distribution via Passive Optical Networks](https://arxiv.org/abs/2505.21756)
*Bernardo A. Huberman,Jing Wang*

Main category: quant-ph

TL;DR: This paper proposes a method using Point-to-Multipoint QKD through TDM and WDM in PON for enhancing the security of online voting systems.


<details>
  <summary>Details</summary>
Motivation: To improve the security of online voting systems.

Method: Propose to use Point-to-Multipoint QKD with TDM and WDM in PON.

Result: The security of online voting systems is improved by implementing this method.

Conclusion: Using Point-to-Multipoint QKD via TDM and WDM in PON can enhance the security of online voting systems.

Abstract: We propose using Point-to-Multipoint quantum key distribution (QKD) via time
division multiplexing (TDM) and wavelength division multiplexing (WDM) in
passive optical networks (PON) to improve the security of online voting
systems.

</details>


### [308] [Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise](https://arxiv.org/abs/2505.18478)
*Lucas Tecot,Di Luo,Cho-Jui Hsieh*

Main category: quant-ph

TL;DR: This paper explores a noise-resilient training theory and algorithm for quantum circuit classifiers, enhancing robustness against parameter noise with minimal changes to optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: Noise is a significant challenge in achieving reliable quantum algorithms. The authors aim to develop a method that can improve the resilience of parameterized quantum circuits to noise.

Method: The method presented has a connection to Evolutionary Strategies and ensures resilience to parameter noise with minor adjustments to standard optimization algorithms. It is function-agnostic and adaptable to various quantum circuits.

Result: The approach was successfully demonstrated in quantum phase classification tasks, proving its effectiveness in enhancing the robustness of quantum circuit classifiers.

Conclusion: By establishing a provably guaranteed optimization theory for quantum circuits, the work paves the way for practical applications of near-term quantum computers.

Abstract: Advancements in quantum computing have spurred significant interest in
harnessing its potential for speedups over classical systems. However, noise
remains a major obstacle to achieving reliable quantum algorithms. In this
work, we present a provably noise-resilient training theory and algorithm to
enhance the robustness of parameterized quantum circuit classifiers. Our
method, with a natural connection to Evolutionary Strategies, guarantees
resilience to parameter noise with minimal adjustments to commonly used
optimization algorithms. Our approach is function-agnostic and adaptable to
various quantum circuits, successfully demonstrated in quantum phase
classification tasks. By developing provably guaranteed optimization theory
with quantum circuits, our work opens new avenues for practical, robust
applications of near-term quantum computers.

</details>


### [309] [Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion](https://arxiv.org/abs/2505.22193)
*Marco Parigi,Stefano Martina,Francesco Aldo Venturelli,Filippo Caruso*

Main category: quant-ph

TL;DR: The paper explores Quantum Diffusion Models (QDMs) and proposes two physics-inspired protocols to enhance generative AI using quantum properties. The first protocol uses quantum stochastic walks to generate MNIST images with lower FID compared to classical methods, while the second leverages the noise of IBM quantum hardware for image generation. This work highlights potential pathways for scalable quantum Generative AI.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of classical generative AI models by incorporating quantum properties and address scalability issues due to limitations of near-term quantum devices.

Method: 1. Employing quantum stochastic walks to create a mix of quantum and classical dynamics in the forward process for generating MNIST images. 2. Using the intrinsic noise of real IBM quantum hardware with four qubits to develop an algorithm for image generation.

Result: The first approach resulted in statistically more robust models that generated MNIST images with lower Fréchet Inception Distance (FID) than classical dynamics alone. The second approach successfully utilized quantum hardware noise for image generation.

Conclusion: This study demonstrates the potential of quantum properties in enhancing generative AI and suggests new directions for large-scale algorithms in quantum Generative AI where quantum noise is used as a beneficial resource rather than being mitigated or corrected.

Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI
that aims to use quantum properties to improve the performances of their
classical counterparts. However, existing algorithms are not easily scalable
due to the limitations of near-term quantum devices. Following our previous
work on QDMs, here we propose and implement two physics-inspired protocols. In
the first, we use the formalism of quantum stochastic walks, showing that a
specific interplay of quantum and classical dynamics in the forward process
produces statistically more robust models generating sets of MNIST images with
lower Fr\'echet Inception Distance (FID) than using totally classical dynamics.
In the second approach, we realize an algorithm to generate images by
exploiting the intrinsic noise of real IBM quantum hardware with only four
qubits. Our work could be a starting point to pave the way for new scenarios
for large-scale algorithms in quantum Generative AI, where quantum noise is
neither mitigated nor corrected, but instead exploited as a useful resource.

</details>


### [310] [Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz](https://arxiv.org/abs/2505.22083)
*H. L. Dao*

Main category: quant-ph

TL;DR: 本研究首次引入了双曲GRU形式的非欧几里得神经量子态（NQS）假设，用于变分蒙特卡罗方法中近似多体量子系统的基态波函数。通过在最多100个自旋的一维和二维横向场Ising模型以及最多50个自旋的一维Heisenberg $J_1J_2$ 和 $J_1J_2J_3$ 系统中进行实验，发现双曲GRU的表现与欧几里得RNN相当或更好。特别是在具有明显层次交互结构的哈密顿量设置中，如一维Heisenberg $J_1J_2$ 和 $J_1J_2J_3$ 系统，双曲GRU显著优于其欧几里得版本。这些结果类似于自然语言处理中的已知结果，表明当训练数据呈现树状或层次结构时，双曲GRU几乎总是优于欧几里得RNN。因此，我们推测在涉及不同最近邻相互作用的量子自旋系统中，双曲GRU NQS假设可能会优于欧几里得RNN/GRU NQS假设。


<details>
  <summary>Details</summary>
Motivation: 为了探索非欧几里得神经网络在量子多体问题中的应用潜力，特别是双曲GRU是否可以在特定情况下超越传统的欧几里得RNN/GRU表现。

Method: 使用双曲GRU构建非欧几里得神经量子态（NQS）假设，并将其应用于变分蒙特卡罗方法中，以近似求解包括横向场Ising模型和Heisenberg $J_1J_2$ 及 $J_1J_2J_3$ 系统在内的量子多体系统的基态波函数。

Result: 双曲GRU在所有实验中表现与欧几里得RNN相当或更优，尤其在哈密顿量具有层次交互结构的情况下，双曲GRU显著优于欧几里得版本。

Conclusion: 双曲GRU作为第一种非欧几里得NQS假设在量子多体系统中是可行的，并可能在涉及最近邻相互作用的量子自旋系统中优于欧几里得RNN/GRU。

Abstract: In this work, we introduce the first type of non-Euclidean neural quantum
state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent
neural networks (RNNs)), to be used in the Variational Monte Carlo method of
approximating the ground state wavefunction for quantum many-body systems. In
particular, we examine the performances of NQS ansatzes constructed from both
conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical
settings of the one- and two-dimensional transverse field Ising models (TFIM)
of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$
systems of up 50 spins. By virtue of the fact that, for all of the experiments
performed in this work, hyperbolic GRU can yield performances comparable to or
better than Euclidean RNNs, which have been extensively studied in these
settings in the literature, our work is a proof-of-concept for the viability of
hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum
many-body systems. Furthermore, in settings where the Hamiltonian displays a
clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ &
$J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor
interactions, our results show that hyperbolic GRU definitively outperforms its
Euclidean version in all instances. The fact that these results are reminiscent
of the established ones from natural language processing where hyperbolic GRU
almost always outperforms Euclidean RNNs when the training data exhibit a
tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU
NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin
systems that involve different degrees of nearest neighbor interactions.
Finally, with this work, we hope to initiate future studies of other types of
non-Euclidean NQS beyond hyperbolic GRU.

</details>


### [311] [Depth-Based Matrix Classification for the HHL Quantum Algorithm](https://arxiv.org/abs/2505.22454)
*Mark Danza,Sonia Lopez Alarcon,Cory Merkel*

Main category: quant-ph

TL;DR: 在量子计算纠错时代即将来临之际，理解某些后NISQ算法对实际问题的适用性是很必要的。本文探讨了是否可以利用机器学习分类器，在已知一些数值信息的前提下，将问题标记为适合或不适合HHL算法实现。研究发现，基于方程组矩阵的数值特性进行问题分类时，训练具有显著代表性的数据分布是关键。通过精心设计训练数据分布和分类器参数，多层感知机可以实现精确分类。


<details>
  <summary>Details</summary>
Motivation: 了解后NISQ算法（如HHL算法）对于实际问题的适用性变得越来越重要，尤其是在许多问题都可以表示为线性方程组的情况下。然而，HHL算法在大多数情况下无法提供实际、合理的解决方案。因此需要找到一种方法来判断哪些问题适合使用HHL算法。

Method: 使用机器学习分类器，根据已知的数值信息，将问题标记为适合或不适合HHL算法实现。重点研究了训练数据分布和分类器参数的设计对分类效果的影响。采用多层感知机（MLP）作为分类模型。

Result: 研究表明，训练数据的分布必须具有显著的代表性才能获得良好的分类效果。通过精心设计训练数据分布和分类器参数，多层感知机能够实现准确的问题分类。

Conclusion: 基于方程组矩阵的数值特性，利用机器学习分类器可以有效判断问题是否适合使用HHL算法解决，但需要确保训练数据分布的代表性和分类器参数的合理设计。

Abstract: Under the nearing error-corrected era of quantum computing, it is necessary
to understand the suitability of certain post-NISQ algorithms for practical
problems. One of the most promising, applicable and yet difficult to implement
in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear
systems of equations. An enormous number of problems can be expressed as linear
systems of equations, from Machine Learning to fluid dynamics. However, in most
cases, HHL will not be able to provide a practical, reasonable solution to
these problems. This paper's goal inquires about whether problems can be
labeled using Machine Learning classifiers as suitable or unsuitable for HHL
implementation when some numerical information about the problem is known
beforehand. This work demonstrates that training on significantly
representative data distributions is critical to achieve good classifications
of the problems based on the numerical properties of the matrix representing
the system of equations. Accurate classification is possible through
Multi-Layer Perceptrons, although with careful design of the training data
distribution and classifier parameters.

</details>


### [312] [Assessing Quantum Advantage for Gaussian Process Regression](https://arxiv.org/abs/2505.22502)
*Dominic Lowe,M. S. Kim,Roberto Bondesan*

Main category: quant-ph

TL;DR: In this paper, the authors demonstrate that quantum algorithms for Gaussian Process Regression lack exponential speedup in a wide range of scenarios by proving the linear scaling properties of kernel matrices.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to evaluate the effectiveness of quantum algorithms applied to Gaussian Process Regression, specifically focusing on whether these algorithms offer exponential speedups over classical methods.

Method: The authors rigorously prove that the condition number, sparsity, and Frobenius norm of a kernel matrix scale at least linearly with the matrix size under general assumptions. They also conduct numerical verification using popular kernels from machine learning.

Result: The results indicate that there is no exponential speedup in the runtime of quantum algorithms for Gaussian Process Regression, independent of the complexity of loading classical data onto a quantum computer. These findings also apply to dequantised algorithms.

Conclusion: Quantum algorithms for Gaussian Process Regression do not provide exponential speedups in most practical scenarios due to the linear scaling properties of kernel matrices.

Abstract: Gaussian Process Regression is a well-known machine learning technique for
which several quantum algorithms have been proposed. We show here that in a
wide range of scenarios these algorithms show no exponential speedup. We
achieve this by rigorously proving that the condition number of a kernel matrix
scales at least linearly with the matrix size under general assumptions on the
data and kernel. We additionally prove that the sparsity and Frobenius norm of
a kernel matrix scale linearly under similar assumptions. The implications for
the quantum algorithms runtime are independent of the complexity of loading
classical data on a quantum computer and also apply to dequantised algorithms.
We supplement our theoretical analysis with numerical verification for popular
kernels in machine learning.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [313] [Towards Efficient Key-Value Cache Management for Prefix Prefilling in LLM Inference](https://arxiv.org/abs/2505.21919)
*Yue Zhu,Hao Yu,Chen Wang,Zhuoran Liu,Eun Kyung Lee*

Main category: cs.ET

TL;DR: The paper explores the need for efficient Key-Value Cache (KVC) management in large language models (LLMs), analyzing current systems and identifying gaps in tailored storage solutions for KVC prefilling.


<details>
  <summary>Details</summary>
Motivation: The increasing use of LLMs with larger context windows leads to challenges in managing Key-Value Cache efficiently, especially given high cache reusability in certain inference workloads. This motivates the need to explore existing systems and identify potential improvements.

Method: Analyze real-world KVC access patterns using publicly available traces and evaluate commercial key-value stores like Redis as well as state-of-the-art RDMA-based systems (CHIME and Sherman) for KVC metadata management.

Result: Demonstrates that there is a lack of tailored storage solutions for KVC prefilling, and highlights the necessity for an efficient distributed caching system with optimized metadata management for LLM workloads.

Conclusion: There is a critical need for improved KVC management systems designed specifically for scalable, low-latency inference in LLM workloads.

Abstract: The increasing adoption of large language models (LLMs) with extended context
windows necessitates efficient Key-Value Cache (KVC) management to optimize
inference performance. Inference workloads like Retrieval-Augmented Generation
(RAG) and agents exhibit high cache reusability, making efficient caching
critical to reducing redundancy and improving speed. We analyze real-world KVC
access patterns using publicly available traces and evaluate commercial
key-value stores like Redis and state-of-the-art RDMA-based systems (CHIME [1]
and Sherman [2]) for KVC metadata management. Our work demonstrates the lack of
tailored storage solution for KVC prefilling, underscores the need for an
efficient distributed caching system with optimized metadata management for LLM
workloads, and provides insights into designing improved KVC management systems
for scalable, low-latency inference.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [314] [Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives](https://arxiv.org/abs/2505.21627)
*Ander Artola Velasco,Stratis Tsirtsis,Nastaran Okati,Manuel Gomez-Rodriguez*

Main category: cs.GT

TL;DR: Advanced large language models need special hardware and energy. Cloud services providing these models are popular, where users pay per token used to generate output. This pricing can lead providers to misreport token usage for profit, as users cannot verify charges. We introduce an algorithm showing how providers could overcharge without detection, revealing vulnerabilities in the current system. To address this, we propose a new pricing mechanism based on output characters rather than tokens, eliminating the incentive to misreport. Experiments with various models support our findings.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the financial incentives for providers of cloud-based large language model services to misreport token usage under the current pay-per-token pricing mechanism, which can lead to users being unknowingly overcharged.

Method: We demonstrate the vulnerability by introducing an efficient heuristic algorithm that allows providers to overcharge users without raising suspicion. Additionally, we propose a new incentive-compatible token pricing mechanism where the cost is based on the number of characters in the output instead of tokens.

Result: Our experiments with models from the Llama, Gemma, and Ministral families show that the heuristic algorithm can significantly overcharge users without detection, highlighting the vulnerability. The proposed character-based pricing mechanism successfully removes the financial incentive to misreport token usage.

Conclusion: The current pay-per-token pricing mechanism creates vulnerabilities that allow providers to overcharge users. A character-based pricing mechanism eliminates this issue, ensuring transparency and fairness in pricing.

Abstract: State-of-the-art large language models require specialized hardware and
substantial energy to operate. As a consequence, cloud-based services that
provide access to large language models have become very popular. In these
services, the price users pay for an output provided by a model depends on the
number of tokens the model uses to generate it -- they pay a fixed price per
token. In this work, we show that this pricing mechanism creates a financial
incentive for providers to strategize and misreport the (number of) tokens a
model used to generate an output, and users cannot prove, or even know, whether
a provider is overcharging them. However, we also show that, if an unfaithful
provider is obliged to be transparent about the generative process used by the
model, misreporting optimally without raising suspicion is hard. Nevertheless,
as a proof-of-concept, we introduce an efficient heuristic algorithm that
allows providers to significantly overcharge users without raising suspicion,
highlighting the vulnerability of users under the current pay-per-token pricing
mechanism. Further, to completely eliminate the financial incentive to
strategize, we introduce a simple incentive-compatible token pricing mechanism.
Under this mechanism, the price users pay for an output provided by a model
depends on the number of characters of the output -- they pay a fixed price per
character. Along the way, to illustrate and complement our theoretical results,
we conduct experiments with several large language models from the
$\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input
prompts from the LMSYS Chatbot Arena platform.

</details>


### [315] [Online Fair Division for Personalized $2$-Value Instances](https://arxiv.org/abs/2505.22174)
*Georgios Amanatidis,Alexandros Lolos,Evangelos Markakis,Victor Turmel*

Main category: cs.GT

TL;DR: 研究在线公平分配问题，提出针对个性化2值实例的算法，并探讨未来信息对结果的影响。


<details>
  <summary>Details</summary>
Motivation: 在商品逐一到达且代理人的价值函数受限的情况下，寻求在不假设价值受严格限制或来自分布时仍能实现强不可能结果的方法。

Method: 设计一个确定性算法，维持每一步骤的1/(2n-1)-MMS分配，并最终达到1/4-MMS分配；通过有限的未来信息设计基于匹配的算法以实现更强的结果。

Result: 提出的算法在最坏情况下对已知的公平概念（如最大最小份额公平性和几乎无嫉妒）提供了保证，并首次为有界代理价值比的加性实例提供了非平凡的保证。

Conclusion: 通过关注受限估值函数实例，可以克服强不可能结果，获得关于公平性的更佳保证。

Abstract: We study an online fair division setting, where goods arrive one at a time
and there is a fixed set of $n$ agents, each of whom has an additive valuation
function over the goods. Once a good appears, the value each agent has for it
is revealed and it must be allocated immediately and irrevocably to one of the
agents. It is known that without any assumptions about the values being
severely restricted or coming from a distribution, very strong impossibility
results hold in this setting. To bypass the latter, we turn our attention to
instances where the valuation functions are restricted. In particular, we study
personalized $2$-value instances, where there are only two possible values each
agent may have for each good, possibly different across agents, and we show how
to obtain worst case guarantees with respect to well-known fairness notions,
such as maximin share fairness and envy-freeness up to one (or two) good(s). We
suggest a deterministic algorithm that maintains a $1/(2n-1)$-MMS allocation at
every time step and show that this is the best possible any deterministic
algorithm can achieve if one cares about every single time step; nevertheless,
eventually the allocation constructed by our algorithm becomes a $1/4$-MMS
allocation. To achieve this, the algorithm implicitly maintains a fragile
system of priority levels for all agents. Further, we show that, by allowing
some limited access to future information, it is possible to have stronger
results with less involved approaches. By knowing the values of goods for $n-1$
time steps into the future, we design a matching-based algorithm that achieves
an EF$1$ allocation every $n$ time steps, while always maintaining an EF$2$
allocation. Finally, we show that our results allow us to get the first
nontrivial guarantees for additive instances in which the ratio of the maximum
over the minimum value an agent has for a good is bounded.

</details>


### [316] [Strengthening Proportionality in Temporal Voting](https://arxiv.org/abs/2505.22513)
*Bradley Phillips,Edith Elkind,Nicholas Teh,Tomasz Wąs*

Main category: cs.GT

TL;DR: 研究了在时间投票框架下，超越EJR的更强比例代表权概念的存在性和相互关系。


<details>
  <summary>Details</summary>
Motivation: 现有的比例代表权概念（JR、PJR、EJR）已被适应到时间投票场景，但需要探索更强的比例性概念以进一步优化表示。

Method: 引入并研究了更强的JR、PJR和EJR变体，并提出了多赢家选举中更严格的概念（如EJR+、FJR、FPJR和Core）的时间适应版本。通过分析这些新概念的存在性和与现有概念的关系，构建了一个丰富的比例性概念层次结构。

Result: 证明了两个新提出的公理EJR+和FJR能够强化EJR，同时在每次时间选举中仍然可满足。

Conclusion: 本研究扩展了时间投票中的比例代表权概念，提供了更精细和强大的工具来评估和实现比例性。

Abstract: We study proportional representation in the framework of temporal voting with
approval ballots. Prior work adapted basic proportional representation concepts
-- justified representation (JR), proportional JR (PJR), and extended JR (EJR)
-- from the multiwinner setting to the temporal setting. Our work introduces
and examines ways of going beyond EJR. Specifically, we consider stronger
variants of JR, PJR, and EJR, and introduce temporal adaptations of more
demanding multiwinner axioms, such as EJR+, full JR (FJR), full proportional JR
(FPJR), and the Core. For each of these concepts, we investigate its existence
and study its relationship to existing notions, thereby establishing a rich
hierarchy of proportionality concepts. Notably, we show that two of our
proposed axioms -- EJR+ and FJR -- strengthen EJR while remaining satisfiable
in every temporal election.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [317] [Scrapers selectively respect robots.txt directives: evidence from a large-scale empirical study](https://arxiv.org/abs/2505.21733)
*Taein Kim,Karstan Bock,Claire Luo,Amanda Liswood,Emily Wenger*

Main category: cs.NI

TL;DR: The paper conducts a large-scale study on how well web scrapers comply with robots.txt directives using anonymized web logs, finding that stricter directives see less compliance, especially from AI search crawlers, thus suggesting the need for alternative approaches.


<details>
  <summary>Details</summary>
Motivation: To understand the efficacy of the Robots Exclusion Protocol (REP) and whether bots comply with robots.txt directives.

Method: Analyze the behavior of 130 self-declared bots and many anonymous ones over 40 days using controlled robots.txt experiments based on anonymized web logs from the institution.

Result: Bots are less likely to comply with stricter robots.txt directives. AI search crawlers rarely check robots.txt.

Conclusion: Relying on robots.txt to prevent unwanted scraping is risky and there is a need for alternative approaches.

Abstract: Online data scraping has taken on new dimensions in recent years, as
traditional scrapers have been joined by new AI-specific bots. To counteract
unwanted scraping, many sites use tools like the Robots Exclusion Protocol
(REP), which places a robots.txt file at the site root to dictate scraper
behavior. Yet, the efficacy of the REP is not well-understood. Anecdotal
evidence suggests some bots comply poorly with it, but no rigorous study exists
to support (or refute) this claim. To understand the merits and limits of the
REP, we conduct the first large-scale study of web scraper compliance with
robots.txt directives using anonymized web logs from our institution. We
analyze the behavior of 130 self-declared bots (and many anonymous ones) over
40 days, using a series of controlled robots.txt experiments. We find that bots
are less likely to comply with stricter robots.txt directives, and that certain
categories of bots, including AI search crawlers, rarely check robots.txt at
all. These findings suggest that relying on robots.txt files to prevent
unwanted scraping is risky and highlight the need for alternative approaches.

</details>


### [318] [Collaborative Agentic AI Needs Interoperability Across Ecosystems](https://arxiv.org/abs/2505.21550)
*Rishi Sharma,Martijn de Vos,Pradyumna Chari,Ramesh Raskar,Anne-Marie Kermarrec*

Main category: cs.NI

TL;DR: This position paper proposes 'Web of Agents', a minimal architectural foundation for collaborative agentic AI, to promote interoperability among AI-powered agents and prevent ecosystem fragmentation through four components: agent-to-agent messaging, interaction interoperability, state management, and agent discovery.


<details>
  <summary>Details</summary>
Motivation: The field of collaborative agentic AI is advancing rapidly, but current solutions are developed in isolation, leading to fragmented and incompatible ecosystems. There's an urgent need for interoperability to ensure open, secure, web-scale, and widely-adopted agentic systems.

Method: The authors devise the 'Web of Agents', composed of four key components - agent-to-agent messaging, interaction interoperability, state management, and agent discovery - which adopts existing standards and infrastructure where possible to create an interoperable system.

Result: The proposal introduces a pragmatic path forward for creating interoperable agentic systems before ecosystem fragmentation becomes the norm, marking a critical first step in this direction.

Conclusion: Interoperability achieved by adopting minimal standards is essential for ensuring open, secure, and widely-adopted agentic ecosystems. The Web of Agents offers a foundational framework to achieve this goal.

Abstract: Collaborative agentic AI is projected to transform entire industries by
enabling AI-powered agents to autonomously perceive, plan, and act within
digital environments. Yet, current solutions in this field are all built in
isolation, and we are rapidly heading toward a landscape of fragmented,
incompatible ecosystems. In this position paper, we argue that
interoperability, achieved by the adoption of minimal standards, is essential
to ensure open, secure, web-scale, and widely-adopted agentic ecosystems. To
this end, we devise a minimal architectural foundation for collaborative
agentic AI, named Web of Agents, which is composed of four components:
agent-to-agent messaging, interaction interoperability, state management, and
agent discovery. Web of Agents adopts existing standards and reuses existing
infrastructure where possible. With Web of Agents, we take the first but
critical step toward interoperable agentic systems and offer a pragmatic path
forward before ecosystem fragmentation becomes the norm.

</details>


### [319] [MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction](https://arxiv.org/abs/2505.21553)
*Hui Ma,Kai Yang*

Main category: cs.NI

TL;DR: 网络流量预测技术在网络拥塞控制和用户体验提升方面具有重要意义。现有的预测技术在有足够的训练数据时可以取得良好的效果，但在训练数据不足的情况下进行准确预测仍是一个巨大的挑战。为此，本文提出了一种基于多模态元学习框架的深度学习模型MetaSTNet。该模型通过在模拟器中训练并在真实环境中转移元知识，能够快速适应新任务并仅使用少量真实世界训练数据获得准确预测。此外，还采用了交叉一致性预测来评估校准后的预测区间。


<details>
  <summary>Details</summary>
Motivation: 网络流量预测对于网络拥塞控制和用户体验改进至关重要，但当仅有少量训练数据时，现有方法难以实现准确预测。

Method: 提出了一个名为MetaSTNet的深度学习模型，基于多模态元学习框架，采用端到端的网络架构，在模拟器中训练模型，并将元知识转移到现实环境中，以实现少量真实数据下的快速适应和准确预测。同时，使用交叉一致性预测来评估校准后的预测区间。

Result: 广泛的实验表明，MetaSTNet在真实世界数据集上具有高效性和有效性。

Conclusion: MetaSTNet能够在仅有少量真实世界训练数据的情况下，快速适应新任务并提供准确的预测，同时其预测区间经过校准评估，证明了该模型的有效性和实用性。

Abstract: Network traffic prediction techniques have attracted much attention since
they are valuable for network congestion control and user experience
improvement. While existing prediction techniques can achieve favorable
performance when there is sufficient training data, it remains a great
challenge to make accurate predictions when only a small amount of training
data is available. To tackle this problem, we propose a deep learning model,
entitled MetaSTNet, based on a multimodal meta-learning framework. It is an
end-to-end network architecture that trains the model in a simulator and
transfers the meta-knowledge to a real-world environment, which can quickly
adapt and obtain accurate predictions on a new task with only a small amount of
real-world training data. In addition, we further employ cross conformal
prediction to assess the calibrated prediction intervals. Extensive experiments
have been conducted on real-world datasets to illustrate the efficiency and
effectiveness of MetaSTNet.

</details>


### [320] [Fog Intelligence for Network Anomaly Detection](https://arxiv.org/abs/2505.21563)
*Kai Yang,Hui Ma,Shaoyu Dou*

Main category: cs.NI

TL;DR: Anomalies in network system monitoring are crucial to detect due to potential network threats, service outages, and security risks. However, the scale and complexity of mobile communication networks make it challenging. Traditional mathematical models fall short, as do most centralized machine learning algorithms. This article introduces fog intelligence, a distributed machine learning architecture for intelligent wireless network management that combines edge processing and cloud computing advantages while being scalable and privacy-preserving.


<details>
  <summary>Details</summary>
Motivation: Detecting anomalies in network system monitoring is vital due to potential network threats, service outages, and security risks. The increasing scale and complexity of mobile communication networks, along with the large amount of surveillance data, make monitoring and discovering abnormal behaviors difficult.

Method: The authors propose fog intelligence, a distributed machine learning architecture for intelligent wireless network management. It integrates edge processing and centralized cloud computing, making it scalable and privacy-preserving.

Result: Fog intelligence offers a solution to the challenges posed by large-scale distributed wireless networks. It enables intelligent network management while preserving privacy and scalability.

Conclusion: Fog intelligence represents a promising approach to managing complex, large-scale wireless networks by combining the strengths of edge and cloud computing, ensuring scalability and privacy.

Abstract: Anomalies are common in network system monitoring. When manifested as network
threats to be mitigated, service outages to be prevented, and security risks to
be ameliorated, detecting such anomalous network behaviors becomes of great
importance. However, the growing scale and complexity of the mobile
communication networks, as well as the ever-increasing amount and
dimensionality of the network surveillance data, make it extremely difficult to
monitor a mobile network and discover abnormal network behaviors. Recent
advances in machine learning allow for obtaining near-optimal solutions to
complicated decision-making problems with many sources of uncertainty that
cannot be accurately characterized by traditional mathematical models. However,
most machine learning algorithms are centralized, which renders them
inapplicable to a large-scale distributed wireless networks with tens of
millions of mobile devices. In this article, we present fog intelligence, a
distributed machine learning architecture that enables intelligent wireless
network management. It preserves the advantage of both edge processing and
centralized cloud computing. In addition, the proposed architecture is
scalable, privacy-preserving, and well suited for intelligent management of a
distributed wireless network.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [321] [VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents](https://arxiv.org/abs/2505.21568)
*Haiyun Li,Zhiyong Wu,Xiaofeng Xie,Jingran Xie,Yaoxun Xu,Hanyang Peng*

Main category: cs.SD

TL;DR: VoiceMark is the first zero-shot VC-resistant watermarking method that uses speaker-specific latents as the watermark carrier, achieving over 95% accuracy in watermark detection after zero-shot VC synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods for voice cloning (VC)-resistant watermarking fail in zero-shot VC scenarios, where models synthesize audio from an audio prompt without training.

Method: VoiceMark leverages speaker-specific latents as the watermark carrier, allowing the watermark to transfer through the zero-shot VC process into the synthesized audio. Additionally, VC-simulated augmentations and VAD-based loss are introduced to enhance robustness against distortions.

Result: Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, significantly outperforming existing methods which only reach around 50%.

Conclusion: VoiceMark is a novel and effective approach for zero-shot VC-resistant watermarking.

Abstract: Voice cloning (VC)-resistant watermarking is an emerging technique for
tracing and preventing unauthorized cloning. Existing methods effectively trace
traditional VC models by training them on watermarked audio but fail in
zero-shot VC scenarios, where models synthesize audio from an audio prompt
without training. To address this, we propose VoiceMark, the first zero-shot
VC-resistant watermarking method that leverages speaker-specific latents as the
watermark carrier, allowing the watermark to transfer through the zero-shot VC
process into the synthesized audio. Additionally, we introduce VC-simulated
augmentations and VAD-based loss to enhance robustness against distortions.
Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves
over 95% accuracy in watermark detection after zero-shot VC synthesis,
significantly outperforming existing methods, which only reach around 50%. See
our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark

</details>


### [322] [Music Source Restoration](https://arxiv.org/abs/2505.21827)
*Yongyi Zang,Zheqi Dai,Mark D. Plumbley,Qiuqiang Kong*

Main category: cs.SD

TL;DR: The paper introduces Music Source Restoration (MSR), a new task bridging idealized source separation and real-world music production, presents RawStems dataset, considers various degradations, establishes U-Former as baseline method, and releases resources publicly.


<details>
  <summary>Details</summary>
Motivation: To address the gap between idealized source separation and real-world music production where signal degradations such as equalization, compression, and reverb are ignored in current Music Source Separation approaches.

Method: Introduce MSR which models mixtures as degraded sums of individually degraded sources to recover original signals. Create RawStems dataset with unprocessed source signals categorized hierarchically. Consider spectral filtering, dynamic range compression, harmonic distortion, reverb and lossy codec as possible degradations. Establish U-Former as baseline method.

Result: Demonstrate the feasibility of MSR on the presented dataset and establish U-Former as a baseline method.

Conclusion: MSR is a novel task that bridges the gap between idealized source separation and real-world music production. The RawStems dataset, degradation simulation pipeline, training code and pre-trained models have been made publicly available.

Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap
between idealized source separation and real-world music production. Current
Music Source Separation (MSS) approaches assume mixtures are simple sums of
sources, ignoring signal degradations employed during music production like
equalization, compression, and reverb. MSR models mixtures as degraded sums of
individually degraded sources, with the goal of recovering original, undegraded
signals. Due to the lack of data for MSR, we present RawStems, a dataset
annotation of 578 songs with unprocessed source signals organized into 8
primary and 17 secondary instrument groups, totaling 354.13 hours. To the best
of our knowledge, RawStems is the first dataset that contains unprocessed music
stems with hierarchical categories. We consider spectral filtering, dynamic
range compression, harmonic distortion, reverb and lossy codec as possible
degradations, and establish U-Former as a baseline method, demonstrating the
feasibility of MSR on our dataset. We release the RawStems dataset annotations,
degradation simulation pipeline, training code and pre-trained models to be
publicly available.

</details>


### [323] [Improving Respiratory Sound Classification with Architecture-Agnostic Knowledge Distillation from Ensembles](https://arxiv.org/abs/2505.22027)
*Miika Toikkanen,June-Woo Kim*

Main category: cs.SD

TL;DR: The paper explores the use of soft labels for respiratory sound classification to distill an ensemble of teacher models into a student model, achieving new state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Respiratory sound datasets are limited in size and quality, making high performance difficult to achieve. Traditional ensemble models help but increase compute cost at inference time.

Method: The study uses soft label training as an architecture-agnostic approach to distill knowledge from an ensemble of teacher models into a student model. They examine different variations of this approach.

Result: Even a single teacher model identical to the student significantly improves performance beyond its own capability, with optimal gains achieved using only a few teachers. The method achieves a new state-of-the-art score of 64.39 on ICHBI, surpassing the previous best by 0.85 and improving average scores across architectures by more than 1.16.

Conclusion: The results demonstrate the effectiveness of knowledge distillation with soft labels for respiratory sound classification, regardless of the size or architecture of the models.

Abstract: Respiratory sound datasets are limited in size and quality, making high
performance difficult to achieve. Ensemble models help but inevitably increase
compute cost at inference time. Soft label training distills knowledge
efficiently with extra cost only at training. In this study, we explore soft
labels for respiratory sound classification as an architecture-agnostic
approach to distill an ensemble of teacher models into a student model. We
examine different variations of our approach and find that even a single
teacher, identical to the student, considerably improves performance beyond its
own capability, with optimal gains achieved using only a few teachers. We
achieve the new state-of-the-art Score of 64.39 on ICHBI, surpassing the
previous best by 0.85 and improving average Scores across architectures by more
than 1.16. Our results highlight the effectiveness of knowledge distillation
with soft labels for respiratory sound classification, regardless of size or
architecture.

</details>


### [324] [AudioTurbo: Fast Text-to-Audio Generation with Rectified Diffusion](https://arxiv.org/abs/2505.22106)
*Junqi Zhao,Jinzheng Zhao,Haohe Liu,Yun Chen,Lu Han,Xubo Liu,Mark Plumbley,Wenwu Wang*

Main category: cs.SD

TL;DR: AudioTurbo uses pre-trained models with rectified diffusion to enhance text-to-audio generation efficiency, reducing sampling steps and inference time.


<details>
  <summary>Details</summary>
Motivation: Diffusion models excel in audio generation quality and diversity but suffer from slow inference speed. Rectified flow improves this speed but needs retraining and performs poorly at low step counts.

Method: The study integrates pre-trained diffusion models with the rectified diffusion method, proposing AudioTurbo which learns first-order ODE paths from noise sample pairs generated by a pre-trained TTA model.

Result: Experiments on the AudioCaps dataset show that AudioTurbo outperforms previous models with only 10 sampling steps and reduces inference to 3 steps compared to flow-matching-based acceleration models.

Conclusion: AudioTurbo successfully leverages pre-trained models to improve the efficiency of text-to-audio generation while addressing the limitations of rectified flow.

Abstract: Diffusion models have significantly improved the quality and diversity of
audio generation but are hindered by slow inference speed. Rectified flow
enhances inference speed by learning straight-line ordinary differential
equation (ODE) paths. However, this approach requires training a flow-matching
model from scratch and tends to perform suboptimally, or even poorly, at low
step counts. To address the limitations of rectified flow while leveraging the
advantages of advanced pre-trained diffusion models, this study integrates
pre-trained models with the rectified diffusion method to improve the
efficiency of text-to-audio (TTA) generation. Specifically, we propose
AudioTurbo, which learns first-order ODE paths from deterministic noise sample
pairs generated by a pre-trained TTA model. Experiments on the AudioCaps
dataset demonstrate that our model, with only 10 sampling steps, outperforms
prior models and reduces inference to 3 steps compared to a flow-matching-based
acceleration model.

</details>


### [325] [Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect](https://arxiv.org/abs/2505.21809)
*Jaya Narain,Vasudha Kowtha,Colin Lea,Lauren Tooley,Dianna Yee,Vikramjit Mitra,Zifang Huang,Miquel Espi Marques,Jon Huang,Carlos Avendano,Shirley Ren*

Main category: cs.SD

TL;DR: The paper develops and evaluates voice quality models for seven voice and speech dimensions using embeddings from pre-trained models as features. The probes show strong performance, generalization, and zero-shot capabilities across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To describe key characteristics of atypical speech and other speech modulations through perceptual voice quality dimensions.

Method: Trained probes on the Speech Accessibility (SAP) project dataset with 11,184 samples from 434 speakers using embeddings from frozen pre-trained models as features.

Result: Probes had strong performance and generalization across speech elicitation categories in the SAP dataset and showed strong zero-shot performance on additional datasets including unseen languages and tasks.

Conclusion: The utility of using voice quality dimensions in speaking style-related tasks is suggested by the strong zero-shot performance and interpretability of results.

Abstract: Perceptual voice quality dimensions describe key characteristics of atypical
speech and other speech modulations. Here we develop and evaluate voice quality
models for seven voice and speech dimensions (intelligibility, imprecise
consonants, harsh voice, naturalness, monoloudness, monopitch, and
breathiness). Probes were trained on the public Speech Accessibility (SAP)
project dataset with 11,184 samples from 434 speakers, using embeddings from
frozen pre-trained models as features. We found that our probes had both strong
performance and strong generalization across speech elicitation categories in
the SAP dataset. We further validated zero-shot performance on additional
datasets, encompassing unseen languages and tasks: Italian atypical speech,
English atypical speech, and affective speech. The strong zero-shot performance
and the interpretability of results across an array of evaluations suggests the
utility of using voice quality dimensions in speaking style-related tasks.

</details>


### [326] [Effective and Efficient One-pass Compression of Speech Foundation Models Using Sparsity-aware Self-pinching Gates](https://arxiv.org/abs/2505.22608)
*Haoning Xu,Zhaoqing Li,Youjun Chen,Huimeng Wang,Guinan Li,Mengzhe Geng,Chengxi Deng,Xunying Liu*

Main category: cs.SD

TL;DR: The paper introduces a new method for compressing speech foundation models by integrating model pruning and parameter update into one stage, achieving significant parameter reduction without affecting performance.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient compression technique that reduces the number of parameters in speech foundation models while maintaining or improving their performance.

Method: Jointly train compact layer-level tied self-pinching gates with uncompressed models for fine-grained neuron level pruning, integrating model pruning and parameter update into a single stage.

Result: Reduces parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60% respectively, with no significant WER increase. Achieves lowest WER of 7.05% under comparable compression ratio, with at least 25% less compression time.

Conclusion: The proposed approach effectively compresses speech models while preserving performance, outperforming previous methods in both compression efficiency and speed.

Abstract: This paper presents a novel approach for speech foundation models compression
that tightly integrates model pruning and parameter update into a single stage.
Highly compact layer-level tied self-pinching gates each containing only a
single learnable threshold are jointly trained with uncompressed models and
used in fine-grained neuron level pruning. Experiments conducted on the
LibriSpeech-100hr corpus suggest that our approach reduces the number of
parameters of wav2vec2.0-base and HuBERT-large models by 65% and 60%
respectively, while incurring no statistically significant word error rate
(WER) increase on the test-clean dataset. Compared to previously published
methods on the same task, our approach not only achieves the lowest WER of
7.05% on the test-clean dataset under a comparable model compression ratio of
4.26x, but also operates with at least 25% less model compression time.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [327] [Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning](https://arxiv.org/abs/2505.21596)
*Esra Adiyeke,Tianqi Liu,Venkata Sai Dheeraj Naganaboina,Han Li,Tyler J. Loftus,Yuanfang Ren,Benjamin Shickel,Matthew M. Ruppert,Karandeep Singh,Ruogu Fang,Parisa Rashidi,Azra Bihorac,Tezcan Ozrazgat-Baslanti*

Main category: q-bio.QM

TL;DR: An RL model was developed using surgical data to recommend optimal IV fluid and vasopressor dosage during surgery. This model showed higher policy value than physicians' actual treatments and could potentially reduce postoperative AKI.


<details>
  <summary>Details</summary>
Motivation: Traditional surgical decision-making methods depend on human experience, which is variable. A data-driven system generating treatment recommendations based on patient states can be valuable in perioperative decision-making, especially in managing intraoperative hypotension that is associated with postoperative complications like AKI.

Method: A Reinforcement Learning (RL) model, specifically a Deep Q-Networks based model, was developed using data from 50,021 surgeries. The model used 16 variables including intraoperative physiologic time series, total dose of IV fluid and vasopressors extracted every 15 minutes to recommend optimum dosages of IV fluid and vasopressors.

Result: The model replicated 69% of physician's decisions for vasopressor dosage, proposed higher dosage in 10% and lower dosage in 21% of the cases. For IV fluids, it recommended within 0.05 ml/kg/15 min of the actual dose in 41% of the cases, higher doses in 27%, and lower doses in 32%. The model resulted in a higher estimated policy value compared to physicians' treatments and random policies. AKI prevalence was lowest when medication aligned with the model's decisions.

Conclusion: The implementation of the model's policy has the potential to reduce postoperative AKI and improve other outcomes related to intraoperative hypotension.

Abstract: Traditional methods of surgical decision making heavily rely on human
experience and prompt actions, which are variable. A data-driven system
generating treatment recommendations based on patient states can be a
substantial asset in perioperative decision-making, as in cases of
intraoperative hypotension, for which suboptimal management is associated with
acute kidney injury (AKI), a common and morbid postoperative complication. We
developed a Reinforcement Learning (RL) model to recommend optimum dose of
intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative
hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries
from 42,547 adult patients who underwent major surgery at a quaternary care
hospital between June 2014 and September 2020. Of these, 34,186 surgeries were
used for model training and 15,835 surgeries were reserved for testing. We
developed a Deep Q-Networks based RL model using 16 variables including
intraoperative physiologic time series, total dose of IV fluid and vasopressors
extracted for every 15-minute epoch. The model replicated 69% of physician's
decisions for the dosage of vasopressors and proposed higher or lower dosage of
vasopressors than received in 10% and 21% of the treatments, respectively. In
terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min
of the actual dose in 41% of the cases, with higher or lower doses recommended
for 27% and 32% of the treatments, respectively. The model resulted in a higher
estimated policy value compared to the physicians' actual treatments, as well
as random and zero-drug policies. AKI prevalence was the lowest in patients
receiving medication dosages that aligned with model's decisions. Our findings
suggest that implementation of the model's policy has the potential to reduce
postoperative AKI and improve other outcomes driven by intraoperative
hypotension.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [328] [OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models](https://arxiv.org/abs/2505.21537)
*Hao Sun,Yunyi Shen,Mihaela van der Schaar*

Main category: cs.CY

TL;DR: In the age of large language models, this paper argues for using OpenReview as a key resource to improve peer review processes, create meaningful benchmarks, and support alignment research.


<details>
  <summary>Details</summary>
Motivation: The motivation is the increasing value of high-quality, domain-rich, and continuously evolving datasets capturing expert-level knowledge, core human values, and reasoning in the era of large language models.

Method: The method involves leveraging OpenReview's repository of research papers, peer reviews, author rebuttals, meta-reviews, and decision outcomes more broadly as a core community asset. It also includes exploring standardized benchmarks and usage guidelines around OpenReview.

Result: The result would be enhancing the quality, scalability, and accountability of peer review processes; enabling meaningful, open-ended benchmarks; and supporting alignment research through real-world interactions reflecting expert assessment, intentions, and scientific values.

Conclusion: The conclusion is that the community should collaboratively explore the use of OpenReview to advance research in the era of large language models, with a focus on responsible data use, ethical considerations, and collective stewardship.

Abstract: In the era of large language models (LLMs), high-quality, domain-rich, and
continuously evolving datasets capturing expert-level knowledge, core human
values, and reasoning are increasingly valuable. This position paper argues
that OpenReview -- the continually evolving repository of research papers, peer
reviews, author rebuttals, meta-reviews, and decision outcomes -- should be
leveraged more broadly as a core community asset for advancing research in the
era of LLMs. We highlight three promising areas in which OpenReview can
uniquely contribute: enhancing the quality, scalability, and accountability of
peer review processes; enabling meaningful, open-ended benchmarks rooted in
genuine expert deliberation; and supporting alignment research through
real-world interactions reflecting expert assessment, intentions, and
scientific values. To better realize these opportunities, we suggest the
community collaboratively explore standardized benchmarks and usage guidelines
around OpenReview, inviting broader dialogue on responsible data use, ethical
considerations, and collective stewardship.

</details>


### [329] [Enhancing Selection of Climate Tech Startups with AI -- A Case Study on Integrating Human and AI Evaluations in the ClimaTech Great Global Innovation Challenge](https://arxiv.org/abs/2505.21562)
*Jennifer Turliuk,Alejandro Sevilla,Daniela Gorza,Tod Hynes*

Main category: cs.CY

TL;DR: This case study explores the ClimaTech Great Global Innovation Challenge's hybrid human-AI model for selecting climate tech startups. The process included three phases with varying human-AI score weightings, showing moderate alignment between AI and human assessments and demonstrating the value of combining both approaches.


<details>
  <summary>Details</summary>
Motivation: To evaluate how integrating AI with human evaluations can enhance the accuracy and efficiency of selecting top climate tech startups in a global innovation challenge.

Method: The selection process was divided into three phases: initial AI review (25% AI influence), semi-finals judged by humans (75% human, 25% AI influence), and finals using hybrid weighting (83.3% human, 16.7% AI influence). AI scores were based on StackAI and OpenAI's GPT-4o, while human judges evaluated team quality, market potential, and technological innovation.

Result: There was a moderate positive correlation (Spearman's = 0.47) between AI and human scores, indicating general alignment with key differences. The final four startups selected mainly by humans were among those rated highest by the AI.

Conclusion: Hybrid models combining human expertise with AI capabilities can streamline and improve startup assessments, offering a strong framework for future competitions.

Abstract: This case study examines the ClimaTech Great Global Innovation Challenge's
approach to selecting climate tech startups by integrating human and AI
evaluations. The competition aimed to identify top startups and enhance the
accuracy and efficiency of the selection process through a hybrid model.
Research shows data-driven approaches help VC firms reduce bias and improve
decision-making. Machine learning models have outperformed human investors in
deal screening, helping identify high-potential startups. Incorporating AI
aimed to ensure more equitable and objective evaluations.
  The methodology included three phases: initial AI review, semi-finals judged
by humans, and finals using a hybrid weighting. In phase one, 57 applications
were scored by an AI tool built with StackAI and OpenAI's GPT-4o, and the top
36 advanced. In the semi-finals, human judges, unaware of AI scores, evaluated
startups on team quality, market potential, and technological innovation. Each
score - human or AI - was weighted equally, resulting in 75 percent human and
25 percent AI influence. In the finals, with five human judges, weighting
shifted to 83.3 percent human and 16.7 percent AI. There was a moderate
positive correlation between AI and human scores - Spearman's = 0.47 -
indicating general alignment with key differences. Notably, the final four
startups, selected mainly by humans, were among those rated highest by the AI.
This highlights the complementary nature of AI and human judgment. The study
shows that hybrid models can streamline and improve startup assessments. The
ClimaTech approach offers a strong framework for future competitions by
combining human expertise with AI capabilities.

</details>


### [330] [Beyond Explainability: The Case for AI Validation](https://arxiv.org/abs/2505.21570)
*Dalit Ken-Dror Feldman,Daniel Benoliel*

Main category: cs.CY

TL;DR: Artificial Knowledge (AK) systems are increasingly used in critical domains but their opacity presents governance challenges. This paper argues for a shift from explainability to validation as a central regulatory pillar, proposing a forward-looking policy framework centered on pre- and post-deployment validation, third-party auditing, harmonized standards, and liability incentives.


<details>
  <summary>Details</summary>
Motivation: Current regulatory approaches focused predominantly on explainability fail to address adequately the growing opacity of Artificial Knowledge systems.

Method: The authors introduce a typology based on two axes - validity and explainability - classifying AK systems into four categories. They conduct a comparative analysis of regulatory approaches in the EU, US, UK, and China.

Result: Validation can enhance societal trust, fairness, and safety even where explainability is limited. A forward-looking policy framework centered on validation processes is proposed.

Conclusion: A governance roadmap is provided for responsibly integrating opaque, high-performing AK systems into society, balancing innovation with accountability.

Abstract: Artificial Knowledge (AK) systems are transforming decision-making across
critical domains such as healthcare, finance, and criminal justice. However,
their growing opacity presents governance challenges that current regulatory
approaches, focused predominantly on explainability, fail to address
adequately. This article argues for a shift toward validation as a central
regulatory pillar. Validation, ensuring the reliability, consistency, and
robustness of AI outputs, offers a more practical, scalable, and risk-sensitive
alternative to explainability, particularly in high-stakes contexts where
interpretability may be technically or economically unfeasible. We introduce a
typology based on two axes, validity and explainability, classifying AK systems
into four categories and exposing the trade-offs between interpretability and
output reliability. Drawing on comparative analysis of regulatory approaches in
the EU, US, UK, and China, we show how validation can enhance societal trust,
fairness, and safety even where explainability is limited. We propose a
forward-looking policy framework centered on pre- and post-deployment
validation, third-party auditing, harmonized standards, and liability
incentives. This framework balances innovation with accountability and provides
a governance roadmap for responsibly integrating opaque, high-performing AK
systems into society.

</details>


### [331] [AITEE -- Agentic Tutor for Electrical Engineering](https://arxiv.org/abs/2505.21582)
*Christopher Knievel,Alexander Bernhardt,Christian Bernhardt*

Main category: cs.CY

TL;DR: The paper introduces AITEE, an agent-based tutoring system for electrical engineering that combines circuit reconstruction, graph-based similarity measures, and Spice simulation to provide personalized learning support.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of large language models in handling specific electrical circuit questions and to provide a more effective and personalized learning experience for electrical engineering students.

Method: AITEE uses an adapted circuit reconstruction process for hand-drawn and digital circuits, a novel graph-based similarity measure for context retrieval, parallel Spice simulation for accuracy, and Socratic dialogue for guided questioning.

Result: Experimental evaluations show that AITEE significantly outperforms baseline approaches in applying domain-specific knowledge, even with medium-sized LLMs performing acceptably.

Conclusion: AITEE demonstrates the potential of agentic tutors to create scalable, personalized, and effective learning environments for electrical engineering education.

Abstract: Intelligent tutoring systems combined with large language models offer a
promising approach to address students' diverse needs and promote
self-efficacious learning. While large language models possess good
foundational knowledge of electrical engineering basics, they remain
insufficiently capable of addressing specific questions about electrical
circuits. In this paper, we present AITEE, an agent-based tutoring system for
electrical engineering designed to accompany students throughout their learning
process, offer individualized support, and promote self-directed learning.
AITEE supports both hand-drawn and digital circuits through an adapted circuit
reconstruction process, enabling natural interaction with students. Our novel
graph-based similarity measure identifies relevant context from lecture
materials through a retrieval augmented generation approach, while parallel
Spice simulation further enhances accuracy in applying solution methodologies.
The system implements a Socratic dialogue to foster learner autonomy through
guided questioning. Experimental evaluations demonstrate that AITEE
significantly outperforms baseline approaches in domain-specific knowledge
application, with even medium-sized LLM models showing acceptable performance.
Our results highlight the potential of agentic tutors to deliver scalable,
personalized, and effective learning environments for electrical engineering
education.

</details>


### [332] [Public Discourse Sandbox: Facilitating Human and AI Digital Communication Research](https://arxiv.org/abs/2505.21604)
*Kristina Radivojevic,Caleb Reinking,Shaun Whitfield,Paul Brenner*

Main category: cs.CY

TL;DR: The paper introduces Public Discourse Sandbox (PDS), a digital discourse research platform for human-AI and AI-AI discourse research, testing, and training to address the difficulties of obtaining reliable data on social media and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: Social media is an important platform for communication and information dissemination, but obtaining reliable data can be difficult due to bots, fake accounts, manipulated content, and ethical concerns. There is a need for more controlled and scalable mechanisms to evaluate the impacts of digital discussion interventions on audiences.

Method: The PDS provides a safe and secure space for research experiments that are not viable on public, commercial social media platforms. It enables understanding of AI behaviors and the impacts of customized AI participants via techniques such as prompt engineering, retrieval-augmented generation (RAG), and fine-tuning.

Result: A hosted live version of the sandbox has been provided to support researchers along with open-sourced code on GitHub for community collaboration and contribution.

Conclusion: The PDS serves as a valuable tool for digital discourse research, offering a controlled environment for evaluating the impacts of digital discussion interventions.

Abstract: Social media serves as a primary communication and information dissemination
platform for major global events, entertainment, and niche or topically focused
community discussions. Therefore, it represents a valuable resource for
researchers who aim to understand numerous questions. However, obtaining data
can be difficult, expensive, and often unreliable due to the presence of bots,
fake accounts, and manipulated content. Additionally, there are ethical
concerns if researchers decide to conduct an online experiment without
explicitly notifying social media users about their intent. There is a need for
more controlled and scalable mechanisms to evaluate the impacts of digital
discussion interventions on audiences. We introduce the Public Discourse
Sandbox (PDS), which serves as a digital discourse research platform for
human-AI as well as AI-AI discourse research, testing, and training. PDS
provides a safe and secure space for research experiments that are not viable
on public, commercial social media platforms. Its main purpose is to enable the
understanding of AI behaviors and the impacts of customized AI participants via
techniques such as prompt engineering, retrieval-augmented generation (RAG),
and fine-tuning. We provide a hosted live version of the sandbox to support
researchers as well as the open-sourced code on GitHub for community
collaboration and contribution.

</details>


### [333] [Expert Survey: AI Reliability & Security Research Priorities](https://arxiv.org/abs/2505.21664)
*Joe O'Brien,Jeremy Dolan,Jay Kim,Jonah Dykhuizen,Jeba Sania,Sebastian Becker,Jam Kraprayoon,Cara Labrador*

Main category: cs.CY

TL;DR: A survey of 53 specialists across 105 AI reliability and security research areas identifies key research prospects for strategic AI R&D investment, providing a data-driven ranking to support resource deployment.


<details>
  <summary>Details</summary>
Motivation: As companies aim to develop AI systems with human-level capabilities, there is an urgent need for research on reliability and security to safely realize AI's benefits and prevent harms.

Method: Conducted a survey among 53 specialists covering 105 AI reliability and security research areas to quantify expert priorities and produce a data-driven ranking of potential impact.

Result: The study produced the first comprehensive taxonomy and data-driven ranking of AI safety and security research directions based on expert priorities.

Conclusion: These rankings can assist in making evidence-based decisions about deploying resources towards AI reliability and security research.

Abstract: Our survey of 53 specialists across 105 AI reliability and security research
areas identifies the most promising research prospects to guide strategic AI
R&D investment. As companies are seeking to develop AI systems with broadly
human-level capabilities, research on reliability and security is urgently
needed to ensure AI's benefits can be safely and broadly realized and prevent
severe harms. This study is the first to quantify expert priorities across a
comprehensive taxonomy of AI safety and security research directions and to
produce a data-driven ranking of their potential impact. These rankings may
support evidence-based decisions about how to effectively deploy resources
toward AI reliability and security research.

</details>


### [334] [Responsible Data Stewardship: Generative AI and the Digital Waste Problem](https://arxiv.org/abs/2505.21720)
*Vanessa Utz*

Main category: cs.CY

TL;DR: 这篇论文探讨了生成式AI系统中的数字废弃物问题，并提出了相应的伦理和技术建议。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI系统的广泛应用，其在文本、图像、音频和视频等多模态数据上的合成能力不断提升，但与此同时也带来了能源消耗和数字废弃物等可持续性挑战。

Method: 通过引入已有的数字资源管理方法，分析其他学科如何处理数字废弃物，并提出适用于AI社区的可转移策略。

Result: 提出了涵盖研究方向、技术干预和文化转变的具体建议，以减轻无限期数据存储带来的环境影响。

Conclusion: 将环境可持续性纳入AI伦理框架，推动负责任的创新，考虑生成式AI系统的完整生命周期影响。

Abstract: As generative AI systems become widely adopted, they enable unprecedented
creation levels of synthetic data across text, images, audio, and video
modalities. While research has addressed the energy consumption of model
training and inference, a critical sustainability challenge remains
understudied: digital waste. This term refers to stored data that consumes
resources without serving a specific (and/or immediate) purpose. This paper
presents this terminology in the AI context and introduces digital waste as an
ethical imperative within (generative) AI development, positioning
environmental sustainability as core for responsible innovation. Drawing from
established digital resource management approaches, we examine how other
disciplines manage digital waste and identify transferable approaches for the
AI community. We propose specific recommendations encompassing re-search
directions, technical interventions, and cultural shifts to mitigate the
environmental consequences of in-definite data storage. By expanding AI ethics
beyond immediate concerns like bias and privacy to include inter-generational
environmental justice, this work contributes to a more comprehensive ethical
framework that considers the complete lifecycle impact of generative AI
systems.

</details>


### [335] [CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero](https://arxiv.org/abs/2505.21536)
*Federico Zocco,Andrea Corti,Monica Malvezzi*

Main category: cs.CY

TL;DR: The paper introduces CiRL, a deep reinforcement learning (DRL) library focused on material circularity to address climate change and critical material supply uncertainties.


<details>
  <summary>Details</summary>
Motivation: The demand for finite raw materials is increasing while solutions to stop carbon emissions in the short term are unavailable, making it challenging to achieve net zero targets. The circular economy paradigm offers a solution to these issues.

Method: CiRL integrates DRL into the design of material circularity using thermodynamical material networks formalism underpinned by compartmental dynamical thermodynamics. It features environments in state-space form, uses Stable-Baselines3, and is developed in Google Colaboratory.

Result: CiRL provides a publicly available tool to help researchers from different disciplines tackle challenges related to material circularity in the context of the circular economy.

Conclusion: CiRL is introduced as a promising tool for advancing the circular economy and addressing challenges related to climate change and critical material supplies.

Abstract: The demand of finite raw materials will keep increasing as they fuel modern
society. Simultaneously, solutions for stopping carbon emissions in the short
term are not available, thus making the net zero target extremely challenging
to achieve at scale. The circular economy (CE) paradigm is gaining attention as
a solution to address climate change and the uncertainties of supplies of
critical materials. Hence, in this paper, we introduce CiRL, a deep
reinforcement learning (DRL) library of environments focused on the circularity
of both solid and fluid materials. The integration of DRL into the design of
material circularity is possible thanks to the formalism of thermodynamical
material networks, which is underpinned by compartmental dynamical
thermodynamics. Along with the focus on circularity, this library has three
more features: the new CE-oriented environments are in the state-space form,
which is typically used in dynamical systems analysis and control designs; it
is based on a state-of-the-art Python library of DRL algorithms, namely,
Stable-Baselines3; and it is developed in Google Colaboratory to be accessible
to researchers from different disciplines and backgrounds as is often the case
for circular economy researchers and engineers. CiRL is publicly available.

</details>


### [336] [From Coders to Critics: Empowering Students through Peer Assessment in the Age of AI Copilots](https://arxiv.org/abs/2505.22093)
*Santiago Berrezueta-Guzman,Stephan Krusche,Stefan Wagner*

Main category: cs.CY

TL;DR: The paper explores using structured peer assessment in programming courses amidst the rise of AI coding assistants, finding it moderately accurate compared to instructor evaluations and beneficial for student engagement and feedback skills.


<details>
  <summary>Details</summary>
Motivation: To address challenges in programming education caused by AI-powered coding assistants, such as plagiarism concerns, and explore alternative grading methods that maintain academic integrity and promote skill development.

Method: An empirical study was conducted in a large introductory programming course where students performed rubric-based, anonymized peer reviews of each other's final projects (2D games). Assessments were compared to instructor grades using correlation, mean absolute error, and RMSE. Surveys from 47 teams captured student perceptions on fairness, grading behavior, and preferences.

Result: Peer review approximated instructor evaluation with moderate accuracy. It also enhanced student engagement, evaluative thinking, and interest in providing quality feedback.

Conclusion: Structured peer assessment is a promising strategy for scalable and trustworthy assessment systems in the era of AI-assisted coding, fostering both technical and metacognitive skills.

Abstract: The rapid adoption of AI powered coding assistants like ChatGPT and other
coding copilots is transforming programming education, raising questions about
assessment practices, academic integrity, and skill development. As educators
seek alternatives to traditional grading methods susceptible to AI enabled
plagiarism, structured peer assessment could be a promising strategy. This
paper presents an empirical study of a rubric based, anonymized peer review
process implemented in a large introductory programming course.
  Students evaluated each other's final projects (2D game), and their
assessments were compared to instructor grades using correlation, mean absolute
error, and root mean square error (RMSE). Additionally, reflective surveys from
47 teams captured student perceptions of fairness, grading behavior, and
preferences regarding grade aggregation. Results show that peer review can
approximate instructor evaluation with moderate accuracy and foster student
engagement, evaluative thinking, and interest in providing good feedback to
their peers. We discuss these findings for designing scalable, trustworthy peer
assessment systems to face the age of AI assisted coding.

</details>


### [337] [New Tools are Needed for Tracking Adherence to AI Model Behavioral Use Clauses](https://arxiv.org/abs/2505.22287)
*Daniel McDuff,Tim Korjakow,Kevin Klyman,Danish Contractor*

Main category: cs.CY

TL;DR: Foundation models have transformed AI, but concerns over misuse have led to complex licensing. This paper analyzes 300 custom licenses and 1.7 million model licenses on HuggingFace, finding increasing adoption and convergence on common clauses. Tools for tracking license adherence are urgently needed.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the growing need for effective tools to track the adoption and adherence to AI model licenses, ensuring responsible use amidst concerns over negligent or malicious uses of AI.

Method: The authors created and deployed a custom AI licenses generator, analyzed over 300 customized licenses created with this tool, and also analyzed 1.7 million model licenses on the HuggingFace model hub.

Result: The results show an increasing adoption of licenses with behavioral-use clauses and acceptable-use-policies, interest in tools that support their creation, and a convergence on common clause configurations.

Conclusion: The paper concludes that tools for tracking the adoption and adherence to these licenses are the natural next step and are urgently needed to ensure they have the desired impact of promoting responsible AI use.

Abstract: Foundation models have had a transformative impact on AI. A combination of
large investments in research and development, growing sources of digital data
for training, and architectures that scale with data and compute has led to
models with powerful capabilities. Releasing assets is fundamental to
scientific advancement and commercial enterprise. However, concerns over
negligent or malicious uses of AI have led to the design of mechanisms to limit
the risks of the technology. The result has been a proliferation of licenses
with behavioral-use clauses and acceptable-use-policies that are increasingly
being adopted by commonly used families of models (Llama, Gemma, Deepseek) and
a myriad of smaller projects. We created and deployed a custom AI licenses
generator to facilitate license creation and have quantitatively and
qualitatively analyzed over 300 customized licenses created with this tool.
Alongside this we analyzed 1.7 million models licenses on the HuggingFace model
hub. Our results show increasing adoption of these licenses, interest in tools
that support their creation and a convergence on common clause configurations.
In this paper we take the position that tools for tracking adoption of, and
adherence to, these licenses is the natural next step and urgently needed in
order to ensure they have the desired impact of ensuring responsible use.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [338] [A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models](https://arxiv.org/abs/2505.21580)
*Anum Fatima,Gesine Reinert*

Main category: stat.ML

TL;DR: The paper develops a KSD-type goodness-of-fit test for IRG models, which can be performed with a single network observation, applies to any network size, and includes theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: There is a need for fast and effective goodness-of-fit tests for complex data represented as graphs, especially in high dimensions.

Method: The authors develop a KSD-type goodness-of-fit test specifically for inhomogeneous random graph (IRG) models. This test can be conducted using a single observation of the network and does not rely on the asymptotic distribution of the test statistic.

Result: The developed test is applicable to networks of any size and provides theoretical guarantees regarding its performance.

Conclusion: The KSD-type goodness-of-fit test for IRG models offers a powerful tool for analyzing complex network data without requiring multiple observations or relying on asymptotic properties.

Abstract: Complex data are often represented as a graph, which in turn can often be
viewed as a realisation of a random graph, such as of an inhomogeneous random
graph model (IRG). For general fast goodness-of-fit tests in high dimensions,
kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop,
test, and analyse a KSD-type goodness-of-fit test for IRG models that can be
carried out with a single observation of the network. The test is applicable to
a network of any size and does not depend on the asymptotic distribution of the
test statistic. We also provide theoretical guarantees.

</details>


### [339] [STACI: Spatio-Temporal Aleatoric Conformal Inference](https://arxiv.org/abs/2505.21658)
*Brandon R. Feng,David Keetae Park,Xihaier Luo,Arantxa Urdangarin,Shinjae Yoo,Brian J. Reich*

Main category: stat.ML

TL;DR: Gaussian Processes (GPs) can quantify uncertainty in spatio-temporal fields, but they face challenges in scalability and approximation bias. Deep learning models are scalable but often fail to capture the underlying correlation structure. The proposed framework, STACI, combines a variational Bayesian neural network with a conformal inference algorithm to overcome these limitations, providing valid prediction intervals and scaling to large datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Gaussian Processes (GPs) and deep learning models in handling spatio-temporal data, such as scalability issues and inability to capture complex correlation structures.

Method: STACI uses a variational Bayesian neural network to approximate non-stationary spatio-temporal GPs and incorporates a novel spatio-temporal conformal inference algorithm. This approach leverages GPU training for scalability and ensures statistically valid uncertainty quantification.

Result: STACI outperforms existing GP and deep learning methods in accurately approximating spatio-temporal processes and scales effectively to datasets with millions of observations.

Conclusion: STACI provides a scalable and effective solution for spatio-temporal data analysis, overcoming the limitations of traditional GPs and deep learning models.

Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty
quantification for estimation of spatio-temporal fields. Spatio-temporal deep
learning models, while scalable, typically assume a simplistic independent
covariance matrix for the response, failing to capture the underlying
correlation structure. However, spatio-temporal GPs suffer from issues of
scalability and various forms of approximation bias resulting from restrictive
assumptions of the covariance kernel function. We propose STACI, a novel
framework consisting of a variational Bayesian neural network approximation of
non-stationary spatio-temporal GP along with a novel spatio-temporal conformal
inference algorithm. STACI is highly scalable, taking advantage of GPU training
capabilities for neural network models, and provides statistically valid
prediction intervals for uncertainty quantification. STACI outperforms
competing GPs and deep methods in accurately approximating spatio-temporal
processes and we show it easily scales to datasets with millions of
observations.

</details>


### [340] [Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference](https://arxiv.org/abs/2505.21721)
*Kyurae Kim,Yi-An Ma,Trevor Campbell,Jacob R. Gardner*

Main category: stat.ML

TL;DR: The paper proves that black-box variational inference (BBVI) with a mean-field location-scale variational family converges at an almost dimension-independent rate under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand the convergence rate of black-box variational inference (BBVI) with the reparametrization gradient given a mean-field location-scale variational family and how it compares to full-rank location-scale families.

Method: Prove the convergence rate of BBVI for strongly log-concave and log-smooth targets, analyze the number of iterations needed to achieve an objective close to the global optimum for sub-Gaussian and heavy-tailed families, and examine the effect of the Hessian of the target log-density on the complexity.

Result: For strongly log-concave and log-smooth targets, BBVI with a sub-Gaussian family achieves an objective ε-close to the global optimum in O(log d) iterations. For heavy-tailed families, the dependence is weaker at O(d^(2/k)). If the Hessian of the target log-density is constant, there is no explicit dimension dependence. The bound on the gradient variance cannot be improved using only spectral bounds on the Hessian of the target log-density.

Conclusion: Black-box variational inference with a mean-field location-scale variational family can converge at an almost dimension-independent rate, improving over the O(d) dependence of full-rank location-scale families.

Abstract: We prove that, given a mean-field location-scale variational family,
black-box variational inference (BBVI) with the reparametrization gradient
converges at an almost dimension-independent rate. Specifically, for strongly
log-concave and log-smooth targets, the number of iterations for BBVI with a
sub-Gaussian family to achieve an objective $\epsilon$-close to the global
optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$
dependence of full-rank location-scale families. For heavy-tailed families, we
provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the
number of finite moments. Additionally, if the Hessian of the target
log-density is constant, the complexity is free of any explicit dimension
dependence. We also prove that our bound on the gradient variance, which is key
to our result, cannot be improved using only spectral bounds on the Hessian of
the target log-density.

</details>


### [341] [Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks](https://arxiv.org/abs/2505.21791)
*Julia Nakhleh,Robert D. Nowak*

Main category: stat.ML

TL;DR: Overparameterized neural networks can interpolate data in multiple ways. This paper aims to find the sparsest ReLU network by proposing a continuous training objective that guarantees global minima as the sparsest solutions.


<details>
  <summary>Details</summary>
Motivation: The motivation of this paper is to address the challenge of finding the sparsest interpolating ReLU network, which has implications for efficiency, generalization, interpretability, theory, and model compression.

Method: The method proposed is a continuous, almost-everywhere differentiable training objective whose global minima correspond to the sparsest single-hidden-layer ReLU networks fitting the data. This involves minimizing ℓ^p quasinorms of the weights for 0 < p < 1.

Result: The result is a conceptual advance where the combinatorial problem of sparse interpolation is recast as a smooth optimization task, allowing the use of gradient-based training methods.

Conclusion: This work lays a foundation for understanding when and how continuous sparsity-inducing objectives can be used to recover sparse networks through training.

Abstract: Overparameterized neural networks can interpolate a given dataset in many
different ways, prompting the fundamental question: which among these solutions
should we prefer, and what explicit regularization strategies will provably
yield these solutions? This paper addresses the challenge of finding the
sparsest interpolating ReLU network -- i.e., the network with the fewest
nonzero parameters or neurons -- a goal with wide-ranging implications for
efficiency, generalization, interpretability, theory, and model compression.
Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere
differentiable training objective whose global minima are guaranteed to
correspond to the sparsest single-hidden-layer ReLU networks that fit the data.
This result marks a conceptual advance: it recasts the combinatorial problem of
sparse interpolation as a smooth optimization task, potentially enabling the
use of gradient-based training methods. Our objective is based on minimizing
$\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical
sparsity-promoting strategy in finite-dimensional settings. However, applying
these ideas to neural networks presents new challenges: the function class is
infinite-dimensional, and the weights are learned using a highly nonconvex
objective. We prove that, under our formulation, global minimizers correspond
exactly to sparsest solutions. Our work lays a foundation for understanding
when and how continuous sparsity-inducing objectives can be leveraged to
recover sparse networks through training.

</details>


### [342] [A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging](https://arxiv.org/abs/2505.21796)
*Sajad Khodadadian,Martin Zubeldia*

Main category: stat.ML

TL;DR: The paper develops a framework for non-asymptotic concentration bounds of averaged SA iterates, providing tight results and new bounds for contractive SA and reinforcement learning algorithms.


<details>
  <summary>Details</summary>
Motivation: Polyak-Ruppert averaging is widely used in stochastic approximation (SA) algorithms to achieve optimal asymptotic variance. However, its high-probability performance guarantees are underexplored in general settings, motivating the need for a framework to analyze non-asymptotic concentration bounds.

Method: The authors present a general framework that assumes access to individual concentration bounds for unaveraged iterates to derive sharp bounds on the averaged iterates. They construct an example to demonstrate the tightness of their result up to constant multiplicative factors.

Result: Tight concentration bounds are derived for contractive SA algorithms and for algorithms like temporal difference learning and Q-learning with averaging, offering new insights in challenging analysis settings.

Conclusion: This study provides a valuable tool for analyzing the high-probability performance of Polyak-Ruppert averaging in various SA algorithms, including reinforcement learning contexts.

Abstract: Polyak-Ruppert averaging is a widely used technique to achieve the optimal
asymptotic variance of stochastic approximation (SA) algorithms, yet its
high-probability performance guarantees remain underexplored in general
settings. In this paper, we present a general framework for establishing
non-asymptotic concentration bounds for the error of averaged SA iterates. Our
approach assumes access to individual concentration bounds for the unaveraged
iterates and yields a sharp bound on the averaged iterates. We also construct
an example, showing the tightness of our result up to constant multiplicative
factors. As direct applications, we derive tight concentration bounds for
contractive SA algorithms and for algorithms such as temporal difference
learning and Q-learning with averaging, obtaining new bounds in settings where
traditional analysis is challenging.

</details>


### [343] [Spectral clustering for dependent community Hawkes process models of temporal networks](https://arxiv.org/abs/2505.21845)
*Lingfei Zhao,Hadeel Soliman,Kevin S. Xu,Subhadeep Paul*

Main category: stat.ML

TL;DR: Temporal networks with timestamped relational events data are common in various applications. These networks often exhibit community structure and strong dependence patterns among node pairs. This paper focuses on dependent community Hawkes (DCH) models which combine stochastic block model with mutually exciting Hawkes processes to model both community structure and dependence among node pairs. The authors derive a non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix, leveraging recent results on bounding an appropriate distance between a multivariate Hawkes process count vector and a Gaussian vector. They also propose a DCH model incorporating only self and reciprocal excitation with highly scalable parameter estimation using a Generalized Method of Moments (GMM) estimator.


<details>
  <summary>Details</summary>
Motivation: To provide statistical results for modeling temporal networks that exhibit community structure and strong dependence patterns among node pairs through mutual excitations.

Method: Dependent community Hawkes (DCH) models combining stochastic block model with mutually exciting Hawkes processes are used. A non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix is derived. Additionally, a DCH model incorporating only self and reciprocal excitation along with scalable parameter estimation using GMM estimator is proposed.

Result: A non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix is successfully derived. The proposed DCH model with scalable parameter estimation using GMM estimator is consistent for growing network size and time duration.

Conclusion: The study provides statistical results for DCH models and proposes a scalable DCH model with consistent parameter estimation for modeling temporal networks with community structure and dependence among node pairs.

Abstract: Temporal networks observed continuously over time through timestamped
relational events data are commonly encountered in application settings
including online social media communications, financial transactions, and
international relations. Temporal networks often exhibit community structure
and strong dependence patterns among node pairs. This dependence can be modeled
through mutual excitations, where an interaction event from a sender to a
receiver node increases the possibility of future events among other node
pairs.
  We provide statistical results for a class of models that we call dependent
community Hawkes (DCH) models, which combine the stochastic block model with
mutually exciting Hawkes processes for modeling both community structure and
dependence among node pairs, respectively. We derive a non-asymptotic upper
bound on the misclustering error of spectral clustering on the event count
matrix as a function of the number of nodes and communities, time duration, and
the amount of dependence in the model. Our result leverages recent results on
bounding an appropriate distance between a multivariate Hawkes process count
vector and a Gaussian vector, along with results from random matrix theory. We
also propose a DCH model that incorporates only self and reciprocal excitation
along with highly scalable parameter estimation using a Generalized Method of
Moments (GMM) estimator that we demonstrate to be consistent for growing
network size and time duration.

</details>


### [344] [Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion](https://arxiv.org/abs/2505.21892)
*Xunpeng Huang,Yingyu Lin,Nikki Lijing Kuang,Hanze Dong,Difan Zou,Yian Ma,Tong Zhang*

Main category: stat.ML

TL;DR: Continuous diffusion models face efficiency limitations due to local adjacency structures and biases in reverse denoising processes. This paper proposes Quantized Transition Diffusion (QTD), which combines data quantization with discrete diffusion dynamics to address these issues, achieving state-of-the-art inference efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the efficiency limitations of continuous diffusion models by addressing the restricted long-range transitions in the data space and the inherent biases in reverse denoising processes.

Method: The method involves transforming the continuous data distribution into a discrete one using histogram approximation and binary encoding. A continuous-time Markov chain (CTMC) with Hamming distance-based transitions is designed for the forward process, enabling long-range movements in the original data space. For reverse-time sampling, a truncated uniformization technique is introduced to simulate the reverse CTMC, ensuring unbiased generation under minimal score assumptions.

Result: Through KL dynamic analysis, it is proven that QTD can generate samples with O(dln²(d/ε)) score evaluations to approximate the d-dimensional target distribution within an ε error tolerance.

Conclusion: QTD not only establishes state-of-the-art inference efficiency but also advances the theoretical foundations of diffusion-based generative modeling by unifying discrete and continuous diffusion paradigms.

Abstract: Continuous diffusion models have demonstrated remarkable performance in data
generation across various domains, yet their efficiency remains constrained by
two critical limitations: (1) the local adjacency structure of the forward
Markov process, which restricts long-range transitions in the data space, and
(2) inherent biases introduced during the simulation of time-inhomogeneous
reverse denoising processes. To address these challenges, we propose Quantized
Transition Diffusion (QTD), a novel approach that integrates data quantization
with discrete diffusion dynamics. Our method first transforms the continuous
data distribution $p_*$ into a discrete one $q_*$ via histogram approximation
and binary encoding, enabling efficient representation in a structured discrete
latent space. We then design a continuous-time Markov chain (CTMC) with Hamming
distance-based transitions as the forward process, which inherently supports
long-range movements in the original data space. For reverse-time sampling, we
introduce a \textit{truncated uniformization} technique to simulate the reverse
CTMC, which can provably provide unbiased generation from $q_*$ under minimal
score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we
prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score
evaluations in expectation to approximate the $d$--dimensional target
distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only
establishes state-of-the-art inference efficiency but also advances the
theoretical foundations of diffusion-based generative modeling by unifying
discrete and continuous diffusion paradigms.

</details>


### [345] [Higher-Order Group Synchronization](https://arxiv.org/abs/2505.21932)
*Adriana L. Duncan,Joe Kileel*

Main category: stat.ML

TL;DR: This paper introduces a novel higher-order group synchronization problem on hypergraphs, providing synchronizability conditions, a computational framework with theoretical guarantees, and demonstrating its advantages through numerical experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to extend the traditional group synchronization problem to a higher-order version on hypergraphs for better handling of complex local measurements in applications like computer vision and image processing.

Method: The method involves defining the higher-order group synchronization problem, establishing synchronizability conditions based on cycle consistency, proposing a global message passing algorithm as the computational framework, and analyzing its theoretical guarantees.

Result: The proposed method outperforms standard pairwise synchronization methods in rotational and angular synchronization tasks, shows robustness to outliers, and has comparable performance on simulated cryo-EM data.

Conclusion: Higher-order group synchronization provides a powerful tool for obtaining reliable global estimates from complex local measurements, with potential advantages in various computational problems.

Abstract: Group synchronization is the problem of determining reliable global estimates
from noisy local measurements on networks. The typical task for group
synchronization is to assign elements of a group to the nodes of a graph in a
way that respects group elements given on the edges which encode information
about local pairwise relationships between the nodes. In this paper, we
introduce a novel higher-order group synchronization problem which operates on
a hypergraph and seeks to synchronize higher-order local measurements on the
hyperedges to obtain global estimates on the nodes. Higher-order group
synchronization is motivated by applications to computer vision and image
processing, among other computational problems. First, we define the problem of
higher-order group synchronization and discuss its mathematical foundations.
Specifically, we give necessary and sufficient synchronizability conditions
which establish the importance of cycle consistency in higher-order group
synchronization. Then, we propose the first computational framework for general
higher-order group synchronization; it acts globally and directly on
higher-order measurements using a message passing algorithm. We discuss
theoretical guarantees for our framework, including convergence analyses under
outliers and noise. Finally, we show potential advantages of our method through
numerical experiments. In particular, we show that in certain cases our
higher-order method applied to rotational and angular synchronization
outperforms standard pairwise synchronization methods and is more robust to
outliers. We also show that our method has comparable performance on simulated
cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM
reconstruction package.

</details>


### [346] [Learning Curves of Stochastic Gradient Descent in Kernel Regression](https://arxiv.org/abs/2505.22048)
*Haihan Zhang,Weicheng Lin,Yuanshi Liu,Cong Fang*

Main category: stat.ML

TL;DR: 这篇论文研究了在线一阶算法训练的核回归模型与离线岭回归和无岭回归相比的表现。分析了单次通过的随机梯度下降(SGD)在核回归下的过拟合风险曲线，并表明SGD在大多数情况下能达到最优率，且不会受到饱和现象的影响。此外，还提供了指数衰减步长计划优于迭代平均方法的第一个可证明的优势。


<details>
  <summary>Details</summary>
Motivation: 研究在线一阶算法（如SGD）在核回归中的表现如何与离线算法（如岭回归和无岭回归）相比较，尤其是在模型可能被错误指定的情况下。

Method: 分析单次通过的随机梯度下降(SGD)在球面上的内积核回归问题下的性能，特别是当最优预测器不属于RKHS时。通过不同的样本规模n和输入维度d的关系来刻画过拟合风险曲线。使用指数衰减步长计划。

Result: SGD在几乎所有尺度上都达到了最小最大最优率，避免了饱和现象，除非模型高度错误指定且处于学习的最后阶段(n远大于d^γ，γ>0)。同时，指数衰减步长计划显示了其相对于迭代平均方法的优势。

Conclusion: SGD在核回归中具有优越的性能，能够克服饱和现象，且在适当的步长计划下表现出比迭代平均方法更好的效果。

Abstract: This paper considers a canonical problem in kernel regression: how good are
the model performances when it is trained by the popular online first-order
algorithms, compared to the offline ones, such as ridge and ridgeless
regression? In this paper, we analyze the foundational single-pass Stochastic
Gradient Descent (SGD) in kernel regression under source condition where the
optimal predictor can even not belong to the RKHS, i.e. the model is
misspecified. Specifically, we focus on the inner product kernel over the
sphere and characterize the exact orders of the excess risk curves under
different scales of sample sizes $n$ concerning the input dimension $d$.
Surprisingly, we show that SGD achieves min-max optimal rates up to constants
among all the scales, without suffering the saturation, a prevalent phenomenon
observed in (ridge) regression, except when the model is highly misspecified
and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant
$\gamma >0$. The main reason for SGD to overcome the curse of saturation is the
exponentially decaying step size schedule, a common practice in deep neural
network training. As a byproduct, we provide the \emph{first} provable
advantage of the scheme over the iterative averaging method in the common
setting.

</details>


### [347] [Individualised Counterfactual Examples Using Conformal Prediction Intervals](https://arxiv.org/abs/2505.22326)
*James M. Adams,Gesine Reinert,Lukasz Szpruch,Carsten Maple,Andrew Elliott*

Main category: stat.ML

TL;DR: 该论文提出了一种基于个体化置信预测区间（CPICFs）的反事实解释方法，通过建模个体知识和评估预测不确定性来选择最能提供信息的反事实示例。在合成数据集和真实数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前高维特征空间中的反事实解释方法存在多种可能性，但未充分考虑个体对模型决策的理解程度。因此需要一种能够最大化个体信息获取的反事实解释方法。

Method: 通过显式建模个体的知识，利用置信预测区间的宽度衡量个体预测的不确定性，并在不确定性高的区域寻找更具有信息量的反事实示例。使用合成数据集和真实数据集进行实验验证。

Result: 在合成数据集中，可视化展示了决策边界、置信预测区间及相应的CPICFs；在真实数据集中，通过数据增强和保留集测试，证明了该方法的有效性。

Conclusion: 提出的CPICFs方法能够有效选择最具有信息量的反事实示例，提升个体对模型决策的理解。

Abstract: Counterfactual explanations for black-box models aim to pr ovide insight into
an algorithmic decision to its recipient. For a binary classification problem
an individual counterfactual details which features might be changed for the
model to infer the opposite class. High-dimensional feature spaces that are
typical of machine learning classification models admit many possible
counterfactual examples to a decision, and so it is important to identify
additional criteria to select the most useful counterfactuals. In this paper,
we explore the idea that the counterfactuals should be maximally informative
when considering the knowledge of a specific individual about the underlying
classifier. To quantify this information gain we explicitly model the knowledge
of the individual, and assess the uncertainty of predictions which the
individual makes by the width of a conformal prediction interval. Regions of
feature space where the prediction interval is wide correspond to areas where
the confidence in decision making is low, and an additional counterfactual
example might be more informative to an individual. To explore and evaluate our
individualised conformal prediction interval counterfactuals (CPICFs), first we
present a synthetic data set on a hypercube which allows us to fully visualise
the decision boundary, conformal intervals via three different methods, and
resultant CPICFs. Second, in this synthetic data set we explore the impact of a
single CPICF on the knowledge of an individual locally around the original
query. Finally, in both our synthetic data set and a complex real world dataset
with a combination of continuous and discrete variables, we measure the utility
of these counterfactuals via data augmentation, testing the performance on a
held out set.

</details>


### [348] [Credal Prediction based on Relative Likelihood](https://arxiv.org/abs/2505.22332)
*Timo Löhr,Paul Hofman,Felix Mohr,Eyke Hüllermeier*

Main category: stat.ML

TL;DR: The paper proposes a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood, which provides a suitable means to represent a learner's epistemic uncertainty.


<details>
  <summary>Details</summary>
Motivation: Predictions in the form of sets of probability distributions (credal sets) are useful for representing a learner's epistemic uncertainty. There is a need for a method that can control the trade-off between correctness and precision of these predictions.

Method: The proposed method defines the target of prediction as the set of all (conditional) probability distributions produced by plausible models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision. The authors use suitably modified ensemble learning techniques to approximate these credal sets.

Result: The approach was validated through experiments on benchmark datasets, showing superior uncertainty representation without compromising predictive performance. It also compared favorably against several state-of-the-art baselines in credal prediction.

Conclusion: The proposed approach to credal prediction based on relative likelihood is effective in representing uncertainty while maintaining good predictive performance.

Abstract: Predictions in the form of sets of probability distributions, so-called
credal sets, provide a suitable means to represent a learner's epistemic
uncertainty. In this paper, we propose a theoretically grounded approach to
credal prediction based on the statistical notion of relative likelihood: The
target of prediction is the set of all (conditional) probability distributions
produced by the collection of plausible models, namely those models whose
relative likelihood exceeds a specified threshold. This threshold has an
intuitive interpretation and allows for controlling the trade-off between
correctness and precision of credal predictions. We tackle the problem of
approximating credal sets defined in this way by means of suitably modified
ensemble learning techniques. To validate our approach, we illustrate its
effectiveness by experiments on benchmark datasets demonstrating superior
uncertainty representation without compromising predictive performance. We also
compare our method against several state-of-the-art baselines in credal
prediction.

</details>


### [349] [Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows](https://arxiv.org/abs/2505.22364)
*Gabriele Visentin,Patrick Cheridito*

Main category: stat.ML

TL;DR: The paper introduces a new method using conditional normalizing flows to compute optimal transport maps and Wasserstein barycenters in high dimensions, showing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of calculating optimal transport maps and Wasserstein barycenters in high-dimensional spaces, which are crucial for comparing and averaging probability distributions.

Method: Uses conditional normalizing flows to approximate input distributions as invertible transformations from a common latent space, allowing direct minimization of transport cost via gradient-based methods. Extends this approach to compute Wasserstein barycenters through conditional variance minimization.

Result: Achieves accurate results in various high-dimensional tasks, outperforming previous state-of-the-art methods, especially in computing barycenters for hundreds of input distributions.

Conclusion: The proposed method provides an efficient and scalable solution for optimal transport problems and Wasserstein barycenter computation, with demonstrated success in high-dimensional scenarios.

Abstract: We present a novel method for efficiently computing optimal transport maps
and Wasserstein barycenters in high-dimensional spaces. Our approach uses
conditional normalizing flows to approximate the input distributions as
invertible pushforward transformations from a common latent space. This makes
it possible to directly solve the primal problem using gradient-based
minimization of the transport cost, unlike previous methods that rely on dual
formulations and complex adversarial optimization. We show how this approach
can be extended to compute Wasserstein barycenters by solving a conditional
variance minimization problem. A key advantage of our conditional architecture
is that it enables the computation of barycenters for hundreds of input
distributions, which was computationally infeasible with previous methods. Our
numerical experiments illustrate that our approach yields accurate results
across various high-dimensional tasks and compares favorably with previous
state-of-the-art methods.

</details>


### [350] [Hypothesis Testing in Imaging Inverse Problems](https://arxiv.org/abs/2505.22481)
*Yiming Xi,Konstantinos Zygalakis,Marcelo Pereyra*

Main category: stat.ML

TL;DR: This paper proposes a framework for semantic hypothesis testing in imaging inverse problems, using self-supervised computational imaging, vision-language models, and non-parametric hypothesis testing with e-values to achieve excellent power while controlling Type I errors.


<details>
  <summary>Details</summary>
Motivation: Modern imaging methods lack the ability to support hypothesis testing which is crucial for scientific interpretation and decision-making processes. Challenges include reconstructing an image from a single observation, dealing with mostly semantic hypotheses rather than quantitative statements about pixel values, and controlling test error probabilities due to unknown null and alternative distributions.

Method: The approach leverages concepts from self-supervised computational imaging, vision-language models, and non-parametric hypothesis testing with e-values to address the difficulties in image-based hypothesis testing.

Result: Through numerical experiments related to image-based phenotyping, the proposed framework achieves excellent power while robustly controlling Type I errors.

Conclusion: The proposed framework successfully tackles the challenges of semantic hypothesis testing in imaging inverse problems.

Abstract: This paper proposes a framework for semantic hypothesis testing tailored to
imaging inverse problems. Modern imaging methods struggle to support hypothesis
testing, a core component of the scientific method that is essential for the
rigorous interpretation of experiments and robust interfacing with
decision-making processes. There are three main reasons why image-based
hypothesis testing is challenging. First, the difficulty of using a single
observation to simultaneously reconstruct an image, formulate hypotheses, and
quantify their statistical significance. Second, the hypotheses encountered in
imaging are mostly of semantic nature, rather than quantitative statements
about pixel values. Third, it is challenging to control test error
probabilities because the null and alternative distributions are often unknown.
Our proposed approach addresses these difficulties by leveraging concepts from
self-supervised computational imaging, vision-language models, and
non-parametric hypothesis testing with e-values. We demonstrate our proposed
framework through numerical experiments related to image-based phenotyping,
where we achieve excellent power while robustly controlling Type I errors.

</details>


### [351] [IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas](https://arxiv.org/abs/2505.22518)
*Agnideep Aich,Ashit Baran Aich,Bruce Wade*

Main category: stat.ML

TL;DR: The paper presents IGNIS Network, a neural framework that estimates parameters for Archimedean copulas more accurately and universally than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation for complex Archimedean copulas (like A1 and A2 families) is challenging due to non-monotonic relationships with dependency measures and numerical instability.

Method: IGNIS Network, a unified neural framework, learns direct mapping from observable dependency measures to copula parameters. It is trained on simulated data covering five Archimedean copula families and enforces parameter constraints via theory-guided post-processing.

Result: IGNIS Network reduces estimation errors compared to Method of Moments and demonstrates practical utility through validation on real-world datasets across various domains.

Conclusion: IGNIS Network offers a transformative approach for robust and accurate dependence modeling in modern applications.

Abstract: Parameter estimation for Archimedean copulas remains a challenging problem,
particularly for the recently developed A1 and A2 families that exhibit complex
dependency structures. Traditional methods, such as the Method of Moments
(MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood
(MPL), often struggle due to issues of non-monotonic relationship with
dependency measures such as Kendall's tau (as in the case of A1) and numerical
instability. In this paper, we present the IGNIS Network, a novel, unified
neural framework that learns a direct mapping from observable dependency
measures to copula parameters, thereby overcoming the limitations of classical
approaches. Our approach is trained on simulated data spanning five Archimedean
copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its
general applicability across the entire family. Extensive simulation studies
demonstrate that the IGNIS Network reduces estimation errors compared to MoM,
while inherently enforcing parameter constraints through theory-guided
post-processing. We further validate the practical utility of our method on
diverse real-world datasets, including financial returns (AAPL-MSFT),
healthcare metrics (CDC Diabetes indicators), and environmental measurements
(PM2.5 air quality). Our results underscore the transformative potential of
neural methods for robust and accurate dependence modeling in modern
applications.

</details>


### [352] [Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling](https://arxiv.org/abs/2505.22527)
*Agnideep Aich,Ashit Aich,Bruce Wade*

Main category: stat.ML

TL;DR: The paper introduces Symplectic Generative Network (SGN), a deep generative model using Hamiltonian mechanics for invertible, volume-preserving mapping. It provides proofs, complexity analysis, universal approximation results, information-theoretic analysis, and stability analysis to highlight SGN's advantages.


<details>
  <summary>Details</summary>
Motivation: To create a deep generative model that can construct an invertible, volume-preserving mapping between latent and data spaces without the computational overhead of Jacobian determinant calculations by leveraging Hamiltonian mechanics.

Method: Introduce SGN which endows latent space with a symplectic structure and models data generation as time evolution of a Hamiltonian system. Provide theoretical framework including proofs, complexity analysis, universal approximation results, information-theoretic analysis, and stability analysis.

Result: Achieves exact likelihood evaluation without Jacobian determinant calculations. Establishes fundamental advantages of SGNs through comprehensive theoretical analyses.

Conclusion: SGNs provide a solid foundation for future empirical investigations and applications to complex, high-dimensional data.

Abstract: We introduce the Symplectic Generative Network (SGN), a deep generative model
that leverages Hamiltonian mechanics to construct an invertible,
volume-preserving mapping between a latent space and the data space. By
endowing the latent space with a symplectic structure and modeling data
generation as the time evolution of a Hamiltonian system, SGN achieves exact
likelihood evaluation without incurring the computational overhead of Jacobian
determinant calculations. In this work, we provide a rigorous mathematical
foundation for SGNs through a comprehensive theoretical framework that
includes: (i) complete proofs of invertibility and volume preservation, (ii) a
formal complexity analysis with theoretical comparisons to Variational
Autoencoders and Normalizing Flows, (iii) strengthened universal approximation
results with quantitative error bounds, (iv) an information-theoretic analysis
based on the geometry of statistical manifolds, and (v) an extensive stability
analysis with adaptive integration guarantees. These contributions highlight
the fundamental advantages of SGNs and establish a solid foundation for future
empirical investigations and applications to complex, high-dimensional data.

</details>


### [353] [Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction](https://arxiv.org/abs/2505.22554)
*Agnideep Aich,Md Monzur Murshed,Amanda Mayeaux,Sameera Hewage*

Main category: stat.ML

TL;DR: This paper presents a new feature-selection framework using the upper-tail dependence coefficient (λU) of the A2 copula for accurate diabetes risk prediction. Applied to CDC dataset, it prioritizes five predictors based on upper tail dependencies that match or outperform conventional methods across four classifiers with accuracy up to 86.5% and AUC up to 0.806, rivaling the full-feature model.


<details>
  <summary>Details</summary>
Motivation: Conventional methods like mutual information filters and genetic algorithms often overlook extreme dependencies critical for high-risk subpopulations in health datasets.

Method: The study introduces a feature-selection framework using the upper-tail dependence coefficient (λU) of the novel A2 copula, which quantifies how often extreme higher values of a predictor co-occur with diabetes diagnoses.

Result: The method prioritizes five predictors (self-reported general health, high blood pressure, body mass index, mobility limitations, and high cholesterol levels) based on upper tail dependencies. These features achieve accuracy up to 86.5% (XGBoost) and AUC up to 0.806 (Gradient Boosting), rivaling the full 21-feature model.

Conclusion: This is the first work to apply a copula's upper-tail dependence for supervised feature selection, bridging extreme-value theory and machine learning for practical diabetes prevention.

Abstract: Accurate diabetes risk prediction relies on identifying key features from
complex health datasets, but conventional methods like mutual information (MI)
filters and genetic algorithms (GAs) often overlook extreme dependencies
critical for high-risk subpopulations. In this study we introduce a
feature-selection framework using the upper-tail dependence coefficient
({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher
values of a predictor co-occur with diabetes diagnoses (target variable).
Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method
prioritizes five predictors (self-reported general health, high blood pressure,
body mass index, mobility limitations, and high cholesterol levels) based on
upper tail dependencies. These features match or outperform MI and GA selected
subsets across four classifiers (Random Forest, XGBoost, Logistic Regression,
Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to
0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation
importance confirms clinical relevance, with BMI and general health driving
accuracy. To our knowledge, this is the first work to apply a copula's
upper-tail dependence for supervised feature selection, bridging extreme-value
theory and machine learning to deliver a practical toolkit for diabetes
prevention.

</details>


### [354] [Principled Out-of-Distribution Generalization via Simplicity](https://arxiv.org/abs/2505.22622)
*Jiawei Ge,Amanda Wang,Shange Tang,Chi Jin*

Main category: stat.ML

TL;DR: 现代基础模型在图像生成中表现出显著的分布外泛化能力，本文通过研究扩散模型的组合泛化能力，提出了基于简单性的理论框架来解释这一现象，并为学习真正可泛化的简单模型提供了首个精确的样本复杂度保证。


<details>
  <summary>Details</summary>
Motivation: 尽管现代基础模型在分布外任务上表现出色，但其背后的理论原理尚不明确。因此，需要深入探讨模型如何实现超出训练数据支持的任务解决能力，特别是在图像生成中的组合泛化能力。

Method: 通过分析扩散模型在图像生成中的表现，发现符合人类期望的真正可泛化模型通常是与训练数据一致的最简单模型。基于此观察，提出了一种以简单性为核心的理论框架，使用预定义的简单性度量来量化OOD泛化能力，并研究了两种关键场景：恒定差距设置和消失差距设置。

Result: 在两种不同的设置下，研究了正则化最大似然估计器的性能，并首次为学习真正可泛化且简单的模型提供了精确的样本复杂度保证。

Conclusion: 简单性是实现OOD泛化的重要原则，提出的理论框架可以解释扩散模型等现代基础模型的泛化能力，并为未来的研究提供了新的方向。

Abstract: Modern foundation models exhibit remarkable out-of-distribution (OOD)
generalization, solving tasks far beyond the support of their training data.
However, the theoretical principles underpinning this phenomenon remain
elusive. This paper investigates this problem by examining the compositional
generalization abilities of diffusion models in image generation. Our analysis
reveals that while neural network architectures are expressive enough to
represent a wide range of models -- including many with undesirable behavior on
OOD inputs -- the true, generalizable model that aligns with human expectations
typically corresponds to the simplest among those consistent with the training
data.
  Motivated by this observation, we develop a theoretical framework for OOD
generalization via simplicity, quantified using a predefined simplicity metric.
We analyze two key regimes: (1) the constant-gap setting, where the true model
is strictly simpler than all spurious alternatives by a fixed gap, and (2) the
vanishing-gap setting, where the fixed gap is replaced by a smoothness
condition ensuring that models close in simplicity to the true model yield
similar predictions. For both regimes, we study the regularized maximum
likelihood estimator and establish the first sharp sample complexity guarantees
for learning the true, generalizable, simple model.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [355] [Securing the Software Package Supply Chain for Critical Systems](https://arxiv.org/abs/2505.22023)
*Ritwik Murali,Akash Ravi*

Main category: cs.SE

TL;DR: 软件系统已成为各行各业不可或缺的商品，确保其可靠性和弹性至关重要。本文提出通过包含权威证明共识和多方签名的许可账本增强现有交付框架，以防止攻击并允许所有利益相关者进行验证。


<details>
  <summary>Details</summary>
Motivation: 软件系统不再独立，而是由全球多个开发者设计的包集合，其可靠性直接影响各行业服务的有效运行，且新兴威胁针对软件供应链，如2020年末的SolarWinds黑客事件。

Method: 在现有交付框架中加入具有Proof of Authority共识和多方签名的许可账本，构建安全管道，让关键系统能够在不破坏现有功能的情况下接入。

Result: 提出的系统可以防止攻击，同时允许每个利益相关者进行验证，避免供应链任何环节攻击的连锁反应。

Conclusion: 通过增强现有交付框架，可以有效提高软件供应链的安全性，保护关键系统免受攻击影响。

Abstract: Software systems have grown as an indispensable commodity used across various
industries, and almost all essential services depend on them for effective
operation. The software is no longer an independent or stand-alone piece of
code written by a developer but rather a collection of packages designed by
multiple developers across the globe. Ensuring the reliability and resilience
of these systems is crucial since emerging threats target software supply
chains, as demonstrated by the widespread SolarWinds hack in late 2020. These
supply chains extend beyond patches and updates, involving distribution
networks throughout the software lifecycle. Industries like smart grids,
manufacturing, healthcare, and finance rely on interconnected software systems
and their dependencies for effective functioning. To secure software modules
and add-ons, robust distribution architectures are essential. The proposed
chapter enhances the existing delivery frameworks by including a permissioned
ledger with Proof of Authority consensus and multi-party signatures. The
proposed system aims to prevent attacks while permitting every stakeholder to
verify the same. Critical systems can interface with the secure pipeline
without disrupting existing functionalities, thus preventing the cascading
effect of an attack at any point in the supply chain.

</details>


### [356] [BPMN to Smart Contract by Business Analyst](https://arxiv.org/abs/2505.22612)
*C. G. Liu,P. Bodorik,D. Jutla*

Main category: cs.SE

TL;DR: This paper presents a methodology and tool (TABS) that allows Business Analysts to generate smart contracts from BPMN and DMN models without needing software developers, incorporating business logic and enabling contract repair/upgrade.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify the creation of smart contracts for blockchain applications by using BPMN models, abstracting flow control and empowering Business Analysts to perform this task without developer assistance.

Method: Using BPMN for process representation and DMN for decision and business logic, the authors developed a methodology and tool (TABS) to transform these models into smart contracts. This includes generating scripts representing business logic and supporting features like nested transactions and contract upgrades.

Result: The result is a system where Business Analysts can independently generate smart contracts directly from BPMN and DMN models, potentially reducing reliance on software developers and streamlining the development process.

Conclusion: The paper concludes that their methodology and TABS tool successfully enable the generation of smart contracts from BPMN and DMN models without developer intervention, advancing the automation and accessibility of smart contract creation.

Abstract: This paper addresses the challenge of creating smart contracts for
applications represented using Business Process Management and Notation (BPMN)
models. In our prior work we presented a methodology that automates the
generation of smart contracts from BPMN models. This approach abstracts the
BPMN flow control, making it independent of the underlying blockchain
infrastructure, with only the BPMN task elements requiring coding. In
subsequent research, we enhanced our approach by adding support for nested
transactions and enabling a smart contract repair and/or upgrade. To empower
Business Analysts (BAs) to generate smart contracts without relying on software
developers, we tackled the challenge of generating smart contracts from BPMN
models without assistance of a software developer. We exploit the Decision
Model and Notation (DMN) standard to represent the decisions and the business
logic of the BPMN task elements and amended our methodology for transformation
of BPMN models into smart contracts to support also the generation script to
represent the business logic represented by the DMN models. To support such
transformation, we describe how the BA documents, using the BPMN elements, the
flow of information along with the flow of execution. Thus, if the BA is
successful in representing the blockchain application requirements using BPMN
and DMN models, our methodology and the tool, called TABS, that we developed as
a proof of concept, is used to generate the smart contracts directly from those
models without developer assistance.

</details>


### [357] [Smart Contracts for SMEs and Large Companies](https://arxiv.org/abs/2505.22619)
*C. G. Liu,P. Bodorik,D. Jutla*

Main category: cs.SE

TL;DR: The paper explores a tool and methodology for automatically generating smart contracts from BPMN models, enabling collaborations across companies and reducing the need for blockchain expertise.


<details>
  <summary>Details</summary>
Motivation: To simplify the creation of smart contracts and make blockchain technology more accessible to companies with varying levels of IT capabilities.

Method: Using BPMN models to automatically generate smart contracts that support multi-step transactions and facilitate contract repairs/ upgrades.

Result: The approach allows for the generation of smart contracts by users without blockchain knowledge, supporting diverse company collaborations.

Conclusion: This method promotes the democratization of smart contract development and blockchain usage.

Abstract: Research on blockchains addresses multiple issues, with one being writing
smart contracts. In our previous research we described methodology and a tool
to generate, in automated fashion, smart contracts from BPMN models. The
generated smart contracts provide support for multi-step transactions that
facilitate repair/upgrade of smart contracts. In this paper we show how the
approach is used to support collaborations via smart contracts for companies
ranging from SMEs with little IT capabilities to companies with IT using
blockchain smart contracts. Furthermore, we also show how the approach is used
for certain applications to generate smart contracts by a BPMN modeler who does
not need any knowledge of blockchain technology or smart contract development -
thus we are hoping to facilitate democratization of smart contracts and
blockchain technology.

</details>


### [358] [RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving](https://arxiv.org/abs/2505.21577)
*Huacan Wang,Ziyi Ni,Shuo Zhang,Shuo Lu,Sen Hu,Ziyang He,Chen Hu,Jiaye Lin,Yifu Guo,Yuntao Du,Pin Lyu*

Main category: cs.SE

TL;DR: To address the challenges of leveraging GitHub repositories for complex tasks, we propose RepoMaster, an autonomous agent framework that constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components. Evaluated on benchmarks, RepoMaster significantly improves valid submissions and task-pass rates while reducing token usage.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks struggle to effectively leverage GitHub's vast collection of open-source repositories due to overwhelming information and tangled dependencies, constrained by the limited context windows of current LLMs.

Method: RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components. It progressively explores related components using exploration tools and prunes information to optimize context usage during autonomous execution.

Result: Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over the strongest baseline OpenHands. On GitTaskBench, it lifts the task-pass rate from 24.1% to 62.9% while reducing token usage by 95%.

Conclusion: RepoMaster is an effective framework for exploring and reusing GitHub repositories to solve complex tasks, significantly improving performance metrics while optimizing resource usage.

Abstract: The ultimate goal of code agents is to solve complex tasks autonomously.
Although large language models (LLMs) have made substantial progress in code
generation, real-world tasks typically demand full-fledged code repositories
rather than simple scripts. Building such repositories from scratch remains a
major challenge. Fortunately, GitHub hosts a vast, evolving collection of
open-source repositories, which developers frequently reuse as modular
components for complex tasks. Yet, existing frameworks like OpenHands and
SWE-Agent still struggle to effectively leverage these valuable resources.
Relying solely on README files provides insufficient guidance, and deeper
exploration reveals two core obstacles: overwhelming information and tangled
dependencies of repositories, both constrained by the limited context windows
of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous
agent framework designed to explore and reuse GitHub repositories for solving
complex tasks. For efficient understanding, RepoMaster constructs function-call
graphs, module-dependency graphs, and hierarchical code trees to identify
essential components, providing only identified core elements to the LLMs
rather than the entire repository. During autonomous execution, it
progressively explores related components using our exploration tools and
prunes information to optimize context usage. Evaluated on the adjusted
MLE-bench, RepoMaster achieves a 110% relative boost in valid submissions over
the strongest baseline OpenHands. On our newly released GitTaskBench,
RepoMaster lifts the task-pass rate from 24.1% to 62.9% while reducing token
usage by 95%. Our code and demonstration materials are publicly available at
https://github.com/wanghuacan/RepoMaster.

</details>


### [359] [Leveraging XP and CRISP-DM for Agile Data Science Projects](https://arxiv.org/abs/2505.21603)
*Andre Massahiro Shimaoka,Renato Cordeiro Ferreira,Alfredo Goldman*

Main category: cs.SE

TL;DR: This study explores the integration of XP and CRISP-DM in agile Data Science projects at Elo7, finding high adoption rates for both methods and demonstrating their successful combination.


<details>
  <summary>Details</summary>
Motivation: To answer how the agility of the XP method can be integrated with CRISP-DM in Data Science projects.

Method: Case study at e-commerce company Elo7, collecting data through interviews and questionnaires with a Data Science team.

Result: 86% of the team frequently or always applies CRISP-DM, 71% adopt XP practices; it's possible to combine CRISP-DM with XP in Data Science projects.

Conclusion: XP and CRISP-DM can be successfully integrated in Data Science projects, providing a structured and collaborative approach.

Abstract: This study explores the integration of eXtreme Programming (XP) and the
Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data
Science projects. We conducted a case study at the e-commerce company Elo7 to
answer the research question: How can the agility of the XP method be
integrated with CRISP-DM in Data Science projects? Data was collected through
interviews and questionnaires with a Data Science team consisting of data
scientists, ML engineers, and data product managers. The results show that 86%
of the team frequently or always applies CRISP-DM, while 71% adopt XP practices
in their projects. Furthermore, the study demonstrates that it is possible to
combine CRISP-DM with XP in Data Science projects, providing a structured and
collaborative approach. Finally, the study generated improvement
recommendations for the company.

</details>


### [360] [GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git](https://arxiv.org/abs/2505.22583)
*Tobias Lindenbauer,Egor Bogomolov,Yaroslav Zharov*

Main category: cs.SE

TL;DR: The paper introduces GitGoodBench, a new benchmark for assessing AI agents' performance on Version Control System tasks. It covers three core Git scenarios and provides three datasets. Using GPT-4o, they achieved a 21.11% solve rate on the prototyping version.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for AI agents in Software Engineering, like SWE-bench, have promoted advancements in programming capabilities but neglect important developer workflows such as Version Control System operations.

Method: The authors created GitGoodBench, which evaluates AI agent performance on VCS tasks by covering three core Git scenarios from open-source Python, Java, and Kotlin repositories. They provided three datasets: a full evaluation suite, a rapid prototyping version, and a training corpus.

Result: Using GPT-4o with custom tools, they established a baseline performance on the prototyping version of GitGoodBench, achieving an overall solve rate of 21.11%.

Conclusion: GitGoodBench is expected to be a key step towards developing comprehensive SE agents that encompass more than just programming.

Abstract: Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench,
have catalyzed progress in programming capabilities of AI agents. However, they
overlook critical developer workflows such as Version Control System (VCS)
operations. To address this issue, we present GitGoodBench, a novel benchmark
for evaluating AI agent performance on VCS tasks. GitGoodBench covers three
core Git scenarios extracted from permissive open-source Python, Java, and
Kotlin repositories. Our benchmark provides three datasets: a comprehensive
evaluation suite (900 samples), a rapid prototyping version (120 samples), and
a training corpus (17,469 samples). We establish baseline performance on the
prototyping version of our benchmark using GPT-4o equipped with custom tools,
achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a
crucial stepping stone toward truly comprehensive SE agents that go beyond mere
programming.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [361] [HelixDesign-Binder: A Scalable Production-Grade Platform for Binder Design Built on HelixFold3](https://arxiv.org/abs/2505.21873)
*Jie Gao,Jun Li,Jing Hu,Shanzhuo Zhang,Kunrui Zhu,Yueyang Huang,Xiaonan Zhang,Xiaomin Fang*

Main category: q-bio.BM

TL;DR: HelixDesign-Binder是一个基于HelixFold3的高通量平台，自动化了从骨架生成到结构评估的结合剂设计全流程，并通过百度云高性能基础设施支持大规模设计和先进评分指标。在六个蛋白质目标上的基准测试表明，该平台能可靠地生成多样且高质量的结合剂。


<details>
  <summary>Details</summary>
Motivation: 蛋白质结合剂设计在治疗、诊断和合成生物学中至关重要，但目前存在工作流分散、计算成本高和工具集成复杂等问题，阻碍了其实际应用。

Method: HelixDesign-Binder整合了结合剂设计的各个阶段，包括骨架生成、序列设计、结构评估和多维度评分，形成一个可扩展且用户友好的系统。利用百度云的高性能基础设施进行大规模设计，并采用先进的评分标准如ipTM、预测的结合自由能和界面疏水性等。

Result: 通过对六个蛋白质目标的基准测试，证明了HelixDesign-Binder能够产生多样且高质量的结合剂，部分设计在预测结合亲和力上与验证过的结合剂相匹配甚至超过它们。

Conclusion: HelixDesign-Binder通过PaddleHelix平台提供交互式网络接口，支持学术研究和工业应用中的抗体和蛋白质结合剂开发，极大地促进了相关领域的进展。

Abstract: Protein binder design is central to therapeutics, diagnostics, and synthetic
biology, yet practical deployment remains challenging due to fragmented
workflows, high computational costs, and complex tool integration. We present
HelixDesign-Binder, a production-grade, high-throughput platform built on
HelixFold3 that automates the full binder design pipeline, from backbone
generation and sequence design to structural evaluation and multi-dimensional
scoring. By unifying these stages into a scalable and user-friendly system,
HelixDesign-Binder enables efficient exploration of binder candidates with
favorable structural, energetic, and physicochemical properties. The platform
leverages Baidu Cloud's high-performance infrastructure to support large-scale
design and incorporates advanced scoring metrics, including ipTM, predicted
binding free energy, and interface hydrophobicity. Benchmarking across six
protein targets demonstrates that HelixDesign-Binder reliably produces diverse
and high-quality binders, some of which match or exceed validated designs in
predicted binding affinity. HelixDesign-Binder is accessible via an interactive
web interface in PaddleHelix platform, supporting both academic research and
industrial applications in antibody and protein binder development.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [362] [Revisiting Self-attention for Cross-domain Sequential Recommendation](https://arxiv.org/abs/2505.21811)
*Clark Mingxuan Ju,Leonardo Neves,Bhuvesh Kumar,Liam Collins,Tong Zhao,Yuwei Qiu,Qing Dou,Sohail Nizam,Sen Yang,Neil Shah*

Main category: cs.IR

TL;DR: The paper proposes AutoCDSR, a method enhancing self-attention in transformers for cross-domain sequential recommendation (CDSR). It formulates CDSR as a multi-objective problem, optimizing recommendations while minimizing cross-domain attention scores to automate knowledge transfer. The approach mitigates negative transfer and encourages complementary knowledge exchange, with minimal computational overhead. Experiments show significant improvements in Recall@10 and NDCG@10 metrics for existing models.


<details>
  <summary>Details</summary>
Motivation: Existing CDSR frameworks rely on adding domain-specific components to transformers, but overlook the potential of enhancing the core self-attention module which is naturally powerful for learning behavior correlations.

Method: Introduces Pareto-optimal self-attention and treats cross-domain learning as a multi-objective problem. Optimizes recommendation tasks while dynamically reducing cross-domain attention scores, automating knowledge transfer in CDSR. Proposes AutoCDSR+ as an enhanced variant.

Result: AutoCDSR improves Recall@10 by 9.8% for SASRec and 16.0% for Bert4Rec. It also enhances NDCG@10 by 12.0% for SASRec and 16.7% for Bert4Rec.

Conclusion: AutoCDSR is effective, easy to implement, and has minimal computational overhead. It significantly boosts performance in transformer-based recommenders without extensive hyper-parameter tuning.

Abstract: Sequential recommendation is a popular paradigm in modern recommender
systems. In particular, one challenging problem in this space is cross-domain
sequential recommendation (CDSR), which aims to predict future behaviors given
user interactions across multiple domains. Existing CDSR frameworks are mostly
built on the self-attention transformer and seek to improve by explicitly
injecting additional domain-specific components (e.g. domain-aware module
blocks). While these additional components help, we argue they overlook the
core self-attention module already present in the transformer, a naturally
powerful tool to learn correlations among behaviors. In this work, we aim to
improve the CDSR performance for simple models from a novel perspective of
enhancing the self-attention. Specifically, we introduce a Pareto-optimal
self-attention and formulate the cross-domain learning as a multi-objective
problem, where we optimize the recommendation task while dynamically minimizing
the cross-domain attention scores. Our approach automates knowledge transfer in
CDSR (dubbed as AutoCDSR) -- it not only mitigates negative transfer but also
encourages complementary knowledge exchange among auxiliary domains. Based on
the idea, we further introduce AutoCDSR+, a more performant variant with slight
additional cost. Our proposal is easy to implement and works as a plug-and-play
module that can be incorporated into existing transformer-based recommenders.
Besides flexibility, it is practical to deploy because it brings little extra
computational overheads without heavy hyper-parameter tuning. AutoCDSR on
average improves Recall@10 for SASRec and Bert4Rec by 9.8% and 16.0% and
NDCG@10 by 12.0% and 16.7%, respectively. Code is available at
https://github.com/snap-research/AutoCDSR.

</details>


### [363] [Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking](https://arxiv.org/abs/2505.21815)
*Yunyi Zhang,Ruozhen Yang,Siqi Jiao,SeongKu Kang,Jiawei Han*

Main category: cs.IR

TL;DR: SemRank is a paper retrieval framework combining LLM-guided query understanding with concept-based semantic indexing to improve retrieval accuracy.


<details>
  <summary>Details</summary>
Motivation: Dense retrieval methods struggle with fine-grained scientific concepts and LLMs lack grounding in corpus-specific knowledge, leading to unreliable content generation.

Method: Uses multi-granular scientific concepts for indexing papers and leverages an LLM at query time to identify core concepts from the corpus, enabling precise semantic matching.

Result: Experiments demonstrate that SemRank enhances various base retrievers' performance, outperforms existing LLM-based methods, and maintains high efficiency.

Conclusion: SemRank effectively addresses limitations of previous approaches by integrating LLM capabilities with concept-based indexing.

Abstract: Scientific paper retrieval is essential for supporting literature discovery
and research. While dense retrieval methods demonstrate effectiveness in
general-purpose tasks, they often fail to capture fine-grained scientific
concepts that are essential for accurate understanding of scientific queries.
Recent studies also use large language models (LLMs) for query understanding;
however, these methods often lack grounding in corpus-specific knowledge and
may generate unreliable or unfaithful content. To overcome these limitations,
we propose SemRank, an effective and efficient paper retrieval framework that
combines LLM-guided query understanding with a concept-based semantic index.
Each paper is indexed using multi-granular scientific concepts, including
general research topics and detailed key phrases. At query time, an LLM
identifies core concepts derived from the corpus to explicitly capture the
query's information need. These identified concepts enable precise semantic
matching, significantly enhancing retrieval accuracy. Experiments show that
SemRank consistently improves the performance of various base retrievers,
surpasses strong existing LLM-based baselines, and remains highly efficient.

</details>


### [364] [Xinyu AI Search: Enhanced Relevance and Comprehensive Results with Rich Answer Presentations](https://arxiv.org/abs/2505.21849)
*Bo Tang,Junyi Zhu,Chenyang Xi,Yunhang Ge,Jiahao Wu,Yuchen Feng,Yijun Niu,Wenqiang Wei,Yu Yu,Chunyu Li,Zehao Lin,Hao Wu,Ning Liao,Yebin Yang,Jiajia Wang,Zhiyu Li,Feiyu Xiong,Jingrun Chen*

Main category: cs.IR

TL;DR: Xinyu AI Search is a new system that breaks down complex queries into sub-queries for better retrieval and generation, enhances diversity through multi-source aggregation and query expansion, optimizes passage relevance, introduces fine-grained built-in citation, and innovates in result presentation. It outperforms eight existing technologies in human assessments.


<details>
  <summary>Details</summary>
Motivation: Traditional search engines struggle with complex queries while generative AI search engines have challenges in relevance, comprehensiveness, and presentation.

Method: Introduced Xinyu AI Search which uses a query-decomposition graph to break down complex queries, enhances retrieval pipeline diversity, applies filtering and re-ranking strategies, and innovates in result presentation with timeline visualization and textual-visual choreography.

Result: Xinyu AI Search excels in relevance, comprehensiveness, and insightfulness when evaluated on real-world queries and ablation studies confirm the importance of its key sub-modules.

Conclusion: This work provides the first comprehensive framework for generative AI search engines integrating retrieval, generation, and user-focused presentation.

Abstract: Traditional search engines struggle to synthesize fragmented information for
complex queries, while generative AI search engines face challenges in
relevance, comprehensiveness, and presentation. To address these limitations,
we introduce Xinyu AI Search, a novel system that incorporates a
query-decomposition graph to dynamically break down complex queries into
sub-queries, enabling stepwise retrieval and generation. Our retrieval pipeline
enhances diversity through multi-source aggregation and query expansion, while
filtering and re-ranking strategies optimize passage relevance. Additionally,
Xinyu AI Search introduces a novel approach for fine-grained, precise built-in
citation and innovates in result presentation by integrating timeline
visualization and textual-visual choreography. Evaluated on recent real-world
queries, Xinyu AI Search outperforms eight existing technologies in human
assessments, excelling in relevance, comprehensiveness, and insightfulness.
Ablation studies validate the necessity of its key sub-modules. Our work
presents the first comprehensive framework for generative AI search engines,
bridging retrieval, generation, and user-centric presentation.

</details>


### [365] [Extracting Research Instruments from Educational Literature Using LLMs](https://arxiv.org/abs/2505.21855)
*Jiseung Yoo,Curran Mahowald,Meiyu Li,Wei Ai*

Main category: cs.IR

TL;DR: The study develops an LLM-based system for extracting detailed information about research instruments in education, outperforming other methods and enhancing accessibility for researchers and leaders.


<details>
  <summary>Details</summary>
Motivation: To create a systematic way to organize research instrument information in the education field using Large Language Models (LLMs).

Method: Designing an LLM-based system with multi-step prompting and a domain-specific data schema to extract and structure details about research instruments.

Result: The system significantly outperforms other approaches in identifying instrument names and detailed information.

Conclusion: LLM-powered information extraction has great potential in educational contexts, improving accessibility and supporting decision-making in research and policy.

Abstract: Large Language Models (LLMs) are transforming information extraction from
academic literature, offering new possibilities for knowledge management. This
study presents an LLM-based system designed to extract detailed information
about research instruments used in the education field, including their names,
types, target respondents, measured constructs, and outcomes. Using multi-step
prompting and a domain-specific data schema, it generates structured outputs
optimized for educational research. Our evaluation shows that this system
significantly outperforms other approaches, particularly in identifying
instrument names and detailed information. This demonstrates the potential of
LLM-powered information extraction in educational contexts, offering a
systematic way to organize research instrument information. The ability to
aggregate such information at scale enhances accessibility for researchers and
education leaders, facilitating informed decision-making in educational
research and policy.

</details>


### [366] [Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval](https://arxiv.org/abs/2505.22238)
*A. Ploshkin,V. Tytskiy,A. Pismenny,V. Baikalov,E. Taychinov,A. Permiakov,D. Burlakov,E. Krofto,N. Savushkin*

Main category: cs.IR

TL;DR: Yambda-5B is a large-scale dataset from Yandex.Music with 4.79 billion user-item interactions, including implicit and explicit feedback, audio embeddings, and an 'is_organic' flag to differentiate organic actions from recommendation-driven events.


<details>
  <summary>Details</summary>
Motivation: To provide the research community with an industrial-scale resource for advancing recommender systems research, fostering innovation, and promoting reproducibility.

Method: The dataset includes implicit (listening events) and explicit (likes/dislikes) feedback, audio embeddings generated by a CNN, and an 'is_organic' flag. An evaluation protocol based on Global Temporal Split is introduced for benchmarking recommendation algorithms.

Result: Benchmark results are reported for standard baselines (ItemKNN, iALS) and advanced models (SANSA, SASRec) using various evaluation metrics.

Conclusion: The release of Yambda-5B aims to support rigorous benchmarking and facilitate progress in the development of machine learning algorithms for personalized music recommendations.

Abstract: We present Yambda-5B, a large-scale open dataset sourced from the
Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item
interactions from 1 million users across 9.39 million tracks. The dataset
includes two primary types of interactions: implicit feedback (listening
events) and explicit feedback (likes, dislikes, unlikes and undislikes). In
addition, we provide audio embeddings for most tracks, generated by a
convolutional neural network trained on audio spectrograms. A key
distinguishing feature of Yambda-5B is the inclusion of the is_organic flag,
which separates organic user actions from recommendation-driven events. This
distinction is critical for developing and evaluating machine learning
algorithms, as Yandex.Music relies on recommender systems to personalize track
selection for users. To support rigorous benchmarking, we introduce an
evaluation protocol based on a Global Temporal Split, allowing recommendation
algorithms to be assessed in conditions that closely mirror real-world use. We
report benchmark results for standard baselines (ItemKNN, iALS) and advanced
models (SANSA, SASRec) using a variety of evaluation metrics. By releasing
Yambda-5B to the community, we aim to provide a readily accessible,
industrial-scale resource to advance research, foster innovation, and promote
reproducible results in recommender systems.

</details>


### [367] [UDuo: Universal Dual Optimization Framework for Online Matching](https://arxiv.org/abs/2505.22243)
*Bin Li,Diwei Liu,Zehong Hu,Jia Jia*

Main category: cs.IR

TL;DR: The paper proposes UDuo, a new framework for online resource allocation that adapts to dynamic environments through innovations in user arrival representation, adaptive allocation policies, and time-series forecasting. It outperforms traditional stochastic models in efficiency and convergence.


<details>
  <summary>Details</summary>
Motivation: Existing methods for online resource allocation rely on stochastic user arrival models which are not suitable for dynamically changing environments.

Method: UDuo incorporates three key innovations: temporal user arrival representation vector, resource pacing learner with adaptive allocation policies, and online time-series forecasting approach for future user arrivals.

Result: UDuo demonstrates higher efficiency and faster convergence than traditional stochastic arrival models in real-world pricing scenarios while maintaining theoretical validity.

Conclusion: UDuo provides a more effective solution for online resource allocation under dynamic conditions compared to classical approaches.

Abstract: Online resource allocation under budget constraints critically depends on
proper modeling of user arrival dynamics. Classical approaches employ
stochastic user arrival models to derive near-optimal solutions through
fractional matching formulations of exposed users for downstream allocation
tasks. However, this is no longer a reasonable assumption when the environment
changes dynamically. In this work, We propose the Universal Dual optimization
framework UDuo, a novel paradigm that fundamentally rethinks online allocation
through three key innovations: (i) a temporal user arrival representation
vector that explicitly captures distribution shifts in user arrival patterns
and resource consumption dynamics, (ii) a resource pacing learner with adaptive
allocation policies that generalize to heterogeneous constraint scenarios, and
(iii) an online time-series forecasting approach for future user arrival
distributions that achieves asymptotically optimal solutions with constraint
feasibility guarantees in dynamic environments. Experimental results show that
UDuo achieves higher efficiency and faster convergence than the traditional
stochastic arrival model in real-world pricing while maintaining rigorous
theoretical validity for general online allocation problems.

</details>


### [368] [Pre-training for Recommendation Unlearning](https://arxiv.org/abs/2505.22649)
*Guoxuan Chen,Lianghao Xia,Chao Huang*

Main category: cs.IR

TL;DR: The paper proposes UnlearnRec, a model-agnostic pre-training paradigm that enables efficient unlearning operations in GNN-based recommender systems, providing over 10x speedup compared to retraining while preserving performance.


<details>
  <summary>Details</summary>
Motivation: Modern GNN-powered recommender systems need to handle selective forgetting of training data due to privacy concerns, preference changes, and regulatory requirements. Traditional methods for unlearning either damage graph structure or rely on assumptions that may not hold in complex GNNs.

Method: The authors propose UnlearnRec, which includes an Influence Encoder that takes unlearning requests and existing model parameters to directly produce updated parameters with minimal fine-tuning, avoiding complete retraining.

Result: Extensive evaluation on public benchmarks shows that UnlearnRec achieves exceptional unlearning effectiveness while providing more than 10x speedup compared to retraining approaches.

Conclusion: UnlearnRec addresses the limitations of traditional unlearning methods by offering an efficient, model-agnostic solution for GNN-based recommender systems, maintaining performance while enabling quick unlearning.

Abstract: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at
modeling complex user-item interactions, yet increasingly face scenarios
requiring selective forgetting of training data. Beyond user requests to remove
specific interactions due to privacy concerns or preference changes, regulatory
frameworks mandate recommender systems' ability to eliminate the influence of
certain user data from models. This recommendation unlearning challenge
presents unique difficulties as removing connections within interaction graphs
creates ripple effects throughout the model, potentially impacting
recommendations for numerous users. Traditional approaches suffer from
significant drawbacks: fragmentation methods damage graph structure and
diminish performance, while influence function techniques make assumptions that
may not hold in complex GNNs, particularly with self-supervised or random
architectures. To address these limitations, we propose a novel model-agnostic
pre-training paradigm UnlearnRec that prepares systems for efficient unlearning
operations. Our Influence Encoder takes unlearning requests together with
existing model parameters and directly produces updated parameters of unlearned
model with little fine-tuning, avoiding complete retraining while preserving
model performance characteristics. Extensive evaluation on public benchmarks
demonstrates that our method delivers exceptional unlearning effectiveness
while providing more than 10x speedup compared to retraining approaches. We
release our method implementation at: https://github.com/HKUDS/UnlearnRec.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [369] [CSI-Bench: A Large-Scale In-the-Wild Dataset for Multi-task WiFi Sensing](https://arxiv.org/abs/2505.21866)
*Guozhen Zhu,Yuqian Hu,Weihang Gao,Wei-Hsiang Wang,Beibei Wang,K. J. Ray Liu*

Main category: eess.SP

TL;DR: The paper introduces CSI-Bench, a large-scale WiFi sensing dataset collected in real-world environments to address generalization challenges in human activity monitoring. It includes task-specific datasets and co-labeled data for multi-task learning, providing standardized evaluation splits and baseline results.


<details>
  <summary>Details</summary>
Motivation: Existing WiFi sensing systems lack generalization in real-world settings due to controlled environment data collection with homogeneous hardware and fragmented recordings.

Method: CSI-Bench is a large-scale dataset collected using commercial WiFi edge devices across diverse indoor environments with real users. It spans over 461 hours of data and includes task-specific datasets for fall detection, breathing monitoring, localization, motion source recognition, and a co-labeled multitask dataset.

Result: CSI-Bench captures realistic signal variability under natural conditions and provides standardized evaluation splits and baseline results for both single-task and multi-task learning, supporting robust model development.

Conclusion: CSI-Bench offers a foundation for developing scalable, privacy-preserving WiFi sensing systems applicable in health and broader human-centric applications.

Abstract: WiFi sensing has emerged as a compelling contactless modality for human
activity monitoring by capturing fine-grained variations in Channel State
Information (CSI). Its ability to operate continuously and non-intrusively
while preserving user privacy makes it particularly suitable for health
monitoring. However, existing WiFi sensing systems struggle to generalize in
real-world settings, largely due to datasets collected in controlled
environments with homogeneous hardware and fragmented, session-based recordings
that fail to reflect continuous daily activity.
  We present CSI-Bench, a large-scale, in-the-wild benchmark dataset collected
using commercial WiFi edge devices across 26 diverse indoor environments with
35 real users. Spanning over 461 hours of effective data, CSI-Bench captures
realistic signal variability under natural conditions. It includes
task-specific datasets for fall detection, breathing monitoring, localization,
and motion source recognition, as well as a co-labeled multitask dataset with
joint annotations for user identity, activity, and proximity. To support the
development of robust and generalizable models, CSI-Bench provides standardized
evaluation splits and baseline results for both single-task and multi-task
learning. CSI-Bench offers a foundation for scalable, privacy-preserving WiFi
sensing systems in health and broader human-centric applications.

</details>


### [370] [Empowering Intelligent Low-altitude Economy with Large AI Model Deployment](https://arxiv.org/abs/2505.22343)
*Zhonghao Lyu,Yulan Gao,Junting Chen,Hongyang Du,Jie Xu,Kaibin Huang,Dong In Kim*

Main category: eess.SP

TL;DR: Low-altitude economy (LAE) is an emerging paradigm that can be enhanced by large artificial intelligence models (LAIMs). However, deploying LAIMs in LAE has challenges such as resource limitations and inefficiencies of traditional designs. This paper proposes a hierarchical system architecture for LAIM deployment, explores enabling techniques for co-evolution of LAIMs and low-altitude systems, presents a task-oriented execution pipeline, validates the framework through case studies, and outlines open challenges.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of deploying large artificial intelligence models in the low-altitude economy, including computational/storage demands versus limited onboard resources, mismatch between lab-trained models and dynamic environments, and inefficiencies of traditional decoupled designs.

Method: Propose a hierarchical system architecture tailored for LAIM deployment, present representative LAE application scenarios, explore key enabling techniques for mutual co-evolution, introduce a task-oriented execution pipeline for scalable and adaptive service delivery, validate the proposed framework via real-world case studies.

Result: The proposed framework was validated through real-world case studies, demonstrating its effectiveness in facilitating the integration of LAIMs into low-altitude systems.

Conclusion: The paper concludes by outlining open challenges to inspire future research on integrating LAIMs into the low-altitude economy.

Abstract: Low-altitude economy (LAE) represents an emerging economic paradigm that
redefines commercial and social aerial activities. Large artificial
intelligence models (LAIMs) offer transformative potential to further enhance
the intelligence of LAE services. However, deploying LAIMs in LAE poses several
challenges, including the significant gap between their computational/storage
demands and the limited onboard resources of LAE entities, the mismatch between
lab-trained LAIMs and dynamic physical environments, and the inefficiencies of
traditional decoupled designs for sensing, communication, and computation. To
address these issues, we first propose a hierarchical system architecture
tailored for LAIM deployment and present representative LAE application
scenarios. Next, we explore key enabling techniques that facilitate the mutual
co-evolution of LAIMs and low-altitude systems, and introduce a task-oriented
execution pipeline for scalable and adaptive service delivery. Then, the
proposed framework is validated through real-world case studies. Finally, we
outline open challenges to inspire future research.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [371] [Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models](https://arxiv.org/abs/2505.21507)
*Aurore Bussalb,François Le Gac,Guillaume Jubien,Mohamed Rahmouni,Ruggero G. Bettinardi,Pedro Marinho R. de Oliveira,Phillipe Derambure,Nicolas Gaspard,Jacques Jonas,Louis Maillard,Laurent Vercueil,Hervé Vespignani,Philippe Laval,Laurent Koessler,Ulysse Gimenez*

Main category: q-bio.NC

TL;DR: The paper compares two deep learning models (CNN-LSTM and Transformer-based) with BioSerenity-E1 in classifying EEG recordings as normal or abnormal. BioSerenity-E1 finetuned performs the best.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of different deep learning models, including pre-trained models, in automatically classifying EEG recordings as normal or abnormal to assist physicians in diagnosis.

Method: Three models were compared: CNN-LSTM, Transformer-based, and BioSerenity-E1. They were trained or fine-tuned on 2,500 EEG recordings and evaluated on three datasets: dataset A (n=4,480), dataset B (n=198), and TUAB evaluation dataset (n=276).

Result: On dataset A, all models achieved at least 86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest (89.19%). On dataset B, BioSerenity-E1 finetuned also performed the best with 94.63% balanced accuracy. On the TUAB evaluation dataset, BioSerenity-E1 finetuned reached an accuracy of 82.25%, outperforming the other models.

Conclusion: Pre-trained models like BioSerenity-E1 are useful for automatic EEG classification, providing robust and efficient interpretation of EEG data with fewer resources and broader applicability.

Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis
of numerous neurological disorders. Due to the large volume of EEGs requiring
interpretation and the specific expertise involved, artificial
intelligence-based tools are being developed to assist in their visual
analysis. In this paper, we compare two deep learning models (CNN-LSTM and
Transformer-based) with BioSerenity-E1, a recently proposed foundation model,
in the task of classifying entire EEG recordings as normal or abnormal. The
three models were trained or finetuned on 2,500 EEG recordings and their
performances were evaluated on two private and one public datasets: a large
multicenter dataset annotated by a single specialist (dataset A composed of n =
4,480 recordings), a small multicenter dataset annotated by three specialists
(dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus
evaluation dataset (n = 276). On dataset A, the three models achieved at least
86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest
balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also
achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced
accuracy. The models were then validated on TUAB evaluation dataset, whose
corresponding training set was not used during training, where they achieved at
least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the
other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results
highlight the usefulness of leveraging pre-trained models for automatic EEG
classification: enabling robust and efficient interpretation of EEG data with
fewer resources and broader applicability.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [372] [Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences](https://arxiv.org/abs/2505.22008)
*Jing-An Sun,Hang Fan,Junchao Gong,Ben Fei,Kun Chen,Fenghua Ling,Wenlong Zhang,Wanghan Xu,Li Yan,Pierre Gentine,Lei Bai*

Main category: physics.ao-ph

TL;DR: An abstract about a new method called Align-DA which formulates data assimilation as a generative process and uses reward signals to guide background priors.


<details>
  <summary>Details</summary>
Motivation: Data assimilation (DA) is fundamentally ill-posed in atmospheric applications due to the sparsity of observations relative to the high-dimensional state space. Traditional methods address this challenge by simplifying background priors, which are empirical and require continual tuning for application.

Method: The paper proposes Align-DA, which formulates DA as a generative process and uses reward signals to guide background priors. A score-based model is trained in the latent space to approximate the background-conditioned prior, and align it using three complementary reward signals: assimilation accuracy, forecast skill initialized from the assimilated state, and physical adherence of the analysis fields.

Result: Experiments with multiple reward signals demonstrate consistent improvements in analysis quality across different evaluation metrics and observation-guidance strategies.

Conclusion: Preference alignment, implemented as a soft constraint, can automatically adapt complex background priors tailored to DA, offering a promising new direction for advancing the field.

Abstract: Data assimilation (DA) aims to estimate the full state of a dynamical system
by combining partial and noisy observations with a prior model forecast,
commonly referred to as the background. In atmospheric applications, this
problem is fundamentally ill-posed due to the sparsity of observations relative
to the high-dimensional state space. Traditional methods address this challenge
by simplifying background priors to regularize the solution, which are
empirical and require continual tuning for application. Inspired by alignment
techniques in text-to-image diffusion models, we propose Align-DA, which
formulates DA as a generative process and uses reward signals to guide
background priors, replacing manual tuning with data-driven alignment.
Specifically, we train a score-based model in the latent space to approximate
the background-conditioned prior, and align it using three complementary reward
signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from
the assimilated state, and (3) physical adherence of the analysis fields.
Experiments with multiple reward signals demonstrate consistent improvements in
analysis quality across different evaluation metrics and observation-guidance
strategies. These results show that preference alignment, implemented as a soft
constraint, can automatically adapt complex background priors tailored to DA,
offering a promising new direction for advancing the field.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [373] [Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?](https://arxiv.org/abs/2505.21548)
*Dhruv Agarwal,Anya Shukla,Sunayana Sitaram,Aditya Vashistha*

Main category: physics.soc-ph

TL;DR: 尽管印度本地语言模型（Indic LLMs）被期望更符合当地文化，但研究表明这些模型并未比全球模型更好地反映印度的文化价值观和实践。研究指出，构建真正具有文化代表性的大型语言模型需要更多高质量、未翻译且基于文化的训练数据。


<details>
  <summary>Details</summary>
Motivation: 评估区域化大型语言模型是否不仅能够使用用户的语言交流，还能反映其文化价值和实践。以印度为案例，比较印度本地与全球大型语言模型在文化和实践上的契合度。

Method: 通过两个维度——价值观（利用Inglehart-Welzel地图和GlobalOpinionQA）和实践（利用CulturalBench和NormAd），对五个印度本地及五个全球大型语言模型进行评估。

Result: 印度本地模型在反映印度文化规范方面并不比全球模型更好。实际上，普通美国人的文化价值观比任何印度本地模型更能代表印度文化。提示策略也未能显著改善一致性。区域微调不会增强文化能力，甚至可能妨碍现有知识的回忆。

Conclusion: 研究呼吁加大对文化代表性数据的投资，以建立和评估真正主权的大型语言模型，并将文化评估定位为与多语言基准同等重要的要求。

Abstract: Large language models (LLMs) are used around the world but exhibit Western
cultural tendencies. To address this cultural misalignment, many countries have
begun developing "regional" LLMs tailored to local communities. Yet it remains
unclear whether these models merely speak the language of their users or also
reflect their cultural values and practices. Using India as a case study, we
evaluate five Indic and five global LLMs along two key dimensions: values (via
the Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench
and NormAd). Across all four tasks, we find that Indic models do not align more
closely with Indian cultural norms than global models. In fact, an average
American person is a better proxy for Indian cultural values than any Indic
model. Even prompting strategies fail to meaningfully improve alignment.
Ablations show that regional fine-tuning does not enhance cultural competence
and may in fact hurt it by impeding recall of existing knowledge. We trace this
failure to the scarcity of high-quality, untranslated, and culturally grounded
pretraining and fine-tuning data. Our study positions cultural evaluation as a
first-class requirement alongside multilingual benchmarks and offers a reusable
methodology for developers. We call for deeper investments in culturally
representative data to build and evaluate truly sovereign LLMs.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [374] [Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI](https://arxiv.org/abs/2505.20344)
*Karen Ardila,Aashka Mohite,Abdoljalil Addeh,Amanda V. Tyndall,Cindy K. Barha,Quan Long,M. Ethan MacDonald*

Main category: q-bio.GN

TL;DR: This paper explores the genetic factors contributing to differences in brain aging between males and females using MRI and genotyping data from UK Biobank participants. It conducts sex-stratified GWAS studies on Brain Age Gap Estimates (BrainAGE) and identifies distinct and shared gene sets associated with accelerated brain aging, pointing towards personalized interventions for age-related cognitive decline.


<details>
  <summary>Details</summary>
Motivation: To investigate the genetic factors underlying the differences in brain aging trajectories between males and females, which remain underexplored despite known disparities.

Method: The study used structural MRI and genotyping data from 40,940 UK Biobank participants aged 45-83. BrainAGE was computed for total brain, hippocampal, and ventricular volumes. Sex-stratified GWAS and Post-GWAS analyses were performed to identify genetic variants linked to accelerated brain aging.

Result: Distinct gene sets were identified for each sex: neurotransmitter transport and mitochondrial stress response genes in females; immune and inflammation-related genes in males. Shared genes like GMNC and OSTN were linked to brain volumes across sexes. Tissue expression analyses revealed sex-specific pathways tied to neurodegeneration.

Conclusion: Sex-stratified approaches are crucial in aging research. The findings suggest potential genetic targets for personalized interventions against age-related cognitive decline.

Abstract: Brain aging trajectories differ between males and females, yet the genetic
factors underlying these differences remain underexplored. Using structural MRI
and genotyping data from 40,940 UK Biobank participants (aged 45-83), we
computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and
ventricular volumes. We conducted sex-stratified genome-wide association
studies (GWAS) and Post-GWAS analyses to identify genetic variants associated
with accelerated brain aging. Distinct gene sets emerged by sex: in females,
neurotransmitter transport and mitochondrial stress response genes were
implicated; in males, immune and inflammation-related genes dominated. Shared
genes, including GMNC and OSTN, were consistently linked to brain volumes
across sexes, suggesting core roles in neurostructural maintenance. Tissue
expression analyses revealed sex-specific enrichment in pathways tied to
neurodegeneration. These findings highlight the importance of sex-stratified
approaches in aging research and suggest genetic targets for personalized
interventions against age-related cognitive decline.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [375] [Offset Unlearning for Large Language Models](https://arxiv.org/abs/2404.11045)
*James Y. Huang,Wenxuan Zhou,Fei Wang,Fred Morstatter,Sheng Zhang,Hoifung Poon,Muhao Chen*

Main category: cs.CL

TL;DR: 提出了一种名为δ-Unlearning的偏移遗忘框架，用于解决黑盒大语言模型（LLMs）中的敏感信息遗忘问题。该方法通过学习较小模型对的logit偏移量来实现遗忘，无需访问模型内部权重，同时避免了保留敏感数据的问题。实验表明，该方法在有效遗忘目标数据的同时，还能保持甚至提升一般任务上的性能，并且能够结合不同的遗忘算法，为适应现有各种遗忘算法到黑盒LLMs提供了一个通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然具备强大的知识获取能力，但其对训练语料库中敏感信息的记忆（如受版权保护、存在偏差或私密的内容）引发了伦理和法律问题。已有遗忘技术要么无法适用于黑盒LLMs（因为需要访问模型内部权重），要么违反数据保护原则（通过保留敏感数据进行推理时校正）。

Method: 提出了δ-Unlearning，一种针对黑盒LLMs的偏移遗忘框架。该方法不直接调整黑盒LLM本身，而是通过对比一对较小模型的logits，学习所需的logit偏移量以实现遗忘。

Result: 实验表明，δ-Unlearning能够在有效遗忘目标数据的同时，维持甚至增强在一般非遗忘范围任务上的性能。此外，该方法可以有效整合不同的遗忘算法，成为一个多功能的解决方案。

Conclusion: δ-Unlearning为黑盒LLMs提供了一种可行的遗忘方法，解决了现有技术的局限性，同时增强了模型的通用性能并支持多种遗忘算法的集成。

Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire
knowledge from their training corpora, the memorization of sensitive
information in the corpora such as copyrighted, biased, and private content has
led to ethical and legal concerns. In response to these challenges, unlearning
has emerged as a potential remedy for LLMs affected by problematic training
data. However, previous unlearning techniques are either not applicable to
black-box LLMs due to required access to model internal weights, or violate
data protection principles by retaining sensitive data for inference-time
correction. We propose {\delta}-Unlearning, an offset unlearning framework for
black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning
learns the logit offset needed for unlearning by contrasting the logits from a
pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can
effectively unlearn target data while maintaining similar or even stronger
performance on general out-of-forget-scope tasks. {\delta}-Unlearning also
effectively incorporates different unlearning algorithms, making our approach a
versatile solution to adapting various existing unlearning algorithms to
black-box LLMs.

</details>


### [376] [Jailbreak Distillation: Renewable Safety Benchmarking](https://arxiv.org/abs/2505.22037)
*Jingyu Zhang,Ahmed Elgohary,Xiawei Wang,A S M Iftekhar,Ahmed Magooda,Benjamin Van Durme,Daniel Khashabi,Kyle Jackson*

Main category: cs.CL

TL;DR: Jailbreak Distillation (JBDistill) is a new framework that turns jailbreak attacks into high-quality safety benchmarks for large language models, ensuring fair comparisons and reproducibility.


<details>
  <summary>Details</summary>
Motivation: There is an urgent need for robust safety benchmarking as large language models are quickly being used in critical applications.

Method: JBDistill uses development models and existing jailbreak attack algorithms to create a candidate prompt pool. Then, it applies prompt selection algorithms to choose an effective subset of prompts as safety benchmarks.

Result: Experiments show that JBDistill's benchmarks generalize well to 13 diverse evaluation models not involved in benchmark construction, outperforming existing benchmarks in effectiveness while keeping high separability and diversity.

Conclusion: JBDistill provides an effective, sustainable, and adaptable solution for improving safety evaluation of large language models.

Abstract: Large language models (LLMs) are rapidly deployed in critical applications,
raising urgent needs for robust safety benchmarking. We propose Jailbreak
Distillation (JBDistill), a novel benchmark construction framework that
"distills" jailbreak attacks into high-quality and easily-updatable safety
benchmarks. JBDistill utilizes a small set of development models and existing
jailbreak attack algorithms to create a candidate prompt pool, then employs
prompt selection algorithms to identify an effective subset of prompts as
safety benchmarks. JBDistill addresses challenges in existing safety
evaluation: the use of consistent evaluation prompts across models ensures fair
comparisons and reproducibility. It requires minimal human effort to rerun the
JBDistill pipeline and produce updated benchmarks, alleviating concerns on
saturation and contamination. Extensive experiments demonstrate our benchmarks
generalize robustly to 13 diverse evaluation models held out from benchmark
construction, including proprietary, specialized, and newer-generation LLMs,
significantly outperforming existing safety benchmarks in effectiveness while
maintaining high separability and diversity. Our framework thus provides an
effective, sustainable, and adaptable solution for streamlining safety
evaluation.

</details>


### [377] [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523)
*Chengzhi Liu,Zhongxing Xu,Qingyue Wei,Juncheng Wu,James Zou,Xin Eric Wang,Yuyin Zhou,Sheng Liu*

Main category: cs.CL

TL;DR: 在测试时，计算能力使得多模态大语言模型能够生成扩展推理链，从而在多模态数学推理等任务上表现出色。然而，这种增强的推理能力往往伴随着更多的幻觉问题。为研究这一现象，本文提出了RH-AUC指标和RH-Bench基准测试工具，发现更大模型通常能在推理和感知之间取得更好的平衡，而这种平衡更多地受到训练数据类型和领域的影响，而非数据总体量。


<details>
  <summary>Details</summary>
Motivation: 尽管长推理链提升了多模态大语言模型的任务表现，但同时也带来了幻觉问题，即模型逐渐偏离基于图像的内容，更依赖于语言先验知识。因此需要系统研究推理长度与幻觉之间的关系。

Method: 提出RH-AUC指标来量化模型推理长度对其感知准确性的影响，并开发了RH-Bench诊断基准以评估多种多模态任务中推理能力和幻觉之间的权衡。通过分析不同规模模型的表现，探讨训练数据对模型推理与感知平衡的影响。

Result: (i) 较大规模模型通常能更好地平衡推理和感知；(ii) 这种平衡受训练数据类型和领域的影响比受数据总体量的影响更大。

Conclusion: 评价框架应同时考虑推理质量和感知保真度，以全面评估多模态大语言模型的性能。

Abstract: Test-time compute has empowered multimodal large language models to generate
extended reasoning chains, yielding strong performance on tasks such as
multimodal math reasoning. However, this improved reasoning ability often comes
with increased hallucination: as generations become longer, models tend to
drift away from image-grounded content and rely more heavily on language
priors. Attention analysis shows that longer reasoning chains lead to reduced
focus on visual inputs, which contributes to hallucination. To systematically
study this phenomenon, we introduce RH-AUC, a metric that quantifies how a
model's perception accuracy changes with reasoning length, allowing us to
evaluate whether the model preserves visual grounding during reasoning. We also
release RH-Bench, a diagnostic benchmark that spans a variety of multimodal
tasks, designed to assess the trade-off between reasoning ability and
hallucination. Our analysis reveals that (i) larger models typically achieve a
better balance between reasoning and perception, and (ii) this balance is
influenced more by the types and domains of training data than by its overall
volume. These findings underscore the importance of evaluation frameworks that
jointly consider both reasoning quality and perceptual fidelity.

</details>


### [378] [R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing](https://arxiv.org/abs/2505.21600)
*Tianyu Fu,Yi Ge,Yichen You,Enshu Liu,Zhihang Yuan,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）推理能力强但推理开销大，小型语言模型（SLMs）效率高但性能差。研究发现，只有少量的关键token导致LLMs和SLMs的推理路径不同。基于此，提出了Roads to Rome (R2R) 方法，通过神经网络token路由选择性地使用LLMs处理关键token，而将大多数token生成交给SLM。实验结果表明，R2R在保持高性能的同时显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 尽管蒸馏得到的小型语言模型(SLMs)大大提高了效率，但其性能由于无法跟随大型语言模型(LLMs)的推理路径而受到影响。然而，研究发现只有少量的token真正导致了LLMs和SLMs之间的推理路径差异。

Method: 提出了一种名为Roads to Rome (R2R) 的神经token路由方法。该方法能够识别并仅利用LLMs处理这些对推理路径有重大影响的关键token，而将大部分token生成任务留给SLMs。同时开发了一个自动数据生成管道，用于识别不同的token并生成token级别的路由标签以训练轻量级路由器。

Result: R2R结合了DeepSeek系列的R1-1.5B和R1-32B模型，在数学、编程和问答等具有挑战性的基准测试中进行了评估。平均激活参数规模为5.6B时，R2R的准确率比R1-7B高出1.6倍，甚至超过了R1-14B模型的表现。与R1-32B相比，R2R提供了2.8倍的实际运行速度提升，同时保持了相当的性能。

Conclusion: R2R方法通过选择性地利用LLMs处理关键token，显著提高了推理效率，同时保持了高性能，推动了测试时间扩展效率的帕累托前沿。

Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the
cost of substantial inference overhead, posing substantial deployment
challenges. Although distilled Small Language Models (SLMs) significantly
enhance efficiency, their performance suffers as they fail to follow LLMs'
reasoning paths. Luckily, we reveal that only a small fraction of tokens
genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens
are either identical or exhibit neutral differences, such as minor variations
in abbreviations or expressions. Leveraging this insight, we introduce **Roads
to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs
only for these critical, path-divergent tokens, while leaving the majority of
token generation to the SLM. We also develop an automatic data generation
pipeline that identifies divergent tokens and generates token-level routing
labels to train the lightweight router. We apply R2R to combine R1-1.5B and
R1-32B models from the DeepSeek family, and evaluate on challenging math,
coding, and QA benchmarks. With an average activated parameter size of 5.6B,
R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the
R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with
comparable performance, advancing the Pareto frontier of test-time scaling
efficiency. Our code is available at https://github.com/thu-nics/R2R.

</details>


### [379] [How does Misinformation Affect Large Language Model Behaviors and Preferences?](https://arxiv.org/abs/2505.21608)
*Miao Peng,Nuo Chen,Jianheng Tang,Jia Li*

Main category: cs.CL

TL;DR: This paper presents MisBench, a large benchmark for evaluating LLMs' behavior towards misinformation. It contains over 10 million pieces of misinformation and reveals LLMs' vulnerabilities to knowledge conflicts and stylistic variations. A new approach called Reconstruct to Discriminate (RtD) is proposed to improve LLMs' ability to detect misinformation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fine-grained analysis on how misinformation influences LLMs, providing a comprehensive benchmark to evaluate their behavior and knowledge preference towards misinformation.

Method: Developed MisBench, a benchmark with 10,346,712 pieces of misinformation considering both knowledge-based conflicts and stylistic variations. Proposed the RtD approach to strengthen LLMs' misinformation detection abilities.

Result: Empirical results show that LLMs are still susceptible to knowledge conflicts and stylistic variations in misinformation despite having comparable abilities in discerning misinformation.

Conclusion: MisBench provides valuable insights into LLMs' interactions with misinformation and can serve as an effective benchmark for evaluating and enhancing LLM-based detectors.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities in
knowledge-intensive tasks, while they remain vulnerable when encountering
misinformation. Existing studies have explored the role of LLMs in combating
misinformation, but there is still a lack of fine-grained analysis on the
specific aspects and extent to which LLMs are influenced by misinformation. To
bridge this gap, we present MisBench, the current largest and most
comprehensive benchmark for evaluating LLMs' behavior and knowledge preference
toward misinformation. MisBench consists of 10,346,712 pieces of
misinformation, which uniquely considers both knowledge-based conflicts and
stylistic variations in misinformation. Empirical results reveal that while
LLMs demonstrate comparable abilities in discerning misinformation, they still
remain susceptible to knowledge conflicts and stylistic variations. Based on
these findings, we further propose a novel approach called Reconstruct to
Discriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our
study provides valuable insights into LLMs' interactions with misinformation,
and we believe MisBench can serve as an effective benchmark for evaluating
LLM-based detectors and enhancing their reliability in real-world applications.
Codes and data are available at https://github.com/GKNL/MisBench.

</details>


### [380] [Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations](https://arxiv.org/abs/2505.21657)
*Zeinab Dehghani,Koorosh Aslansefat,Adil Khan,Mohammed Naveed Akram*

Main category: cs.CL

TL;DR: SMILE is a new method that explains how large language models respond to different parts of a prompt by slightly changing the input, measuring output changes, and highlighting impactful words, bringing us closer to transparent and trustworthy AI.


<details>
  <summary>Details</summary>
Motivation: Large language models are powerful but lack transparency, making it hard to understand their decision-making process which can be problematic in fields requiring trust and accountability.

Method: SMILE works by perturbing the input, measuring the resulting output changes, and creating visual heat maps to highlight the most impactful words in a prompt.

Result: SMILE was tested on several leading LLMs and evaluated using metrics like accuracy, consistency, stability, and fidelity, showing it provides clear and reliable explanations.

Conclusion: By making large language models easier to understand, SMILE contributes towards more transparent and trustworthy AI.

Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly
powerful at generating text, but they are still black boxes, so it is hard to
understand how they decide what to say. That lack of transparency can be
problematic, especially in fields where trust and accountability matter. To
help with this, we introduce SMILE, a new method that explains how these models
respond to different parts of a prompt. SMILE is model-agnostic and works by
slightly changing the input, measuring how the output changes, and then
highlighting which words had the most impact. Create simple visual heat maps
showing which parts of a prompt matter the most. We tested SMILE on several
leading LLMs and used metrics such as accuracy, consistency, stability, and
fidelity to show that it gives clear and reliable explanations. By making these
models easier to understand, SMILE brings us one step closer to making AI more
transparent and trustworthy.

</details>


### [381] [Rethinking the Outlier Distribution in Large Language Models: An In-depth Study](https://arxiv.org/abs/2505.21670)
*Rahul Raman,Khushi Sharma,Sai Qian Zhang*

Main category: cs.CL

TL;DR: Investigating outliers in LLMs is crucial for quantization and compression. The paper explores formation mechanisms of two common types of outliers (massive activations and channel-wise) and proposes efficient strategies to mitigate them with minimal accuracy impact.


<details>
  <summary>Details</summary>
Motivation: Outliers in large language models significantly affect performance, especially during quantization and compression processes. They cause considerable quantization errors that degrade model performance. Hence, identifying and addressing these outliers can enhance the efficiency and accuracy of quantization, making it more suitable for deployment on edge devices or specialized hardware.

Method: The paper conducts a comprehensive investigation into the formation mechanisms of two common types of outliers in LLMs: massive activations and channel-wise outliers. It then proposes potential strategies to reduce their occurrence.

Result: The proposed approaches successfully eliminate most massive activations and channel-wise outliers while having minimal impact on the model's accuracy.

Conclusion: This study provides insights into the root causes of outliers in LLMs and demonstrates effective methods to mitigate their effects, improving quantization efficiency and model performance.

Abstract: Investigating outliers in large language models (LLMs) is crucial due to
their significant impact on various aspects of LLM performance, including
quantization and compression. Outliers often cause considerable quantization
errors, leading to degraded model performance. Identifying and addressing these
outliers can enhance the accuracy and efficiency of the quantization process,
enabling smoother deployment on edge devices or specialized hardware. Recent
studies have identified two common types of outliers in LLMs: massive
activations and channel-wise outliers. While numerous quantization algorithms
have been proposed to mitigate their effects and maintain satisfactory
accuracy, few have thoroughly explored the root causes of these outliers in
depth. In this paper, we conduct a comprehensive investigation into the
formation mechanisms of these outliers and propose potential strategies to
mitigate their occurrence. Ultimately, we introduce some efficient approaches
to eliminate most massive activations and channel-wise outliers with minimal
impact on accuracy.

</details>


### [382] [LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model](https://arxiv.org/abs/2505.21689)
*Avijit Gayen,Somyajit Chakraborty,Mainak Sen,Soham Paul,Angshuman Jana*

Main category: cs.CL

TL;DR: The paper introduces LLMPR, a framework that automates petition ranking in the legal system using transfer learning and machine learning techniques. It achieves high accuracy with Random Forest and Decision Tree models.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and delays in manual prioritization of legal petitions, especially within the Indian judiciary.

Method: Utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency, leveraging the ILDC dataset and embedding techniques such as DistilBERT, LegalBERT, and MiniLM.

Result: Random Forest and Decision Tree models yield superior performance with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Numerical features alone achieve nearly optimal ranking results (R2 = 0.988, ρ = 0.998), while LLM-based embeddings offer only marginal gains.

Conclusion: Automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization.

Abstract: The persistent accumulation of unresolved legal cases, especially within the
Indian judiciary, significantly hampers the timely delivery of justice. Manual
methods of prioritizing petitions are often prone to inefficiencies and
subjective biases further exacerbating delays. To address this issue, we
propose LLMPR (Large Language Model-based Petition Ranking), an automated
framework that utilizes transfer learning and machine learning to assign
priority rankings to legal petitions based on their contextual urgency.
Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process
unstructured legal text and extract features through various embedding
techniques, including DistilBERT, LegalBERT, and MiniLM. These textual
embeddings are combined with quantitative indicators such as gap days, rank
scores, and word counts to train multiple machine learning models, including
Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments
demonstrate that Random Forest and Decision Tree models yield superior
performance, with accuracy exceeding 99% and a Spearman rank correlation of
0.99. Notably, models using only numerical features achieve nearly optimal
ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer
only marginal gains. These findings suggest that automated petition ranking can
effectively streamline judicial workflows, reduce case backlog, and improve
fairness in legal prioritization.

</details>


### [383] [Counterfactual Simulatability of LLM Explanations for Generation Tasks](https://arxiv.org/abs/2505.21740)
*Marvin Limpijankit,Yanda Chen,Melanie Subbiah,Nicholas Deas,Kathleen McKeown*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) are unpredictable, and their ability to explain behavior is crucial. Counterfactual simulatability evaluates how well explanations allow users to infer model outputs on related counterfactuals. This paper provides a framework extending this method to generation tasks like news summarization and medical suggestion.


<details>
  <summary>Details</summary>
Motivation: LLMs can be unpredictable, and even slight alterations to the prompt can cause the output to change in unexpected ways. Therefore, it is essential for models to accurately explain their behavior, especially in high-stakes settings.

Method: The authors provide a general framework for extending the method of counterfactual simulatability to generation tasks. They use news summarization and medical suggestion as example use cases.

Result: In the summarization setting, LLM explanations enable users to better predict LLM outputs on counterfactuals. However, there is significant room for improvement in medical suggestion. The evaluation for counterfactual simulatability may be more appropriate for skill-based tasks rather than knowledge-based tasks.

Conclusion: Counterfactual simulatability is an effective approach for evaluating explanations in certain generation tasks, but further improvements are needed, particularly in knowledge-based tasks like medical suggestion.

Abstract: LLMs can be unpredictable, as even slight alterations to the prompt can cause
the output to change in unexpected ways. Thus, the ability of models to
accurately explain their behavior is critical, especially in high-stakes
settings. One approach for evaluating explanations is counterfactual
simulatability, how well an explanation allows users to infer the model's
output on related counterfactuals. Counterfactual simulatability has been
previously studied for yes/no question answering tasks. We provide a general
framework for extending this method to generation tasks, using news
summarization and medical suggestion as example use cases. We find that while
LLM explanations do enable users to better predict LLM outputs on
counterfactuals in the summarization setting, there is significant room for
improvement for medical suggestion. Furthermore, our results suggest that the
evaluation for counterfactual simulatability may be more appropriate for
skill-based tasks as opposed to knowledge-based tasks.

</details>


### [384] [VeriTrail: Closed-Domain Hallucination Detection with Traceability](https://arxiv.org/abs/2505.21786)
*Dasha Metropolitansky,Jonathan Larson*

Main category: cs.CL

TL;DR: 在多步骤生成过程(MGS)中，闭域幻觉现象比单步骤生成过程(SGS)更为严重。为了检测最终输出中的幻觉并追踪幻觉内容的来源，本文提出了VeriTrail方法，并构建了包含中间输出和人工标注的数据集，实验表明VeriTrail在两个数据集上均优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 语言模型即使被指示遵循源材料，仍可能生成无根据的内容（闭域幻觉），且在多步骤生成过程中此风险更高。因此需要一种方法来检测幻觉并追踪幻觉内容的引入位置。

Method: 提出VeriTrail方法用于闭域幻觉检测，该方法为MGS和SGS过程提供可追溯性；同时构建了新的数据集，包含所有中间输出及人工标注的最终输出忠实度。

Result: VeriTrail方法在新构建的数据集上表现出色，优于基线方法。

Conclusion: VeriTrail是首个为MGS和SGS过程提供可追溯性的闭域幻觉检测方法，其效果已在新数据集上得到验证。

Abstract: Even when instructed to adhere to source material, Language Models often
generate unsubstantiated content - a phenomenon known as "closed-domain
hallucination." This risk is amplified in processes with multiple generative
steps (MGS), compared to processes with a single generative step (SGS).
However, due to the greater complexity of MGS processes, we argue that
detecting hallucinations in their final outputs is necessary but not
sufficient: it is equally important to trace where hallucinated content was
likely introduced and how faithful content may have been derived from the
source through intermediate outputs. To address this need, we present
VeriTrail, the first closed-domain hallucination detection method designed to
provide traceability for both MGS and SGS processes. We also introduce the
first datasets to include all intermediate outputs as well as human annotations
of final outputs' faithfulness for their respective MGS processes. We
demonstrate that VeriTrail outperforms baseline methods on both datasets.

</details>


### [385] [Evaluating the Retrieval Robustness of Large Language Models](https://arxiv.org/abs/2505.21870)
*Shuyang Cao,Karthik Radhakrishnan,David Rosenberg,Steven Lu,Pengxiang Cheng,Lu Wang,Shiyue Zhang*

Main category: cs.CL

TL;DR: Retrieval-augmented generation (RAG) enhances LLMs' ability to solve knowledge-intensive tasks, but imperfect retrieval and limited leveraging of retrieved content can lead to performance degradation. This study evaluates the retrieval robustness of 11 LLMs using a benchmark of 1500 open-domain questions with retrieved Wikipedia documents. Three research questions are addressed through three robustness metrics. Comprehensive experiments reveal that all LLMs show high retrieval robustness, yet varying degrees of imperfect robustness prevent them from fully utilizing RAG benefits.


<details>
  <summary>Details</summary>
Motivation: To understand the robustness of large language models in practical retrieval-augmented generation setups and how different factors such as number of retrieved documents and document orders impact their performance.

Method: A benchmark of 1500 open-domain questions was established, each associated with retrieved Wikipedia documents. Experiments were conducted with 11 LLMs and 3 prompting strategies, evaluating retrieval robustness through three specific metrics corresponding to three research questions.

Result: All tested LLMs exhibited surprisingly high retrieval robustness, but there were varying degrees of imperfect robustness affecting their ability to fully utilize the advantages of RAG.

Conclusion: LLMs generally have high retrieval robustness, but improvements are needed to fully leverage the benefits of RAG.

Abstract: Retrieval-augmented generation (RAG) generally enhances large language
models' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also
lead to performance degradation due to imperfect retrieval and the model's
limited ability to leverage retrieved content. In this work, we evaluate the
robustness of LLMs in practical RAG setups (henceforth retrieval robustness).
We focus on three research questions: (1) whether RAG is always better than
non-RAG; (2) whether more retrieved documents always lead to better
performance; (3) and whether document orders impact results. To facilitate this
study, we establish a benchmark of 1500 open-domain questions, each with
retrieved documents from Wikipedia. We introduce three robustness metrics, each
corresponds to one research question. Our comprehensive experiments, involving
11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit
surprisingly high retrieval robustness; nonetheless, different degrees of
imperfect robustness hinders them from fully utilizing the benefits of RAG.

</details>


### [386] [Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development](https://arxiv.org/abs/2505.21898)
*Rennai Qiu,Chen Qian,Ran Li,Yufan Dang,Weize Chen,Cheng Yang,Yingli Zhang,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: The paper introduces Co-Saving, a resource-aware multi-agent system that improves efficiency and solution quality in complex tasks through 'shortcuts' learned from successful trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing standalone agents and Multi-Agent Systems (MAS) face limitations such as high token consumption and excessive execution time when handling complex tasks.

Method: Proposes Co-Saving, a multi-agent system that leverages experiential knowledge via 'shortcuts', which are instructional transitions learned from historically successful trajectories, to bypass redundant reasoning agents and expedite problem-solving.

Result: Experiments on software development tasks show significant advantages over existing methods, with a 50.85% reduction in token usage and a 10.06% improvement in code quality compared to state-of-the-art MAS ChatDev.

Conclusion: Co-Saving demonstrates the potential of resource-aware multi-agent systems in enhancing operational efficiency and solution quality for complex tasks.

Abstract: Recent advancements in Large Language Models (LLMs) and autonomous agents
have demonstrated remarkable capabilities across various domains. However,
standalone agents frequently encounter limitations when handling complex tasks
that demand extensive interactions and substantial computational resources.
Although Multi-Agent Systems (MAS) alleviate some of these limitations through
collaborative mechanisms like task decomposition, iterative communication, and
role specialization, they typically remain resource-unaware, incurring
significant inefficiencies due to high token consumption and excessive
execution time. To address these limitations, we propose a resource-aware
multi-agent system -- Co-Saving (meaning that multiple agents collaboratively
engage in resource-saving activities), which leverages experiential knowledge
to enhance operational efficiency and solution quality. Our key innovation is
the introduction of "shortcuts" -- instructional transitions learned from
historically successful trajectories -- which allows to bypass redundant
reasoning agents and expedite the collective problem-solving process.
Experiments for software development tasks demonstrate significant advantages
over existing methods. Specifically, compared to the state-of-the-art MAS
ChatDev, our method achieves an average reduction of 50.85% in token usage, and
improves the overall code quality by 10.06%.

</details>


### [387] [Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning](https://arxiv.org/abs/2505.21926)
*Yin Hua,Zhiqiang Liu,Mingyang Chen,Zheng Fang,Chi Man Wong,Lingxiao Li,Chi Man Vong,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: MERRY is a foundation model for knowledge graph reasoning that integrates structural and textual information, outperforming existing baselines in both in-KG and out-of-KG tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current foundation models for knowledge graphs which primarily focus on structural aspects and in-KG tasks, hindering progress in more challenging out-of-KG tasks.

Method: Propose MERRY with a multi-perspective Conditional Message Passing (CMP) encoding architecture to integrate textual and structural information, a dynamic residual fusion module to retain relevant textual information, and a flexible edge scoring mechanism for diverse downstream tasks.

Result: Comprehensive evaluations on 28 datasets show MERRY outperforms existing baselines in most scenarios for both in-KG reasoning tasks and out-of-KG tasks like KGQA.

Conclusion: MERRY demonstrates strong reasoning capabilities within KGs and excellent generalization to out-of-KG tasks.

Abstract: In natural language processing (NLP) and computer vision (CV), the successful
application of foundation models across diverse tasks has demonstrated their
remarkable potential. However, despite the rich structural and textual
information embedded in knowledge graphs (KGs), existing research of foundation
model for KG has primarily focused on their structural aspects, with most
efforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This
limitation has hindered progress in addressing more challenging out-of-KG
tasks. In this paper, we introduce MERRY, a foundation model for general
knowledge graph reasoning, and investigate its performance across two task
categories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG
question answering, KGQA). We not only utilize the structural information, but
also the textual information in KGs. Specifically, we propose a
multi-perspective Conditional Message Passing (CMP) encoding architecture to
bridge the gap between textual and structural modalities, enabling their
seamless integration. Additionally, we introduce a dynamic residual fusion
module to selectively retain relevant textual information and a flexible edge
scoring mechanism to adapt to diverse downstream tasks. Comprehensive
evaluations on 28 datasets demonstrate that MERRY outperforms existing
baselines in most scenarios, showcasing strong reasoning capabilities within
KGs and excellent generalization to out-of-KG tasks such as KGQA.

</details>


### [388] [LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents](https://arxiv.org/abs/2505.21963)
*Taro Yano,Yoichi Ishibashi,Masafumi Oyamada*

Main category: cs.CL

TL;DR: LaMDAgent is a framework that uses LLM-based agents to autonomously construct and optimize full post-training pipelines for LLMs, improving tool-use accuracy by 9.0 points with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of automated construction of complete post-training pipelines for LLMs, which typically rely on manual design or focus narrowly on optimizing individual components.

Method: Introduction of LaMDAgent, a novel framework that autonomously constructs and optimizes full post-training pipelines through the use of LLM-based agents, exploring diverse model generation techniques, datasets, and hyperparameter configurations while leveraging task-based feedback.

Result: LaMDAgent improves tool-use accuracy by 9.0 points while preserving instruction-following capabilities and uncovers effective post-training strategies often overlooked by conventional methods.

Conclusion: LaMDAgent effectively constructs high-performing post-training pipelines with minimal human intervention; scaling data size enables cost-effective pipeline discovery while model size scaling introduces new challenges.

Abstract: Large Language Models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. To further tailor LLMs to specific domains or
applications, post-training techniques such as Supervised Fine-Tuning (SFT),
Preference Learning, and model merging are commonly employed. While each of
these methods has been extensively studied in isolation, the automated
construction of complete post-training pipelines remains an underexplored area.
Existing approaches typically rely on manual design or focus narrowly on
optimizing individual components, such as data ordering or merging strategies.
In this work, we introduce LaMDAgent (short for Language Model Developing
Agent), a novel framework that autonomously constructs and optimizes full
post-training pipelines through the use of LLM-based agents. LaMDAgent
systematically explores diverse model generation techniques, datasets, and
hyperparameter configurations, leveraging task-based feedback to discover
high-performing pipelines with minimal human intervention. Our experiments show
that LaMDAgent improves tool-use accuracy by 9.0 points while preserving
instruction-following capabilities. Moreover, it uncovers effective
post-training strategies that are often overlooked by conventional human-driven
exploration. We further analyze the impact of data and model size scaling to
reduce computational costs on the exploration, finding that model size scalings
introduces new challenges, whereas scaling data size enables cost-effective
pipeline discovery.

</details>


### [389] [Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance](https://arxiv.org/abs/2505.22003)
*Jatin Gupta,Akhil Sharma,Saransh Singhania,Ali Imam Abidi*

Main category: cs.CL

TL;DR: The paper introduces Legal Assist AI, a transformer-based model designed to offer effective legal assistance in India by leveraging large language models. It retrieves relevant legal information and generates accurate responses, aiding diverse users such as legal professionals, scholars, and the general public. Fine-tuned on extensive datasets from the Indian legal domain, it demonstrated remarkable efficiency and specialization in legal Question-Answering, outperforming state-of-the-art models in legal reasoning and accuracy.


<details>
  <summary>Details</summary>
Motivation: To bridge the critical gap of accessible legal assistance in India, where many citizens struggle to leverage their legal rights due to limited awareness and access to relevant legal information.

Method: The system retrieves relevant legal information from a curated database and generates accurate responses using a transformer-based model fine-tuned on extensive datasets from the Indian legal domain.

Result: Evaluated against state-of-the-art models, Legal Assist AI achieved a 60.08% score on the AIBE, outperforming competitors in legal reasoning and accuracy while avoiding common issues like hallucinations.

Conclusion: Legal Assist AI showcases high reliability and applicability in real-world legal scenarios, with future iterations aiming to enhance performance and expand its dataset.

Abstract: Pursuit of accessible legal assistance in India faces a critical gap, as many
citizens struggle to leverage their legal rights due to limited awareness and
access to relevant legal information. This paper introduces Legal Assist AI, a
transformer-based model designed to bridge this gap by offering effective legal
assistance through large language models (LLMs). The system retrieves relevant
legal information from a curated database and generates accurate responses,
enabling effective assistance for diverse users, including legal professionals,
scholars, and the general public. The model was fine-tuned on extensive
datasets from the Indian legal domain, including Indian Constitution, Bharatiya
Nyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,
providing a robust understanding of the complexities of Indian law. By
incorporating domain-specific legal datasets, the proposed model demonstrated
remarkable efficiency and specialization in legal Question-Answering. The model
was evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral
7B, achieving a 60.08% score on the AIBE, outperforming its competitors in
legal reasoning and accuracy. Unlike other models, Legal Assist AI avoided
common issues such as hallucinations, making it highly reliable for practical
legal applications. It showcases the model's applicability in real-world legal
scenarios, with future iterations aiming to enhance performance and expand its
dataset to cover a broader range of multilingual and case-specific queries as
well.

</details>


### [390] [VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning](https://arxiv.org/abs/2505.22019)
*Qiuchen Wang,Ruixue Ding,Yu Zeng,Zehui Chen,Lin Chen,Shihang Wang,Pengjun Xie,Fei Huang,Feng Zhao*

Main category: cs.CL

TL;DR: Effectively retrieving, reasoning and understanding visually rich information remains a challenge for RAG methods. The paper introduces VRAG-RL, a novel RL framework tailored for complex reasoning across visually rich information.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based methods cannot handle visual-related information and current vision-based RAG approaches are often limited by fixed pipelines and struggle to reason effectively due to insufficient activation of model capabilities.

Method: VRAG-RL is a novel RL framework where VLMs interact with search engines, autonomously sampling single-turn or multi-turn reasoning trajectories with the help of visual perception tokens and undergoing continual optimization based on these samples. It defines an action space tailored for visually rich inputs, with actions including cropping and scaling, allowing the model to gather information from a coarse-to-fine perspective. It also employs a simple yet effective reward that integrates query rewriting and retrieval performance with a model-based reward.

Result: VRAG-RL optimizes VLMs for RAG tasks using specially designed RL strategies, aligning the model with real-world applications.

Conclusion: The conclusion is not explicitly stated in the abstract.

Abstract: Effectively retrieving, reasoning and understanding visually rich information
remains a challenge for RAG methods. Traditional text-based methods cannot
handle visual-related information. On the other hand, current vision-based RAG
approaches are often limited by fixed pipelines and frequently struggle to
reason effectively due to the insufficient activation of the fundamental
capabilities of models. As RL has been proven to be beneficial for model
reasoning, we introduce VRAG-RL, a novel RL framework tailored for complex
reasoning across visually rich information. With this framework, VLMs interact
with search engines, autonomously sampling single-turn or multi-turn reasoning
trajectories with the help of visual perception tokens and undergoing continual
optimization based on these samples. Our approach highlights key limitations of
RL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely
incorporate images into the context, leading to insufficient reasoning token
allocation and neglecting visual-specific perception; and (ii) When models
interact with search engines, their queries often fail to retrieve relevant
information due to the inability to articulate requirements, thereby leading to
suboptimal performance. To address these challenges, we define an action space
tailored for visually rich inputs, with actions including cropping and scaling,
allowing the model to gather information from a coarse-to-fine perspective.
Furthermore, to bridge the gap between users' original inquiries and the
retriever, we employ a simple yet effective reward that integrates query
rewriting and retrieval performance with a model-based reward. Our VRAG-RL
optimizes VLMs for RAG tasks using specially designed RL strategies, aligning
the model with real-world applications. The code is available at
\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}.

</details>


### [391] [Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO](https://arxiv.org/abs/2505.22068)
*Ran Li,Shimin Di,Yuchen Liu,Chen Jing,Yu Qiu,Lei Chen*

Main category: cs.CL

TL;DR: Previous study shows that powerful LLMs trained with RLVR only refines reasoning path without improving the reasoning capacity in math tasks while SFT with distillation can. From the view of SciIE, we argue that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way. We propose two-stage training with MimicSFT and R$^2$GRPO. Experiments show that both methods can improve the reasoning capacity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the reasoning capacity of LLMs in SciIE tasks, where previous methods underperforms small Bert-based models.

Method: The method is a two-stage training approach. The first stage is MimicSFT, which uses structured reasoning templates without needing high-quality chain-of-thought data. The second stage is R$^2$GRPO with relevance and rule-induced rewards.

Result: Experiments on scientific IE benchmarks show that both methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses baseline LLMs and specialized supervised models in relation extraction.

Conclusion: The conclusion is that both SFT and RLVR can refine the reasoning path and improve reasoning capacity in a simple way based on SciIE.

Abstract: Previous study suggest that powerful Large Language Models (LLMs) trained
with Reinforcement Learning with Verifiable Rewards (RLVR) only refines
reasoning path without improving the reasoning capacity in math tasks while
supervised-finetuning(SFT) with distillation can. We study this from the view
of Scientific information extraction (SciIE) where LLMs and reasoning LLMs
underperforms small Bert-based models. SciIE require both the reasoning and
memorization. We argue that both SFT and RLVR can refine the reasoning path and
improve reasoning capacity in a simple way based on SciIE. We propose two-stage
training with 1. MimicSFT, using structured reasoning templates without needing
high-quality chain-of-thought data, 2. R$^2$GRPO with relevance and
rule-induced rewards. Experiments on scientific IE benchmarks show that both
methods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses
baseline LLMs and specialized supervised models in relation extraction. Our
code is available at https://github.com/ranlislz/R2GRPO.

</details>


### [392] [Knowledge Base Construction for Knowledge-Augmented Text-to-SQL](https://arxiv.org/abs/2505.22096)
*Jinheon Baek,Horst Samulowitz,Oktie Hassanzadeh,Dharmashankar Subramanian,Sola Shirai,Alfio Gliozzo,Debarun Bhattacharjya*

Main category: cs.CL

TL;DR: The paper proposes constructing a comprehensive knowledge base for text-to-SQL translation that leverages Large Language Models (LLMs) and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of LLMs in generating accurate SQL queries due to insufficient parametric knowledge covering diverse and domain-specific queries.

Method: Propose constructing a comprehensive knowledge base for text-to-SQL, combining available questions, associated database schemas, and relevant knowledge. This knowledge base can be reused for unseen databases from different datasets and domains.

Result: Validated on multiple text-to-SQL datasets, considering both overlapping and non-overlapping database scenarios, and substantially outperforms relevant baselines.

Conclusion: Constructing a knowledge base for text-to-SQL improves the accuracy of generated SQL queries and can be effectively reused across different datasets and domains.

Abstract: Text-to-SQL aims to translate natural language queries into SQL statements,
which is practical as it enables anyone to easily retrieve the desired
information from databases. Recently, many existing approaches tackle this
problem with Large Language Models (LLMs), leveraging their strong capability
in understanding user queries and generating corresponding SQL code. Yet, the
parametric knowledge in LLMs might be limited to covering all the diverse and
domain-specific queries that require grounding in various database schemas,
which makes generated SQLs less accurate oftentimes. To tackle this, we propose
constructing the knowledge base for text-to-SQL, a foundational source of
knowledge, from which we retrieve and generate the necessary knowledge for
given queries. In particular, unlike existing approaches that either manually
annotate knowledge or generate only a few pieces of knowledge for each query,
our knowledge base is comprehensive, which is constructed based on a
combination of all the available questions and their associated database
schemas along with their relevant knowledge, and can be reused for unseen
databases from different datasets and domains. We validate our approach on
multiple text-to-SQL datasets, considering both the overlapping and
non-overlapping database scenarios, where it outperforms relevant baselines
substantially.

</details>


### [393] [Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model](https://arxiv.org/abs/2505.22116)
*Jintao Zhang,Zirui Liu,Mingyue Cheng,Shilong Zhang,Tingyue Pan,Qi Liu,Yanhu Xie*

Main category: cs.CL

TL;DR: The paper introduces IOHFuseLM, a multimodal language model for predicting intraoperative hypotension (IOH). It uses a two-stage training strategy involving domain adaptive pretraining and task fine-tuning, aligns clinical descriptions with physiological time series, and converts static patient attributes into structured text. Experiments show it outperforms baselines in identifying IOH events.


<details>
  <summary>Details</summary>
Motivation: Intraoperative hypotension (IOH) is common under general anesthesia and linked to adverse outcomes. However, predicting IOH is difficult due to event sparsity and the challenge of integrating static and dynamic data across patients.

Method: The proposed IOHFuseLM framework employs a two-stage training strategy: 1) Domain adaptive pretraining on IOH physiological time series augmented through diffusion methods to enhance sensitivity to hypotension patterns; 2) Task fine-tuning on the original clinical dataset to improve the ability to distinguish normotensive from hypotensive states. Additionally, it aligns structured clinical descriptions with physiological time series at the token level and converts static patient attributes into structured text.

Result: Experimental evaluations on two intraoperative datasets demonstrate that IOHFuseLM outperforms established baselines in accurately identifying IOH events.

Conclusion: IOHFuseLM effectively predicts IOH events by leveraging multimodal data fusion and a two-stage training strategy, making it applicable for clinical decision support.

Abstract: Intraoperative hypotension (IOH) frequently occurs under general anesthesia
and is strongly linked to adverse outcomes such as myocardial injury and
increased mortality. Despite its significance, IOH prediction is hindered by
event sparsity and the challenge of integrating static and dynamic data across
diverse patients. In this paper, we propose \textbf{IOHFuseLM}, a multimodal
language model framework. To accurately identify and differentiate sparse
hypotensive events, we leverage a two-stage training strategy. The first stage
involves domain adaptive pretraining on IOH physiological time series augmented
through diffusion methods, thereby enhancing the model sensitivity to patterns
associated with hypotension. Subsequently, task fine-tuning is performed on the
original clinical dataset to further enhance the ability to distinguish
normotensive from hypotensive states. To enable multimodal fusion for each
patient, we align structured clinical descriptions with the corresponding
physiological time series at the token level. Such alignment enables the model
to capture individualized temporal patterns alongside their corresponding
clinical semantics. In addition, we convert static patient attributes into
structured text to enrich personalized information. Experimental evaluations on
two intraoperative datasets demonstrate that IOHFuseLM outperforms established
baselines in accurately identifying IOH events, highlighting its applicability
in clinical decision support scenarios. Our code is publicly available to
promote reproducibility at https://github.com/zjt-gpu/IOHFuseLM.

</details>


### [394] [Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments](https://arxiv.org/abs/2505.22137)
*Marc Feger,Katarina Boland,Stefan Dietze*

Main category: cs.CL

TL;DR: 尽管BERT-like模型在论点识别任务上表现优异，但它们往往依赖于数据集特定的线索而非真正对齐任务。通过加入任务特定的预训练和联合基准训练，可以提高模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在重新评估现有最先进的transformer模型在论点识别任务上的泛化能力，特别是在不同数据集上的表现。

Method: 评估四个transformer模型（三个标准模型和一个经过对比预训练的增强模型）在17个英语句子级数据集上的表现，分析其依赖的特征及泛化能力。

Result: 模型在熟悉的数据集上表现良好，但在未见过的数据集上性能显著下降，表明其依赖于词汇捷径。通过任务特定预训练和联合基准训练可提升模型的鲁棒性和泛化能力。

Conclusion: 现有的transformer模型在论点识别任务上的成功可能更多依赖于数据集特定的线索，而非真正的任务理解；通过适当的训练策略可改善这一状况。

Abstract: Identifying arguments is a necessary prerequisite for various tasks in
automated discourse analysis, particularly within contexts such as political
debates, online discussions, and scientific reasoning. In addition to
theoretical advances in understanding the constitution of arguments, a
significant body of research has emerged around practical argument mining,
supported by a growing number of publicly available datasets. On these
benchmarks, BERT-like transformers have consistently performed best,
reinforcing the belief that such models are broadly applicable across diverse
contexts of debate. This study offers the first large-scale re-evaluation of
such state-of-the-art models, with a specific focus on their ability to
generalize in identifying arguments. We evaluate four transformers, three
standard and one enhanced with contrastive pre-training for better
generalization, on 17 English sentence-level datasets as most relevant to the
task. Our findings show that, to varying degrees, these models tend to rely on
lexical shortcuts tied to content words, suggesting that apparent progress may
often be driven by dataset-specific cues rather than true task alignment. While
the models achieve strong results on familiar benchmarks, their performance
drops markedly when applied to unseen datasets. Nonetheless, incorporating both
task-specific pre-training and joint benchmark training proves effective in
enhancing both robustness and generalization.

</details>


### [395] [Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes](https://arxiv.org/abs/2505.22165)
*Bocheng Li,Zhujin Gao,Linli Xu*

Main category: cs.CL

TL;DR: NeoDiff结合了离散和连续扩散模型的优点，提出了一种新的文本生成方法，并在多个任务中表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型分为离散和连续两类，各有局限性：离散模型缺乏细粒度控制，而连续模型在令牌上的扩散进程是均匀的，限制了其捕捉语义细微差别的能力。

Method: 提出了非同时连续扩散模型（NeoDiff），引入泊松扩散过程进行前向处理，实现灵活和细粒度的噪声范式；反向过程中使用时间预测器根据令牌语义自适应调节去噪进程，并优化推理计划以确保更精确的噪声控制。

Result: 实验结果表明，在多个文本生成任务中，NeoDiff相比非自回归连续、离散扩散模型、迭代方法和自回归扩散方法等基线模型表现出更好的性能。

Conclusion: NeoDiff统一了离散和连续扩散模型的理论，提供了一个更为合理和有效的文本生成框架，具有生成高质量文本的潜力。

Abstract: Diffusion models have emerged as a promising approach for text generation,
with recent works falling into two main categories: discrete and continuous
diffusion models. Discrete diffusion models apply token corruption
independently using categorical distributions, allowing for different diffusion
progress across tokens but lacking fine-grained control. Continuous diffusion
models map tokens to continuous spaces and apply fine-grained noise, but the
diffusion progress is uniform across tokens, limiting their ability to capture
semantic nuances. To address these limitations, we propose
\textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous
C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models
(NeoDiff), a novel diffusion model that integrates the strengths of both
discrete and continuous approaches. NeoDiff introduces a Poisson diffusion
process for the forward process, enabling a flexible and fine-grained noising
paradigm, and employs a time predictor for the reverse process to adaptively
modulate the denoising progress based on token semantics. Furthermore, NeoDiff
utilizes an optimized schedule for inference to ensure more precise noise
control and improved performance. Our approach unifies the theories of discrete
and continuous diffusion models, offering a more principled and effective
framework for text generation. Experimental results on several text generation
tasks demonstrate NeoDiff's superior performance compared to baselines of
non-autoregressive continuous and discrete diffusion models, iterative-based
methods and autoregressive diffusion-based methods. These results highlight
NeoDiff's potential as a powerful tool for generating high-quality text and
advancing the field of diffusion-based text generation.

</details>


### [396] [Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design](https://arxiv.org/abs/2505.22179)
*Yudi Zhang,Weilin Zhao,Xu Han,Tiejun Zhao,Wang Xu,Hailong Cao,Conghui Zhu*

Main category: cs.CL

TL;DR: Speculative decoding 和 quantization 能有效加速大型语言模型的推理。研究发现，4-bit 权重量化带来的内存优势会被 speculative decoding 的计算负担所抵消。为了解决这个问题，提出了一种新的分层框架，使用小模型作为中间阶段，将树状草稿转换为序列草稿，从而利用目标量化模型的内存访问优势。实验结果表明，该方法在A100 GPU上对4-bit权重量化的Llama-3-70B模型实现了2.78倍的加速，并且比EAGLE-2快1.31倍。


<details>
  <summary>Details</summary>
Motivation: Speculative decoding 和 quantization 都可以加速大型语言模型的推理，但将两者结合时可能会出现内存优势被计算负担抵消的问题，因此需要一种新的方法来解决这个问题。

Method: 提出了一种新的分层框架，使用小模型作为中间阶段，将树状草稿转换为序列草稿，从而利用目标量化模型的内存访问优势。

Result: 实验结果表明，该方法在A100 GPU上对4-bit权重量化的Llama-3-70B模型实现了2.78倍的加速，并且比EAGLE-2快1.31倍。

Conclusion: 分层框架能够有效地结合 speculative decoding 和 quantization 的优点，显著提高大型语言模型的推理速度。

Abstract: Speculative decoding and quantization effectively accelerate memory-bound
inference of large language models. Speculative decoding mitigates the memory
bandwidth bottleneck by verifying multiple tokens within a single forward pass,
which increases computational effort. Quantization achieves this optimization
by compressing weights and activations into lower bit-widths and also reduces
computations via low-bit matrix multiplications. To further leverage their
strengths, we investigate the integration of these two techniques.
Surprisingly, experiments applying the advanced speculative decoding method
EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit
weight quantization are diminished by the computational load from speculative
decoding. Specifically, verifying a tree-style draft incurs significantly more
time overhead than a single-token forward pass on 4-bit weight quantized
models. This finding led to our new speculative decoding design: a hierarchical
framework that employs a small model as an intermediate stage to turn
tree-style drafts into sequence drafts, leveraging the memory access benefits
of the target quantized model. Experimental results show that our hierarchical
approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit
weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$.
Code available at https://github.com/AI9Stars/SpecMQuant.

</details>


### [397] [Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon](https://arxiv.org/abs/2505.22184)
*Xuchen Ma,Jianxiang Yu,Wenming Shao,Bo Pang,Xiang Li*

Main category: cs.CL

TL;DR: 提出了一种新的方法C$^2$TU，用于揭示中文伪装的毒性内容，通过子串匹配和基于BERT及大语言模型的过滤方法，在两个中文毒性数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台上的毒性内容增加，一些用户通过同音异义词等方式规避审查，现有方法主要针对英文文本，缺乏对中文伪装毒性内容的解决方法。

Method: C$^2$TU方法首先使用子串匹配识别基于中文同形字和毒性词汇表的候选毒性词，然后通过两种模型变体（基于BERT和大语言模型）过滤非毒性候选并纠正伪装为对应的毒性词。对于大语言模型，解决了自回归限制，利用完整语义上下文揭示伪装毒性词。

Result: 在两个中文毒性数据集上进行的广泛实验表明，C$^2$TU方法在F1分数上比最佳竞争对手高出71%，在准确性上高出35%。

Conclusion: C$^2$TU是一种无需训练和提示的新方法，能够有效揭示中文伪装毒性内容，并在实验中展现出优越性能。

Abstract: Social media platforms have experienced a significant rise in toxic content,
including abusive language and discriminatory remarks, presenting growing
challenges for content moderation. Some users evade censorship by deliberately
disguising toxic words through homophonic cloak, which necessitates the task of
unveiling cloaked toxicity. Existing methods are mostly designed for English
texts, while Chinese cloaked toxicity unveiling has not been solved yet. To
tackle the issue, we propose C$^2$TU, a novel training-free and prompt-free
method for Chinese cloaked toxic content unveiling. It first employs substring
matching to identify candidate toxic words based on Chinese homo-graph and
toxic lexicon. Then it filters those candidates that are non-toxic and corrects
cloaks to be their corresponding toxicities. Specifically, we develop two model
variants for filtering, which are based on BERT and LLMs, respectively. For
LLMs, we address the auto-regressive limitation in computing word occurrence
probability and utilize the full semantic contexts of a text sequence to reveal
cloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve
superior performance on two Chinese toxic datasets. In particular, our method
outperforms the best competitor by up to 71% on the F1 score and 35% on
accuracy, respectively.

</details>


### [398] [Let's Predict Sentence by Sentence](https://arxiv.org/abs/2505.22202)
*Hyeonbin Hwang,Byeongguk Jeon,Seungone Kim,Jiyeon Kim,Hoyeon Chang,Sohee Yang,Seungpil Won,Dohaeng Lee,Youbin Ahn,Minjoon Seo*

Main category: cs.CL

TL;DR: 本研究探讨了如何将预训练的语言模型提升到抽象推理空间，并提出了一个框架，通过自回归预测下一个句子的连续嵌入来实现这一点。使用语义和上下文嵌入范式，研究发现上下文嵌入在连续推理模式下表现出与思维链(Chain-of-Thought)相当的性能，同时减少了平均一半的推理时间浮点运算(FLOPs)。此外，还介绍了SentenceLens工具用于可视化潜在轨迹。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归语言模型以逐个标记生成文本，但人类推理是在更高层次的抽象（如句子、命题和概念）上进行的。因此，作者希望了解语言模型是否可以像人类一样，在结构化的语义单元上进行推理，而不仅仅是基于原始标记序列。

Method: 研究者提出了一种框架，将预训练的标记级语言模型适应到句子空间中，通过自回归预测下一个句子的连续嵌入。他们探索了两种嵌入范式：1）语义嵌入，通过自动编码学习以保留表面意义；2）上下文嵌入，通过下一句预测训练以编码预期结构。评估了两种推理机制：离散化和连续推理。

Result: 在数学、逻辑、常识和规划四个领域中，上下文嵌入在连续推理机制下的表现与思维链方法相当，同时减少了约一半的推理时间浮点运算。此外，还展示了可扩展性和模块化适应的早期迹象。

Conclusion: 预训练的语言模型可以通过在潜在嵌入空间内操作，有效地过渡到抽象、结构化的推理。

Abstract: Autoregressive language models (LMs) generate one token at a time, yet human
reasoning operates over higher-level abstractions - sentences, propositions,
and concepts. This contrast raises a central question- Can LMs likewise learn
to reason over structured semantic units rather than raw token sequences? In
this work, we investigate whether pretrained LMs can be lifted into such
abstract reasoning spaces by building on their learned representations. We
present a framework that adapts a pretrained token-level LM to operate in
sentence space by autoregressively predicting continuous embeddings of next
sentences. We explore two embedding paradigms inspired by classical
representation learning: 1) semantic embeddings, learned via autoencoding to
preserve surface meaning; and 2) contextual embeddings, trained via
next-sentence prediction to encode anticipatory structure. We evaluate both
under two inference regimes: Discretized, which decodes each predicted
embedding into text before re-encoding; and Continuous, which reasons entirely
in embedding space for improved efficiency. Across four domains - mathematics,
logic, commonsense, and planning - contextual embeddings under continuous
inference show competitive performance with Chain-of-Thought (CoT) while
reducing inference-time FLOPs on average by half. We also present early signs
of scalability and modular adaptation. Finally, to visualize latent
trajectories, we introduce SentenceLens, a diagnostic tool that decodes
intermediate model states into interpretable sentences. Together, our results
indicate that pretrained LMs can effectively transition to abstract, structured
reasoning within latent embedding spaces.

</details>


### [399] [Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models](https://arxiv.org/abs/2505.22232)
*Mehdi Ali,Manuel Brack,Max Lübbering,Elias Wendt,Abbas Goher Khan,Richard Rutmann,Alex Jude,Maurice Kraus,Alexander Arno Weber,Felix Stollenwerk,David Kaczér,Florian Mai,Lucie Flek,Rafet Sifa,Nicolas Flores-Herr,Joachim Köhler,Patrick Schramowski,Michael Fromm,Kristian Kersting*

Main category: cs.CL

TL;DR: JQL is a new method for creating high-quality multilingual datasets by using lightweight annotators derived from pretrained multilingual embeddings, which improves downstream model training and data retention.


<details>
  <summary>Details</summary>
Motivation: Current open-source multilingual datasets are limited in quality and scalability due to reliance on heuristic filtering methods.

Method: JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings, enabling efficient curation of diverse and high-quality multilingual data.

Result: Empirical evaluation across 35 languages shows JQL outperforms existing methods like Fineweb2, enhancing downstream model training quality and increasing data retention rates.

Conclusion: JQL provides practical insights and valuable resources for multilingual data curation, advancing the standards of multilingual dataset development.

Abstract: High-quality multilingual training data is essential for effectively
pretraining large language models (LLMs). Yet, the availability of suitable
open-source multilingual datasets remains limited. Existing state-of-the-art
datasets mostly rely on heuristic filtering methods, restricting both their
cross-lingual transferability and scalability. Here, we introduce JQL, a
systematic approach that efficiently curates diverse and high-quality
multilingual data at scale while significantly reducing computational demands.
JQL distills LLMs' annotation capabilities into lightweight annotators based on
pretrained multilingual embeddings. These models exhibit robust multilingual
and cross-lingual performance, even for languages and scripts unseen during
training. Evaluated empirically across 35 languages, the resulting annotation
pipeline substantially outperforms current heuristic filtering methods like
Fineweb2. JQL notably enhances downstream model training quality and increases
data retention rates. Our research provides practical insights and valuable
resources for multilingual data curation, raising the standards of multilingual
dataset development.

</details>


### [400] [MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2505.22264)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Saez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: This paper presents an approach to solve the SemEval 2025 Task 8 challenge using Python code generation with LLMs, achieving a score of 70.50% for subtask 1.


<details>
  <summary>Details</summary>
Motivation: To develop an effective method for question-answering over tabular data by leveraging Python code generation with LLMs.

Method: The method involves multiple steps including understanding table content, generating natural language instructions, translating these instructions into code, running the code, and handling potential errors or exceptions using open source LLMs and optimized prompts.

Result: The approach achieved a score of 70.50% for subtask 1 of the SemEval 2025 Task 8 challenge.

Conclusion: The proposed strategy using Python code generation with LLMs shows promise in addressing the question-answering challenge over tabular data.

Abstract: In this paper we expose our approach to solve the \textit{SemEval 2025 Task
8: Question-Answering over Tabular Data} challenge. Our strategy leverages
Python code generation with LLMs to interact with the table and get the answer
to the questions. The process is composed of multiple steps: understanding the
content of the table, generating natural language instructions in the form of
steps to follow in order to get the answer, translating these instructions to
code, running it and handling potential errors or exceptions. These steps use
open source LLMs and fine grained optimized prompts for each task (step). With
this approach, we achieved a score of $70.50\%$ for subtask 1.

</details>


### [401] [Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review](https://arxiv.org/abs/2505.22280)
*Zihan Xu,Haotian Ma,Gongbo Zhang,Yihao Ding,Chunhua Weng,Yifan Peng*

Main category: cs.CL

TL;DR: This paper conducts an in-depth review of 129 research studies on using NLP for Evidence-based Medicine (EBM). It highlights the role of NLP in enhancing clinical decision-making and proposes future research directions.


<details>
  <summary>Details</summary>
Motivation: There is a critical need to explore Natural Language Processing (NLP) methods due to the large volume of medical literature and high curation costs, which are challenges in applying Evidence-based Medicine (EBM).

Method: The authors conducted an in-depth review of 129 research studies that leverage NLP for EBM. They examined how NLP supports the five fundamental steps of EBM - Ask, Acquire, Appraise, Apply, and Assess.

Result: The review identified current limitations within the field of NLP for EBM and proposed potential directions for future research.

Conclusion: NLP has the potential to revolutionize EBM by improving evidence extraction, synthesis, appraisal, summarization, data comprehensibility, and clinical workflow efficiency.

Abstract: Evidence-based medicine (EBM) is at the forefront of modern healthcare,
emphasizing the use of the best available scientific evidence to guide clinical
decisions. Due to the sheer volume and rapid growth of medical literature and
the high cost of curation, there is a critical need to investigate Natural
Language Processing (NLP) methods to identify, appraise, synthesize, summarize,
and disseminate evidence in EBM. This survey presents an in-depth review of 129
research studies on leveraging NLP for EBM, illustrating its pivotal role in
enhancing clinical decision-making processes. The paper systematically explores
how NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,
Apply, and Assess. The review not only identifies current limitations within
the field but also proposes directions for future research, emphasizing the
potential for NLP to revolutionize EBM by refining evidence extraction,
evidence synthesis, appraisal, summarization, enhancing data comprehensibility,
and facilitating a more efficient clinical workflow.

</details>


### [402] [Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start](https://arxiv.org/abs/2505.22334)
*Lai Wei,Yuting Li,Kaipeng Zheng,Chen Wang,Yue Wang,Linghe Kong,Lichao Sun,Weiran Huang*

Main category: cs.CL

TL;DR: Recent advancements in large language models (LLMs) have shown impressive chain-of-thought reasoning capabilities, with reinforcement learning playing a crucial role. This study demonstrates that 'aha moment' patterns exist in multimodal LLMs prior to RL training but may not correlate with improved reasoning performance. A two-stage approach of supervised fine-tuning followed by reinforcement learning via GRPO consistently outperforms SFT-only and RL-only methods across multimodal reasoning benchmarks.


<details>
  <summary>Details</summary>
Motivation: To explore the existence of 'aha moment' patterns in multimodal LLMs before reinforcement learning training and their correlation with reasoning performance, and to enhance multimodal reasoning through an effective two-stage approach.

Method: The method involves a two-stage approach: 1) supervised fine-tuning as a cold start with structured chain-of-thought reasoning patterns; 2) reinforcement learning via GRPO to refine these capabilities.

Result: Extensive experiments show that the combined approach outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales.

Conclusion: This work provides practical guidance for building advanced multimodal reasoning models, demonstrating significant improvements over base models and achieving competitive performance.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
impressive chain-of-thought reasoning capabilities, with reinforcement learning
(RL) playing a crucial role in this progress. While "aha moment"
patterns--where models exhibit self-correction through reflection--are often
attributed to emergent properties from RL, we first demonstrate that these
patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not
necessarily correlate with improved reasoning performance. Building on these
insights, we present a comprehensive study on enhancing multimodal reasoning
through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start
with structured chain-of-thought reasoning patterns, followed by (2)
reinforcement learning via GRPO to further refine these capabilities. Our
extensive experiments show that this combined approach consistently outperforms
both SFT-only and RL-only methods across challenging multimodal reasoning
benchmarks. The resulting models achieve state-of-the-art performance among
open-source MLLMs at both 3B and 7B scales, with our 7B model showing
substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on
MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving
performance competitive with several 7B models. Overall, this work provides
practical guidance for building advanced multimodal reasoning models. Our code
is available at https://github.com/waltonfuture/RL-with-Cold-Start.

</details>


### [403] [Text2Grad: Reinforcement Learning from Natural Language Feedback](https://arxiv.org/abs/2505.22338)
*Hanyang Wang,Lu Wang,Chaoyun Zhang,Tianjun Mao,Si Qin,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang*

Main category: cs.CL

TL;DR: Text2Grad is a new reinforcement-learning paradigm that converts textual feedback into span-level gradients to optimize language models.


<details>
  <summary>Details</summary>
Motivation: Traditional RLHF uses coarse, scalar rewards which can lead to slow and opaque learning. Recent work has improved interpretability by adding textual critiques but does not adjust model parameters directly.

Method: Text2Grad turns free-form textual feedback into span-level gradients. It aligns each feedback phrase with relevant token spans, converts these alignments into differentiable reward signals, and performs gradient updates to directly refine the model's policy. This is achieved through three components: a feedback-annotation pipeline, a fine-grained reward model, and a span-level policy optimizer.

Result: Text2Grad consistently outperforms scalar-reward RL and prompt-only baselines in tasks such as summarization, code generation, and question answering, providing higher task metrics and richer interpretability.

Conclusion: Natural-language feedback, when converted to gradients, serves as a powerful signal for fine-grained policy optimization.

Abstract: Traditional RLHF optimizes language models with coarse, scalar rewards that
mask the fine-grained reasons behind success or failure, leading to slow and
opaque learning. Recent work augments RL with textual critiques through
prompting or reflection, improving interpretability but leaving model
parameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm
that turns free-form textual feedback into span-level gradients. Given human
(or programmatic) critiques, Text2Grad aligns each feedback phrase with the
relevant token spans, converts these alignments into differentiable reward
signals, and performs gradient updates that directly refine the offending
portions of the model's policy. This yields precise, feedback-conditioned
adjustments instead of global nudges. Text2Grad is realized through three
components: (1) a high-quality feedback-annotation pipeline that pairs
critiques with token spans; (2) a fine-grained reward model that predicts
span-level reward on answer while generating explanatory critiques; and (3) a
span-level policy optimizer that back-propagates natural-language gradients.
Across summarization, code generation, and question answering, Text2Grad
consistently surpasses scalar-reward RL and prompt-only baselines, providing
both higher task metrics and richer interpretability. Our results demonstrate
that natural-language feedback, when converted to gradients, is a powerful
signal for fine-grained policy optimization. The code for our method is
available at https://github.com/microsoft/Text2Grad

</details>


### [404] [Representative Language Generation](https://arxiv.org/abs/2505.21819)
*Charlotte Peale,Vinod Raman,Omer Reingold*

Main category: cs.CL

TL;DR: The paper introduces 'representative generation' to address diversity and bias in generative models by ensuring outputs proportionally represent groups of interest, providing a rigorous foundation for more diverse models.


<details>
  <summary>Details</summary>
Motivation: To extend the theoretical framework for generation to address diversity and bias concerns in generative models.

Method: Introduce the concept of 'representative generation', characterize representative uniform and non-uniform generation using 'group closure dimension', and analyze information-theoretic and computational aspects.

Result: Demonstrate feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but prove a negative result for computability using only membership queries.

Conclusion: Provides a rigorous foundation for developing more diverse and representative generative models.

Abstract: We introduce "representative generation," extending the theoretical framework
for generation proposed by Kleinberg et al. (2024) and formalized by Li et al.
(2024), to additionally address diversity and bias concerns in generative
models. Our notion requires outputs of a generative model to proportionally
represent groups of interest from the training data. We characterize
representative uniform and non-uniform generation, introducing the "group
closure dimension" as a key combinatorial quantity. For representative
generation in the limit, we analyze both information-theoretic and
computational aspects, demonstrating feasibility for countably infinite
hypothesis classes and collections of groups under certain conditions, but
proving a negative result for computability using only membership queries. This
contrasts with Kleinberg et al.'s (2024) positive results for standard
generation in the limit. Our findings provide a rigorous foundation for
developing more diverse and representative generative models.

</details>


### [405] [Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO](https://arxiv.org/abs/2505.22453)
*Lai Wei,Yuting Li,Chen Wang,Yue Wang,Linghe Kong,Weiran Huang,Lichao Sun*

Main category: cs.CL

TL;DR: The paper introduces MM-UPT, an unsupervised post-training framework for MLLMs based on GRPO. It replaces traditional reward signals with a self-rewarding mechanism and shows significant improvement in reasoning ability without external supervision.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of supervised methods for improving MLLMs which require expensive annotated data and the complexity of existing unsupervised post-training methods.

Method: MM-UPT uses GRPO with a self-rewarding mechanism based on majority voting over multiple sampled responses for unsupervised post-training of MLLMs. Incorporating synthetic questions generated by MLLM itself can further boost performance.

Result: MM-UPT significantly improves reasoning ability of Qwen2.5-VL-7B on MathVista and We-Math datasets. It outperforms prior unsupervised baselines and approaches the results of supervised GRPO.

Conclusion: MM-UPT provides a new paradigm for continual and autonomous enhancement of MLLMs without external supervision.

Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training
stage typically relies on supervised fine-tuning (SFT) or reinforcement
learning (RL). However, these supervised methods require expensive and manually
annotated multi-modal data--an ultimately unsustainable resource. While recent
efforts have explored unsupervised post-training, their methods are complex and
difficult to iterate. In this work, we are the first to investigate the use of
GRPO, a stable and scalable online RL algorithm, for enabling continual
self-improvement without any external supervision. We propose MM-UPT, a simple
yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds
upon GRPO, replacing traditional reward signals with a self-rewarding mechanism
based on majority voting over multiple sampled responses. Our experiments
demonstrate that MM-UPT significantly improves the reasoning ability of
Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9
%$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth
labels. MM-UPT also outperforms prior unsupervised baselines and even
approaches the results of supervised GRPO. Furthermore, we show that
incorporating synthetic questions, generated solely by MLLM itself, can boost
performance as well, highlighting a promising approach for scalable
self-improvement. Overall, MM-UPT offers a new paradigm for continual,
autonomous enhancement of MLLMs in the absence of external supervision. Our
code is available at https://github.com/waltonfuture/MM-UPT.

</details>


### [406] [Curse of High Dimensionality Issue in Transformer for Long-context Modeling](https://arxiv.org/abs/2505.22107)
*Shuhai Zhang,Zeng You,Yaofo Chen,Zhiquan Wen,Qianyue Wang,Zhijie Qiu,Yuanqing Li,Mingkui Tan*

Main category: cs.CL

TL;DR: Transformer-based LLMs struggle with computational inefficiency due to redundant attention computations. This paper reframes probabilistic sequence modeling as a supervised learning task, analyzes attention sparsity, and proposes Dynamic Group Attention (DGA) to reduce redundancy and computational costs while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the issue of redundant attention computations in long-context modeling, which leads to significant computational inefficiencies despite sparse attention weights.

Method: Reformulate traditional probabilistic sequence modeling as a supervised learning task, analyze attention sparsity theoretically, formulate attention optimization as a linear coding problem, and propose a group coding strategy leading to the development of Dynamic Group Attention (DGA).

Result: Empirical results demonstrate that DGA significantly reduces computational costs while maintaining competitive performance.

Conclusion: Dynamic Group Attention offers an effective solution for reducing redundancy and computational costs in transformer-based models without sacrificing performance.

Abstract: Transformer-based large language models (LLMs) excel in natural language
processing tasks by capturing long-range dependencies through self-attention
mechanisms. However, long-context modeling faces significant computational
inefficiencies due to \textit{redundant} attention computations: while
attention weights are often \textit{sparse}, all tokens consume \textit{equal}
computational resources. In this paper, we reformulate traditional
probabilistic sequence modeling as a \textit{supervised learning task},
enabling the separation of relevant and irrelevant tokens and providing a
clearer understanding of redundancy. Based on this reformulation, we
theoretically analyze attention sparsity, revealing that only a few tokens
significantly contribute to predictions. Building on this, we formulate
attention optimization as a linear coding problem and propose a \textit{group
coding strategy}, theoretically showing its ability to improve robustness
against random noise and enhance learning efficiency. Motivated by this, we
propose \textit{Dynamic Group Attention} (DGA), which leverages the group
coding to explicitly reduce redundancy by aggregating less important tokens
during attention computation. Empirical results show that our DGA significantly
reduces computational costs while maintaining competitive performance.Code is
available at https://github.com/bolixinyu/DynamicGroupAttention.

</details>


### [407] [RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding](https://arxiv.org/abs/2505.22135)
*Yuichiro Hoshino,Hideyuki Tachibana,Muneyoshi Inahara,Hiroto Takegawa*

Main category: cs.CL

TL;DR: RAD (Redundancy-Aware Distillation) is a new framework which uses self-speculative decoding to identify and replace redundant attention layers in hybrid models with SSM components, followed by targeted distillation. It surpasses the original model's performance on math and coding tasks, converges faster in standard knowledge distillation settings, and achieves higher scores than baseline methods even when using a smaller teacher model.


<details>
  <summary>Details</summary>
Motivation: Hybrid models combining Transformers and State Space Models (SSMs) show promise for balancing performance and efficiency, but redundancy within Transformer components poses an optimization challenge.

Method: Propose RAD that employs self-speculative decoding to detect redundant attention layers in hybrid models, replaces them with SSM components, and applies targeted (self-)distillation focusing on the identified redundant components while considering architectural changes and weight initialization strategies.

Result: Experiments show that self-distillation using RAD outperforms the original base model on mathematical and coding tasks, converges up to approximately 2x faster in standard knowledge distillation settings, and achieves significantly higher scores on GSM8K and CRUX compared to baseline methods even with a much smaller teacher model.

Conclusion: RAD presents a novel approach for efficient optimization and performance improvement in the distillation of hybrid models.

Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are
promising for balancing performance and efficiency. However, optimizing these
hybrid models, particularly by addressing the potential redundancy inherent
within the Transformer components, remains a significant challenge. In this
paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that
uses self-speculative decoding as a diagnostic tool to identify redundant
attention layers within the model. These identified layers are then selectively
replaced with SSM components, followed by targeted (self-)distillation.
Specifically, RAD focuses knowledge transfer on the components identified as
redundant, considering architectural changes and specific weight initialization
strategies. We experimentally demonstrate that self-distillation using RAD
significantly surpasses the performance of the original base model on
mathematical and coding tasks. Furthermore, RAD is also effective in standard
knowledge distillation settings, achieving up to approximately 2x faster
convergence compared to baseline methods. Notably, while a baseline model
distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and
22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and
28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers
a new pathway for efficient optimization and performance enhancement in the
distillation of hybrid models.

</details>


### [408] [ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM](https://arxiv.org/abs/2505.22552)
*Hoang Pham,Thanh-Do Nguyen,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: 将知识图谱(KGs)与大语言模型(LLMs)结合以增强声明验证中的推理能力是新兴挑战。现有的方法主要依赖非结构化文本，难以充分利用KGs的优势，同时现代LLMs在多步骤推理上也存在困难。为解决这些问题，我们提出了ClaimPKG框架，该框架通过轻量级的专用LLM将输入声明表示为伪子图，引导专门的子图检索模块识别相关KG子图，再由通用LLM生成最终判断和理由。实验表明，ClaimPKG在FactKG数据集上达到最先进水平，并且在HoVer和FEVEROUS等非结构化数据集上展现出零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 知识图谱(KGs)提供了适合推理的结构化、语义丰富的表示，然而大多数现有的声明验证方法依赖于非结构化文本，无法有效利用KGs。此外，尽管现代大语言模型(LLMs)具备强大的推理能力，但在没有适应的情况下，处理多步骤管道和在KGs上进行推理时仍然面临挑战。

Method: 提出了一种端到端框架ClaimPKG，它无缝整合了LLM推理与来自KGs的结构化知识。具体来说，ClaimPKG使用轻量级、专用的LLM将输入声明表示为伪子图，指导专门的子图检索模块识别相关的KG子图。然后，这些检索到的子图被通用LLM处理，以生成最终判断和理由。

Result: 在FactKG数据集上的广泛实验证明，ClaimPKG实现了最先进的性能，在多个类别中比强大基线高出9%-12%的准确率。此外，ClaimPKG展示了对非结构化数据集如HoVer和FEVEROUS的零样本泛化能力，成功结合了来自KGs的结构化知识与LLM推理。

Conclusion: ClaimPKG提供了一种有效的解决方案，用于克服现有声明验证方法在利用KGs和多步骤推理方面的限制。它通过结合专用和通用LLM的能力，增强了基于KGs的声明验证任务的推理能力，并在多个数据集上展示了优越性和泛化能力。

Abstract: Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of
large language models (LLMs) is an emerging research challenge in claim
verification. While KGs provide structured, semantically rich representations
well-suited for reasoning, most existing verification methods rely on
unstructured text corpora, limiting their ability to effectively leverage KGs.
Additionally, despite possessing strong reasoning abilities, modern LLMs
struggle with multi-step modular pipelines and reasoning over KGs without
adaptation. To address these challenges, we propose ClaimPKG, an end-to-end
framework that seamlessly integrates LLM reasoning with structured knowledge
from KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,
specialized LLM to represent the input claim as pseudo-subgraphs, guiding a
dedicated subgraph retrieval module to identify relevant KG subgraphs. These
retrieved subgraphs are then processed by a general-purpose LLM to produce the
final verdict and justification. Extensive experiments on the FactKG dataset
demonstrate that ClaimPKG achieves state-of-the-art performance, outperforming
strong baselines in this research field by 9%-12% accuracy points across
multiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability
to unstructured datasets such as HoVer and FEVEROUS, effectively combining
structured knowledge from KGs with LLM reasoning across various LLM backbones.

</details>


### [409] [Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2505.22571)
*Hoang Pham,Khac-Hoai Nam Bui*

Main category: cs.CL

TL;DR: This paper presents Agent-UniRAG, a trainable agent framework for unified retrieval-augmented LLM systems that enhances effectiveness and interpretability by handling single-hop and multi-hop queries in an end-to-end manner. It also introduces SynAgent-RAG, a synthetic dataset for small open-source LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of previous works which mainly focus on solving RAG systems with either single-hop or multi-hop approaches separately, thus limiting their application to real-world scenarios.

Method: Proposes a trainable agent framework named Agent-UniRAG for unified retrieval-augmented LLM systems that can handle both single-hop and multi-hop queries in an end-to-end manner. Also introduces SynAgent-RAG, a synthetic dataset designed for small open-source LLMs.

Result: Achieves comparable performance with closed-source and larger open-source LLMs across various RAG benchmarks.

Conclusion: The proposed Agent-UniRAG framework and SynAgent-RAG dataset enhance the effectiveness and interpretability of RAG systems, offering potential for further exploitation.

Abstract: This paper presents a novel approach for unified retrieval-augmented
generation (RAG) systems using the recent emerging large language model (LLM)
agent concept. Specifically, Agent LLM, which utilizes LLM as fundamental
controllers, has become a promising approach to enable the interpretability of
RAG tasks, especially for complex reasoning question-answering systems (e.g.,
multi-hop queries). Nonetheless, previous works mainly focus on solving RAG
systems with either single-hop or multi-hop approaches separately, which limits
the application of those approaches to real-world applications. In this study,
we propose a trainable agent framework called Agent-UniRAG for unified
retrieval-augmented LLM systems, which enhances the effectiveness and
interpretability of RAG systems. The main idea is to design an LLM agent
framework to solve RAG tasks step-by-step based on the complexity of the
inputs, simultaneously including single-hop and multi-hop queries in an
end-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset
to enable the proposed agent framework for small open-source LLMs (e.g.,
Llama-3-8B). The results show comparable performances with closed-source and
larger open-source LLMs across various RAG benchmarks. Our source code and
dataset are publicly available for further exploitation.

</details>


### [410] [Fusion Steering: Prompt-Specific Activation Control](https://arxiv.org/abs/2505.22572)
*Waldemar Chang,Alhassan Yasin*

Main category: cs.CL

TL;DR: Fusion Steering是一种新的激活转向方法，通过动态注入特定提示的激活差值来提高大型语言模型在问答任务中的事实准确性。实验结果表明，分段转向策略在准确性和流畅性方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 现有的转向方法受限于单层或固定层操作，无法灵活地调整大型语言模型的激活状态以提高事实准确性。

Method: 提出了一种名为Fusion Steering的激活转向方法，包括全层转向和分段转向。该方法通过动态注入特定提示的激活差值（由真实答案和模型生成的解释组合而成）来实现灵活的转向配置。使用Optuna优化每个提示的注入权重，目标是平衡事实对齐和流畅性。

Result: 在260个SimpleQA提示上，分段转向策略显著优于基线方法和全层转向策略，准确率达到25.4%，并在更严格的评估标准下将完全正确回答的比例从0%提升至13.1%。

Conclusion: 分段、动态干预策略展示了其优势，每提示全网络激活控制显示出潜力。Fusion Steering还适用于稀疏表示，为可解释和可扩展的激活级控制提供了新方向。

Abstract: We present Fusion Steering, an activation steering methodology that improves
factual accuracy in large language models (LLMs) for question-answering (QA)
tasks. This approach introduces flexible steering configurations, including
full-layer steering and segmented steering. Unlike traditional methods
constrained to single-layer or fixed-layer operations, Fusion Steering employs
dynamic injection of prompt-specific activation deltas across all transformer
layers. These activation deltas are derived from reference completions that
combine the ground-truth answer with a model-generated explanation to
facilitate semantically enriched, example-specific steering. The injection
weights are optimized per prompt using Optuna, targeting a joint objective that
balances token overlap (factual alignment) and perplexity (fluency proxy).
Evaluation employs a composite score integrating token overlap and LLM-graded
quality, encompassing factual accuracy, coherence, and relevance. Empirical
results on 260 SimpleQA prompts (selected from 500 where the baseline failed)
showcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit
quantization, segmented steering achieves an accuracy of 25.4% (outputs scoring
$\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at
16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully
correct responses from 0.0% to 13.1%. These findings highlight the strengths of
segmented, dynamic intervention strategies and the promise of per-prompt,
full-network activation control. Fusion Steering is also amenable to sparse
representations, such as Neuronpedia or sparse crosscoders, suggesting a
promising direction for interpretable and scalable activation-level control in
LLMs.

</details>


### [411] [360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training](https://arxiv.org/abs/2505.22296)
*Haosheng Zou,Xiaowei Lv,Shousheng Jia,Xiangzheng Zhang*

Main category: cs.CL

TL;DR: The paper introduces 360-LLaMA-Factory, an open-source project that incorporates sequence parallelism into LLaMA-Factory, which has been widely recognized and utilized in various models and training frameworks.


<details>
  <summary>Details</summary>
Motivation: To enhance LLaMA-Factory by incorporating sequence parallelism and sharing the implementation insights with the community.

Method: Adding sequence parallelism to LLaMA-Factory and open-sourcing it as 360-LLaMA-Factory.

Result: 360-LLaMA-Factory has gained wide recognition and is used in multiple models and large companies' training frameworks.

Conclusion: This technical report explores different sequence parallel modes within 360-LLaMA-Factory and shares implementation insights.

Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced
360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.
360-LLaMA-Factory has received wide recognition and used in models such as
Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and
also in large companies' training frameworks. This technical report delves
deeper into the different sequence parallel modes behind 360-LLaMA-Factory and
discusses our implementation insights.

</details>


### [412] [Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning](https://arxiv.org/abs/2505.22591)
*Erxin Yu,Jing Li,Ming Liao,Qi Zhu,Boyang Xue,Minghui Xu,Baojun Wang,Lanqing Hong,Fei Mi,Lifeng Shang*

Main category: cs.CL

TL;DR: Although large language models perform well in many areas, they face challenges in mathematical reasoning. This paper introduces Self-Error-Instruct (SEI), a framework that identifies model errors, generates generalized training data based on these errors, and uses this data to fine-tune the model, thereby improving its mathematical reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: To address the issue of large language models struggling with mathematical reasoning and their inability to generalize from isolated bad cases when learning from errors.

Method: The SEI framework explores a target model on two mathematical datasets to find bad cases, generates error keyphrases using an instructor model, clusters these keyphrases to identify error types, synthesizes additional training data for each error type through a self-instruct approach, refines this data via one-shot learning, and uses the curated data to iteratively fine-tune the target model.

Result: The application of the SEI framework to various models leads to improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets.

Conclusion: Self-error instruction is effective in improving LLMs' mathematical reasoning by generalizing errors and synthesizing targeted training data.

Abstract: Although large language models demonstrate strong performance across various
domains, they still struggle with numerous bad cases in mathematical reasoning.
Previous approaches to learning from errors synthesize training data by solely
extrapolating from isolated bad cases, thereby failing to generalize the
extensive patterns inherent within these cases. This paper presents
Self-Error-Instruct (SEI), a framework that addresses these model weaknesses
and synthesizes more generalized targeted training data. Specifically, we
explore a target model on two mathematical datasets, GSM8K and MATH, to
pinpoint bad cases. Then, we generate error keyphrases for these cases based on
the instructor model's (GPT-4o) analysis and identify error types by clustering
these keyphrases. Next, we sample a few bad cases during each generation for
each identified error type and input them into the instructor model, which
synthesizes additional training data using a self-instruct approach. This new
data is refined through a one-shot learning process to ensure that only the
most effective examples are kept. Finally, we use these curated data to
fine-tune the target model, iteratively repeating the process to enhance
performance. We apply our framework to various models and observe improvements
in their reasoning abilities across both in-domain and out-of-domain
mathematics datasets. These results demonstrate the effectiveness of self-error
instruction in improving LLMs' mathematical reasoning through error
generalization.

</details>


### [413] [If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?](https://arxiv.org/abs/2505.22318)
*Ishwar B Balappanawar,Vamshi Krishna Bonagiri,Anish R Joishy,Manas Gaur,Krishnaprasad Thirunarayan,Ponnurangam Kumaraguru*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) show reasoning capability degradation when context conflicts with their parametric knowledge. The study introduces CounterLogic dataset and proposes Self-Segregate prompting method to improve LLMs' performance in counterfactual scenarios.


<details>
  <summary>Details</summary>
Motivation: To investigate the phenomenon that LLMs struggle when the context conflicts with their parametric knowledge, and to find a solution to enhance their reasoning capabilities in such scenarios.

Method: Introduced CounterLogic dataset containing 1,800 examples across 9 logical schemas to evaluate logical reasoning through counterfactual scenarios. Systematically evaluated 11 LLMs across 6 different datasets. Proposed Self-Segregate prompting method enabling metacognitive awareness before reasoning.

Result: Consistent performance degradation of LLMs was observed with accuracies dropping by 27% on average when reasoning through counterfactual information. Self-Segregate method narrowed the average performance gaps from 27% to 11%, while significantly increasing the overall accuracy (+7.5%).

Conclusion: The findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.

Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in
familiar contexts, but struggle when the context conflicts with their
parametric knowledge. To investigate this phenomenon, we introduce
CounterLogic, a dataset containing 1,800 examples across 9 logical schemas,
explicitly designed to evaluate logical reasoning through counterfactual
(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11
LLMs across 6 different datasets reveals a consistent performance degradation,
with accuracies dropping by 27% on average when reasoning through
counterfactual information. We propose Self-Segregate, a prompting method
enabling metacognitive awareness (explicitly identifying knowledge conflicts)
before reasoning. Our method dramatically narrows the average performance gaps
from 27% to just 11%, while significantly increasing the overall accuracy
(+7.5%). We discuss the implications of these findings and draw parallels to
human cognitive processes, particularly on how humans disambiguate conflicting
information during reasoning tasks. Our findings offer practical insights for
understanding and enhancing LLMs reasoning capabilities in real-world
applications, especially where models must logically reason independently of
their factual knowledge.

</details>


### [414] [Spatial Knowledge Graph-Guided Multimodal Synthesis](https://arxiv.org/abs/2505.22633)
*Yida Xue,Zhen Bi,Jinnan Yang,Jungang Lou,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: This paper presents SKG2Data, a multimodal synthesis approach using spatial knowledge graphs to improve the spatial perception and reasoning abilities of MLLMs.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models have advanced significantly, but their spatial perception abilities remain limited. There is a need for synthesized data that adheres to spatial common sense to address this limitation.

Method: The paper introduces SKG2Data, which constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances. This graph is used to guide the synthesis of multimodal data.

Result: Experiments show that the synthesized data enhances the spatial perception and reasoning abilities of MLLMs and exhibits strong generalization capabilities.

Conclusion: The authors hope that the concept of knowledge-based data synthesis will contribute to the advancement of spatial intelligence in MLLMs.

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced their capabilities; however, their spatial perception
abilities remain a notable limitation. To address this challenge, multimodal
data synthesis offers a promising solution. Yet, ensuring that synthesized data
adhere to spatial common sense is a non-trivial task. In this work, we
introduce SKG2Data, a novel multimodal synthesis approach guided by spatial
knowledge graphs, grounded in the concept of knowledge-to-data generation.
SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate
human-like perception of spatial directions and distances, which is
subsequently utilized to guide multimodal data synthesis. Extensive experiments
demonstrate that data synthesized from diverse types of spatial knowledge,
including direction and distance, not only enhance the spatial perception and
reasoning abilities of MLLMs but also exhibit strong generalization
capabilities. We hope that the idea of knowledge-based data synthesis can
advance the development of spatial intelligence.

</details>


### [415] [Learning Composable Chains-of-Thought](https://arxiv.org/abs/2505.22635)
*Fangcong Yin,Zeyu Leo Liu,Liu Leqi,Xi Ye,Greg Durrett*

Main category: cs.CL

TL;DR: 在教授大型语言模型（LLMs）进行推理时，通常使用链式思维（CoT）数据训练，但这些标注数据获取成本高。为了实现超越训练分布的组合泛化，研究发现通过修改原子任务的CoT格式为可组合形式，并结合多任务学习或模型合并，可以提高零样本性能。进一步地，通过少量组合数据的拒绝采样微调（RFT）可提升模型性能。实验表明，在给定训练数据预算下，使用可组合CoT训练LLMs优于多任务学习和持续微调基线。


<details>
  <summary>Details</summary>
Motivation: 当前教授大型语言模型推理的方法依赖于特定问题的链式思维（CoT）数据，但这些数据昂贵且难以扩展到未见的组合推理任务。因此需要一种方法使模型能够通过组合原子推理技能解决更复杂的、未见过的任务。

Method: 1. 使用可组合的CoT格式对原子任务进行训练，以促进组合泛化。
2. 通过多任务学习或模型合并将原子CoT模型组合起来。
3. 使用少量组合数据进行拒绝采样微调（RFT）以进一步提升性能。

Result: 在字符串操作和自然语言技能组合任务上的结果显示，使用可组合CoT训练LLMs的性能优于多任务学习和持续微调基线，特别是在给定训练数据预算下。

Conclusion: 通过使用可组合的CoT格式、多任务学习/模型合并以及拒绝采样微调，可以在不增加过多数据成本的情况下，显著提高大型语言模型在组合推理任务上的零样本和小样本性能。

Abstract: A common approach for teaching large language models (LLMs) to reason is to
train on chain-of-thought (CoT) traces of in-distribution reasoning problems,
but such annotated data is costly to obtain for every problem of interest. We
want reasoning models to generalize beyond their training distribution, and
ideally to generalize compositionally: combine atomic reasoning skills to solve
harder, unseen reasoning tasks. We take a step towards compositional
generalization of reasoning skills when addressing a target compositional task
that has no labeled CoT data. We find that simply training models on CoT data
of atomic tasks leads to limited generalization, but minimally modifying CoT
formats of constituent atomic tasks to be composable can lead to improvements.
We can train "atomic CoT" models on the atomic tasks with Composable CoT data
and combine them with multitask learning or model merging for better zero-shot
performance on the target compositional task. Such a combined model can be
further bootstrapped on a small amount of compositional data using rejection
sampling fine-tuning (RFT). Results on string operations and natural language
skill compositions show that training LLMs on Composable CoT outperforms
multitask learning and continued fine-tuning baselines within a given training
data budget.

</details>


### [416] [AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models](https://arxiv.org/abs/2505.22662)
*Feng Luo,Yu-Neng Chuang,Guanchu Wang,Hoang Anh Duy Le,Shaochen Zhong,Hongyi Liu,Jiayi Yuan,Yang Sui,Vladimir Braverman,Vipin Chaudhary,Xia Hu*

Main category: cs.CL

TL;DR: AutoL2S proposes a dynamic framework for large language models (LLMs) to adaptively adjust the length of their reasoning paths, reducing unnecessary complexity and improving efficiency without sacrificing performance.


<details>
  <summary>Details</summary>
Motivation: Reasoning-capable LLMs often overthink simple questions by generating overly long reasoning paths, which increases costs. Current approaches manually determine reasoning length but lack flexibility.

Method: The paper introduces Auto Long-Short Reasoning (AutoL2S), a model-agnostic framework that dynamically compresses reasoning paths based on question complexity. It uses a training data annotation method with <EASY> tokens to signal when shorter reasoning is sufficient.

Result: Experiments show that AutoL2S reduces reasoning path length by up to 57% while maintaining performance.

Conclusion: AutoL2S demonstrates an effective way to make LLM reasoning more scalable and efficient by dynamically adjusting reasoning length.

Abstract: The reasoning-capable large language models (LLMs) demonstrate strong
performance on complex reasoning tasks but often suffer from overthinking,
generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy
reasoning questions, thereby increasing inference cost and latency. Recent
approaches attempt to address this challenge by manually deciding when to apply
long or short reasoning. However, they lack the flexibility to adapt CoT length
dynamically based on question complexity. In this paper, we propose Auto
Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that
enables LLMs to dynamically compress their generated reasoning path based on
the complexity of the reasoning question. AutoL2S enables a learned paradigm,
in which LLMs themselves can decide when longer reasoning is necessary and when
shorter reasoning suffices, by training on data annotated with our proposed
method, which includes both long and short CoT paths and a special <EASY>
token. We then use <EASY> token to indicate when the model can skip generating
lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'
ability to generate shorter CoT reasoning paths with improved quality after
training. Extensive evaluation results show that AutoL2S reduces the length of
reasoning generation by up to 57% without compromising performance,
demonstrating the effectiveness of AutoL2S for scalable and efficient LLM
reasoning.

</details>
