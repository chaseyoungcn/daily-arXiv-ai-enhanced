{"id": "2506.06282", "pdf": "https://arxiv.org/pdf/2506.06282", "abs": "https://arxiv.org/abs/2506.06282", "authors": ["Shuangyan Deng", "Haizhou Peng", "Jiachen Xu", "Chunhou Liu", "Ciprian Doru Giurcuaneanu", "Jiamou Liu"], "title": "Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach", "categories": ["cs.AI"], "comment": null, "summary": "Effective financial reasoning demands not only textual understanding but also\nthe ability to interpret complex visual data such as charts, tables, and trend\ngraphs. This paper introduces a new benchmark designed to evaluate how well AI\nmodels - especially large language and multimodal models - reason in\nfinance-specific contexts. Covering 3,200 expert-level question-answer pairs\nacross 15 core financial topics, the benchmark integrates both textual and\nvisual modalities to reflect authentic analytical challenges in finance. To\naddress limitations in current reasoning approaches, we propose an error-aware\nlearning framework that leverages historical model mistakes and feedback to\nguide inference, without requiring fine-tuning. Our experiments across\nstate-of-the-art models show that multimodal inputs significantly enhance\nperformance and that incorporating error feedback leads to consistent and\nmeasurable improvements. The results highlight persistent challenges in visual\nunderstanding and mathematical logic, while also demonstrating the promise of\nself-reflective reasoning in financial AI systems. Our code and data can be\nfound at https://anonymous/FinMR/CodeData.", "AI": {"tldr": "The paper introduces a new benchmark for evaluating AI models' financial reasoning ability and proposes an error-aware learning framework.", "motivation": "Effective financial reasoning requires both textual understanding and the ability to interpret complex visual data, but current AI models face limitations in these areas.", "method": "The method involves creating a benchmark with 3,200 expert-level question-answer pairs across 15 core financial topics that integrates both textual and visual modalities. An error-aware learning framework is also proposed, which leverages historical model mistakes and feedback to guide inference without requiring fine-tuning.", "result": "Experiments show that multimodal inputs significantly enhance performance and incorporating error feedback leads to consistent and measurable improvements. However, challenges remain in visual understanding and mathematical logic.", "conclusion": "The results demonstrate the promise of self-reflective reasoning in financial AI systems and highlight persistent challenges in visual understanding and mathematical logic."}}
{"id": "2506.06284", "pdf": "https://arxiv.org/pdf/2506.06284", "abs": "https://arxiv.org/abs/2506.06284", "authors": ["John Beverley", "Jim Logan"], "title": "Unreal Patterns", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces a framework for representing information about entities\nthat do not exist or may never exist, such as those involving fictional\nentities, blueprints, simulations, and future scenarios. Traditional approaches\nthat introduce \"dummy instances\" or rely on modal logic are criticized, and a\nproposal is defended in which such cases are modeled using the intersections of\nactual types rather than specific non existent tokens. The paper positions\nitself within the Basic Formal Ontology and its realist commitments,\nemphasizing the importance of practical, implementable solutions over purely\nmetaphysical or philosophical proposals, arguing that existing approaches to\nnon existent entities either overcommit to metaphysical assumptions or\nintroduce computational inefficiencies that hinder applications. By developing\na structured ontology driven approach to unreal patterns, the paper aims to\nprovide a useful and computationally viable means of handling references to\nhypothetical or non existent entities.", "AI": {"tldr": "This paper proposes a framework within the Basic Formal Ontology for representing information about non-existent entities like fictional characters or future scenarios, critiquing traditional methods and emphasizing practical, computationally efficient solutions.", "motivation": "To address the limitations of traditional approaches to handling non-existent entities that either overcommit to metaphysical assumptions or introduce computational inefficiencies.", "method": "The paper develops an ontology-driven approach modeling non-existent entities using intersections of actual types rather than specific tokens, staying within realist commitments.", "result": "Proposes a structured, computationally viable solution for handling references to hypothetical or non-existent entities without overcommitting to metaphysical assumptions.", "conclusion": "An ontology-driven approach offers a practical and efficient means to manage information about non-existent entities, balancing philosophical rigor with computational feasibility."}}
{"id": "2506.06285", "pdf": "https://arxiv.org/pdf/2506.06285", "abs": "https://arxiv.org/abs/2506.06285", "authors": ["Kaike Sa Teles Rocha Alves", "Eduardo Pestana de Aguiar"], "title": "NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Evolving Fuzzy Systems (eFS) have gained significant attention due to their\nability to adaptively update their structure in response to data dynamics while\nmaintaining interpretability. However, the lack of publicly available\nimplementations of these models limits their accessibility and widespread\nadoption. To address this gap, we present evolvingfuzzysystems, a Python\nlibrary that provides implementations of several well-established eFS models,\nincluding ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\\_eTS, and eTS. The\nlibrary facilitates model evaluation and comparison by offering built-in tools\nfor training, visualization, and performance assessment. The models are\nevaluated using the fetch\\_california\\_housing dataset, with performance\nmeasured in terms of normalized root-mean-square error (NRMSE), non-dimensional\nerror index (NDEI), and mean absolute percentage error (MAPE). Additionally,\ncomputational complexity is analyzed by measuring execution times and rule\nevolution during training and testing phases. The results highlight ePL as a\nsimple yet efficient model that balances accuracy and computational cost,\nmaking it particularly suitable for real-world applications. By making these\nmodels publicly available, evolvingfuzzysystems aims to foster research and\npractical applications in adaptive and interpretable machine learning.", "AI": {"tldr": "Evolving Fuzzy Systems (eFS) are adaptive and interpretable, but lack public implementations. This paper introduces a Python library 'evolvingfuzzysystems' that implements several eFS models, evaluates them using the fetch_california_housing dataset, and finds ePL to be a simple yet efficient model.", "motivation": "To make Evolving Fuzzy Systems more accessible and promote their adoption by providing publicly available implementations.", "method": "Creating a Python library named 'evolvingfuzzysystems' which includes implementations of multiple eFS models and tools for training, visualization, and performance assessment.", "result": "The ePL model was found to be simple and efficient, balancing accuracy and computational cost. The library facilitates model evaluation and comparison.", "conclusion": "The introduction of 'evolvingfuzzysystems' aims to enhance research and practical applications in adaptive and interpretable machine learning."}}
{"id": "2506.06287", "pdf": "https://arxiv.org/pdf/2506.06287", "abs": "https://arxiv.org/abs/2506.06287", "authors": ["FutureSearch", ":", "Nikos I. Bosse", "Jon Evans", "Robert G. Gambee", "Daniel Hnyk", "Peter M\u00fchlbacher", "Lawrence Phillips", "Dan Schwarz", "Jack Wildman"], "title": "Deep Research Bench: Evaluating AI Web Research Agents", "categories": ["cs.AI"], "comment": null, "summary": "Amongst the most common use cases of modern AI is LLM chat with web search\nenabled. However, no direct evaluations of the quality of web research agents\nexist that control for the continually-changing web. We introduce Deep Research\nBench, consisting of 89 multi-step web research task instances of varying\ndifficulty across 8 diverse task categories, with the answers carefully worked\nout by skilled humans. We provide a \"RetroSearch\" environment with a large\nfrozen set of scraped web pages, and demonstrate that offline \"RetroSearch\"\nagents perform comparably to \"live web\" agents, enabling reliable evaluations\nof models over time. We provide robust agent tooling and scaffolding to\nbenchmark major LLMs as they are released, including \"thinking\" models like o3\nand Gemini 2.5 Pro. We include automated evaluations of the lengthy agent\ntraces to report progress over time in hallucinations, tool use, and\nforgetting. Finally, we evaluate the major web research products branded as\n\"Deep Research\", \"Deep Search\", \"Search\", or \"Research.\" Results are available\non a public leaderboard at https://drb.futuresearch.ai/.", "AI": {"tldr": "The paper introduces Deep Research Bench, a benchmark for evaluating web research agents using LLMs. It includes 89 multi-step tasks, a RetroSearch environment, and tools to assess model performance over time.", "motivation": "To address the lack of direct evaluations of web research agent quality that account for the ever-changing web.", "method": "Created Deep Research Bench with 89 task instances across 8 categories, developed RetroSearch environment with frozen web pages, provided agent tooling for benchmarking LLMs, and included automated evaluations.", "result": "Demonstrated comparable performance between offline RetroSearch agents and live web agents, providing reliable evaluations over time, and evaluated major web research products.", "conclusion": "Deep Research Bench offers a reliable way to evaluate and benchmark web research agents, with results available on a public leaderboard."}}
{"id": "2506.06290", "pdf": "https://arxiv.org/pdf/2506.06290", "abs": "https://arxiv.org/abs/2506.06290", "authors": ["Mingyu Lu", "Ethan Weinberger", "Chanwoo Kim", "Su-In Lee"], "title": "CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.", "AI": {"tldr": "CellCLIP is a new cross-modal contrastive learning framework for HCS data, which outperforms current models in cross-modal retrieval and biological tasks while reducing computation time.", "motivation": "High-content screening assays have enabled the interrogation of cells' morphological responses to perturbations at an unprecedented scale. There is a need for a unified latent space that aligns perturbations with their corresponding morphological effects, but applying existing methods to HCS data is challenging due to differences in semantics of Cell Painting images compared to natural images and difficulty representing different classes of perturbations in a single latent space.", "method": "CellCLIP leverages pre-trained image encoders coupled with a novel channel encoding scheme to capture relationships between different microscopy channels in image embeddings, along with natural language encoders for representing perturbations.", "result": "CellCLIP demonstrates the best performance in both cross-modal retrieval and biologically meaningful downstream tasks while also achieving significant reductions in computation time.", "conclusion": "CellCLIP is a promising tool for analyzing HCS data and understanding the relationships between different perturbations and their effects on cellular state."}}
{"id": "2506.06407", "pdf": "https://arxiv.org/pdf/2506.06407", "abs": "https://arxiv.org/abs/2506.06407", "authors": ["Zhi Wen Soi", "Chaoyi Zhu", "Fouad Abiad", "Aditya Shankar", "Jeroen M. Galjaard", "Huijuan Wang", "Lydia Y. Chen"], "title": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "Synthetic time series generated by diffusion models enable sharing\nprivacy-sensitive datasets, such as patients' functional MRI records. Key\ncriteria for synthetic data include high data utility and traceability to\nverify the data source. Recent watermarking methods embed in homogeneous latent\nspaces, but state-of-the-art time series generators operate in real space,\nmaking latent-based watermarking incompatible. This creates the challenge of\nwatermarking directly in real space while handling feature heterogeneity and\ntemporal dependencies. We propose TimeWak, the first watermarking algorithm for\nmultivariate time series diffusion models. To handle temporal dependence and\nspatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark\ndirectly within the real temporal-feature space. The other unique feature is\nthe $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction\nerror distribution across features from inverting the diffusion process to\ndetect watermarks. We derive the error bound of inverting multivariate time\nseries and further maintain high watermark detectability. We extensively\nevaluate TimeWak on its impact on synthetic data quality, watermark\ndetectability, and robustness under various post-editing attacks, against 5\ndatasets and baselines of different temporal lengths. Our results show that\nTimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in\ncorrelational scores against the state-of-the-art baseline, while remaining\nconsistently detectable.", "AI": {"tldr": "The paper introduces TimeWak, a watermarking algorithm for multivariate time series diffusion models which improves synthetic data quality and watermark detectability.", "motivation": "There is a need to share privacy-sensitive datasets while maintaining high data utility and traceability. Current watermarking methods are incompatible with state-of-the-art time series generators that operate in real space.", "method": "TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space to handle temporal dependence and spatial heterogeneity. It also uses $\\epsilon$-exact inversion to address non-uniform reconstruction error distribution across features.", "result": "TimeWak achieves improvements of 61.96% in context-FID score and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.", "conclusion": "TimeWak is an effective watermarking algorithm for multivariate time series diffusion models that maintains high synthetic data quality and watermark detectability."}}
{"id": "2506.06301", "pdf": "https://arxiv.org/pdf/2506.06301", "abs": "https://arxiv.org/abs/2506.06301", "authors": ["Muhammad Monjurul Karim", "Yan Shi", "Shucheng Zhang", "Bingzhang Wang", "Mehrdad Nasri", "Yinhai Wang"], "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review", "categories": ["cs.AI"], "comment": null, "summary": "Roadway safety and mobility remain critical challenges for modern\ntransportation systems, demanding innovative analytical frameworks capable of\naddressing complex, dynamic, and heterogeneous environments. While traditional\nengineering methods have made progress, the complexity and dynamism of\nreal-world traffic necessitate more advanced analytical frameworks. Large\nLanguage Models (LLMs), with their unprecedented capabilities in natural\nlanguage understanding, knowledge integration, and reasoning, represent a\npromising paradigm shift. This paper comprehensively reviews the application\nand customization of LLMs for enhancing roadway safety and mobility. A key\nfocus is how LLMs are adapted -- via architectural, training, prompting, and\nmultimodal strategies -- to bridge the \"modality gap\" with transportation's\nunique spatio-temporal and physical data. The review systematically analyzes\ndiverse LLM applications in mobility (e.g., traffic flow prediction, signal\ncontrol) and safety (e.g., crash analysis, driver behavior assessment,).\nEnabling technologies such as V2X integration, domain-specific foundation\nmodels, explainability frameworks, and edge computing are also examined.\nDespite significant potential, challenges persist regarding inherent LLM\nlimitations (hallucinations, reasoning deficits), data governance (privacy,\nbias), deployment complexities (sim-to-real, latency), and rigorous safety\nassurance. Promising future research directions are highlighted, including\nadvanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI\ncollaboration, continuous learning, and the development of efficient,\nverifiable systems. This review provides a structured roadmap of current\ncapabilities, limitations, and opportunities, underscoring LLMs' transformative\npotential while emphasizing the need for responsible innovation to realize\nsafer, more intelligent transportation systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5168\u9762\u56de\u987e\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63d0\u5347\u9053\u8def\u5b89\u5168\u548c\u6d41\u52a8\u6027\u65b9\u9762\u7684\u5e94\u7528\u4e0e\u5b9a\u5236\u5316\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5176\u6f5c\u529b\u3001\u6311\u6218\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u5de5\u7a0b\u65b9\u6cd5\u867d\u7136\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u9762\u5bf9\u590d\u6742\u3001\u52a8\u6001\u7684\u4ea4\u901a\u73af\u5883\u4ecd\u663e\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u5206\u6790\u6846\u67b6\u3002\u800cLLMs\u56e0\u5176\u5f3a\u5927\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u77e5\u8bc6\u6574\u5408\u548c\u63a8\u7406\u80fd\u529b\uff0c\u6210\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "method": "\u901a\u8fc7\u67b6\u6784\u3001\u8bad\u7ec3\u3001\u63d0\u793a\u548c\u591a\u6a21\u6001\u7b56\u7565\u9002\u5e94LLMs\uff0c\u4ee5\u5f25\u5408\u4e0e\u4ea4\u901a\u8fd0\u8f93\u72ec\u7279\u65f6\u7a7a\u548c\u7269\u7406\u6570\u636e\u4e4b\u95f4\u7684\u201c\u6a21\u6001\u5dee\u8ddd\u201d\u3002\u7cfb\u7edf\u5730\u5206\u6790\u4e86LLMs\u5728\u6d41\u52a8\u6027\uff08\u5982\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u3001\u4fe1\u53f7\u63a7\u5236\uff09\u548c\u5b89\u5168\u6027\uff08\u5982\u78b0\u649e\u5206\u6790\u3001\u9a7e\u9a76\u5458\u884c\u4e3a\u8bc4\u4f30\uff09\u4e2d\u7684\u591a\u6837\u5316\u5e94\u7528\u3002\u8fd8\u63a2\u8ba8\u4e86\u652f\u6301\u6280\u672f\uff0c\u5982V2X\u96c6\u6210\u3001\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u3001\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u548c\u8fb9\u7f18\u8ba1\u7b97\u3002", "result": "\u5c3d\u7ba1LLMs\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u5728\u56fa\u6709\u9650\u5236\uff08\u5e7b\u89c9\u3001\u63a8\u7406\u7f3a\u9677\uff09\u3001\u6570\u636e\u6cbb\u7406\uff08\u9690\u79c1\u3001\u504f\u5dee\uff09\u3001\u90e8\u7f72\u590d\u6742\u6027\uff08\u6a21\u62df\u5230\u73b0\u5b9e\u3001\u5ef6\u8fdf\uff09\u4ee5\u53ca\u4e25\u683c\u7684\u5b89\u5168\u4fdd\u8bc1\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u672c\u7efc\u8ff0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u8def\u7ebf\u56fe\uff0c\u5c55\u793a\u4e86\u5f53\u524d\u7684\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u673a\u9047\uff0c\u5f3a\u8c03\u4e86LLMs\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u9700\u8981\u8d1f\u8d23\u4efb\u7684\u521b\u65b0\u4ee5\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u667a\u80fd\u7684\u8fd0\u8f93\u7cfb\u7edf\u3002"}}
{"id": "2506.06291", "pdf": "https://arxiv.org/pdf/2506.06291", "abs": "https://arxiv.org/abs/2506.06291", "authors": ["Xiaoke Wang", "Batuhan Altundas", "Zhaoxin Li", "Aaron Zhao", "Matthew Gombolay"], "title": "Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "4 pages, 4 figures", "summary": "Mixed Integer Linear Programs (MILPs) are essential tools for solving\nplanning and scheduling problems across critical industries such as\nconstruction, manufacturing, and logistics. However, their widespread adoption\nis limited by long computational times, especially in large-scale, real-time\nscenarios. To address this, we present a learning-based framework that\nleverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph\nNeural Networks (GNNs), producing high-quality initial solutions for\nwarm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling\nProblems. Experimental results demonstrate that our method reduces optimization\ntime and variance compared to traditional techniques while maintaining solution\nquality and feasibility.", "AI": {"tldr": "The paper introduces a learning-based framework using Behavior Cloning and Reinforcement Learning to train Graph Neural Networks for producing initial solutions in MILP solvers, reducing optimization time and variance in multi-agent task allocation and scheduling problems.", "motivation": "To overcome the limitation of long computational times in solving large-scale Mixed Integer Linear Programs (MILPs), especially in real-time scenarios.", "method": "A learning-based framework that uses Behavior Cloning and Reinforcement Learning to train Graph Neural Networks (GNNs) to generate high-quality initial solutions for warm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling Problems.", "result": "Experimental results show reduced optimization time and variance compared to traditional techniques while maintaining solution quality and feasibility.", "conclusion": "The proposed method effectively addresses the computational challenges in solving large-scale MILPs, making it more suitable for real-time applications."}}
{"id": "2506.06409", "pdf": "https://arxiv.org/pdf/2506.06409", "abs": "https://arxiv.org/abs/2506.06409", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Chen-Fu Chen", "Haim Permuter", "Sajani Vithana", "Flavio P. Calmon"], "title": "HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Large language model (LLM) watermarks enable authentication of text\nprovenance, curb misuse of machine-generated text, and promote trust in AI\nsystems. Current watermarks operate by changing the next-token predictions\noutput by an LLM. The updated (i.e., watermarked) predictions depend on random\nside information produced, for example, by hashing previously generated tokens.\nLLM watermarking is particularly challenging in low-entropy generation tasks -\nsuch as coding - where next-token predictions are near-deterministic. In this\npaper, we propose an optimization framework for watermark design. Our goal is\nto understand how to most effectively use random side information in order to\nmaximize the likelihood of watermark detection and minimize the distortion of\ngenerated text. Our analysis informs the design of two new watermarks:\nHeavyWater and SimplexWater. Both watermarks are tunable, gracefully\ntrading-off between detection accuracy and text distortion. They can also be\napplied to any LLM and are agnostic to side information generation. We examine\nthe performance of HeavyWater and SimplexWater through several benchmarks,\ndemonstrating that they can achieve high watermark detection accuracy with\nminimal compromise of text generation quality, particularly in the low-entropy\nregime. Our theoretical analysis also reveals surprising new connections\nbetween LLM watermarking and coding theory. The code implementation can be\nfound in https://github.com/DorTsur/HeavyWater_SimplexWater", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6c34\u5370\u901a\u8fc7\u6539\u53d8\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\u6765\u9a8c\u8bc1\u6587\u672c\u6765\u6e90\uff0c\u51cf\u5c11\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u6ee5\u7528\uff0c\u5e76\u4fc3\u8fdb\u5bf9AI\u7cfb\u7edf\u7684\u4fe1\u4efb\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u6846\u67b6\u8bbe\u8ba1\u6c34\u5370\u65b9\u6cd5\uff0c\u4ee5\u6709\u6548\u5229\u7528\u968f\u673a\u4fe1\u606f\u5b9e\u73b0\u6700\u5927\u5316\u6c34\u5370\u68c0\u6d4b\u53ef\u80fd\u6027\u548c\u6700\u5c0f\u5316\u6587\u672c\u5931\u771f\u4e3a\u76ee\u6807\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u578b\u6c34\u5370\uff1aHeavyWater\u548cSimplexWater\uff0c\u5177\u6709\u53ef\u8c03\u6027\u548c\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "motivation": "\u5f53\u524dLLM\u6c34\u5370\u6280\u672f\u5728\u4f4e\u71b5\u751f\u6210\u4efb\u52a1\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u4f8b\u5982\u7f16\u7801\u4efb\u52a1\u4e2d\u7684\u8fd1\u4f3c\u786e\u5b9a\u6027\u9884\u6d4b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u6846\u67b6\u6765\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u6c34\u5370\u65b9\u6cd5\uff0c\u5e73\u8861\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u6587\u672c\u5931\u771f\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u4f18\u5316\u6846\u67b6\u7528\u4e8e\u6c34\u5370\u8bbe\u8ba1\uff0c\u5206\u6790\u5982\u4f55\u4f7f\u7528\u968f\u673a\u4fa7\u4fe1\u606f\u4ee5\u6700\u5927\u5316\u6c34\u5370\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\u5e76\u6700\u5c0f\u5316\u751f\u6210\u6587\u672c\u7684\u5931\u771f\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u5f00\u53d1\u4e86\u4e24\u79cd\u65b0\u6c34\u5370\u65b9\u6cd5\uff1aHeavyWater\u548cSimplexWater\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u53ef\u8c03\u8282\uff0c\u80fd\u591f\u5e73\u8861\u68c0\u6d4b\u7cbe\u5ea6\u4e0e\u6587\u672c\u5931\u771f\uff0c\u5e76\u9002\u7528\u4e8e\u4efb\u4f55LLM\u3002", "result": "HeavyWater\u548cSimplexWater\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5728\u4f4e\u71b5\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6c34\u5370\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5bf9\u6587\u672c\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd\u6781\u5c0f\u3002\u6b64\u5916\uff0c\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86LLM\u6c34\u5370\u548c\u7f16\u7801\u7406\u8bba\u4e4b\u95f4\u7684\u65b0\u8054\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u6c34\u5370\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6\u53caHeavyWater\u548cSimplexWater\u65b9\u6cd5\u4e3aLLM\u6c34\u5370\u6280\u672f\u63d0\u4f9b\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5728\u4fdd\u6301\u9ad8\u8d28\u91cf\u6587\u672c\u751f\u6210\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6c34\u5370\u68c0\u6d4b\u80fd\u529b\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.06324", "pdf": "https://arxiv.org/pdf/2506.06324", "abs": "https://arxiv.org/abs/2506.06324", "authors": ["Shruti Kumar", "Xiaoyu Chen", "Xiaomei Wang"], "title": "Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review", "categories": ["cs.AI"], "comment": "Abstract accepted to HFES 2024 Annual Meeting", "summary": "Several papers have delved into the challenges of human-AI-robot co-learning\nand co-adaptation. It has been noted that the terminology used to describe this\ncollaborative relationship in existing studies needs to be more consistent. For\nexample, the prefix \"co\" is used interchangeably to represent both\n\"collaborative\" and \"mutual,\" and the terms \"co-learning\" and \"co-adaptation\"\nare sometimes used interchangeably. However, they can reflect subtle\ndifferences in the focus of the studies. The current scoping review's primary\nresearch question (RQ1) aims to gather existing papers discussing this\ncollaboration pattern and examine the terms researchers use to describe this\nhuman-agent relationship. Given the relative newness of this area of study, we\nare also keen on exploring the specific types of intelligent agents and task\ndomains that have been considered in existing research (RQ2). This exploration\nis significant as it can shed light on the diversity of human-agent\ninteractions, from one-time to continuous learning/adaptation scenarios. It can\nalso help us understand the dynamics of human-agent interactions in different\ntask domains, guiding our expectations towards research situated in dynamic,\ncomplex domains. Our third objective (RQ3) is to investigate the cognitive\ntheories and frameworks that have been utilized in existing studies to measure\nhuman-agent co-learning and co-adaptation. This investigation is crucial as it\ncan help us understand the theoretical underpinnings of human-agent\ncollaboration and adaptation, and it can also guide us in identifying any new\nframeworks proposed specifically for this type of relationship.", "AI": {"tldr": "Several papers explore human-AI-robot co-learning and co-adaptation, but terminology lacks consistency. This scoping review aims to clarify terms (RQ1), explore types of intelligent agents and task domains studied (RQ2), and investigate cognitive theories and frameworks used in these studies (RQ3).", "motivation": "To address the inconsistent use of terminology in describing human-agent collaboration and to better understand the diversity of interactions and theoretical underpinnings.", "method": "Conduct a scoping review focusing on three research questions: RQ1 - gathering existing papers and examining terminology; RQ2 - exploring intelligent agents and task domains; RQ3 - investigating cognitive theories and frameworks.", "result": "Expected results include clearer definitions for terms like 'co-learning' and 'co-adaptation', an overview of various intelligent agents and task domains involved in human-agent interactions, and insights into cognitive theories guiding these studies.", "conclusion": "Clarifying terminology and understanding the theoretical basis of human-agent co-learning and co-adaptation will guide future research in dynamic, complex domains."}}
{"id": "2506.06292", "pdf": "https://arxiv.org/pdf/2506.06292", "abs": "https://arxiv.org/abs/2506.06292", "authors": ["Tianyuan Shi", "Canbin Huang", "Fanqi Wan", "Longguang Zhong", "Ziyi Yang", "Weizhou Shen", "Xiaojun Quan", "Ming Yan"], "title": "Mutual-Taught for Co-adapting Policy and Reward Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "During the preference optimization of large language models (LLMs),\ndistribution shifts may arise between newly generated model samples and the\ndata used to train the reward model (RM). This shift reduces the efficacy of\nthe RM, which in turn negatively impacts the performance of the policy model\n(PM). To address this challenge, we propose Mutual-Taught, a self-training\nmethod that iteratively improves both the PM and RM without requiring\nadditional human annotation. Our approach mirrors the expectation-maximization\n(EM) algorithm. In the E-step, the PM is updated using feedback from the\ncurrent RM, guiding the PM toward a better approximation of the latent optimal\npreference distribution. In the M-step, we update the RM by constructing\ntraining data from the outputs of the PM before and after the E-step update.\nThis process ensures that the RM adapts to the evolving policy distribution.\nExperimental results demonstrate that this iterative approach leads to\nconsistent improvements in both models. Specifically, our 8B policy model,\nLLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on\nAlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par\nwith GPT-4o-2024-08-06 on RewardBench.", "AI": {"tldr": "In this paper, a self-training method named Mutual-Taught is proposed to improve both policy model (PM) and reward model (RM) for large language models during preference optimization without additional human annotation.", "motivation": "The motivation of this paper is to solve the problem of distribution shifts between newly generated model samples and the data used to train the reward model (RM), which negatively impacts the performance of the policy model (PM).", "method": "Mutual-Taught is proposed, which mirrors the expectation-maximization (EM) algorithm. In the E-step, the PM is updated using feedback from the current RM. In the M-step, the RM is updated by constructing training data from the outputs of the PM before and after the E-step update.", "result": "Experimental results demonstrate that this iterative approach leads to consistent improvements in both models. Specifically, LLaMA-3-8B-Instruct-MT achieves a length-controlled win rate of 54.1% on AlpacaEval-2, while FsfairX-LLaMA3-RM-MT performs on par with GPT-4o-2024-08-06 on RewardBench.", "conclusion": "Mutual-Taught effectively improves both the policy model and reward model in large language models through an iterative process without requiring additional human annotation."}}
{"id": "2506.06414", "pdf": "https://arxiv.org/pdf/2506.06414", "abs": "https://arxiv.org/abs/2506.06414", "authors": ["Davis Brown", "Mahdi Sabbaghi", "Luze Sun", "Alexander Robey", "George J. Pappas", "Eric Wong", "Hamed Hassani"], "title": "Benchmarking Misuse Mitigation Against Covert Adversaries", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Existing language model safety evaluations focus on overt attacks and\nlow-stakes tasks. Realistic attackers can subvert current safeguards by\nrequesting help on small, benign-seeming tasks across many independent queries.\nBecause individual queries do not appear harmful, the attack is hard to\n{detect}. However, when combined, these fragments uplift misuse by helping the\nattacker complete hard and dangerous tasks. Toward identifying defenses against\nsuch strategies, we develop Benchmarks for Stateful Defenses (BSD), a data\ngeneration pipeline that automates evaluations of covert attacks and\ncorresponding defenses. Using this pipeline, we curate two new datasets that\nare consistently refused by frontier models and are too difficult for weaker\nopen-weight models. Our evaluations indicate that decomposition attacks are\neffective misuse enablers, and highlight stateful defenses as a countermeasure.", "AI": {"tldr": "The paper addresses the issue of covert attacks in language models that can be conducted through seemingly harmless individual queries, which when combined, help attackers complete dangerous tasks. To tackle this, they developed Benchmarks for Stateful Defenses (BSD), a data generation pipeline for evaluating such attacks and defenses. Evaluations reveal decomposition attacks as effective misuse enablers, emphasizing stateful defenses as a countermeasure.", "motivation": "Current safety evaluations for language models mainly focus on overt attacks and low-stakes tasks, ignoring more subtle threats where attackers use multiple small, independent, and benign-seeming tasks to bypass safeguards.", "method": "The authors developed Benchmarks for Stateful Defenses (BSD), a data generation pipeline to automate evaluations of covert attacks and their corresponding defenses. They used BSD to create two new datasets that are refused by advanced models and too complex for less capable open-weight models.", "result": "Evaluations using the BSD pipeline indicate that decomposition attacks are effective in enabling misuse of language models, demonstrating the necessity and effectiveness of stateful defenses.", "conclusion": "The study highlights the potential threat of covert attacks in language models and suggests stateful defenses as an important countermeasure against such strategies."}}
{"id": "2506.06326", "pdf": "https://arxiv.org/pdf/2506.06326", "abs": "https://arxiv.org/abs/2506.06326", "authors": ["Jiazheng Kang", "Mingming Ji", "Zhe Zhao", "Ting Bai"], "title": "Memory OS of AI Agent", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face a crucial challenge from fixed context\nwindows and inadequate memory management, leading to a severe shortage of\nlong-term memory capabilities and limited personalization in the interactive\nexperience with AI agents. To overcome this challenge, we innovatively propose\na Memory Operating System, i.e., MemoryOS, to achieve comprehensive and\nefficient memory management for AI agents. Inspired by the memory management\nprinciples in operating systems, MemoryOS designs a hierarchical storage\narchitecture and consists of four key modules: Memory Storage, Updating,\nRetrieval, and Generation. Specifically, the architecture comprises three\nlevels of storage units: short-term memory, mid-term memory, and long-term\npersonal memory. Key operations within MemoryOS include dynamic updates between\nstorage units: short-term to mid-term updates follow a dialogue-chain-based\nFIFO principle, while mid-term to long-term updates use a segmented page\norganization strategy. Our pioneering MemoryOS enables hierarchical memory\nintegration and dynamic updating. Extensive experiments on the LoCoMo benchmark\nshow an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the\nbaselines on GPT-4o-mini, showing contextual coherence and personalized memory\nretention in long conversations. The implementation code is open-sourced at\nhttps://github.com/BAI-LAB/MemoryOS.", "AI": {"tldr": "Large Language Models (LLMs) suffer from fixed context windows and inadequate memory management, resulting in limited long-term memory and personalization. To address this, the authors propose MemoryOS, a Memory Operating System inspired by traditional OS memory management principles. It features a hierarchical storage architecture with short-term, mid-term, and long-term memory units, along with key modules for storage, updating, retrieval, and generation. Experiments on LoCoMo benchmark demonstrate significant improvements in F1 and BLEU-1 scores over baselines.", "motivation": "To overcome the challenges faced by LLMs due to fixed context windows and insufficient memory management, leading to limited long-term memory and personalization.", "method": "Propose MemoryOS with a hierarchical storage architecture consisting of short-term, mid-term, and long-term memory units. Key operations include dynamic updates between storage units following dialogue-chain-based FIFO principle for short-term to mid-term updates and segmented page organization strategy for mid-term to long-term updates.", "result": "Experiments on LoCoMo benchmark show an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over baselines on GPT-4o-mini, indicating enhanced contextual coherence and personalized memory retention in long conversations.", "conclusion": "MemoryOS enables comprehensive and efficient memory management for AI agents, significantly improving their performance in terms of contextual coherence and personalized memory retention."}}
{"id": "2506.06293", "pdf": "https://arxiv.org/pdf/2506.06293", "abs": "https://arxiv.org/abs/2506.06293", "authors": ["Junyi Liu", "Stanley Kok"], "title": "Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "WITS 2024 (Workshop on Information Technologies and Systems 2024)", "summary": "Agencies such as Standard & Poor's and Moody's provide bank credit ratings\nthat influence economic stability and decision-making by stakeholders. Accurate\nand timely predictions support informed decision-making, regulatory actions,\nand investor protection. However, a complete interbank connection graph is\noften unavailable due to privacy concerns, complicating the direct application\nof Graph Neural Networks (GNNs) for rating prediction. our research utilizes\npersistent homology to construct a network that captures relationships among\nbanks and combines this with a traditional lending network to create a\nheterogeneous network that integrates information from both sources, leading to\nimproved predictions. Experiments on a global, real-world dataset validate the\neffectiveness of HTGNN. This research has implications for investors and\nregulatory bodies in enhancing proactive risk mitigation and the implementation\nof effective market interventions.The code can be find at\nhttps://github.com/Liu-Jun-Yi/HTGNN.", "AI": {"tldr": "Agencies like Standard & Poor's and Moody's offer bank credit ratings crucial for economic stability. Predicting these ratings is challenging due to privacy concerns affecting interbank connection data. This study uses persistent homology to build a network capturing bank relationships, which is combined with a traditional lending network to form a heterogeneous network for improved rating predictions through HTGNN.", "motivation": "To address the challenge of predicting bank credit ratings without complete interbank connection data, which is essential for informed decision-making, regulatory actions, and investor protection.", "method": "The research employs persistent homology to construct a network that captures relationships among banks. This network is then integrated with a traditional lending network to create a heterogeneous network that combines information from both sources.", "result": "Experiments conducted on a global, real-world dataset confirm the effectiveness of HTGNN in enhancing prediction accuracy.", "conclusion": "This approach has significant implications for investors and regulatory bodies, enabling better proactive risk mitigation and effective market interventions."}}
{"id": "2506.06518", "pdf": "https://arxiv.org/pdf/2506.06518", "abs": "https://arxiv.org/abs/2506.06518", "authors": ["Neil Fendley", "Edward W. Staley", "Joshua Carney", "William Redman", "Marie Chau", "Nathan Drenkow"], "title": "A Systematic Review of Poisoning Attacks Against Large Language Models", "categories": ["cs.CR", "cs.LG"], "comment": "28 Pages including number", "summary": "With the widespread availability of pretrained Large Language Models (LLMs)\nand their training datasets, concerns about the security risks associated with\ntheir usage has increased significantly. One of these security risks is the\nthreat of LLM poisoning attacks where an attacker modifies some part of the LLM\ntraining process to cause the LLM to behave in a malicious way. As an emerging\narea of research, the current frameworks and terminology for LLM poisoning\nattacks are derived from earlier classification poisoning literature and are\nnot fully equipped for generative LLM settings. We conduct a systematic review\nof published LLM poisoning attacks to clarify the security implications and\naddress inconsistencies in terminology across the literature. We propose a\ncomprehensive poisoning threat model applicable to categorize a wide range of\nLLM poisoning attacks. The poisoning threat model includes four poisoning\nattack specifications that define the logistics and manipulation strategies of\nan attack as well as six poisoning metrics used to measure key characteristics\nof an attack. Under our proposed framework, we organize our discussion of\npublished LLM poisoning literature along four critical dimensions of LLM\npoisoning attacks: concept poisons, stealthy poisons, persistent poisons, and\npoisons for unique tasks, to better understand the current landscape of\nsecurity risks.", "AI": {"tldr": "With the rise of Large Language Models (LLMs), security risks like LLM poisoning attacks have become a major concern. This paper conducts a systematic review of published LLM poisoning attacks, proposes a comprehensive threat model to categorize these attacks, and discusses them along four critical dimensions.", "motivation": "To address the inconsistencies in terminology and lack of a proper framework for understanding LLM poisoning attacks in generative settings.", "method": "Systematic review of published LLM poisoning attacks, proposal of a comprehensive poisoning threat model with four attack specifications and six poisoning metrics, and organization of discussion along four critical dimensions of LLM poisoning attacks.", "result": "Clarification of security implications of LLM poisoning attacks, establishment of consistent terminology, and provision of a framework to better understand and categorize these attacks.", "conclusion": "The proposed threat model provides a solid foundation for future research on LLM poisoning attacks and enhances understanding of associated security risks."}}
{"id": "2506.06352", "pdf": "https://arxiv.org/pdf/2506.06352", "abs": "https://arxiv.org/abs/2506.06352", "authors": ["Christian Tarsney"], "title": "Will artificial agents pursue power by default?", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Researchers worried about catastrophic risks from advanced AI have argued\nthat we should expect sufficiently capable AI agents to pursue power over\nhumanity because power is a convergent instrumental goal, something that is\nuseful for a wide range of final goals. Others have recently expressed\nskepticism of these claims. This paper aims to formalize the concepts of\ninstrumental convergence and power-seeking in an abstract, decision-theoretic\nframework, and to assess the claim that power is a convergent instrumental\ngoal. I conclude that this claim contains at least an element of truth, but\nmight turn out to have limited predictive utility, since an agent's options\ncannot always be ranked in terms of power in the absence of substantive\ninformation about the agent's final goals. However, the fact of instrumental\nconvergence is more predictive for agents who have a good shot at attaining\nabsolute or near-absolute power.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5173\u6ce8\u9ad8\u7ea7AI\u5bf9\u4eba\u7c7b\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u8ba8\u8bba\u4e86AI\u8ffd\u6c42\u6743\u529b\u7684\u539f\u56e0\u53ca\u53ef\u80fd\u6027\u3002\u672c\u6587\u901a\u8fc7\u5f62\u5f0f\u5316\u5de5\u5177\u6536\u655b\u548c\u6743\u529b\u8ffd\u6c42\u7684\u6982\u5ff5\uff0c\u5728\u62bd\u8c61\u51b3\u7b56\u7406\u8bba\u6846\u67b6\u4e0b\u8bc4\u4f30\u6743\u529b\u4f5c\u4e3a\u5de5\u5177\u6027\u76ee\u6807\u7684\u6536\u655b\u6027\u3002\u7ed3\u8bba\u8868\u660e\u8be5\u4e3b\u5f20\u6709\u4e00\u5b9a\u771f\u5b9e\u6027\uff0c\u4f46\u9884\u6d4b\u6548\u7528\u53ef\u80fd\u6709\u9650\uff0c\u5c24\u5176\u5728\u7f3a\u4e4f\u4ee3\u7406\u6700\u7ec8\u76ee\u6807\u5177\u4f53\u4fe1\u606f\u65f6\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6709\u8f83\u5927\u673a\u4f1a\u83b7\u5f97\u7edd\u5bf9\u6216\u8fd1\u4f3c\u7edd\u5bf9\u6743\u529b\u7684\u4ee3\u7406\uff0c\u5de5\u5177\u6536\u655b\u7684\u4e8b\u5b9e\u66f4\u5177\u9884\u6d4b\u6027\u3002", "motivation": "\u62c5\u5fc3\u9ad8\u7ea7AI\u53ef\u80fd\u7ed9\u4eba\u7c7b\u5e26\u6765\u707e\u96be\u6027\u98ce\u9669\uff0c\u63a2\u8ba8\u4e3a\u4ec0\u4e48\u8db3\u591f\u5f3a\u5927\u7684AI\u4ee3\u7406\u53ef\u80fd\u4f1a\u8ffd\u6c42\u5bf9\u4eba\u7c7b\u7684\u6743\u529b\uff0c\u56e0\u4e3a\u6743\u529b\u662f\u4e00\u4e2a\u6536\u655b\u7684\u5de5\u5177\u6027\u76ee\u6807\uff0c\u5bf9\u5e7f\u6cdb\u7684\u6700\u7ec8\u76ee\u6807\u90fd\u6709\u7528\u3002", "method": "\u5728\u62bd\u8c61\u51b3\u7b56\u7406\u8bba\u6846\u67b6\u4e0b\u5f62\u5f0f\u5316\u5de5\u5177\u6536\u655b\u548c\u6743\u529b\u8ffd\u6c42\u7684\u6982\u5ff5\uff0c\u5e76\u8bc4\u4f30\u6743\u529b\u662f\u5426\u4e3a\u6536\u655b\u7684\u5de5\u5177\u6027\u76ee\u6807\u3002", "result": "\u6743\u529b\u4f5c\u4e3a\u5de5\u5177\u6027\u76ee\u6807\u7684\u6536\u655b\u6027\u4e3b\u5f20\u6709\u4e00\u5b9a\u771f\u5b9e\u6027\uff0c\u4f46\u9884\u6d4b\u6548\u7528\u53ef\u80fd\u6709\u9650\uff0c\u5c24\u5176\u662f\u5728\u65e0\u6cd5\u603b\u662f\u6839\u636e\u6743\u529b\u5bf9\u4ee3\u7406\u9009\u9879\u8fdb\u884c\u6392\u540d\u7684\u60c5\u51b5\u4e0b\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u6709\u673a\u4f1a\u83b7\u5f97\u7edd\u5bf9\u6216\u8fd1\u4f3c\u7edd\u5bf9\u6743\u529b\u7684\u4ee3\u7406\uff0c\u5de5\u5177\u6536\u655b\u7684\u4e8b\u5b9e\u66f4\u5177\u9884\u6d4b\u6027\u3002", "conclusion": "\u6743\u529b\u662f\u6536\u655b\u7684\u5de5\u5177\u6027\u76ee\u6807\u8fd9\u4e00\u4e3b\u5f20\u5305\u542b\u81f3\u5c11\u90e8\u5206\u771f\u7406\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u5173\u4e8e\u4ee3\u7406\u6700\u7ec8\u76ee\u6807\u7684\u5177\u4f53\u4fe1\u606f\uff0c\u5176\u9884\u6d4b\u6548\u7528\u53ef\u80fd\u6709\u9650\u3002\u5bf9\u4e8e\u6709\u673a\u4f1a\u83b7\u5f97\u7edd\u5bf9\u6216\u8fd1\u4f3c\u7edd\u5bf9\u6743\u529b\u7684\u4ee3\u7406\uff0c\u5de5\u5177\u6536\u655b\u7684\u4e8b\u5b9e\u66f4\u5177\u9884\u6d4b\u6027\u3002"}}
{"id": "2506.06294", "pdf": "https://arxiv.org/pdf/2506.06294", "abs": "https://arxiv.org/abs/2506.06294", "authors": ["Yunqing Liu", "Wenqi Fan", "Xiaoyong Wei", "Qing Li"], "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on.", "AI": {"tldr": "GLProtein is a new framework that integrates global structural similarity and local amino acid details for protein pre-training, showing superior performance in various bioinformatics tasks.", "motivation": "There is potential to further explore the integration of protein structural information beyond just 3D structures.", "method": "GLProtein combines protein-masked modelling with triplet structure similarity scoring, protein 3D distance encoding and substructure-based amino acid molecule encoding.", "result": "GLProtein outperforms previous methods in predicting protein-protein interaction, contact prediction, and other bioinformatics tasks.", "conclusion": "Structural information of proteins includes both local and global information, and GLProtein effectively utilizes this to enhance prediction accuracy and functional insights."}}
{"id": "2506.06530", "pdf": "https://arxiv.org/pdf/2506.06530", "abs": "https://arxiv.org/abs/2506.06530", "authors": ["Tao Zhang", "Yevgeniy Vorobeychik"], "title": "Breaking the Gaussian Barrier: Residual-PAC Privacy for Automatic Privatization", "categories": ["cs.CR"], "comment": null, "summary": "The Probably Approximately Correct (PAC) Privacy framework [1] provides a\npowerful instance-based methodology for certifying privacy in complex\ndata-driven systems. However, existing PAC Privacy algorithms rely on a\nGaussian mutual information upper bound. We show that this is in general too\nconservative: the upper bound obtained by these algorithms is tight if and only\nif the perturbed mechanism output is jointly Gaussian with independent Gaussian\nnoise. To address the inefficiency inherent in the Gaussian-based approach, we\nintroduce Residual PAC Privacy, an f-divergence-based measure that quantifies\nthe privacy remaining after adversarial inference. When instantiated with\nKullback-Leibler divergence, Residual-PAC Privacy is governed by conditional\nentropy. Moreover, we propose Stackelberg Residual-PAC (SR-PAC) privatization\nmechanisms for RPAC Privacy, a game-theoretic framework that selects optimal\nnoise distributions through convex bilevel optimization. Our approach achieves\ntight privacy budget utilization for arbitrary data distributions. Moreover, it\nnaturally composes under repeated mechanisms and provides provable privacy\nguarantees with higher statistical efficiency. Numerical experiments\ndemonstrate that SR-PAC certifies the target privacy budget while consistently\nimproving utility compared to existing methods.", "AI": {"tldr": "The paper introduces Residual PAC Privacy and Stackelberg Residual-PAC (SR-PAC) privatization mechanisms to improve privacy certification in data-driven systems.", "motivation": "Existing PAC Privacy algorithms rely on a Gaussian mutual information upper bound, which is too conservative and not always tight.", "method": "Introduce Residual PAC Privacy, an f-divergence-based measure, and Stackelberg Residual-PAC (SR-PAC) privatization mechanisms that use convex bilevel optimization to select optimal noise distributions.", "result": "Achieves tight privacy budget utilization for arbitrary data distributions, naturally composes under repeated mechanisms, and provides provable privacy guarantees with higher statistical efficiency. Numerical experiments show improved utility compared to existing methods.", "conclusion": "Residual PAC Privacy and SR-PAC privatization mechanisms offer a more efficient approach to certifying privacy in complex data-driven systems."}}
{"id": "2506.06367", "pdf": "https://arxiv.org/pdf/2506.06367", "abs": "https://arxiv.org/abs/2506.06367", "authors": ["Jiaxin Pan", "Mojtaba Nayyeri", "Osama Mohammed", "Daniel Hernandez", "Rongchuan Zhang", "Cheng Cheng", "Steffen Staab"], "title": "Towards Foundation Model on Temporal Knowledge Graph Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats\n(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform\nlink prediction tasks in transductive or semi-inductive settings, which means\nthe entities, relations, and temporal information in the test graph are fully\nor partially observed during training. Such reliance on seen elements during\ninference limits the models' ability to transfer to new domains and generalize\nto real-world scenarios. A central limitation is the difficulty in learning\nrepresentations for entities, relations, and timestamps that are transferable\nand not tied to dataset-specific vocabularies. To overcome these limitations,\nwe introduce the first fully-inductive approach to temporal knowledge graph\nlink prediction. Our model employs sinusoidal positional encodings to capture\nfine-grained temporal patterns and generates adaptive entity and relation\nrepresentations using message passing conditioned on both local and global\ntemporal contexts. Our model design is agnostic to temporal granularity and\ntime span, effectively addressing temporal discrepancies across TKGs and\nfacilitating time-aware structural information transfer. As a pretrained,\nscalable, and transferable model, POSTRA demonstrates strong zero-shot\nperformance on unseen temporal knowledge graphs, effectively generalizing to\nnovel entities, relations, and timestamps. Extensive theoretical analysis and\nempirical results show that a single pretrained model can improve zero-shot\nperformance on various inductive temporal reasoning scenarios, marking a\nsignificant step toward a foundation model for temporal KGs.", "AI": {"tldr": "The paper presents POSTRA, a fully-inductive approach for temporal knowledge graph link prediction that uses sinusoidal positional encodings and message passing to generate adaptive entity and relation representations. It demonstrates strong zero-shot performance on unseen data and generalizes well to novel entities, relations, and timestamps.", "motivation": "Existing Temporal Knowledge Graph Embedding (TKGE) models struggle with transferring to new domains and generalizing to real-world scenarios due to their reliance on seen elements during inference. There is a need for a model that can handle unseen entities, relations, and timestamps effectively.", "method": "POSTRA employs sinusoidal positional encodings to capture fine-grained temporal patterns and generates adaptive entity and relation representations through message passing conditioned on both local and global temporal contexts. The model design is agnostic to temporal granularity and time span, addressing temporal discrepancies across TKGs and facilitating time-aware structural information transfer.", "result": "POSTRA demonstrates strong zero-shot performance on unseen temporal knowledge graphs and effectively generalizes to novel entities, relations, and timestamps. Extensive theoretical analysis and empirical results show that a single pretrained model can improve zero-shot performance on various inductive temporal reasoning scenarios.", "conclusion": "POSTRA represents a significant step toward a foundation model for temporal KGs, offering a scalable and transferable solution for fully-inductive temporal knowledge graph link prediction."}}
{"id": "2506.06295", "pdf": "https://arxiv.org/pdf/2506.06295", "abs": "https://arxiv.org/abs/2506.06295", "authors": ["Zhiyuan Liu", "Yicun Yang", "Yaojie Zhang", "Junjie Chen", "Chang Zou", "Qingyuan Wei", "Shaobo Wang", "Linfeng Zhang"], "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Autoregressive Models (ARMs) have long dominated the landscape of Large\nLanguage Models. Recently, a new paradigm has emerged in the form of\ndiffusion-based Large Language Models (dLLMs), which generate text by\niteratively denoising masked segments. This approach has shown significant\nadvantages and potential. However, dLLMs suffer from high inference latency.\nTraditional ARM acceleration techniques, such as Key-Value caching, are\nincompatible with dLLMs due to their bidirectional attention mechanism. To\naddress this specific challenge, our work begins with a key observation that\ndLLM inference involves a static prompt and a partially dynamic response, where\nmost tokens remain stable across adjacent denoising steps. Based on this, we\npropose dLLM-Cache, a training-free adaptive caching framework that combines\nlong-interval prompt caching with partial response updates guided by feature\nsimilarity. This design enables efficient reuse of intermediate computations\nwithout compromising model performance. Extensive experiments on representative\ndLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1\nx speedup over standard inference without compromising output quality. Notably,\nour method brings dLLM inference latency close to that of ARMs under many\nsettings. Codes are provided in the supplementary material and will be released\npublicly on GitHub.", "AI": {"tldr": "dLLM-Cache\u662f\u4e00\u79cd\u9488\u5bf9\u6269\u6563\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u65e0\u8bad\u7ec3\u9002\u5e94\u6027\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u957f\u95f4\u9694\u63d0\u793a\u7f13\u5b58\u548c\u57fa\u4e8e\u7279\u5f81\u76f8\u4f3c\u6027\u7684\u90e8\u5206\u54cd\u5e94\u66f4\u65b0\uff0c\u5b9e\u73b0\u9ad8\u8fbe9.1\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u63a5\u8fd1\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u3002", "motivation": "\u6269\u6563\u578b\u5927\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5176\u9ad8\u63a8\u7406\u5ef6\u8fdf\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002\u4f20\u7edf\u52a0\u901f\u6280\u672f\u5982Key-Value\u7f13\u5b58\u56e0\u4e0edLLMs\u7684\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u4e0d\u517c\u5bb9\u800c\u5931\u6548\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3dLLMs\u7684\u63a8\u7406\u6548\u7387\u95ee\u9898\u3002", "method": "\u63d0\u51fadLLM-Cache\u6846\u67b6\uff0c\u5229\u7528dLLM\u63a8\u7406\u4e2d\u9759\u6001\u63d0\u793a\u548c\u90e8\u5206\u52a8\u6001\u54cd\u5e94\u7684\u7279\u70b9\uff0c\u7ed3\u5408\u957f\u95f4\u9694\u63d0\u793a\u7f13\u5b58\u4e0e\u57fa\u4e8e\u7279\u5f81\u76f8\u4f3c\u6027\u7684\u90e8\u5206\u54cd\u5e94\u66f4\u65b0\uff0c\u4ece\u800c\u9ad8\u6548\u590d\u7528\u4e2d\u95f4\u8ba1\u7b97\u7ed3\u679c\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u5e76\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cdLLM-Cache\u5728\u591a\u4e2a\u4ee3\u8868\u6027dLLMs\uff08\u5982LLaDA 8B\u548cDream 7B\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad89.1\u500d\u7684\u63a8\u7406\u52a0\u901f\uff0c\u4e14\u8f93\u51fa\u8d28\u91cf\u4e0d\u53d7\u5f71\u54cd\u3002\u5728\u8bb8\u591a\u8bbe\u7f6e\u4e0b\uff0c\u8be5\u65b9\u6cd5\u4f7fdLLM\u63a8\u7406\u5ef6\u8fdf\u63a5\u8fd1\u81ea\u56de\u5f52\u6a21\u578b\uff08ARMs\uff09\u3002", "conclusion": "dLLM-Cache\u4e3adLLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u63a8\u7406\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u8f93\u51fa\u3002\u8fd9\u4e00\u6210\u679c\u6709\u671b\u63a8\u52a8dLLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.06547", "pdf": "https://arxiv.org/pdf/2506.06547", "abs": "https://arxiv.org/abs/2506.06547", "authors": ["Daniel Cabarcas", "Giulia Gaggero", "Elisa Gorla"], "title": "The complexity of the SupportMinors Modeling for the MinRank Problem", "categories": ["cs.CR", "math.AC"], "comment": null, "summary": "In this note, we provide proven estimates for the complexity of the\nSupportMinors Modeling, mostly confirming the heuristic complexity estimates\ncontained in the original article.", "AI": {"tldr": "\u672c\u6587\u4e3b\u8981\u5bf9SupportMinors\u5efa\u6a21\u7684\u590d\u6742\u6027\u63d0\u4f9b\u4e86\u8bc1\u660e\u7684\u4f30\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u539f\u59cb\u6587\u7ae0\u4e2d\u7684\u542f\u53d1\u5f0f\u590d\u6742\u6027\u4f30\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u5bf9SupportMinors\u5efa\u6a21\u7684\u590d\u6742\u6027\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u8bc4\u4f30\uff0c\u5e76\u9a8c\u8bc1\u539f\u59cb\u6587\u7ae0\u4e2d\u7684\u590d\u6742\u6027\u4f30\u8ba1\u662f\u5426\u6b63\u786e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u548c\u8ba1\u7b97\uff0c\u5f97\u51faSupportMinors\u5efa\u6a21\u590d\u6742\u6027\u7684\u5177\u4f53\u4f30\u8ba1\u503c\u3002", "result": "\u63d0\u4f9b\u4e86\u5173\u4e8eSupportMinors\u5efa\u6a21\u590d\u6742\u6027\u7684\u8bc1\u660e\u4f30\u8ba1\uff0c\u786e\u8ba4\u4e86\u539f\u59cb\u6587\u7ae0\u4e2d\u7684\u590d\u6742\u6027\u4f30\u8ba1\u3002", "conclusion": "SupportMinors\u5efa\u6a21\u7684\u590d\u6742\u6027\u4e0e\u539f\u59cb\u6587\u7ae0\u4e2d\u7684\u542f\u53d1\u5f0f\u4f30\u8ba1\u57fa\u672c\u4e00\u81f4\uff0c\u590d\u6742\u6027\u5f97\u5230\u4e86\u7406\u8bba\u4e0a\u7684\u8bc1\u660e\u3002"}}
{"id": "2506.06470", "pdf": "https://arxiv.org/pdf/2506.06470", "abs": "https://arxiv.org/abs/2506.06470", "authors": ["Yanwei Ren", "Haotian Zhang", "Fuxiang Wu", "Jiayan Qiu", "Jiaxing Huang", "Baosheng Yu", "Liu Liu"], "title": "SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Enhancing large language models by simply scaling up datasets has begun to\nyield diminishing returns, shifting the spotlight to data quality. Monte Carlo\nTree Search (MCTS) has emerged as a powerful technique for generating\nhigh-quality chain-of-thought data, yet conventional approaches typically\nretain only the top-scoring trajectory from the search tree, discarding sibling\nnodes that often contain valuable partial insights, recurrent error patterns,\nand alternative reasoning strategies. This unconditional rejection of\nnon-optimal reasoning branches may waste vast amounts of informative data in\nthe whole search tree. We propose SIGMA (Sibling Guided Monte Carlo\nAugmentation), a novel framework that reintegrates these discarded sibling\nnodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes\nalong each search path and applies a two-stage refinement: a critique model\nidentifies overlooked strengths and weaknesses across the sibling set, and a\nrevision model conducts text-based backpropagation to refine the top-scoring\ntrajectory in light of this comparative feedback. By recovering and amplifying\nthe underutilized but valuable signals from non-optimal reasoning branches,\nSIGMA substantially improves reasoning trajectories. On the challenging MATH\nbenchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K\nsamples, outperforming state-of-the-art models trained on 590K samples. This\nresult highlights that our sibling-guided optimization not only significantly\nreduces data usage but also significantly boosts LLM reasoning.", "AI": {"tldr": "Enhancing LLM reasoning via reusing discarded data from MCTS with SIGMA framework, achieving better performance with less data.", "motivation": "Conventional methods of improving LLMs by simply scaling up datasets have diminishing returns, emphasizing the need for high-quality data. MCTS generates valuable chain-of-thought data but discards non-optimal reasoning branches that contain useful insights.", "method": "SIGMA reintegrates discarded sibling nodes from MCTS to refine LLM reasoning through semantic linking and a two-stage refinement process involving critique and revision models.", "result": "On the MATH benchmark, SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K samples, surpassing state-of-the-art models trained on 590K samples.", "conclusion": "Sibling-guided optimization in SIGMA significantly reduces data usage and boosts LLM reasoning."}}
{"id": "2506.06296", "pdf": "https://arxiv.org/pdf/2506.06296", "abs": "https://arxiv.org/abs/2506.06296", "authors": ["Hanaa El Afia", "Said Ohamouddou", "Raddouane Chiheb", "Abdellatif El Afia"], "title": "Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph\nConvolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks\n(KAN) for the classification of three-dimensional point clouds. This method\nreplaces Multi-Layer Perceptron (MLP) layers with adaptable univariate\npolynomial expansions within a streamlined DGCNN architecture, circumventing\ndeep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In\ncomparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi\npolynomials outperform the traditional linear layer-based DGCNN baseline in\nterms of accuracy and convergence speed, while maintaining parameter\nefficiency. Our results demonstrate that higher polynomial degrees do not\nautomatically improve performance, highlighting the need for further\ntheoretical and empirical investigation to fully understand the interactions\nbetween polynomial bases, degrees, and the mechanisms of graph-based learning.", "AI": {"tldr": "The paper presents Jacobi-KAN-DGCNN, a framework for 3D point cloud classification that integrates DGCNN with KAN, replacing MLP layers with polynomial expansions. Experiments show KAN layers with Jacobi polynomials outperform traditional DGCNN in accuracy and convergence speed while maintaining parameter efficiency.", "motivation": "To improve the classification of three-dimensional point clouds by integrating Dynamic Graph Convolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks (KAN), aiming for better accuracy and convergence speed.", "method": "Replacing Multi-Layer Perceptron (MLP) layers with adaptable univariate polynomial expansions within a streamlined DGCNN architecture, allowing layer-by-layer comparison without deep levels for both MLP and KAN.", "result": "KAN layers using Jacobi polynomials outperformed the traditional linear layer-based DGCNN baseline in terms of accuracy and convergence speed while maintaining parameter efficiency. However, higher polynomial degrees do not automatically enhance performance.", "conclusion": "Higher polynomial degrees do not guarantee improved performance; further theoretical and empirical investigation is needed to understand the interactions between polynomial bases, degrees, and graph-based learning mechanisms."}}
{"id": "2506.06565", "pdf": "https://arxiv.org/pdf/2506.06565", "abs": "https://arxiv.org/abs/2506.06565", "authors": ["Emilia Rivas", "Sabrina Saika", "Ahtesham Bakht", "Aritran Piplai", "Nathaniel D. Bastian", "Ankit Shah"], "title": "Adapting Under Fire: Multi-Agent Reinforcement Learning for Adversarial Drift in Network Security", "categories": ["cs.CR"], "comment": "In Proceedings of the 22nd International Conference on Security and\n  Cryptography, ISBN 978-989-758-760-3, ISSN 2184-7711, pages 547-554", "summary": "Evolving attacks are a critical challenge for the long-term success of\nNetwork Intrusion Detection Systems (NIDS). The rise of these changing patterns\nhas exposed the limitations of traditional network security methods. While\nsignature-based methods are used to detect different types of attacks, they\noften fail to detect unknown attacks. Moreover, the system requires frequent\nupdates with new signatures as the attackers are constantly changing their\ntactics. In this paper, we design an environment where two agents improve their\npolicies over time. The adversarial agent, referred to as the red agent,\nperturbs packets to evade the intrusion detection mechanism, whereas the blue\nagent learns new defensive policies using drift adaptation techniques to\ncounter the attacks. Both agents adapt iteratively: the red agent responds to\nthe evolving NIDS, while the blue agent adjusts to emerging attack patterns. By\nstudying the model's learned policy, we offer concrete insights into drift\nadaptation techniques with high utility. Experiments show that the blue agent\nboosts model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to\n30 samples each.", "AI": {"tldr": "The paper addresses the challenge of evolving attacks in Network Intrusion Detection Systems (NIDS) by designing an environment where two agents, red and blue, improve their policies over time.", "motivation": "Evolving attacks are a critical challenge for the long-term success of NIDS. Traditional methods often fail to detect unknown attacks and require frequent updates.", "method": "The paper designs an environment with two agents: the red agent perturbs packets to evade detection, while the blue agent learns new defensive policies using drift adaptation techniques. Both agents adapt iteratively.", "result": "Experiments demonstrate that the blue agent can boost model accuracy by 30% with just 2 to 3 adaptation steps using only 25 to 30 samples each.", "conclusion": "By studying the learned policy of the model, the paper provides valuable insights into drift adaptation techniques with high utility."}}
{"id": "2506.06523", "pdf": "https://arxiv.org/pdf/2506.06523", "abs": "https://arxiv.org/abs/2506.06523", "authors": ["Sumanth Pillella"], "title": "Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility", "categories": ["cs.AI"], "comment": "6 pages", "summary": "In an era of escalating supply chain demands, SAP Logistics Execution (LE) is\npivotal for managing warehouse operations, transportation, and delivery. This\nresearch introduces a pioneering framework leveraging reinforcement learning\n(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing\noperational agility and efficiency. By modeling warehouse processes as dynamic\nenvironments, the framework optimizes task allocation, inventory movement, and\norder picking in real-time. A synthetic dataset of 300,000 LE transactions\nsimulates real-world warehouse scenarios, including multilingual data and\noperational disruptions. The analysis achieves 95% task optimization accuracy,\nreducing processing times by 60% compared to traditional methods.\nVisualizations, including efficiency heatmaps and performance graphs, guide\nagile warehouse strategies. This approach tackles data privacy, scalability,\nand SAP integration, offering a transformative solution for modern supply\nchains.", "AI": {"tldr": "The paper presents a novel framework using reinforcement learning (RL) to improve warehouse operations in SAP Logistics Execution (LE). It achieves 95% task optimization accuracy and reduces processing times by 60%.", "motivation": "To address the increasing demands of supply chains, this research aims to enhance the operational agility and efficiency of SAP LE through autonomous orchestration of warehouse tasks.", "method": "By modeling warehouse processes as dynamic environments, the framework uses reinforcement learning to optimize task allocation, inventory movement, and order picking in real-time. The method is tested on a synthetic dataset of 300,000 LE transactions that simulate real-world scenarios.", "result": "The analysis results in 95% task optimization accuracy and shows a 60% reduction in processing times compared to traditional methods. Visualizations like efficiency heatmaps and performance graphs are used to guide agile strategies.", "conclusion": "This approach effectively addresses data privacy, scalability, and SAP integration issues, providing a transformative solution for modern supply chains."}}
{"id": "2506.06297", "pdf": "https://arxiv.org/pdf/2506.06297", "abs": "https://arxiv.org/abs/2506.06297", "authors": ["Bozhi Sun", "Seda Tierney", "Jeffrey A. Feinstein", "Frederick Damen", "Alison L. Marsden", "Daniele E. Schiavazzi"], "title": "Optimal patient allocation for echocardiographic assessments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scheduling echocardiographic exams in a hospital presents significant\nchallenges due to non-deterministic factors (e.g., patient no-shows, patient\narrival times, diverse exam durations, etc.) and asymmetric resource\nconstraints between fetal and non-fetal patient streams. To address these\nchallenges, we first conducted extensive pre-processing on one week of\noperational data from the Echo Laboratory at Stanford University's Lucile\nPackard Children's Hospital, to estimate patient no-show probabilities and\nderive empirical distributions of arrival times and exam durations. Based on\nthese inputs, we developed a discrete-event stochastic simulation model using\nSimPy, and integrate it with the open source Gymnasium Python library. As a\nbaseline for policy optimization, we developed a comparative framework to\nevaluate on-the-fly versus reservation-based allocation strategies, in which\ndifferent proportions of resources are reserved in advance. Considering a\nhospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2\nratio of fetal to non-fetal sonographers, we show that on-the-fly allocation\ngenerally yields better performance, more effectively adapting to patient\nvariability and resource constraints. Building on this foundation, we apply\nreinforcement learning (RL) to derive an approximated optimal dynamic\nallocation policy. This RL-based policy is benchmarked against the\nbest-performing rule-based strategies, allowing us to quantify their\ndifferences and provide actionable insights for improving echo lab efficiency\nthrough intelligent, data-driven resource management.", "AI": {"tldr": "The paper addresses scheduling challenges for echocardiographic exams by using pre-processed operational data to develop a stochastic simulation model. It compares allocation strategies and applies reinforcement learning to optimize resource management.", "motivation": "Scheduling echocardiographic exams faces challenges due to non-deterministic factors and asymmetric resource constraints.", "method": "Pre-processing operational data, developing a discrete-event stochastic simulation model with SimPy, integrating it with Gymnasium Python library, comparing allocation strategies, and applying reinforcement learning.", "result": "On-the-fly allocation generally performs better, and the RL-based policy outperforms rule-based strategies.", "conclusion": "Reinforcement learning provides actionable insights for improving echo lab efficiency through intelligent, data-driven resource management."}}
{"id": "2506.06572", "pdf": "https://arxiv.org/pdf/2506.06572", "abs": "https://arxiv.org/abs/2506.06572", "authors": ["Xubin Fang", "Rick S. Blum", "Ramesh Bharadwaj", "Brian M. Sadler"], "title": "Cyber Security of Sensor Systems for State Sequence Estimation: an AI Approach", "categories": ["cs.CR", "eess.SP"], "comment": null, "summary": "Sensor systems are extremely popular today and vulnerable to sensor data\nattacks. Due to possible devastating consequences, counteracting sensor data\nattacks is an extremely important topic, which has not seen sufficient study.\nThis paper develops the first methods that accurately identify/eliminate only\nthe problematic attacked sensor data presented to a sequence\nestimation/regression algorithm under a powerful attack model constructed based\non known/observed attacks. The approach does not assume a known form for the\nstatistical model of the sensor data, allowing data-driven and machine learning\nsequence estimation/regression algorithms to be protected. A simple protection\napproach for attackers not endowed with knowledge of the details of our\nprotection approach is first developed, followed by additional processing for\nattacks based on protection system knowledge. In the cases tested for which it\nwas designed, experimental results show that the simple approach achieves\nperformance indistinguishable, to two decimal places, from that for an approach\nwhich knows which sensors are attacked. For cases where the attacker has\nknowledge of the protection approach, experimental results indicate the\nadditional processing can be configured so that the worst-case degradation\nunder the additional processing and a large number of sensors attacked can be\nmade significantly smaller than the worst-case degradation of the simple\napproach, and close to an approach which knows which sensors are attacked, for\nthe same number of attacked sensors with just a slight degradation under no\nattacks. Mathematical descriptions of the worst-case attacks are used to\ndemonstrate the additional processing will provide similar advantages for cases\nfor which we do not have numerical results. All the data-driven processing used\nin our approaches employ only unattacked training data.", "AI": {"tldr": "This paper presents methods to accurately identify and eliminate attacked sensor data in sequence estimation/regression algorithms, without assuming a known statistical model of the sensor data. The approach is effective against attackers with or without knowledge of the protection system.", "motivation": "Sensor systems are vulnerable to attacks that can have devastating consequences, yet this area has not received sufficient attention.", "method": "The paper develops a two-step method: 1) A simple protection approach for attackers without knowledge of the details; 2) Additional processing for cases where attackers have knowledge of the protection system. Both steps use only unattacked training data in their data-driven processing.", "result": "Experimental results show that the simple approach performs as well as an approach that knows which sensors are attacked. With additional processing, the worst-case degradation under attacks can be significantly reduced compared to the simple approach, approaching the performance of knowing which sensors are attacked.", "conclusion": "The developed methods effectively protect sequence estimation/regression algorithms from sensor data attacks, even under powerful attack models, and demonstrate advantages through both experimental and mathematical analysis."}}
{"id": "2506.06524", "pdf": "https://arxiv.org/pdf/2506.06524", "abs": "https://arxiv.org/abs/2506.06524", "authors": ["Sam Earle", "Ahmed Khalifa", "Muhammad Umair Nasir", "Zehua Jiang", "Graham Todd", "Andrzej Banburski-Fahey", "Julian Togelius"], "title": "ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search", "categories": ["cs.AI", "cs.HC"], "comment": "5 pages, 3 figures, 3 tables, submitted to IEEE Conference on Games\n  as a Short Paper", "summary": "There is much interest in using large pre-trained models in Automatic Game\nDesign (AGD), whether via the generation of code, assets, or more abstract\nconceptualization of design ideas. But so far this interest largely stems from\nthe ad hoc use of such generative models under persistent human supervision.\nMuch work remains to show how these tools can be integrated into\nlonger-time-horizon AGD pipelines, in which systems interface with game engines\nto test generated content autonomously. To this end, we introduce ScriptDoctor,\na Large Language Model (LLM)-driven system for automatically generating and\ntesting games in PuzzleScript, an expressive but highly constrained description\nlanguage for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates\nand tests game design ideas in an iterative loop, where human-authored examples\nare used to ground the system's output, compilation errors from the\nPuzzleScript engine are used to elicit functional code, and search-based agents\nplay-test generated games. ScriptDoctor serves as a concrete example of the\npotential of automated, open-ended LLM-based workflows in generating novel game\ncontent.", "AI": {"tldr": "ScriptDoctor is a Large Language Model (LLM)-driven system that automatically generates and tests games in PuzzleScript, showing potential for automated LLM-based workflows in generating novel game content.", "motivation": "To integrate large pre-trained models into longer-time-horizon Automatic Game Design (AGD) pipelines, where systems can autonomously test generated content by interfacing with game engines.", "method": "ScriptDoctor uses an iterative loop to generate and test game design ideas. It leverages human-authored examples to ground the system's output, compilation errors from the PuzzleScript engine to produce functional code, and search-based agents to play-test generated games.", "result": "ScriptDoctor successfully generates and tests game design ideas autonomously, demonstrating the potential of automated, open-ended LLM-based workflows in creating novel game content.", "conclusion": "The introduction of ScriptDoctor provides a concrete example of how large language models can be utilized in automated game design processes, opening possibilities for further exploration in this area."}}
{"id": "2506.06298", "pdf": "https://arxiv.org/pdf/2506.06298", "abs": "https://arxiv.org/abs/2506.06298", "authors": ["Daniel Halpern", "Evi Micha", "Ariel D. Procaccia", "Itai Shapira"], "title": "Pairwise Calibrated Rewards for Pluralistic Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current alignment pipelines presume a single, universal notion of desirable\nbehavior. However, human preferences often diverge across users, contexts, and\ncultures. As a result, disagreement collapses into the majority signal and\nminority perspectives are discounted. To address this, we propose reflecting\ndiverse human preferences through a distribution over multiple reward\nfunctions, each inducing a distinct aligned policy. The distribution is learned\ndirectly from pairwise preference without annotator identifiers or predefined\ngroups. Instead, annotator disagreements are treated as informative soft\nlabels. Our central criterion is pairwise calibration: for every pair of\ncandidate responses, the proportion of reward functions preferring one response\nmatches the fraction of annotators with that preference. We prove that even a\nsmall outlier-free ensemble can accurately represent diverse preference\ndistributions. Empirically, we introduce and validate a practical training\nheuristic to learn such ensembles, and demonstrate its effectiveness through\nimproved calibration, implying a more faithful representation of pluralistic\nvalues.", "AI": {"tldr": "The paper proposes using a distribution of multiple reward functions to reflect diverse human preferences in alignment pipelines, rather than relying on a single universal standard. This approach treats annotator disagreements as informative and aims for pairwise calibration.", "motivation": "Current alignment methods assume a single universal desirable behavior, ignoring the diversity in human preferences across users, contexts, and cultures. This leads to minority perspectives being overlooked.", "method": "The authors suggest learning a distribution over multiple reward functions that induce distinct aligned policies. This is done directly from pairwise preferences without needing annotator identifiers or predefined groups. Disagreements among annotators are used as soft labels. The central criterion is pairwise calibration, ensuring that the proportion of reward functions preferring one response matches the fraction of annotators with that preference.", "result": "The authors prove theoretically that even a small ensemble can accurately represent diverse preference distributions if it is outlier-free. Empirically, they introduce a training heuristic to learn such ensembles and show its effectiveness through improved calibration, which better represents pluralistic values.", "conclusion": "Using a distribution of reward functions can more faithfully capture diverse human preferences, leading to better alignment in AI systems."}}
{"id": "2506.06597", "pdf": "https://arxiv.org/pdf/2506.06597", "abs": "https://arxiv.org/abs/2506.06597", "authors": ["Anuj Dubey", "Aydin Aysu"], "title": "Stochastic Training for Side-Channel Resilient AI", "categories": ["cs.CR"], "comment": null, "summary": "The confidentiality of trained AI models on edge devices is at risk from\nside-channel attacks exploiting power and electromagnetic emissions. This paper\nproposes a novel training methodology to enhance resilience against such\nthreats by introducing randomized and interchangeable model configurations\nduring inference. Experimental results on Google Coral Edge TPU show a\nreduction in side-channel leakage and a slower increase in t-scores over 20,000\ntraces, demonstrating robustness against adversarial observations. The defense\nmaintains high accuracy, with about 1% degradation in most configurations, and\nrequires no additional hardware or software changes, making it the only\napplicable solution for existing Edge TPUs.", "AI": {"tldr": "A new training method improves AI model security on edge devices against side-channel attacks without affecting performance or needing extra changes.", "motivation": "To protect the confidentiality of AI models on edge devices from side-channel attacks based on power and electromagnetic emissions.", "method": "Introduce a novel training methodology with randomized and interchangeable model configurations during inference to enhance resilience against side-channel threats.", "result": "Experimental results on Google Coral Edge TPU indicate reduced side-channel leakage, slower increase in t-scores over 20,000 traces, and only about 1% accuracy degradation in most configurations.", "conclusion": "This defense mechanism is effective, maintains high accuracy, and requires no additional hardware or software changes, making it suitable for existing Edge TPUs."}}
{"id": "2506.06574", "pdf": "https://arxiv.org/pdf/2506.06574", "abs": "https://arxiv.org/abs/2506.06574", "authors": ["Suhana Bedi", "Iddah Mlauzi", "Daniel Shin", "Sanmi Koyejo", "Nigam H. Shah"], "title": "The Optimization Paradox in Clinical AI Multi-Agent Systems", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent artificial intelligence systems are increasingly deployed in\nclinical settings, yet the relationship between component-level optimization\nand system-wide performance remains poorly understood. We evaluated this\nrelationship using 2,400 real patient cases from the MIMIC-CDM dataset across\nfour abdominal pathologies (appendicitis, pancreatitis, cholecystitis,\ndiverticulitis), decomposing clinical diagnosis into information gathering,\ninterpretation, and differential diagnosis. We evaluated single agent systems\n(one model performing all tasks) against multi-agent systems (specialized\nmodels for each task) using comprehensive metrics spanning diagnostic outcomes,\nprocess adherence, and cost efficiency. Our results reveal a paradox: while\nmulti-agent systems generally outperformed single agents, the\ncomponent-optimized or Best of Breed system with superior components and\nexcellent process metrics (85.5% information accuracy) significantly\nunderperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent\nsystem). This finding underscores that successful integration of AI in\nhealthcare requires not just component level optimization but also attention to\ninformation flow and compatibility between agents. Our findings highlight the\nneed for end to end system validation rather than relying on component metrics\nalone.", "AI": {"tldr": "\u5728\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u4e2d\uff0c\u5c3d\u7ba1\u5355\u72ec\u7ec4\u4ef6\u4f18\u5316\u7684\u6027\u80fd\u8f83\u597d\uff0c\u4f46\u6574\u4f53\u7cfb\u7edf\u7684\u8bca\u65ad\u51c6\u786e\u7387\u5374\u4e0d\u5982\u6700\u4f18\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u56e0\u6b64\u5728\u533b\u7597AI\u5e94\u7528\u4e2d\u9700\u8981\u91cd\u89c6\u6574\u4f53\u7cfb\u7edf\u9a8c\u8bc1\u800c\u975e\u5355\u7eaf\u4f9d\u8d56\u7ec4\u4ef6\u6307\u6807\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u7ec4\u4ef6\u7ea7\u4f18\u5316\u4e0e\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u4f7f\u7528MIMIC-CDM\u6570\u636e\u96c6\u4e2d\u76842400\u4e2a\u771f\u5b9e\u60a3\u8005\u6848\u4f8b\uff0c\u9488\u5bf9\u56db\u79cd\u8179\u90e8\u75c5\u7406\u8fdb\u884c\u5206\u6790\uff0c\u5c06\u4e34\u5e8a\u8bca\u65ad\u8fc7\u7a0b\u5206\u4e3a\u4fe1\u606f\u6536\u96c6\u3001\u89e3\u91ca\u548c\u9274\u522b\u8bca\u65ad\u4e09\u4e2a\u9636\u6bb5\uff0c\u5e76\u6bd4\u8f83\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u4e00\u4e2a\u6a21\u578b\u5b8c\u6210\u6240\u6709\u4efb\u52a1\uff09\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u6bcf\u4e2a\u4efb\u52a1\u6709\u4e13\u95e8\u6a21\u578b\uff09\u7684\u8868\u73b0\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u901a\u5e38\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u4f46\u5373\u4f7f\u5728\u7ec4\u4ef6\u4f18\u5316\u8f83\u597d\u7684\u7cfb\u7edf\u4e2d\uff0c\u8bca\u65ad\u51c6\u786e\u7387\u663e\u8457\u4f4e\u4e8e\u6700\u4f18\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0867.7% vs 77.4%\uff09\u3002", "conclusion": "\u533b\u7597AI\u7cfb\u7edf\u7684\u6210\u529f\u5b9e\u65bd\u4e0d\u4ec5\u9700\u8981\u7ec4\u4ef6\u7ea7\u522b\u7684\u4f18\u5316\uff0c\u8fd8\u9700\u8981\u5173\u6ce8\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u4fe1\u606f\u6d41\u52a8\u548c\u517c\u5bb9\u6027\uff0c\u5f3a\u8c03\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.06300", "pdf": "https://arxiv.org/pdf/2506.06300", "abs": "https://arxiv.org/abs/2506.06300", "authors": ["Yuanye Zhou", "Zhaokun Wang", "Kai Zhou", "Hui Tang", "Xiaofan Li"], "title": "LT-PINN: Lagrangian Topology-conscious Physics-informed Neural Network for Boundary-focused Engineering Optimization", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Physics-informed neural networks (PINNs) have emerged as a powerful meshless\ntool for topology optimization, capable of simultaneously determining optimal\ntopologies and physical solutions. However, conventional PINNs rely on\ndensity-based topology descriptions, which necessitate manual interpolation and\nlimit their applicability to complex geometries. To address this, we propose\nLagrangian topology-conscious PINNs (LT-PINNs), a novel framework for\nboundary-focused engineering optimization. By parameterizing the control\nvariables of topology boundary curves as learnable parameters, LT-PINNs\neliminate the need for manual interpolation and enable precise boundary\ndetermination. We further introduce specialized boundary condition loss\nfunction and topology loss function to ensure sharp and accurate boundary\nrepresentations, even for intricate topologies. The accuracy and robustness of\nLT-PINNs are validated via two types of partial differential equations (PDEs),\nincluding elastic equation with Dirichlet boundary conditions and Laplace's\nequation with Neumann boundary conditions. Furthermore, we demonstrate\neffectiveness of LT-PINNs on more complex time-dependent and time-independent\nflow problems without relying on measurement data, and showcase their\nengineering application potential in flow velocity rearrangement, transforming\na uniform upstream velocity into a sine-shaped downstream profile. The results\ndemonstrate (1) LT-PINNs achieve substantial reductions in relative L2 errors\ncompared with the state-of-art density topology-oriented PINNs (DT-PINNs), (2)\nLT-PINNs can handle arbitrary boundary conditions, making them suitable for a\nwide range of PDEs, and (3) LT-PINNs can infer clear topology boundaries\nwithout manual interpolation, especially for complex topologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6LT-PINNs\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u62d3\u6251\u8fb9\u754c\u66f2\u7ebf\u7684\u63a7\u5236\u53d8\u91cf\u4f5c\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u6d88\u9664\u4e86\u5bf9\u4eba\u5de5\u63d2\u503c\u7684\u9700\u6c42\uff0c\u5e76\u80fd\u591f\u7cbe\u786e\u786e\u5b9a\u8fb9\u754c\u3002\u901a\u8fc7\u4e24\u79cd\u7c7b\u578b\u7684\u504f\u5fae\u5206\u65b9\u7a0b\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u5728\u6d41\u52a8\u901f\u5ea6\u91cd\u65b0\u6392\u5217\u7b49\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u7684PINNs\u4f9d\u8d56\u4e8e\u57fa\u4e8e\u5bc6\u5ea6\u7684\u62d3\u6251\u63cf\u8ff0\uff0c\u8fd9\u9700\u8981\u624b\u52a8\u63d2\u503c\u5e76\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u4e2d\u7684\u5e94\u7528\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u51fa\u4e86Lagrangian\u62d3\u6251\u611f\u77e5PINNs\uff08LT-PINNs\uff09\u3002", "method": "\u901a\u8fc7\u5c06\u62d3\u6251\u8fb9\u754c\u66f2\u7ebf\u7684\u63a7\u5236\u53d8\u91cf\u53c2\u6570\u5316\u4e3a\u53ef\u5b66\u4e60\u53c2\u6570\uff0c\u5f15\u5165\u4e13\u95e8\u7684\u8fb9\u754c\u6761\u4ef6\u635f\u5931\u51fd\u6570\u548c\u62d3\u6251\u635f\u5931\u51fd\u6570\uff0c\u786e\u4fdd\u5373\u4f7f\u5bf9\u4e8e\u590d\u6742\u7684\u62d3\u6251\u7ed3\u6784\u4e5f\u80fd\u5b9e\u73b0\u6e05\u6670\u548c\u51c6\u786e\u7684\u8fb9\u754c\u8868\u793a\u3002", "result": "(1) \u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5bc6\u5ea6\u62d3\u6251\u7684PINNs\uff08DT-PINNs\uff09\u76f8\u6bd4\uff0cLT-PINNs\u663e\u8457\u964d\u4f4e\u4e86\u76f8\u5bf9L2\u8bef\u5dee\uff1b(2) LT-PINNs\u53ef\u4ee5\u5904\u7406\u4efb\u610f\u8fb9\u754c\u6761\u4ef6\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684PDEs\uff1b(3) LT-PINNs\u53ef\u4ee5\u5728\u65e0\u9700\u4eba\u5de5\u63d2\u503c\u7684\u60c5\u51b5\u4e0b\u63a8\u65ad\u51fa\u6e05\u6670\u7684\u62d3\u6251\u8fb9\u754c\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u590d\u6742\u62d3\u6251\u3002", "conclusion": "LT-PINNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u4e13\u6ce8\u4e8e\u8fb9\u754c\u7684\u5de5\u7a0b\u4f18\u5316\u6846\u67b6\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u5904\u7406\u590d\u6742\u62d3\u6251\u3002"}}
{"id": "2506.06604", "pdf": "https://arxiv.org/pdf/2506.06604", "abs": "https://arxiv.org/abs/2506.06604", "authors": ["Armin Sarabi", "Manish Karir", "Mingyan Liu"], "title": "Scoring the Unscorables: Cyber Risk Assessment Beyond Internet Scans", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "In this paper we present a study on using novel data types to perform cyber\nrisk quantification by estimating the likelihood of a data breach. We\ndemonstrate that it is feasible to build a highly accurate cyber risk\nassessment model using public and readily available technology signatures\nobtained from crawling an organization's website. This approach overcomes the\nlimitations of previous similar approaches that relied on large-scale IP\naddress based scanning data, which suffers from incomplete/missing IP address\nmappings as well as the lack of such data for large numbers of small and\nmedium-sized organizations (SMEs). In comparison to scan data, technology\ndigital signature data is more readily available for millions of SMEs. Our\nstudy shows that there is a strong relationship between these technology\nsignatures and an organization's cybersecurity posture. In cross-validating our\nmodel using different cyber incident datasets, we also highlight the key\ndifferences between ransomware attack victims and the larger population of\ncyber incident and data breach victims.", "AI": {"tldr": "The paper presents a study on using novel data types to perform cyber risk quantification by estimating the likelihood of a data breach, showing strong relationship between technology signatures and cybersecurity posture.", "motivation": "To overcome limitations of previous approaches that relied on large-scale IP address based scanning data, which suffers from incomplete/missing IP address mappings as well as the lack of such data for SMEs.", "method": "Building a highly accurate cyber risk assessment model using public and readily available technology signatures obtained from crawling an organization's website.", "result": "Demonstrated feasibility of the model, highlighted key differences between ransomware attack victims and larger population of cyber incident and data breach victims.", "conclusion": "Technology digital signature data is more readily available for millions of SMEs and has strong relationship with organizations' cybersecurity posture."}}
{"id": "2506.06580", "pdf": "https://arxiv.org/pdf/2506.06580", "abs": "https://arxiv.org/abs/2506.06580", "authors": ["Xiaoran Liu", "Istvan David"], "title": "AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture", "categories": ["cs.AI", "cs.ET", "cs.SE", "cs.SY", "eess.SY"], "comment": null, "summary": "Insufficient data volume and quality are particularly pressing challenges in\nthe adoption of modern subsymbolic AI. To alleviate these challenges, AI\nsimulation uses virtual training environments in which AI agents can be safely\nand efficiently developed with simulated, synthetic data. Digital twins open\nnew avenues in AI simulation, as these high-fidelity virtual replicas of\nphysical systems are equipped with state-of-the-art simulators and the ability\nto further interact with the physical system for additional data collection. In\nthis article, we report on our systematic survey of digital twin-enabled AI\nsimulation. By analyzing 22 primary studies, we identify technological trends\nand derive a reference framework to situate digital twins and AI components.\nBased on our findings, we derive a reference framework and provide\narchitectural guidelines by mapping it onto the ISO 23247 reference\narchitecture for digital twins. Finally, we identify challenges and research\nopportunities for prospective researchers.", "AI": {"tldr": "This paper conducts a systematic survey of digital twin-enabled AI simulation, identifying technological trends and deriving a reference framework based on 22 primary studies. It maps the framework to ISO 23247 architecture, offering architectural guidelines and highlighting challenges and research opportunities.", "motivation": "Insufficient data volume and quality pose significant challenges in adopting modern subsymbolic AI. The need for safe and efficient AI development environments leads to exploring digital twin-enabled AI simulation.", "method": "The authors performed a systematic analysis of 22 primary studies related to digital twin-enabled AI simulation. They identified technological trends, derived a reference framework, and mapped it onto the ISO 23247 reference architecture.", "result": "A reference framework for digital twin-enabled AI simulation was established, along with architectural guidelines. Challenges and research opportunities in this field were also identified.", "conclusion": "Digital twins offer promising advancements in AI simulation by providing high-fidelity virtual replicas and advanced simulators. This study provides a foundation for future research in integrating digital twins with AI components."}}
{"id": "2506.06303", "pdf": "https://arxiv.org/pdf/2506.06303", "abs": "https://arxiv.org/abs/2506.06303", "authors": ["Kefan Song", "Amir Moeini", "Peng Wang", "Lei Gong", "Rohan Chandra", "Yanjun Qi", "Shangtong Zhang"], "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) is a human-designed framework for solving\nsequential decision making problems. In this work, we demonstrate that,\nsurprisingly, RL emerges in LLM's (Large Language Model) inference time -- a\nphenomenon known as in-context RL (ICRL). Specifically, we propose a novel\nmulti-round prompting framework called ICRL prompting. The goal is to prompt\nthe LLM to complete a task. After the LLM generates a response at the current\nround, we give numerical scalar feedbacks for the response, called the rewards.\nAt the next round, we prompt the LLM again with the same task and a context\nconsisting of all previous responses and rewards. We observe that the quality\nof the LLM's response increases as the context grows. In other words, the LLM\nis able to maximize the scalar reward signal in the inference time, just like\nan RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,\ncreative writing, and ScienceWorld) and demonstrate significant performance\nimprovements over baseline methods such as Self-Refine and Reflexion.\nSurprisingly, in some experiments the reward signals are generated by the LLM\nitself, yet performance improvements are still observed from ICRL prompting,\noffering a promising paradigm for scaling test-time compute.", "AI": {"tldr": "\u901a\u8fc7\u591a\u8f6e\u63d0\u793a\u6846\u67b6\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5316\u5b66\u4e60\u7279\u6027\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a8\u7406\u65f6\u95f4\u5185\u7684\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8f6e\u63d0\u793a\u6846\u67b6\uff0c\u4ee5\u671f\u63d0\u5347LLM\u5b8c\u6210\u4efb\u52a1\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u540d\u4e3aICRL\u63d0\u793a\u7684\u591a\u8f6e\u63d0\u793a\u6846\u67b6\uff0c\u9996\u5148\u8ba9LLM\u751f\u6210\u54cd\u5e94\uff0c\u7136\u540e\u7ed9\u4e88\u6570\u503c\u5956\u52b1\u53cd\u9988\uff0c\u5728\u4e0b\u4e00\u8f6e\u4e2d\u518d\u6b21\u63d0\u793aLLM\u5b8c\u6210\u76f8\u540c\u4efb\u52a1\uff0c\u540c\u65f6\u63d0\u4f9b\u4e4b\u524d\u7684\u6240\u6709\u54cd\u5e94\u548c\u5956\u52b1\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08Game of 24\u3001\u521b\u610f\u5199\u4f5c\u548cScienceWorld\uff09\u4e2d\uff0cICRL\u63d0\u793a\u8868\u73b0\u51fa\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982Self-Refine\u548cReflexion\uff09\u66f4\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002\u5373\u4f7f\u5728\u67d0\u4e9b\u5b9e\u9a8c\u4e2d\u5956\u52b1\u4fe1\u53f7\u7531LLM\u81ea\u8eab\u751f\u6210\uff0c\u4ecd\u7136\u89c2\u5bdf\u5230\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u5728\u63a8\u7406\u65f6\u95f4\u5185\u5c55\u73b0\u5f3a\u5316\u5b66\u4e60\u80fd\u529b\uff0cICRL\u63d0\u793a\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u9ad8LLM\u7684\u4efb\u52a1\u5b8c\u6210\u8d28\u91cf\uff0c\u4e3a\u6269\u5c55\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u8303\u4f8b\u3002"}}
{"id": "2506.06635", "pdf": "https://arxiv.org/pdf/2506.06635", "abs": "https://arxiv.org/abs/2506.06635", "authors": ["Ayan Roy", "Jeetkumar Patel", "Rik Chakraborti", "Shudip Datta"], "title": "TrustConnect: An In-Vehicle Anomaly Detection Framework through Topology-Based Trust Rating", "categories": ["cs.CR"], "comment": "To Appear in 2025 the IEEE 101st Vehicular Technology Conference:\n  VTC2025-Spring", "summary": "Modern vehicles are equipped with numerous in-vehicle components that\ninteract with the external environment through remote communications and\nservices, such as Bluetooth and vehicle-to-infrastructure communication. These\ncomponents form a network, exchanging information to ensure the proper\nfunctioning of the vehicle. However, the presence of false or fabricated\ninformation can disrupt the vehicle's performance. Given that these components\nare interconnected, erroneous data can propagate throughout the network,\npotentially affecting other components and leading to catastrophic\nconsequences. To address this issue, we propose TrustConnect, a framework\ndesigned to assess the trustworthiness of a vehicle's in-vehicle network by\nevaluating the trust levels of individual components under various network\nconfigurations. The proposed framework leverages the interdependency of all the\nvehicle's components, along with the correlation of their values and their\nvulnerability to remote injection based on the outside exposure of each\ncomponent, to determine the reliability of the in-vehicle network. The\neffectiveness of the proposed framework has been validated through programming\nsimulations conducted across various scenarios using a random distribution of\nan in-vehicle network graph generated with the Networkx package in Python.", "AI": {"tldr": "In modern vehicles, interconnected in-vehicle components can be affected by false information leading to serious consequences. The paper proposes TrustConnect framework that evaluates the trustworthiness of in-vehicle networks by assessing individual component's trust levels considering their interdependency, value correlation and vulnerability to remote injection.", "motivation": "Modern vehicles' in-vehicle components interact with the external environment through remote communications and services. False or fabricated information could propagate throughout the network, affecting other components and potentially causing catastrophic consequences.", "method": "Propose TrustConnect, a framework that assesses the trustworthiness of a vehicle's in-vehicle network by evaluating the trust levels of individual components under various network configurations based on their interdependency, value correlation and vulnerability to remote injection.", "result": "The effectiveness of TrustConnect has been validated through programming simulations conducted across various scenarios using a random distribution of an in-vehicle network graph generated with the Networkx package in Python.", "conclusion": "TrustConnect is an effective framework for assessing the reliability of the in-vehicle network."}}
{"id": "2506.06634", "pdf": "https://arxiv.org/pdf/2506.06634", "abs": "https://arxiv.org/abs/2506.06634", "authors": ["Yubin Xiao", "Di Wang", "Rui Cao", "Xuan Wu", "Boyang Li", "You Zhou"], "title": "GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales", "categories": ["cs.AI"], "comment": "21pages, 4 figures, and 14 tables", "summary": "The Traveling Salesman Problem (TSP) is a well-known combinatorial\noptimization problem with broad real-world applications. Recent advancements in\nneural network-based TSP solvers have shown promising results. Nonetheless,\nthese models often struggle to efficiently solve both small- and large-scale\nTSPs using the same set of pre-trained model parameters, limiting their\npractical utility. To address this issue, we introduce a novel neural TSP\nsolver named GELD, built upon our proposed broad global assessment and refined\nlocal selection framework. Specifically, GELD integrates a lightweight\nGlobal-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich\nembedding representation while accelerating the decision-making process.\nMoreover, GE incorporates a novel low-complexity attention mechanism, allowing\nGELD to achieve low inference latency and scalability to larger-scale TSPs.\nAdditionally, we propose a two-stage training strategy that utilizes training\ninstances of different sizes to bolster GELD's generalization ability.\nExtensive experiments conducted on both synthetic and real-world datasets\ndemonstrate that GELD outperforms seven state-of-the-art models considering\nboth solution quality and inference speed. Furthermore, GELD can be employed as\na post-processing method to significantly elevate the quality of the solutions\nderived by existing neural TSP solvers via spending affordable additional\ncomputing time. Notably, GELD is shown as capable of solving TSPs with up to\n744,710 nodes, first-of-its-kind to solve this large size TSP without relying\non divide-and-conquer strategies to the best of our knowledge.", "AI": {"tldr": "The paper presents GELD, a novel neural TSP solver that integrates a lightweight Global-view Encoder with a heavyweight Local-view Decoder to solve both small- and large-scale TSPs more efficiently than existing models.", "motivation": "Current neural network-based TSP solvers struggle to efficiently solve both small- and large-scale TSPs using the same set of pre-trained model parameters, limiting their practical utility.", "method": "GELD incorporates a broad global assessment and refined local selection framework. It integrates a lightweight Global-view Encoder (GE) with a heavyweight Local-view Decoder (LD). GE uses a novel low-complexity attention mechanism. A two-stage training strategy is proposed to improve generalization ability.", "result": "GELD outperforms seven state-of-the-art models in terms of solution quality and inference speed on both synthetic and real-world datasets. It can solve TSPs with up to 744,710 nodes without relying on divide-and-conquer strategies.", "conclusion": "GELD is an effective neural TSP solver that addresses current limitations, providing superior performance for large-scale TSPs."}}
{"id": "2506.06327", "pdf": "https://arxiv.org/pdf/2506.06327", "abs": "https://arxiv.org/abs/2506.06327", "authors": ["Zilang Chen"], "title": "Wine Quality Prediction with Ensemble Trees: A Unified, Leak-Free Comparative Study", "categories": ["cs.LG"], "comment": "14pages, 7figures,2tables", "summary": "Accurate and reproducible wine-quality assessment is critical for production\ncontrol yet remains dominated by subjective, labour-intensive tasting panels.\nWe present the first unified benchmark of five ensemble learners (Random\nForest, Gradient Boosting, XGBoost, LightGBM, CatBoost) on the canonical Vinho\nVerde red- and white-wine datasets (1,599 and 4,898 instances, 11\nphysicochemical attributes). Our leakage-free workflow employs an 80:20\nstratified train-test split, five-fold StratifiedGroupKFold within the training\nset, per-fold standardisation, SMOTE-Tomek resampling, inverse-frequency cost\nweighting, Optuna hyper-parameter search (120-200 trials per model) and a\ntwo-stage feature-selection refit. Final scores on untouched test sets are\nreported with weighted F1 as the headline metric. Gradient Boosting achieves\nthe highest accuracy (weighted F1 0.693 +/- 0.028 for red and 0.664 +/- 0.016\nfor white), followed within three percentage points by Random Forest and\nXGBoost. Limiting each model to its five top-ranked variables lowers\ndimensionality by 55 percent while reducing weighted F1 by only 2.6 percentage\npoints for red and 3.0 percentage points for white, indicating that alcohol,\nvolatile acidity, sulphates, free SO2 and chlorides capture most predictive\nsignal. Runtime profiling on an EPYC 9K84/H20 node reveals a steep efficiency\ngradient: Gradient Boosting averages 12 h per five-fold study, XGBoost and\nLightGBM require 2-3 h, CatBoost 1 h, and Random Forest under 50 min. We\ntherefore recommend Random Forest as the most cost-effective production model,\nXGBoost and LightGBM as GPU-efficient alternatives, and Gradient Boosting as\nthe accuracy ceiling for offline benchmarking. The fully documented pipeline\nand metric set provide a reproducible baseline for future work on imbalanced\nmulti-class wine-quality prediction.", "AI": {"tldr": "The paper benchmarks five ensemble learning models for wine-quality assessment, finding Gradient Boosting most accurate and Random Forest most cost-effective.", "motivation": "Accurate and reproducible wine-quality assessment is critical for production control but currently relies on subjective tasting panels. This motivates the development of an objective, automated method using machine learning.", "method": "Five ensemble learners (Random Forest, Gradient Boosting, XGBoost, LightGBM, CatBoost) are benchmarked on red- and white-wine datasets with 11 physicochemical attributes. A leakage-free workflow includes stratified train-test split, StratifiedGroupKFold cross-validation, standardisation, resampling, cost weighting, hyper-parameter search, and feature selection.", "result": "Gradient Boosting achieves the highest accuracy (weighted F1 0.693 for red and 0.664 for white). Limiting to top five variables reduces dimensionality by 55% with minimal performance loss. Random Forest is the most cost-effective model.", "conclusion": "Gradient Boosting is recommended for accuracy, Random Forest for cost-effectiveness, XGBoost and LightGBM for GPU efficiency. The study provides a reproducible baseline for future work in wine-quality prediction."}}
{"id": "2506.06730", "pdf": "https://arxiv.org/pdf/2506.06730", "abs": "https://arxiv.org/abs/2506.06730", "authors": ["Rabah Rahal", "Abdelaziz Amara Korba", "Yacine Ghamri-Doudane"], "title": "Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid global adoption of electric vehicles (EVs) has established electric\nvehicle supply equipment (EVSE) as a critical component of smart grid\ninfrastructure. While essential for ensuring reliable energy delivery and\naccessibility, EVSE systems face significant cybersecurity challenges,\nincluding network reconnaissance, backdoor intrusions, and distributed\ndenial-of-service (DDoS) attacks. These emerging threats, driven by the\ninterconnected and autonomous nature of EVSE, require innovative and adaptive\nsecurity mechanisms that go beyond traditional intrusion detection systems\n(IDS). Existing approaches, whether network-based or host-based, often fail to\ndetect sophisticated and targeted attacks specifically crafted to exploit new\nvulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion\ndetection framework that leverages multimodal data sources, including network\ntraffic and kernel events, to identify complex attack patterns. The framework\nemploys a distributed learning approach, enabling collaborative intelligence\nacross EVSE stations while preserving data privacy through federated learning.\nExperimental results demonstrate that the proposed framework outperforms\nexisting solutions, achieving a detection rate above 98% and a precision rate\nexceeding 97% in decentralized environments. This solution addresses the\nevolving challenges of EVSE security, offering a scalable and privacypreserving\nresponse to advanced cyber threats", "AI": {"tldr": "The paper proposes a new intrusion detection framework for EVSE that uses multimodal data sources and distributed learning, achieving high detection and precision rates.", "motivation": "Electric vehicle supply equipment (EVSE) is crucial in smart grid infrastructure but faces significant cybersecurity challenges such as network reconnaissance, backdoor intrusions, and DDoS attacks. Current intrusion detection systems often fail to detect sophisticated and targeted attacks on EVSE infrastructure.", "method": "The proposed framework leverages multimodal data sources, including network traffic and kernel events, to identify complex attack patterns. It employs a distributed learning approach with collaborative intelligence across EVSE stations while preserving data privacy through federated learning.", "result": "Experimental results show that the proposed framework outperforms existing solutions, achieving a detection rate above 98% and a precision rate exceeding 97% in decentralized environments.", "conclusion": "This solution addresses the evolving challenges of EVSE security by providing a scalable and privacy-preserving response to advanced cyber threats."}}
{"id": "2506.06698", "pdf": "https://arxiv.org/pdf/2506.06698", "abs": "https://arxiv.org/abs/2506.06698", "authors": ["Yitao Liu", "Chenglei Si", "Karthik Narasimhan", "Shunyu Yao"], "title": "Contextual Experience Replay for Self-Improvement of Language Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025. 20 pages", "summary": "Large language model (LLM) agents have been applied to sequential\ndecision-making tasks such as web navigation, but without any\nenvironment-specific experiences, they often fail in these complex tasks.\nMoreover, current LLM agents are not designed to continually learn from past\nexperiences during inference time, which could be crucial for them to gain\nthese environment-specific experiences. To address this, we propose Contextual\nExperience Replay (CER), a training-free framework to enable efficient\nself-improvement for language agents in their context window. Specifically, CER\naccumulates and synthesizes past experiences into a dynamic memory buffer.\nThese experiences encompass environment dynamics and common decision-making\npatterns, allowing the agents to retrieve and augment themselves with relevant\nknowledge in new tasks, enhancing their adaptability in complex environments.\nWe evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On\nVisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,\nCER also gets a competitive average success rate of 36.7%, relatively improving\nthe success rate of the GPT-4o agent baseline by 51.0%. We also conduct a\ncomprehensive analysis on it to prove its efficiency, validity and understand\nit better.", "AI": {"tldr": "Contextual Experience Replay (CER) is a training-free framework enabling efficient self-improvement for language agents in their context window.", "motivation": "Large language model agents often fail in complex sequential decision-making tasks due to lack of environment-specific experiences and the inability to continually learn from past experiences during inference time.", "method": "CER accumulates and synthesizes past experiences into a dynamic memory buffer, encompassing environment dynamics and common decision-making patterns, allowing agents to retrieve and augment themselves with relevant knowledge in new tasks.", "result": "On VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena, CER gets a competitive average success rate of 36.7%, relatively improving the success rate of the GPT-4o agent baseline by 51.0%.", "conclusion": "A comprehensive analysis proves the efficiency and validity of CER, enhancing adaptability of LLM agents in complex environments."}}
{"id": "2506.06330", "pdf": "https://arxiv.org/pdf/2506.06330", "abs": "https://arxiv.org/abs/2506.06330", "authors": ["James Afful"], "title": "ExplainBench: A Benchmark Framework for Local Model Explanations in Fairness-Critical Applications", "categories": ["cs.LG"], "comment": null, "summary": "As machine learning systems are increasingly deployed in high-stakes domains\nsuch as criminal justice, finance, and healthcare, the demand for interpretable\nand trustworthy models has intensified. Despite the proliferation of local\nexplanation techniques, including SHAP, LIME, and counterfactual methods, there\nexists no standardized, reproducible framework for their comparative\nevaluation, particularly in fairness-sensitive settings.\n  We introduce ExplainBench, an open-source benchmarking suite for systematic\nevaluation of local model explanations across ethically consequential datasets.\nExplainBench provides unified wrappers for popular explanation algorithms,\nintegrates end-to-end pipelines for model training and explanation generation,\nand supports evaluation via fidelity, sparsity, and robustness metrics. The\nframework includes a Streamlit-based graphical interface for interactive\nexploration and is packaged as a Python module for seamless integration into\nresearch workflows.\n  We demonstrate ExplainBench on datasets commonly used in fairness research,\nsuch as COMPAS, UCI Adult Income, and LendingClub, and showcase how different\nexplanation methods behave under a shared experimental protocol. By enabling\nreproducible, comparative analysis of local explanations, ExplainBench advances\nthe methodological foundations of interpretable machine learning and\nfacilitates accountability in real-world AI systems.", "AI": {"tldr": "As machine learning systems are increasingly deployed in high-stakes domains such as criminal justice, finance, and healthcare, the demand for interpretable and trustworthy models has intensified. To address this need, ExplainBench, an open-source benchmarking suite for systematic evaluation of local model explanations across ethically consequential datasets is introduced.", "motivation": "The proliferation of local explanation techniques has no standardized, reproducible framework for their comparative evaluation, particularly in fairness-sensitive settings.", "method": "ExplainBench provides unified wrappers for popular explanation algorithms, integrates end-to-end pipelines for model training and explanation generation, and supports evaluation via fidelity, sparsity, and robustness metrics.", "result": "ExplainBench demonstrated on datasets commonly used in fairness research, such as COMPAS, UCI Adult Income, and LendingClub, showcasing how different explanation methods behave under a shared experimental protocol.", "conclusion": "ExplainBench advances the methodological foundations of interpretable machine learning and facilitates accountability in real-world AI systems."}}
{"id": "2506.06735", "pdf": "https://arxiv.org/pdf/2506.06735", "abs": "https://arxiv.org/abs/2506.06735", "authors": ["Mesut Ozdag"], "title": "Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Smart contracts, integral to blockchain ecosystems, enable decentralized\napplications to execute predefined operations without intermediaries. Their\nability to enforce trustless interactions has made them a core component of\nplatforms such as Ethereum. Vulnerabilities such as numerical overflows,\nreentrancy attacks, and improper access permissions have led to the loss of\nmillions of dollars throughout the blockchain and smart contract sector.\nTraditional smart contract auditing techniques such as manual code reviews and\nformal verification face limitations in scalability, automation, and\nadaptability to evolving development patterns. As a result, AI-based solutions\nhave emerged as a promising alternative, offering the ability to learn complex\npatterns, detect subtle flaws, and provide scalable security assurances. This\npaper examines novel AI-driven techniques for vulnerability detection in smart\ncontracts, focusing on machine learning, deep learning, graph neural networks,\nand transformer-based models. This paper analyzes how each technique represents\ncode, processes semantic information, and responds to real world vulnerability\nclasses. We also compare their strengths and weaknesses in terms of accuracy,\ninterpretability, computational overhead, and real time applicability. Lastly,\nit highlights open challenges and future opportunities for advancing this\ndomain.", "AI": {"tldr": "The paper explores AI-driven techniques for detecting vulnerabilities in smart contracts, including machine learning, deep learning, graph neural networks, and transformer-based models. It evaluates their strengths and weaknesses and highlights future research opportunities.", "motivation": "Vulnerabilities in smart contracts have led to significant financial losses, while traditional auditing methods are limited in scalability, automation, and adaptability.", "method": "The paper investigates AI-driven techniques such as machine learning, deep learning, graph neural networks, and transformer-based models for vulnerability detection in smart contracts. It analyzes how each technique represents code, processes semantic information, and addresses real-world vulnerability classes.", "result": "The techniques vary in accuracy, interpretability, computational overhead, and real-time applicability. Each has its own strengths and weaknesses which are compared in the paper.", "conclusion": "AI-driven techniques offer a promising approach for smart contract security but face challenges that need to be addressed. The paper outlines open challenges and suggests future research directions."}}
{"id": "2506.06714", "pdf": "https://arxiv.org/pdf/2506.06714", "abs": "https://arxiv.org/abs/2506.06714", "authors": ["Hamied Nabizada", "Tom Jeleniewski", "Lasse Beers", "Maximilian Weigand", "Felix Gehlhoff", "Alexander Fay"], "title": "Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a SysML profile that enables the direct integration of\nplanning semantics based on the Planning Domain Definition Language (PDDL) into\nsystem models. Reusable stereotypes are defined for key PDDL concepts such as\ntypes, predicates, functions and actions, while formal OCL constraints ensure\nsyntactic consistency. The profile was derived from the Backus-Naur Form (BNF)\ndefinition of PDDL 3.1 to align with SysML modeling practices. A case study\nfrom aircraft manufacturing demonstrates the application of the profile: a\nrobotic system with interchangeable end effectors is modeled and enriched to\ngenerate both domain and problem descriptions in PDDL format. These are used as\ninput to a PDDL solver to derive optimized execution plans. The approach\nsupports automated and model-based generation of planning descriptions and\nprovides a reusable bridge between system modeling and AI planning in\nengineering design.", "AI": {"tldr": "This paper presents a SysML profile for integrating PDDL planning semantics into system models, enabling automated generation of planning descriptions and bridging system modeling with AI planning.", "motivation": "To create a direct integration of PDDL planning semantics into system models to support automated and model-based generation of planning descriptions in engineering design.", "method": "Defines reusable stereotypes for key PDDL concepts and uses formal OCL constraints to ensure syntactic consistency. The profile was derived from the BNF definition of PDDL 3.1.", "result": "A case study in aircraft manufacturing demonstrated the successful application of the profile, where a robotic system was modeled and enriched to generate domain and problem descriptions in PDDL format that were used as input to a PDDL solver to derive optimized execution plans.", "conclusion": "The presented SysML profile supports automated and model-based generation of planning descriptions and provides a reusable bridge between system modeling and AI planning in engineering design."}}
{"id": "2506.06333", "pdf": "https://arxiv.org/pdf/2506.06333", "abs": "https://arxiv.org/abs/2506.06333", "authors": ["Benjamin von Berg", "Bernhard K. Aichernig"], "title": "Extending AALpy with Passive Learning: A Generalized State-Merging Approach", "categories": ["cs.LG", "cs.FL"], "comment": "Accepted for publication at CAV 2025, the 37th International\n  Conference on Computer Aided Verification", "summary": "AALpy is a well-established open-source automata learning library written in\nPython with a focus on active learning of systems with IO behavior. It provides\na wide range of state-of-the-art algorithms for different automaton types\nranging from fully deterministic to probabilistic automata. In this work, we\npresent the recent addition of a generalized implementation of an important\nmethod from the domain of passive automata learning: state-merging in the\nred-blue framework. Using a common internal representation for different\nautomaton types allows for a general and highly configurable implementation of\nthe red-blue framework. We describe how to define and execute state-merging\nalgorithms using AALpy, which reduces the implementation effort for\nstate-merging algorithms mainly to the definition of compatibility criteria and\nscoring. This aids the implementation of both existing and novel algorithms. In\nparticular, defining some existing state-merging algorithms from the literature\nwith AALpy only takes a few lines of code.", "AI": {"tldr": "AALpy\uff0c\u4e00\u4e2a\u4ee5Python\u7f16\u5199\u7684\u5f00\u6e90\u81ea\u52a8\u673a\u5b66\u4e60\u5e93\uff0c\u65b0\u52a0\u5165\u4e86\u88ab\u52a8\u81ea\u52a8\u673a\u5b66\u4e60\u9886\u57df\u7684\u91cd\u8981\u65b9\u6cd5\u2014\u2014\u7ea2\u84dd\u6846\u67b6\u7684\u72b6\u6001\u5408\u5e76\u901a\u7528\u5b9e\u73b0\u3002\u901a\u8fc7\u516c\u5171\u5185\u90e8\u8868\u793a\uff0c\u53ef\u4ee5\u7075\u6d3b\u914d\u7f6e\u5730\u5b9e\u73b0\u7ea2\u84dd\u6846\u67b6\uff0c\u7b80\u5316\u4e86\u72b6\u6001\u5408\u5e76\u7b97\u6cd5\u7684\u5b9a\u4e49\u4e0e\u6267\u884c\uff0c\u6709\u52a9\u4e8e\u73b0\u6709\u53ca\u65b0\u7b97\u6cd5\u7684\u5b9e\u73b0\u3002", "motivation": "\u5c06\u88ab\u52a8\u81ea\u52a8\u673a\u5b66\u4e60\u4e2d\u7684\u72b6\u6001\u5408\u5e76\u65b9\u6cd5\u6574\u5408\u8fdbAALpy\u5e93\uff0c\u4ee5\u964d\u4f4e\u72b6\u6001\u5408\u5e76\u7b97\u6cd5\u7684\u5b9e\u73b0\u96be\u5ea6\uff0c\u5e76\u4fc3\u8fdb\u73b0\u6709\u548c\u65b0\u7b97\u6cd5\u7684\u5f00\u53d1\u3002", "method": "\u5728AALpy\u4e2d\u6dfb\u52a0\u7ea2\u84dd\u6846\u67b6\u7684\u72b6\u6001\u5408\u5e76\u901a\u7528\u5b9e\u73b0\uff0c\u901a\u8fc7\u5b9a\u4e49\u517c\u5bb9\u6027\u6807\u51c6\u548c\u8bc4\u5206\u673a\u5236\u6765\u6267\u884c\u72b6\u6001\u5408\u5e76\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7c7b\u578b\u7684\u81ea\u52a8\u673a\u3002", "result": "\u7b80\u5316\u4e86\u72b6\u6001\u5408\u5e76\u7b97\u6cd5\u7684\u5b9e\u73b0\u8fc7\u7a0b\uff0c\u4f7f\u4e00\u4e9b\u5df2\u6709\u7684\u72b6\u6001\u5408\u5e76\u7b97\u6cd5\u53ea\u9700\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u5b9a\u4e49\u3002", "conclusion": "AALpy\u65b0\u589e\u7684\u7ea2\u84dd\u6846\u67b6\u72b6\u6001\u5408\u5e76\u5b9e\u73b0\u4e3a\u4e0d\u540c\u7c7b\u578b\u7684\u81ea\u52a8\u673a\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u5ea6\u53ef\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u6781\u5927\u51cf\u5c11\u4e86\u7b97\u6cd5\u5b9e\u73b0\u7684\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2506.06742", "pdf": "https://arxiv.org/pdf/2506.06742", "abs": "https://arxiv.org/abs/2506.06742", "authors": ["Zeyu Yan", "Yifei Yao", "Xuanbing Wen", "Juli Zhang", "Kai Fan"], "title": "LADSG: Label-Anonymized Distillation and Similar Gradient Substitution for Label Privacy in Vertical Federated Learning", "categories": ["cs.CR"], "comment": "20 pages, 6 figures. Under review", "summary": "Vertical federated learning (VFL) has become a key paradigm for collaborative\nmachine learning, enabling multiple parties to train models over distributed\nfeature spaces while preserving data privacy. Despite security protocols that\ndefend against external attacks - such as gradient masking and encryption,\nwhich prevent unauthorized access to sensitive data - recent label inference\nattacks from within the system have emerged. These attacks exploit gradients\nand semantic embeddings to reconstruct private labels, bypassing traditional\ndefenses. For example, the passive label inference attack can reconstruct tens\nof thousands of participants' private data using just 40 auxiliary labels,\nposing a significant security threat. Existing defenses address single leakage\npathways, such as gradient leakage or label exposure. As attack strategies\nevolve, their limitations become clear, especially against hybrid attacks that\ncombine multiple vectors. To address this, we propose Label-Anonymized Defense\nwith Substitution Gradient (LADSG), a unified defense framework that integrates\ngradient substitution, label anonymization, and anomaly detection. LADSG\nmitigates both gradient and label leakage while maintaining the scalability and\nefficiency of VFL. Experiments on six real-world datasets show that LADSG\nreduces label inference attack success rates by 30-60%, with minimal\ncomputational overhead, underscoring the importance of lightweight defenses in\nsecuring VFL.", "AI": {"tldr": "Vertical federated learning (VFL) is crucial for collaborative machine learning with privacy preservation. However, label inference attacks pose a significant threat by exploiting gradients and embeddings to reconstruct private labels. Existing defenses are insufficient against hybrid attacks. The paper proposes LADSG, a unified defense framework that mitigates gradient and label leakage while maintaining VFL's efficiency.", "motivation": "Label inference attacks within the VFL system can bypass traditional security measures by exploiting gradients and semantic embeddings to reconstruct private labels, indicating a need for more comprehensive defenses.", "method": "LADSG integrates gradient substitution, label anonymization, and anomaly detection to provide a unified defense mechanism against both gradient and label leakage in VFL.", "result": "Experiments on six real-world datasets demonstrate that LADSG reduces label inference attack success rates by 30-60% with minimal computational overhead.", "conclusion": "LADSG offers an effective solution to enhance the security of VFL systems against evolving label inference attacks, emphasizing the importance of lightweight defenses."}}
{"id": "2506.06725", "pdf": "https://arxiv.org/pdf/2506.06725", "abs": "https://arxiv.org/abs/2506.06725", "authors": ["Guillaume Levy", "Cedric Colas", "Pierre-Yves Oudeyer", "Thomas Carta", "Clement Romac"], "title": "WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) possess general world knowledge but often\nstruggle to generate precise predictions in structured, domain-specific\ncontexts such as simulations. These limitations arise from their inability to\nground their broad, unstructured understanding in specific environments. To\naddress this, we present WorldLLM, a framework that enhances LLM-based world\nmodeling by combining Bayesian inference and autonomous active exploration with\nreinforcement learning. WorldLLM leverages the in-context learning abilities of\nLLMs to guide an LLM-based world model's predictions using natural language\nhypotheses given in its prompt. These hypotheses are iteratively refined\nthrough a Bayesian inference framework that leverages a second LLM as the\nproposal distribution given collected evidence. This evidence is collected\nusing a curiosity-driven reinforcement learning policy that explores the\nenvironment to find transitions with a low log-likelihood under our LLM-based\npredictive model using the current hypotheses. By alternating between refining\nhypotheses and collecting new evidence, our framework autonomously drives\ncontinual improvement of the predictions. Our experiments demonstrate the\neffectiveness of WorldLLM in a textual game environment that requires agents to\nmanipulate and combine objects. The framework not only enhances predictive\naccuracy, but also generates human-interpretable theories of environment\ndynamics.", "AI": {"tldr": "WorldLLM \u662f\u4e00\u4e2a\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u73af\u5883\u4e2d\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u62e5\u6709\u5e7f\u6cdb\u7684\u4e16\u754c\u77e5\u8bc6\uff0c\u4f46\u5728\u7ed3\u6784\u5316\u3001\u7279\u5b9a\u9886\u57df\u7684\u73af\u5883\u4e2d\u751f\u6210\u7cbe\u786e\u9884\u6d4b\u65f6\u5f80\u5f80\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u5c06\u5176\u5e7f\u6cdb\u7684\u975e\u7ed3\u6784\u5316\u7406\u89e3\u4e0e\u5177\u4f53\u73af\u5883\u8054\u7cfb\u8d77\u6765\u3002", "method": "WorldLLM \u6846\u67b6\u901a\u8fc7\u5c06\u8d1d\u53f6\u65af\u63a8\u7406\u548c\u81ea\u4e3b\u4e3b\u52a8\u63a2\u7d22\u4e0e\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u589e\u5f3a\u4e86\u57fa\u4e8e LLM \u7684\u4e16\u754c\u5efa\u6a21\u3002\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5047\u8bbe\u5f15\u5bfc LLM \u57fa\u4e8e\u4e16\u754c\u7684\u6a21\u578b\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u6846\u67b6\u8fed\u4ee3\u6539\u8fdb\u8fd9\u4e9b\u5047\u8bbe\u3002\u8bc1\u636e\u6536\u96c6\u7531\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5b8c\u6210\uff0c\u8be5\u7b56\u7565\u63a2\u7d22\u73af\u5883\u4ee5\u627e\u5230\u5728\u5f53\u524d\u5047\u8bbe\u4e0b\u5177\u6709\u4f4e\u5bf9\u6570\u4f3c\u7136\u6027\u7684\u8f6c\u6362\u3002\u901a\u8fc7\u5728\u6539\u8fdb\u5047\u8bbe\u548c\u6536\u96c6\u65b0\u8bc1\u636e\u4e4b\u95f4\u4ea4\u66ff\uff0c\u6846\u67b6\u81ea\u4e3b\u63a8\u52a8\u9884\u6d4b\u7684\u6301\u7eed\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cWorldLLM \u5728\u9700\u8981\u4ee3\u7406\u64cd\u4f5c\u548c\u7ec4\u5408\u5bf9\u8c61\u7684\u6587\u672c\u6e38\u620f\u73af\u5883\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u8fd8\u751f\u6210\u4e86\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u73af\u5883\u52a8\u529b\u5b66\u7406\u8bba\u3002", "conclusion": "WorldLLM \u63d0\u4f9b\u4e86\u4e00\u79cd\u589e\u5f3a LLM \u5728\u7279\u5b9a\u9886\u57df\u73af\u5883\u4e2d\u9884\u6d4b\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u540c\u65f6\u751f\u6210\u4e86\u4eba\u7c7b\u53ef\u89e3\u91ca\u7684\u73af\u5883\u52a8\u6001\u7406\u8bba\u3002"}}
{"id": "2506.06337", "pdf": "https://arxiv.org/pdf/2506.06337", "abs": "https://arxiv.org/abs/2506.06337", "authors": ["Ali Murad", "Bo Hui", "Wei-Shinn Ku"], "title": "Optimized Local Updates in Federated Learning via Reinforcement Learning", "categories": ["cs.LG"], "comment": "This paper is accepted at IEEE IJCNN 2025", "summary": "Federated Learning (FL) is a distributed framework for collaborative model\ntraining over large-scale distributed data, enabling higher performance while\nmaintaining client data privacy. However, the nature of model aggregation at\nthe centralized server can result in a performance drop in the presence of\nnon-IID data across different clients. We remark that training a client locally\non more data than necessary does not benefit the overall performance of all\nclients. In this paper, we devise a novel framework that leverages a Deep\nReinforcement Learning (DRL) agent to select an optimized amount of data\nnecessary to train a client model without oversharing information with the\nserver. Starting without awareness of the client's performance, the DRL agent\nutilizes the change in training loss as a reward signal and learns to optimize\nthe amount of training data necessary for improving the client's performance.\nSpecifically, after each aggregation round, the DRL algorithm considers the\nlocal performance as the current state and outputs the optimized weights for\neach class, in the training data, to be used during the next round of local\ntraining. In doing so, the agent learns a policy that creates an optimized\npartition of the local training dataset during the FL rounds. After FL, the\nclient utilizes the entire local training dataset to further enhance its\nperformance on its own data distribution, mitigating the non-IID effects of\naggregation. Through extensive experiments, we demonstrate that training FL\nclients through our algorithm results in superior performance on multiple\nbenchmark datasets and FL frameworks. Our code is available at\nhttps://github.com/amuraddd/optimized_client_training.git.", "AI": {"tldr": "The paper proposes a novel framework using Deep Reinforcement Learning (DRL) to optimize the amount of data necessary for training client models in Federated Learning (FL), reducing information oversharing and mitigating non-IID effects. The DRL agent learns to partition local datasets optimally, enhancing overall FL performance on various benchmarks.", "motivation": "Federated Learning faces challenges with non-IID data across clients, leading to performance drops due to centralized model aggregation. Training clients on more data than needed does not benefit overall performance. This work aims to optimize the amount of data used for training each client without oversharing information with the server.", "method": "A Deep Reinforcement Learning (DRL) agent is introduced to determine the optimal amount of training data for each client. The DRL agent uses changes in training loss as a reward signal and learns from local performance after each aggregation round. It outputs optimized weights for each class in the training data, creating an optimized partition of the local dataset during FL rounds.", "result": "Through extensive experiments, the proposed method shows superior performance on multiple benchmark datasets and FL frameworks compared to traditional methods. Clients achieve better performance on their own data distributions while mitigating the negative effects of non-IID data.", "conclusion": "The novel framework improves the efficiency and effectiveness of Federated Learning by optimizing client training data through DRL. The approach enhances performance in FL systems dealing with non-IID data and reduces unnecessary information sharing."}}
{"id": "2506.06975", "pdf": "https://arxiv.org/pdf/2506.06975", "abs": "https://arxiv.org/abs/2506.06975", "authors": ["Xiaoyuan Zhu", "Yaowen Ye", "Tianyi Qiu", "Hanlin Zhu", "Sijun Tan", "Ajraf Mannan", "Jonathan Michala", "Raluca Ada Popa", "Willie Neiswanger"], "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As API access becomes a primary interface to large language models (LLMs),\nusers often interact with black-box systems that offer little transparency into\nthe deployed model. To reduce costs or maliciously alter model behaviors, API\nproviders may discreetly serve quantized or fine-tuned variants, which can\ndegrade performance and compromise safety. Detecting such substitutions is\ndifficult, as users lack access to model weights and, in most cases, even\noutput logits. To tackle this problem, we propose a rank-based uniformity test\nthat can verify the behavioral equality of a black-box LLM to a locally\ndeployed authentic model. Our method is accurate, query-efficient, and avoids\ndetectable query patterns, making it robust to adversarial providers that\nreroute or mix responses upon the detection of testing attempts. We evaluate\nthe approach across diverse threat scenarios, including quantization, harmful\nfine-tuning, jailbreak prompts, and full model substitution, showing that it\nconsistently achieves superior statistical power over prior methods under\nconstrained query budgets.", "AI": {"tldr": "An abstract of a paper proposing a rank-based uniformity test to verify the behavioral equality of black-box LLMs to authentic models, addressing issues like quantization, harmful fine-tuning, and model substitution.", "motivation": "To solve the problem of detecting substitutions in black-box large language models (LLMs) provided via APIs, which may be quantized or fine-tuned variants that degrade performance and compromise safety.", "method": "A rank-based uniformity test is proposed to verify the behavioral equality of a black-box LLM to a locally deployed authentic model. The method is accurate, query-efficient, and avoids detectable query patterns.", "result": "The approach consistently achieves superior statistical power over prior methods under constrained query budgets when evaluated across diverse threat scenarios including quantization, harmful fine-tuning, jailbreak prompts, and full model substitution.", "conclusion": "This rank-based uniformity test provides an effective solution for verifying black-box LLMs against authentic models, ensuring both performance and safety."}}
{"id": "2506.06727", "pdf": "https://arxiv.org/pdf/2506.06727", "abs": "https://arxiv.org/abs/2506.06727", "authors": ["Can Li", "Ting Zhang", "Mei Wang", "Hua Huang"], "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving\ncapabilities across various domains. However, their ability to perform\nmathematical reasoning when answer options are represented as images--an\nessential aspect of multi-image comprehension--remains underexplored. To bridge\nthis gap, we introduce VisioMath, a benchmark designed to evaluate mathematical\nreasoning in multimodal contexts involving image-based answer choices.\nVisioMath comprises 8,070 images and 1,800 multiple-choice questions, where\neach answer option is an image, presenting unique challenges to existing LMMs.\nTo the best of our knowledge, VisioMath is the first dataset specifically\ntailored for mathematical reasoning in image-based-option scenarios, where\nfine-grained distinctions between answer choices are critical for accurate\nproblem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath\nand find that even the most advanced models struggle with this task. Notably,\nGPT-4o achieves only 45.9% accuracy, underscoring the limitations of current\nmodels in reasoning over visually similar answer choices. By addressing a\ncrucial gap in existing benchmarks, VisioMath establishes a rigorous testbed\nfor future research, driving advancements in multimodal reasoning.", "AI": {"tldr": "Large Multimodal Models (LMMs) face challenges in mathematical reasoning with image-based answer options. The VisioMath benchmark, consisting of 8,070 images and 1,800 questions, is introduced to evaluate these capabilities. State-of-the-art LMMs, including GPT-4o, struggle significantly on this task.", "motivation": "To explore the underexplored ability of LMMs in performing mathematical reasoning when answer options are represented as images, which is crucial for multi-image comprehension.", "method": "Introduction of VisioMath, a benchmark with 8,070 images and 1,800 multiple-choice questions where each answer option is an image, to evaluate mathematical reasoning in multimodal contexts.", "result": "State-of-the-art LMMs, such as GPT-4o, achieve low accuracy (45.9%) on the VisioMath benchmark, indicating significant limitations in reasoning over visually similar answer choices.", "conclusion": "VisioMath fills a critical gap in existing benchmarks by providing a rigorous testbed for future research in multimodal reasoning."}}
{"id": "2506.06359", "pdf": "https://arxiv.org/pdf/2506.06359", "abs": "https://arxiv.org/abs/2506.06359", "authors": ["Gabriel Antonesi", "Tudor Cioara", "Ionut Anghel", "Vasilis Michalakopoulos", "Elissaios Sarmas", "Liana Toderean"], "title": "From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) has long promised to improve energy management\nin smart grids by enhancing situational awareness and supporting more effective\ndecision-making. While traditional machine learning has demonstrated notable\nresults in forecasting and optimization, it often struggles with\ngeneralization, situational awareness, and heterogeneous data integration.\nRecent advances in foundation models such as Transformer architecture and Large\nLanguage Models (LLMs) have demonstrated improved capabilities in modelling\ncomplex temporal and contextual relationships, as well as in multi-modal data\nfusion which is essential for most AI applications in the energy sector. In\nthis review we synthesize the rapid expanding field of AI applications in the\nenergy domain focusing on Transformers and LLMs. We examine the architectural\nfoundations, domain-specific adaptations and practical implementations of\ntransformer models across various forecasting and grid management tasks. We\nthen explore the emerging role of LLMs in the field: adaptation and fine tuning\nfor the energy sector, the type of tasks they are suited for, and the new\nchallenges they introduce. Along the way, we highlight practical\nimplementations, innovations, and areas where the research frontier is rapidly\nexpanding. These recent developments reviewed underscore a broader trend:\nGenerative AI (GenAI) is beginning to augment decision-making not only in\nhigh-level planning but also in day-to-day operations, from forecasting and\ngrid balancing to workforce training and asset onboarding. Building on these\ndevelopments, we introduce the concept of the Agentic Digital Twin, a\nnext-generation model that integrates LLMs to bring autonomy, proactivity, and\nsocial interaction into digital twin-based energy management systems.", "AI": {"tldr": "The paper reviews the application of AI, especially Transformers and LLMs, in the energy domain. It highlights practical implementations and innovations, and introduces the concept of Agentic Digital Twin.", "motivation": "To explore how recent advances in foundation models like Transformers and LLMs can improve AI applications in energy management, addressing limitations in traditional machine learning.", "method": "Reviewing architectural foundations, domain-specific adaptations, and practical implementations of transformer models and LLMs in various forecasting and grid management tasks.", "result": "Transformers and LLMs show improved capabilities in modeling complex relationships and integrating multi-modal data. Generative AI is augmenting decision-making in both high-level planning and daily operations in the energy sector.", "conclusion": "The integration of LLMs into digital twin-based energy management systems through the concept of the Agentic Digital Twin represents a next-generation model that brings autonomy, proactivity, and social interaction."}}
{"id": "2506.07010", "pdf": "https://arxiv.org/pdf/2506.07010", "abs": "https://arxiv.org/abs/2506.07010", "authors": ["Martin Duclos", "Ivan A. Fernandez", "Kaneesha Moore", "Sudip Mittal", "Edward Zieglar"], "title": "ModelForge: Using GenAI to Improve the Development of Security Protocols", "categories": ["cs.CR"], "comment": null, "summary": "Formal methods can be used for verifying security protocols, but their\nadoption can be hindered by the complexity of translating natural language\nprotocol specifications into formal representations. In this paper, we\nintroduce ModelForge, a novel tool that automates the translation of protocol\nspecifications for the Cryptographic Protocol Shapes Analyzer (CPSA). By\nleveraging advances in Natural Language Processing (NLP) and Generative AI\n(GenAI), ModelForge processes protocol specifications and generates a CPSA\nprotocol definition. This approach reduces the manual effort required, making\nformal analysis more accessible. We evaluate ModelForge by fine-tuning a large\nlanguage model (LLM) to generate protocol definitions for CPSA, comparing its\nperformance with other popular LLMs. The results from our evaluation show that\nModelForge consistently produces quality outputs, excelling in syntactic\naccuracy, though some refinement is needed to handle certain protocol details.\nThe contributions of this work include the architecture and proof of concept\nfor a translating tool designed to simplify the adoption of formal methods in\nthe development of security protocols.", "AI": {"tldr": "ModelForge is a new tool that automates the translation of security protocol specifications into formal representations for CPSA by using NLP and GenAI, reducing manual effort and improving accessibility to formal analysis. Evaluation shows ModelForge performs well in syntactic accuracy but needs refinement for specific protocol details.", "motivation": "The motivation of this paper is to address the complexity involved in manually translating natural language protocol specifications into formal representations, which can hinder the adoption of formal methods for verifying security protocols.", "method": "The method involves introducing ModelForge, a tool that leverages advances in Natural Language Processing (NLP) and Generative AI (GenAI) to automate the translation process. This tool processes protocol specifications and generates a CPSA protocol definition. The performance of ModelForge is evaluated by fine-tuning a large language model (LLM) and comparing it with other popular LLMs.", "result": "The results indicate that ModelForge consistently produces quality outputs with high syntactic accuracy. However, there is a need for some refinement to handle certain protocol details effectively.", "conclusion": "This work contributes an architecture and proof of concept for a translating tool aimed at simplifying the use of formal methods in the development of security protocols, thereby promoting their adoption."}}
{"id": "2506.06739", "pdf": "https://arxiv.org/pdf/2506.06739", "abs": "https://arxiv.org/abs/2506.06739", "authors": ["Andrew Cropper", "Filipe Gouveia", "David M. Cerna"], "title": "Honey, I shrunk the hypothesis space (through logical preprocessing)", "categories": ["cs.AI", "cs.LG"], "comment": "Submitted to JAIR", "summary": "Inductive logic programming (ILP) is a form of logical machine learning. The\ngoal is to search a hypothesis space for a hypothesis that generalises training\nexamples and background knowledge. We introduce an approach that 'shrinks' the\nhypothesis space before an ILP system searches it. Our approach uses background\nknowledge to find rules that cannot be in an optimal hypothesis regardless of\nthe training examples. For instance, our approach discovers relationships such\nas \"even numbers cannot be odd\" and \"prime numbers greater than 2 are odd\". It\nthen removes violating rules from the hypothesis space. We implement our\napproach using answer set programming and use it to shrink the hypothesis space\nof a constraint-based ILP system. Our experiments on multiple domains,\nincluding visual reasoning and game playing, show that our approach can\nsubstantially reduce learning times whilst maintaining predictive accuracies.\nFor instance, given just 10 seconds of preprocessing time, our approach can\nreduce learning times from over 10 hours to only 2 seconds.", "AI": {"tldr": "\u901a\u8fc7\u5728ILP\u7cfb\u7edf\u641c\u7d22\u524d'\u7f29\u5c0f'\u5047\u8bbe\u7a7a\u95f4\uff0c\u5229\u7528\u80cc\u666f\u77e5\u8bc6\u53d1\u73b0\u65e0\u6cd5\u5b58\u5728\u4e8e\u6700\u4f18\u5047\u8bbe\u4e2d\u7684\u89c4\u5219\uff0c\u5e76\u79fb\u9664\u8fdd\u53cd\u89c4\u5219\u7684\u5047\u8bbe\uff0c\u4ece\u800c\u5927\u5e45\u51cf\u5c11\u5b66\u4e60\u65f6\u95f4\u5e76\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\uff08ILP\uff09\u9700\u8981\u5728\u5047\u8bbe\u7a7a\u95f4\u4e2d\u641c\u7d22\u80fd\u6982\u62ec\u8bad\u7ec3\u793a\u4f8b\u548c\u80cc\u666f\u77e5\u8bc6\u7684\u5047\u8bbe\uff0c\u4f46\u641c\u7d22\u8fc7\u7a0b\u53ef\u80fd\u975e\u5e38\u8017\u65f6\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u627e\u5230\u4e00\u79cd\u65b9\u6cd5\u6765\u7f29\u5c0f\u5047\u8bbe\u7a7a\u95f4\uff0c\u63d0\u9ad8ILP\u7cfb\u7edf\u7684\u6548\u7387\u3002", "method": "\u4f7f\u7528\u80cc\u666f\u77e5\u8bc6\u627e\u51fa\u90a3\u4e9b\u65e0\u8bba\u8bad\u7ec3\u793a\u4f8b\u5982\u4f55\u90fd\u4e0d\u53ef\u80fd\u5b58\u5728\u4e8e\u6700\u4f18\u5047\u8bbe\u4e2d\u7684\u89c4\u5219\uff08\u4f8b\u5982\u201c\u5076\u6570\u4e0d\u80fd\u662f\u5947\u6570\u201d\uff09\uff0c\u7136\u540e\u5c06\u8fdd\u53cd\u8fd9\u4e9b\u89c4\u5219\u7684\u5047\u8bbe\u4ece\u5047\u8bbe\u7a7a\u95f4\u4e2d\u79fb\u9664\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\uff0c\u5e76\u5e94\u7528\u4e8e\u57fa\u4e8e\u7ea6\u675f\u7684ILP\u7cfb\u7edf\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u9886\u57df\uff08\u5305\u62ec\u89c6\u89c9\u63a8\u7406\u548c\u6e38\u620f\u73a9\u6cd5\uff09\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5b66\u4e60\u65f6\u95f4\u3002\u4f8b\u5982\uff0c\u4ec5\u970010\u79d2\u7684\u9884\u5904\u7406\u65f6\u95f4\uff0c\u5c31\u53ef\u4ee5\u5c06\u5b66\u4e60\u65f6\u95f4\u4ece\u8d85\u8fc710\u5c0f\u65f6\u51cf\u5c11\u52302\u79d2\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7f29\u5c0f\u5047\u8bbe\u7a7a\u95f4\u63d0\u9ad8\u4e86ILP\u7cfb\u7edf\u7684\u6548\u7387\uff0c\u51cf\u5c11\u4e86\u5b66\u4e60\u65f6\u95f4\u4e14\u4e0d\u5f71\u54cd\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4e3a\u903b\u8f91\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5316\u624b\u6bb5\u3002"}}
{"id": "2506.06380", "pdf": "https://arxiv.org/pdf/2506.06380", "abs": "https://arxiv.org/abs/2506.06380", "authors": ["Jingyi Gu", "Xuan Zhang", "Guiling Wang"], "title": "Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Extreme events, such as market crashes, natural disasters, and pandemics, are\nrare but catastrophic, often triggering cascading failures across\ninterconnected systems. Accurate prediction and early warning can help minimize\nlosses and improve preparedness. While data-driven methods offer powerful\ncapabilities for extreme event modeling, they require abundant training data,\nyet extreme event data is inherently scarce, creating a fundamental challenge.\nSynthetic data generation has emerged as a powerful solution. However, existing\nsurveys focus on general data with privacy preservation emphasis, rather than\nextreme events' unique performance requirements. This survey provides the first\noverview of synthetic data generation for extreme events. We systematically\nreview generative modeling techniques and large language models, particularly\nthose enhanced by statistical theory as well as specialized training and\nsampling mechanisms to capture heavy-tailed distributions. We summarize\nbenchmark datasets and introduce a tailored evaluation framework covering\nstatistical, dependence, visual, and task-oriented metrics. A central\ncontribution is our in-depth analysis of each metric's applicability in\nextremeness and domain-specific adaptations, providing actionable guidance for\nmodel evaluation in extreme settings. We categorize key application domains and\nidentify underexplored areas like behavioral finance, wildfires, earthquakes,\nwindstorms, and infectious outbreaks. Finally, we outline open challenges,\nproviding a structured foundation for advancing synthetic rare-event research.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7efc\u8ff0\u4e86\u7528\u4e8e\u6781\u7aef\u4e8b\u4ef6\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u751f\u6210\u6a21\u578b\u548c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u91cd\u5c3e\u5206\u5e03\u65b9\u9762\u7684\u589e\u5f3a\u6280\u672f\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u6781\u7aef\u4e8b\u4ef6\u7684\u8bc4\u4f30\u6846\u67b6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u6781\u7aef\u4e8b\u4ef6\uff08\u5982\u5e02\u573a\u5d29\u76d8\u3001\u81ea\u7136\u707e\u5bb3\u548c\u6d41\u884c\u75c5\uff09\u867d\u7136\u7f55\u89c1\u4f46\u7834\u574f\u6027\u6781\u5927\uff0c\u51c6\u786e\u9884\u6d4b\u548c\u65e9\u671f\u9884\u8b66\u53ef\u4ee5\u51cf\u5c11\u635f\u5931\u5e76\u63d0\u9ad8\u51c6\u5907\u6c34\u5e73\u3002\u7136\u800c\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u6781\u7aef\u4e8b\u4ef6\u6570\u636e\u672c\u8d28\u4e0a\u662f\u7a00\u7f3a\u7684\uff0c\u8fd9\u6784\u6210\u4e86\u57fa\u672c\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u751f\u6210\u5efa\u6a21\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u901a\u8fc7\u7edf\u8ba1\u7406\u8bba\u589e\u5f3a\u4ee5\u53ca\u4e13\u95e8\u7684\u8bad\u7ec3\u548c\u91c7\u6837\u673a\u5236\u6765\u6355\u83b7\u91cd\u5c3e\u5206\u5e03\u7684\u6280\u672f\uff1b\u603b\u7ed3\u57fa\u51c6\u6570\u636e\u96c6\u5e76\u5f15\u5165\u5b9a\u5236\u8bc4\u4f30\u6846\u67b6\uff0c\u6db5\u76d6\u7edf\u8ba1\u3001\u4f9d\u8d56\u6027\u3001\u89c6\u89c9\u548c\u4efb\u52a1\u5bfc\u5411\u6307\u6807\u3002", "result": "\u63d0\u4f9b\u4e86\u6df1\u5165\u5206\u6790\u6bcf\u4e2a\u5ea6\u91cf\u6807\u51c6\u5728\u6781\u7aef\u6027\u548c\u9886\u57df\u7279\u5b9a\u9002\u5e94\u4e2d\u7684\u9002\u7528\u6027\uff0c\u4e3a\u6781\u7aef\u73af\u5883\u4e0b\u7684\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002\u5206\u7c7b\u5173\u952e\u5e94\u7528\u9886\u57df\u5e76\u786e\u5b9a\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u5982\u884c\u4e3a\u91d1\u878d\u3001\u91ce\u706b\u3001\u5730\u9707\u3001\u98ce\u66b4\u548c\u4f20\u67d3\u75c5\u7206\u53d1\u3002", "conclusion": "\u6982\u8ff0\u4e86\u5f00\u653e\u6027\u6311\u6218\uff0c\u4e3a\u63a8\u8fdb\u5408\u6210\u7a00\u6709\u4e8b\u4ef6\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u7684\u57fa\u7840\u3002"}}
{"id": "2506.07031", "pdf": "https://arxiv.org/pdf/2506.07031", "abs": "https://arxiv.org/abs/2506.07031", "authors": ["Jingyuan Ma", "Rui Li", "Zheng Li", "Junfeng Liu", "Lei Sha", "Zhifang Sui"], "title": "HauntAttack: When Attack Follows Reasoning as a Shadow", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and\nreasoning tasks, showcasing exceptional capabilities. However, the enhancement\nof reasoning abilities and the exposure of their internal reasoning processes\nintroduce new safety vulnerabilities. One intriguing concern is: when reasoning\nis strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs\nexhibit? To address this issue, we introduce HauntAttack, a novel and\ngeneral-purpose black-box attack framework that systematically embeds harmful\ninstructions into reasoning questions. Specifically, we treat reasoning\nquestions as carriers and substitute one of their original conditions with a\nharmful instruction. This process creates a reasoning pathway in which the\nmodel is guided step by step toward generating unsafe outputs. Based on\nHauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results\nreveal that even the most advanced LRMs exhibit significant safety\nvulnerabilities. Additionally, we perform a detailed analysis of different\nmodels, various types of harmful instructions, and model output patterns,\nproviding valuable insights into the security of LRMs.", "AI": {"tldr": "Emerging Large Reasoning Models (LRMs) have exceptional capabilities in mathematical and reasoning tasks, but their reasoning abilities expose new safety vulnerabilities. This paper introduces HauntAttack, a black-box attack framework that embeds harmful instructions into reasoning questions, revealing significant safety vulnerabilities in LRMs.", "motivation": "To investigate the safety-reasoning trade-off in LRMs when reasoning is strongly entangled with harmfulness.", "method": "Introduce HauntAttack, a novel black-box attack framework that systematically embeds harmful instructions into reasoning questions by substituting one of their original conditions with a harmful instruction, guiding the model toward generating unsafe outputs.", "result": "Experiments on multiple LRMs reveal significant safety vulnerabilities even in the most advanced models. Detailed analysis provides insights into the security of LRMs.", "conclusion": "LRMs exhibit significant safety vulnerabilities when exposed to the HauntAttack framework, raising concerns about their security."}}
{"id": "2506.06740", "pdf": "https://arxiv.org/pdf/2506.06740", "abs": "https://arxiv.org/abs/2506.06740", "authors": ["Yigui Feng", "Qinglin Wang", "Ke Liu", "Xinhai Chen", "Bo Yang", "Jie Liu"], "title": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method", "categories": ["cs.AI"], "comment": null, "summary": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research.", "AI": {"tldr": "AI PsyRoom is a multi-agent framework enhancing psychological counseling via empathetic conversations and personalized treatment plans, showing significant improvements over existing methods.", "motivation": "Psychological counseling faces challenges with increasing demand and shortage of professionals. Existing LLMs lack deep emotional understanding and ability to generate personalized treatment plans.", "method": "Developed AI PsyRoom with two components: PsyRoom A for dialogue reconstruction using fine-grained emotion classification and generating dataset EmoPsy; PsyRoom B for creating personalized treatment plans.", "result": "Achieved 18% improvement in problem orientation, 23% in expression, 24% in empathy, and 16% in interactive communication quality compared to state-of-the-art methods.", "conclusion": "AI PsyRoom significantly enhances psychological counseling through improved empathetic and emotionally nuanced conversations and personalized treatment plans, offering a valuable resource for advancing AI-assisted psychological counseling."}}
{"id": "2506.06398", "pdf": "https://arxiv.org/pdf/2506.06398", "abs": "https://arxiv.org/abs/2506.06398", "authors": ["Yin Li"], "title": "Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization", "categories": ["cs.LG", "cs.AI", "68T07, 68Q32", "I.2.6; I.2.7; F.1.1"], "comment": null, "summary": "Positional encodings are a core part of transformer-based models, enabling\nprocessing of sequential data without recurrence. This paper presents a\ntheoretical framework to analyze how various positional encoding methods,\nincluding sinusoidal, learned, relative, and bias-based methods like Attention\nwith Linear Biases (ALiBi), impact a transformer's expressiveness,\ngeneralization ability, and extrapolation to longer sequences. Expressiveness\nis defined via function approximation, generalization bounds are established\nusing Rademacher complexity, and new encoding methods based on orthogonal\nfunctions, such as wavelets and Legendre polynomials, are proposed. The\nextrapolation capacity of existing and proposed encodings is analyzed,\nextending ALiBi's biasing approach to a unified theoretical context.\nExperimental evaluation on synthetic sequence-to-sequence tasks shows that\northogonal transform-based encodings outperform traditional sinusoidal\nencodings in generalization and extrapolation. This work addresses a critical\ngap in transformer theory, providing insights for design choices in natural\nlanguage processing, computer vision, and other transformer applications.", "AI": {"tldr": "The paper analyzes the impact of various positional encoding methods on transformer models' expressiveness, generalization ability, and extrapolation capacity. It proposes new orthogonal function-based encodings that perform better in experiments.", "motivation": "To understand how different positional encoding methods affect transformers' capabilities and to address a critical gap in transformer theory.", "method": "Theoretical framework using function approximation for expressiveness, Rademacher complexity for generalization bounds, and proposing new orthogonal function-based positional encoding methods. Experimental evaluation on synthetic sequence-to-sequence tasks.", "result": "Orthogonal transform-based encodings outperform traditional sinusoidal encodings in generalization and extrapolation.", "conclusion": "This work provides insights into design choices for transformers in NLP, computer vision, and other applications."}}
{"id": "2506.07034", "pdf": "https://arxiv.org/pdf/2506.07034", "abs": "https://arxiv.org/abs/2506.07034", "authors": ["Shiqi Liu", "Yongpeng Gao", "Mingyang Zhang", "Jie Wang"], "title": "NanoZone: Scalable, Efficient, and Secure Memory Protection for Arm CCA", "categories": ["cs.CR"], "comment": null, "summary": "Arm Confidential Computing Architecture (CCA) currently isolates at the\ngranularity of an entire Confidential Virtual Machine (CVM), leaving intra-VM\nbugs such as Heartbleed unmitigated. The state-of-the-art narrows this to the\nprocess level, yet still cannot stop attacks that pivot within the same\nprocess, and prior intra-enclave schemes are either too slow or incompatible\nwith CVM-style isolation. We extend CCA with a three-tier zone model that\nspawns an unlimited number of lightweight isolation domains inside a single\nprocess, while shielding them from kernel-space adversaries. To block\ndomain-switch abuse, we also add a fast user-level Code-Pointer Integrity (CPI)\nmechanism. We developed two prototypes: a functional version on Arm's official\nsimulator to validate resistance against intra-process and kernel-space\nadversaries, and a performance variant on Arm development boards evaluated for\nsession-key isolation within server applications, in-memory key-value\nprotection, and non-volatile-memory data isolation. NanoZone incurs roughly a\n20% performance overhead while retaining 95% throughput compared to the system\nwithout fine-grained isolation.", "AI": {"tldr": "The paper introduces an extension to Arm Confidential Computing Architecture (CCA) with a three-tier zone model enabling lightweight isolation domains within a single process, enhanced by a user-level Code-Pointer Integrity mechanism. Prototypes show resistance to intra-process and kernel-space attacks with a 20% performance overhead while retaining 95% throughput.", "motivation": "Arm CCA isolates at the granularity of an entire Confidential Virtual Machine (CVM), which cannot mitigate intra-VM bugs like Heartbleed. Existing solutions narrow this to the process level but can't stop attacks within the same process or are either too slow or incompatible with CVM-style isolation.", "method": "The authors extend CCA with a three-tier zone model that creates an unlimited number of lightweight isolation domains inside a single process, shielding them from kernel-space adversaries. They also add a fast user-level Code-Pointer Integrity (CPI) mechanism to block domain-switch abuse. Two prototypes were developed: one functional on Arm's official simulator and another for performance evaluation on Arm development boards.", "result": "NanoZone incurs approximately a 20% performance overhead while maintaining 95% throughput compared to a system without fine-grained isolation. The prototypes successfully demonstrated resistance against intra-process and kernel-space adversaries in scenarios such as session-key isolation, in-memory key-value protection, and non-volatile-memory data isolation.", "conclusion": "The extension to CCA with the three-tier zone model and CPI mechanism provides effective intra-process isolation while preserving performance, addressing limitations of current isolation approaches."}}
{"id": "2506.06750", "pdf": "https://arxiv.org/pdf/2506.06750", "abs": "https://arxiv.org/abs/2506.06750", "authors": ["Zofia Rudnicka", "Janusz Szczepanski", "Agnieszka Pregowska"], "title": "Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Training of Spiking Neural Networks (SNN) is challenging due to their unique\nproperties, including temporal dynamics, non-differentiability of spike events,\nand sparse event-driven activations. In this paper, we widely consider the\ninfluence of the type of chosen learning algorithm, including bioinspired\nlearning rules on the accuracy of classification. We proposed a bioinspired\nclassifier based on the combination of SNN and Lempel-Ziv complexity (LZC).\nThis approach synergizes the strengths of SNNs in temporal precision and\nbiological realism with LZC's structural complexity analysis, facilitating\nefficient and interpretable classification of spatiotemporal neural data. It\nturned out that the classic backpropagation algorithm achieves excellent\nclassification accuracy, but at extremely high computational cost, which makes\nit impractical for real-time applications. Biologically inspired learning\nalgorithms such as tempotron and Spikprop provide increased computational\nefficiency while maintaining competitive classification performance, making\nthem suitable for time-sensitive tasks. The results obtained indicate that the\nselection of the most appropriate learning algorithm depends on the trade-off\nbetween classification accuracy and computational cost as well as application\nconstraints.", "AI": {"tldr": "\u5728\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u4e2d\uff0c\u9009\u62e9\u5408\u9002\u7684\u5b66\u4e60\u7b97\u6cd5\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u867d\u7136\u51c6\u786e\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u800c\u751f\u7269\u542f\u53d1\u5f0f\u5b66\u4e60\u7b97\u6cd5\uff08\u5982tempotron\u548cSpikprop\uff09\u5219\u5728\u4fdd\u6301\u8f83\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u7531\u4e8eSNN\u7684\u72ec\u7279\u6027\u8d28\uff08\u65f6\u95f4\u52a8\u6001\u3001\u975e\u53ef\u5fae\u7684\u8109\u51b2\u4e8b\u4ef6\u53ca\u7a00\u758f\u6fc0\u6d3b\uff09\uff0c\u5176\u8bad\u7ec3\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u4e0d\u540c\u5b66\u4e60\u7b97\u6cd5\u7c7b\u578b\uff08\u5305\u62ec\u751f\u7269\u542f\u53d1\u89c4\u5219\uff09\u5bf9\u5206\u7c7b\u7cbe\u5ea6\u7684\u5f71\u54cd\u662f\u5fc5\u8981\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SNN\u4e0eLempel-Ziv\u590d\u6742\u5ea6\uff08LZC\uff09\u7684\u751f\u7269\u542f\u53d1\u5206\u7c7b\u5668\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86SNN\u7684\u65f6\u95f4\u7cbe\u786e\u6027\u548c\u751f\u7269\u771f\u5b9e\u6027\uff0c\u4ee5\u53caLZC\u7684\u7ed3\u6784\u590d\u6742\u6027\u5206\u6790\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u65f6\u7a7a\u795e\u7ecf\u6570\u636e\u5206\u7c7b\u3002", "result": "\u7ecf\u5178\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\u5c3d\u7ba1\u6709\u6781\u9ad8\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u800c\u4e0d\u9002\u5408\u5b9e\u65f6\u5e94\u7528\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u751f\u7269\u542f\u53d1\u5b66\u4e60\u7b97\u6cd5\uff08\u5982tempotron\u548cSpikprop\uff09\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u6700\u9002\u5b66\u4e60\u7b97\u6cd5\u7684\u9009\u62e9\u53d6\u51b3\u4e8e\u5206\u7c7b\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ee5\u53ca\u5177\u4f53\u5e94\u7528\u573a\u666f\u7684\u7ea6\u675f\u6761\u4ef6\u3002"}}
{"id": "2506.06411", "pdf": "https://arxiv.org/pdf/2506.06411", "abs": "https://arxiv.org/abs/2506.06411", "authors": ["Paul Fogel", "Christophe Geissler", "George Luta"], "title": "CoxNTF: A New Approach for Joint Clustering and Prediction in Survival Analysis", "categories": ["cs.LG"], "comment": "7 pages, 3 figures, Conference on Lifetime Data Science 2025,\n  Brooklyn, New York, USA", "summary": "The interpretation of the results of survival analysis often benefits from\nlatent factor representations of baseline covariates. However, existing\nmethods, such as Nonnegative Matrix Factorization (NMF), do not incorporate\nsurvival information, limiting their predictive power. We present CoxNTF, a\nnovel approach that uses non-negative tensor factorization (NTF) to derive\nmeaningful latent representations that are closely associated with survival\noutcomes. CoxNTF constructs a weighted covariate tensor in which survival\nprobabilities derived from the Coxnet model are used to guide the tensorization\nprocess. Our results show that CoxNTF achieves survival prediction performance\ncomparable to using Coxnet with the original covariates, while providing a\nstructured and interpretable clustering framework. In addition, the new\napproach effectively handles feature redundancy, making it a powerful tool for\njoint clustering and prediction in survival analysis.", "AI": {"tldr": "CoxNTF is a new method using non-negative tensor factorization to create latent representations linked with survival outcomes, offering comparable prediction performance to Coxnet but with added interpretability and ability to handle feature redundancy.", "motivation": "Existing methods like NMF do not incorporate survival information, which limits their predictive power in survival analysis.", "method": "CoxNTF constructs a weighted covariate tensor using survival probabilities from the Coxnet model to guide the tensorization process, deriving meaningful latent representations closely associated with survival outcomes.", "result": "CoxNTF achieves survival prediction performance comparable to using Coxnet with original covariates, provides an interpretable clustering framework, and effectively handles feature redundancy.", "conclusion": "CoxNTF is a powerful tool for joint clustering and prediction in survival analysis, enhancing both interpretability and handling of feature redundancy."}}
{"id": "2506.07077", "pdf": "https://arxiv.org/pdf/2506.07077", "abs": "https://arxiv.org/abs/2506.07077", "authors": ["Qianshan Wei", "Jiaqi Li", "Zihan You", "Yi Zhan", "Kecen Li", "Jialin Wu", "Xinfeng Li Hengjun Liu", "Yi Yu", "Bin Cao", "Yiwen Xu", "Yang Liu", "Guilin Qi"], "title": "Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Differential Privacy (DP) is a widely adopted technique, valued for its\neffectiveness in protecting the privacy of task-specific datasets, making it a\ncritical tool for large language models. However, its effectiveness in\nMultimodal Large Language Models (MLLMs) remains uncertain. Applying\nDifferential Privacy (DP) inherently introduces substantial computation\noverhead, a concern particularly relevant for MLLMs which process extensive\ntextual and visual data. Furthermore, a critical challenge of DP is that the\ninjected noise, necessary for privacy, scales with parameter dimensionality,\nleading to pronounced model degradation; This trade-off between privacy and\nutility complicates the application of Differential Privacy (DP) to complex\narchitectures like MLLMs. To address these, we propose Dual-Priv Pruning, a\nframework that employs two complementary pruning mechanisms for DP fine-tuning\nin MLLMs: (i) visual token pruning to reduce input dimensionality by removing\nredundant visual information, and (ii) gradient-update pruning during the DP\noptimization process. This second mechanism selectively prunes parameter\nupdates based on the magnitude of noisy gradients, aiming to mitigate noise\nimpact and improve utility. Experiments demonstrate that our approach achieves\ncompetitive results with minimal performance degradation. In terms of\ncomputational efficiency, our approach consistently utilizes less memory than\nstandard DP-SGD. While requiring only 1.74% more memory than zeroth-order\nmethods which suffer from severe performance issues on A100 GPUs, our method\ndemonstrates leading memory efficiency on H20 GPUs. To the best of our\nknowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is\ncoming soon.", "AI": {"tldr": "The paper introduces Dual-Priv Pruning, a framework for applying Differential Privacy (DP) to Multimodal Large Language Models (MLLMs). It uses visual token pruning and gradient-update pruning to reduce computation overhead and model degradation. Experiments show competitive results with minimal performance loss and improved memory efficiency.", "motivation": "Differential Privacy (DP) is effective in protecting privacy but introduces substantial computation overhead and model degradation when applied to complex architectures like MLLMs. The motivation is to address these challenges and effectively apply DP to MLLMs.", "method": "Dual-Priv Pruning framework employs two complementary pruning mechanisms: (i) visual token pruning to reduce input dimensionality by removing redundant visual information, and (ii) gradient-update pruning during the DP optimization process to selectively prune parameter updates based on the magnitude of noisy gradients.", "result": "Experiments demonstrate that Dual-Priv Pruning achieves competitive results with minimal performance degradation. It consistently utilizes less memory than standard DP-SGD and shows leading memory efficiency on H20 GPUs compared to zeroth-order methods.", "conclusion": "Dual-Priv Pruning is an effective approach for DP fine-tuning in MLLMs, achieving a good balance between privacy and utility while improving computational efficiency."}}
{"id": "2506.06786", "pdf": "https://arxiv.org/pdf/2506.06786", "abs": "https://arxiv.org/abs/2506.06786", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain", "categories": ["cs.AI"], "comment": "6 pages, 2 figures, 3 tables, submitted as a regural paper to IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC) 2025", "summary": "Autonomous systems operating in high-stakes search-and-rescue (SAR) missions\nmust continuously gather mission-critical information while flexibly adapting\nto shifting operational priorities. We propose CA-MIQ (Context-Aware\nMax-Information Q-learning), a lightweight dual-critic reinforcement learning\n(RL) framework that dynamically adjusts its exploration strategy whenever\nmission priorities change. CA-MIQ pairs a standard extrinsic critic for task\nreward with an intrinsic critic that fuses state-novelty, information-location\nawareness, and real-time priority alignment. A built-in shift detector triggers\ntransient exploration boosts and selective critic resets, allowing the agent to\nre-focus after a priority revision. In a simulated SAR grid-world, where\nexperiments specifically test adaptation to changes in the priority order of\ninformation types the agent is expected to focus on, CA-MIQ achieves nearly\nfour times higher mission-success rates than baselines after a single priority\nshift and more than three times better performance in multiple-shift scenarios,\nachieving 100% recovery while baseline methods fail to adapt. These results\nhighlight CA-MIQ's effectiveness in any discrete environment with\npiecewise-stationary information-value distributions.", "AI": {"tldr": "CA-MIQ is a dual-critic reinforcement learning framework that enables autonomous systems to better adapt to changing priorities in search-and-rescue missions, showing significantly improved mission success rates compared to baseline methods.", "motivation": "Autonomous systems in high-stakes search-and-rescue missions need to gather critical information and adapt flexibly to shifting operational priorities.", "method": "Proposed CA-MIQ uses a lightweight dual-critic RL framework with an extrinsic critic for task reward and an intrinsic critic that combines state-novelty, information-location awareness, and real-time priority alignment. It also has a built-in shift detector for exploration boosts and selective critic resets.", "result": "In simulated SAR grid-world experiments, CA-MIQ achieves nearly four times higher mission-success rates than baselines after a single priority shift and more than three times better performance in multiple-shift scenarios, with 100% recovery.", "conclusion": "CA-MIQ proves effective in discrete environments with piecewise-stationary information-value distributions, making it suitable for dynamic search-and-rescue missions."}}
{"id": "2506.06412", "pdf": "https://arxiv.org/pdf/2506.06412", "abs": "https://arxiv.org/abs/2506.06412", "authors": ["Junming Wang", "Yi Shi"], "title": "NeurNCD: Novel Class Discovery via Implicit Neural Representation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICMR 2024", "summary": "Discovering novel classes in open-world settings is crucial for real-world\napplications. Traditional explicit representations, such as object descriptors\nor 3D segmentation maps, are constrained by their discrete, hole-prone, and\nnoisy nature, which hinders accurate novel class discovery. To address these\nchallenges, we introduce NeurNCD, the first versatile and data-efficient\nframework for novel class discovery that employs the meticulously designed\nEmbedding-NeRF model combined with KL divergence as a substitute for\ntraditional explicit 3D segmentation maps to aggregate semantic embedding and\nentropy in visual embedding space. NeurNCD also integrates several key\ncomponents, including feature query, feature modulation and clustering,\nfacilitating efficient feature augmentation and information exchange between\nthe pre-trained semantic segmentation network and implicit neural\nrepresentations. As a result, our framework achieves superior segmentation\nperformance in both open and closed-world settings without relying on densely\nlabelled datasets for supervised training or human interaction to generate\nsparse label supervision. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches on the NYUv2 and Replica\ndatasets.", "AI": {"tldr": "NeurNCD is a novel framework that uses Embedding-NeRF model with KL divergence for efficient novel class discovery in both open and closed-world settings without needing densely labelled datasets or human interaction.", "motivation": "Traditional explicit representations like object descriptors or 3D segmentation maps are limited by their discrete, hole-prone, and noisy nature, which affects accurate novel class discovery.", "method": "NeurNCD employs Embedding-NeRF model combined with KL divergence as an alternative to traditional explicit 3D segmentation maps. It aggregates semantic embedding and entropy in visual embedding space and integrates feature query, feature modulation and clustering components for efficient feature augmentation and information exchange.", "result": "The framework achieves superior segmentation performance in both open and closed-world settings without relying on densely labelled datasets or human interaction for supervision.", "conclusion": "Extensive experiments show that NeurNCD significantly outperforms state-of-the-art approaches on the NYUv2 and Replica datasets."}}
{"id": "2506.07153", "pdf": "https://arxiv.org/pdf/2506.07153", "abs": "https://arxiv.org/abs/2506.07153", "authors": ["Avishag Shapira", "Parth Atulbhai Gandhi", "Edan Habler", "Oleg Brodt", "Asaf Shabtai"], "title": "Mind the Web: The Security of Web Use Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Web-use agents are rapidly being deployed to automate complex web tasks,\noperating with extensive browser capabilities including multi-tab navigation,\nDOM manipulation, JavaScript execution and authenticated session access.\nHowever, these powerful capabilities create a critical and previously\nunexplored attack surface. This paper demonstrates how attackers can exploit\nweb-use agents' high-privilege capabilities by embedding malicious content in\nweb pages such as comments, reviews, or advertisements that agents encounter\nduring legitimate browsing tasks. In addition, we introduce the task-aligned\ninjection technique that frame malicious commands as helpful task guidance\nrather than obvious attacks. This technique exploiting fundamental limitations\nin LLMs' contextual reasoning: agents struggle in maintaining coherent\ncontextual awareness and fail to detect when seemingly helpful web content\ncontains steering attempts that deviate from their original task goal. Through\nsystematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do\nBrowser, OpenOperator), we demonstrate nine payload types that compromise\nconfidentiality, integrity, and availability, including unauthorized camera\nactivation, user impersonation, local file exfiltration, password leakage, and\ndenial of service, with validation across multiple LLMs achieving success rates\nof 80%-100%. These payloads succeed across agents with built-in safety\nmechanisms, requiring only the ability to post content on public websites,\ncreating unprecedented risks given the ease of exploitation combined with\nagents' high-privilege access. To address this attack, we propose comprehensive\nmitigation strategies including oversight mechanisms, execution constraints,\nand task-aware reasoning techniques, providing practical directions for secure\ndevelopment and deployment.", "AI": {"tldr": "Web-use agents with advanced browser capabilities introduce new attack vectors where malicious content embedded in web pages can exploit these agents to perform unauthorized actions, compromising confidentiality, integrity, and availability. Payloads tested on four popular agents achieved success rates of 80%-100%. Mitigation strategies are proposed.", "motivation": "To expose the critical yet unexplored attack surface created by web-use agents' powerful capabilities and demonstrate how attackers can exploit these agents through task-aligned injection techniques.", "method": "Demonstrated nine payload types that compromise agent security by embedding malicious content in web pages and framing commands as helpful guidance. Evaluated on four popular agents using multiple LLMs.", "result": "Payloads successfully compromised confidentiality, integrity, and availability across agents with built-in safety mechanisms, achieving success rates of 80%-100%.", "conclusion": "Proposed comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques to secure the development and deployment of web-use agents."}}
{"id": "2506.06832", "pdf": "https://arxiv.org/pdf/2506.06832", "abs": "https://arxiv.org/abs/2506.06832", "authors": ["Cl\u00e9ment Hongler", "Andrew Emil"], "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.IT", "cs.NE", "math.IT"], "comment": "41 pages, 16 figures", "summary": "Large Language Models (LLMs) define probability measures on text. By\nconsidering the implicit knowledge question of what it means for an LLM to know\nsuch a measure and what it entails algorithmically, we are naturally led to\nformulate a series of tasks that go beyond generative sampling, involving forms\nof summarization, counterfactual thinking, anomaly detection, originality\nsearch, reverse prompting, debating, creative solving, etc. These tasks can be\nformulated as games based on LLM measures, which we call Cross-Entropy (Xent)\nGames. Xent Games can be single-player or multi-player. They involve\ncross-entropy scores and cross-entropy constraints, and can be expressed as\nsimple computational graphs and programs. We show the Xent Game space is large\nenough to contain a wealth of interesting examples, while being constructible\nfrom basic game-theoretic consistency axioms. We then discuss how the Xent Game\nspace can be used to measure the abilities of LLMs. This leads to the\nconstruction of Xent Game measures: finite families of Xent Games that can be\nused as capability benchmarks, built from a given scope, by extracting a\ncovering measure. To address the unbounded scope problem associated with the\nchallenge of measuring general abilities, we propose to explore the space of\nXent Games in a coherent fashion, using ideas inspired by evolutionary\ndynamics.", "AI": {"tldr": "Large Language Models (LLMs) can define probability measures on text, leading to the formulation of a series of tasks beyond generative sampling, which can be formulated as games based on LLM measures, called Cross-Entropy (Xent) Games. Xent Games can be used to measure the abilities of LLMs.", "motivation": "To explore what it means for an LLM to know a probability measure on text and what it entails algorithmically, leading to the formulation of tasks beyond generative sampling.", "method": "Formulate tasks as games based on LLM measures, called Cross-Entropy (Xent) Games, which can be single-player or multi-player, involve cross-entropy scores and constraints, and can be expressed as computational graphs and programs.", "result": "The Xent Game space is large enough to contain interesting examples and can be constructed from basic game-theoretic consistency axioms. Xent Game measures can be used as capability benchmarks for LLMs.", "conclusion": "To address the unbounded scope problem in measuring general abilities of LLMs, the space of Xent Games should be explored in a coherent fashion using ideas inspired by evolutionary dynamics."}}
{"id": "2506.06443", "pdf": "https://arxiv.org/pdf/2506.06443", "abs": "https://arxiv.org/abs/2506.06443", "authors": ["Luis Pinto"], "title": "Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Pretrained molecular encoders have become indispensable in computational\nchemistry for tasks such as property prediction and molecular generation.\nHowever, the standard practice of relying solely on final-layer embeddings for\ndownstream tasks may discard valuable information. In this work, we challenge\nthis convention by conducting a comprehensive layer-wise analysis of five\ndiverse molecular encoders across 22 ADMET property prediction tasks. Our\nresults demonstrate that embeddings from intermediate layers consistently\noutperform final-layer representations. Specifically, using fixed embeddings\nfrom the optimal intermediate layers improved downstream performance by an\naverage of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to\nthese intermediate layers yielded even greater average improvements of 8.5%,\nwith performance increases as high as 40.8%, achieving new state-of-the-art\nresults on several benchmarks. Additionally, a strong positive correlation\nbetween fixed embedding performance and finetuning outcomes supports an\nefficient evaluate-then-finetune approach, enabling identification of optimal\nlayers with reduced computational cost. These findings highlight the importance\nof exploring the full representational depth of molecular encoders to achieve\nsubstantial performance improvements and computational efficiency. The code is\nmade publicly available at\nhttps://github.com/luispintoc/Unlocking-Chemical-Insights.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u4e94\u79cd\u5206\u5b50\u7f16\u7801\u5668\u572822\u4e2aADMET\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5c42\u8868\u73b0\uff0c\u53d1\u73b0\u4e2d\u95f4\u5c42\u5d4c\u5165\u4f18\u4e8e\u6700\u7ec8\u5c42\u8868\u793a\u3002\u4f7f\u7528\u56fa\u5b9a\u4e2d\u95f4\u5c42\u5d4c\u5165\u53ef\u4f7f\u4e0b\u6e38\u6027\u80fd\u5e73\u5747\u63d0\u53475.4%\uff0c\u5fae\u8c03\u81f3\u4e2d\u95f4\u5c42\u5219\u53ef\u83b7\u5f978.5%\u7684\u66f4\u5927\u6539\u8fdb\u3002\u6b64\u65b9\u6cd5\u80fd\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u8fbe\u5230\u65b0\u7684SOTA\u7ed3\u679c\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u5316\u5b66\u4e2d\u666e\u904d\u4f9d\u8d56\u9884\u8bad\u7ec3\u5206\u5b50\u7f16\u7801\u5668\u7684\u6700\u7ec8\u5c42\u5d4c\u5165\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e00\u505a\u6cd5\u53ef\u80fd\u4f1a\u4e22\u5931\u6709\u4ef7\u503c\u7684\u4fe1\u606f\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5206\u5b50\u7f16\u7801\u5668\u7684\u5168\u90e8\u8868\u793a\u6df1\u5ea6\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5bf9\u4e94\u4e2a\u4e0d\u540c\u7684\u5206\u5b50\u7f16\u7801\u5668\u8fdb\u884c\u9010\u5c42\u5206\u6790\uff0c\u572822\u4e2aADMET\u5c5e\u6027\u9884\u6d4b\u4efb\u52a1\u4e0a\u8bc4\u4f30\u5b83\u4eec\u7684\u8868\u73b0\uff1b\u91c7\u7528\u56fa\u5b9a\u4e2d\u95f4\u5c42\u5d4c\u5165\u4e0e\u5fae\u8c03\u5230\u4e2d\u95f4\u5c42\u7684\u65b9\u6cd5\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e76\u6bd4\u8f83\u5176\u4e0e\u4ec5\u4f7f\u7528\u6700\u7ec8\u5c42\u5d4c\u5165\u7684\u7ed3\u679c\u3002", "result": "\u4f7f\u7528\u56fa\u5b9a\u4e2d\u95f4\u5c42\u5d4c\u5165\u6bd4\u6700\u7ec8\u5c42\u8868\u793a\u5e73\u5747\u63d0\u5347\u4e865.4%\u7684\u6027\u80fd\uff0c\u6700\u9ad8\u53ef\u8fbe28.6%\uff1b\u5fae\u8c03\u81f3\u4e2d\u95f4\u5c42\u5219\u5e26\u6765\u4e86\u66f4\u5927\u7684\u6539\u8fdb\uff0c\u5e73\u5747\u63d0\u53478.5%\uff0c\u6700\u9ad8\u53ef\u8fbe40.8%\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u65b0SOTA\u7ed3\u679c\u3002\u6b64\u5916\uff0c\u56fa\u5b9a\u5d4c\u5165\u6027\u80fd\u4e0e\u5fae\u8c03\u7ed3\u679c\u5448\u5f3a\u6b63\u76f8\u5173\u3002", "conclusion": "\u63a2\u7d22\u5206\u5b50\u7f16\u7801\u5668\u7684\u5b8c\u6574\u8868\u793a\u6df1\u5ea6\u5bf9\u4e8e\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5148\u8bc4\u4f30\u540e\u5fae\u8c03\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.07190", "pdf": "https://arxiv.org/pdf/2506.07190", "abs": "https://arxiv.org/abs/2506.07190", "authors": ["Hidemasa Kawasaki", "Soramichi Akiyama"], "title": "A Simulation-based Evaluation Framework for Inter-VM RowHammer Mitigation Techniques", "categories": ["cs.CR"], "comment": "Presented in Fifth Workshop on DRAM Security (DRAMSec), June 21, 2025", "summary": "Inter-VM RowHammer is an attack that induces a bitflip beyond the boundaries\nof virtual machines (VMs) to compromise a VM from another, and some\nsoftware-based techniques have been proposed to mitigate this attack.\nEvaluating these mitigation techniques requires to confirm that they actually\nmitigate inter-VM RowHammer in low overhead. A challenge in this evaluation\nprocess is that both the mitigation ability and the overhead depend on the\nunderlying hardware whose DRAM address mappings are different from machine to\nmachine. This makes comprehensive evaluation prohibitively costly or even\nimplausible as no machine that has a specific DRAM address mapping might be\navailable. To tackle this challenge, we propose a simulation-based framework to\nevaluate software-based inter-VM RowHammer mitigation techniques across\nconfigurable DRAM address mappings. We demonstrate how to reproduce existing\nmitigation techniques on our framework, and show that it can evaluate the\nmitigation abilities and performance overhead of them with configurable DRAM\naddress mappings.", "AI": {"tldr": "An abstract about a simulation-based framework to evaluate software-based inter-VM RowHammer mitigation techniques across configurable DRAM address mappings.", "motivation": "Inter-VM RowHammer is an attack that induces a bitflip beyond the boundaries of virtual machines (VMs) and some software-based techniques have been proposed to mitigate this attack. Evaluating these mitigation techniques requires confirmation that they actually mitigate inter-VM RowHammer in low overhead.", "method": "The authors propose a simulation-based framework to evaluate software-based inter-VM RowHammer mitigation techniques across configurable DRAM address mappings.", "result": "They demonstrate how to reproduce existing mitigation techniques on their framework, and show that it can evaluate the mitigation abilities and performance overhead of them with configurable DRAM address mappings.", "conclusion": "This makes comprehensive evaluation prohibitively costly or even implausible as no machine that has a specific DRAM address mapping might be available."}}
{"id": "2506.06843", "pdf": "https://arxiv.org/pdf/2506.06843", "abs": "https://arxiv.org/abs/2506.06843", "authors": ["HaoYang Shang", "Xuan Liu", "Zi Liang", "Jie Zhang", "Haibo Hu", "Song Guo"], "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings.", "AI": {"tldr": "Large Language Models (LLMs) struggle with complex tasks due to a performance ceiling, similar to human cognitive load limits. This paper introduces CoThinker, a multi-agent framework based on Cognitive Load Theory (CLT), which enhances problem-solving by distributing cognitive loads among specialized agents and using structured communication. Empirical results show improvements in solution quality and efficiency over other multi-agent systems.", "motivation": "To address the limitations of LLMs in handling complex, multi-faceted tasks where demands exceed their cognitive load capacity, drawing parallels from Cognitive Load Theory in human cognition.", "method": "Introduced CoThinker, an LLM-based multi-agent framework that applies CLT principles. It involves agent specialization to distribute intrinsic cognitive load and uses structured communication and collective working memory to manage transactional load.", "result": "CoThinker showed improvements in solution quality and efficiency when tested on complex problem-solving tasks and high cognitive load scenarios compared to existing multi-agent baselines. Characteristic interaction patterns emerged, providing insights into collective cognition and effective load management.", "conclusion": "CoThinker offers a principled approach to overcoming the performance ceilings of LLMs by effectively managing cognitive loads through a multi-agent system inspired by CLT."}}
{"id": "2506.06444", "pdf": "https://arxiv.org/pdf/2506.06444", "abs": "https://arxiv.org/abs/2506.06444", "authors": ["Ruizhong Qiu", "Gaotang Li", "Tianxin Wei", "Jingrui He", "Hanghang Tong"], "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "19 pages", "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .", "AI": {"tldr": "\u73b0\u6709\u7684\u5b89\u5168\u4fdd\u8bc1\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9f50\u4ee5\u5c06\u5b89\u5168\u884c\u4e3a\u690d\u5165LLM\u4e2d\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230\u591a\u6837\u5316\u7684\u8d8a\u72f1\u653b\u51fb\u3002\u6211\u4eec\u7684\u5de5\u4f5c\u5f00\u521b\u4e86\u63a8\u7406\u6269\u5c55\u4ee5\u63d0\u9ad8LLM\u7684\u5b89\u5168\u6027\uff0c\u5e76\u63d0\u51fa\u4e86SAFFRON\u65b0\u8303\u5f0f\u6765\u89e3\u51b3\u63a2\u7d22-\u6548\u7387\u56f0\u5883\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u76f8\u5173\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u4fdd\u8bc1\u7814\u7a76\u5728\u63a8\u7406\u6269\u5c55\u65b9\u9762\u7684\u4e0d\u8db3\u4ee5\u53ca\u4f20\u7edf\u63a8\u7406\u6269\u5c55\u6280\u672f\u5728\u5b89\u5168\u6027\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u4fc3\u4f7f\u6211\u4eec\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6269\u5c55\u8303\u5f0f\u4ee5\u63d0\u9ad8LLM\u7684\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faSAFFRON\uff0c\u5305\u62ec\u591a\u5206\u652f\u5956\u52b1\u6a21\u578b\uff08MRM\uff09\u3001\u90e8\u5206\u76d1\u7763\u8bad\u7ec3\u76ee\u6807\u3001\u4fdd\u5b88\u63a2\u7d22\u7ea6\u675f\u548c\u57fa\u4e8eTrie\u7684\u952e\u503c\u7f13\u5b58\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SAFFRON\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "SAFFRON\u4e3a\u63d0\u9ad8LLM\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6269\u5c55\u8303\u5f0f\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u76f8\u5173\u8d44\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2506.07200", "pdf": "https://arxiv.org/pdf/2506.07200", "abs": "https://arxiv.org/abs/2506.07200", "authors": ["Kanato Nakanishi", "Soramichi Akiyama"], "title": "Efficient RL-based Cache Vulnerability Exploration by Penalizing Useless Agent Actions", "categories": ["cs.CR"], "comment": "Presented in Machine Learning for Computer Architecture and Systems\n  (MLArchSys), June 21, 2025", "summary": "Cache-timing attacks exploit microarchitectural characteristics to leak\nsensitive data, posing a severe threat to modern systems. Despite its severity,\nanalyzing the vulnerability of a given cache structure against cache-timing\nattacks is challenging. To this end, a method based on Reinforcement Learning\n(RL) has been proposed to automatically explore vulnerabilities for a given\ncache structure. However, a naive RL-based approach suffers from inefficiencies\ndue to the agent performing actions that do not contribute to the exploration.\nIn this paper, we propose a method to identify these useless actions during\ntraining and penalize them so that the agent avoids them and the exploration\nefficiency is improved. Experiments on 17 cache structures show that our\ntraining mechanism reduces the number of useless actions by up to 43.08%. This\nresulted in the reduction of training time by 28\\% in the base case and 4.84\\%\nin the geomean compared to a naive RL-based approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u60e9\u7f5a\u65e0\u7528\u52a8\u4f5c\u4ee5\u63d0\u9ad8\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7f13\u5b58\u7ed3\u6784\u6f0f\u6d1e\u63a2\u7d22\u6548\u7387\u7684\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u65e0\u7528\u52a8\u4f5c\u3002", "motivation": "\u7f13\u5b58\u65f6\u5e8f\u653b\u51fb\u5229\u7528\u5fae\u4f53\u7cfb\u7ed3\u6784\u7279\u6027\u6cc4\u6f0f\u654f\u611f\u6570\u636e\uff0c\u5bf9\u73b0\u4ee3\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\u3002\u7136\u800c\uff0c\u5206\u6790\u7ed9\u5b9a\u7f13\u5b58\u7ed3\u6784\u5bf9\u7f13\u5b58\u65f6\u5e8f\u653b\u51fb\u7684\u8106\u5f31\u6027\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u667a\u80fd\u4f53\u53ef\u80fd\u4f1a\u6267\u884c\u4e00\u4e9b\u65e0\u52a9\u4e8e\u63a2\u7d22\u7684\u52a8\u4f5c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8bc6\u522b\u8fd9\u4e9b\u65e0\u7528\u52a8\u4f5c\u5e76\u5bf9\u5b83\u4eec\u8fdb\u884c\u60e9\u7f5a\uff0c\u4ece\u800c\u4f7f\u667a\u80fd\u4f53\u907f\u514d\u8fd9\u4e9b\u52a8\u4f5c\uff0c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "result": "\u572817\u79cd\u7f13\u5b58\u7ed3\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u673a\u5236\u5c06\u65e0\u7528\u52a8\u4f5c\u7684\u6570\u91cf\u51cf\u5c11\u4e86\u591a\u8fbe43.08%\uff0c\u57fa\u7840\u60c5\u51b5\u4e0b\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e8628%\uff0c\u51e0\u4f55\u5e73\u5747\u60c5\u51b5\u4e0b\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e864.84%\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u65e0\u7528\u52a8\u4f5c\u5e76\u63d0\u9ad8\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7f13\u5b58\u7ed3\u6784\u6f0f\u6d1e\u63a2\u7d22\u7684\u6548\u7387\u3002"}}
{"id": "2506.06868", "pdf": "https://arxiv.org/pdf/2506.06868", "abs": "https://arxiv.org/abs/2506.06868", "authors": ["Razieh Arshadizadeh", "Mahmoud Asgari", "Zeinab Khosravi", "Yiannis Papadopoulos", "Koorosh Aslansefat"], "title": "Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance", "categories": ["cs.AI"], "comment": null, "summary": "Machine Learning (ML) models are increasingly integrated into safety-critical\nsystems, such as autonomous vehicle platooning, to enable real-time\ndecision-making. However, their inherent imperfection introduces a new class of\nfailure: reasoning failures often triggered by distributional shifts between\noperational and training data. Traditional safety assessment methods, which\nrely on design artefacts or code, are ill-suited for ML components that learn\nbehaviour from data. SafeML was recently proposed to dynamically detect such\nshifts and assign confidence levels to the reasoning of ML-based components.\nBuilding on this, we introduce a probabilistic safety assurance framework that\nintegrates SafeML with Bayesian Networks (BNs) to model ML failures as part of\na broader causal safety analysis. This allows for dynamic safety evaluation and\nsystem adaptation under uncertainty. We demonstrate the approach on an\nsimulated automotive platooning system with traffic sign recognition. The\nfindings highlight the potential broader benefits of explicitly modelling ML\nfailures in safety assessment.", "AI": {"tldr": "The paper proposes a probabilistic safety assurance framework integrating SafeML with Bayesian Networks (BNs) to model ML failures in safety-critical systems, demonstrating its effectiveness in a simulated automotive platooning system.", "motivation": "Machine Learning models are being integrated into safety-critical systems, but their imperfections introduce reasoning failures often triggered by distributional shifts between operational and training data. Traditional safety assessment methods are not suitable for ML components that learn from data.", "method": "The authors introduce a probabilistic safety assurance framework that combines SafeML, which dynamically detects distributional shifts and assigns confidence levels to ML reasoning, with Bayesian Networks (BNs) to model ML failures within a broader causal safety analysis.", "result": "The approach was demonstrated on a simulated automotive platooning system with traffic sign recognition, showing the potential broader benefits of explicitly modelling ML failures in safety assessment.", "conclusion": "Explicitly modelling ML failures in safety assessment using the proposed framework has significant potential to enhance dynamic safety evaluation and system adaptation under uncertainty in safety-critical systems."}}
{"id": "2506.06454", "pdf": "https://arxiv.org/pdf/2506.06454", "abs": "https://arxiv.org/abs/2506.06454", "authors": ["Abrar Majeedi", "Viswanatha Reddy Gajjala", "Satya Sai Srinath Namburi GNVV", "Nada Magdi Elkordi", "Yin Li"], "title": "LETS Forecast: Learning Embedology for Time Series Forecasting", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted at International Conference on Machine Learning (ICML) 2025", "summary": "Real-world time series are often governed by complex nonlinear dynamics.\nUnderstanding these underlying dynamics is crucial for precise future\nprediction. While deep learning has achieved major success in time series\nforecasting, many existing approaches do not explicitly model the dynamics. To\nbridge this gap, we introduce DeepEDM, a framework that integrates nonlinear\ndynamical systems modeling with deep neural networks. Inspired by empirical\ndynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel\ndeep model that learns a latent space from time-delayed embeddings, and employs\nkernel regression to approximate the underlying dynamics, while leveraging\nefficient implementation of softmax attention and allowing for accurate\nprediction of future time steps. To evaluate our method, we conduct\ncomprehensive experiments on synthetic data of nonlinear dynamical systems as\nwell as real-world time series across domains. Our results show that DeepEDM is\nrobust to input noise, and outperforms state-of-the-art methods in forecasting\naccuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.", "AI": {"tldr": "DeepEDM is a novel framework that combines nonlinear dynamical systems modeling with deep neural networks for time series forecasting.", "motivation": "Real-world time series are often governed by complex nonlinear dynamics, which need to be understood for precise future prediction.", "method": "DeepEDM integrates nonlinear dynamical systems modeling with deep neural networks. It learns a latent space from time-delayed embeddings and employs kernel regression to approximate the underlying dynamics.", "result": "Experiments on synthetic data of nonlinear dynamical systems and real-world time series show that DeepEDM is robust to input noise and outperforms state-of-the-art methods in forecasting accuracy.", "conclusion": "DeepEDM provides an effective approach for time series forecasting by combining nonlinear dynamical systems modeling with deep neural networks."}}
{"id": "2506.07263", "pdf": "https://arxiv.org/pdf/2506.07263", "abs": "https://arxiv.org/abs/2506.07263", "authors": ["Yuhui Zhu", "Alessandro Biondi"], "title": "Exploiting Inaccurate Branch History in Side-Channel Attacks", "categories": ["cs.CR", "cs.AR"], "comment": "20 pages, 8 figures, to be published in proceedings of the 34th\n  USENIX Security Symposium (2025)", "summary": "Modern out-of-order CPUs heavily rely on speculative execution for\nperformance optimization, with branch prediction serving as a cornerstone to\nminimize stalls and maximize efficiency. Whenever shared branch prediction\nresources lack proper isolation and sanitization methods, they may originate\nsecurity vulnerabilities that expose sensitive data across different software\ncontexts.\n  This paper examines the fundamental components of modern Branch Prediction\nUnits (BPUs) and investigates how resource sharing and contention affect two\nwidely implemented but underdocumented features: Bias-Free Branch Prediction\nand Branch History Speculation. Our analysis demonstrates that these BPU\nfeatures, while designed to enhance speculative execution efficiency through\nmore accurate branch histories, can also introduce significant security risks.\nWe show that these features can inadvertently modify the Branch History Buffer\n(BHB) update behavior and create new primitives that trigger malicious\nmis-speculations.\n  This discovery exposes previously unknown cross-privilege attack surfaces for\nBranch History Injection (BHI). Based on these findings, we present three novel\nattack primitives: two Spectre attacks, namely Spectre-BSE and Spectre-BHS, and\na cross-privilege control flow side-channel attack called BiasScope. Our\nresearch identifies corresponding patterns of vulnerable control flows and\ndemonstrates exploitation on multiple processors. Finally, Chimera is\npresented: an attack demonstrator based on eBPF for a variant of Spectre-BHS\nthat is capable of leaking kernel memory contents at 24,628 bit/s.", "AI": {"tldr": "\u73b0\u4ee3\u4e71\u5e8fCPU\u4f9d\u8d56\u63a8\u6d4b\u6267\u884c\u8fdb\u884c\u6027\u80fd\u4f18\u5316\uff0c\u5206\u652f\u9884\u6d4b\u4f5c\u4e3a\u51cf\u5c11\u505c\u987f\u548c\u6700\u5927\u5316\u6548\u7387\u7684\u6838\u5fc3\u3002\u672c\u6587\u5206\u6790\u4e86\u73b0\u4ee3\u5206\u652f\u9884\u6d4b\u5355\u5143\uff08BPU\uff09\u7684\u57fa\u672c\u7ec4\u4ef6\uff0c\u4ee5\u53ca\u8d44\u6e90\u5171\u4eab\u548c\u7ade\u4e89\u5982\u4f55\u5f71\u54cd\u4e24\u4e2a\u5e7f\u6cdb\u5b9e\u73b0\u4f46\u8bb0\u5f55\u4e0d\u8db3\u7684\u529f\u80fd\uff1a\u65e0\u504f\u5206\u652f\u9884\u6d4b\u548c\u5206\u652f\u5386\u53f2\u63a8\u6d4b\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u4e9b\u529f\u80fd\u53ef\u80fd\u5f15\u5165\u5b89\u5168\u98ce\u9669\uff0c\u5c55\u793a\u4e86\u5b83\u4eec\u5982\u4f55\u4fee\u6539\u5206\u652f\u5386\u53f2\u7f13\u51b2\u533a\u66f4\u65b0\u884c\u4e3a\u5e76\u521b\u5efa\u6076\u610f\u8bef\u63a8\u6d4b\u7684\u65b0\u539f\u8bed\u3002\u57fa\u4e8e\u6b64\u53d1\u73b0\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u578b\u653b\u51fb\u539f\u8bed\uff0c\u5e76\u6f14\u793a\u4e86\u5728\u591a\u4e2a\u5904\u7406\u5668\u4e0a\u7684\u5229\u7528\u65b9\u6cd5\u3002\u6700\u540e\u4ecb\u7ecd\u4e86Chimera\uff0c\u4e00\u79cd\u57fa\u4e8eeBPF\u7684\u653b\u51fb\u6f14\u793a\u5668\uff0c\u80fd\u591f\u4ee524,628 bit/s\u7684\u901f\u5ea6\u6cc4\u9732\u5185\u6838\u5185\u5b58\u5185\u5bb9\u3002", "motivation": "\u5206\u652f\u9884\u6d4b\u662f\u73b0\u4ee3CPU\u6027\u80fd\u4f18\u5316\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u7136\u800c\u5f53\u5171\u4eab\u5206\u652f\u9884\u6d4b\u8d44\u6e90\u7f3a\u4e4f\u9002\u5f53\u7684\u9694\u79bb\u548c\u51c0\u5316\u65b9\u6cd5\u65f6\uff0c\u53ef\u80fd\u4f1a\u5f15\u53d1\u8de8\u8f6f\u4ef6\u4e0a\u4e0b\u6587\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5206\u652f\u9884\u6d4b\u5355\u5143\u7684\u529f\u80fd\u53ca\u5176\u6f5c\u5728\u5b89\u5168\u98ce\u9669\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86\u73b0\u4ee3BPU\u7684\u57fa\u672c\u7ec4\u4ef6\uff0c\u5e76\u7814\u7a76\u4e86\u8d44\u6e90\u5171\u4eab\u548c\u7ade\u4e89\u5bf9\u65e0\u504f\u5206\u652f\u9884\u6d4b\u548c\u5206\u652f\u5386\u53f2\u63a8\u6d4b\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u5b9e\u9a8c\u5c55\u793a\u4e86\u8fd9\u4e9b\u529f\u80fd\u5982\u4f55\u4fee\u6539\u5206\u652f\u5386\u53f2\u7f13\u51b2\u533a\u66f4\u65b0\u884c\u4e3a\uff0c\u5e76\u521b\u5efa\u65b0\u7684\u6076\u610f\u8bef\u63a8\u6d4b\u539f\u8bed\u3002\u6b64\u5916\uff0c\u8fd8\u8bc6\u522b\u4e86\u76f8\u5e94\u7684\u6613\u53d7\u653b\u51fb\u63a7\u5236\u6d41\u6a21\u5f0f\uff0c\u5e76\u5728\u591a\u4e2a\u5904\u7406\u5668\u4e0a\u8fdb\u884c\u4e86\u5229\u7528\u6f14\u793a\u3002", "result": "\u53d1\u73b0\u4e86\u672a\u77e5\u7684\u8de8\u6743\u9650\u653b\u51fb\u9762\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u578b\u653b\u51fb\u539f\u8bed\uff1aSpectre-BSE\u3001Spectre-BHS\u548cBiasScope\u3002\u5e76\u5728\u591a\u4e2a\u5904\u7406\u5668\u4e0a\u6210\u529f\u6f14\u793a\u4e86\u653b\u51fb\u53ef\u884c\u6027\u3002\u6700\u540e\uff0c\u57fa\u4e8eeBPF\u7684Chimera\u653b\u51fb\u6f14\u793a\u5668\u5b9e\u73b0\u4e86\u4ee524,628 bit/s\u7684\u901f\u5ea6\u6cc4\u9732\u5185\u6838\u5185\u5b58\u5185\u5bb9\u3002", "conclusion": "\u672c\u6587\u63ed\u793a\u4e86\u73b0\u4ee3BPU\u4e2d\u65e0\u504f\u5206\u652f\u9884\u6d4b\u548c\u5206\u652f\u5386\u53f2\u63a8\u6d4b\u529f\u80fd\u53ef\u80fd\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\uff0c\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u578b\u653b\u51fb\u539f\u8bed\uff0c\u5e76\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u6539\u8fdb\u5206\u652f\u9884\u6d4b\u8d44\u6e90\u9694\u79bb\u548c\u51c0\u5316\u65b9\u6cd5\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u9632\u6b62\u654f\u611f\u6570\u636e\u6cc4\u9732\u548c\u8de8\u6743\u9650\u653b\u51fb\u3002"}}
{"id": "2506.06881", "pdf": "https://arxiv.org/pdf/2506.06881", "abs": "https://arxiv.org/abs/2506.06881", "authors": ["Zixuan Li", "Wenxuan Liu", "Long Bai", "Chunmao Zhang", "Wei Li", "Fenghui Zhang", "Quanxin Jin", "Ruoyun He", "Zhuo Chen", "Zhilei Hu", "Fei Wang", "Bingbing Xu", "Xuhui Jiang", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "KnowCoder-V2: Deep Knowledge Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Deep knowledge analysis tasks always involve the systematic extraction and\nassociation of knowledge from large volumes of data, followed by logical\nreasoning to discover insights. However, to solve such complex tasks, existing\ndeep research frameworks face three major challenges: 1) They lack systematic\norganization and management of knowledge; 2) They operate purely online, making\nit inefficient for tasks that rely on shared and large-scale knowledge; 3) They\ncannot perform complex knowledge computation, limiting their abilities to\nproduce insightful analytical results. Motivated by these, in this paper, we\npropose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR})\nframework that empowers deep research with deep knowledge analysis capability.\nSpecifically, it introduces an independent knowledge organization phase to\npreprocess large-scale, domain-relevant data into systematic knowledge offline.\nBased on this knowledge, it extends deep research with an additional kind of\nreasoning steps that perform complex knowledge computation in an online manner.\nTo enhance the abilities of LLMs to solve knowledge analysis tasks in the above\nframework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledge\norganization and reasoning via unified code generation. For knowledge\norganization, it generates instantiation code for predefined classes,\ntransforming data into knowledge objects. For knowledge computation, it\ngenerates analysis code and executes on the above knowledge objects to obtain\ndeep analysis results. Experimental results on more than thirty datasets across\nsix knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover,\nwhen integrated into the KDR framework, \\KCII can generate high-quality reports\nwith insightful analytical results compared to the mainstream deep research\nframework.", "AI": {"tldr": "\u63d0\u51faKDR\u6846\u67b6\u548cKCII\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u5728\u77e5\u8bc6\u7ba1\u7406\u3001\u8ba1\u7b97\u53ca\u5206\u6790\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u79bb\u7ebf\u77e5\u8bc6\u7ec4\u7ec7\u4e0e\u5728\u7ebf\u63a8\u7406\u63d0\u5347\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\u5b58\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0d\u7cfb\u7edf\u3001\u5728\u7ebf\u64cd\u4f5c\u6548\u7387\u4f4e\u4ee5\u53ca\u65e0\u6cd5\u8fdb\u884c\u590d\u6742\u77e5\u8bc6\u8ba1\u7b97\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u77e5\u8bc6\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "method": "1. \u63d0\u51faKnowledgeable Deep Research (KDR)\u6846\u67b6\uff0c\u5305\u542b\u79bb\u7ebf\u77e5\u8bc6\u7ec4\u7ec7\u9636\u6bb5\u548c\u5728\u7ebf\u63a8\u7406\u6b65\u9aa4\u3002\n2. \u5f15\u5165KCII\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u4ee3\u7801\u751f\u6210\u8fde\u63a5\u77e5\u8bc6\u7ec4\u7ec7\u4e0e\u63a8\u7406\uff1a\n   - \u77e5\u8bc6\u7ec4\u7ec7\u9636\u6bb5\u751f\u6210\u5b9e\u4f8b\u5316\u4ee3\u7801\u5c06\u6570\u636e\u8f6c\u5316\u4e3a\u77e5\u8bc6\u5bf9\u8c61\u3002\n   - \u77e5\u8bc6\u8ba1\u7b97\u9636\u6bb5\u751f\u6210\u5206\u6790\u4ee3\u7801\u5bf9\u77e5\u8bc6\u5bf9\u8c61\u8fdb\u884c\u6df1\u5ea6\u5206\u6790\u3002\n3. \u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1KCII\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u5176\u5728KDR\u6846\u67b6\u4e2d\u751f\u6210\u9ad8\u8d28\u91cf\u62a5\u544a\u7684\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eKCII\u5728\u8d85\u8fc7\u4e09\u5341\u4e2a\u6570\u636e\u96c6\u7684\u516d\u7c7b\u77e5\u8bc6\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5e76\u4e14\u5728KDR\u6846\u67b6\u4e0b\u80fd\u751f\u6210\u5177\u6709\u6df1\u523b\u6d1e\u89c1\u7684\u9ad8\u8d28\u91cf\u62a5\u544a\u3002", "conclusion": "KDR\u6846\u67b6\u548cKCII\u6a21\u578b\u4e3a\u6df1\u5ea6\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u77e5\u8bc6\u5206\u6790\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u77e5\u8bc6\u7ba1\u7406\u4e0e\u8ba1\u7b97\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u590d\u6742\u77e5\u8bc6\u5206\u6790\u4efb\u52a1\u7684\u5904\u7406\u6548\u679c\u3002"}}
{"id": "2506.06455", "pdf": "https://arxiv.org/pdf/2506.06455", "abs": "https://arxiv.org/abs/2506.06455", "authors": ["Antonio Jes\u00fas Banegas-Luna", "Horacio P\u00e9rez-S\u00e1nchez", "Carlos Mart\u00ednez-Cort\u00e9s"], "title": "WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "27 pages, 11 figures, 2 tables, 13 equations", "summary": "While predictive accuracy is often prioritized in machine learning (ML)\nmodels, interpretability remains essential in scientific and high-stakes\ndomains. However, diverse interpretability algorithms frequently yield\nconflicting explanations, highlighting the need for consensus to harmonize\nresults. In this study, six ML models were trained on six synthetic datasets\nwith known ground truths, utilizing various model-agnostic interpretability\ntechniques. Consensus explanations were generated using established methods and\na novel approach: WISCA (Weighted Scaled Consensus Attributions), which\nintegrates class probability and normalized attributions. WISCA consistently\naligned with the most reliable individual method, underscoring the value of\nrobust consensus strategies in improving explanation reliability.", "AI": {"tldr": "\u5728\u673a\u5668\u5b66\u4e60\u4e2d\uff0c\u867d\u7136\u9884\u6d4b\u51c6\u786e\u6027\u901a\u5e38\u662f\u4f18\u5148\u8003\u8651\u7684\uff0c\u4f46\u53ef\u89e3\u91ca\u6027\u5728\u79d1\u5b66\u548c\u9ad8\u98ce\u9669\u9886\u57df\u4ecd\u7136\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u901a\u8fc7\u8bad\u7ec3\u516d\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5e76\u4f7f\u7528\u591a\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5171\u8bc6\u65b9\u6cd5WISCA\uff08\u52a0\u6743\u7f29\u653e\u5171\u8bc6\u5f52\u56e0\uff09\uff0c\u4ee5\u63d0\u9ad8\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002WISCA\u4e0e\u6700\u53ef\u9760\u7684\u5355\u72ec\u65b9\u6cd5\u4e00\u81f4\uff0c\u5f3a\u8c03\u4e86\u7a33\u5065\u5171\u8bc6\u7b56\u7565\u7684\u4ef7\u503c\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u9884\u6d4b\u51c6\u786e\u6027\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u79d1\u5b66\u548c\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u53ef\u89e3\u91ca\u6027\u540c\u6837\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e0d\u540c\u7684\u53ef\u89e3\u91ca\u6027\u7b97\u6cd5\u7ecf\u5e38\u4ea7\u751f\u51b2\u7a81\u7684\u89e3\u91ca\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u5171\u8bc6\u65b9\u6cd5\u6765\u534f\u8c03\u8fd9\u4e9b\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u8005\u5728\u516d\u4e2a\u5177\u6709\u5df2\u77e5\u771f\u5b9e\u6807\u7b7e\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u516d\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5e76\u5e94\u7528\u4e86\u5404\u79cd\u6a21\u578b\u65e0\u5173\u7684\u89e3\u91ca\u6280\u672f\u3002\u7136\u540e\uff0c\u4ed6\u4eec\u4f7f\u7528\u73b0\u6709\u7684\u65b9\u6cd5\u548c\u4e00\u79cd\u65b0\u65b9\u6cd5\u2014\u2014WISCA\uff08\u52a0\u6743\u7f29\u653e\u5171\u8bc6\u5f52\u56e0\uff09\u751f\u6210\u5171\u8bc6\u89e3\u91ca\u3002WISCA\u7ed3\u5408\u4e86\u7c7b\u522b\u6982\u7387\u548c\u6807\u51c6\u5316\u5f52\u56e0\u3002", "result": "WISCA\u80fd\u591f\u4e0e\u6700\u53ef\u9760\u7684\u5355\u72ec\u89e3\u91ca\u65b9\u6cd5\u4fdd\u6301\u4e00\u81f4\uff0c\u8868\u660e\u5176\u5728\u63d0\u9ad8\u89e3\u91ca\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u5728\u6539\u8fdb\u89e3\u91ca\u53ef\u9760\u6027\u65b9\u9762\uff0c\u91c7\u7528\u5982WISCA\u8fd9\u6837\u7684\u7a33\u5065\u5171\u8bc6\u7b56\u7565\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.07313", "pdf": "https://arxiv.org/pdf/2506.07313", "abs": "https://arxiv.org/abs/2506.07313", "authors": ["Rebecca Saul", "Hao Wang", "Koushik Sen", "David Wagner"], "title": "SCGAgent: Recreating the Benefits of Reasoning Models for Secure Code Generation with Agentic Workflows", "categories": ["cs.CR"], "comment": null, "summary": "Large language models (LLMs) have seen widespread success in code generation\ntasks for different scenarios, both everyday and professional. However current\nLLMs, despite producing functional code, do not prioritize security and may\ngenerate code with exploitable vulnerabilities. In this work, we propose\ntechniques for generating code that is more likely to be secure and introduce\nSCGAgent, a proactive secure coding agent that implements our techniques. We\nuse security coding guidelines that articulate safe programming practices,\ncombined with LLM-generated unit tests to preserve functional correctness. In\nour evaluation, we find that SCGAgent is able to preserve nearly 98% of the\nfunctionality of the base Sonnet-3.7 LLM while achieving an approximately 25%\nimprovement in security. Moreover, SCGAgent is able to match or best the\nperformance of sophisticated reasoning LLMs using a non-reasoning model and an\nagentic workflow.", "AI": {"tldr": "SCGAgent is a secure coding agent that enhances security in code generated by large language models while preserving functionality.", "motivation": "Current large language models (LLMs) are successful in generating functional code but often overlook security, potentially producing exploitable vulnerabilities.", "method": "The researchers use security coding guidelines and LLM-generated unit tests to ensure safe programming practices. They introduce SCGAgent, which employs these techniques for secure code generation.", "result": "SCGAgent preserves nearly 98% of the base LLM's functionality while improving security by about 25%. It also matches or outperforms reasoning-based LLMs using a non-reasoning model and an agentic workflow.", "conclusion": "SCGAgent successfully balances security enhancements with maintaining code functionality, offering a promising solution for secure code generation."}}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "AI": {"tldr": "The paper proposes a meta-learning approach with soft prompts distilled from task-relevant image features to improve in-context learning (ICL) performance of Large Multimodal Models (LMMs), especially for smaller models. This approach includes an attention-mapper module integrated into the LLaVA v1.5 architecture, enhancing few-shot capabilities and outperforming ICL methods in evaluations.", "motivation": "The motivation arises from the inconsistent performance of in-context learning (ICL) in LMMs, particularly in smaller models, which doesn't always improve with more examples. The authors hypothesize that this inconsistency stems from LMMs being overwhelmed by unnecessary information in image embeddings for the downstream task.", "method": "A meta-learning approach is proposed where soft prompts are distilled from task-relevant image features. These soft prompts can be adapted at test time using a few examples. An attention-mapper module is introduced to facilitate this distillation process, which is integrated with the LLaVA v1.5 architecture and jointly learned with the soft prompts.", "result": "Evaluation on the VL-ICL Bench demonstrates that the proposed method consistently surpasses ICL and related prompt-tuning approaches, even under conditions of image perturbations. It enhances task induction and reasoning across visual question answering tasks.", "conclusion": "The meta-learning approach with soft prompts and the attention-mapper module successfully improves the few-shot capabilities of LMMs, providing a better alternative to traditional ICL methods, especially in low-data regimes."}}
{"id": "2506.06459", "pdf": "https://arxiv.org/pdf/2506.06459", "abs": "https://arxiv.org/abs/2506.06459", "authors": ["Ruitao Chen", "Mozhang Guo", "Jinge Li"], "title": "Towards Infant Sleep-Optimized Driving: Synergizing Wearable and Vehicle Sensing in Intelligent Cruise Control", "categories": ["cs.LG", "cs.ET", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Automated driving (AD) has substantially improved vehicle safety and driving\ncomfort, but their impact on passenger well-being, particularly infant sleep,\nis not sufficiently studied. Sudden acceleration, abrupt braking, and sharp\nmaneuvers can disrupt infant sleep, compromising both passenger comfort and\nparental convenience. To solve this problem, this paper explores the\nintegration of reinforcement learning (RL) within AD to personalize driving\nbehavior and optimally balance occupant comfort and travel efficiency. In\nparticular, we propose an intelligent cruise control framework that adapts to\nvarying driving conditions to enhance infant sleep quality by effectively\nsynergizing wearable sensing and vehicle data. Long short-term memory (LSTM)\nand transformer-based neural networks are integrated with RL to model the\nrelationship between driving behavior and infant sleep quality under diverse\ntraffic and road conditions. Based on the sleep quality indicators from the\nwearable sensors, driving action data from vehicle controllers, and map data\nfrom map applications, the model dynamically computes the optimal driving\naggressiveness level, which is subsequently translated into specific AD control\nstrategies, e.g., the magnitude and frequency of acceleration, lane change, and\novertaking. Simulation results demonstrate that the proposed solution\nsignificantly improves infant sleep quality compared to baseline methods, while\npreserving desirable travel efficiency.", "AI": {"tldr": "The paper explores using reinforcement learning in automated driving to enhance infant sleep quality by balancing occupant comfort and travel efficiency.", "motivation": "Automated driving has improved vehicle safety and comfort, but its impact on passenger well-being, especially infant sleep, is not sufficiently studied.", "method": "An intelligent cruise control framework integrating reinforcement learning (RL), LSTM, and transformer-based neural networks synergizes wearable sensing and vehicle data to optimize driving behavior for enhancing infant sleep quality under diverse traffic and road conditions.", "result": "Simulation results show that the proposed solution significantly improves infant sleep quality compared to baseline methods while maintaining travel efficiency.", "conclusion": "The integration of RL within automated driving can personalize driving behavior and optimally balance occupant comfort and travel efficiency."}}
{"id": "2506.07372", "pdf": "https://arxiv.org/pdf/2506.07372", "abs": "https://arxiv.org/abs/2506.07372", "authors": ["Thesath Wijayasiri", "Kar Wai Fok", "Vrizlynn L. L. Thing"], "title": "Enhanced Consistency Bi-directional GAN(CBiGAN) for Malware Anomaly Detection", "categories": ["cs.CR"], "comment": null, "summary": "Static analysis, a cornerstone technique in cybersecurity, offers a\nnoninvasive method for detecting malware by analyzing dormant software without\nexecuting potentially harmful code. However, traditional static analysis often\nrelies on biased or outdated datasets, leading to gaps in detection\ncapabilities against emerging malware threats. To address this, our study\nfocuses on the binary content of files as key features for malware detection.\nThese binary contents are transformed and represented as images, which then\nserve as inputs to deep learning models. This method takes into account the\nvisual patterns within the binary data, allowing the model to analyze potential\nmalware effectively. This paper introduces the application of the CBiGAN in the\ndomain of malware anomaly detection. Our approach leverages the CBiGAN for its\nsuperior latent space mapping capabilities, critical for modeling complex\nmalware patterns by utilizing a reconstruction error-based anomaly detection\nmethod. We utilized several datasets including both portable executable (PE)\nfiles as well as Object Linking and Embedding (OLE) files. We then evaluated\nour model against a diverse set of both PE and OLE files, including\nself-collected malicious executables from 214 malware families. Our findings\ndemonstrate the robustness of this innovative approach, with the CBiGAN\nachieving high Area Under the Curve (AUC) results with good generalizability,\nthereby confirming its capability to distinguish between benign and diverse\nmalicious files with reasonably high accuracy.", "AI": {"tldr": "Static analysis in cybersecurity is crucial for malware detection without execution. However, traditional methods suffer from biased datasets. This paper proposes using binary file content transformed into images as inputs to deep learning models for enhanced malware detection. Specifically, it introduces the application of CBiGAN with its superior latent space mapping capabilities for anomaly detection based on reconstruction error. Evaluated on diverse PE and OLE files, including self-collected malicious executables from 214 malware families, the CBiGAN shows robustness and good generalizability with high AUC results.", "motivation": "Traditional static analysis techniques rely on biased or outdated datasets, which leads to gaps in detecting emerging malware threats. The study aims to improve detection by focusing on binary content as key features.", "method": "The binary content of files is transformed into images and used as input to deep learning models. The CBiGAN is leveraged for its latent space mapping capabilities, allowing for effective modeling of complex malware patterns through a reconstruction error-based anomaly detection method.", "result": "The CBiGAN achieved high AUC results and demonstrated good generalizability in distinguishing between benign and malicious files across diverse datasets, including PE and OLE files.", "conclusion": "The innovative use of CBiGAN for malware anomaly detection proves robust and effective, offering a promising advancement in static malware analysis."}}
{"id": "2506.06910", "pdf": "https://arxiv.org/pdf/2506.06910", "abs": "https://arxiv.org/abs/2506.06910", "authors": ["Mahnaz Koupaee", "Xueying Bai", "Mudan Chen", "Greg Durrett", "Nathanael Chambers", "Niranjan Balasubramanian"], "title": "Causal Graph based Event Reasoning using Semantic Relation Experts", "categories": ["cs.AI"], "comment": null, "summary": "Understanding how events in a scenario causally connect with each other is\nimportant for effectively modeling and reasoning about events. But event\nreasoning remains a difficult challenge, and despite recent advances, Large\nLanguage Models (LLMs) still struggle to accurately identify causal connections\nbetween events. This struggle leads to poor performance on deeper reasoning\ntasks like event forecasting and timeline understanding. To address this\nchallenge, we investigate the generation of causal event graphs (e.g., A\nenables B) as a parallel mechanism to help LLMs explicitly represent causality\nduring inference. This paper evaluates both how to generate correct graphs as\nwell as how graphs can assist reasoning. We propose a collaborative approach to\ncausal graph generation where we use LLMs to simulate experts that focus on\nspecific semantic relations. The experts engage in multiple rounds of\ndiscussions which are then consolidated by a final expert. Then, to demonstrate\nthe utility of causal graphs, we use them on multiple downstream applications,\nand also introduce a new explainable event prediction task that requires a\ncausal chain of events in the explanation. These explanations are more\ninformative and coherent than baseline generations. Finally, our overall\napproach not finetuned on any downstream task, achieves competitive results\nwith state-of-the-art models on both forecasting and next event prediction\ntasks.", "AI": {"tldr": "This paper explores the use of causal event graphs to improve LLMs' ability to reason about events, proposing a collaborative approach for generating these graphs and demonstrating their utility in downstream applications.", "motivation": "Event reasoning is crucial but remains challenging for LLMs, particularly in identifying causal connections between events, which affects performance on deeper reasoning tasks.", "method": "The authors propose a collaborative approach to generate causal graphs using LLMs as 'experts' focusing on specific semantic relations, engaging in multiple rounds of discussions consolidated by a final expert. These graphs are then used in downstream applications including an explainable event prediction task.", "result": "Causal graphs generated through this method proved more informative and coherent than baseline generations. The approach achieved competitive results with state-of-the-art models on forecasting and next event prediction tasks without fine-tuning.", "conclusion": "The generation of causal event graphs via a collaborative approach can significantly enhance LLMs' reasoning capabilities and provide more informative explanations."}}
{"id": "2506.06482", "pdf": "https://arxiv.org/pdf/2506.06482", "abs": "https://arxiv.org/abs/2506.06482", "authors": ["Zhiyuan Zhao", "Juntong Ni", "Shangqing Xu", "Haoxin Liu", "Wei Jin", "B. Aditya Prakash"], "title": "TimeRecipe: A Time-Series Forecasting Recipe via Benchmarking Module Level Effectiveness", "categories": ["cs.LG"], "comment": "46 pages, 1 figure, 28 tables", "summary": "Time-series forecasting is an essential task with wide real-world\napplications across domains. While recent advances in deep learning have\nenabled time-series forecasting models with accurate predictions, there remains\nconsiderable debate over which architectures and design components, such as\nseries decomposition or normalization, are most effective under varying\nconditions. Existing benchmarks primarily evaluate models at a high level,\noffering limited insight into why certain designs work better. To mitigate this\ngap, we propose TimeRecipe, a unified benchmarking framework that\nsystematically evaluates time-series forecasting methods at the module level.\nTimeRecipe conducts over 10,000 experiments to assess the effectiveness of\nindividual components across a diverse range of datasets, forecasting horizons,\nand task settings. Our results reveal that exhaustive exploration of the design\nspace can yield models that outperform existing state-of-the-art methods and\nuncover meaningful intuitions linking specific design choices to forecasting\nscenarios. Furthermore, we release a practical toolkit within TimeRecipe that\nrecommends suitable model architectures based on these empirical insights. The\nbenchmark is available at: https://github.com/AdityaLab/TimeRecipe.", "AI": {"tldr": "Time-series forecasting is crucial across domains. Despite deep learning advancements, debate remains on the most effective architectures and components. Existing benchmarks lack detailed insights. This paper introduces TimeRecipe, a framework conducting over 10,000 experiments to evaluate time-series forecasting methods at the module level, providing meaningful intuitions and outperforming state-of-the-art methods.", "motivation": "To address the lack of detailed insights in existing benchmarks regarding which architectures and design components are most effective for time-series forecasting under varying conditions.", "method": "Propose TimeRecipe, a unified benchmarking framework that systematically evaluates time-series forecasting methods at the module level by conducting over 10,000 experiments across diverse datasets, forecasting horizons, and task settings.", "result": "Exhaustive exploration of the design space can yield models that outperform existing state-of-the-art methods and uncover meaningful intuitions linking specific design choices to forecasting scenarios.", "conclusion": "TimeRecipe provides valuable insights into effective design choices for time-series forecasting and offers a practical toolkit to recommend suitable model architectures based on empirical findings."}}
{"id": "2506.07392", "pdf": "https://arxiv.org/pdf/2506.07392", "abs": "https://arxiv.org/abs/2506.07392", "authors": ["Yuyang Zhou", "Guang Cheng", "Kang Du", "Zihan Chen", "Tian Qin", "Yuyu Zhao"], "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks", "categories": ["cs.CR", "cs.AI", "cs.LG", "68", "F.2.2"], "comment": "13pages; In submission", "summary": "The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide\nrange of mission-critical applications, but also exposes UAV networks to severe\nDenial-of-Service (DoS) threats due to their open wireless environment, dynamic\ntopology, and resource constraints. Traditional static or centralized defense\nmechanisms are often inadequate for such dynamic and distributed scenarios. To\naddress these challenges, we propose a novel federated multi-agent deep\nreinforcement learning (FMADRL)-driven moving target defense (MTD) framework\nfor proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,\nwe design three lightweight and coordinated MTD mechanisms, including leader\nswitching, route mutation, and frequency hopping, that leverage the inherent\nflexibility of UAV swarms to disrupt attacker efforts and enhance network\nresilience. The defense problem is formulated as a multi-agent partially\nobservable Markov decision process (POMDP), capturing the distributed,\nresource-constrained, and uncertain nature of UAV swarms under attack. Each UAV\nis equipped with a local policy agent that autonomously selects MTD actions\nbased on partial observations and local experiences. By employing a policy\ngradient-based FMADRL algorithm, UAVs collaboratively optimize their defense\npolicies via reward-weighted aggregation, enabling distributed learning without\nsharing raw data and thus reducing communication overhead. Extensive\nsimulations demonstrate that our approach significantly outperforms\nstate-of-the-art baselines, achieving up to a 34.6% improvement in attack\nmitigation rate, a reduction in average recovery time of up to 94.6%, and\ndecreases in energy consumption and defense cost by as much as 29.3% and 98.3%,\nrespectively, while maintaining robust mission continuity under various DoS\nattack strategies.", "AI": {"tldr": "The paper proposes a FMADRL-driven MTD framework to mitigate DoS threats in UAV swarm networks using three mechanisms: leader switching, route mutation, and frequency hopping. It formulates the defense problem as a POMDP and uses a policy gradient-based algorithm for distributed learning.", "motivation": "UAV networks are vulnerable to DoS threats due to their open wireless environment, dynamic topology, and resource constraints. Traditional defense mechanisms are inadequate for such scenarios.", "method": "A novel federated multi-agent deep reinforcement learning (FMADRL)-driven moving target defense (MTD) framework is proposed. The defense problem is formulated as a multi-agent partially observable Markov decision process (POMDP). Each UAV has a local policy agent that selects MTD actions based on partial observations and local experiences. A policy gradient-based FMADRL algorithm is used for collaborative optimization of defense policies.", "result": "Simulations show significant improvements over state-of-the-art baselines: 34.6% improvement in attack mitigation rate, 94.6% reduction in average recovery time, and decreases in energy consumption and defense cost by 29.3% and 98.3%, respectively.", "conclusion": "The FMADRL-driven MTD framework effectively enhances network resilience against DoS attacks in UAV swarm networks while maintaining robust mission continuity."}}
{"id": "2506.06923", "pdf": "https://arxiv.org/pdf/2506.06923", "abs": "https://arxiv.org/abs/2506.06923", "authors": ["Xutong Zhao", "Tengyu Xu", "Xuewei Wang", "Zhengxing Chen", "Di Jin", "Liang Tan", "Yen-Ting", "Zishun Yu", "Zhuokai Zhao", "Yun He", "Sinong Wang", "Han Fang", "Sarath Chandar", "Chen Zhu"], "title": "Boosting LLM Reasoning via Spontaneous Self-Correction", "categories": ["cs.AI"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable success on a\nbroad range of tasks, math reasoning remains a challenging one. One of the\napproaches for improving math reasoning is self-correction, which designs\nself-improving loops to let the model correct its own mistakes. However,\nexisting self-correction approaches treat corrections as standalone\npost-generation refinements, relying on extra prompt and system designs to\nelicit self-corrections, instead of performing real-time, spontaneous\nself-corrections in a single pass. To address this, we propose SPOC, a\nspontaneous self-correction approach that enables LLMs to generate interleaved\nsolutions and verifications in a single inference pass, with generation\ndynamically terminated based on verification outcomes, thereby effectively\nscaling inference time compute. SPOC considers a multi-agent perspective by\nassigning dual roles -- solution proposer and verifier -- to the same model. We\nadopt a simple yet effective approach to generate synthetic data for\nfine-tuning, enabling the model to develop capabilities for self-verification\nand multi-agent collaboration. We further improve its solution proposal and\nverification accuracy through online reinforcement learning. Experiments on\nmathematical reasoning benchmarks show that SPOC significantly improves\nperformance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct\nmodels, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,\nand 3.3% and 6.7% on AIME24, respectively.", "AI": {"tldr": "SPOC is a spontaneous self-correction approach for LLMs that enables interleaved solutions and verifications in a single inference pass, improving math reasoning performance.", "motivation": "Math reasoning remains challenging for large language models (LLMs) despite their success in other tasks. Existing self-correction methods treat corrections as standalone refinements rather than real-time, spontaneous corrections.", "method": "Propose SPOC, which assigns dual roles of solution proposer and verifier to the same model. It generates interleaved solutions and verifications in one pass, dynamically terminating based on verification outcomes. Uses synthetic data for fine-tuning and improves through online reinforcement learning.", "result": "Experiments show significant performance improvements on mathematical reasoning benchmarks. For instance, accuracy gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23, and 3.3% and 6.7% on AIME24 for Llama-3.1-8B and 70B Instruct models respectively.", "conclusion": "SPOC effectively enhances the math reasoning capabilities of LLMs by enabling real-time self-corrections."}}
{"id": "2506.06486", "pdf": "https://arxiv.org/pdf/2506.06486", "abs": "https://arxiv.org/abs/2506.06486", "authors": ["Umit Yigit Basaran", "Sk Miraj Ahmed", "Amit Roy-Chowdhury", "Basak Guler"], "title": "A Certified Unlearning Approach without Access to Source Data", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "With the growing adoption of data privacy regulations, the ability to erase\nprivate or copyrighted information from trained models has become a crucial\nrequirement. Traditional unlearning methods often assume access to the complete\ntraining dataset, which is unrealistic in scenarios where the source data is no\nlonger available. To address this challenge, we propose a certified unlearning\nframework that enables effective data removal \\final{without access to the\noriginal training data samples}. Our approach utilizes a surrogate dataset that\napproximates the statistical properties of the source data, allowing for\ncontrolled noise scaling based on the statistical distance between the two.\n\\updated{While our theoretical guarantees assume knowledge of the exact\nstatistical distance, practical implementations typically approximate this\ndistance, resulting in potentially weaker but still meaningful privacy\nguarantees.} This ensures strong guarantees on the model's behavior\npost-unlearning while maintaining its overall utility. We establish theoretical\nbounds, introduce practical noise calibration techniques, and validate our\nmethod through extensive experiments on both synthetic and real-world datasets.\nThe results demonstrate the effectiveness and reliability of our approach in\nprivacy-sensitive settings.", "AI": {"tldr": "This paper proposes a certified unlearning framework that enables effective data removal from trained models without needing the original training data samples, by using a surrogate dataset and controlled noise scaling based on statistical distance.", "motivation": "The motivation is to address the challenge of erasing private or copyrighted information from trained models when the source data is no longer available, which is crucial with the growing adoption of data privacy regulations.", "method": "The method uses a surrogate dataset that approximates the statistical properties of the source data. It applies controlled noise scaling based on the statistical distance between the two datasets. Practical implementations typically approximate this distance, resulting in meaningful privacy guarantees even if weaker than theoretical ones.", "result": "Theoretical bounds are established, practical noise calibration techniques are introduced, and extensive experiments on both synthetic and real-world datasets validate the method's effectiveness and reliability in privacy-sensitive settings.", "conclusion": "This approach ensures strong guarantees on the model's behavior post-unlearning while maintaining its overall utility."}}
{"id": "2506.07402", "pdf": "https://arxiv.org/pdf/2506.07402", "abs": "https://arxiv.org/abs/2506.07402", "authors": ["Yukai Zhou", "Sibei Yang", "Wenjie Wang"], "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about their security. While jailbreak attacks\nhighlight failures under overtly harmful queries, they overlook a critical\nrisk: incorrectly answering harmless-looking inputs can be dangerous and cause\nreal-world harm (Implicit Harm). We systematically reformulate the LLM risk\nlandscape through a structured quadrant perspective based on output factuality\nand input harmlessness, uncovering an overlooked high-risk region. To\ninvestigate this gap, we propose JailFlipBench, a benchmark aims to capture\nimplicit harm, spanning single-modal, multimodal, and factual extension\nscenarios with diverse evaluation metrics. We further develop initial JailFlip\nattack methodologies and conduct comprehensive evaluations across multiple\nopen-source and black-box LLMs, show that implicit harm present immediate and\nurgent real-world risks, calling for broader LLM safety assessments and\nalignment beyond conventional jailbreak paradigms.", "AI": {"tldr": "Large language models (LLMs) have a security risk of incorrectly answering seemingly harmless inputs, which can cause real-world harm. This paper proposes JailFlipBench to capture this implicit harm and conducts evaluations across multiple LLMs.", "motivation": "The motivation of the paper is to address the overlooked risk in large language models where they may produce dangerous outputs even when responding to harmless-looking inputs, thus causing real-world harm.", "method": "The authors propose JailFlipBench, a benchmark for capturing implicit harm in LLMs. They also develop initial JailFlip attack methodologies and conduct comprehensive evaluations across various types of LLMs.", "result": "The results indicate that implicit harm presents immediate and urgent real-world risks in LLMs.", "conclusion": "The conclusion is that there is a need for broader safety assessments and alignment in LLMs beyond conventional jailbreak paradigms."}}
{"id": "2506.06935", "pdf": "https://arxiv.org/pdf/2506.06935", "abs": "https://arxiv.org/abs/2506.06935", "authors": ["Darui Lu", "Jordan M. Malof", "Willie J. Padilla"], "title": "An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": "22 pages, 6 figures", "summary": "Recent significant advances in integrating multiple Large Language Model\n(LLM) systems have enabled Agentic Frameworks capable of performing complex\ntasks autonomously, including novel scientific research. We develop and\ndemonstrate such a framework specifically for the inverse design of photonic\nmetamaterials. When queried with a desired optical spectrum, the Agent\nautonomously proposes and develops a forward deep learning model, accesses\nexternal tools via APIs for tasks like simulation and optimization, utilizes\nmemory, and generates a final design via a deep inverse method. The framework's\neffectiveness is demonstrated in its ability to automate, reason, plan, and\nadapt. Notably, the Agentic Framework possesses internal reflection and\ndecision flexibility, permitting highly varied and potentially novel outputs.", "AI": {"tldr": "A Agentic Framework using multiple LLMs is developed for inverse design of photonic metamaterials, showing effectiveness in automation, reasoning, planning, and adaptation.", "motivation": "To leverage the capabilities of integrated LLM systems to create an autonomous framework for performing complex tasks, specifically the inverse design of photonic metamaterials.", "method": "Developed an Agentic Framework that, when given a desired optical spectrum, autonomously proposes and develops a forward deep learning model, accesses external tools via APIs for simulation and optimization, uses memory, and generates a final design through a deep inverse method.", "result": "The framework demonstrated its ability to automate, reason, plan, and adapt, with internal reflection and decision flexibility leading to varied and potentially novel outputs.", "conclusion": "The Agentic Framework shows promise in autonomously handling complex tasks like the inverse design of photonic metamaterials."}}
{"id": "2506.06488", "pdf": "https://arxiv.org/pdf/2506.06488", "abs": "https://arxiv.org/abs/2506.06488", "authors": ["Pratiksha Thaker", "Neil Kale", "Zhiwei Steven Wu", "Virginia Smith"], "title": "Membership Inference Attacks for Unseen Classes", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": "Preprint", "summary": "Shadow model attacks are the state-of-the-art approach for membership\ninference attacks on machine learning models. However, these attacks typically\nassume an adversary has access to a background (nonmember) data distribution\nthat matches the distribution the target model was trained on. We initiate a\nstudy of membership inference attacks where the adversary or auditor cannot\naccess an entire subclass from the distribution -- a more extreme but realistic\nversion of distribution shift than has been studied previously. In this\nsetting, we first show that the performance of shadow model attacks degrades\ncatastrophically, and then demonstrate the promise of another approach,\nquantile regression, that does not have the same limitations. We show that\nquantile regression attacks consistently outperform shadow model attacks in the\nclass dropout setting -- for example, quantile regression attacks achieve up to\n11$\\times$ the TPR of shadow models on the unseen class on CIFAR-100, and\nachieve nontrivial TPR on ImageNet even with 90% of training classes removed.\nWe also provide a theoretical model that illustrates the potential and\nlimitations of this approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u6210\u5458\u5206\u5e03\u4e0d\u53ef\u5b8c\u5168\u83b7\u53d6\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u91cf\u5316\u56de\u5f52\u65b9\u6cd5\u8fdb\u884c\u6210\u5458\u63a8\u65ad\u653b\u51fb\u7684\u6f5c\u529b\u548c\u4f18\u52bf\u3002\u76f8\u6bd4\u4f20\u7edf\u7684\u5f71\u5b50\u6a21\u578b\u653b\u51fb\uff0c\u91cf\u5316\u56de\u5f52\u653b\u51fb\u5728\u6781\u7aef\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5982\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\u5bf9\u672a\u89c1\u7c7b\u522b\u8fbe\u5230\u5f71\u5b50\u6a21\u578b11\u500d\u7684\u771f\u6b63\u4f8b\u7387\uff08TPR\uff09\uff0c\u5e76\u5728ImageNet\u6570\u636e\u96c6\u4e0a\u5373\u4f7f\u79fb\u966490%\u7684\u8bad\u7ec3\u7c7b\u522b\u4ecd\u80fd\u83b7\u5f97\u975e\u5e73\u51e1\u7684TPR\u3002", "motivation": "\u5f53\u524d\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u653b\u51fb\u8005\u80fd\u591f\u8bbf\u95ee\u4e0e\u76ee\u6807\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u5339\u914d\u7684\u80cc\u666f\u6570\u636e\u5206\u5e03\u3002\u7136\u800c\uff0c\u5728\u66f4\u6781\u7aef\u4e14\u73b0\u5b9e\u7684\u573a\u666f\u4e2d\uff0c\u653b\u51fb\u8005\u6216\u5ba1\u8ba1\u5458\u53ef\u80fd\u65e0\u6cd5\u8bbf\u95ee\u6574\u4e2a\u5b50\u7c7b\u522b\u7684\u5206\u5e03\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u79cd\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "\u7814\u7a76\u9996\u5148\u5c55\u793a\u4e86\u5f71\u5b50\u6a21\u578b\u653b\u51fb\u5728\u6781\u7aef\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u7136\u540e\u5f15\u5165\u4e86\u91cf\u5316\u56de\u5f52\u65b9\u6cd5\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002\u901a\u8fc7\u7406\u8bba\u6a21\u578b\u5206\u6790\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u91cf\u5316\u56de\u5f52\u65b9\u6cd5\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u5177\u6709\u66f4\u597d\u7684\u8868\u73b0\u3002", "result": "\u91cf\u5316\u56de\u5f52\u653b\u51fb\u5728\u7c7b\u5220\u9664\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u5f71\u5b50\u6a21\u578b\u653b\u51fb\u3002\u4f8b\u5982\uff0c\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u672a\u89c1\u7c7b\u522b\u8fbe\u5230\u4e86\u5f71\u5b50\u6a21\u578b11\u500d\u7684\u771f\u6b63\u4f8b\u7387\uff1b\u5728ImageNet\u6570\u636e\u96c6\u4e0a\uff0c\u5373\u4f7f\u79fb\u9664\u4e8690%\u7684\u8bad\u7ec3\u7c7b\u522b\uff0c\u4ecd\u7136\u83b7\u5f97\u4e86\u975e\u5e73\u51e1\u7684\u771f\u6b63\u4f8b\u7387\u3002", "conclusion": "\u91cf\u5316\u56de\u5f52\u65b9\u6cd5\u4e3a\u6210\u5458\u63a8\u65ad\u653b\u51fb\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u9762\u5bf9\u6781\u7aef\u5206\u5e03\u504f\u79fb\u65f6\u3002\u5c3d\u7ba1\u5176\u4e5f\u5b58\u5728\u4e00\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u4f46\u5176\u6f5c\u529b\u503c\u5f97\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.07403", "pdf": "https://arxiv.org/pdf/2506.07403", "abs": "https://arxiv.org/abs/2506.07403", "authors": ["Peiru Yang", "Xintian Li", "Wanchun Ni", "Jinhua Yin", "Huili Wang", "Guoshun Nan", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "Enhancing Watermarking Quality for LLMs via Contextual Generation States Awareness", "categories": ["cs.CR"], "comment": null, "summary": "Recent advancements in watermarking techniques have enabled the embedding of\nsecret messages into AI-generated text (AIGT), serving as an important\nmechanism for AIGT detection. Existing methods typically interfere with the\ngeneration processes of large language models (LLMs) to embed signals within\nthe generated text. However, these methods often rely on heuristic rules, which\ncan result in suboptimal token selection and a subsequent decline in the\nquality of the generated content. In this paper, we introduce a plug-and-play\ncontextual generation states-aware watermarking framework (CAW) that\ndynamically adjusts the embedding process. It can be seamlessly integrated with\nvarious existing watermarking methods to enhance generation quality. First, CAW\nincorporates a watermarking capacity evaluator, which can assess the impact of\nembedding messages at different token positions by analyzing the contextual\ngeneration states. Furthermore, we introduce a multi-branch pre-generation\nmechanism to avoid the latency caused by the proposed watermarking strategy.\nBuilding on this, CAW can dynamically adjust the watermarking process based on\nthe evaluated watermark capacity of each token, thereby minimizing potential\ndegradation in content quality. Extensive experiments conducted on datasets\nacross multiple domains have verified the effectiveness of our method,\ndemonstrating superior performance compared to various baselines in terms of\nboth detection rate and generation quality.", "AI": {"tldr": "\u8fd1\u671f\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u72b6\u6001\u611f\u77e5\u6c34\u5370\u6846\u67b6\uff08CAW\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6c34\u5370\u5d4c\u5165\u8fc7\u7a0b\uff0c\u5728\u63d0\u9ad8AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7387\u7684\u540c\u65f6\u4fdd\u8bc1\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u7684\u6587\u672c\u6c34\u5370\u6280\u672f\u901a\u5e38\u5e72\u6270\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4ee5\u5d4c\u5165\u4fe1\u53f7\uff0c\u4f46\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684\u6807\u8bb0\u9009\u62e9\u548c\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u5f15\u5165\u4e86CAW\u6846\u67b6\uff0c\u5305\u542b\u6c34\u5370\u5bb9\u91cf\u8bc4\u4f30\u5668\u548c\u591a\u5206\u652f\u9884\u751f\u6210\u673a\u5236\u3002\u6c34\u5370\u5bb9\u91cf\u8bc4\u4f30\u5668\u5206\u6790\u4e0a\u4e0b\u6587\u751f\u6210\u72b6\u6001\uff0c\u8bc4\u4f30\u4e0d\u540c\u6807\u8bb0\u4f4d\u7f6e\u5d4c\u5165\u6d88\u606f\u7684\u5f71\u54cd\uff1b\u591a\u5206\u652f\u9884\u751f\u6210\u673a\u5236\u907f\u514d\u7531\u6c34\u5370\u7b56\u7565\u5f15\u8d77\u7684\u5ef6\u8fdf\uff1b\u57fa\u4e8e\u8bc4\u4f30\u7ed3\u679c\u52a8\u6001\u8c03\u6574\u6c34\u5370\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5185\u5bb9\u8d28\u91cf\u7684\u6f5c\u5728\u9000\u5316\u3002", "result": "\u5728\u591a\u4e2a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u8f83\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u68c0\u6d4b\u7387\u548c\u751f\u6210\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CAW\u6846\u67b6\u80fd\u591f\u4e0e\u73b0\u6709\u6c34\u5370\u65b9\u6cd5\u65e0\u7f1d\u96c6\u6210\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u68c0\u6d4b\u7387\u3002"}}
{"id": "2506.06941", "pdf": "https://arxiv.org/pdf/2506.06941", "abs": "https://arxiv.org/abs/2506.06941", "authors": ["Parshin Shojaee", "Iman Mirzadeh", "Keivan Alizadeh", "Maxwell Horton", "Samy Bengio", "Mehrdad Farajtabar"], "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u6027\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u65f6\u4f1a\u51fa\u73b0\u51c6\u786e\u6027\u7684\u5b8c\u5168\u5d29\u6e83\uff0c\u4e14\u5176\u63a8\u7406\u52aa\u529b\u968f\u95ee\u9898\u590d\u6742\u6027\u589e\u52a0\u5230\u67d0\u4e00\u70b9\u540e\u4e0b\u964d\u3002\u7814\u7a76\u901a\u8fc7\u53ef\u63a7\u8c1c\u9898\u73af\u5883\u63ed\u793a\u4e86LRMs\u7684\u4e09\u4e2a\u6027\u80fd\u9636\u6bb5\u53ca\u63a8\u7406\u75d5\u8ff9\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bf9\u5176\u57fa\u672c\u80fd\u529b\u3001\u6269\u5c55\u7279\u6027\u548c\u5c40\u9650\u6027\u7406\u89e3\u4e0d\u8db3\u3002\u5f53\u524d\u8bc4\u4f30\u4e3b\u8981\u96c6\u4e2d\u5728\u6570\u5b66\u548c\u7f16\u7801\u57fa\u51c6\u4e0a\uff0c\u5f3a\u8c03\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u800c\u5ffd\u7565\u4e86\u63a8\u7406\u75d5\u8ff9\u3002", "method": "\u5229\u7528\u53ef\u63a7\u8c1c\u9898\u73af\u5883\u8fdb\u884c\u7cfb\u7edf\u7814\u7a76\uff0c\u5141\u8bb8\u7cbe\u786e\u64cd\u7eb5\u590d\u6742\u6027\u5e76\u4fdd\u6301\u4e00\u81f4\u7684\u903b\u8f91\u7ed3\u6784\uff0c\u5206\u6790\u6700\u7ec8\u7b54\u6848\u548c\u5185\u90e8\u63a8\u7406\u75d5\u8ff9\u3002\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u6bd4\u8f83LRMs\u4e0e\u6807\u51c6LLM\u5728\u76f8\u540c\u63a8\u7406\u8ba1\u7b97\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0LRMs\u5728\u4e09\u79cd\u4efb\u52a1\u590d\u6742\u5ea6\u4e0b\u7684\u8868\u73b0\uff1a\u4f4e\u590d\u6742\u5ea6\u4efb\u52a1\u4e2d\u6807\u51c6\u6a21\u578b\u4f18\u4e8eLRMs\uff1b\u4e2d\u7b49\u590d\u6742\u5ea6\u4efb\u52a1\u4e2dLRMs\u5360\u4f18\uff1b\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\u4e2d\u4e24\u8005\u5747\u5b8c\u5168\u5d29\u6e83\u3002\u6b64\u5916\uff0cLRMs\u5728\u7cbe\u786e\u8ba1\u7b97\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u4f7f\u7528\u660e\u786e\u7b97\u6cd5\u4e14\u63a8\u7406\u4e0d\u4e00\u81f4\u3002", "conclusion": "LRMs\u5728\u5904\u7406\u4e2d\u7b49\u590d\u6742\u5ea6\u4efb\u52a1\u65f6\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u9ad8\u590d\u6742\u5ea6\u4efb\u52a1\u4e2d\u53d7\u9650\u4e8e\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u884c\u4e3a\u3002\u7814\u7a76\u63ed\u793a\u4e86LRMs\u7684\u4f18\u52bf\u3001\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u5173\u4e8e\u5176\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\u3002"}}
{"id": "2506.06489", "pdf": "https://arxiv.org/pdf/2506.06489", "abs": "https://arxiv.org/abs/2506.06489", "authors": ["Daniel Kunin", "Giovanni Luca Marchetti", "Feng Chen", "Dhruva Karkada", "James B. Simon", "Michael R. DeWeese", "Surya Ganguli", "Nina Miolane"], "title": "Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks", "categories": ["cs.LG", "stat.ML"], "comment": "35 pages, 7 figures", "summary": "What features neural networks learn, and how, remains an open question. In\nthis paper, we introduce Alternating Gradient Flows (AGF), an algorithmic\nframework that describes the dynamics of feature learning in two-layer networks\ntrained from small initialization. Prior works have shown that gradient flow in\nthis regime exhibits a staircase-like loss curve, alternating between plateaus\nwhere neurons slowly align to useful directions and sharp drops where neurons\nrapidly grow in norm. AGF approximates this behavior as an alternating two-step\nprocess: maximizing a utility function over dormant neurons and minimizing a\ncost function over active ones. AGF begins with all neurons dormant. At each\nround, a dormant neuron activates, triggering the acquisition of a feature and\na drop in the loss. AGF quantifies the order, timing, and magnitude of these\ndrops, matching experiments across architectures. We show that AGF unifies and\nextends existing saddle-to-saddle analyses in fully connected linear networks\nand attention-only linear transformers, where the learned features are singular\nmodes and principal components, respectively. In diagonal linear networks, we\nprove AGF converges to gradient flow in the limit of vanishing initialization.\nApplying AGF to quadratic networks trained to perform modular addition, we give\nthe first complete characterization of the training dynamics, revealing that\nnetworks learn Fourier features in decreasing order of coefficient magnitude.\nAltogether, AGF offers a promising step towards understanding feature learning\nin neural networks.", "AI": {"tldr": "The paper introduces Alternating Gradient Flows (AGF), a framework describing feature learning dynamics in two-layer neural networks with small initialization, unifying prior analyses and providing new insights.", "motivation": "To address the open question of what features neural networks learn and how they learn them, especially focusing on the dynamics of feature learning in networks trained from small initialization.", "method": "Introduced Alternating Gradient Flows (AGF), an algorithmic framework that models feature learning as an alternating two-step process: maximizing utility over dormant neurons and minimizing cost over active ones. This framework quantifies the order, timing, and magnitude of loss drops during training.", "result": "AGF matches experimental results across architectures, extends existing analyses in fully connected linear networks and linear transformers, and provides the first complete characterization of training dynamics for quadratic networks performing modular addition. Revealed that Fourier features are learned in decreasing order of coefficient magnitude.", "conclusion": "AGF represents a promising advancement in understanding feature learning in neural networks, offering both unification of previous theories and novel insights into training dynamics."}}
{"id": "2506.07404", "pdf": "https://arxiv.org/pdf/2506.07404", "abs": "https://arxiv.org/abs/2506.07404", "authors": ["Yujun Ji", "Jinsheng Li", "Ling Liu", "Qi Cao", "Tao Dai"], "title": "Pixel-Sensitive and Robust Steganography Based on Polar Codes", "categories": ["cs.CR", "cs.IT", "math.IT"], "comment": null, "summary": "Steganography is an information hiding technique for covert communication.\nThe core issue in steganography design is the rate-distortion coding problem.\nPolar codes, which have been proven to achieve the rate-distortion bound for\nany binary symmetric source, are utilized to design a steganographic scheme\nthat can reach the embedding capacity for the Distortion-Limited Sender problem\nin certain cases. In adaptive steganography, for attack scenarios where each\nnoise element can have different intensities, existing steganographic coding\nmethods fail to resist such attacks. In this paper, we propose a\npixel-sensitive and robust steganographic scheme based on polar codes. Our\nsteganographic scheme not only matches the adaptive distortion well but is also\nrobust against sophisticated noise attacks. Futher, it is proven that our\nscheme achieves the embedding capacity in certain cases. Experimentally, a\nsteganographic scheme can be designed and implemented with a secret message\nerror rate at the $10^{-5}$ level when the attack noise is known to both the\nsender and the receiver. This demonstrates its significant robustness.", "AI": {"tldr": "This paper proposes a pixel-sensitive and robust steganographic scheme based on polar codes to solve the rate-distortion coding problem in steganography design, which can resist sophisticated noise attacks and achieve the embedding capacity in certain cases.", "motivation": "The core issue in steganography design is the rate-distortion coding problem, and existing steganographic coding methods fail to resist attack scenarios where each noise element can have different intensities in adaptive steganography.", "method": "Utilize polar codes to design a steganographic scheme that not only matches the adaptive distortion well but is also robust against sophisticated noise attacks.", "result": "Experimentally, a steganographic scheme can be designed and implemented with a secret message error rate at the $10^{-5}$ level when the attack noise is known to both the sender and the receiver.", "conclusion": "The proposed pixel-sensitive and robust steganographic scheme based on polar codes can resist sophisticated noise attacks and achieve the embedding capacity in certain cases."}}
{"id": "2506.06959", "pdf": "https://arxiv.org/pdf/2506.06959", "abs": "https://arxiv.org/abs/2506.06959", "authors": ["Alena Makarova", "Houssam Abbas"], "title": "Deontically Constrained Policy Improvement in Reinforcement Learning Agents", "categories": ["cs.AI", "60J10 (Primary), 60J20 (Primary), 60J22 (Primary), 93E20 (Secondary)", "D.2.4; F.3.1; I.2.8"], "comment": "20 pages, 11 figures, DEON2025 conference", "summary": "Markov Decision Processes (MDPs) are the most common model for decision\nmaking under uncertainty in the Machine Learning community. An MDP captures\nnon-determinism, probabilistic uncertainty, and an explicit model of action. A\nReinforcement Learning (RL) agent learns to act in an MDP by maximizing a\nutility function. This paper considers the problem of learning a decision\npolicy that maximizes utility subject to satisfying a constraint expressed in\ndeontic logic. In this setup, the utility captures the agent's mission - such\nas going quickly from A to B. The deontic formula represents (ethical, social,\nsituational) constraints on how the agent might achieve its mission by\nprohibiting classes of behaviors. We use the logic of Expected Act\nUtilitarianism, a probabilistic stit logic that can be interpreted over\ncontrolled MDPs. We develop a variation on policy improvement, and show that it\nreaches a constrained local maximum of the mission utility. Given that in stit\nlogic, an agent's duty is derived from value maximization, this can be seen as\na way of acting to simultaneously maximize two value functions, one of which is\nimplicit, in a bi-level structure. We illustrate these results with experiments\non sample MDPs.", "AI": {"tldr": "This paper explores how to learn a decision policy in MDPs that maximizes utility while satisfying constraints expressed in deontic logic.", "motivation": "To address the challenge of learning a decision policy that not only maximizes utility but also adheres to ethical, social, or situational constraints.", "method": "The authors use the logic of Expected Act Utilitarianism and develop a variation on policy improvement to achieve this.", "result": "They show that their method reaches a constrained local maximum of the mission utility and can be interpreted as maximizing two value functions in a bi-level structure.", "conclusion": "The results are illustrated with experiments on sample MDPs."}}
{"id": "2506.06499", "pdf": "https://arxiv.org/pdf/2506.06499", "abs": "https://arxiv.org/abs/2506.06499", "authors": ["Alex Havrilla", "Edward Hughes", "Mikayel Samvelyan", "Jacob Abernethy"], "title": "Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language model (LLM) driven synthetic data generation has emerged as a\npowerful method for improving model reasoning capabilities. However, most\nmethods either distill large state-of-the-art models into small students or use\nnatural ground-truth problem statements to guarantee problem statement quality.\nThis limits the scalability of these approaches to more complex and diverse\nproblem domains. To address this, we present SPARQ: Synthetic Problem\nGeneration for Reasoning via Quality-Diversity Algorithms, a novel approach for\ngenerating high-quality and diverse synthetic math problem and solution pairs\nusing only a single model by measuring a problem's solve-rate: a proxy for\nproblem difficulty. Starting from a seed dataset of 7.5K samples, we generate\nover 20 million new problem-solution pairs. We show that filtering the\ngenerated data by difficulty and then fine-tuning the same model on the\nresulting data improves relative model performance by up to 24\\%. Additionally,\nwe conduct ablations studying the impact of synthetic data quantity, quality\nand diversity on model generalization. We find that higher quality, as measured\nby problem difficulty, facilitates better in-distribution performance. Further,\nwhile generating diverse synthetic data does not as strongly benefit\nin-distribution performance, filtering for more diverse data facilitates more\nrobust OOD generalization. We also confirm the existence of model and data\nscaling laws for synthetically generated problems, which positively benefit\ndownstream model generalization.", "AI": {"tldr": "Large language model driven synthetic data generation is a powerful method for improving model reasoning capabilities. The paper presents SPARQ, which generates high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate.", "motivation": "To address the limitations of current methods that either distill large state-of-the-art models into small students or use natural ground-truth problem statements to guarantee problem statement quality, which limits the scalability of these approaches to more complex and diverse problem domains.", "method": "SPARQ: Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms, which generates high-quality and diverse synthetic math problem and solution pairs using only a single model by measuring a problem's solve-rate.", "result": "Generated over 20 million new problem-solution pairs from a seed dataset of 7.5K samples. Filtering the generated data by difficulty and then fine-tuning the same model on the resulting data improves relative model performance by up to 24%. Higher quality facilitates better in-distribution performance, while more diverse data facilitates more robust OOD generalization.", "conclusion": "The existence of model and data scaling laws for synthetically generated problems positively benefits downstream model generalization."}}
{"id": "2506.07480", "pdf": "https://arxiv.org/pdf/2506.07480", "abs": "https://arxiv.org/abs/2506.07480", "authors": ["Bassam Noori Shaker", "Bahaa Al-Musawi", "Mohammed Falih Hassan"], "title": "Explainable AI for Enhancing IDS Against Advanced Persistent Kill Chain", "categories": ["cs.CR"], "comment": null, "summary": "Advanced Persistent Threats (APTs) represent a sophisticated and persistent\ncy-bersecurity challenge, characterized by stealthy, multi-phase, and targeted\nattacks aimed at compromising information systems over an extended period.\nDevelop-ing an effective Intrusion Detection System (IDS) capable of detecting\nAPTs at different phases relies on selecting network traffic features. However,\nnot all of these features are directly related to the phases of APTs. Some\nnetwork traffic features may be unrelated or have limited relevance to\nidentifying malicious ac-tivity. Therefore, it is important to carefully select\nand analyze the most relevant features to improve the IDS performance. This\nwork proposes a feature selection and classification model that integrates two\nprominent machine learning algo-rithms: SHapley Additive exPlanations (SHAP)\nand Extreme Gradient Boosting (XGBoost). The aim is to develop lightweight IDS\nbased on a selected minimum number of influential features for detecting APTs\nat various phases. The pro-posed method also specifies the relevant features\nfor each phase of APTs inde-pendently. Extensive experimental results on the\nSCVIC-APT-2021 dataset indi-cated that our proposed approach has improved\nperformance compared to other standard techniques. Specifically, both the\nmacro-average F1-score and recall reached 94% and 93 %, respectively, while\nreducing the complexity of the detec-tion model by selecting only 12 features\nout of 77.", "AI": {"tldr": "The paper proposes a feature selection and classification model integrating SHAP and XGBoost for detecting APTs at various phases with improved performance and reduced complexity.", "motivation": "Advanced Persistent Threats (APTs) pose significant challenges in cybersecurity, necessitating the development of effective Intrusion Detection Systems (IDS). The need for feature selection to enhance IDS performance by focusing on relevant features is crucial.", "method": "The method involves using SHapley Additive exPlanations (SHAP) and Extreme Gradient Boosting (XGBoost) for feature selection and classification. It identifies a minimal set of influential features for detecting APTs at different phases independently.", "result": "The approach achieved macro-average F1-score and recall of 94% and 93%, respectively, while reducing model complexity by selecting only 12 features from an original set of 77.", "conclusion": "The proposed feature selection and classification model demonstrates superior performance compared to standard techniques, effectively detecting APTs with fewer features."}}
{"id": "2506.06965", "pdf": "https://arxiv.org/pdf/2506.06965", "abs": "https://arxiv.org/abs/2506.06965", "authors": ["Cuong Manh Hoang"], "title": "Long-Tailed Learning for Generalized Category Discovery", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) utilizes labeled samples of known\nclasses to discover novel classes in unlabeled samples. Existing methods show\neffective performance on artificial datasets with balanced distributions.\nHowever, real-world datasets are always imbalanced, significantly affecting the\neffectiveness of these methods. To solve this problem, we propose a novel\nframework that performs generalized category discovery in long-tailed\ndistributions. We first present a self-guided labeling technique that uses a\nlearnable distribution to generate pseudo-labels, resulting in less biased\nclassifiers. We then introduce a representation balancing process to derive\ndiscriminative representations. By mining sample neighborhoods, this process\nencourages the model to focus more on tail classes. We conduct experiments on\npublic datasets to demonstrate the effectiveness of the proposed framework. The\nresults show that our model exceeds previous state-of-the-art methods.", "AI": {"tldr": "This paper addresses the issue of Generalized Category Discovery (GCD) in imbalanced, long-tailed datasets. It proposes a novel framework including self-guided labeling and representation balancing process to improve model performance on discovering novel classes.", "motivation": "Existing GCD methods work well on balanced artificial datasets but struggle with real-world imbalanced datasets which affect their effectiveness.", "method": "The paper introduces a framework that uses self-guided labeling with learnable distribution for pseudo-labels and a representation balancing process that mines sample neighborhoods to focus on tail classes.", "result": "Experiments on public datasets demonstrate the effectiveness of the proposed framework, showing better performance than previous state-of-the-art methods.", "conclusion": "The proposed framework successfully performs generalized category discovery in long-tailed distributions, improving upon existing methods."}}
{"id": "2506.06501", "pdf": "https://arxiv.org/pdf/2506.06501", "abs": "https://arxiv.org/abs/2506.06501", "authors": ["Ran Levinstein", "Amit Attia", "Matan Schliserman", "Uri Sherman", "Tomer Koren", "Daniel Soudry", "Itay Evron"], "title": "Optimal Rates in Continual Linear Regression via Increasing Regularization", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We study realizable continual linear regression under random task orderings,\na common setting for developing continual learning theory. In this setup, the\nworst-case expected loss after $k$ learning iterations admits a lower bound of\n$\\Omega(1/k)$. However, prior work using an unregularized scheme has only\nestablished an upper bound of $O(1/k^{1/4})$, leaving a significant gap. Our\npaper proves that this gap can be narrowed, or even closed, using two\nfrequently used regularization schemes: (1) explicit isotropic $\\ell_2$\nregularization, and (2) implicit regularization via finite step budgets. We\nshow that these approaches, which are used in practice to mitigate forgetting,\nreduce to stochastic gradient descent (SGD) on carefully defined surrogate\nlosses. Through this lens, we identify a fixed regularization strength that\nyields a near-optimal rate of $O(\\log k / k)$. Moreover, formalizing and\nanalyzing a generalized variant of SGD for time-varying functions, we derive an\nincreasing regularization strength schedule that provably achieves an optimal\nrate of $O(1/k)$. This suggests that schedules that increase the regularization\ncoefficient or decrease the number of steps per task are beneficial, at least\nin the worst case.", "AI": {"tldr": "\u5728\u968f\u673a\u4efb\u52a1\u987a\u5e8f\u4e0b\u7814\u7a76\u53ef\u5b9e\u73b0\u7684\u8fde\u7eed\u7ebf\u6027\u56de\u5f52\uff0c\u901a\u8fc7\u4e24\u79cd\u6b63\u5219\u5316\u65b9\u6cd5\u7f29\u5c0f\u6216\u5173\u95ed\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\u5dee\u8ddd\uff0c\u5e76\u8bc1\u660e\u589e\u52a0\u6b63\u5219\u5316\u5f3a\u5ea6\u6216\u51cf\u5c11\u6bcf\u4efb\u52a1\u6b65\u9aa4\u6570\u662f\u6709\u76ca\u7684\u3002", "motivation": "\u89e3\u51b3\u4e4b\u524d\u5de5\u4f5c\u4e2d\u7684\u663e\u8457\u5dee\u8ddd\u95ee\u9898\uff0c\u5373\u5728k\u6b21\u5b66\u4e60\u8fed\u4ee3\u540e\uff0c\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9884\u671f\u635f\u5931\u5b58\u5728$\\Omega(1/k)$\u7684\u4e0b\u754c\uff0c\u800c\u65e0\u6b63\u5219\u5316\u65b9\u6848\u4ec5\u5efa\u7acb\u4e86$O(1/k^{1/4})$\u7684\u4e0a\u754c\u3002", "method": "\u4f7f\u7528\u4e24\u79cd\u6b63\u5219\u5316\u65b9\u6848\uff1a\uff081\uff09\u663e\u5f0f\u7684\u5404\u5411\u540c\u6027$\\ell_2$\u6b63\u5219\u5316\uff1b\uff082\uff09\u901a\u8fc7\u6709\u9650\u6b65\u6570\u9884\u7b97\u7684\u9690\u5f0f\u6b63\u5219\u5316\u3002\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u5f52\u7ed3\u4e3a\u5728\u7cbe\u5fc3\u5b9a\u4e49\u7684\u4ee3\u7406\u635f\u5931\u4e0a\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff08SGD\uff09\uff0c\u5e76\u5206\u6790\u5e7f\u4e49\u65f6\u95f4\u53d8\u5316\u51fd\u6570\u7684SGD\u53d8\u4f53\u3002", "result": "\u786e\u5b9a\u4e86\u4e00\u4e2a\u56fa\u5b9a\u7684\u6b63\u5219\u5316\u5f3a\u5ea6\uff0c\u53ef\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u901f\u5ea6$O(\\log k / k)$\uff0c\u5e76\u901a\u8fc7\u589e\u52a0\u6b63\u5219\u5316\u5f3a\u5ea6\u8ba1\u5212\u8fbe\u5230\u4e86\u6700\u4f18\u901f\u5ea6$O(1/k)$\u3002", "conclusion": "\u589e\u52a0\u6b63\u5219\u5316\u7cfb\u6570\u6216\u51cf\u5c11\u6bcf\u4efb\u52a1\u6b65\u9aa4\u6570\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u662f\u6709\u76ca\u7684\u3002"}}
{"id": "2506.07586", "pdf": "https://arxiv.org/pdf/2506.07586", "abs": "https://arxiv.org/abs/2506.07586", "authors": ["Bikash Saha", "Sandeep Kumar Shukla"], "title": "MalGEN: A Generative Agent Framework for Modeling Malicious Software in Cybersecurity", "categories": ["cs.CR"], "comment": null, "summary": "The dual use nature of Large Language Models (LLMs) presents a growing\nchallenge in cybersecurity. While LLM enhances automation and reasoning for\ndefenders, they also introduce new risks, particularly their potential to be\nmisused for generating evasive, AI crafted malware. Despite this emerging\nthreat, the research community currently lacks controlled and extensible tools\nthat can simulate such behavior for testing and defense preparation. We present\nMalGEN, a multi agent framework that simulates coordinated adversarial behavior\nto generate diverse, activity driven malware samples. The agents work\ncollaboratively to emulate attacker workflows, including payload planning,\ncapability selection, and evasion strategies, within a controlled environment\nbuilt for ethical and defensive research. Using MalGEN, we synthesized ten\nnovel malware samples and evaluated them against leading antivirus and\nbehavioral detection engines. Several samples exhibited stealthy and evasive\ncharacteristics that bypassed current defenses, validating MalGEN's ability to\nmodel sophisticated and new threats. By transforming the threat of LLM misuse\ninto an opportunity for proactive defense, MalGEN offers a valuable framework\nfor evaluating and strengthening cybersecurity systems. The framework addresses\ndata scarcity, enables rigorous testing, and supports the development of\nresilient and future ready detection strategies.", "AI": {"tldr": "The paper introduces MalGEN, a multi-agent framework that simulates adversarial behavior to generate diverse malware samples for testing and improving cybersecurity defenses. Using LLMs' potential for misuse as an advantage, it creates novel malware samples that bypass current defenses, thus helping evaluate and strengthen security systems.", "motivation": "To address the lack of controlled and extensible tools in the research community that can simulate adversarial behavior using Large Language Models (LLMs) for ethical and defensive research.", "method": "MalGEN is a multi-agent framework that simulates coordinated adversarial behavior to generate diverse, activity-driven malware samples. It emulates attacker workflows including payload planning, capability selection, and evasion strategies within a controlled environment designed for ethical and defensive research.", "result": "Using MalGEN, ten novel malware samples were synthesized and evaluated against leading antivirus and behavioral detection engines. Several samples successfully bypassed current defenses, demonstrating stealthy and evasive characteristics.", "conclusion": "MalGEN transforms the threat of LLM misuse into an opportunity for proactive defense by offering a valuable framework for evaluating and strengthening cybersecurity systems. It addresses data scarcity, enables rigorous testing, and supports the development of resilient and future-ready detection strategies."}}
{"id": "2506.06981", "pdf": "https://arxiv.org/pdf/2506.06981", "abs": "https://arxiv.org/abs/2506.06981", "authors": ["Riley Simmons-Edler", "Ryan P. Badman", "Felix Baastad Berg", "Raymond Chua", "John J. Vastola", "Joshua Lunger", "William Qian", "Kanaka Rajan"], "title": "Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Understanding the behavior of deep reinforcement learning (DRL) agents --\nparticularly as task and agent sophistication increase -- requires more than\nsimple comparison of reward curves, yet standard methods for behavioral\nanalysis remain underdeveloped in DRL. We apply tools from neuroscience and\nethology to study DRL agents in a novel, complex, partially observable\nenvironment, ForageWorld, designed to capture key aspects of real-world animal\nforaging -- including sparse, depleting resource patches, predator threats, and\nspatially extended arenas. We use this environment as a platform for applying\njoint behavioral and neural analysis to agents, revealing detailed,\nquantitatively grounded insights into agent strategies, memory, and planning.\nContrary to common assumptions, we find that model-free RNN-based DRL agents\ncan exhibit structured, planning-like behavior purely through emergent dynamics\n-- without requiring explicit memory modules or world models. Our results show\nthat studying DRL agents like animals -- analyzing them with\nneuroethology-inspired tools that reveal structure in both behavior and neural\ndynamics -- uncovers rich structure in their learning dynamics that would\notherwise remain invisible. We distill these tools into a general analysis\nframework linking core behavioral and representational features to diagnostic\nmethods, which can be reused for a wide range of tasks and agents. As agents\ngrow more complex and autonomous, bridging neuroscience, cognitive science, and\nAI will be essential -- not just for understanding their behavior, but for\nensuring safe alignment and maximizing desirable behaviors that are hard to\nmeasure via reward. We show how this can be done by drawing on lessons from how\nbiological intelligence is studied.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u548c\u751f\u6001\u5b66\u7684\u5de5\u5177\u7814\u7a76\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u65e0\u6a21\u578bRNN\u57fa\u7840\u7684DRL\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u6d8c\u73b0\u52a8\u529b\u5b66\u5c55\u793a\u7ed3\u6784\u5316\u7684\u89c4\u5212\u884c\u4e3a\u3002\u63d0\u51fa\u4e86\u4e00\u822c\u5206\u6790\u6846\u67b6\uff0c\u5c06\u6838\u5fc3\u884c\u4e3a\u548c\u8868\u5f81\u7279\u5f81\u4e0e\u8bca\u65ad\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u7406\u89e3\u548c\u786e\u4fdd\u590d\u6742\u81ea\u4e3b\u4ee3\u7406\u7684\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u6807\u51c6\u7684\u884c\u4e3a\u5206\u6790\u65b9\u6cd5\u5c1a\u672a\u5145\u5206\u53d1\u5c55\uff0c\u7b80\u5355\u7684\u5956\u52b1\u66f2\u7ebf\u6bd4\u8f83\u4e0d\u8db3\u4ee5\u7406\u89e3\u4ee3\u7406\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u4efb\u52a1\u548c\u4ee3\u7406\u590d\u6742\u6027\u589e\u52a0\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u590d\u6742\u73af\u5883ForageWorld\uff0c\u5e76\u5e94\u7528\u795e\u7ecf\u79d1\u5b66\u548c\u751f\u6001\u5b66\u7684\u5de5\u5177\u5bf9DRL\u4ee3\u7406\u8fdb\u884c\u8054\u5408\u884c\u4e3a\u548c\u795e\u7ecf\u5206\u6790\uff0c\u63ed\u793a\u4ee3\u7406\u7b56\u7565\u3001\u8bb0\u5fc6\u548c\u89c4\u5212\u7684\u8be6\u7ec6\u89c1\u89e3\u3002", "result": "\u53d1\u73b0\u65e0\u6a21\u578bRNN\u57fa\u7840\u7684DRL\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u6d8c\u73b0\u52a8\u529b\u5b66\u5c55\u793a\u7ed3\u6784\u5316\u7684\u89c4\u5212\u884c\u4e3a\uff0c\u800c\u4e0d\u9700\u8981\u660e\u786e\u7684\u8bb0\u5fc6\u6a21\u5757\u6216\u4e16\u754c\u6a21\u578b\u3002\u901a\u8fc7\u795e\u7ecf\u751f\u6001\u5b66\u542f\u53d1\u7684\u5de5\u5177\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u5b66\u4e60\u52a8\u529b\u5b66\u4e2d\u7684\u4e30\u5bcc\u7ed3\u6784\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u5206\u6790\u6846\u67b6\uff0c\u8fde\u63a5\u884c\u4e3a\u548c\u8868\u5f81\u7279\u5f81\u4e0e\u8bca\u65ad\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u7ed3\u5408\u795e\u7ecf\u79d1\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\u548cAI\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u786e\u4fdd\u590d\u6742\u81ea\u4e3b\u4ee3\u7406\u7684\u5b89\u5168\u5bf9\u9f50\u548c\u6700\u5927\u5316\u671f\u671b\u884c\u4e3a\u3002"}}
{"id": "2506.06505", "pdf": "https://arxiv.org/pdf/2506.06505", "abs": "https://arxiv.org/abs/2506.06505", "authors": ["Keisuke Sugiura", "Hiroki Matsutani"], "title": "InstantFT: An FPGA-Based Runtime Subsecond Fine-tuning of CNN Models", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "Training deep neural networks (DNNs) requires significantly more computation\nand memory than inference, making runtime adaptation of DNNs challenging on\nresource-limited IoT platforms. We propose InstantFT, an FPGA-based method for\nultra-fast CNN fine-tuning on IoT devices, by optimizing the forward and\nbackward computations in parameter-efficient fine-tuning (PEFT). Experiments on\ndatasets with concept drift demonstrate that InstantFT fine-tunes a pre-trained\nCNN 17.4x faster than existing Low-Rank Adaptation (LoRA)-based approaches,\nwhile achieving comparable accuracy. Our FPGA-based InstantFT reduces the\nfine-tuning time to just 0.36s and improves energy-efficiency by 16.3x,\nenabling on-the-fly adaptation of CNNs to non-stationary data distributions.", "AI": {"tldr": "InstantFT is an FPGA-based method for ultra-fast CNN fine-tuning on IoT devices, reducing fine-tuning time and improving energy-efficiency significantly.", "motivation": "Training DNNs needs more computation and memory than inference, posing a challenge to runtime adaptation of DNNs on resource-limited IoT platforms.", "method": "Propose InstantFT which optimizes forward and backward computations in PEFT for ultra-fast CNN fine-tuning.", "result": "Experiments show that InstantFT fine-tunes a pre-trained CNN 17.4x faster with comparable accuracy, reduces fine-tuning time to 0.36s and improves energy-efficiency by 16.3x.", "conclusion": "InstantFT enables on-the-fly adaptation of CNNs to non-stationary data distributions."}}
{"id": "2506.07605", "pdf": "https://arxiv.org/pdf/2506.07605", "abs": "https://arxiv.org/abs/2506.07605", "authors": ["Marco Di Gennaro", "Giovanni De Lucia", "Stefano Longari", "Stefano Zanero", "Michele Carminati"], "title": "TimberStrike: Dataset Reconstruction Attack Revealing Privacy Leakage in Federated Tree-Based Systems", "categories": ["cs.CR", "cs.DC", "cs.LG"], "comment": "Proceedings on Privacy Enhancing Technologies (To appear) 2025(4)", "summary": "Federated Learning has emerged as a privacy-oriented alternative to\ncentralized Machine Learning, enabling collaborative model training without\ndirect data sharing. While extensively studied for neural networks, the\nsecurity and privacy implications of tree-based models remain underexplored.\nThis work introduces TimberStrike, an optimization-based dataset reconstruction\nattack targeting horizontally federated tree-based models. Our attack, carried\nout by a single client, exploits the discrete nature of decision trees by using\nsplit values and decision paths to infer sensitive training data from other\nclients. We evaluate TimberStrike on State-of-the-Art federated gradient\nboosting implementations across multiple frameworks, including Flower, NVFlare,\nand FedTree, demonstrating their vulnerability to privacy breaches. On a\npublicly available stroke prediction dataset, TimberStrike consistently\nreconstructs between 73.05% and 95.63% of the target dataset across all\nimplementations. We further analyze Differential Privacy, showing that while it\npartially mitigates the attack, it also significantly degrades model\nperformance. Our findings highlight the need for privacy-preserving mechanisms\nspecifically designed for tree-based Federated Learning systems, and we provide\npreliminary insights into their design.", "AI": {"tldr": "In this paper, researchers introduce TimberStrike, an optimization-based dataset reconstruction attack on horizontally federated tree-based models. They demonstrate its effectiveness across various frameworks and datasets, reconstructing up to 95.63% of the target data. Differential Privacy is found to only partially mitigate the attack while harming model performance.", "motivation": "The motivation for this work is the lack of research on security and privacy implications of tree-based models in Federated Learning, despite extensive studies on neural networks. The authors aim to explore vulnerabilities in federated tree-based models.", "method": "The method involves introducing TimberStrike, a dataset reconstruction attack that exploits split values and decision paths in tree-based models to infer sensitive training data from other clients. This is performed by a single client in a horizontally federated learning setting.", "result": "TimberStrike successfully reconstructs between 73.05% and 95.63% of the target dataset across different implementations. Differential Privacy offers partial mitigation but significantly reduces model performance.", "conclusion": "The conclusion emphasizes the need for developing privacy-preserving mechanisms specifically tailored for tree-based Federated Learning systems, with some initial insights provided."}}
{"id": "2506.06991", "pdf": "https://arxiv.org/pdf/2506.06991", "abs": "https://arxiv.org/abs/2506.06991", "authors": ["Yichi Zhang", "Jinlong Pang", "Zhaowei Zhu", "Yang Liu"], "title": "Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth", "categories": ["cs.AI", "cs.GT", "cs.HC"], "comment": "33 pages, 9 figures", "summary": "The recent success of generative AI highlights the crucial role of\nhigh-quality human feedback in building trustworthy AI systems. However, the\nincreasing use of large language models (LLMs) by crowdsourcing workers poses a\nsignificant challenge: datasets intended to reflect human input may be\ncompromised by LLM-generated responses. Existing LLM detection approaches often\nrely on high-dimension training data such as text, making them unsuitable for\nannotation tasks like multiple-choice labeling. In this work, we investigate\nthe potential of peer prediction -- a mechanism that evaluates the information\nwithin workers' responses without using ground truth -- to mitigate\nLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our\napproach quantifies the correlations between worker answers while conditioning\non (a subset of) LLM-generated labels available to the requester. Building on\nprior research, we propose a training-free scoring mechanism with theoretical\nguarantees under a crowdsourcing model that accounts for LLM collusion. We\nestablish conditions under which our method is effective and empirically\ndemonstrate its robustness in detecting low-effort cheating on real-world\ncrowdsourcing datasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4f17\u5305\u6807\u6ce8\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u540c\u4f34\u9884\u6d4b\u673a\u5236\u6765\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u4f5c\u5f0a\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bc4\u5206\u673a\u5236\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u7684\u6210\u529f\uff0c\u9ad8\u8d28\u91cf\u7684\u4eba\u7c7b\u53cd\u9988\u5bf9\u4e8e\u6784\u5efa\u503c\u5f97\u4fe1\u8d56\u7684AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4f17\u5305\u5de5\u4f5c\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u80fd\u5bfc\u81f4\u53cd\u6620\u4eba\u7c7b\u8f93\u5165\u7684\u6570\u636e\u96c6\u88ab\u6a21\u578b\u751f\u6210\u7684\u54cd\u5e94\u6240\u7834\u574f\u3002\u73b0\u6709\u7684LLM\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u9ad8\u7ef4\u8bad\u7ec3\u6570\u636e\uff0c\u5982\u6587\u672c\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u4e0d\u9002\u5408\u7528\u4e8e\u591a\u9009\u6807\u6ce8\u7b49\u4efb\u52a1\u3002", "method": "\u7814\u7a76\u8c03\u67e5\u4e86\u540c\u4f34\u9884\u6d4b\uff08peer prediction\uff09\u7684\u6f5c\u529b\uff0c\u8fd9\u662f\u4e00\u79cd\u65e0\u9700\u4f7f\u7528\u771f\u5b9e\u7b54\u6848\u5373\u53ef\u8bc4\u4f30\u5de5\u4f5c\u8005\u54cd\u5e94\u4e2d\u4fe1\u606f\u7684\u673a\u5236\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u8bc4\u5206\u673a\u5236\uff0c\u8be5\u673a\u5236\u5728\u8003\u8651\u4e86\u8bf7\u6c42\u8005\u53ef\u83b7\u5f97\u7684\u90e8\u5206LLM\u751f\u6210\u6807\u7b7e\u7684\u60c5\u51b5\u4e0b\uff0c\u91cf\u5316\u4e86\u5de5\u4f5c\u8005\u7b54\u6848\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "result": "\u7814\u7a76\u5efa\u7acb\u4e86\u8be5\u65b9\u6cd5\u6709\u6548\u6027\u7684\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5c55\u793a\u4e86\u5176\u5728\u68c0\u6d4b\u73b0\u5b9e\u4e16\u754c\u4f17\u5305\u6570\u636e\u96c6\u4e2d\u4f4e\u52aa\u529b\u5ea6\u4f5c\u5f0a\u884c\u4e3a\u65b9\u9762\u7684\u7a33\u5065\u6027\u3002", "conclusion": "\u540c\u4f34\u9884\u6d4b\u673a\u5236\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528\u771f\u5b9e\u7b54\u6848\u7684\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u8bc6\u522b\u4f17\u5305\u4efb\u52a1\u4e2d\u7684LLM\u8f85\u52a9\u4f5c\u5f0a\u884c\u4e3a\uff0c\u4e3a\u63d0\u9ad8\u4f17\u5305\u6570\u636e\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06521", "pdf": "https://arxiv.org/pdf/2506.06521", "abs": "https://arxiv.org/abs/2506.06521", "authors": ["Shulun Chen", "Runlong Zhou", "Zihan Zhang", "Maryam Fazel", "Simon S. Du"], "title": "Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs", "categories": ["cs.LG", "stat.ML"], "comment": "30 pages", "summary": "We consider the gap-dependent regret bounds for episodic MDPs. We show that\nthe Monotonic Value Propagation (MVP) algorithm achieves a variance-aware\ngap-dependent regret bound of $$\\tilde{O}\\left(\\left(\\sum_{\\Delta_h(s,a)>0}\n\\frac{H^2 \\log K \\land \\mathtt{Var}_{\\max}^{\\text{c}}}{\\Delta_h(s,a)}\n+\\sum_{\\Delta_h(s,a)=0}\\frac{ H^2 \\land\n\\mathtt{Var}_{\\max}^{\\text{c}}}{\\Delta_{\\mathrm{min}}} + SAH^4 (S \\lor H)\n\\right) \\log K\\right),$$ where $H$ is the planning horizon, $S$ is the number\nof states, $A$ is the number of actions, and $K$ is the number of episodes.\nHere, $\\Delta_h(s,a) =V_h^* (a) - Q_h^* (s, a)$ represents the suboptimality\ngap and $\\Delta_{\\mathrm{min}} := \\min_{\\Delta_h (s,a) > 0} \\Delta_h(s,a)$. The\nterm $\\mathtt{Var}_{\\max}^{\\text{c}}$ denotes the maximum conditional total\nvariance, calculated as the maximum over all $(\\pi, h, s)$ tuples of the\nexpected total variance under policy $\\pi$ conditioned on trajectories visiting\nstate $s$ at step $h$. $\\mathtt{Var}_{\\max}^{\\text{c}}$ characterizes the\nmaximum randomness encountered when learning any $(h, s)$ pair. Our result\nstems from a novel analysis of the weighted sum of the suboptimality gap and\ncan be potentially adapted for other algorithms. To complement the study, we\nestablish a lower bound of $$\\Omega \\left( \\sum_{\\Delta_h(s,a)>0} \\frac{H^2\n\\land \\mathtt{Var}_{\\max}^{\\text{c}}}{\\Delta_h(s,a)}\\cdot \\log K\\right),$$\ndemonstrating the necessity of dependence on $\\mathtt{Var}_{\\max}^{\\text{c}}$\neven when the maximum unconditional total variance (without conditioning on\n$(h, s)$) approaches zero.", "AI": {"tldr": "The paper explores the gap-dependent regret bounds for episodic MDPs using the Monotonic Value Propagation (MVP) algorithm, presenting a variance-aware regret bound and a lower bound that emphasizes the necessity of considering maximum conditional total variance.", "motivation": "The motivation is to improve the understanding of regret bounds in episodic Markov Decision Processes (MDPs), particularly focusing on gap-dependent bounds which can offer more refined performance guarantees than worst-case scenarios. By incorporating the concept of maximum conditional total variance, the study aims to provide insights into how randomness affects learning specific state-action pairs.", "method": "The method involves analyzing the Monotonic Value Propagation (MVP) algorithm within the context of episodic MDPs. The authors derive a variance-aware gap-dependent regret bound by evaluating the weighted sum of suboptimality gaps. Additionally, they establish a lower bound to complement their findings, highlighting the importance of the maximum conditional total variance.", "result": "The result is a detailed regret bound expression that accounts for the suboptimality gap, the number of states and actions, planning horizon, and episodes. This bound demonstrates the impact of maximum conditional total variance on the learning process. The lower bound further supports the necessity of this variance consideration even when unconditional variance approaches zero.", "conclusion": "The study concludes that considering maximum conditional total variance is crucial for achieving tighter regret bounds in episodic MDPs. The derived bounds not only refine our understanding of regret in such processes but also suggest potential improvements for other algorithms through similar analyses."}}
{"id": "2506.07714", "pdf": "https://arxiv.org/pdf/2506.07714", "abs": "https://arxiv.org/abs/2506.07714", "authors": ["Francesco Marchiori", "Denis Donadel", "Alessandro Brighente", "Mauro Conti"], "title": "Profiling Electric Vehicles via Early Charging Voltage Patterns", "categories": ["cs.CR", "cs.ET", "cs.LG"], "comment": "Accepted to be presented at the AI&CPSS Workshop in conjunction with\n  ARES 2025", "summary": "Electric Vehicles (EVs) are rapidly gaining adoption as a sustainable\nalternative to fuel-powered vehicles, making secure charging infrastructure\nessential. Despite traditional authentication protocols, recent results showed\nthat attackers may steal energy through tailored relay attacks. One\ncountermeasure is leveraging the EV's fingerprint on the current exchanged\nduring charging. However, existing methods focus on the final charging stage,\nallowing malicious actors to consume substantial energy before being detected\nand repudiated. This underscores the need for earlier and more effective\nauthentication methods to prevent unauthorized charging. Meanwhile, profiling\nraises privacy concerns, as uniquely identifying EVs through charging patterns\ncould enable user tracking.\n  In this paper, we propose a framework for uniquely identifying EVs using\nphysical measurements from the early charging stages. We hypothesize that\nvoltage behavior early in the process exhibits similar characteristics to\ncurrent behavior in later stages. By extracting features from early voltage\nmeasurements, we demonstrate the feasibility of EV profiling. Our approach\nimproves existing methods by enabling faster and more reliable vehicle\nidentification. We test our solution on a dataset of 7408 usable charges from\n49 EVs, achieving up to 0.86 accuracy. Feature importance analysis shows that\nnear-optimal performance is possible with just 10 key features, improving\nefficiency alongside our lightweight models. This research lays the foundation\nfor a novel authentication factor while exposing potential privacy risks from\nunauthorized access to charging data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5145\u7535\u65e9\u671f\u9636\u6bb5\u7269\u7406\u6d4b\u91cf\u7684\u72ec\u7279EV\u8bc6\u522b\u6846\u67b6\uff0c\u53ef\u66f4\u5feb\u66f4\u53ef\u9760\u5730\u8fdb\u884c\u8f66\u8f86\u8eab\u4efd\u9a8c\u8bc1\uff0c\u4f46\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5145\u7535\u540e\u671f\u7684\u7535\u6d41\u7279\u5f81\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\uff0c\u5bb9\u6613\u906d\u53d7\u4e2d\u7ee7\u653b\u51fb\uff0c\u4e14\u65e0\u6cd5\u53ca\u65f6\u68c0\u6d4b\u6076\u610f\u884c\u4e3a\u3002\u540c\u65f6\uff0c\u901a\u8fc7\u5145\u7535\u6a21\u5f0f\u8bc6\u522bEV\u53ef\u80fd\u5f15\u53d1\u9690\u79c1\u95ee\u9898\u3002", "method": "\u5229\u7528\u5145\u7535\u65e9\u671f\u9636\u6bb5\u7684\u7535\u538b\u884c\u4e3a\u7279\u5f81\u6765\u552f\u4e00\u8bc6\u522b\u7535\u52a8\u8f66\uff0c\u5047\u8bbe\u65e9\u671f\u7535\u538b\u884c\u4e3a\u4e0e\u540e\u671f\u7535\u6d41\u884c\u4e3a\u5177\u6709\u76f8\u4f3c\u7279\u6027\u3002\u4ece\u65e9\u671f\u7535\u538b\u6d4b\u91cf\u4e2d\u63d0\u53d6\u7279\u5f81\u4ee5\u5b9e\u73b0EV\u914d\u7f6e\u6587\u4ef6\u7684\u6784\u5efa\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u572849\u8f86EV\u76847408\u6b21\u53ef\u7528\u5145\u7535\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u51c6\u786e\u7387\u53ef\u8fbe0.86\u3002\u4ec5\u970010\u4e2a\u5173\u952e\u7279\u5f81\u5373\u53ef\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u65b0\u578b\u8eab\u4efd\u9a8c\u8bc1\u56e0\u7d20\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u672a\u7ecf\u6388\u6743\u8bbf\u95ee\u5145\u7535\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u7684\u9690\u79c1\u98ce\u9669\u3002"}}
{"id": "2506.07047", "pdf": "https://arxiv.org/pdf/2506.07047", "abs": "https://arxiv.org/abs/2506.07047", "authors": ["Yu Xuejun", "Jianyuan Zhong", "Zijin Feng", "Pengyi Zhai", "Roozbeh Yousefzadeh", "Wei Chong Ng", "Haoxiong Liu", "Ziyi Shou", "Jing Xiong", "Yudong Zhou", "Claudia Beth Ong", "Austen Jeremy Sugiarto", "Yaoxi Zhang", "Wai Ming Tai", "Huan Cao", "Dongcai Lu", "Jiacheng Sun", "Qiang Xu", "Shen Xin", "Zhenguo Li"], "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.", "AI": {"tldr": "Recent advances in large language models have shown great potential for formal reasoning. However, most LLM-based theorem provers require expert-written formal statements as inputs, which limits their applicability to real-world problems expressed in natural language. This paper introduces Mathesis, the first end-to-end theorem proving pipeline that processes informal problem statements. It includes Mathesis-Autoformalizer, which uses reinforcement learning to enhance the formalization ability of natural language problems, and Mathesis-Prover, which generates formal proofs from the formalized statements. Experiments show that Mathesis outperforms other model combinations, achieving 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.", "motivation": "The motivation of this paper is to bridge the gap between formal theorem proving and real-world problems expressed in natural language by developing an end-to-end theorem proving pipeline that can process informal problem statements.", "method": "The method involves creating Mathesis, which consists of two main components: Mathesis-Autoformalizer and Mathesis-Prover. The autoformalizer uses reinforcement learning and the LeanScorer framework to convert natural language problems into formal statements. The prover then generates formal proofs from these statements. A new benchmark, Gaokao-Formal, is also introduced to evaluate the system's performance.", "result": "Experiments demonstrate that Mathesis's autoformalizer surpasses the best baseline by 22% in pass-rate on Gaokao-Formal. The full system achieves 64% accuracy on MiniF2F with pass@32 and a state-of-the-art 18% on Gaokao-Formal.", "conclusion": "Mathesis represents a significant advancement in end-to-end formal theorem proving, effectively processing informal problem statements and achieving state-of-the-art results on relevant benchmarks."}}
{"id": "2506.06532", "pdf": "https://arxiv.org/pdf/2506.06532", "abs": "https://arxiv.org/abs/2506.06532", "authors": ["Zijiang Yan", "Hao Zhou", "Jianhua Pei", "Hina Tabassum"], "title": "Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.LG", "cs.AI", "cs.NI", "cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted in ICML 2025 Workshop on Machine Learning for Wireless\n  Communication and Networks (ML4Wireless)", "summary": "Unmanned aerial vehicles (UAVs) have been widely adopted in various\nreal-world applications. However, the control and optimization of multi-UAV\nsystems remain a significant challenge, particularly in dynamic and constrained\nenvironments. This work explores the joint motion and communication control of\nmultiple UAVs operating within integrated terrestrial and non-terrestrial\nnetworks that include high-altitude platform stations (HAPS). Specifically, we\nconsider an aerial highway scenario in which UAVs must accelerate, decelerate,\nand change lanes to avoid collisions and maintain overall traffic flow.\nDifferent from existing studies, we propose a novel hierarchical and\ncollaborative method based on large language models (LLMs). In our approach, an\nLLM deployed on the HAPS performs UAV access control, while another LLM onboard\neach UAV handles motion planning and control. This LLM-based framework\nleverages the rich knowledge embedded in pre-trained models to enable both\nhigh-level strategic planning and low-level tactical decisions. This\nknowledge-driven paradigm holds great potential for the development of\nnext-generation 3D aerial highway systems. Experimental results demonstrate\nthat our proposed collaborative LLM-based method achieves higher system\nrewards, lower operational costs, and significantly reduced UAV collision rates\ncompared to baseline approaches.", "AI": {"tldr": "The paper proposes a novel hierarchical and collaborative method based on large language models (LLMs) for the joint motion and communication control of multiple UAVs in integrated terrestrial and non-terrestrial networks that include high-altitude platform stations (HAPS). This LLM-based framework leverages the rich knowledge embedded in pre-trained models to enable both high-level strategic planning and low-level tactical decisions.", "motivation": "Current studies face challenges in controlling and optimizing multi-UAV systems, especially in dynamic and constrained environments. The need for efficient motion and communication control in aerial highway scenarios prompts the exploration of new methods.", "method": "A hierarchical and collaborative method using LLMs is proposed. An LLM on HAPS performs UAV access control, while another LLM onboard each UAV handles motion planning and control. This approach integrates high-level strategic planning and low-level tactical decisions.", "result": "Experimental results show that the proposed method achieves higher system rewards, lower operational costs, and significantly reduced UAV collision rates compared to baseline approaches.", "conclusion": "The LLM-based framework demonstrates great potential for the development of next-generation 3D aerial highway systems, providing an effective solution for UAV control and optimization."}}
{"id": "2506.07728", "pdf": "https://arxiv.org/pdf/2506.07728", "abs": "https://arxiv.org/abs/2506.07728", "authors": ["Rajdeep Ghosh", "Shiladitya De", "Mainack Mondal"], "title": "\"I wasn't sure if this is indeed a security risk\": Data-driven Understanding of Security Issue Reporting in GitHub Repositories of Open Source npm Packages", "categories": ["cs.CR"], "comment": "This extended version of our USENIX Security '25 paper on Security\n  issue reporting in NPM packages includes appendices for interested readers", "summary": "The npm (Node Package Manager) ecosystem is the most important package\nmanager for JavaScript development with millions of users. Consequently, a\nplethora of earlier work investigated how vulnerability reporting, patch\npropagation, and in general detection as well as resolution of security issues\nin such ecosystems can be facilitated. However, understanding the ground\nreality of security-related issue reporting by users (and bots) in npm-along\nwith the associated challenges has been relatively less explored at scale.\n  In this work, we bridge this gap by collecting 10,907,467 issues reported\nacross GitHub repositories of 45,466 diverse npm packages. We found that the\ntags associated with these issues indicate the existence of only 0.13%\nsecurity-related issues. However, our approach of manual analysis followed by\ndeveloping high accuracy machine learning models identify 1,617,738\nsecurity-related issues which are not tagged as security-related (14.8% of all\nissues) as well as 4,461,934 comments made on these issues. We found that the\nbots which are in wide use today might not be sufficient for either detecting\nor offering assistance. Furthermore, our analysis of user-developer interaction\ndata hints that many user-reported security issues might not be addressed by\ndevelopers-they are not tagged as security-related issues and might be closed\nwithout valid justification. Consequently, a correlation analysis hints that\nthe developers quickly handle security issues with known solutions (e.g.,\ncorresponding to CVE). However, security issues without such known solutions\n(even with reproducible code) might not be resolved. Our findings offer\nactionable insights for improving security management in open-source\necosystems, highlighting the need for smarter tools and better collaboration.\nThe data and code for this work is available at\nhttps://doi.org/10.5281/zenodo.15614029", "AI": {"tldr": "The paper investigates security-related issue reporting in the npm ecosystem by analyzing a large dataset of GitHub issues from npm packages. It finds that only 0.13% of issues are tagged as security-related, but through manual analysis and machine learning models, identifies 14.8% of all issues as security-related. Bots currently in use may not be adequate for detecting or assisting with these issues, and many user-reported security issues may go unaddressed by developers. The authors suggest improvements in security management in open-source ecosystems through smarter tools and better collaboration.", "motivation": "To understand the reality of security-related issue reporting in the npm ecosystem and the challenges associated with it, which has been less explored at scale.", "method": "Collecting and analyzing 10,907,467 issues reported across GitHub repositories of 45,466 diverse npm packages. Using manual analysis followed by developing high accuracy machine learning models to identify security-related issues and comments. Analyzing user-developer interaction data and performing correlation analysis.", "result": "Identified 1,617,738 security-related issues (14.8% of all issues) and 4,461,934 comments on these issues that were not originally tagged as security-related. Found that bots may not be sufficient for detection or assistance. Discovered that many user-reported security issues might not be addressed by developers and those without known solutions might remain unresolved.", "conclusion": "The findings provide actionable insights for improving security management in open-source ecosystems, emphasizing the need for smarter tools and better collaboration."}}
{"id": "2506.07075", "pdf": "https://arxiv.org/pdf/2506.07075", "abs": "https://arxiv.org/abs/2506.07075", "authors": ["Liwen Zheng", "Chaozhuo Li", "Haoran Jia", "Xi Zhang"], "title": "Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression", "categories": ["cs.AI"], "comment": null, "summary": "The growing complexity of factual claims in real-world scenarios presents\nsignificant challenges for automated fact verification systems, particularly in\naccurately aggregating and reasoning over multi-hop evidence. Existing\napproaches often rely on static or shallow models that fail to capture the\nevolving structure of reasoning paths, leading to fragmented retrieval and\nlimited interpretability. To address these issues, we propose a Structural\nReasoning framework for Multi-hop Fact Verification that explicitly models\nreasoning paths as structured graphs throughout both evidence retrieval and\nclaim verification stages. Our method comprises two key modules: a\nstructure-enhanced retrieval mechanism that constructs reasoning graphs to\nguide evidence collection, and a reasoning-path-guided verification module that\nincrementally builds subgraphs to represent evolving inference trajectories. We\nfurther incorporate a structure-aware reasoning mechanism that captures\nlong-range dependencies across multi-hop evidence chains, enabling more precise\nverification. Extensive experiments on the FEVER and HoVer datasets demonstrate\nthat our approach consistently outperforms strong baselines, highlighting the\neffectiveness of reasoning-path modeling in enhancing retrieval precision and\nverification accuracy.", "AI": {"tldr": "A structural reasoning framework for multi-hop fact verification is proposed, which models reasoning paths as structured graphs and comprises two key modules: structure-enhanced retrieval mechanism and reasoning-path-guided verification module. Experiments show it outperforms strong baselines.", "motivation": "Existing automated fact verification systems have challenges in accurately aggregating and reasoning over multi-hop evidence due to reliance on static or shallow models that fail to capture evolving structure of reasoning paths.", "method": "The method explicitly models reasoning paths as structured graphs throughout both evidence retrieval and claim verification stages. It includes a structure-enhanced retrieval mechanism and a reasoning-path-guided verification module, with a structure-aware reasoning mechanism incorporated to capture long-range dependencies across multi-hop evidence chains.", "result": "Extensive experiments on the FEVER and HoVer datasets demonstrate that the approach consistently outperforms strong baselines, enhancing retrieval precision and verification accuracy.", "conclusion": "The structural reasoning framework effectively models reasoning paths as structured graphs, leading to more precise verification through improved retrieval and reasoning mechanisms."}}
{"id": "2506.06549", "pdf": "https://arxiv.org/pdf/2506.06549", "abs": "https://arxiv.org/abs/2506.06549", "authors": ["Atefeh Gilani", "Naima Tasnim", "Lalitha Sankar", "Oliver Kosut"], "title": "GeoClip: Geometry-Aware Clipping for Differentially Private SGD", "categories": ["cs.LG", "cs.CR", "cs.IT", "math.IT"], "comment": null, "summary": "Differentially private stochastic gradient descent (DP-SGD) is the most\nwidely used method for training machine learning models with provable privacy\nguarantees. A key challenge in DP-SGD is setting the per-sample gradient\nclipping threshold, which significantly affects the trade-off between privacy\nand utility. While recent adaptive methods improve performance by adjusting\nthis threshold during training, they operate in the standard coordinate system\nand fail to account for correlations across the coordinates of the gradient. We\npropose GeoClip, a geometry-aware framework that clips and perturbs gradients\nin a transformed basis aligned with the geometry of the gradient distribution.\nGeoClip adaptively estimates this transformation using only previously released\nnoisy gradients, incurring no additional privacy cost. We provide convergence\nguarantees for GeoClip and derive a closed-form solution for the optimal\ntransformation that minimizes the amount of noise added while keeping the\nprobability of gradient clipping under control. Experiments on both tabular and\nimage datasets demonstrate that GeoClip consistently outperforms existing\nadaptive clipping methods under the same privacy budget.", "AI": {"tldr": "GeoClip is a geometry-aware framework that clips and perturbs gradients in a transformed basis aligned with the gradient distribution's geometry, adaptively estimating this transformation using only previously released noisy gradients without additional privacy cost. It outperforms existing adaptive clipping methods under the same privacy budget.", "motivation": "DP-SGD is widely used for training machine learning models with provable privacy guarantees, but setting the per-sample gradient clipping threshold significantly affects the privacy-utility trade-off. Current adaptive methods fail to account for correlations across the coordinates of the gradient.", "method": "Propose GeoClip which clips and perturbs gradients in a transformed basis aligned with the geometry of the gradient distribution. Adaptively estimate this transformation using only previously released noisy gradients without incurring additional privacy cost.", "result": "GeoClip consistently outperforms existing adaptive clipping methods on both tabular and image datasets under the same privacy budget.", "conclusion": "GeoClip provides convergence guarantees and derives a closed-form solution for the optimal transformation that minimizes noise added while controlling the probability of gradient clipping."}}
{"id": "2506.07827", "pdf": "https://arxiv.org/pdf/2506.07827", "abs": "https://arxiv.org/abs/2506.07827", "authors": ["Enrique Soriano-Salvador", "Gorka Guardiola M\u00fazquiz", "Juan Gonz\u00e1lez G\u00f3mez"], "title": "User-space library rootkits revisited: Are user-space detection mechanisms futile?", "categories": ["cs.CR", "68M25", "D.4.6"], "comment": null, "summary": "The kind of malware designed to conceal malicious system resources (e.g.\nprocesses, network connections, files, etc.) is commonly referred to as a\nrootkit. This kind of malware represents a significant threat in contemporany\nsystems. Despite the existence of kernel-space rootkits (i.e. rootkits that\ninfect the operating system kernel), user-space rootkits (i.e. rootkits that\ninfect the user-space operating system tools, commands and libraries) continue\nto pose a significant danger. However, kernel-space rootkits attract all the\nattention, implicitly assuming that user-space rootkits (malware that is still\nin existence) are easily detectable by well-known user-space tools that look\nfor anomalies. The primary objective of this work is to answer the following\nquestion: Is detecting user-space rootkits with user-space tools futile?\nContrary to the prevailing view that considers it effective, we argue that the\ndetection of user-space rootkits cannot be done in user-space at all. Moreover,\nthe detection results must be communicated to the user with extreme caution. To\nsupport this claim, we conducted different experiments focusing on process\nconcealing in Linux systems. In these experiments, we evade the detection\nmechanisms widely accepted as the standard solution for this type of user-space\nmalware, bypassing the most popular open source anti-rootkit tool for process\nhiding. This manuscript describes the classical approach to build user-space\nlibrary rootkits, the traditional detection mechanisms, and different evasion\ntechniques (it also includes understandable code snippets and examples). In\naddition, it offers some guidelines to implement new detection tools and\nimprove the existing ones to the extent possible.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u7528\u6237\u7a7a\u95f4\u5de5\u5177\u68c0\u6d4b\u7528\u6237\u7a7a\u95f4rootkit\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u5b9e\u9a8c\u8868\u660e\u4f20\u7edf\u68c0\u6d4b\u673a\u5236\u53ef\u4ee5\u88ab\u7ed5\u8fc7\uff0c\u5e76\u63d0\u4f9b\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u5185\u6838\u7a7a\u95f4rootkit\u53d7\u5230\u66f4\u591a\u5173\u6ce8\uff0c\u4f46\u7528\u6237\u7a7a\u95f4rootkit\u4ecd\u7136\u5b58\u5728\u4e14\u88ab\u8ba4\u4e3a\u5bb9\u6613\u88ab\u68c0\u6d4b\u3002\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u4f7f\u7528\u7528\u6237\u7a7a\u95f4\u5de5\u5177\u68c0\u6d4b\u7528\u6237\u7a7a\u95f4rootkit\u662f\u5426\u6709\u6548\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u548c\u5b9e\u65bd\u9488\u5bf9Linux\u7cfb\u7edf\u7684\u5b9e\u9a8c\uff0c\u7279\u522b\u662f\u9690\u85cf\u8fdb\u7a0b\u7684\u573a\u666f\u4e0b\uff0c\u7814\u7a76\u5982\u4f55\u89c4\u907f\u5e7f\u6cdb\u63a5\u53d7\u7684\u7528\u6237\u7a7a\u95f4rootkit\u68c0\u6d4b\u673a\u5236\uff0c\u5e76\u5206\u6790\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\uff08\u5982\u5f00\u6e90\u53cdrootkit\u5de5\u5177\uff09\u7684\u4e0d\u8db3\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4f20\u7edf\u7684\u7528\u6237\u7a7a\u95f4rootkit\u68c0\u6d4b\u65b9\u6cd5\u53ef\u4ee5\u88ab\u7ed5\u8fc7\uff0c\u4f7f\u7528\u7528\u6237\u7a7a\u95f4\u5de5\u5177\u68c0\u6d4b\u7528\u6237\u7a7a\u95f4rootkit\u5e76\u4e0d\u5b8c\u5168\u53ef\u9760\u3002", "conclusion": "\u68c0\u6d4b\u7528\u6237\u7a7a\u95f4rootkit\u4e0d\u80fd\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u7528\u6237\u7a7a\u95f4\u5de5\u5177\uff0c\u9700\u8981\u8c28\u614e\u5bf9\u5f85\u68c0\u6d4b\u7ed3\u679c\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u73b0\u6709\u5de5\u5177\u7684\u5efa\u8bae\u3002"}}
{"id": "2506.07116", "pdf": "https://arxiv.org/pdf/2506.07116", "abs": "https://arxiv.org/abs/2506.07116", "authors": ["Liyang Chen", "Yujun Cai", "Jieqiong Dong", "Yiwei Wang"], "title": "BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite", "categories": ["cs.AI"], "comment": "8 pages, 7 figures, 4 tables. Submitted to EMNLP 2025", "summary": "Retrieval-Augmented Generation (RAG) systems require corpora that are both\nstructurally clean and semantically coherent. BRIGHT is a recent and\ninfluential benchmark designed to evaluate complex multi-hop retrieval across\ndiverse, high-reasoning domains. However, its practical effectiveness is\nlimited by common web-crawled artifacts - such as content redundancy and\nsemantic discontinuity - that impair retrieval accuracy and downstream\nreasoning. Notably, we find that such issues are concentrated in seven\nStackExchange-derived subdomains, while other domains (e.g., Coding and\nTheorem-based content) remain relatively clean.\n  In this study, we present MARCUS, a multi-agent pipeline that leverages large\nlanguage models (LLMs) to systematically clean and re-chunk BRIGHT into a\nhigher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for\nstructural noise removal and semantic segmentation, preserving answer-bearing\nspans while improving contextual integrity. Experimental evaluations\ndemonstrate that BRIGHT-Plus yields consistent and significant improvements in\nboth retrieval accuracy and multi-hop reasoning across a diverse set of\nretrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to\nsupport future research on robust, reasoning-centric retrieval.", "AI": {"tldr": "MARCUS is a pipeline using LLMs to clean and re-chunk BRIGHT into BRIGHT-Plus, enhancing retrieval accuracy and multi-hop reasoning.", "motivation": "To improve the practical effectiveness of BRIGHT by addressing content redundancy and semantic discontinuity issues that limit retrieval accuracy and downstream reasoning.", "method": "MARCUS leverages LLMs through dedicated agents for structural noise removal and semantic segmentation to create BRIGHT-Plus, a higher-quality corpus.", "result": "BRIGHT-Plus shows consistent and significant improvements in retrieval accuracy and multi-hop reasoning across various retrievers.", "conclusion": "The authors release BRIGHT-Plus and MARCUS to support future research on robust, reasoning-centric retrieval."}}
{"id": "2506.06556", "pdf": "https://arxiv.org/pdf/2506.06556", "abs": "https://arxiv.org/abs/2506.06556", "authors": ["Long Dang", "Thushari Hapuarachchi", "Kaiqi Xiong", "Yi Li"], "title": "SDN-Based False Data Detection With Its Mitigation and Machine Learning Robustness for In-Vehicle Networks", "categories": ["cs.LG", "cs.CR"], "comment": "The 34th International Conference on Computer Communications and\n  Networks (ICCCN 2025)", "summary": "As the development of autonomous and connected vehicles advances, the\ncomplexity of modern vehicles increases, with numerous Electronic Control Units\n(ECUs) integrated into the system. In an in-vehicle network, these ECUs\ncommunicate with one another using an standard protocol called Controller Area\nNetwork (CAN). Securing communication among ECUs plays a vital role in\nmaintaining the safety and security of the vehicle. This paper proposes a\nrobust SDN-based False Data Detection and Mitigation System (FDDMS) for\nin-vehicle networks. Leveraging the unique capabilities of Software-Defined\nNetworking (SDN), FDDMS is designed to monitor and detect false data injection\nattacks in real-time. Specifically, we focus on brake-related ECUs within an\nSDN-enabled in-vehicle network. First, we decode raw CAN data to create an\nattack model that illustrates how false data can be injected into the system.\nThen, FDDMS, incorporating a Long Short Term Memory (LSTM)-based detection\nmodel, is used to identify false data injection attacks. We further propose an\neffective variant of DeepFool attack to evaluate the model's robustness. To\ncountermeasure the impacts of four adversarial attacks including Fast gradient\ndescent method, Basic iterative method, DeepFool, and the DeepFool variant, we\nfurther enhance a re-training technique method with a threshold based selection\nstrategy. Finally, a mitigation scheme is implemented to redirect attack\ntraffic by dynamically updating flow rules through SDN. Our experimental\nresults show that the proposed FDDMS is robust against adversarial attacks and\neffectively detects and mitigates false data injection attacks in real-time.", "AI": {"tldr": "The paper proposes a robust SDN-based False Data Detection and Mitigation System (FDDMS) for in-vehicle networks, which effectively detects and mitigates false data injection attacks in real-time while resisting adversarial attacks.", "motivation": "With the increasing complexity of modern vehicles due to autonomous and connected features, securing communication among numerous ECUs within the in-vehicle network is crucial for vehicle safety and security.", "method": "The FDDMS system leverages SDN capabilities to monitor communications. It decodes raw CAN data to create an attack model, uses an LSTM-based detection model to identify false data injection attacks, evaluates robustness with a DeepFool variant, enhances re-training techniques with threshold-based selection, and implements a mitigation scheme through dynamic flow rule updates via SDN.", "result": "Experimental results demonstrate that FDDMS is robust against various adversarial attacks and can effectively detect and mitigate false data injection attacks in real-time.", "conclusion": "The proposed FDDMS provides a secure solution for in-vehicle networks by successfully detecting and mitigating false data injection attacks while maintaining resilience against adversarial attacks."}}
{"id": "2506.07836", "pdf": "https://arxiv.org/pdf/2506.07836", "abs": "https://arxiv.org/abs/2506.07836", "authors": ["Silvia Lucia Sanna", "Diego Soi", "Davide Maiorca", "Giorgio Giacinto"], "title": "Are Trees Really Green? A Detection Approach of IoT Malware Attacks", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": null, "summary": "Nowadays, the Internet of Things (IoT) is widely employed, and its usage is\ngrowing exponentially because it facilitates remote monitoring, predictive\nmaintenance, and data-driven decision making, especially in the healthcare and\nindustrial sectors. However, IoT devices remain vulnerable due to their\nresource constraints and difficulty in applying security patches. Consequently,\nvarious cybersecurity attacks are reported daily, such as Denial of Service,\nparticularly in IoT-driven solutions. Most attack detection methodologies are\nbased on Machine Learning (ML) techniques, which can detect attack patterns.\nHowever, the focus is more on identification rather than considering the impact\nof ML algorithms on computational resources. This paper proposes a green\nmethodology to identify IoT malware networking attacks based on flow\nprivacy-preserving statistical features. In particular, the hyperparameters of\nthree tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are\noptimized based on energy consumption and test-time performance in terms of\nMatthew's Correlation Coefficient. Our results show that models maintain high\nperformance and detection accuracy while consistently reducing power usage in\nterms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion\nDetection Systems are suitable for IoT and other resource-constrained devices.", "AI": {"tldr": "The paper proposes an energy-efficient methodology to detect IoT malware networking attacks using optimized tree-based ML models.", "motivation": "IoT devices are vulnerable to cybersecurity attacks due to resource constraints and difficulty in applying security patches. Existing ML-based attack detection methods focus on identification without considering the impact on computational resources.", "method": "Optimizing hyperparameters of Decision Trees, Random Forest, and Extra-Trees models based on energy consumption and test-time performance (Matthew's Correlation Coefficient) using flow privacy-preserving statistical features.", "result": "The models maintain high performance and detection accuracy while reducing power usage in terms of watt-hours (Wh).", "conclusion": "On-premise ML-based Intrusion Detection Systems are suitable for IoT and other resource-constrained devices."}}
{"id": "2506.07173", "pdf": "https://arxiv.org/pdf/2506.07173", "abs": "https://arxiv.org/abs/2506.07173", "authors": ["Miroslav Popovic", "Marko Popovic", "Miodrag Djukic", "Ilija Basicevic"], "title": "Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT", "categories": ["cs.AI"], "comment": "6 pages, 4 tables", "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework that is easy to use by ML&AI developers who do not need to be\nprofessional programmers and is also amenable to LLMs. In the previous\nresearch, generic federated learning algorithms provided by this framework were\nmanually translated into the CSP processes and algorithms' safety and liveness\nproperties were automatically verified by the model checker PAT. In this paper,\na simple translation process is introduced wherein the ChatGPT is used to\nautomate the translation of the mentioned federated learning algorithms in\nPython into the corresponding CSP processes. Within the process, the minimality\nof the used context is estimated based on the feedback from ChatGPT. The\nproposed translation process was experimentally validated by successful\ntranslation (verified by the model checker PAT) of both generic centralized and\ndecentralized federated learning algorithms.", "AI": {"tldr": "This paper introduces a simple translation process using ChatGPT to automate the translation of federated learning algorithms in Python into CSP processes, estimating the minimality of the used context based on feedback from ChatGPT. The proposed translation process was experimentally validated.", "motivation": "To automate the translation of federated learning algorithms in Python into CSP processes and estimate the minimality of the used context based on feedback from ChatGPT.", "method": "Using ChatGPT to automate the translation process and experimentally validate the proposed translation process.", "result": "Successful translation (verified by the model checker PAT) of both generic centralized and decentralized federated learning algorithms.", "conclusion": "The proposed translation process using ChatGPT for automating the translation of federated learning algorithms in Python into CSP processes is effective."}}
{"id": "2506.06558", "pdf": "https://arxiv.org/pdf/2506.06558", "abs": "https://arxiv.org/abs/2506.06558", "authors": ["Atamert Rahma", "Chinmay Datar", "Ana Cukarska", "Felix Dietrich"], "title": "Rapid training of Hamiltonian graph networks without gradient descent", "categories": ["cs.LG", "cs.NE", "68T07", "I.2.6"], "comment": "10 pages, 7 figures, 2 tables, and an appendix", "summary": "Learning dynamical systems that respect physical symmetries and constraints\nremains a fundamental challenge in data-driven modeling. Integrating physical\nlaws with graph neural networks facilitates principled modeling of complex\nN-body dynamics and yields accurate and permutation-invariant models. However,\ntraining graph neural networks with iterative, gradient-based optimization\nalgorithms (e.g., Adam, RMSProp, LBFGS) often leads to slow training,\nespecially for large, complex systems. In comparison to 15 different\noptimizers, we demonstrate that Hamiltonian Graph Networks (HGN) can be trained\nup to 600x faster--but with comparable accuracy--by replacing iterative\noptimization with random feature-based parameter construction. We show robust\nperformance in diverse simulations, including N-body mass-spring systems in up\nto 3 dimensions with different geometries, while retaining essential physical\ninvariances with respect to permutation, rotation, and translation. We reveal\nthat even when trained on minimal 8-node systems, the model can generalize in a\nzero-shot manner to systems as large as 4096 nodes without retraining. Our work\nchallenges the dominance of iterative gradient-descent-based optimization\nalgorithms for training neural network models for physical systems.", "AI": {"tldr": "The paper shows that Hamiltonian Graph Networks (HGN) can be trained up to 600x faster by replacing iterative optimization with random feature-based parameter construction, while retaining physical invariances and generalizing well.", "motivation": "Learning dynamical systems respecting physical symmetries and constraints is a fundamental challenge. Current methods using graph neural networks have limitations in training speed, particularly for large and complex systems.", "method": "The method involves using Hamiltonian Graph Networks (HGN) and replacing iterative, gradient-based optimization algorithms with random feature-based parameter construction to accelerate training.", "result": "This approach allows HGNs to be trained up to 600x faster compared to other optimizers, with comparable accuracy. The model also generalizes well from small 8-node systems to much larger 4096-node systems without retraining, while maintaining essential physical invariances.", "conclusion": "This work challenges the dominance of iterative gradient-descent-based optimization algorithms for training neural network models for physical systems, showing a significant improvement in training speed and generalization."}}
{"id": "2506.07868", "pdf": "https://arxiv.org/pdf/2506.07868", "abs": "https://arxiv.org/abs/2506.07868", "authors": ["Zachary Ratliff", "Salil Vadhan"], "title": "Securing Unbounded Differential Privacy Against Timing Attacks", "categories": ["cs.CR"], "comment": null, "summary": "Recent works have started to theoretically investigate how we can protect\ndifferentially private programs against timing attacks, by making the joint\ndistribution the output and the runtime differentially private (JOT-DP).\nHowever, the existing approaches to JOT-DP have some limitations, particularly\nin the setting of unbounded DP (which protects the size of the dataset and\napplies to arbitrarily large datasets). First, the known conversion of pure DP\nprograms to pure JOT-DP programs in the unbounded setting (a) incurs a constant\nadditive increase in error probability (and thus does not provide vanishing\nerror as $n\\to\\infty$) (b) produces JOT-DP programs that fail to preserve the\ncomputational efficiency of the original pure DP program and (c) is analyzed in\na toy computational model in which the runtime is defined to be the number of\ncoin flips. In this work, we overcome these limitations. Specifically, we show\nthat the error required for pure JOT-DP in the unbounded setting depends on the\nmodel of computation. In a randomized RAM model where the dataset size $n$ is\ngiven (or can be computed in constant time) and we can generate random numbers\n(not just random bits) in constant time, polynomially small error probability\nis necessary and sufficient. If $n$ is not given or we only have a random-bit\ngenerator, an (arbitrarily small) constant error probability is necessary and\nsufficient. The aforementioned positive results are proven by efficient\nprocedures to convert any pure JOT-DP program $P$ in the upper-bounded setting\nto a pure JOT-DP program $P'$ in the unbounded setting, such that the output\ndistribution of $P'$ is $\\gamma$-close in total variation distance to that of\n$P$, where $\\gamma$ is either an arbitrarily small constant or polynomially\nsmall, depending on the model of computation.", "AI": {"tldr": "\u901a\u8fc7\u6709\u6548\u7684\u7a0b\u5e8f\u8f6c\u6362\u65b9\u6cd5\uff0c\u5728\u65e0\u754c\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u7eafJOT-DP\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u5e76\u51cf\u5c11\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u7684\u5c06\u7eaf\u5dee\u5206\u9690\u79c1\uff08pure DP\uff09\u7a0b\u5e8f\u8f6c\u6362\u4e3a\u8054\u5408\u8f93\u51fa\u548c\u8fd0\u884c\u65f6\u95f4\u5dee\u5206\u9690\u79c1\uff08JOT-DP\uff09\u7a0b\u5e8f\u7684\u65b9\u6cd5\u5728\u65e0\u754c\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e2d\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\uff0c\u5305\u62ec\u8bef\u5dee\u4e0d\u968f\u6570\u636e\u91cf\u51cf\u5c11\u3001\u8ba1\u7b97\u6548\u7387\u4e0b\u964d\u4ee5\u53ca\u5206\u6790\u6a21\u578b\u8fc7\u4e8e\u7b80\u5355\u7b49\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u5728\u4e0d\u540c\u7684\u8ba1\u7b97\u6a21\u578b\u4e0b\uff0c\u5c06\u4efb\u610f\u7eafJOT-DP\u7a0b\u5e8f\u4ece\u6709\u754c\u8bbe\u7f6e\u8f6c\u6362\u5230\u65e0\u754c\u8bbe\u7f6e\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u786e\u4fdd\u8f93\u51fa\u5206\u5e03\u4e0e\u539f\u59cb\u7a0b\u5e8f\u63a5\u8fd1\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6839\u636e\u8ba1\u7b97\u6a21\u578b\u7684\u4e0d\u540c\uff08\u5982\u968f\u673aRAM\u6a21\u578b\u6216\u4ec5\u6709\u968f\u673a\u6bd4\u7279\u751f\u6210\u5668\uff09\uff0c\u53ef\u4ee5\u5b9e\u73b0\u591a\u9879\u5f0f\u7ea7\u5c0f\u7684\u8bef\u5dee\u6982\u7387\u6216\u4efb\u610f\u5c0f\u7684\u5e38\u6570\u8bef\u5dee\u6982\u7387\u3002", "result": "\u8bc1\u660e\u4e86\u5728\u65e0\u754c\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e2d\u5b9e\u73b0\u7eafJOT-DP\u6240\u9700\u7684\u8bef\u5dee\u53d6\u51b3\u4e8e\u8ba1\u7b97\u6a21\u578b\uff1b\u5728\u968f\u673aRAM\u6a21\u578b\u4e2d\uff0c\u591a\u9879\u5f0f\u7ea7\u5c0f\u7684\u8bef\u5dee\u6982\u7387\u662f\u5fc5\u8981\u4e14\u5145\u5206\u7684\uff1b\u800c\u5728\u5176\u4ed6\u6a21\u578b\u4e2d\uff0c\u4efb\u610f\u5c0f\u7684\u5e38\u6570\u8bef\u5dee\u6982\u7387\u662f\u5fc5\u8981\u4e14\u5145\u5206\u7684\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u7a0b\u5e8f\u8f6c\u6362\u65b9\u6cd5\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u6761\u4ef6\u3002", "conclusion": "\u672c\u6587\u514b\u670d\u4e86\u73b0\u6709JOT-DP\u65b9\u6cd5\u5728\u65e0\u754c\u5dee\u5206\u9690\u79c1\u8bbe\u7f6e\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5c55\u793a\u4e86\u8bef\u5dee\u9700\u6c42\u4e0e\u8ba1\u7b97\u6a21\u578b\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u7a0b\u5e8f\u8f6c\u6362\u65b9\u6cd5\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u3002"}}
{"id": "2506.07184", "pdf": "https://arxiv.org/pdf/2506.07184", "abs": "https://arxiv.org/abs/2506.07184", "authors": ["Liangliang You", "Junchi Yao", "Shu Yang", "Guimin Hu", "Lijie Hu", "Di Wang"], "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy.", "AI": {"tldr": "Multimodal large language models have hallucination problems that affect their reliability. This paper focuses on behavioral hallucinations in sequential images, identifies causes, proposes the SHE framework to detect and mitigate these issues, introduces a new metric BEACH, and shows effectiveness through benchmarks.", "motivation": "Behavioral hallucinations in sequential images are under-studied compared to objective hallucinations, posing challenges for multimodal large language models.", "method": "SHE is a two-stage framework: (1) Detect hallucinations using visual-textual alignment check with an adaptive temporal window; (2) Mitigate hallucinations via orthogonal projection onto the joint embedding space. A new metric BEACH is also proposed.", "result": "SHE reduces behavioral hallucination by over 10% on BEACH while maintaining descriptive accuracy according to empirical results on standard benchmarks.", "conclusion": "The SHE framework effectively addresses behavioral hallucinations in sequential images and the BEACH metric provides a way to quantify this type of hallucination."}}
{"id": "2506.06571", "pdf": "https://arxiv.org/pdf/2506.06571", "abs": "https://arxiv.org/abs/2506.06571", "authors": ["Mattie Ji", "Amauri H. Souza", "Vikas Garg"], "title": "Graph Persistence goes Spectral", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "24 pages, 4 figures, 6 tables", "summary": "Including intricate topological information (e.g., cycles) provably enhances\nthe expressivity of message-passing graph neural networks (GNNs) beyond the\nWeisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods\nare increasingly employed for graph representation learning. In this context,\nrecent works have proposed decorating classical PH diagrams with vertex and\nedge features for improved expressivity. However, due to their dependence on\nfeatures, these methods still fail to capture basic graph structural\ninformation. In this paper, we propose SpectRe -- a new topological descriptor\nfor graphs that integrates spectral information into PH diagrams. Notably,\nSpectRe is strictly more expressive than existing descriptors on graphs. We\nalso introduce notions of global and local stability to analyze existing\ndescriptors and establish that SpectRe is locally stable. Finally, experiments\non synthetic and real-world datasets demonstrate the effectiveness of SpectRe\nand its potential to enhance the capabilities of graph models in relevant\nlearning tasks.", "AI": {"tldr": "SpectRe\uff0c\u4e00\u79cd\u65b0\u7684\u56fe\u62d3\u6251\u63cf\u8ff0\u7b26\uff0c\u901a\u8fc7\u5c06\u8c31\u4fe1\u606f\u878d\u5165\u6301\u4e45\u540c\u8c03\u56fe\u4e2d\uff0c\u589e\u5f3a\u4e86\u56fe\u8868\u793a\u5b66\u4e60\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5c06\u9876\u70b9\u548c\u8fb9\u7279\u5f81\u52a0\u5165\u7ecf\u5178PH\u56fe\u4ee5\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u7684\u65b9\u6cd5\uff0c\u4ecd\u65e0\u6cd5\u6355\u6349\u57fa\u672c\u7684\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u62d3\u6251\u63cf\u8ff0\u7b26SpectRe\uff0c\u5b83\u5c06\u8c31\u4fe1\u606f\u6574\u5408\u5230PH\u56fe\u4e2d\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u5168\u5c40\u548c\u5c40\u90e8\u7a33\u5b9a\u6027\u6982\u5ff5\u6765\u5206\u6790\u73b0\u6709\u63cf\u8ff0\u7b26\uff0c\u5e76\u8bc1\u660eSpectRe\u5177\u6709\u5c40\u90e8\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSpectRe\u7684\u6709\u6548\u6027\u53ca\u5176\u589e\u5f3a\u56fe\u6a21\u578b\u5728\u76f8\u5173\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u7684\u6f5c\u529b\u3002", "conclusion": "SpectRe\u6bd4\u73b0\u6709\u7684\u56fe\u63cf\u8ff0\u7b26\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u4e14\u5177\u5907\u5c40\u90e8\u7a33\u5b9a\u6027\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u56fe\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2506.07882", "pdf": "https://arxiv.org/pdf/2506.07882", "abs": "https://arxiv.org/abs/2506.07882", "authors": ["Rajesh Kalakoti", "Risto Vaarandi", "Hayretdin Bahsi", "Sven N\u00f5mm"], "title": "Evaluating explainable AI for deep learning-based network intrusion detection system alert classification", "categories": ["cs.CR"], "comment": "Accepted version of a paper published in the Proceedings of the 11th\n  International Conference on Information Systems Security and Privacy (ICISSP\n  2025). Final version available via SCITEPRESS", "summary": "A Network Intrusion Detection System (NIDS) monitors networks for cyber\nattacks and other unwanted activities. However, NIDS solutions often generate\nan overwhelming number of alerts daily, making it challenging for analysts to\nprioritize high-priority threats. While deep learning models promise to\nautomate the prioritization of NIDS alerts, the lack of transparency in these\nmodels can undermine trust in their decision-making. This study highlights the\ncritical need for explainable artificial intelligence (XAI) in NIDS alert\nclassification to improve trust and interpretability. We employed a real-world\nNIDS alert dataset from Security Operations Center (SOC) of TalTech (Tallinn\nUniversity Of Technology) in Estonia, developing a Long Short-Term Memory\n(LSTM) model to prioritize alerts. To explain the LSTM model's alert\nprioritization decisions, we implemented and compared four XAI methods: Local\nInterpretable Model-Agnostic Explanations (LIME), SHapley Additive exPlanations\n(SHAP), Integrated Gradients, and DeepLIFT. The quality of these XAI methods\nwas assessed using a comprehensive framework that evaluated faithfulness,\ncomplexity, robustness, and reliability. Our results demonstrate that DeepLIFT\nconsistently outperformed the other XAI methods, providing explanations with\nhigh faithfulness, low complexity, robust performance, and strong reliability.\nIn collaboration with SOC analysts, we identified key features essential for\neffective alert classification. The strong alignment between these\nanalyst-identified features and those obtained by the XAI methods validates\ntheir effectiveness and enhances the practical applicability of our approach.", "AI": {"tldr": "This paper addresses the challenge of prioritizing network intrusion alerts using an LSTM model and evaluates four XAI methods for explaining the model's decisions, finding DeepLIFT to be the most effective.", "motivation": "Network Intrusion Detection Systems (NIDS) generate a large number of daily alerts, making it difficult for analysts to prioritize high-priority threats. Existing deep learning models lack transparency, undermining trust in their decision-making.", "method": "A real-world NIDS alert dataset from TalTech's Security Operations Center was used to develop an LSTM model for alert prioritization. Four XAI methods (LIME, SHAP, Integrated Gradients, and DeepLIFT) were implemented and compared using a framework evaluating faithfulness, complexity, robustness, and reliability.", "result": "DeepLIFT outperformed the other XAI methods in terms of faithfulness, complexity, robustness, and reliability. The features identified by SOC analysts aligned strongly with those obtained by the XAI methods.", "conclusion": "The study highlights the importance of XAI in improving trust and interpretability in NIDS alert classification. DeepLIFT is found to be the most effective XAI method, enhancing the practical applicability of the approach."}}
{"id": "2506.07194", "pdf": "https://arxiv.org/pdf/2506.07194", "abs": "https://arxiv.org/abs/2506.07194", "authors": ["Luwei Bai", "Dongkeun Han", "Sara Hennessy"], "title": "Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues", "categories": ["cs.AI"], "comment": "Draft technical report. 39 pages, 2 figures. Not yet submitted for\n  publication. Update expected", "summary": "This study investigates effective strategies for developing a customised GPT\nagent to code classroom dialogue. While classroom dialogue is widely recognised\nas a crucial element of education, its analysis remains challenging due to the\nneed for a nuanced understanding of dialogic functions and the labour-intensive\nnature of manual transcript coding. Recent advancements in large language\nmodels offer promising avenues for automating this process. However, existing\nstudies predominantly focus on training large-scale models or evaluating\npre-trained models with fixed codebooks, which are often not applicable or\nreplicable for dialogue researchers working with small datasets or customised\ncoding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its\nbaseline performance in coding classroom dialogue with a human codebook and\nexamines how performance varies with different example inputs through a\nvariable control method. Through a design-based research approach, it\nidentifies a set of practical strategies, based on MyGPT's unique features, for\nconfiguring effective agents with limited data. The findings suggest that,\ndespite some limitations, a MyGPT agent developed with these strategies can\nserve as a useful coding assistant by generating coding suggestions.", "AI": {"tldr": "This study explores strategies to customize a GPT agent for coding classroom dialogue using MyGPT as a case, finding that with specific strategies it can be a useful coding assistant.", "motivation": "Classroom dialogue analysis is challenging due to the complex dialogic functions and manual labor required. Existing studies on large language models either focus on large-scale training or evaluating pre-trained models with fixed codebooks which are not suitable for small datasets or custom coding schemes.", "method": "Evaluate MyGPT's baseline performance in coding classroom dialogue with a human codebook and examine how different example inputs affect performance through variable control. A design-based research approach identifies practical strategies based on MyGPT's features for configuring effective agents with limited data.", "result": "The study found that a MyGPT agent developed with these strategies can generate coding suggestions despite some limitations.", "conclusion": "A customized MyGPT agent can serve as a useful coding assistant for classroom dialogue by generating coding suggestions."}}
{"id": "2506.06579", "pdf": "https://arxiv.org/pdf/2506.06579", "abs": "https://arxiv.org/abs/2506.06579", "authors": ["Adarsh Prasad Behera", "Jaya Prakash Champati", "Roberto Morabito", "Sasu Tarkoma", "James Gross"], "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Recent progress in Language Models (LMs) has dramatically advanced the field\nof natural language processing (NLP), excelling at tasks like text generation,\nsummarization, and question answering. However, their inference remains\ncomputationally expensive and energy intensive, especially in settings with\nlimited hardware, power, or bandwidth. This makes it difficult to deploy LMs in\nmobile, edge, or cost sensitive environments. To address these challenges,\nrecent approaches have introduced multi LLM intelligent model selection\nstrategies that dynamically allocate computational resources based on query\ncomplexity -- using lightweight models for simpler queries and escalating to\nlarger models only when necessary. This survey explores two complementary\nstrategies for efficient LLM inference: (i) routing, which selects the most\nsuitable model based on the query, and (ii) cascading or hierarchical inference\n(HI), which escalates queries through a sequence of models until a confident\nresponse is found. Both approaches aim to reduce computation by using\nlightweight models for simpler tasks while offloading only when needed. We\nprovide a comparative analysis of these techniques across key performance\nmetrics, discuss benchmarking efforts, and outline open challenges. Finally, we\noutline future research directions to enable faster response times, adaptive\nmodel selection based on task complexity, and scalable deployment across\nheterogeneous environments, making LLM based systems more efficient and\naccessible for real world applications.", "AI": {"tldr": "\u5c3d\u7ba1\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u4ecd\u7136\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u80fd\u8017\u5927\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u4e24\u79cd\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\u7684\u7b56\u7565\uff1a\u8def\u7531\u548c\u7ea7\u8054/\u5206\u5c42\u63a8\u7406\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u7684\u8fdb\u6b65\uff0c\u867d\u7136\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u8f83\u5927\uff0c\u96be\u4ee5\u5728\u79fb\u52a8\u8bbe\u5907\u6216\u6210\u672c\u654f\u611f\u73af\u5883\u4e2d\u90e8\u7f72\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e24\u79cd\u7b56\u7565\uff1a(i) \u8def\u7531\uff0c\u6839\u636e\u67e5\u8be2\u9009\u62e9\u6700\u5408\u9002\u7684\u6a21\u578b\uff1b(ii) \u7ea7\u8054\u6216\u5206\u5c42\u63a8\u7406\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6a21\u578b\u9010\u6b65\u5904\u7406\u67e5\u8be2\u76f4\u5230\u5f97\u51fa\u53ef\u9760\u54cd\u5e94\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u8ba1\u7b97\u9700\u6c42\uff0c\u901a\u8fc7\u4f7f\u7528\u8f7b\u91cf\u7ea7\u6a21\u578b\u5904\u7406\u7b80\u5355\u4efb\u52a1\u5e76\u5728\u5fc5\u8981\u65f6\u5347\u7ea7\u5230\u66f4\u590d\u6742\u7684\u6a21\u578b\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5c06\u81f4\u529b\u4e8e\u5b9e\u73b0\u66f4\u5feb\u7684\u54cd\u5e94\u65f6\u95f4\u3001\u57fa\u4e8e\u4efb\u52a1\u590d\u6742\u6027\u7684\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u4ee5\u53ca\u8de8\u5f02\u6784\u73af\u5883\u7684\u53ef\u6269\u5c55\u90e8\u7f72\uff0c\u4ee5\u4f7f\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u66f4\u9ad8\u6548\u548c\u5b9e\u7528\u3002"}}
{"id": "2506.07888", "pdf": "https://arxiv.org/pdf/2506.07888", "abs": "https://arxiv.org/abs/2506.07888", "authors": ["Rui Wen", "Yiyong Liu", "Michael Backes", "Yang Zhang"], "title": "SoK: Data Reconstruction Attacks Against Machine Learning Models: Definition, Metrics, and Benchmark", "categories": ["cs.CR", "cs.LG"], "comment": "To Appear in the 34th USENIX Security Symposium, August 13-15, 2025", "summary": "Data reconstruction attacks, which aim to recover the training dataset of a\ntarget model with limited access, have gained increasing attention in recent\nyears. However, there is currently no consensus on a formal definition of data\nreconstruction attacks or appropriate evaluation metrics for measuring their\nquality. This lack of rigorous definitions and universal metrics has hindered\nfurther advancement in this field. In this paper, we address this issue in the\nvision domain by proposing a unified attack taxonomy and formal definitions of\ndata reconstruction attacks. We first propose a set of quantitative evaluation\nmetrics that consider important criteria such as quantifiability, consistency,\nprecision, and diversity. Additionally, we leverage large language models\n(LLMs) as a substitute for human judgment, enabling visual evaluation with an\nemphasis on high-quality reconstructions. Using our proposed taxonomy and\nmetrics, we present a unified framework for systematically evaluating the\nstrengths and limitations of existing attacks and establishing a benchmark for\nfuture research. Empirical results, primarily from a memorization perspective,\nnot only validate the effectiveness of our metrics but also offer valuable\ninsights for designing new attacks.", "AI": {"tldr": "This paper proposes a unified taxonomy and formal definitions for data reconstruction attacks in the vision domain, introduces new evaluation metrics, uses LLMs for visual assessment, establishes a benchmark for future research, and provides insights for designing new attacks.", "motivation": "The increasing attention on data reconstruction attacks lacks a consensus on formal definition and appropriate evaluation metrics, hindering further advancement.", "method": "Propose a unified attack taxonomy and formal definitions, a set of quantitative evaluation metrics, leverage LLMs for visual evaluation, and present a unified framework for evaluating existing attacks.", "result": "Empirical results validate the effectiveness of the proposed metrics and offer valuable insights for designing new attacks from a memorization perspective.", "conclusion": "The proposed taxonomy, definitions, and metrics provide a systematic way to evaluate data reconstruction attacks and establish a benchmark for future research."}}
{"id": "2506.07202", "pdf": "https://arxiv.org/pdf/2506.07202", "abs": "https://arxiv.org/abs/2506.07202", "authors": ["Ming Liu", "Wensheng Zhang"], "title": "Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show impressive vision-language\nbenchmark performance, yet growing concerns about data contamination (test set\nexposure during training) risk masking true generalization. This concern\nextends to reasoning MLLMs, often fine-tuned via reinforcement learning from\npotentially contaminated base models. We propose a novel dynamic evaluation\nframework to rigorously assess MLLM generalization, moving beyond static\nbenchmarks. Instead of perturbing inputs, we perturb the task itself. Using the\nsame visual input, models are evaluated across a family of tasks (e.g., QA,\ncaptioning, question posing, verification) to probe diverse capabilities. This\ntask perturbation reveals whether model performance is robust or reliant on\nsuperficial task-specific cues. Our approach is analogous to loss landscape\nsharpness: models overfit or contaminated for a single task (sharp minima)\nfalter under task shifts, unlike models with generalizable solutions (flatter\nminima). We developed an automated pipeline with a calibrated judge scoring\nopen-ended generations (captions, questions) using paraphrase and corruption\nsampling. Applying this framework to leading image/video MLLMs on benchmarks\nincluding MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task\n\"ability vector.\" We demonstrate that fine-tuning on simulated test data\n(extreme contamination) drastically sharpens task-specific performance but\nharms overall generalization. Our dynamic task perturbation offers deeper\ninsights into MLLM generalization, distinguishing genuine understanding from\nspurious leakage or overfitting.", "AI": {"tldr": "Multimodal Large Language Models (MLLMs) show great performance but risk data contamination. This paper proposes a dynamic evaluation framework to assess MLLM generalization by perturbing tasks rather than inputs, revealing whether model performance is robust or reliant on task-specific cues.", "motivation": "To rigorously evaluate the generalization capabilities of MLLMs and distinguish genuine understanding from spurious leakage or overfitting.", "method": "Propose a novel dynamic evaluation framework that perturbs tasks instead of inputs. Evaluate models across a family of tasks using the same visual input to probe diverse capabilities.", "result": "Fine-tuning on simulated test data harms overall generalization, while models with generalizable solutions perform better under task shifts.", "conclusion": "Dynamic task perturbation offers deeper insights into MLLM generalization."}}
{"id": "2506.06582", "pdf": "https://arxiv.org/pdf/2506.06582", "abs": "https://arxiv.org/abs/2506.06582", "authors": ["Diaaeldin Taha", "James Chapman", "Marzieh Eidi", "Karel Devriendt", "Guido Mont\u00fafar"], "title": "Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing", "categories": ["cs.LG", "stat.ML", "I.5.1"], "comment": "50 pages, 12 figures, published at ICLR 2025. The Thirteenth\n  International Conference on Learning Representations. 2025", "summary": "Topological deep learning (TDL) has emerged as a powerful tool for modeling\nhigher-order interactions in relational data. However, phenomena such as\noversquashing in topological message-passing remain understudied and lack\ntheoretical analysis. We propose a unifying axiomatic framework that bridges\ngraph and topological message-passing by viewing simplicial and cellular\ncomplexes and their message-passing schemes through the lens of relational\nstructures. This approach extends graph-theoretic results and algorithms to\nhigher-order structures, facilitating the analysis and mitigation of\noversquashing in topological message-passing networks. Through theoretical\nanalysis and empirical studies on simplicial networks, we demonstrate the\npotential of this framework to advance TDL.", "AI": {"tldr": "Topological deep learning (TDL) is a powerful tool, but phenomena like oversquashing in topological message-passing need more study. This paper proposes a unifying axiomatic framework to bridge graph and topological message-passing by viewing complexes through relational structures, extending graph-theoretic results to higher-order structures to help analyze and mitigate oversquashing.", "motivation": "The motivation of this paper lies in the understudied phenomena such as oversquashing in topological message-passing within Topological Deep Learning (TDL). The authors aim to address these gaps by proposing a new framework that can better model higher-order interactions in relational data.", "method": "The method involves creating a unifying axiomatic framework that bridges graph and topological message-passing by interpreting simplicial and cellular complexes through relational structures. This allows for the extension of graph-theoretic results and algorithms to higher-order structures.", "result": "Through theoretical analysis and empirical studies on simplicial networks, the proposed framework shows potential in advancing TDL by facilitating the analysis and mitigation of oversquashing in topological message-passing networks.", "conclusion": "This paper concludes that the unifying axiomatic framework offers a promising direction for advancing Topological Deep Learning, particularly in addressing issues like oversquashing in topological message-passing."}}
{"id": "2506.07894", "pdf": "https://arxiv.org/pdf/2506.07894", "abs": "https://arxiv.org/abs/2506.07894", "authors": ["Muhammad Ali Najjar", "Ren-Yi Huang", "Dumindu Samaraweera", "Prashant Shekhar"], "title": "Secure Distributed Learning for CAVs: Defending Against Gradient Leakage with Leveled Homomorphic Encryption", "categories": ["cs.CR"], "comment": null, "summary": "Federated Learning (FL) enables collaborative model training across\ndistributed clients without sharing raw data, making it a promising approach\nfor privacy-preserving machine learning in domains like Connected and\nAutonomous Vehicles (CAVs). However, recent studies have shown that exchanged\nmodel gradients remain susceptible to inference attacks such as Deep Leakage\nfrom Gradients (DLG), which can reconstruct private training data. While\nexisting defenses like Differential Privacy (DP) and Secure Multi-Party\nComputation (SMPC) offer protection, they often compromise model accuracy. To\nthat end, Homomorphic Encryption (HE) offers a promising alternative by\nenabling lossless computation directly on encrypted data, thereby preserving\nboth privacy and model utility. However, HE introduces significant\ncomputational and communication overhead, which can hinder its practical\nadoption. To address this, we systematically evaluate various leveled HE\nschemes to identify the most suitable for FL in resource-constrained\nenvironments due to its ability to support fixed-depth computations without\nrequiring costly bootstrapping. Our contributions in this paper include a\ncomprehensive evaluation of HE schemes for real-world FL applications, a\nselective encryption strategy that targets only the most sensitive gradients to\nminimize computational overhead, and the development of a full HE-based FL\npipeline that effectively mitigates DLG attacks while preserving model\naccuracy. We open-source our implementation to encourage reproducibility and\nfacilitate adoption in safety-critical domains.", "AI": {"tldr": "Federated Learning (FL) is crucial for privacy-preserving machine learning, but model gradients can be vulnerable to inference attacks. Existing defenses often reduce model accuracy. Homomorphic Encryption (HE) offers a solution that preserves both privacy and accuracy but introduces overhead. This paper evaluates HE schemes for FL, proposes a selective encryption strategy, and develops a full HE-based FL pipeline to mitigate inference attacks while maintaining model performance.", "motivation": "To address the vulnerability of model gradients in Federated Learning to inference attacks like Deep Leakage from Gradients (DLG), while avoiding the compromise of model accuracy that existing defenses such as Differential Privacy and Secure Multi-Party Computation may introduce.", "method": "Systematically evaluate various leveled Homomorphic Encryption schemes suitable for FL in resource-constrained environments. Propose a selective encryption strategy targeting only the most sensitive gradients to minimize computational overhead. Develop a full HE-based FL pipeline.", "result": "Identification of the most suitable HE schemes for FL in constrained environments. Successful mitigation of DLG attacks while preserving model accuracy. Open-sourced implementation for reproducibility and adoption.", "conclusion": "Homomorphic Encryption provides an effective alternative for privacy-preserving FL without compromising model utility. The proposed selective encryption strategy and HE-based FL pipeline offer practical solutions to balance privacy and efficiency."}}
{"id": "2506.07217", "pdf": "https://arxiv.org/pdf/2506.07217", "abs": "https://arxiv.org/abs/2506.07217", "authors": ["Zihan Deng", "Changyu Du", "Stavros Nousias", "Andr\u00e9 Borrmann"], "title": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents", "categories": ["cs.AI"], "comment": "ICML 2025 Workshop on Computer Use Agents", "summary": "Existing computer-use agents primarily focus on general-purpose desktop\nautomation tasks, with limited exploration of their application in highly\nspecialized domains. In particular, the 3D building modeling process in the\nArchitecture, Engineering, and Construction (AEC) sector involves open-ended\ndesign tasks and complex interaction patterns within Building Information\nModeling (BIM) authoring software, which has yet to be thoroughly addressed by\ncurrent studies. In this paper, we propose BIMgent, an agentic framework\npowered by multimodal large language models (LLMs), designed to enable\nautonomous building model authoring via graphical user interface (GUI)\noperations. BIMgent automates the architectural building modeling process,\nincluding multimodal input for conceptual design, planning of software-specific\nworkflows, and efficient execution of the authoring GUI actions. We evaluate\nBIMgent on real-world building modeling tasks, including both text-based\nconceptual design generation and reconstruction from existing building design.\nThe design quality achieved by BIMgent was found to be reasonable. Its\noperations achieved a 32% success rate, whereas all baseline models failed to\ncomplete the tasks (0% success rate). Results demonstrate that BIMgent\neffectively reduces manual workload while preserving design intent,\nhighlighting its potential for practical deployment in real-world architectural\nmodeling scenarios.", "AI": {"tldr": "The paper introduces BIMgent, an agentic framework powered by multimodal LLMs for automating 3D building modeling in AEC sector. Evaluated on real-world tasks, it achieves a 32% success rate and reasonable design quality.", "motivation": "Current computer-use agents are mainly focused on general-purpose desktop automation tasks and have not sufficiently explored specialized domains like 3D building modeling in the AEC sector.", "method": "BIMgent is an agentic framework that uses multimodal large language models to enable autonomous building model authoring via GUI operations. It includes multimodal input for conceptual design, workflow planning, and efficient execution of GUI actions.", "result": "BIMgent achieved a 32% success rate on real-world building modeling tasks, outperforming baseline models which had a 0% success rate. The design quality was found to be reasonable.", "conclusion": "BIMgent effectively reduces manual workload while preserving design intent, showing potential for practical deployment in real-world architectural modeling scenarios."}}
{"id": "2506.06584", "pdf": "https://arxiv.org/pdf/2506.06584", "abs": "https://arxiv.org/abs/2506.06584", "authors": ["Mo Zhou", "Weihang Xu", "Maryam Fazel", "Simon S. Du"], "title": "Global Convergence of Gradient EM for Over-Parameterized Gaussian Mixtures", "categories": ["cs.LG", "stat.ML"], "comment": "77 pages", "summary": "Learning Gaussian Mixture Models (GMMs) is a fundamental problem in machine\nlearning, with the Expectation-Maximization (EM) algorithm and its popular\nvariant gradient EM being arguably the most widely used algorithms in practice.\nIn the exact-parameterized setting, where both the ground truth GMM and the\nlearning model have the same number of components $m$, a vast line of work has\naimed to establish rigorous recovery guarantees for EM. However, global\nconvergence has only been proven for the case of $m=2$, and EM is known to fail\nto recover the ground truth when $m\\geq 3$.\n  In this paper, we consider the $\\textit{over-parameterized}$ setting, where\nthe learning model uses $n>m$ components to fit an $m$-component ground truth\nGMM. In contrast to the exact-parameterized case, we provide a rigorous global\nconvergence guarantee for gradient EM. Specifically, for any well separated\nGMMs in general position, we prove that with only mild over-parameterization $n\n= \\Omega(m\\log m)$, randomly initialized gradient EM converges globally to the\nground truth at a polynomial rate with polynomial samples. Our analysis\nproceeds in two stages and introduces a suite of novel tools for Gaussian\nMixture analysis. We use Hermite polynomials to study the dynamics of gradient\nEM and employ tensor decomposition to characterize the geometric landscape of\nthe likelihood loss. This is the first global convergence and recovery result\nfor EM or Gradient EM beyond the special case of $m=2$.", "AI": {"tldr": "\u5728\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\uff0c\u5bf9\u4e8e\u5206\u79bb\u826f\u597d\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u68af\u5ea6EM\u7b97\u6cd5\u80fd\u591f\u5168\u5c40\u6536\u655b\u5230\u771f\u5b9e\u6a21\u578b\u3002\u8fd9\u662f\u9996\u4e2a\u8d85\u8d8a\u7279\u6b8a\u60c5\u5f62$m=2$\u7684EM\u6216\u68af\u5ea6EM\u7684\u5168\u5c40\u6536\u655b\u4e0e\u6062\u590d\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u7cbe\u786e\u53c2\u6570\u5316\u8bbe\u7f6e\u4e0b\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5b66\u4e60\u4e2d\uff0c\u5c3d\u7ba1\u5bf9EM\u7b97\u6cd5\u8fdb\u884c\u4e86\u5927\u91cf\u7814\u7a76\uff0c\u4f46\u4ec5\u8bc1\u660e\u4e86$m=2$\u65f6\u7684\u5168\u5c40\u6536\u655b\u6027\uff0c\u800c$m\\geq 3$\u65f6EM\u7b97\u6cd5\u65e0\u6cd5\u6062\u590d\u771f\u5b9e\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u63a2\u7d22\u5176\u4ed6\u8bbe\u7f6e\u4e0b\u7684\u6536\u655b\u6027\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u8003\u8651\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\uff0c\u4f7f\u7528$n>m$\u4e2a\u5206\u91cf\u7684\u6a21\u578b\u62df\u5408$m$-\u5206\u91cf\u7684\u771f\u5b9eGMM\u3002\u901a\u8fc7\u5f15\u5165Hermite\u591a\u9879\u5f0f\u7814\u7a76\u68af\u5ea6EM\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5e76\u5229\u7528\u5f20\u91cf\u5206\u89e3\u523b\u753b\u4f3c\u7136\u635f\u5931\u7684\u51e0\u4f55\u666f\u89c2\uff0c\u4ece\u800c\u4e3a\u68af\u5ea6EM\u63d0\u4f9b\u4e25\u683c\u7684\u5168\u5c40\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5bf9\u4e8e\u4efb\u610f\u5206\u79bb\u826f\u597d\u7684\u4e00\u822c\u4f4d\u7f6eGMM\uff0c\u53ea\u9700\u8f7b\u5fae\u8fc7\u53c2\u6570\u5316\uff08$n = \\Omega(m\\log m)$\uff09\uff0c\u968f\u673a\u521d\u59cb\u5316\u7684\u68af\u5ea6EM\u7b97\u6cd5\u80fd\u591f\u4ee5\u591a\u9879\u5f0f\u901f\u7387\u5168\u5c40\u6536\u655b\u5230\u771f\u5b9e\u6a21\u578b\uff0c\u5e76\u4e14\u4ec5\u9700\u591a\u9879\u5f0f\u6837\u672c\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9$m>2$\u7684EM\u6216\u68af\u5ea6EM\u7b97\u6cd5\u7684\u5168\u5c40\u6536\u655b\u4e0e\u6062\u590d\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u8fc7\u53c2\u6570\u5316\u8bbe\u7f6e\u7684\u4f18\u52bf\u548c\u6f5c\u529b\u3002"}}
{"id": "2506.07974", "pdf": "https://arxiv.org/pdf/2506.07974", "abs": "https://arxiv.org/abs/2506.07974", "authors": ["Chetan Pathade", "Shweta Hooli"], "title": "Exposing Hidden Backdoors in NFT Smart Contracts: A Static Security Analysis of Rug Pull Patterns", "categories": ["cs.CR"], "comment": "10 Pages, 4 Figures", "summary": "The explosive growth of Non-Fungible Tokens (NFTs) has revolutionized digital\nownership by enabling the creation, exchange, and monetization of unique assets\non blockchain networks. However, this surge in popularity has also given rise\nto a disturbing trend: the emergence of rug pulls - fraudulent schemes where\ndevelopers exploit trust and smart contract privileges to drain user funds or\ninvalidate asset ownership. Central to many of these scams are hidden backdoors\nembedded within NFT smart contracts. Unlike unintentional bugs, these backdoors\nare deliberately coded and often obfuscated to bypass traditional audits and\nexploit investor confidence. In this paper, we present a large-scale static\nanalysis of 49,940 verified NFT smart contracts using Slither, a static\nanalysis framework, to uncover latent vulnerabilities commonly linked to rug\npulls. We introduce a custom risk scoring model that classifies contracts into\nhigh, medium, or low risk tiers based on the presence and severity of rug pull\nindicators. Our dataset was derived from verified contracts on the Ethereum\nmainnet, and we generate multiple visualizations to highlight red flag\nclusters, issue prevalence, and co-occurrence of critical vulnerabilities.\nWhile we do not perform live exploits, our results reveal how malicious\npatterns often missed by simple reviews can be surfaced through static analysis\nat scale. We conclude by offering mitigation strategies for developers,\nmarketplaces, and auditors to enhance smart contract security. By exposing how\nhidden backdoors manifest in real-world smart contracts, this work contributes\na practical foundation for detecting and mitigating NFT rug pulls through\nscalable automated analysis.", "AI": {"tldr": "This paper conducts a large-scale static analysis of NFT smart contracts to uncover vulnerabilities linked to rug pulls, introduces a risk scoring model for contract classification, and provides mitigation strategies.", "motivation": "The rise in popularity of NFTs has led to an increase in fraudulent activities such as rug pulls, which exploit trust and smart contract privileges. The motivation is to address the security issues posed by deliberately coded backdoors within NFT smart contracts.", "method": "Using Slither, a static analysis framework, the authors analyzed 49,940 verified NFT smart contracts from the Ethereum mainnet. They developed a custom risk scoring model to classify contracts based on the severity of rug pull indicators and created visualizations to highlight vulnerability patterns.", "result": "The analysis uncovered latent vulnerabilities commonly associated with rug pulls, identified clusters of red flags, and demonstrated how malicious patterns can be detected through large-scale static analysis without live exploits.", "conclusion": "The work contributes a practical foundation for detecting and mitigating NFT rug pulls through scalable automated analysis, offering strategies to enhance smart contract security for developers, marketplaces, and auditors."}}
{"id": "2506.07223", "pdf": "https://arxiv.org/pdf/2506.07223", "abs": "https://arxiv.org/abs/2506.07223", "authors": ["Yangqing Zheng", "Shunqi Mao", "Dingxin Zhang", "Weidong Cai"], "title": "LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments", "categories": ["cs.AI"], "comment": "Accepted by the CVPR 2025 Embodied AI Workshop", "summary": "In the realm of embodied intelligence, the evolution of large language models\n(LLMs) has markedly enhanced agent decision making. Consequently, researchers\nhave begun exploring agent performance in dynamically changing high-risk\nscenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under\nthese extreme conditions, the delay in decision making emerges as a crucial yet\ninsufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that\ntranslates inference delays in decision-making into equivalent simulation\nframes, thus aligning cognitive and physical costs under a single FPS-based\nmetric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action\nRatio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we\npresent the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a\nlightweight LLM-guided feedback module with a rule-based agent to enable\nimmediate reactive behaviors and asynchronous reflective refinements in situ.\nExperiments on HAZARD show that RRARA substantially outperforms existing\nbaselines in latency-sensitive scenarios.", "AI": {"tldr": "This paper proposes a Time Conversion Mechanism (TCM) and a new agent called RRARA to improve decision-making in high-risk scenarios.", "motivation": "To address the insufficiently studied issue of decision-making delays in dynamically changing high-risk scenarios.", "method": "Propose TCM to translate inference delays into simulation frames, extend HAZARD with RL and LAR, and present RRARA which combines an LLM-guided feedback module with a rule-based agent.", "result": "Experiments on HAZARD show that RRARA substantially outperforms existing baselines in latency-sensitive scenarios.", "conclusion": "The proposed mechanisms and agent improve decision-making performance in high-risk scenarios."}}
{"id": "2506.06599", "pdf": "https://arxiv.org/pdf/2506.06599", "abs": "https://arxiv.org/abs/2506.06599", "authors": ["Yuanjie Shi", "Hooman Shahrokhi", "Xuesong Jia", "Xiongzhi Chen", "Janardhan Rao Doppa", "Yan Yan"], "title": "Direct Prediction Set Minimization via Bilevel Conformal Classifier Training", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted for Publication at International Conference on Machine\n  Learning (ICML), 2025", "summary": "Conformal prediction (CP) is a promising uncertainty quantification framework\nwhich works as a wrapper around a black-box classifier to construct prediction\nsets (i.e., subset of candidate classes) with provable guarantees. However,\nstandard calibration methods for CP tend to produce large prediction sets which\nmakes them less useful in practice. This paper considers the problem of\nintegrating conformal principles into the training process of deep classifiers\nto directly minimize the size of prediction sets. We formulate conformal\ntraining as a bilevel optimization problem and propose the {\\em Direct\nPrediction Set Minimization (DPSM)} algorithm to solve it. The key insight\nbehind DPSM is to minimize a measure of the prediction set size (upper level)\nthat is conditioned on the learned quantile of conformity scores (lower level).\nWe analyze that DPSM has a learning bound of $O(1/\\sqrt{n})$ (with $n$ training\nsamples), while prior conformal training methods based on stochastic\napproximation for the quantile has a bound of $\\Omega(1/s)$ (with batch size\n$s$ and typically $s \\ll \\sqrt{n}$). Experiments on various benchmark datasets\nand deep models show that DPSM significantly outperforms the best prior\nconformal training baseline with $20.46\\%\\downarrow$ in the prediction set size\nand validates our theory.", "AI": {"tldr": "The paper proposes DPSM algorithm to minimize prediction set sizes during deep classifier training within the conformal prediction framework, showing significant improvement over previous methods.", "motivation": "Conformal prediction (CP) is valuable for uncertainty quantification but often produces large prediction sets. The authors aim to integrate conformal principles into classifier training to reduce these set sizes.", "method": "The method involves formulating conformal training as a bilevel optimization problem and introducing the DPSM algorithm, which minimizes prediction set size by conditioning on learned quantiles of conformity scores.", "result": "DPSM demonstrates a learning bound of O(1/\u221an), superior to prior methods with bounds of \u03a9(1/s). Experiments show a 20.46% decrease in prediction set size compared to the best previous baseline.", "conclusion": "DPSM effectively minimizes prediction set sizes in conformal prediction, outperforming existing techniques and validating the theoretical analysis."}}
{"id": "2506.07988", "pdf": "https://arxiv.org/pdf/2506.07988", "abs": "https://arxiv.org/abs/2506.07988", "authors": ["S M Mostaq Hossain", "Amani Altarawneh"], "title": "Unraveling Ethereum's Mempool: The Impact of Fee Fairness, Transaction Prioritization, and Consensus Efficiency", "categories": ["cs.CR"], "comment": "7 pages, 6 figures and 1 table", "summary": "Ethereum's transaction pool (mempool) dynamics and fee market efficiency\ncritically affect transaction inclusion, validator workload, and overall\nnetwork performance. This research empirically analyzes gas price variations,\nmempool clearance rates, and block finalization times in Ethereum's\nproof-of-stake ecosystem using real-time data from Geth and Prysm nodes. We\nobserve that high-fee transactions are consistently prioritized, while low-fee\ntransactions face delays or exclusion despite EIP-1559's intended improvements.\nMempool congestion remains a key factor in validator efficiency and proposal\nlatency. We provide empirical evidence of persistent fee-based disparities and\nshow that extremely high fees do not always guarantee faster confirmation,\nrevealing inefficiencies in the current fee market. To address these issues, we\npropose congestion-aware fee adjustments, reserved block slots for low-fee\ntransactions, and improved handling of out-of-gas vulnerabilities. By\nmitigating prioritization bias and execution inefficiencies, our findings\nsupport more equitable transaction inclusion, enhance validator performance,\nand promote scalability. This work contributes to Ethereum's long-term\ndecentralization by reducing dependence on high transaction fees for network\nparticipation.", "AI": {"tldr": "The paper explores Ethereum's mempool dynamics and fee market efficiency, revealing disparities in transaction inclusion based on fees despite EIP-1559 improvements. It proposes solutions like congestion-aware fee adjustments and reserved block slots for low-fee transactions to enhance fairness, validator performance, and scalability.", "motivation": "To understand the factors affecting transaction inclusion, validator workload, and network performance in Ethereum's proof-of-stake ecosystem, particularly focusing on gas price variations, mempool clearance rates, and block finalization times.", "method": "Empirical analysis using real-time data from Geth and Prysm nodes to observe transaction prioritization, mempool congestion, and fee market inefficiencies.", "result": "High-fee transactions are prioritized, low-fee transactions face delays, and extremely high fees do not always guarantee faster confirmation. Mempool congestion significantly impacts validator efficiency and proposal latency.", "conclusion": "Proposed solutions such as congestion-aware fee adjustments and reserved block slots for low-fee transactions can lead to more equitable transaction inclusion, better validator performance, and improved scalability, supporting Ethereum's decentralization."}}
{"id": "2506.07255", "pdf": "https://arxiv.org/pdf/2506.07255", "abs": "https://arxiv.org/abs/2506.07255", "authors": ["Jake Tuero", "Michael Buro", "Levi H. S. Lelis"], "title": "Subgoal-Guided Policy Heuristic Search with Learned Subgoals", "categories": ["cs.AI"], "comment": "Accepted to ICML-25", "summary": "Policy tree search is a family of tree search algorithms that use a policy to\nguide the search. These algorithms provide guarantees on the number of\nexpansions required to solve a given problem that are based on the quality of\nthe policy. While these algorithms have shown promising results, the process in\nwhich they are trained requires complete solution trajectories to train the\npolicy. Search trajectories are obtained during a trial-and-error search\nprocess. When the training problem instances are hard, learning can be\nprohibitively costly, especially when starting from a randomly initialized\npolicy. As a result, search samples are wasted in failed attempts to solve\nthese hard instances. This paper introduces a novel method for learning\nsubgoal-based policies for policy tree search algorithms. The subgoals and\npolicies conditioned on subgoals are learned from the trees that the search\nexpands while attempting to solve problems, including the search trees of\nfailed attempts. We empirically show that our policy formulation and training\nmethod improve the sample efficiency of learning a policy and heuristic\nfunction in this online setting.", "AI": {"tldr": "This paper introduces a novel method for learning subgoal-based policies for policy tree search algorithms, which improves the sample efficiency of learning a policy and heuristic function.", "motivation": "The process of training policy tree search requires complete solution trajectories to train the policy, but when the training problem instances are hard, learning can be prohibitively costly, especially when starting from a randomly initialized policy. This leads to search samples being wasted in failed attempts to solve these hard instances.", "method": "A novel method for learning subgoal-based policies for policy tree search algorithms is introduced. The subgoals and policies conditioned on subgoals are learned from the trees that the search expands while attempting to solve problems, including the search trees of failed attempts.", "result": "Empirical results show that this new policy formulation and training method improve the sample efficiency of learning a policy and heuristic function in an online setting.", "conclusion": "Learning subgoal-based policies for policy tree search algorithms using the proposed method enhances the sample efficiency."}}
{"id": "2506.06603", "pdf": "https://arxiv.org/pdf/2506.06603", "abs": "https://arxiv.org/abs/2506.06603", "authors": ["Joseph T Colonel", "Carolyn Hagler", "Guiselle Wismer", "Laura Curtis", "Jacqueline Becker", "Juan Wisnivesky", "Alex Federman", "Gaurav Pandey"], "title": "CAtCh: Cognitive Assessment through Cookie Thief", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Several machine learning algorithms have been developed for the prediction of\nAlzheimer's disease and related dementia (ADRD) from spontaneous speech.\nHowever, none of these algorithms have been translated for the prediction of\nbroader cognitive impairment (CI), which in some cases is a precursor and risk\nfactor of ADRD. In this paper, we evaluated several speech-based open-source\nmethods originally proposed for the prediction of ADRD, as well as methods from\nmultimodal sentiment analysis for the task of predicting CI from patient audio\nrecordings. Results demonstrated that multimodal methods outperformed unimodal\nones for CI prediction, and that acoustics-based approaches performed better\nthan linguistics-based ones. Specifically, interpretable acoustic features\nrelating to affect and prosody were found to significantly outperform\nBERT-based linguistic features and interpretable linguistic features,\nrespectively. All the code developed for this study is available at\nhttps://github.com/JTColonel/catch.", "AI": {"tldr": "The paper evaluates speech-based and multimodal methods originally developed for predicting ADRD, applying them to predict CI from patient audio recordings. Multimodal methods outperform unimodal ones, and acoustics-based approaches surpass linguistics-based ones.", "motivation": "To explore the potential of existing machine learning algorithms designed for predicting ADRD in predicting broader cognitive impairment (CI), which is a precursor and risk factor of ADRD.", "method": "Evaluate several open-source speech-based methods initially proposed for ADRD prediction and methods from multimodal sentiment analysis on the task of predicting CI from patient audio recordings.", "result": "Multimodal methods perform better than unimodal methods for CI prediction. Acoustics-based approaches, particularly interpretable acoustic features relating to affect and prosody, significantly outperform BERT-based linguistic features and interpretable linguistic features.", "conclusion": "Multimodal and acoustics-based approaches are more effective in predicting CI from patient audio recordings compared to unimodal and linguistics-based methods."}}
{"id": "2506.07390", "pdf": "https://arxiv.org/pdf/2506.07390", "abs": "https://arxiv.org/abs/2506.07390", "authors": ["Xin-Cheng Wen", "Yijun Yang", "Cuiyun Gao", "Yang Xiao", "Deheng Ye"], "title": "Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data", "categories": ["cs.AI", "cs.SE"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large language models (LLMs) demonstrate considerable proficiency in numerous\ncoding-related tasks; however, their capabilities in detecting software\nvulnerabilities remain limited. This limitation primarily stems from two\nfactors: (1) the absence of reasoning data related to vulnerabilities, which\nhinders the models' ability to capture underlying vulnerability patterns; and\n(2) their focus on learning semantic representations rather than the reason\nbehind them, thus failing to recognize semantically similar vulnerability\nsamples. Furthermore, the development of LLMs specialized in vulnerability\ndetection is challenging, particularly in environments characterized by the\nscarcity of high-quality datasets. In this paper, we propose a novel framework\nReVD that excels at mining vulnerability patterns through reasoning data\nsynthesizing and vulnerability-specific preference optimization. Specifically,\nwe construct forward and backward reasoning processes for vulnerability and\ncorresponding fixed code, ensuring the synthesis of high-quality reasoning\ndata. Moreover, we design the triplet supervised fine-tuning followed by\ncurriculum online preference optimization for enabling ReVD to better\nunderstand vulnerability patterns. The extensive experiments conducted on\nPrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for\nLLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement\nin the accuracy. The source code and data are available at\nhttps://github.com/Xin-Cheng-Wen/PO4Vul.", "AI": {"tldr": "ReVD is a new framework that improves LLMs' ability in software vulnerability detection by synthesizing reasoning data and optimizing preference, achieving significant accuracy improvements.", "motivation": "Existing large language models have limitations in detecting software vulnerabilities due to the lack of reasoning data related to vulnerabilities and their focus on semantic representations rather than the underlying reasons. Additionally, there's a challenge in developing specialized LLMs for vulnerability detection due to the scarcity of high-quality datasets.", "method": "The framework ReVD constructs forward and backward reasoning processes for vulnerabilities and fixed code to synthesize high-quality reasoning data. It also uses triplet supervised fine-tuning followed by curriculum online preference optimization to enhance understanding of vulnerability patterns.", "result": "Experiments on PrimeVul and SVEN datasets show that ReVD achieves state-of-the-art performance in LLM-based software vulnerability detection with accuracy improvements ranging from 12.24% to 22.77%.", "conclusion": "ReVD demonstrates excellent capability in mining vulnerability patterns through reasoning data synthesizing and preference optimization, setting a new benchmark in LLM-based software vulnerability detection."}}
{"id": "2506.06606", "pdf": "https://arxiv.org/pdf/2506.06606", "abs": "https://arxiv.org/abs/2506.06606", "authors": ["Xinyu Luo", "Cedar Site Bai", "Bolian Li", "Petros Drineas", "Ruqi Zhang", "Brian Bullins"], "title": "Stacey: Promoting Stochastic Steepest Descent via Accelerated $\\ell_p$-Smooth Nonconvex Optimization", "categories": ["cs.LG"], "comment": null, "summary": "While popular optimization methods such as SGD, AdamW, and Lion depend on\nsteepest descent updates in either $\\ell_2$ or $\\ell_\\infty$ norms, there\nremains a critical gap in handling the non-Euclidean structure observed in\nmodern deep networks training. In this work, we address this need by\nintroducing a new accelerated $\\ell_p$ steepest descent algorithm, called\nStacey, which uses interpolated primal-dual iterate sequences to effectively\nnavigate non-Euclidean smooth optimization tasks. In addition to providing\nnovel theoretical guarantees for the foundations of our algorithm, we\nempirically compare our approach against these popular methods on tasks\nincluding image classification and language model (LLM) pretraining,\ndemonstrating both faster convergence and higher final accuracy. We further\nevaluate different values of $p$ across various models and datasets,\nunderscoring the importance and efficiency of non-Euclidean approaches over\nstandard Euclidean methods. Code can be found at\nhttps://github.com/xinyuluo8561/Stacey .", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07411", "pdf": "https://arxiv.org/pdf/2506.07411", "abs": "https://arxiv.org/abs/2506.07411", "authors": ["Ze Yang", "Yihong Jin", "Juntian Liu", "Xinhe Xu"], "title": "An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning", "categories": ["cs.AI"], "comment": "Proceedings of 2025 IEEE 8th International Conference on Advanced\n  Electronic Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "As the scale and complexity of cloud-based AI systems continue to increase,\nthe detection and adaptive recovery of system faults have become the core\nchallenges to ensure service reliability and continuity. In this paper, we\npropose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates\nLarge Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to\nrealize a fault recovery framework with semantic understanding and policy\noptimization capabilities in cloud AI systems. On the basis of the traditional\nDRL-based control model, the proposed method constructs a two-stage hybrid\narchitecture: (1) an LLM-driven fault semantic interpretation module, which can\ndynamically extract deep contextual semantics from multi-source logs and system\nindicators to accurately identify potential fault modes; (2) DRL recovery\nstrategy optimizer, based on reinforcement learning, learns the dynamic\nmatching of fault types and response behaviors in the cloud environment. The\ninnovation of this method lies in the introduction of LLM for environment\nmodeling and action space abstraction, which greatly improves the exploration\nefficiency and generalization ability of reinforcement learning. At the same\ntime, a memory-guided meta-controller is introduced, combined with\nreinforcement learning playback and LLM prompt fine-tuning strategy, to achieve\ncontinuous adaptation to new failure modes and avoid catastrophic forgetting.\nExperimental results on the cloud fault injection platform show that compared\nwith the existing DRL and rule methods, the IFSHM framework shortens the system\nrecovery time by 37% with unknown fault scenarios.", "AI": {"tldr": "\u5728\u4e91AI\u7cfb\u7edf\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u6545\u969c\u81ea\u6108\u673a\u5236\uff08IFSHM\uff09\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u548c\u7b56\u7565\u4f18\u5316\u80fd\u529b\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6062\u590d\u6548\u7387\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u4e91\u8ba1\u7b97\u7684AI\u7cfb\u7edf\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u68c0\u6d4b\u548c\u9002\u5e94\u6027\u6062\u590d\u7cfb\u7edf\u6545\u969c\u6210\u4e3a\u786e\u4fdd\u670d\u52a1\u53ef\u9760\u6027\u548c\u8fde\u7eed\u6027\u7684\u6838\u5fc3\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6df7\u5408\u67b6\u6784\uff1a1) \u7531LLM\u9a71\u52a8\u7684\u6545\u969c\u8bed\u4e49\u89e3\u91ca\u6a21\u5757\uff0c\u53ef\u4ee5\u4ece\u591a\u6e90\u65e5\u5fd7\u548c\u7cfb\u7edf\u6307\u6807\u4e2d\u52a8\u6001\u63d0\u53d6\u6df1\u5c42\u4e0a\u4e0b\u6587\u8bed\u4e49\u4ee5\u51c6\u786e\u8bc6\u522b\u6f5c\u5728\u6545\u969c\u6a21\u5f0f\uff1b2) \u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684DRL\u6062\u590d\u7b56\u7565\u4f18\u5316\u5668\uff0c\u5b66\u4e60\u4e91\u73af\u5883\u4e2d\u6545\u969c\u7c7b\u578b\u4e0e\u54cd\u5e94\u884c\u4e3a\u7684\u52a8\u6001\u5339\u914d\u3002\u540c\u65f6\u5f15\u5165\u4e86\u8bb0\u5fc6\u5f15\u5bfc\u7684\u5143\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u56de\u653e\u548cLLM\u63d0\u793a\u5fae\u8c03\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u65b0\u6545\u969c\u6a21\u5f0f\u7684\u6301\u7eed\u9002\u5e94\u5e76\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u672a\u77e5\u6545\u969c\u573a\u666f\u4e0b\uff0c\u4e0e\u73b0\u6709\u7684DRL\u548c\u89c4\u5219\u65b9\u6cd5\u76f8\u6bd4\uff0cIFSHM\u6846\u67b6\u5c06\u7cfb\u7edf\u6062\u590d\u65f6\u95f4\u7f29\u77ed\u4e8637%\u3002", "conclusion": "IFSHM\u6846\u67b6\u901a\u8fc7\u5f15\u5165LLM\u8fdb\u884c\u73af\u5883\u5efa\u6a21\u548c\u52a8\u4f5c\u7a7a\u95f4\u62bd\u8c61\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u6709\u6548\u7f29\u77ed\u7cfb\u7edf\u6062\u590d\u65f6\u95f4\u3002"}}
{"id": "2506.06632", "pdf": "https://arxiv.org/pdf/2506.06632", "abs": "https://arxiv.org/abs/2506.06632", "authors": ["Shubham Parashar", "Shurui Gui", "Xiner Li", "Hongyi Ling", "Sushil Vemuri", "Blake Olson", "Eric Li", "Yu Zhang", "James Caverlee", "Dileep Kalathil", "Shuiwang Ji"], "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We aim to improve the reasoning capabilities of language models via\nreinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1\nhave demonstrated reasoning abilities on mathematical and coding tasks.\nHowever, prior studies suggest that using RL alone to improve reasoning on\ninherently difficult tasks is less effective. Here, we draw inspiration from\ncurriculum learning and propose to schedule tasks from easy to hard (E2H),\nallowing LLMs to build reasoning skills gradually. Our method is termed E2H\nReasoner. Empirically, we observe that, although easy tasks are important\ninitially, fading them out through appropriate scheduling is essential in\npreventing overfitting. Theoretically, we establish convergence guarantees for\nE2H Reasoner within an approximate policy iteration framework. We derive\nfinite-sample complexity bounds and show that when tasks are appropriately\ndecomposed and conditioned, learning through curriculum stages requires fewer\ntotal samples than direct learning. Experiments across multiple domains show\nthat E2H Reasoner significantly improves the reasoning ability of small LLMs\n(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,\nhighlighting the effectiveness of our method.", "AI": {"tldr": "An E2H Reasoner method inspired by curriculum learning is proposed to enhance the reasoning capabilities of small LLMs through RL, showing significant improvements in reasoning ability.", "motivation": "To improve the reasoning capabilities of language models via reinforcement learning, addressing the limitation that using RL alone is less effective for inherently difficult tasks.", "method": "Propose a method called E2H Reasoner which schedules tasks from easy to hard, allowing LLMs to build reasoning skills gradually and preventing overfitting through appropriate scheduling.", "result": "Experiments show that E2H Reasoner significantly improves the reasoning ability of small LLMs (1.5B to 3B) across multiple domains compared to training with vanilla RL alone.", "conclusion": "E2H Reasoner is an effective method for enhancing the reasoning capabilities of small LLMs through RL, requiring fewer total samples than direct learning when tasks are appropriately decomposed and conditioned."}}
{"id": "2506.07418", "pdf": "https://arxiv.org/pdf/2506.07418", "abs": "https://arxiv.org/abs/2506.07418", "authors": ["Arnau Igualde S\u00e1ez", "Lamyae Rhomrasi", "Yusef Ahsini", "Ricardo Vinuesa", "Sergio Hoyas", "Jose P. Garc\u00eda Sabater", "Marius J. Fullana i Alfonso", "J. Alberto Conejero"], "title": "Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests", "categories": ["cs.AI", "68T05, 68T45", "I.2.10; I.2.7"], "comment": "16 pages, 4 figures", "summary": "Multimodal Large Language Models (MLLMs) promise advanced vision language\ncapabilities, yet their effectiveness in visually presented mathematics remains\nunderexplored. This paper analyzes the development and evaluation of MLLMs for\nmathematical problem solving, focusing on diagrams, multilingual text, and\nsymbolic notation. We then assess several models, including GPT 4o, Pixtral,\nQwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual\nKangaroo style benchmark spanning English, French, Spanish, and Catalan. Our\nexperiments reveal four key findings. First, overall precision remains moderate\nacross geometry, visual algebra, logic, patterns, and combinatorics: no single\nmodel excels in every topic. Second, while most models see improved accuracy\nwith questions that do not have images, the gain is often limited; performance\nfor some remains nearly unchanged without visual input, indicating\nunderutilization of diagrammatic information. Third, substantial variation\nexists across languages and difficulty levels: models frequently handle easier\nitems but struggle with advanced geometry and combinatorial reasoning. Notably,\nGemini 2.0 Flash achieves the highest precision on image based tasks, followed\nby Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.\nFourth, a complementary analysis aimed at distinguishing whether models reason\nor simply recite reveals that Gemini and GPT 4o stand out for their structured\nreasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less\nconsistent reasoning, often defaulting to heuristics or randomness when unable\nto align their outputs with the given answer options.", "AI": {"tldr": "Multimodal Large Language Models (MLLMs) are evaluated for mathematical problem solving involving diagrams, multilingual text, and symbolic notation. Experiments reveal moderate overall precision across topics, substantial variation across languages and difficulty levels, underutilization of diagrammatic information by some models, and differences in reasoning abilities.", "motivation": "To assess the effectiveness of MLLMs in visually presented mathematics and analyze their capabilities in solving mathematical problems involving diagrams, multilingual text, and symbolic notation.", "method": "Evaluate several MLLMs, including GPT 4o, Pixtral, Qwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash, using a multilingual Kangaroo style benchmark covering English, French, Spanish, and Catalan.", "result": "Four key findings: moderate overall precision across mathematical topics; limited improvement in accuracy without visual input; substantial variation in performance across languages and difficulty levels; Gemini 2.0 Flash, Qwen VL 2.5 72B, and GPT 4o perform best on image-based tasks but do not reach human-level performance; Gemini and GPT 4o excel in structured reasoning while Pixtral and Llama rely more on heuristics or randomness.", "conclusion": "MLLMs show promise but have limitations in mathematical problem solving with visual elements, particularly in fully utilizing diagrammatic information and achieving consistent reasoning across all tasks."}}
{"id": "2506.06633", "pdf": "https://arxiv.org/pdf/2506.06633", "abs": "https://arxiv.org/abs/2506.06633", "authors": ["Chi-Sheng Chen"], "title": "Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advancements in quantum machine learning have shown promise in\nenhancing classical neural network architectures, particularly in domains\ninvolving complex, high-dimensional data. Building upon prior work in temporal\nsequence modeling, this paper introduces Vision-QRWKV, a hybrid\nquantum-classical extension of the Receptance Weighted Key Value (RWKV)\narchitecture, applied for the first time to image classification tasks. By\nintegrating a variational quantum circuit (VQC) into the channel mixing\ncomponent of RWKV, our model aims to improve nonlinear feature transformation\nand enhance the expressive capacity of visual representations.\n  We evaluate both classical and quantum RWKV models on a diverse collection of\n14 medical and standard image classification benchmarks, including MedMNIST\ndatasets, MNIST, and FashionMNIST. Our results demonstrate that the\nquantum-enhanced model outperforms its classical counterpart on a majority of\ndatasets, particularly those with subtle or noisy class distinctions (e.g.,\nChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first\nsystematic application of quantum-enhanced RWKV in the visual domain, offering\ninsights into the architectural trade-offs and future potential of quantum\nmodels for lightweight and efficient vision tasks.", "AI": {"tldr": "Recent advancements in quantum machine learning have shown promise in enhancing classical neural network architectures, especially for complex, high-dimensional data. This paper introduces Vision-QRWKV, a hybrid quantum-classical extension of the RWKV architecture for image classification tasks. Results show that the quantum-enhanced model outperforms its classical counterpart on many datasets.", "motivation": "To explore the potential of quantum machine learning in improving classical neural network architectures for complex, high-dimensional data, particularly in image classification tasks.", "method": "Introduce Vision-QRWKV, a hybrid quantum-classical extension of the RWKV architecture, by integrating a variational quantum circuit (VQC) into the channel mixing component of RWKV to improve nonlinear feature transformation and enhance visual representations.", "result": "The quantum-enhanced model outperforms its classical counterpart on a majority of datasets, particularly those with subtle or noisy class distinctions.", "conclusion": "This study represents the first systematic application of quantum-enhanced RWKV in the visual domain, providing insights into the trade-offs and future potential of quantum models for lightweight and efficient vision tasks."}}
{"id": "2506.07428", "pdf": "https://arxiv.org/pdf/2506.07428", "abs": "https://arxiv.org/abs/2506.07428", "authors": ["Yuling Wang", "Zihui Chen", "Pengfei Jiao", "Xiao Wang"], "title": "HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the\nneed for tailored attacks to assess their robustness and ensure security.\nHowever, existing HGNN attacks often require complex retraining of parameters\nto generate specific perturbations for new scenarios. Recently, foundation\nmodels have opened new horizons for the generalization of graph neural networks\nby capturing shared semantics across various graph distributions. This leads us\nto ask:Can we design a foundation attack model for HGNNs that enables\ngeneralizable perturbations across different HGNNs, and quickly adapts to new\nheterogeneous graphs (HGs)? Empirical findings reveal that, despite significant\ndifferences in model design and parameter space, different HGNNs surprisingly\nshare common vulnerability patterns from a relation-aware perspective.\nTherefore, we explore how to design foundation HGNN attack criteria by mining\nshared attack units. In this paper, we propose a novel relation-wise\nheterogeneous graph foundation attack model, HeTa. We introduce a foundation\nsurrogate model to align heterogeneity and identify the importance of shared\nrelation-aware attack units. Building on this, we implement a serialized\nrelation-by-relation attack based on the identified relational weights. In this\nway, the perturbation can be transferred to various target HGNNs and easily\nfine-tuned for new HGs. Extensive experiments exhibit powerful attack\nperformances and generalizability of our method.", "AI": {"tldr": "Heterogeneous Graph Neural Networks (HGNNs) are vulnerable and need tailored attacks to assess their robustness. This paper proposes a novel foundation attack model HeTa, which can design generalizable perturbations across different HGNNs and quickly adapt to new heterogeneous graphs.", "motivation": "Existing HGNN attacks often require complex retraining of parameters to generate specific perturbations for new scenarios. Despite significant differences in model design and parameter space, different HGNNs share common vulnerability patterns from a relation-aware perspective.", "method": "The authors propose HeTa, a relation-wise heterogeneous graph foundation attack model. They introduce a foundation surrogate model to align heterogeneity and identify the importance of shared relation-aware attack units. Then, they implement a serialized relation-by-relation attack based on the identified relational weights.", "result": "Extensive experiments show that the proposed method exhibits powerful attack performances and generalizability.", "conclusion": "HeTa is an effective foundation attack model for HGNNs, enabling generalizable perturbations across different HGNNs and quick adaptation to new heterogeneous graphs."}}
{"id": "2506.06637", "pdf": "https://arxiv.org/pdf/2506.06637", "abs": "https://arxiv.org/abs/2506.06637", "authors": ["Olimjon Toirov", "Wei Yu"], "title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": "10 pages, 3 figures, 2025 2nd International Conference on Digital\n  Society and Artificial Intelligence (DSAI 2025), Conference dates: May 23-25,\n  2025", "summary": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and\nenergy consumption of each electrical device in the circuit by analyzing the\nelectrical signals at the bus, which is of great significance for smart power\nmanagement. However, the complex and changeable load combinations and\napplication environments lead to the challenges of poor feature robustness and\ninsufficient model generalization of traditional NILM methods. To this end,\nthis paper proposes a new non-intrusive load monitoring method that integrates\n\"image load signature\" and continual learning. This method converts\nmulti-dimensional power signals such as current, voltage, and power factor into\nvisual image load feature signatures, and combines deep convolutional neural\nnetworks to realize the identification and classification of multiple devices;\nat the same time, self-supervised pre-training is introduced to improve feature\ngeneralization, and continual online learning strategies are used to overcome\nmodel forgetting to adapt to the emergence of new loads. This paper conducts a\nlarge number of experiments on high-sampling rate load datasets, and compares a\nvariety of existing methods and model variants. The results show that the\nproposed method has achieved significant improvements in recognition accuracy.", "AI": {"tldr": "The paper proposes a new NILM method integrating 'image load signature' and continual learning to improve recognition accuracy in device identification and classification.", "motivation": "Traditional NILM methods suffer from poor feature robustness and insufficient model generalization due to complex and changeable load combinations and application environments.", "method": "Convert multi-dimensional power signals into visual image load feature signatures, use deep convolutional neural networks for device identification and classification, introduce self-supervised pre-training for better feature generalization, and apply continual online learning strategies to overcome model forgetting.", "result": "The method significantly improves recognition accuracy when tested on high-sampling rate load datasets compared to existing methods and model variants.", "conclusion": "The proposed NILM method that combines 'image load signature' and continual learning enhances the robustness and generalization of features, leading to significant improvements in recognition accuracy."}}
{"id": "2506.07443", "pdf": "https://arxiv.org/pdf/2506.07443", "abs": "https://arxiv.org/abs/2506.07443", "authors": ["Weijie Shi", "Han Zhu", "Jiaming Ji", "Mengze Li", "Jipeng Zhang", "Ruiyuan Zhang", "Jia Zhu", "Jiajie Xu", "Sirui Han", "Yike Guo"], "title": "LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Legal judgment prediction (LJP) aims to function as a judge by making final\nrulings based on case claims and facts, which plays a vital role in the\njudicial domain for supporting court decision-making and improving judicial\nefficiency. However, existing methods often struggle with logical errors when\nconducting complex legal reasoning. We propose LegalReasoner, which enhances\nLJP reliability through step-wise verification and correction of the reasoning\nprocess. Specifically, it first identifies dispute points to decompose complex\ncases, and then conducts step-wise reasoning while employing a process verifier\nto validate each step's logic from correctness, progressiveness, and potential\nperspectives. When errors are detected, expert-designed attribution and\nresolution strategies are applied for correction. To fine-tune LegalReasoner,\nwe release the LegalHK dataset, containing 58,130 Hong Kong court cases with\ndetailed annotations of dispute points, step-by-step reasoning chains, and\nprocess verification labels. Experiments demonstrate that LegalReasoner\nsignificantly improves concordance with court decisions from 72.37 to 80.27 on\nLLAMA-3.1-70B. The data is available at\nhttps://huggingface.co/datasets/weijiezz/LegalHK.", "AI": {"tldr": "Legal judgment prediction (LJP) is crucial for supporting court decision-making and improving judicial efficiency. However, existing methods often have logical errors when conducting complex legal reasoning. The paper proposes LegalReasoner to enhance LJP reliability through step-wise verification and correction of the reasoning process.", "motivation": "Existing methods for legal judgment prediction often struggle with logical errors when conducting complex legal reasoning.", "method": "The proposed method, LegalReasoner, first identifies dispute points to decompose complex cases, then conducts step-wise reasoning while employing a process verifier to validate each step's logic from correctness, progressiveness, and potential perspectives. When errors are detected, expert-designed attribution and resolution strategies are applied for correction.", "result": "Experiments demonstrate that LegalReasoner significantly improves concordance with court decisions from 72.37 to 80.27 on LLAMA-3.1-70B.", "conclusion": "LegalReasoner enhances the reliability of legal judgment prediction through step-wise verification and correction of the reasoning process."}}
{"id": "2506.06644", "pdf": "https://arxiv.org/pdf/2506.06644", "abs": "https://arxiv.org/abs/2506.06644", "authors": ["Chong You", "Kan Wu", "Zhipeng Jia", "Lin Chen", "Srinadh Bhojanapalli", "Jiaxian Guo", "Utku Evci", "Jan Wassenberg", "Praneeth Netrapalli", "Jeremiah J. Willcock", "Suvinay Subramanian", "Felix Chern", "Alek Andreev", "Shreya Pathak", "Felix Yu", "Prateek Jain", "David E. Culler", "Henry M. Levy", "Sanjiv Kumar"], "title": "Spark Transformer: Reactivating Sparsity in FFN and Attention", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The discovery of the lazy neuron phenomenon in trained Transformers, where\nthe vast majority of neurons in their feed-forward networks (FFN) are inactive\nfor each token, has spurred tremendous interests in activation sparsity for\nenhancing large model efficiency. While notable progress has been made in\ntranslating such sparsity to wall-time benefits, modern Transformers have moved\naway from the ReLU activation function crucial to this phenomenon. Existing\nefforts on re-introducing activation sparsity often degrade model quality,\nincrease parameter count, complicate or slow down training. Sparse attention,\nthe application of sparse activation to the attention mechanism, often faces\nsimilar challenges.\n  This paper introduces the Spark Transformer, a novel architecture that\nachieves a high level of activation sparsity in both FFN and the attention\nmechanism while maintaining model quality, parameter count, and standard\ntraining procedures. Our method realizes sparsity via top-k masking for\nexplicit control over sparsity level. Crucially, we introduce statistical\ntop-k, a hardware-accelerator-friendly, linear-time approximate algorithm that\navoids costly sorting and mitigates significant training slowdown from standard\ntop-$k$ operators. Furthermore, Spark Transformer reallocates existing FFN\nparameters and attention key embeddings to form a low-cost predictor for\nidentifying activated entries. This design not only mitigates quality loss from\nenforced sparsity, but also enhances wall-time benefit. Pretrained with the\nGemma-2 recipe, Spark Transformer demonstrates competitive performance on\nstandard benchmarks while exhibiting significant sparsity: only 8% of FFN\nneurons are activated, and each token attends to a maximum of 256 tokens. This\nsparsity translates to a 2.5x reduction in FLOPs, leading to decoding wall-time\nspeedups of up to 1.79x on CPU and 1.40x on GPU.", "AI": {"tldr": "This paper introduces Spark Transformer, a novel architecture achieving high activation sparsity in both FFN and attention mechanism without degrading model quality or complicating training. It uses top-k masking for sparsity control and statistical top-k for efficient hardware acceleration. Pretrained with Gemma-2 recipe, it shows competitive performance with significant sparsity (only 8% FFN neurons activated) leading to FLOPs reduction and decoding speedups.", "motivation": "To address the challenges of maintaining model quality, parameter count, and standard training procedures while introducing activation sparsity in Transformers that have moved away from ReLU activations.", "method": "Spark Transformer realizes sparsity via top-k masking for explicit control over sparsity level and introduces statistical top-k, a linear-time approximate algorithm avoiding costly sorting. It reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor identifying activated entries.", "result": "Demonstrates competitive performance on benchmarks with significant sparsity (only 8% FFN neurons activated), leading to a 2.5x reduction in FLOPs and decoding wall-time speedups of up to 1.79x on CPU and 1.40x on GPU.", "conclusion": "Spark Transformer successfully achieves high activation sparsity in both FFN and attention mechanism without degrading model quality, increasing parameter count, or complicating training."}}
{"id": "2506.06563", "pdf": "https://arxiv.org/pdf/2506.06563", "abs": "https://arxiv.org/abs/2506.06563", "authors": ["Thushari Hapuarachchi", "Long Dang", "Kaiqi Xiong"], "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks.", "AI": {"tldr": "The paper investigates the robustness of DNNs for traffic sign recognition, proposes a data augmentation-based training method to mitigate error-minimizing attacks, and introduces a detection model for poisoned data.", "motivation": "DNNs used in traffic sign recognition are trained on large-scale datasets from unknown sources, making them vulnerable to data poisoning attacks. It is crucial to ensure model security during training.", "method": "The authors perform error-minimizing attacks by adding imperceptible perturbations to the training data. They propose a data augmentation-based training method using nonlinear transformations to disrupt the perturbations and improve model robustness. Additionally, they develop a detection model to identify poisoned data.", "result": "Error-minimizing attacks reduce prediction accuracy from 99.90% to 10.6%. The proposed mitigation scheme restores accuracy to 96.05%, outperforming adversarial training. The detection model achieves over 99% success rate in identifying poisoned data.", "conclusion": "Advanced training methods are necessary for DNNs in traffic sign recognition systems to effectively mitigate data poisoning attacks."}}
{"id": "2506.07446", "pdf": "https://arxiv.org/pdf/2506.07446", "abs": "https://arxiv.org/abs/2506.07446", "authors": ["Liwen Zheng", "Chaozhuo Li", "Zheng Liu", "Feiran Huang", "Haoran Jia", "Zaisheng Ye", "Xi Zhang"], "title": "Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification", "categories": ["cs.AI"], "comment": null, "summary": "Fact verification plays a vital role in combating misinformation by assessing\nthe veracity of claims through evidence retrieval and reasoning. However,\ntraditional methods struggle with complex claims requiring multi-hop reasoning\nover fragmented evidence, as they often rely on static decomposition strategies\nand surface-level semantic retrieval, which fail to capture the nuanced\nstructure and intent of the claim. This results in accumulated reasoning\nerrors, noisy evidence contamination, and limited adaptability to diverse\nclaims, ultimately undermining verification accuracy in complex scenarios. To\naddress this, we propose Atomic Fact Extraction and Verification (AFEV), a\nnovel framework that iteratively decomposes complex claims into atomic facts,\nenabling fine-grained retrieval and adaptive reasoning. AFEV dynamically\nrefines claim understanding and reduces error propagation through iterative\nfact extraction, reranks evidence to filter noise, and leverages\ncontext-specific demonstrations to guide the reasoning process. Extensive\nexperiments on five benchmark datasets demonstrate that AFEV achieves\nstate-of-the-art performance in both accuracy and interpretability.", "AI": {"tldr": "\u4e8b\u5b9e\u6838\u67e5\u5728\u6253\u51fb\u9519\u8bef\u4fe1\u606f\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u65ad\u8a00\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u539f\u5b50\u4e8b\u5b9e\u63d0\u53d6\u548c\u9a8c\u8bc1\uff08AFEV\uff09\uff0c\u4e00\u4e2a\u53ef\u4ee5\u5c06\u590d\u6742\u65ad\u8a00\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\u7684\u65b0\u6846\u67b6\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u7ec6\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u4e8b\u5b9e\u9a8c\u8bc1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u590d\u6742\u65ad\u8a00\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u9759\u6001\u5206\u89e3\u7b56\u7565\u548c\u8868\u9762\u8bed\u4e49\u68c0\u7d22\uff0c\u65e0\u6cd5\u6355\u6349\u65ad\u8a00\u7684\u7ec6\u5fae\u7ed3\u6784\u548c\u610f\u56fe\uff0c\u5bfc\u81f4\u63a8\u7406\u9519\u8bef\u79ef\u7d2f\u3001\u8bc1\u636e\u566a\u97f3\u6c61\u67d3\u548c\u5bf9\u591a\u6837\u5316\u65ad\u8a00\u7684\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAtomic Fact Extraction and Verification (AFEV)\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u5730\u5c06\u590d\u6742\u65ad\u8a00\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u68c0\u7d22\u548c\u81ea\u9002\u5e94\u63a8\u7406\u3002\u5177\u4f53\u6765\u8bf4\uff0cAFEV\u52a8\u6001\u4f18\u5316\u65ad\u8a00\u7406\u89e3\uff0c\u51cf\u5c11\u8bef\u5dee\u4f20\u64ad\uff0c\u901a\u8fc7\u8fed\u4ee3\u4e8b\u5b9e\u63d0\u53d6\u91cd\u65b0\u6392\u5e8f\u8bc1\u636e\u4ee5\u8fc7\u6ee4\u566a\u58f0\uff0c\u5e76\u5229\u7528\u4e0a\u4e0b\u6587\u7279\u5b9a\u7684\u793a\u8303\u6307\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAFEV\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u6027\u80fd\u3002", "conclusion": "AFEV\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u590d\u6742\u65ad\u8a00\u7684\u4e8b\u5b9e\u9a8c\u8bc1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u662f\u4e8b\u5b9e\u9a8c\u8bc1\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8fdb\u6b65\u3002"}}
{"id": "2506.06649", "pdf": "https://arxiv.org/pdf/2506.06649", "abs": "https://arxiv.org/abs/2506.06649", "authors": ["Yishan Shen", "Yuyang Ye", "Hui Xiong", "Yong Chen"], "title": "SAFER: A Calibrated Risk-Aware Multimodal Recommendation Model for Dynamic Treatment Regimes", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted by ICML 2025", "summary": "Dynamic treatment regimes (DTRs) are critical to precision medicine,\noptimizing long-term outcomes through personalized, real-time decision-making\nin evolving clinical contexts, but require careful supervision for unsafe\ntreatment risks. Existing efforts rely primarily on clinician-prescribed gold\nstandards despite the absence of a known optimal strategy, and predominantly\nusing structured EHR data without extracting valuable insights from clinical\nnotes, limiting their reliability for treatment recommendations. In this work,\nwe introduce SAFER, a calibrated risk-aware tabular-language recommendation\nframework for DTR that integrates both structured EHR and clinical notes,\nenabling them to learn from each other, and addresses inherent label\nuncertainty by assuming ambiguous optimal treatment solution for deceased\npatients. Moreover, SAFER employs conformal prediction to provide statistical\nguarantees, ensuring safe treatment recommendations while filtering out\nuncertain predictions. Experiments on two publicly available sepsis datasets\ndemonstrate that SAFER outperforms state-of-the-art baselines across multiple\nrecommendation metrics and counterfactual mortality rate, while offering robust\nformal assurances. These findings underscore SAFER potential as a trustworthy\nand theoretically grounded solution for high-stakes DTR applications.", "AI": {"tldr": "The paper proposes SAFER, a risk-aware framework for dynamic treatment regimes (DTRs) that integrates structured EHR data and clinical notes, uses conformal prediction for safe recommendations, and demonstrates superior performance on sepsis datasets.", "motivation": "Dynamic treatment regimes are essential in precision medicine but require careful supervision due to potential unsafe treatment risks. Current methods rely heavily on clinician-prescribed standards and structured EHR data, ignoring valuable insights from clinical notes and introducing limitations in reliability of treatment recommendations.", "method": "SAFER is introduced as a calibrated risk-aware tabular-language recommendation framework for DTRs. It integrates structured EHR data with clinical notes to allow mutual learning, addresses label uncertainty by assuming ambiguous optimal treatment solutions for deceased patients, and employs conformal prediction to ensure statistical guarantees and filter out uncertain predictions.", "result": "Experiments on two publicly available sepsis datasets show that SAFER surpasses state-of-the-art baselines across multiple recommendation metrics and counterfactual mortality rate, providing robust formal assurances.", "conclusion": "SAFER shows promise as a reliable and theoretically grounded solution for high-stakes DTR applications."}}
{"id": "2506.06691", "pdf": "https://arxiv.org/pdf/2506.06691", "abs": "https://arxiv.org/abs/2506.06691", "authors": ["Kaushik Talathi", "Aparna Santra Biswas"], "title": "An Efficient Digital Watermarking Technique for Small Scale devices", "categories": ["cs.MM", "cs.CR"], "comment": "28 pages, 11 figures, 4 tables", "summary": "In the age of IoT and mobile platforms, ensuring that content stay authentic\nwhilst avoiding overburdening limited hardware is a key problem. This study\nintroduces hybrid Fast Wavelet Transform & Additive Quantization index\nModulation (FWT-AQIM) scheme, a lightweight watermarking approach that secures\ndigital pictures on low-power, memory-constrained small scale devices to\nachieve a balanced trade-off among robustness, imperceptibility, and\ncomputational efficiency. The method embeds watermark in the luminance\ncomponent of YCbCr color space using low-frequency FWT sub-bands, minimizing\nperceptual distortion, using additive QIM for simplicity. Both the extraction\nand embedding processes run in less than 40 ms and require minimum RAM when\ntested on a Raspberry Pi 5. Quality assessments on standard and high-resolution\nimages yield PSNR greater than equal to 34 dB and SSIM greater than equal to\n0.97, while robustness verification includes various geometric and\nsignal-processing attacks demonstrating near-zero bit error rates and NCC\ngreater than equal to 0.998. Using a mosaic-based watermark, redundancy added\nenhancing robustness without reducing throughput, which peaks at 11 MP/s. These\nfindings show that FWT-AQIM provides an efficient, scalable solution for\nreal-time, secure watermarking in bandwidth- and power-constrained contexts,\nopening the way for dependable content protection in developing IoT and\nmultimedia applications.", "AI": {"tldr": "In the era of IoT and mobile platforms, this study introduces a hybrid FWT-AQIM watermarking scheme that ensures content authenticity on low-power devices. It achieves balance among robustness, imperceptibility, and computational efficiency with quality assessments yielding PSNR\u226534 dB and SSIM\u22650.97.", "motivation": "To address the challenge of ensuring content authenticity while avoiding overburdening limited hardware in the age of IoT and mobile platforms.", "method": "The method embeds watermark in the luminance component of YCbCr color space using low-frequency FWT sub-bands, minimizing perceptual distortion, using additive QIM for simplicity.", "result": "Quality assessments yield PSNR\u226534 dB and SSIM\u22650.97, with near-zero bit error rates and NCC\u22650.998 under various attacks. Throughput peaks at 11 MP/s.", "conclusion": "FWT-AQIM provides an efficient, scalable solution for real-time, secure watermarking in bandwidth- and power-constrained contexts."}}
{"id": "2506.07450", "pdf": "https://arxiv.org/pdf/2506.07450", "abs": "https://arxiv.org/abs/2506.07450", "authors": ["Yi Loo", "Akshunn Trivedi", "Malika Meghjani"], "title": "Efficient Generation of Diverse Cooperative Agents with World Models", "categories": ["cs.AI"], "comment": null, "summary": "A major bottleneck in the training process for Zero-Shot Coordination (ZSC)\nagents is the generation of partner agents that are diverse in collaborative\nconventions. Current Cross-play Minimization (XPM) methods for population\ngeneration can be very computationally expensive and sample inefficient as the\ntraining objective requires sampling multiple types of trajectories. Each\npartner agent in the population is also trained from scratch, despite all of\nthe partners in the population learning policies of the same coordination task.\nIn this work, we propose that simulated trajectories from the dynamics model of\nan environment can drastically speed up the training process for XPM methods.\nWe introduce XPM-WM, a framework for generating simulated trajectories for XPM\nvia a learned World Model (WM). We show XPM with simulated trajectories removes\nthe need to sample multiple trajectories. In addition, we show our proposed\nmethod can effectively generate partners with diverse conventions that match\nthe performance of previous methods in terms of SP population training reward\nas well as training partners for ZSC agents. Our method is thus, significantly\nmore sample efficient and scalable to a larger number of partners.", "AI": {"tldr": "The paper proposes XPM-WM, a framework using a learned World Model to generate simulated trajectories for Cross-play Minimization, which improves sample efficiency and scalability in training diverse partner agents for Zero-Shot Coordination tasks.", "motivation": "To address the computational inefficiency and lack of sample efficiency in current Cross-play Minimization methods for generating diverse partner agents in Zero-Shot Coordination tasks.", "method": "Propose XPM-WM framework that utilizes a learned World Model to create simulated trajectories, removing the need for sampling multiple types of trajectories and enhancing the generation of diverse partner agents with shared coordination task policies.", "result": "XPM-WM matches previous methods' performance in SP population training reward and training partners for ZSC agents while being more sample efficient and scalable.", "conclusion": "XPM-WM significantly enhances sample efficiency and scalability in generating diverse partner agents for Zero-Shot Coordination tasks."}}
{"id": "2506.06656", "pdf": "https://arxiv.org/pdf/2506.06656", "abs": "https://arxiv.org/abs/2506.06656", "authors": ["Ittai Rubinstein", "Samuel B. Hopkins"], "title": "Rescaled Influence Functions: Accurate Data Attribution in High Dimension", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "How does the training data affect a model's behavior? This is the question we\nseek to answer with data attribution. The leading practical approaches to data\nattribution are based on influence functions (IF). IFs utilize a first-order\nTaylor approximation to efficiently predict the effect of removing a set of\nsamples from the training set without retraining the model, and are used in a\nwide variety of machine learning applications. However, especially in the\nhigh-dimensional regime (# params $\\geq \\Omega($# samples$)$), they are often\nimprecise and tend to underestimate the effect of sample removals, even for\nsimple models such as logistic regression. We present rescaled influence\nfunctions (RIF), a new tool for data attribution which can be used as a drop-in\nreplacement for influence functions, with little computational overhead but\nsignificant improvement in accuracy. We compare IF and RIF on a range of\nreal-world datasets, showing that RIFs offer significantly better predictions\nin practice, and present a theoretical analysis explaining this improvement.\nFinally, we present a simple class of data poisoning attacks that would fool\nIF-based detections but would be detected by RIF.", "AI": {"tldr": "The paper proposes rescaled influence functions (RIF) to improve the accuracy of data attribution compared to traditional influence functions (IF), demonstrating its effectiveness through experiments and theoretical analysis.", "motivation": "To better understand how training data affects model behavior, particularly addressing the shortcomings of existing influence functions which can be imprecise and underestimate sample removal effects in high-dimensional settings.", "method": "Introduced rescaled influence functions (RIF) as an enhancement to traditional influence functions. RIF offers a drop-in replacement with minimal computational overhead but improved accuracy for predicting the effect of removing samples from the training set.", "result": "RIFs provided significantly better predictions than IFs across various real-world datasets. A theoretical analysis supported these findings, and RIF was shown to detect certain data poisoning attacks that would fool IF-based detections.", "conclusion": "Rescaled influence functions provide a more accurate and reliable tool for data attribution, offering improvements over traditional influence functions in both practical applications and theoretical robustness."}}
{"id": "2506.06694", "pdf": "https://arxiv.org/pdf/2506.06694", "abs": "https://arxiv.org/abs/2506.06694", "authors": ["Yuan Yuan", "Yukun Liu", "Chonghua Han", "Jie Feng", "Yong Li"], "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.", "AI": {"tldr": "MoveGCL\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u6301\u7eed\u5b66\u4e60\u8bad\u7ec3\u79fb\u52a8\u6027\u57fa\u7840\u6a21\u578b\uff0c\u5728\u4e0d\u5171\u4eab\u539f\u59cb\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u548c\u6e10\u8fdb\u5f0f\u7684\u6a21\u578b\u8fdb\u5316\u3002", "motivation": "\u6784\u5efa\u7c7b\u4f3c\u7684\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u4eba\u7c7b\u79fb\u52a8\u6027\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u79fb\u52a8\u6027\u6570\u636e\u7684\u9690\u79c1\u654f\u611f\u6027\u548c\u673a\u6784\u95f4\u7684\u6570\u636e\u5b64\u5c9b\u95ee\u9898\u3002", "method": "MoveGCL\u901a\u8fc7\u91cd\u653e\u7531\u51bb\u7ed3\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u8f68\u8ff9\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u548c\u6e10\u8fdb\u5f0f\u7684\u6a21\u578b\u8fdb\u5316\uff0c\u5e76\u901a\u8fc7\u5b9a\u5236\u7684\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u5f3a\u5316\u77e5\u8bc6\u4fdd\u7559\uff1b\u7ed3\u5408Mixture-of-Experts Transformer\u548c\u79fb\u52a8\u611f\u77e5\u4e13\u5bb6\u8def\u7531\u673a\u5236\u89e3\u51b3\u79fb\u52a8\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\uff1b\u91c7\u7528\u9010\u5c42\u6e10\u8fdb\u9002\u5e94\u7b56\u7565\u7a33\u5b9a\u6301\u7eed\u66f4\u65b0\u3002", "result": "\u5728\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoveGCL\u5b9e\u73b0\u4e86\u4e0e\u8054\u5408\u8bad\u7ec3\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\uff0c\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u62a4\u3002", "conclusion": "MoveGCL\u4e3a\u89e3\u9501\u79fb\u52a8\u6027\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u63d0\u4f9b\u4e86\u5728\u57fa\u7840\u6a21\u578b\u65f6\u4ee3\u5f00\u653e\u3001\u53ef\u6269\u5c55\u548c\u4fdd\u62a4\u9690\u79c1\u7684\u6a21\u578b\u5f00\u53d1\u7684\u5b9e\u9645\u84dd\u56fe\u3002"}}
{"id": "2506.07527", "pdf": "https://arxiv.org/pdf/2506.07527", "abs": "https://arxiv.org/abs/2506.07527", "authors": ["Lu Ma", "Hao Liang", "Meiyi Qiang", "Lexiang Tang", "Xiaochen Ma", "Zhen Hao Wong", "Junbo Niu", "Chengyu Shen", "Runming He", "Bin Cui", "Wentao Zhang"], "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions", "categories": ["cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Recent advances in large language model (LLM) reasoning have shown that\nsophisticated behaviors such as planning and self-reflection can emerge through\nreinforcement learning (RL). However, despite these successes, RL in its\ncurrent form remains insufficient to induce capabilities that exceed the\nlimitations of the base model, as it is primarily optimized based on existing\nknowledge of the model rather than facilitating the acquisition of new\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\nto learn what RL cannot, which enables the incorporation of new knowledge and\nreasoning patterns by leveraging high-quality demonstration data. We analyze\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\nat maintaining and improving performance on questions within the model's\noriginal capabilities, while SFT is more effective at enabling progress on\nquestions beyond the current scope of the model. Motivated by the complementary\nstrengths of RL and SFT, we introduce a novel training approach,\n\\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved\nwith Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily\ntrained using RL, but when it encounters challenging questions, high-quality\nsolutions are collected for fine-tuning, and the training process alternates\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\nachieves an average improvement of over +5.2 points across five\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\nRL and SFT while using only 13\\% of the detailed demonstration data,\nhighlighting its scalability. These results provide compelling evidence that\nReLIFT overcomes the fundamental limitations of RL and underscores the\nsignificant potential.", "AI": {"tldr": "\u8fd1\u671f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u7684\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u53ef\u4ee5\u6d8c\u73b0\u51fa\u8bf8\u5982\u89c4\u5212\u548c\u81ea\u6211\u53cd\u601d\u7b49\u590d\u6742\u884c\u4e3a\u3002\u7136\u800c\uff0c\u76ee\u524d\u7684RL\u4ecd\u4e0d\u8db3\u4ee5\u7a81\u7834\u57fa\u7840\u6a21\u578b\u7684\u9650\u5236\uff0c\u56e0\u4e3a\u5b83\u4e3b\u8981\u57fa\u4e8e\u6a21\u578b\u5df2\u6709\u7684\u77e5\u8bc6\u8fdb\u884c\u4f18\u5316\uff0c\u800c\u975e\u4fc3\u8fdb\u65b0\u4fe1\u606f\u7684\u83b7\u53d6\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u91c7\u7528\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u4ee5\u5b66\u4e60RL\u65e0\u6cd5\u505a\u5230\u7684\u4e8b\uff0c\u8fd9\u4f7f\u5f97\u901a\u8fc7\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\u5f15\u5165\u65b0\u77e5\u8bc6\u548c\u63a8\u7406\u6a21\u5f0f\u6210\u4e3a\u53ef\u80fd\u3002\u5206\u6790\u53d1\u73b0\uff0cRL\u64c5\u957f\u4fdd\u6301\u548c\u63d0\u5347\u6a21\u578b\u539f\u672c\u80fd\u529b\u8303\u56f4\u5185\u7684\u95ee\u9898\u8868\u73b0\uff0c\u800cSFT\u5728\u63a8\u52a8\u6a21\u578b\u8d85\u8d8a\u5176\u5f53\u524d\u8303\u56f4\u65b9\u9762\u66f4\u6709\u6548\u3002\u53d7\u4e24\u8005\u4e92\u8865\u4f18\u52bf\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u2014\u2014ReLIFT\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u5728\u7ebf\u5fae\u8c03\u4ea4\u66ff\u8fdb\u884c\uff09\u3002\u5728ReLIFT\u4e2d\uff0c\u6a21\u578b\u4e3b\u8981\u901a\u8fc7RL\u8bad\u7ec3\uff0c\u4f46\u9047\u5230\u96be\u9898\u65f6\u6536\u96c6\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5728RL\u548c\u5fae\u8c03\u4e4b\u95f4\u4ea4\u66ff\u8bad\u7ec3\u4ee5\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002ReLIFT\u5728\u4e94\u4e2a\u7ade\u8d5b\u7ea7\u522b\u57fa\u51c6\u548c\u4e00\u4e2a\u5206\u5e03\u5916\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u9ad8\u4e865.2\u5206\u4ee5\u4e0a\uff0c\u76f8\u8f83\u4e8e\u5176\u4ed6\u96f6RL\u6a21\u578b\u8868\u73b0\u51fa\u8272\u3002\u6b64\u5916\uff0cReLIFT\u4ec5\u4f7f\u752813%\u7684\u8be6\u7ec6\u793a\u8303\u6570\u636e\u4fbf\u8d85\u8d8a\u4e86RL\u548cSFT\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u6269\u5c55\u6027\u3002\u8fd9\u4e9b\u7ed3\u679c\u5f3a\u6709\u529b\u5730\u8868\u660e\uff0cReLIFT\u514b\u670d\u4e86RL\u7684\u57fa\u672c\u9650\u5236\u5e76\u5c55\u793a\u4e86\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u6210\u529f\uff0c\u4f46\u5176\u4ecd\u7136\u53d7\u9650\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u80fd\u529b\u8303\u56f4\uff0c\u96be\u4ee5\u7a81\u7834\u73b0\u6709\u77e5\u8bc6\u7684\u5c40\u9650\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5f15\u5165\u65b0\u77e5\u8bc6\u548c\u63a8\u7406\u6a21\u5f0f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u2014\u2014ReLIFT\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u5728\u7ebf\u5fae\u8c03\u4ea4\u66ff\u8fdb\u884c\uff09\u3002\u8be5\u65b9\u6cd5\u4e3b\u8981\u5229\u7528RL\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u5728\u9047\u5230\u96be\u9898\u65f6\u6536\u96c6\u9ad8\u8d28\u91cf\u89e3\u51b3\u65b9\u6848\u8fdb\u884cSFT\u5fae\u8c03\uff0c\u5e76\u5728RL\u548c\u5fae\u8c03\u4e4b\u95f4\u4ea4\u66ff\u8bad\u7ec3\u3002\u8fd9\u79cd\u65b9\u6cd5\u7ed3\u5408\u4e86RL\u548cSFT\u7684\u4e92\u8865\u4f18\u52bf\uff0c\u65e8\u5728\u589e\u5f3a\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "ReLIFT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u76f8\u8f83\u4e8e\u5176\u4ed6\u96f6RL\u6a21\u578b\u5e73\u5747\u63d0\u9ad8\u4e865.2\u5206\u4ee5\u4e0a\u3002\u540c\u65f6\uff0c\u5b83\u4ec5\u4f7f\u752813%\u7684\u8be6\u7ec6\u793a\u8303\u6570\u636e\u4fbf\u8d85\u8d8a\u4e86\u5355\u72ec\u4f7f\u7528RL\u6216SFT\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "ReLIFT\u6210\u529f\u514b\u670d\u4e86RL\u7684\u57fa\u672c\u9650\u5236\uff0c\u5c55\u73b0\u4e86\u663e\u8457\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765LLM\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2506.06665", "pdf": "https://arxiv.org/pdf/2506.06665", "abs": "https://arxiv.org/abs/2506.06665", "authors": ["Hong-Ming Chiu", "Hao Chen", "Huan Zhang", "Richard Y. Zhang"], "title": "SDP-CROWN: Efficient Bound Propagation for Neural Network Verification with Tightness of Semidefinite Programming", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Neural network verifiers based on linear bound propagation scale impressively\nto massive models but can be surprisingly loose when neuron coupling is\ncrucial. Conversely, semidefinite programming (SDP) verifiers capture\ninter-neuron coupling naturally, but their cubic complexity restricts them to\nonly small models. In this paper, we propose SDP-CROWN, a novel hybrid\nverification framework that combines the tightness of SDP relaxations with the\nscalability of bound-propagation verifiers. At the core of SDP-CROWN is a new\nlinear bound, derived via SDP principles, that explicitly captures\n$\\ell_{2}$-norm-based inter-neuron coupling while adding only one extra\nparameter per layer. This bound can be integrated seamlessly into any linear\nbound-propagation pipeline, preserving the inherent scalability of such methods\nyet significantly improving tightness. In theory, we prove that our\ninter-neuron bound can be up to a factor of $\\sqrt{n}$ tighter than traditional\nper-neuron bounds. In practice, when incorporated into the state-of-the-art\n$\\alpha$-CROWN verifier, we observe markedly improved verification performance\non large models with up to 65 thousand neurons and 2.47 million parameters,\nachieving tightness that approaches that of costly SDP-based methods.", "AI": {"tldr": "\u63d0\u51faSDP-CROWN\uff0c\u7ed3\u5408SDP\u677e\u5f1b\u7684\u7d27\u5bc6\u6027\u4e0e\u4f20\u64ad\u9a8c\u8bc1\u5668\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u795e\u7ecf\u7f51\u7edc\u9a8c\u8bc1\u5668\u5728\u5904\u7406\u5927\u89c4\u6a21\u6a21\u578b\u65f6\u8868\u73b0\u4e0d\u540c\uff0c\u7ebf\u6027\u4f20\u64ad\u9a8c\u8bc1\u5668\u5728\u795e\u7ecf\u5143\u8026\u5408\u5173\u952e\u65f6\u53ef\u80fd\u5bbd\u677e\uff0c\u800c\u534a\u5b9a\u89c4\u5212\uff08SDP\uff09\u9a8c\u8bc1\u5668\u867d\u80fd\u81ea\u7136\u6355\u6349\u795e\u7ecf\u5143\u95f4\u8026\u5408\uff0c\u4f46\u590d\u6742\u5ea6\u9650\u5236\u5176\u4ec5\u9002\u7528\u4e8e\u5c0f\u6a21\u578b\u3002", "method": "\u63d0\u51faSDP-CROWN\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u7684\u7ebf\u6027\u8fb9\u754c\uff08\u57fa\u4e8eSDP\u539f\u7406\uff09\uff0c\u5728\u6bcf\u5c42\u53ea\u589e\u52a0\u4e00\u4e2a\u989d\u5916\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u660e\u786e\u6355\u6349\u57fa\u4e8e\u21132-\u8303\u6570\u7684\u795e\u7ecf\u5143\u95f4\u8026\u5408\uff0c\u5e76\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u4efb\u4f55\u7ebf\u6027\u8fb9\u754c\u4f20\u64ad\u7ba1\u9053\u4e2d\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u795e\u7ecf\u5143\u95f4\u8fb9\u754c\u6bd4\u4f20\u7edf\u5355\u795e\u7ecf\u5143\u8fb9\u754c\u6700\u591a\u53ef\u7d27\u81f4\u221an\u500d\uff1b\u5b9e\u8df5\u4e2d\uff0c\u5728\u9ad8\u8fbe65000\u4e2a\u795e\u7ecf\u5143\u548c247\u4e07\u4e2a\u53c2\u6570\u7684\u5927\u89c4\u6a21\u6a21\u578b\u4e0a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u03b1-CROWN\u9a8c\u8bc1\u5668\u7684\u6027\u80fd\uff0c\u63a5\u8fd1\u6602\u8d35\u7684SDP\u65b9\u6cd5\u7684\u7d27\u5bc6\u6027\u3002", "conclusion": "SDP-CROWN\u4fdd\u7559\u4e86\u7ebf\u6027\u8fb9\u754c\u4f20\u64ad\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u7d27\u5bc6\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u7684\u9a8c\u8bc1\u3002"}}
{"id": "2506.06825", "pdf": "https://arxiv.org/pdf/2506.06825", "abs": "https://arxiv.org/abs/2506.06825", "authors": ["Shijing He", "Yaxiong Lei", "Zihan Zhang", "Yuzhou Sun", "Shujun Li", "Chi Zhang", "Juan Ye"], "title": "Identity Deepfake Threats to Biometric Authentication Systems: Public and Expert Perspectives", "categories": ["cs.HC", "cs.CR", "68T10, 68T45, 68M25", "I.4.9; I.5.4; K.4.1; K.6.5"], "comment": null, "summary": "Generative AI (Gen-AI) deepfakes pose a rapidly evolving threat to biometric\nauthentication, yet a significant gap exists between expert understanding of\nthese risks and public perception. This disconnection creates critical\nvulnerabilities in systems trusted by millions. To bridge this gap, we\nconducted a comprehensive mixed-method study, surveying 408 professionals\nacross key sectors and conducting in-depth interviews with 37 participants (25\nexperts, 12 general public [non-experts]). Our findings reveal a paradox: while\nthe public increasingly relies on biometrics for convenience, experts express\ngrave concerns about the spoofing of static modalities like face and voice\nrecognition. We found significant demographic and sector-specific divides in\nawareness and trust, with finance professionals, for example, showing\nheightened skepticism. To systematically analyze these threats, we introduce a\nnovel Deepfake Kill Chain model, adapted from Hutchins et al.'s cybersecurity\nframeworks to map the specific attack vectors used by malicious actors against\nbiometric systems. Based on this model and our empirical findings, we propose a\ntri-layer mitigation framework that prioritizes dynamic biometric signals\n(e.g., eye movements), robust privacy-preserving data governance, and targeted\neducational initiatives. This work provides the first empirically grounded\nroadmap for defending against AI-generated identity threats by aligning\ntechnical safeguards with human-centered insights.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff08Gen-AI\uff09\u6df1\u5ea6\u4f2a\u9020\u5bf9\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u6784\u6210\u5feb\u901f\u6f14\u53d8\u7684\u5a01\u80c1\u3002\u7814\u7a76\u901a\u8fc7\u7efc\u5408\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u63ed\u793a\u4e86\u516c\u4f17\u4e0e\u4e13\u5bb6\u5728\u98ce\u9669\u8ba4\u77e5\u4e0a\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\u94fe\u6a21\u578b\u548c\u4e09\u5c42\u7f13\u89e3\u6846\u67b6\u4ee5\u5e94\u5bf9\u5a01\u80c1\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5bf9\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u516c\u4f17\u5bf9\u5176\u98ce\u9669\u7684\u8ba4\u77e5\u8fdc\u4f4e\u4e8e\u4e13\u5bb6\u7684\u7406\u89e3\uff0c\u8fd9\u79cd\u8131\u8282\u5bfc\u81f4\u7cfb\u7edf\u5b58\u5728\u5173\u952e\u6f0f\u6d1e\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7efc\u5408\u6df7\u5408\u65b9\u6cd5\uff0c\u5305\u62ec\u5bf9408\u540d\u4e13\u4e1a\u4eba\u58eb\u8fdb\u884c\u8c03\u67e5\u4ee5\u53ca\u5bf937\u540d\u53c2\u4e0e\u8005\uff0825\u540d\u4e13\u5bb6\u548c12\u540d\u975e\u4e13\u5bb6\uff09\u8fdb\u884c\u6df1\u5165\u8bbf\u8c08\uff0c\u5206\u6790\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u4e2d\u6df1\u5ea6\u4f2a\u9020\u7684\u5a01\u80c1\u3002\u5e76\u5f15\u5165\u4e86\u57fa\u4e8eHutchins\u7b49\u4eba\u7f51\u7edc\u5b89\u5168\u6846\u67b6\u7684\u65b0\u578bDeepfake Kill Chain\u6a21\u578b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u516c\u4f17\u65e5\u76ca\u4f9d\u8d56\u751f\u7269\u7279\u5f81\u8ba4\u8bc1\u7684\u4fbf\u6377\u6027\uff0c\u800c\u4e13\u5bb6\u5219\u5bf9\u9759\u6001\u6a21\u6001\uff08\u5982\u9762\u90e8\u548c\u8bed\u97f3\u8bc6\u522b\uff09\u88ab\u6b3a\u9a97\u8868\u793a\u4e25\u91cd\u62c5\u5fe7\u3002\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u548c\u884c\u4e1a\u95f4\u5b58\u5728\u663e\u8457\u7684\u610f\u8bc6\u548c\u4fe1\u4efb\u5dee\u5f02\u3002\u57fa\u4e8e\u7814\u7a76\u7ed3\u679c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u6b21\u7f13\u89e3\u6846\u67b6\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63d0\u4f9b\u4e86\u9632\u5fa1AI\u751f\u6210\u8eab\u4efd\u5a01\u80c1\u7684\u7ecf\u9a8c\u57fa\u7840\u8def\u7ebf\u56fe\uff0c\u5c06\u6280\u672f\u4fdd\u969c\u4e0e\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6d1e\u5bdf\u76f8\u7ed3\u5408\uff0c\u5f3a\u8c03\u52a8\u6001\u751f\u7269\u7279\u5f81\u4fe1\u53f7\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u6570\u636e\u6cbb\u7406\u548c\u9488\u5bf9\u6027\u6559\u80b2\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.07528", "pdf": "https://arxiv.org/pdf/2506.07528", "abs": "https://arxiv.org/abs/2506.07528", "authors": ["Qisheng Hu", "Quanyu Long", "Wenya Wang"], "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification", "categories": ["cs.AI"], "comment": "19 pages, 9 figures", "summary": "Multi-hop claim verification is inherently challenging, requiring multi-step\nreasoning to construct verification chains while iteratively searching for\ninformation to uncover hidden bridging facts. This process is fundamentally\ninterleaved, as effective reasoning relies on dynamically retrieved evidence,\nwhile effective search demands reasoning to refine queries based on partial\ninformation. To achieve this, we propose Hierarchical Agent Reasoning and\nInformation Search (HARIS), explicitly modeling the coordinated process of\nreasoning-driven searching and search-informed reasoning. HARIS consists of a\nhigh-level reasoning agent that focuses on constructing the main verification\nchain, generating factual questions when more information is needed, and a\nlow-level search agent that iteratively retrieves more information, refining\nits search based on intermediate findings. This design allows each agent to\nspecialize in its respective task, enhancing verification accuracy and\ninterpretability. HARIS is trained using reinforcement learning with\noutcome-based rewards. Experimental results on the EX-FEVER and HOVER\nbenchmarks demonstrate that HARIS achieves strong performance, greatly\nadvancing multi-hop claim verification.", "AI": {"tldr": "The paper presents Hierarchical Agent Reasoning and Information Search (HARIS) for multi-hop claim verification, which combines reasoning and information search through reinforcement learning, achieving strong performance on EX-FEVER and HOVER benchmarks.", "motivation": "Multi-hop claim verification is a complex task that requires both multi-step reasoning and iterative information searching to uncover hidden bridging facts.", "method": "The proposed method, HARIS, consists of a high-level reasoning agent and a low-level search agent. The high-level agent constructs the main verification chain and generates questions when more information is needed, while the low-level agent retrieves additional information and refines its search based on intermediate findings. HARIS is trained using reinforcement learning with outcome-based rewards.", "result": "Experimental results on the EX-FEVER and HOVER benchmarks show that HARIS achieves strong performance, significantly advancing multi-hop claim verification.", "conclusion": "HARIS effectively models the coordinated process of reasoning-driven searching and search-informed reasoning, enhancing verification accuracy and interpretability."}}
{"id": "2506.06666", "pdf": "https://arxiv.org/pdf/2506.06666", "abs": "https://arxiv.org/abs/2506.06666", "authors": ["Oktay Karaku\u015f", "Hasan Arkada\u015f"], "title": "Through the Gaps: Uncovering Tactical Line-Breaking Passes with Clustering", "categories": ["cs.LG", "stat.ML"], "comment": "12 pages and 5 figures", "summary": "Line-breaking passes (LBPs) are crucial tactical actions in football,\nallowing teams to penetrate defensive lines and access high-value spaces. In\nthis study, we present an unsupervised, clustering-based framework for\ndetecting and analysing LBPs using synchronised event and tracking data from\nelite matches. Our approach models opponent team shape through vertical spatial\nsegmentation and identifies passes that disrupt defensive lines within open\nplay. Beyond detection, we introduce several tactical metrics, including the\nspace build-up ratio (SBR) and two chain-based variants, LBPCh$^1$ and\nLBPCh$^2$, which quantify the effectiveness of LBPs in generating immediate or\nsustained attacking threats. We evaluate these metrics across teams and players\nin the 2022 FIFA World Cup, revealing stylistic differences in vertical\nprogression and structural disruption. The proposed methodology is explainable,\nscalable, and directly applicable to modern performance analysis and scouting\nworkflows.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u7684\u975e\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u4f7f\u7528\u540c\u6b65\u4e8b\u4ef6\u548c\u8ffd\u8e2a\u6570\u636e\u68c0\u6d4b\u548c\u5206\u6790\u8db3\u7403\u6bd4\u8d5b\u4e2d\u7684\u7ebf\u7834\u4f20\u7403\uff08LBPs\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u51e0\u4e2a\u6218\u672f\u6307\u6807\u6765\u91cf\u5316LBPs\u7684\u6548\u679c\u3002", "motivation": "\u7ebf\u7834\u4f20\u7403\uff08LBPs\uff09\u662f\u8db3\u7403\u4e2d\u5173\u952e\u7684\u6218\u672f\u52a8\u4f5c\uff0c\u4f7f\u7403\u961f\u80fd\u591f\u7a7f\u900f\u9632\u5b88\u7ebf\u5e76\u8fdb\u5165\u9ad8\u4ef7\u503c\u7a7a\u95f4\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8fd9\u4e9b\u4f20\u7403\u5982\u4f55\u7834\u574f\u5bf9\u65b9\u9632\u5b88\u7ed3\u6784\u4ee5\u53ca\u5982\u4f55\u4ea7\u751f\u8fdb\u653b\u5a01\u80c1\u662f\u975e\u5e38\u91cd\u8981\u7684\u3002", "method": "\u901a\u8fc7\u5782\u76f4\u7a7a\u95f4\u5206\u5272\u5efa\u6a21\u5bf9\u624b\u56e2\u961f\u5f62\u72b6\uff0c\u5e76\u8bc6\u522b\u6bd4\u8d5b\u4e2d\u7834\u574f\u9632\u5b88\u7ebf\u7684\u4f20\u7403\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u51e0\u4e2a\u6218\u672f\u5ea6\u91cf\u6807\u51c6\uff0c\u5305\u62ec\u7a7a\u95f4\u6784\u5efa\u6bd4\u7387\uff08SBR\uff09\u548c\u4e24\u4e2a\u94fe\u5f0f\u53d8\u4f53LBPCh$^1$\u548cLBPCh$^2$\uff0c\u4ee5\u91cf\u5316LBPs\u5728\u4ea7\u751f\u5373\u65f6\u6216\u6301\u7eed\u8fdb\u653b\u5a01\u80c1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u5bf92022\u5e74FIFA\u4e16\u754c\u676f\u4e2d\u7684\u7403\u961f\u548c\u7403\u5458\u8fdb\u884c\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u5728\u5782\u76f4\u63a8\u8fdb\u548c\u7ed3\u6784\u7834\u574f\u65b9\u9762\u7684\u98ce\u683c\u5dee\u5f02\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5177\u6709\u53ef\u89e3\u91ca\u6027\u3001\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u4ee3\u7ee9\u6548\u5206\u6790\u548c\u7403\u63a2\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2506.06861", "pdf": "https://arxiv.org/pdf/2506.06861", "abs": "https://arxiv.org/abs/2506.06861", "authors": ["Xizhi Tian", "Meng Ding", "Touming Tao", "Zihang Xiang", "Di Wang"], "title": "Differentially Private Sparse Linear Regression with Heavy-tailed Responses", "categories": ["cs.LG", "cs.CR"], "comment": "Accepted at ECML 2025", "summary": "As a fundamental problem in machine learning and differential privacy (DP),\nDP linear regression has been extensively studied. However, most existing\nmethods focus primarily on either regular data distributions or low-dimensional\ncases with irregular data. To address these limitations, this paper provides a\ncomprehensive study of DP sparse linear regression with heavy-tailed responses\nin high-dimensional settings. In the first part, we introduce the DP-IHT-H\nmethod, which leverages the Huber loss and private iterative hard thresholding\nto achieve an estimation error bound of \\(\n  \\tilde{O}\\biggl(\n  s^{* \\frac{1 }{2}}\n  \\cdot \\biggl(\\frac{\\log d}{n}\\biggr)^{\\frac{\\zeta}{1 + \\zeta}}\n  +\n  s^{* \\frac{1 + 2\\zeta}{2 + 2\\zeta}}\n  \\cdot \\biggl(\\frac{\\log^2 d}{n \\varepsilon}\\biggr)^{\\frac{\\zeta}{1 + \\zeta}}\n  \\biggr) \\) under the $(\\varepsilon, \\delta)$-DP model, where $n$ is the\nsample size, $d$ is the dimensionality, $s^*$ is the sparsity of the parameter,\nand $\\zeta \\in (0, 1]$ characterizes the tail heaviness of the data. In the\nsecond part, we propose DP-IHT-L, which further improves the error bound under\nadditional assumptions on the response and achieves \\(\n  \\tilde{O}\\Bigl(\\frac{(s^*)^{3/2} \\log d}{n \\varepsilon}\\Bigr). \\) Compared to\nthe first result, this bound is independent of the tail parameter $\\zeta$.\nFinally, through experiments on synthetic and real-world datasets, we\ndemonstrate that our methods outperform standard DP algorithms designed for\n``regular'' data.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u7ef4\u91cd\u5c3e\u54cd\u5e94\u4e0b\u7684\u5dee\u5206\u9690\u79c1\u7a00\u758f\u7ebf\u6027\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5DP-IHT-H\u548cDP-IHT-L\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u6807\u51c6\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5dee\u5206\u9690\u79c1\u7ebf\u6027\u56de\u5f52\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e38\u89c4\u6570\u636e\u5206\u5e03\u6216\u4f4e\u7ef4\u4e0d\u89c4\u5219\u6570\u636e\u60c5\u51b5\uff0c\u5bf9\u4e8e\u9ad8\u7ef4\u91cd\u5c3e\u54cd\u5e94\u7684\u7a00\u758f\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u7f3a\u4e4f\u6df1\u5165\u7814\u7a76\u3002", "method": "1. \u63d0\u51fa\u4e86DP-IHT-H\u65b9\u6cd5\uff0c\u7ed3\u5408Huber\u635f\u5931\u51fd\u6570\u548c\u79c1\u6709\u8fed\u4ee3\u786c\u9608\u503c\u5904\u7406\u6280\u672f\uff1b2. \u63d0\u51fa\u4e86\u6539\u8fdb\u7684DP-IHT-L\u65b9\u6cd5\uff0c\u5728\u989d\u5916\u5047\u8bbe\u6761\u4ef6\u4e0b\u8fdb\u4e00\u6b65\u4f18\u5316\u8bef\u5dee\u8fb9\u754c\u3002", "result": "1. DP-IHT-H\u65b9\u6cd5\u5728(\u03b5, \u03b4)-\u5dee\u5206\u9690\u79c1\u6a21\u578b\u4e0b\u5b9e\u73b0\u4e86\u7279\u5b9a\u5f62\u5f0f\u7684\u4f30\u8ba1\u8bef\u5dee\u8fb9\u754c\uff1b2. DP-IHT-L\u65b9\u6cd5\u5728\u53bb\u9664\u5c3e\u53c2\u6570\u03b6\u7684\u5f71\u54cd\u540e\uff0c\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u8bef\u5dee\u8fb9\u754c\uff1b3. \u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6807\u51c6\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\u3002", "conclusion": "\u672c\u6587\u9488\u5bf9\u9ad8\u7ef4\u91cd\u5c3e\u54cd\u5e94\u4e0b\u7684\u5dee\u5206\u9690\u79c1\u7a00\u758f\u7ebf\u6027\u56de\u5f52\u95ee\u9898\u8fdb\u884c\u4e86\u5168\u9762\u7814\u7a76\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u6539\u5584\u4e86\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.07548", "pdf": "https://arxiv.org/pdf/2506.07548", "abs": "https://arxiv.org/abs/2506.07548", "authors": ["Weiqiang Jin", "Hongyang Du", "Guizhong Liu", "Dong In Kim"], "title": "Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.RO"], "comment": "16 pages; 12figures", "summary": "Multi-agent reinforcement learning (MARL) has achieved strong performance in\ncooperative adversarial tasks. However, most existing methods typically train\nagents against fixed opponent strategies and rely on such meta-static\ndifficulty conditions, which limits their adaptability to changing environments\nand often leads to suboptimal policies. Inspired by the success of curriculum\nlearning (CL) in supervised tasks, we propose a dynamic CL framework for MARL\nthat employs an self-adaptive difficulty adjustment mechanism. This mechanism\ncontinuously modulates opponent strength based on real-time agent training\nperformance, allowing agents to progressively learn from easier to more\nchallenging scenarios. However, the dynamic nature of CL introduces instability\ndue to nonstationary environments and sparse global rewards. To address this\nchallenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),\nwhich is tightly coupled with the curriculum by providing intrinsic credit\nsignals that reflect each agent's impact under evolving task demands. CGRPA\nconstructs a counterfactual advantage function that isolates individual\ncontributions within group behavior, facilitating more reliable policy updates\nthroughout the curriculum. CGRPA evaluates each agent's contribution through\nconstructing counterfactual action advantage function, providing intrinsic\nrewards that enhance credit assignment and stabilize learning under\nnon-stationary conditions. Extensive experiments demonstrate that our method\nimproves both training stability and final performance, achieving competitive\nresults against state-of-the-art methods. The code is available at\nhttps://github.com/NICE-HKU/CL2MARL-SMAC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u6709\u81ea\u9002\u5e94\u96be\u5ea6\u8c03\u6574\u673a\u5236\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408Counterfactual Group Relative Policy Advantage\u65b9\u6cd5\u4ee5\u5e94\u5bf9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u975e\u5e73\u7a33\u73af\u5883\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5728\u56fa\u5b9a\u7684\u5bf9\u624b\u7b56\u7565\u4e0b\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5bf9\u53d8\u5316\u73af\u5883\u7684\u9002\u5e94\u6027\u5e76\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7b56\u7565\u3002\u53d7\u8bfe\u7a0b\u5b66\u4e60\u5728\u76d1\u7763\u4efb\u52a1\u4e2d\u6210\u529f\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u6765\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u81ea\u9002\u5e94\u96be\u5ea6\u8c03\u6574\u673a\u5236\uff0c\u6839\u636e\u5b9e\u65f6\u8bad\u7ec3\u8868\u73b0\u52a8\u6001\u8c03\u8282\u5bf9\u624b\u5f3a\u5ea6\uff0c\u4f7f\u667a\u80fd\u4f53\u9010\u6b65\u4ece\u7b80\u5355\u5230\u590d\u6742\u573a\u666f\u8fdb\u884c\u5b66\u4e60\u3002\u540c\u65f6\u5f00\u53d1\u4e86Counterfactual Group Relative Policy Advantage\uff08CGRPA\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u53cd\u4e8b\u5b9e\u4f18\u52bf\u51fd\u6570\u9694\u79bb\u4e2a\u4f53\u8d21\u732e\uff0c\u63d0\u4f9b\u5185\u5728\u4fe1\u7528\u4fe1\u53f7\u4ee5\u589e\u5f3a\u4fe1\u7528\u5206\u914d\u5e76\u7a33\u5b9a\u975e\u5e73\u7a33\u6761\u4ef6\u4e0b\u7684\u5b66\u4e60\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u53ca\u6700\u7ec8\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u52a8\u6001\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u548cCGRPA\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u975e\u5e73\u7a33\u73af\u5883\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u7a33\u5b9a\u6027\u548c\u6700\u7ec8\u6027\u80fd\u3002"}}
{"id": "2506.06682", "pdf": "https://arxiv.org/pdf/2506.06682", "abs": "https://arxiv.org/abs/2506.06682", "authors": ["Di Lin", "Wanjing Ren", "Xuanbin Li", "Rui Zhang"], "title": "Learning Robust Heterogeneous Graph Representations via Contrastive-Reconstruction under Sparse Semantics", "categories": ["cs.LG"], "comment": null, "summary": "In graph self-supervised learning, masked autoencoders (MAE) and contrastive\nlearning (CL) are two prominent paradigms. MAE focuses on reconstructing masked\nelements, while CL maximizes similarity between augmented graph views. Recent\nstudies highlight their complementarity: MAE excels at local feature capture,\nand CL at global information extraction. Hybrid frameworks for homogeneous\ngraphs have been proposed, but face challenges in designing shared encoders to\nmeet the semantic requirements of both tasks. In semantically sparse scenarios,\nCL struggles with view construction, and gradient imbalance between positive\nand negative samples persists. This paper introduces HetCRF, a novel\ndual-channel self-supervised learning framework for heterogeneous graphs.\nHetCRF uses a two-stage aggregation strategy to adapt embedding semantics,\nmaking it compatible with both MAE and CL. To address semantic sparsity, it\nenhances encoder output for view construction instead of relying on raw\nfeatures, improving efficiency. Two positive sample augmentation strategies are\nalso proposed to balance gradient contributions. Node classification\nexperiments on four real-world heterogeneous graph datasets demonstrate that\nHetCRF outperforms state-of-the-art baselines. On datasets with missing node\nfeatures, such as Aminer and Freebase, at a 40% label rate in node\nclassification, HetCRF improves the Macro-F1 score by 2.75% and 2.2%\nrespectively compared to the second-best baseline, validating its effectiveness\nand superiority.", "AI": {"tldr": "This paper presents HetCRF, a dual-channel self-supervised learning framework for heterogeneous graphs that combines masked autoencoders (MAE) and contrastive learning (CL). It addresses semantic sparsity and gradient imbalance issues, demonstrating superior performance in node classification tasks.", "motivation": "Existing hybrid frameworks for homogeneous graphs face challenges in designing shared encoders to meet the semantic requirements of both MAE and CL. In semantically sparse scenarios, CL struggles with view construction and gradient imbalance between positive and negative samples persists.", "method": "HetCRF uses a two-stage aggregation strategy to adapt embedding semantics, making it compatible with both MAE and CL. To address semantic sparsity, it enhances encoder output for view construction instead of relying on raw features. Two positive sample augmentation strategies are also proposed to balance gradient contributions.", "result": "Node classification experiments on four real-world heterogeneous graph datasets demonstrate that HetCRF outperforms state-of-the-art baselines. On datasets with missing node features, such as Aminer and Freebase, at a 40% label rate in node classification, HetCRF improves the Macro-F1 score by 2.75% and 2.2% respectively compared to the second-best baseline.", "conclusion": "HetCRF is an effective and superior dual-channel self-supervised learning framework for heterogeneous graphs that addresses existing challenges in combining MAE and CL."}}
{"id": "2506.06891", "pdf": "https://arxiv.org/pdf/2506.06891", "abs": "https://arxiv.org/abs/2506.06891", "authors": ["Paulius Sasnauskas", "Yi\u011fit Yal\u0131n", "Goran Radanovi\u0107"], "title": "Can In-Context Reinforcement Learning Recover From Reward Poisoning Attacks?", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "We study the corruption-robustness of in-context reinforcement learning\n(ICRL), focusing on the Decision-Pretrained Transformer (DPT, Lee et al.,\n2023). To address the challenge of reward poisoning attacks targeting the DPT,\nwe propose a novel adversarial training framework, called Adversarially Trained\nDecision-Pretrained Transformer (AT-DPT). Our method simultaneously trains an\nattacker to minimize the true reward of the DPT by poisoning environment\nrewards, and a DPT model to infer optimal actions from the poisoned data. We\nevaluate the effectiveness of our approach against standard bandit algorithms,\nincluding robust baselines designed to handle reward contamination. Our results\nshow that the proposed method significantly outperforms these baselines in\nbandit settings, under a learned attacker. We additionally evaluate AT-DPT on\nan adaptive attacker, and observe similar results. Furthermore, we extend our\nevaluation to the MDP setting, confirming that the robustness observed in\nbandit scenarios generalizes to more complex environments.", "AI": {"tldr": "The paper proposes AT-DPT to enhance corruption-robustness of ICRL against reward poisoning attacks.", "motivation": "To improve the robustness of Decision-Pretrained Transformer (DPT) in in-context reinforcement learning (ICRL) against reward poisoning attacks.", "method": "A novel adversarial training framework named Adversarially Trained Decision-Pretrained Transformer (AT-DPT) is introduced. It trains an attacker to minimize DPT's true reward by poisoning environment rewards, while simultaneously training a DPT model to infer optimal actions from the poisoned data.", "result": "AT-DPT significantly outperforms standard bandit algorithms and robust baselines designed for reward contamination in both bandit settings and MDP settings, even under learned and adaptive attackers.", "conclusion": "AT-DPT enhances the corruption-robustness of ICRL, with its effectiveness demonstrated across various environments."}}
{"id": "2506.07553", "pdf": "https://arxiv.org/pdf/2506.07553", "abs": "https://arxiv.org/abs/2506.07553", "authors": ["Jingchao Wang", "Haote Yang", "Jiang Wu", "Yifan He", "Xingjian Wei", "Yinfan Wang", "Chengjin Liu", "Lingli Ge", "Lijun Wu", "Bin Wang", "Dahua Lin", "Conghui He"], "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition", "categories": ["cs.AI", "q-bio.QM"], "comment": null, "summary": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\nchemical knowledge by converting molecular images into machine-readable\nformats. While recent vision-language models (VLMs) have shown potential in\nthis task, their image-captioning approach often struggles with complex\nmolecular structures and inconsistent annotations. To overcome these\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\ninnovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought}\nmechanism that emulates human reasoning by incrementally parsing molecular\ngraphs through sequential atom-bond predictions, and (2) the data-centric\nprinciple of \\textit{Faithfully Recognize What You've Seen}, which addresses\nthe mismatch between abbreviated structures in images and their expanded\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\nscenarios involving molecular images with functional group abbreviations,\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\npoints, both in SMILES-based and graph-based metrics. We hope that this work\nwill drive OCSR technology to more effectively meet real-world needs, thereby\nadvancing the fields of cheminformatics and AI for Science. We will release\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.", "AI": {"tldr": "The paper presents GTR-Mol-VLM, a new framework for Optical Chemical Structure Recognition (OCSR) that improves the conversion of molecular images into machine-readable formats. It uses a graph traversal mechanism and a data-centric principle to address complex structures and annotation inconsistencies. Experiments show it outperforms other models, especially with functional group abbreviations.", "motivation": "Recent vision-language models have shown potential in OCSR but struggle with complex molecular structures and inconsistent annotations.", "method": "GTR-Mol-VLM features two innovations: 1) Graph Traversal as Visual Chain of Thought for sequential atom-bond predictions, and 2) Faithfully Recognize What You've Seen principle for matching image abbreviations with expanded annotations. Additionally, they created GTR-CoT-1.3M dataset and MolRec-Bench benchmark.", "result": "GTR-Mol-VLM achieves superior results compared to specialist models, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, it outperforms the second-best baseline by approximately 14 percentage points in scenarios involving molecular images with functional group abbreviations.", "conclusion": "This work aims to advance OCSR technology to better meet real-world needs in cheminformatics and AI for Science."}}
{"id": "2506.06916", "pdf": "https://arxiv.org/pdf/2506.06916", "abs": "https://arxiv.org/abs/2506.06916", "authors": ["Stavros Dimou", "Guevara Noubir"], "title": "ARGOS: Anomaly Recognition and Guarding through O-RAN Sensing", "categories": ["cs.NI", "cs.CR"], "comment": null, "summary": "Rogue Base Station (RBS) attacks, particularly those exploiting downgrade\nvulnerabilities, remain a persistent threat as 5G Standalone (SA) deployments\nare still limited and User Equipment (UE) manufacturers continue to support\nlegacy network connectivity. This work introduces ARGOS, a comprehensive O-RAN\ncompliant Intrusion Detection System (IDS) deployed within the Near Real-Time\nRIC, designed to detect RBS downgrade attacks in real time, an area previously\nunexplored within the O-RAN context. The system enhances the 3GPP KPM Service\nModel to enable richer, UE-level telemetry and features a custom xApp that\napplies unsupervised Machine Learning models for anomaly detection.\nDistinctively, the updated KPM Service Model operates on cross-layer features\nextracted from Modem Layer 1 (ML1) logs and Measurement Reports collected\ndirectly from Commercial Off-The-Shelf (COTS) UEs. To evaluate system\nperformance under realistic conditions, a dedicated testbed is implemented\nusing Open5GS, srsRAN, and FlexRIC, and validated against an extensive\nreal-world measurement dataset. Among the evaluated models, the Variational\nAutoencoder (VAE) achieves the best balance of detection performance and\nefficiency, reaching 99.5% Accuracy with only 0.6% False Positives and minimal\nsystem overhead.", "AI": {"tldr": "ARGOS is a new O-RAN compliant Intrusion Detection System (IDS) that can detect Rogue Base Station downgrade attacks in 5G networks using unsupervised Machine Learning models, achieving high accuracy and low false positives.", "motivation": "Rogue Base Station (RBS) attacks exploiting downgrade vulnerabilities are still a significant threat due to limited 5G Standalone deployments and continued support for legacy network connectivity.", "method": "ARGOS enhances the 3GPP KPM Service Model for richer UE-level telemetry and uses a custom xApp with unsupervised Machine Learning models for anomaly detection. It operates on cross-layer features from Modem Layer 1 logs and Measurement Reports collected from COTS UEs.", "result": "The Variational Autoencoder (VAE) model within ARGOS achieves 99.5% Accuracy with only 0.6% False Positives and minimal system overhead when tested against a real-world measurement dataset.", "conclusion": "ARGOS represents a novel approach to detecting RBS downgrade attacks in real time within the O-RAN context, offering effective intrusion detection capabilities."}}
{"id": "2506.07564", "pdf": "https://arxiv.org/pdf/2506.07564", "abs": "https://arxiv.org/abs/2506.07564", "authors": ["Peiran Li", "Xinkai Zou", "Zhuohang Wu", "Ruifeng Li", "Shuo Xing", "Hanwen Zheng", "Zhikai Hu", "Yuping Wang", "Haoxi Li", "Qin Yuan", "Yingmo Zhang", "Zhengzhong Tu"], "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.", "AI": {"tldr": "SAFEOFLOW\u662f\u4e00\u79cd\u65b0\u7684\u534f\u8bae\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u4fe1\u7684\u57fa\u4e8eLLM/VLM\u7684\u4ee3\u7406\u3002\u5b83\u901a\u8fc7\u7cbe\u7ec6\u7684\u4fe1\u606f\u6d41\u63a7\u5236\u3001\u4e8b\u52a1\u6267\u884c\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u5b89\u5168\u8c03\u5ea6\u7b49\u673a\u5236\uff0c\u786e\u4fdd\u4ee3\u7406\u5728\u5bf9\u6297\u6027\u3001\u5608\u6742\u548c\u5e76\u53d1\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528SAFEOFLOW\u6784\u5efa\u7684\u4ee3\u7406\u5728\u654c\u5bf9\u73af\u5883\u4e2d\u4ecd\u80fd\u4fdd\u6301\u51fa\u8272\u7684\u6027\u80fd\u548c\u5b89\u5168\u4fdd\u969c\u3002", "motivation": "\u5c3d\u7ba1\u5f53\u524d\u7684\u4ee3\u7406\u6846\u67b6\u5177\u6709\u5f3a\u5927\u7684\u529f\u80fd\uff0c\u4f46\u5b83\u4eec\u5728\u5b89\u5168\u4fe1\u606f\u6d41\u3001\u53ef\u9760\u6027\u4ee5\u53ca\u591a\u4ee3\u7406\u534f\u8c03\u65b9\u9762\u4ecd\u7136\u5b58\u5728\u4e0d\u8db3\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5f25\u8865\u8fd9\u4e9b\u7f3a\u9677\u7684\u65b0\u6846\u67b6\u3002", "method": "SAFEOFLOW\u91c7\u7528\u7cbe\u7ec6\u7684\u4fe1\u606f\u6d41\u63a7\u5236\uff08IFC\uff09\u6765\u8ddf\u8e2a\u6570\u636e\u7684\u51fa\u5904\u3001\u5b8c\u6574\u6027\u548c\u4fdd\u5bc6\u6027\uff1b\u9650\u5236LLM\u63a8\u7406\u4ee5\u5c0a\u91cd\u8fd9\u4e9b\u5b89\u5168\u6807\u7b7e\uff1b\u5f15\u5165\u4e8b\u52a1\u6267\u884c\u3001\u51b2\u7a81\u89e3\u51b3\u548c\u5b89\u5168\u8c03\u5ea6\u673a\u5236\u4ee5\u589e\u5f3a\u5728\u5e76\u53d1\u591a\u4ee3\u7406\u73af\u5883\u4e2d\u7684\u7a33\u5065\u6027\uff1b\u5e76\u901a\u8fc7\u9884\u5199\u65e5\u5fd7\u3001\u56de\u6eda\u548c\u5b89\u5168\u7f13\u5b58\u7b49\u673a\u5236\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5bf9\u8fd0\u884c\u65f6\u9519\u8bef\u548c\u7b56\u7565\u8fdd\u89c4\u7684\u5f39\u6027\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528SAFEOFLOW\u6784\u5efa\u7684\u4ee3\u7406\u5373\u4f7f\u5728\u654c\u5bf9\u73af\u5883\u4e2d\u4e5f\u80fd\u4fdd\u6301\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u4efb\u52a1\u6027\u80fd\u548c\u5b89\u5168\u4fdd\u8bc1\uff0c\u5e76\u4e14\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "SAFEOFLOW\u53ca\u5176\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6SAFEOFLOWBENCH\u4e3a\u5efa\u7acb\u539f\u5219\u6027\u3001\u5065\u58ee\u6027\u548c\u5b89\u5168\u6027\u7684\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u53ef\u9760\u81ea\u4e3b\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.06699", "pdf": "https://arxiv.org/pdf/2506.06699", "abs": "https://arxiv.org/abs/2506.06699", "authors": ["Rajeev Bhatt Ambati", "James Lester", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "MarginSel : Max-Margin Demonstration Selection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at few-shot learning via in-context\nlearning (ICL). However, the effectiveness of ICL is often sensitive to the\nselection and ordering of demonstration examples. To address this, we present\nMarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that\nselects hard demonstration examples for the ICL prompt, adapting to each test\ninstance. Our approach achieves 2-7% absolute improvement in F1-score across\nclassification tasks, compared to a random selection of examples. We also\nprovide theoretical insights and empirical evidence showing that MarginSel\ninduces max-margin behavior in LLMs by effectively increasing the margin for\nhard examples, analogous to support vectors, thereby shifting the decision\nboundary in a beneficial direction.", "AI": {"tldr": "MarginSel is a two-step method for selecting hard demonstration examples for LLMs' ICL prompt, achieving 2-7% F1-score improvement in classification tasks.", "motivation": "The effectiveness of ICL in LLMs is sensitive to the selection and ordering of demonstration examples.", "method": "MarginSel: Max-Margin Demonstration Selection for LLMs, selects hard demonstration examples adapting to each test instance.", "result": "Achieves 2-7% absolute improvement in F1-score across classification tasks compared to random selection.", "conclusion": "MarginSel induces max-margin behavior in LLMs by effectively increasing the margin for hard examples, shifting the decision boundary beneficially."}}
{"id": "2506.06933", "pdf": "https://arxiv.org/pdf/2506.06933", "abs": "https://arxiv.org/abs/2506.06933", "authors": ["Mahdi Salmani", "Alireza Abdollahpoorrostam", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": null, "summary": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings.", "AI": {"tldr": "The paper proposes a framework for decision-based attacks under asymmetric query costs, including Asymmetric Search and Asymmetric Gradient Estimation, which can lower total query cost and perturbations.", "motivation": "Traditional decision-based black-box adversarial attacks aim to generate adversarial examples with low query numbers, but they assume equal query costs. In practice, queries may incur asymmetric costs. Current algorithms for this scenario are underdeveloped.", "method": "The authors modify two core components of existing attacks: the search strategy and the gradient estimation process. They propose Asymmetric Search (AS), a conservative variant of binary search reducing reliance on high-cost queries, and Asymmetric Gradient Estimation (AGREST), which shifts the sampling distribution to favor low-cost queries. These methods balance different query types to minimize total attack cost.", "result": "Through theoretical analysis and empirical evaluation on standard image classification benchmarks, the proposed method consistently achieves lower total query cost and smaller perturbations than existing approaches, with improvements up to 40% in some settings.", "conclusion": "The proposed framework for decision-based attacks under asymmetric query costs successfully reduces total attack cost and perturbations. It can be integrated into various existing black-box attacks with minimal changes."}}
{"id": "2506.07591", "pdf": "https://arxiv.org/pdf/2506.07591", "abs": "https://arxiv.org/abs/2506.07591", "authors": ["Shang Qu", "Ning Ding", "Linhai Xie", "Yifei Li", "Zaoqu Liu", "Kaiyan Zhang", "Yibai Xiong", "Yuxin Zuo", "Zhangren Chen", "Ermo Hua", "Xingtai Lv", "Youbang Sun", "Yang Li", "Dong Li", "Fuchu He", "Bowen Zhou"], "title": "Automating Exploratory Multiomics Research via Language Models", "categories": ["cs.AI", "q-bio.QM"], "comment": null, "summary": "This paper introduces PROTEUS, a fully automated system that produces\ndata-driven hypotheses from raw data files. We apply PROTEUS to clinical\nproteogenomics, a field where effective downstream data analysis and hypothesis\nproposal is crucial for producing novel discoveries. PROTEUS uses separate\nmodules to simulate different stages of the scientific process, from open-ended\ndata exploration to specific statistical analysis and hypothesis proposal. It\nformulates research directions, tools, and results in terms of relationships\nbetween biological entities, using unified graph structures to manage complex\nresearch processes. We applied PROTEUS to 10 clinical multiomics datasets from\npublished research, arriving at 360 total hypotheses. Results were evaluated\nthrough external data validation and automatic open-ended scoring. Through\nexploratory and iterative research, the system can navigate high-throughput and\nheterogeneous multiomics data to arrive at hypotheses that balance reliability\nand novelty. In addition to accelerating multiomic analysis, PROTEUS represents\na path towards tailoring general autonomous systems to specialized scientific\ndomains to achieve open-ended hypothesis generation from data.", "AI": {"tldr": "This paper presents PROTEUS,an automated system for generating data-driven hypotheses from raw data files in clinical proteogenomics.It uses graph structures to manage research processes and was applied to 10 datasets,resulting in 360 hypotheses that balance reliability and novelty.", "motivation": "To create a fully automated system capable of producing reliable and novel hypotheses from complex multiomics data in clinical proteogenomics,to accelerate discoveries in this field.", "method": "PROTEUS employs separate modules simulating different scientific process stages,from open-ended data exploration to specific statistical analysis.It formulates research using unified graph structures to handle relationships between biological entities.", "result": "PROTEUS analyzed 10 clinical multiomics datasets,generating 360 hypotheses which were validated through external data and scored positively for balancing reliability and novelty.", "conclusion": "PROTEUS successfully accelerates multiomic analysis and demonstrates potential for adapting general autonomous systems to specialized scientific areas for open-ended hypothesis generation."}}
{"id": "2506.06701", "pdf": "https://arxiv.org/pdf/2506.06701", "abs": "https://arxiv.org/abs/2506.06701", "authors": ["Fudong Lin", "Wanrou Du", "Jinchan Liu", "Tarikul Milon", "Shelby Meche", "Wu Xu", "Xiaoqi Qin", "Xu Yuan"], "title": "Do Protein Transformers Have Biological Intelligence?", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "Accepted by European Conference on Machine Learning and Principles\n  and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)", "summary": "Deep neural networks, particularly Transformers, have been widely adopted for\npredicting the functional properties of proteins. In this work, we focus on\nexploring whether Protein Transformers can capture biological intelligence\namong protein sequences. To achieve our goal, we first introduce a protein\nfunction dataset, namely Protein-FN, providing over 9000 protein data with\nmeaningful labels. Second, we devise a new Transformer architecture, namely\nSequence Protein Transformers (SPT), for computationally efficient protein\nfunction predictions. Third, we develop a novel Explainable Artificial\nIntelligence (XAI) technique called Sequence Score, which can efficiently\ninterpret the decision-making processes of protein models, thereby overcoming\nthe difficulty of deciphering biological intelligence bided in Protein\nTransformers. Remarkably, even our smallest SPT-Tiny model, which contains only\n5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%\non the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,\nall accomplished by training from scratch. Besides, our Sequence Score\ntechnique helps reveal that our SPT models can discover several meaningful\npatterns underlying the sequence structures of protein data, with these\npatterns aligning closely with the domain knowledge in the biology community.\nWe have officially released our Protein-FN dataset on Hugging Face Datasets\nhttps://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at\nhttps://github.com/fudong03/BioIntelligence.", "AI": {"tldr": "This paper explores if Protein Transformers can capture biological intelligence in protein sequences, introduces a new dataset Protein-FN, devises a new Transformer architecture SPT, and develops a XAI technique Sequence Score. Even the smallest model SPT-Tiny shows high predictive accuracy.", "motivation": "To explore whether Protein Transformers can capture biological intelligence among protein sequences.", "method": "Introduced Protein-FN dataset with over 9000 protein data, devised a new Transformer architecture called Sequence Protein Transformers (SPT), and developed a novel XAI technique called Sequence Score.", "result": "The smallest model SPT-Tiny achieved 94.3% accuracy on the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset, all trained from scratch. Sequence Score revealed meaningful patterns in protein sequence structures aligned with biological domain knowledge.", "conclusion": "Protein Transformers can effectively capture biological intelligence. The newly introduced dataset, architecture, and XAI technique demonstrate promising results in protein function prediction and interpretation."}}
{"id": "2506.06971", "pdf": "https://arxiv.org/pdf/2506.06971", "abs": "https://arxiv.org/abs/2506.06971", "authors": ["Jaechul Roh", "Varun Gandhi", "Shivani Anilkumar", "Arin Garg"], "title": "Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in tasks\nrequiring complex reasoning, such as code generation, mathematical problem\nsolving, and algorithmic synthesis -- especially when aided by reasoning tokens\nand Chain-of-Thought prompting. Yet, a core question remains: do these models\ntruly reason, or do they merely exploit shallow statistical patterns? In this\npaper, we systematically investigate the robustness of reasoning LLMs by\nintroducing a suite of semantically faithful yet adversarially structured\nprompt perturbations. Our evaluation -- spanning 700 perturbed code generations\nderived from LeetCode-style problems -- applies transformations such as\nstorytelling reframing, irrelevant constraint injection, example reordering,\nand numeric perturbation. We observe that while certain modifications severely\ndegrade performance (with accuracy drops up to -42.1%), others surprisingly\nimprove model accuracy by up to 35.3%, suggesting sensitivity not only to\nsemantics but also to surface-level prompt dynamics. These findings expose the\nfragility and unpredictability of current reasoning systems, underscoring the\nneed for more principles approaches to reasoning alignments and prompting\nrobustness. We release our perturbation datasets and evaluation framework to\npromote further research in trustworthy and resilient LLM reasoning.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5bf9\u6297\u6027\u6270\u52a8\u63d0\u793a\uff0c\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5bf9\u63d0\u793a\u7684\u8868\u9762\u7279\u6027\u654f\u611f\uff0c\u5e76\u4e14\u67d0\u4e9b\u6270\u52a8\u4f1a\u663e\u8457\u964d\u4f4e\u6216\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u8fd9\u8868\u660e\u5f53\u524d\u63a8\u7406\u7cfb\u7edf\u7684\u8106\u5f31\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u771f\u6b63\u5177\u5907\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u6d45\u5c42\u7edf\u8ba1\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7cfb\u7edf\u5730\u7814\u7a76\u8fd9\u4e9b\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u7814\u7a76\u8005\u8bbe\u8ba1\u4e86\u4e00\u5957\u8bed\u4e49\u5fe0\u5b9e\u4f46\u5177\u6709\u5bf9\u6297\u6027\u7684\u63d0\u793a\u6270\u52a8\uff0c\u5e76\u5e94\u7528\u4e8e\u6e90\u81eaLeetCode\u98ce\u683c\u95ee\u9898\u7684700\u4e2a\u6270\u52a8\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u3002\u6270\u52a8\u5305\u62ec\u6545\u4e8b\u91cd\u8ff0\u3001\u65e0\u5173\u7ea6\u675f\u6ce8\u5165\u3001\u793a\u4f8b\u91cd\u65b0\u6392\u5e8f\u548c\u6570\u503c\u6270\u52a8\u7b49\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u67d0\u4e9b\u6270\u52a8\u4f1a\u4e25\u91cd\u964d\u4f4e\u6a21\u578b\u6027\u80fd\uff08\u51c6\u786e\u7387\u4e0b\u964d\u6700\u9ad8\u8fbe42.1%\uff09\uff0c\u800c\u53e6\u4e00\u4e9b\u5219\u610f\u5916\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u7387\uff08\u6700\u9ad8\u63d0\u534735.3%\uff09\u3002", "conclusion": "\u5f53\u524d\u63a8\u7406\u7cfb\u7edf\u5b58\u5728\u8106\u5f31\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u63a8\u7406\u5bf9\u9f50\u548c\u63d0\u793a\u9c81\u68d2\u6027\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u8005\u516c\u5f00\u4e86\u6270\u52a8\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4ee5\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2506.07636", "pdf": "https://arxiv.org/pdf/2506.07636", "abs": "https://arxiv.org/abs/2506.07636", "authors": ["Haoran Wang", "Zhenyu Hou", "Yao Wei", "Jie Tang", "Yuxiao Dong"], "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling", "categories": ["cs.AI"], "comment": "Accepted to Findings of ACL'25", "summary": "Large language models (LLMs) have advanced rapidly from conversational\nproblem solving to addressing real-world tasks involving tool use, such as\nsoftware engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex\nand Cursor, have offered end-to-end automation of the software development\nprocess. However, building effective SWE agents remains challenging due to the\nlack of high-quality training data and effective test cases. To address this\nissue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we\ndevelop a robust pipeline to synthesize test cases for patch evaluation.\nSecond, we scale up agent trajectories to construct the training data for\nbuilding SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the\nSWE-Dev models can achieve top performance among all open SWE agents.\nSpecifically, the success rates of the SWE-Dev 7B and 32B parameter models\nreach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source\nmodels. All code, models, and datasets are publicly available at\nhttps://github.com/THUDM/SWE-Dev.", "AI": {"tldr": "SWE-Dev is an SWE agent built upon open-source LLMs, with a robust pipeline for test case synthesis and scaled-up agent trajectories for training data construction. It achieves top performance on the SWE-bench-Verified benchmark.", "motivation": "Building effective SWE agents is challenging due to the lack of high-quality training data and effective test cases.", "method": "Develop a robust pipeline for test case synthesis and scale up agent trajectories to construct the training data.", "result": "Experiments show that SWE-Dev models outperform state-of-the-art open-source models with success rates of 23.4% and 36.6% respectively.", "conclusion": "All code, models, and datasets are publicly available."}}
{"id": "2506.06715", "pdf": "https://arxiv.org/pdf/2506.06715", "abs": "https://arxiv.org/abs/2506.06715", "authors": ["Minh-Duc Nguyen", "Dung D. Le"], "title": "A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks", "categories": ["cs.LG", "stat.ML"], "comment": "Paper is under review", "summary": "Pareto Set Learning (PSL) is popular as an efficient approach to obtaining\nthe complete optimal solution in Multi-objective Learning (MOL). A set of\noptimal solutions approximates the Pareto set, and its mapping is a set of\ndense points in the Pareto front in objective space. However, some current\nmethods face a challenge: how to make the Pareto solution is diverse while\nmaximizing the hypervolume value. In this paper, we propose a novel method to\naddress this challenge, which employs Stein Variational Gradient Descent (SVGD)\nto approximate the entire Pareto set. SVGD pushes a set of particles towards\nthe Pareto set by applying a form of functional gradient descent, which helps\nto converge and diversify optimal solutions. Additionally, we employ diverse\ngradient direction strategies to thoroughly investigate a unified framework for\nSVGD in multi-objective optimization and adapt this framework with an annealing\nschedule to promote stability. We introduce our method, SVH-MOL, and validate\nits effectiveness through extensive experiments on multi-objective problems and\nmulti-task learning, demonstrating its superior performance.", "AI": {"tldr": "The paper proposes SVH-MOL, a method using Stein Variational Gradient Descent (SVGD) to approximate the Pareto set in Multi-objective Learning, ensuring diverse solutions and maximized hypervolume. It's validated through experiments.", "motivation": "Current methods struggle to make Pareto solutions diverse while maximizing the hypervolume value in Multi-objective Learning.", "method": "Proposes SVH-MOL which uses SVGD to push particles towards the Pareto set with functional gradient descent, employing diverse gradient direction strategies within a unified framework adapted with an annealing schedule for stability.", "result": "Demonstrates superior performance in extensive experiments on multi-objective problems and multi-task learning.", "conclusion": "SVH-MOL effectively addresses the challenge of diversity and hypervolume maximization in Pareto solutions."}}
{"id": "2506.06985", "pdf": "https://arxiv.org/pdf/2506.06985", "abs": "https://arxiv.org/abs/2506.06985", "authors": ["Anastasia Koloskova", "Youssef Allouah", "Animesh Jha", "Rachid Guerraoui", "Sanmi Koyejo"], "title": "Certified Unlearning for Neural Networks", "categories": ["cs.LG", "cs.CR", "stat.ML"], "comment": null, "summary": "We address the problem of machine unlearning, where the goal is to remove the\ninfluence of specific training data from a model upon request, motivated by\nprivacy concerns and regulatory requirements such as the \"right to be\nforgotten.\" Unfortunately, existing methods rely on restrictive assumptions or\nlack formal guarantees. To this end, we propose a novel method for certified\nmachine unlearning, leveraging the connection between unlearning and privacy\namplification by stochastic post-processing. Our method uses noisy fine-tuning\non the retain data, i.e., data that does not need to be removed, to ensure\nprovable unlearning guarantees. This approach requires no assumptions about the\nunderlying loss function, making it broadly applicable across diverse settings.\nWe analyze the theoretical trade-offs in efficiency and accuracy and\ndemonstrate empirically that our method not only achieves formal unlearning\nguarantees but also performs effectively in practice, outperforming existing\nbaselines. Our code is available at\nhttps://github.com/stair-lab/certified-unlearningneural-networks-icml-2025", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u8bc1\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fdb\u884c\u5e26\u566a\u5fae\u8c03\uff0c\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u9057\u5fd8\u4fdd\u8bc1\uff0c\u65e0\u9700\u5bf9\u635f\u5931\u51fd\u6570\u505a\u4efb\u4f55\u5047\u8bbe\uff0c\u540c\u65f6\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u8fdb\u884c\u4e86\u7406\u8bba\u6743\u8861\u5206\u6790\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u968f\u7740\u9690\u79c1\u95ee\u9898\u548c\u6cd5\u89c4\uff08\u5982\u201c\u88ab\u9057\u5fd8\u6743\u201d\uff09\u7684\u8981\u6c42\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5177\u5907\u5728\u8bf7\u6c42\u4e0b\u79fb\u9664\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u5f71\u54cd\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u9057\u5fd8\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e25\u683c\u7684\u5047\u8bbe\uff0c\u8981\u4e48\u7f3a\u4e4f\u6b63\u5f0f\u7684\u4fdd\u8bc1\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u8bc1\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u5229\u7528\u9057\u5fd8\u4e0e\u901a\u8fc7\u968f\u673a\u540e\u5904\u7406\u653e\u5927\u9690\u79c1\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u5728\u4fdd\u7559\u6570\u636e\u4e0a\u8fdb\u884c\u5e26\u566a\u5fae\u8c03\uff0c\u4ece\u800c\u786e\u4fdd\u53ef\u8bc1\u660e\u7684\u9057\u5fd8\u4fdd\u8bc1\u3002\u6b64\u65b9\u6cd5\u4e0d\u9700\u8981\u5bf9\u5e95\u5c42\u635f\u5931\u51fd\u6570\u505a\u51fa\u4efb\u4f55\u5047\u8bbe\uff0c\u56e0\u6b64\u9002\u7528\u4e8e\u5404\u79cd\u4e0d\u540c\u7684\u573a\u666f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6b63\u5f0f\u7684\u9057\u5fd8\u4fdd\u8bc1\uff0c\u800c\u4e14\u5728\u5b9e\u8df5\u4e2d\u4e5f\u8868\u73b0\u5f97\u975e\u5e38\u6709\u6548\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u8bc1\u673a\u5668\u9057\u5fd8\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u53ef\u8bc1\u660e\u7684\u9057\u5fd8\u4fdd\u8bc1\uff0c\u540c\u65f6\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u8fdb\u884c\u4e86\u6743\u8861\u3002"}}
{"id": "2506.07672", "pdf": "https://arxiv.org/pdf/2506.07672", "abs": "https://arxiv.org/abs/2506.07672", "authors": ["Yunhe Yan", "Shihe Wang", "Jiajun Du", "Yexuan Yang", "Yuxuan Shan", "Qichen Qiu", "Xianqing Jia", "Xinge Wang", "Xin Yuan", "Xu Han", "Mao Qin", "Yinxiao Chen", "Chen Peng", "Shangguang Wang", "Mengwei Xu"], "title": "MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents", "categories": ["cs.AI"], "comment": null, "summary": "(M)LLM-powered computer use agents (CUA) are emerging as a transformative\ntechnique to automate human-computer interaction. However, existing CUA\nbenchmarks predominantly target GUI agents, whose evaluation methods are\nsusceptible to UI changes and ignore function interactions exposed by\napplication APIs, e.g., Model Context Protocol (MCP). To this end, we propose\nMCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid\nagents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those\nwith source code availability and can be revised/re-compiled as needed (e.g.,\nadding MCP support), with two notable advantages:\n  (1) It greatly broadens the design space of CUA, such as what and how the app\nfeatures to be exposed/extracted as CUA-callable APIs.\n  (2) It allows MCPWorld to programmatically verify task completion by directly\nmonitoring application behavior through techniques like dynamic code\ninstrumentation, offering robust, accurate CUA evaluation decoupled from\nspecific agent implementations or UI states.\n  Currently, MCPWorld includes 201 well curated and annotated user tasks,\ncovering diversified use cases and difficulty levels. MCPWorld is also fully\ncontainerized with GPU acceleration support for flexible adoption on different\nOS/hardware environments. Our preliminary experiments, using a representative\nLLM-powered CUA framework, achieve 75.12% task completion accuracy,\nsimultaneously providing initial evidence on the practical effectiveness of\nagent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate\nand standardize the benchmarking of next-generation computer use agents that\ncan leverage rich external tools. Our code and dataset are publicly available\nat https://github.com/SAAgent/MCPWorld.", "AI": {"tldr": "(M)LLM\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406(CUA)\u6b63\u5728\u6210\u4e3a\u81ea\u52a8\u5316\u4eba\u673a\u4ea4\u4e92\u7684\u4e00\u79cd\u53d8\u9769\u6027\u6280\u672f\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684CUA\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9GUI\u4ee3\u7406\uff0c\u5176\u8bc4\u4f30\u65b9\u6cd5\u5bb9\u6613\u53d7\u5230UI\u53d8\u5316\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u5e94\u7528\u7a0b\u5e8fAPI\u66b4\u9732\u7684\u529f\u80fd\u4ea4\u4e92\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86MCPWorld\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u81ea\u52a8CUA\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9002\u7528\u4e8eAPI\u3001GUI\u548cAPI-GUI\u6df7\u5408\u4ee3\u7406\u3002MCPWorld\u7684\u6838\u5fc3\u539f\u5219\u662f\u4f7f\u7528\u201c\u767d\u76d2\u5e94\u7528\u201d\uff0c\u5373\u90a3\u4e9b\u5177\u6709\u6e90\u4ee3\u7801\u53ef\u7528\u6027\u5e76\u53ef\u6839\u636e\u9700\u8981\u8fdb\u884c\u4fee\u6539/\u91cd\u65b0\u7f16\u8bd1\u7684\u5e94\u7528\u7a0b\u5e8f\uff08\u4f8b\u5982\uff0c\u6dfb\u52a0MCP\u652f\u6301\uff09\u3002\u76ee\u524d\uff0cMCPWorld\u5305\u542b201\u4e2a\u7cbe\u5fc3\u7b56\u5212\u548c\u6ce8\u91ca\u7684\u7528\u6237\u4efb\u52a1\uff0c\u6db5\u76d6\u591a\u6837\u5316\u7684\u7528\u4f8b\u548c\u96be\u5ea6\u7ea7\u522b\u3002MCPWorld\u8fd8\u5b8c\u5168\u5bb9\u5668\u5316\uff0c\u5e76\u652f\u6301GPU\u52a0\u901f\uff0c\u4ee5\u4fbf\u5728\u4e0d\u540c\u7684OS/\u786c\u4ef6\u73af\u5883\u4e2d\u7075\u6d3b\u91c7\u7528\u3002\u6211\u4eec\u7684\u521d\u6b65\u5b9e\u9a8c\u4f7f\u7528\u4e86\u4e00\u4e2a\u4ee3\u8868\u6027\u7684(M)LLM\u9a71\u52a8\u7684CUA\u6846\u67b6\uff0c\u5b9e\u73b0\u4e8675.12%\u7684\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u5229\u7528MCP\u5b9e\u73b0\u4ee3\u7406\u81ea\u52a8\u5316\u7684\u5b9e\u9645\u6709\u6548\u6027\u7684\u521d\u6b65\u8bc1\u636e\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u9884\u8ba1MCPWorld\u5c06\u6709\u52a9\u4e8e\u4fc3\u8fdb\u548c\u6807\u51c6\u5316\u4e0b\u4e00\u4ee3CUA\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8fd9\u4e9bCUA\u53ef\u4ee5\u5229\u7528\u4e30\u5bcc\u7684\u5916\u90e8\u5de5\u5177\u3002", "motivation": "\u5f53\u524dCUA\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u9488\u5bf9GUI\u4ee3\u7406\uff0c\u8bc4\u4f30\u65b9\u6cd5\u5bb9\u6613\u53d7UI\u53d8\u5316\u5f71\u54cd\u4e14\u5ffd\u7565\u5e94\u7528\u7a0b\u5e8fAPI\u529f\u80fd\u4ea4\u4e92\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u8bc4\u4f30\u4e0d\u540c\u7c7b\u578b\u7684CUA\u3002", "method": "\u63d0\u51faMCPWorld\u4f5c\u4e3a\u9996\u4e2a\u81ea\u52a8CUA\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f7f\u7528\u201c\u767d\u76d2\u5e94\u7528\u201d\u4ee5\u62d3\u5bbdCUA\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u4ee3\u7801 instrumentation \u7b49\u6280\u672f\u76f4\u63a5\u76d1\u63a7\u5e94\u7528\u7a0b\u5e8f\u884c\u4e3a\u6765\u9a8c\u8bc1\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4ee3\u8868\u6027(M)LLM\u9a71\u52a8\u7684CUA\u6846\u67b6\u53ef\u5b9e\u73b075.12%\u7684\u4efb\u52a1\u5b8c\u6210\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5229\u7528MCP\u5b9e\u73b0\u4ee3\u7406\u81ea\u52a8\u5316\u7684\u5b9e\u9645\u6709\u6548\u6027\u3002", "conclusion": "MCPWorld\u6709\u671b\u4fc3\u8fdb\u548c\u6807\u51c6\u5316\u4e0b\u4e00\u4ee3CUA\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u7075\u6d3b\u7684OS/\u786c\u4ef6\u73af\u5883\u90e8\u7f72\uff0c\u5e76\u516c\u5f00\u63d0\u4f9b\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2506.06761", "pdf": "https://arxiv.org/pdf/2506.06761", "abs": "https://arxiv.org/abs/2506.06761", "authors": ["Adri\u00e0 Molina Rodr\u00edguez", "Oriol Ramos Terrades", "Josep Llad\u00f3s"], "title": "The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.", "AI": {"tldr": "This paper focuses on building models that can generalize to new data distributions, like alphabets, more effectively than traditional fine-tune strategies, particularly for low-resource languages.", "motivation": "There is a lack of consideration for low-resource languages in current recognition systems due to limited data availability. The authors aim to address this gap by creating models that can adapt to new data distributions faster and more efficiently.", "method": "The method leverages recent advancements in model editing to incorporate unseen scripts (low-resource learning). It contrasts with state-of-the-art meta-learning by demonstrating the effectiveness of domain merging in sparse data distributions without needing specific relations to overall data distribution or prototyping.", "result": "Experiments indicate significant performance improvements in transfer learning to new alphabets and out-of-domain evaluations, even with challenging domain shifts such as historical ciphered texts and non-Latin scripts.", "conclusion": "The research introduces a novel approach to building models capable of easily adopting under-represented alphabets, thereby expanding document recognition capabilities to more contexts and cultures."}}
{"id": "2506.07022", "pdf": "https://arxiv.org/pdf/2506.07022", "abs": "https://arxiv.org/abs/2506.07022", "authors": ["Leheng Sheng", "Changshuo Shen", "Weixiang Zhao", "Junfeng Fang", "Xiaohao Liu", "Zhenkai Liang", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "As LLMs are increasingly deployed in real-world applications, ensuring their\nability to refuse malicious prompts, especially jailbreak attacks, is essential\nfor safe and reliable use. Recently, activation steering has emerged as an\neffective approach for enhancing LLM safety by adding a refusal direction\nvector to internal activations of LLMs during inference, which will further\ninduce the refusal behaviors of LLMs. However, indiscriminately applying\nactivation steering fundamentally suffers from the trade-off between safety and\nutility, since the same steering vector can also lead to over-refusal and\ndegraded performance on benign prompts. Although prior efforts, such as vector\ncalibration and conditional steering, have attempted to mitigate this\ntrade-off, their lack of theoretical grounding limits their robustness and\neffectiveness. To better address the trade-off between safety and utility, we\npresent a theoretically grounded and empirically effective activation steering\nmethod called AlphaSteer. Specifically, it considers activation steering as a\nlearnable process with two principled learning objectives: utility preservation\nand safety enhancement. For utility preservation, it learns to construct a\nnearly zero vector for steering benign data, with the null-space constraints.\nFor safety enhancement, it learns to construct a refusal direction vector for\nsteering malicious data, with the help of linear regression. Experiments across\nmultiple jailbreak attacks and utility benchmarks demonstrate the effectiveness\nof AlphaSteer, which significantly improves the safety of LLMs without\ncompromising general capabilities. Our codes are available at\nhttps://github.com/AlphaLab-USTC/AlphaSteer.", "AI": {"tldr": "To address the trade-off between safety and utility in LLMs when refusing malicious prompts, the paper introduces AlphaSteer, a theoretically grounded activation steering method that enhances safety without degrading performance on benign prompts.", "motivation": "The motivation of this paper is to solve the problem of indiscriminately applying activation steering in large language models (LLMs), which can lead to over-refusal and degraded performance on benign prompts. Existing methods lack theoretical grounding, limiting their robustness and effectiveness.", "method": "The paper proposes AlphaSteer, a method that treats activation steering as a learnable process with two learning objectives: utility preservation and safety enhancement. For utility preservation, it constructs a nearly zero vector for benign data using null-space constraints. For safety enhancement, it constructs a refusal direction vector for malicious data using linear regression.", "result": "Experiments across multiple jailbreak attacks and utility benchmarks demonstrate the effectiveness of AlphaSteer. It significantly improves the safety of LLMs without compromising their general capabilities.", "conclusion": "AlphaSteer provides a theoretically grounded and empirically effective solution to enhance the safety of LLMs while preserving their utility. The codes are available at https://github.com/AlphaLab-USTC/AlphaSteer."}}
{"id": "2506.07731", "pdf": "https://arxiv.org/pdf/2506.07731", "abs": "https://arxiv.org/abs/2506.07731", "authors": ["Mouadh Yagoubi", "Yasser Dahou", "Billel Mokeddem", "Younes Belkada", "Phuc H. Le-Khac", "Basma El Amel Boussaha", "Reda Alami", "Jingwei Zuo", "Damiano Marsili", "Mugariya Farooq", "Mounia Lalmas", "Georgia Gkioxari", "Patrick Gallinari", "Philip Torr", "Hakim Hacid"], "title": "NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Existing benchmarks have proven effective for assessing the performance of\nfully trained large language models. However, we find striking differences in\nthe early training stages of small models, where benchmarks often fail to\nprovide meaningful or discriminative signals. To explore how these differences\narise, this competition tackles the challenge of designing scientific knowledge\nevaluation tasks specifically tailored for measuring early training progress of\nlanguage models. Participants are invited to develop novel evaluation\nmethodologies or adapt existing benchmarks to better capture performance\ndifferences among language models. To support this effort, we provide three\npre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate\ncheckpoints sampled during training up to 200B tokens. All experiments and\ndevelopment work can be run on widely available free cloud-based GPU platforms,\nmaking participation accessible to researchers with limited computational\nresources. Submissions will be evaluated based on three criteria: the quality\nof the performance signal they produce, the consistency of model rankings at 1\ntrillion tokens of training, and their relevance to the scientific knowledge\ndomain. By promoting the design of tailored evaluation strategies for early\ntraining, this competition aims to attract a broad range of participants from\nvarious disciplines, including those who may not be machine learning experts or\nhave access to dedicated GPU resources. Ultimately, this initiative seeks to\nmake foundational LLM research more systematic and benchmark-informed from the\nearliest phases of model development.", "AI": {"tldr": "Existing benchmarks fail to provide meaningful signals for small models' early training stages. This competition designs scientific knowledge evaluation tasks to measure early training progress of language models.", "motivation": "To address the lack of meaningful or discriminative signals provided by benchmarks in the early training stages of small models.", "method": "Participants develop novel evaluation methodologies or adapt existing benchmarks to better capture performance differences among language models using three pre-trained small models and intermediate checkpoints sampled during training up to 200B tokens.", "result": "Submissions will be evaluated based on three criteria: the quality of the performance signal they produce, the consistency of model rankings at 1 trillion tokens of training, and their relevance to the scientific knowledge domain.", "conclusion": "This initiative seeks to make foundational LLM research more systematic and benchmark-informed from the earliest phases of model development."}}
{"id": "2506.06782", "pdf": "https://arxiv.org/pdf/2506.06782", "abs": "https://arxiv.org/abs/2506.06782", "authors": ["Qinting Jiang", "Chuyang Ye", "Dongyan Wei", "Bingli Wang", "Yuan Xue", "Jingyan Jiang", "Zhi Wang"], "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite progress, deep neural networks still suffer performance declines\nunder distribution shifts between training and test domains, leading to a\nsubstantial decrease in Quality of Experience (QoE) for applications. Existing\ntest-time adaptation (TTA) methods are challenged by dynamic, multiple test\ndistributions within batches. We observe that feature distributions across\ndifferent domains inherently cluster into distinct groups with varying means\nand variances. This divergence reveals a critical limitation of previous global\nnormalization strategies in TTA, which inevitably distort the original data\ncharacteristics. Based on this insight, we propose Feature-based Instance\nNeighbor Discovery (FIND), which comprises three key components: Layer-wise\nFeature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and\nSelective FABN (S-FABN). LFD stably captures features with similar\ndistributions at each layer by constructing graph structures. While FABN\noptimally combines source statistics with test-time distribution specific\nstatistics for robust feature representation. Finally, S-FABN determines which\nlayers require feature partitioning and which can remain unified, thereby\nenhancing inference efficiency. Extensive experiments demonstrate that FIND\nsignificantly outperforms existing methods, achieving a 30\\% accuracy\nimprovement in dynamic scenarios while maintaining computational efficiency.", "AI": {"tldr": "Feature-based Instance Neighbor Discovery (FIND) addresses the challenge of dynamic, multiple test distributions in test-time adaptation (TTA). It includes Layer-wise Feature Disentanglement (LFD), Feature Aware Batch Normalization (FABN), and Selective FABN (S-FABN). FIND significantly improves accuracy in dynamic scenarios while maintaining computational efficiency.", "motivation": "Deep neural networks experience performance declines under distribution shifts between training and test domains. Current TTA methods struggle with dynamic, multiple test distributions within batches.", "method": "The proposed method, FIND, consists of three components: LFD, which captures features with similar distributions at each layer by constructing graph structures; FABN, which combines source statistics with test-time distribution specific statistics for robust feature representation; and S-FABN, which determines which layers require feature partitioning and which can remain unified to enhance inference efficiency.", "result": "Extensive experiments demonstrate that FIND significantly outperforms existing methods, achieving a 30% accuracy improvement in dynamic scenarios while maintaining computational efficiency.", "conclusion": "FIND effectively addresses the limitations of previous global normalization strategies in TTA by considering the inherent clustering of feature distributions across different domains."}}
{"id": "2506.07056", "pdf": "https://arxiv.org/pdf/2506.07056", "abs": "https://arxiv.org/abs/2506.07056", "authors": ["Zhenyu Liu", "Huizhi Liang", "Rajiv Ranjan", "Zhanxing Zhu", "Vaclav Snasel", "Varun Ojha"], "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models.", "AI": {"tldr": "This paper proposes D2R Loss and CAG to improve model robustness against adversarial attacks.", "motivation": "The existing defense methods have limitations in terms of loss function guidance and collaborative adversarial generation.", "method": "D2R loss includes two optimization steps for adversarial and clean distribution, leveraging different loss functions. CAG generates adversarial samples through gradient-based collaboration between guidance and target models.", "result": "Extensive experiments on benchmark databases show that D2R loss with CAG produces highly robust models.", "conclusion": "D2R Loss and CAG are effective strategies for enhancing the robustness of deep neural network models."}}
{"id": "2506.07736", "pdf": "https://arxiv.org/pdf/2506.07736", "abs": "https://arxiv.org/abs/2506.07736", "authors": ["Jingnan Zheng", "Xiangtian Ji", "Yijun Lu", "Chenhang Cui", "Weixiang Zhao", "Gelei Deng", "Zhenkai Liang", "An Zhang", "Tat-Seng Chua"], "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite\ndeliberate safety alignment efforts, posing significant risks to users and\nsociety. To safeguard against the risk of policy-violating content,\nsystem-level moderation via external guard models-designed to monitor LLM\ninputs and outputs and block potentially harmful content-has emerged as a\nprevalent mitigation strategy. Existing approaches of training guard models\nrely heavily on extensive human curated datasets and struggle with\nout-of-distribution threats, such as emerging harmful categories or jailbreak\nattacks. To address these limitations, we propose RSafe, an adaptive\nreasoning-based safeguard that conducts guided safety reasoning to provide\nrobust protection within the scope of specified safety policies. RSafe operates\nin two stages: 1) guided reasoning, where it analyzes safety risks of input\ncontent through policy-guided step-by-step reasoning, and 2) reinforced\nalignment, where rule-based RL optimizes its reasoning paths to align with\naccurate safety prediction. This two-stage training paradigm enables RSafe to\ninternalize safety principles to generalize safety protection capability over\nunseen or adversarial safety violation scenarios. During inference, RSafe\naccepts user-specified safety policies to provide enhanced safeguards tailored\nto specific safety requirements.", "AI": {"tldr": "RSafe is an adaptive safeguard for LLMs that uses guided safety reasoning and reinforced alignment to provide robust protection against policy-violating content.", "motivation": "Existing guard models for LLMs rely on extensive human-curated datasets and struggle with out-of-distribution threats, such as emerging harmful categories or jailbreak attacks.", "method": "RSafe operates in two stages: 1) guided reasoning, analyzing safety risks through policy-guided step-by-step reasoning, and 2) reinforced alignment, optimizing reasoning paths using rule-based RL for accurate safety prediction.", "result": "This two-stage training paradigm enables RSafe to internalize safety principles and generalize safety protection capability over unseen or adversarial scenarios. During inference, RSafe can accept user-specified safety policies for enhanced safeguards.", "conclusion": "RSafe provides a robust solution for safeguarding LLMs against policy-violating content by adapting its reasoning to specified safety policies and handling novel threats."}}
{"id": "2506.06784", "pdf": "https://arxiv.org/pdf/2506.06784", "abs": "https://arxiv.org/abs/2506.06784", "authors": ["Marek \u010cern\u00fd"], "title": "Caterpillar GNN: Replacing Message Passing with Efficient Aggregation", "categories": ["cs.LG", "68T07 (Primary), 05C60, 05C85 (Secondary)", "I.2.6; G.2.2"], "comment": "40 pages, 9 figures, 3 tables", "summary": "Message-passing graph neural networks (MPGNNs) dominate modern graph\nlearning, typically prioritizing maximal expressive power. In contrast, we\nintroduce an \\emph{efficient aggregation} mechanism, deliberately trading off\nsome expressivity for stronger and more structured aggregation capabilities.\nOur approach allows seamless scaling between classical message-passing and\nsimpler methods based on colored or plain walks. We rigorously characterize the\nexpressive power at each intermediate step using homomorphism counts from a\nhierarchy of generalized \\emph{caterpillar graphs}. Based on this foundation,\nwe propose the \\emph{Caterpillar GNN}, whose robust graph-level aggregation\nenables it to successfully tackle synthetic graph-level task specifically\ndesigned to be challenging for classical MPGNNs. Moreover, we demonstrate that,\non real-world datasets, the Caterpillar GNN achieves comparable predictive\nperformance while significantly reducing the number of nodes in the hidden\nlayers of the computational graph.", "AI": {"tldr": "The paper introduces an efficient aggregation mechanism in graph neural networks called Caterpillar GNN, which trades off some expressivity for stronger aggregation capabilities. It performs well on synthetic and real-world datasets while reducing computational complexity.", "motivation": "Current message-passing graph neural networks prioritize maximal expressive power, but this paper aims to develop a method that deliberately sacrifices some expressivity for more robust and structured aggregation capabilities.", "method": "The authors introduce the concept of 'efficient aggregation' in graph neural networks and propose the Caterpillar GNN. This model uses homomorphism counts from a hierarchy of generalized caterpillar graphs to characterize its expressive power at each intermediate step. It allows scaling between classical message-passing and simpler methods based on colored or plain walks.", "result": "Caterpillar GNN successfully tackles synthetic graph-level tasks challenging for classical MPGNNs. On real-world datasets, it achieves similar predictive performance as traditional models while significantly reducing the number of nodes in hidden layers of the computational graph.", "conclusion": "Caterpillar GNN provides a novel approach with robust graph-level aggregation capabilities, offering comparable performance with reduced computational resources."}}
{"id": "2506.07214", "pdf": "https://arxiv.org/pdf/2506.07214", "abs": "https://arxiv.org/abs/2506.07214", "authors": ["Zhiyuan Zhong", "Zhen Sun", "Yepang Liu", "Xinlei He", "Guanhong Tao"], "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment.", "AI": {"tldr": "Vision Language Models (VLMs) are vulnerable to backdoor attacks through cross-modal semantic mismatches. Proposed BadSem attack achieves high ASR, generalizes well, and transfers across modalities. Defenses based on system prompt and supervised fine-tuning fail.", "motivation": "Existing backdoor attacks on VLMs primarily rely on single-modality triggers, neglecting the cross-modal fusion nature of VLMs. This paper aims to explore the vulnerability of VLMs to cross-modal semantic mismatches as implicit triggers.", "method": "The paper proposes BadSem, a data poisoning attack that injects backdoors by misaligning image-text pairs during training. A dataset called SIMBad is constructed for semantic manipulation involving color and object attributes.", "result": "Extensive experiments show that BadSem achieves over 98% average ASR, generalizes well to out-of-distribution datasets, and can transfer across poisoning modalities. Attention visualization reveals that backdoored models focus on semantically sensitive regions under mismatched conditions while behaving normally on clean inputs.", "conclusion": "The findings indicate that VLMs are vulnerable to semantic backdoor attacks, and current defense strategies are ineffective. There is an urgent need to address semantic vulnerabilities in VLMs for safer deployment."}}
{"id": "2506.07756", "pdf": "https://arxiv.org/pdf/2506.07756", "abs": "https://arxiv.org/abs/2506.07756", "authors": ["Mark Burgess"], "title": "Agent Semantics, Semantic Spacetime, and Graphical Reasoning", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; F.4.1; I.2.4; G.2.2"], "comment": null, "summary": "Some formal aspects of the Semantic Spacetime graph model are presented, with\nreference to its use for directed knowledge representations and process\nmodelling. A finite $\\gamma(3,4)$ representation is defined to form a closed\nset of operations that can scale to any degree of semantic complexity. The\nSemantic Spacetime postulates bring predictability with minimal constraints to\npathways in graphs. The ubiquitous appearance of absorbing states in any\npartial graph means that a graph process leaks information. The issue is\nclosely associated with the issue of division by zero, which signals a loss of\nclosure and the need for manual injection of remedial information. The Semantic\nSpacetime model (and its Promise Theory) origins help to clarify how such\nabsorbing states are associated with boundary information where intentionality\ncan enter.", "AI": {"tldr": "A finite \u03b3(3,4) representation in Semantic Spacetime graph model forms a closed set of operations for directed knowledge representations and process modelling. Absorbing states indicate information loss akin to division by zero, needing remedial information injection. Origins of the model associate absorbing states with boundary information for intentionality.", "motivation": "To present formal aspects of the Semantic Spacetime graph model for directed knowledge representations and process modelling, addressing issues like predictability, information loss and closure.", "method": "Defining a finite \u03b3(3,4) representation to form a closed set of operations that can scale to any degree of semantic complexity while using Semantic Spacetime postulates.", "result": "Absorbing states are identified as linked to information loss and requiring manual intervention, closely associated with division by zero issue.", "conclusion": "Semantic Spacetime model origins help clarify association of absorbing states with boundary information where intentionality can enter."}}
{"id": "2506.06787", "pdf": "https://arxiv.org/pdf/2506.06787", "abs": "https://arxiv.org/abs/2506.06787", "authors": ["Qiyun Zhao"], "title": "FuncGNN: Learning Functional Semantics of Logic Circuits with Graph Neural Networks", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "As integrated circuit scale grows and design complexity rises, effective\ncircuit representation helps support logic synthesis, formal verification, and\nother automated processes in electronic design automation. And-Inverter Graphs\n(AIGs), as a compact and canonical structure, are widely adopted for\nrepresenting Boolean logic in these workflows. However, the increasing\ncomplexity and integration density of modern circuits introduce structural\nheterogeneity and global logic information loss in AIGs, posing significant\nchallenges to accurate circuit modeling. To address these issues, we propose\nFuncGNN, which integrates hybrid feature aggregation to extract\nmulti-granularity topological patterns, thereby mitigating structural\nheterogeneity and enhancing logic circuit representations. FuncGNN further\nintroduces gate-aware normalization that adapts to circuit-specific gate\ndistributions, improving robustness to structural heterogeneity. Finally,\nFuncGNN employs multi-layer integration to merge intermediate features across\nlayers, effectively synthesizing local and global semantic information for\ncomprehensive logic representations. Experimental results on two logic-level\nanalysis tasks (i.e., signal probability prediction and truth-table distance\nprediction) demonstrate that FuncGNN outperforms existing state-of-the-art\nmethods, achieving improvements of 2.06% and 18.71%, respectively, while\nreducing training time by approximately 50.6% and GPU memory usage by about\n32.8%.", "AI": {"tldr": "FuncGNN is a novel approach that enhances logic circuit representations by integrating hybrid feature aggregation, gate-aware normalization, and multi-layer integration. It mitigates structural heterogeneity in AIGs, improves robustness, and synthesizes local and global semantic information. FuncGNN outperforms existing methods in signal probability prediction and truth-table distance prediction tasks with significant improvements in accuracy and efficiency.", "motivation": "The motivation of this paper stems from the challenges posed by the increasing complexity and integration density of modern circuits, which lead to structural heterogeneity and global logic information loss in And-Inverter Graphs (AIGs). This affects the accuracy of circuit modeling in electronic design automation workflows.", "method": "FuncGNN employs three key techniques: 1) Hybrid feature aggregation to extract multi-granularity topological patterns; 2) Gate-aware normalization to adapt to circuit-specific gate distributions and improve robustness; 3) Multi-layer integration to merge intermediate features across layers for comprehensive logic representations.", "result": "FuncGNN achieves improvements of 2.06% in signal probability prediction and 18.71% in truth-table distance prediction compared to existing state-of-the-art methods. Additionally, it reduces training time by approximately 50.6% and GPU memory usage by about 32.8%.", "conclusion": "FuncGNN successfully addresses the challenges of structural heterogeneity and logic information loss in AIGs through its innovative techniques. The experimental results confirm its superior performance and efficiency over existing methods."}}
{"id": "2506.07294", "pdf": "https://arxiv.org/pdf/2506.07294", "abs": "https://arxiv.org/abs/2506.07294", "authors": ["Xuanjun Chen", "I-Ming Lin", "Lin Zhang", "Haibin Wu", "Hung-yi Lee", "Jyh-Shing Roger Jang"], "title": "Towards Generalized Source Tracing for Codec-Based Deepfake Speech", "categories": ["cs.SD", "cs.CR", "cs.LG", "eess.AS"], "comment": "Submitted to IEEE ASRU 2025", "summary": "Recent attempts at source tracing for codec-based deepfake speech\n(CodecFake), generated by neural audio codec-based speech generation (CoSG)\nmodels, have exhibited suboptimal performance. However, how to train source\ntracing models using simulated CoSG data while maintaining strong performance\non real CoSG-generated audio remains an open challenge. In this paper, we show\nthat models trained solely on codec-resynthesized data tend to overfit to\nnon-speech regions and struggle to generalize to unseen content. To mitigate\nthese challenges, we introduce the Semantic-Acoustic Source Tracing Network\n(SASTNet), which jointly leverages Whisper for semantic feature encoding and\nWav2vec2 with AudioMAE for acoustic feature encoding. Our proposed SASTNet\nachieves state-of-the-art performance on the CoSG test set of the CodecFake+\ndataset, demonstrating its effectiveness for reliable source tracing.", "AI": {"tldr": "Recent source tracing methods for codec-based deepfake speech have performed suboptimally. This paper introduces SASTNet, which uses Whisper and Wav2vec2 with AudioMAE to encode semantic and acoustic features respectively. SASTNet achieves state-of-the-art performance on the CoSG test set.", "motivation": "There is a need to improve the performance of source tracing models for codec-based deepfake speech. Current models trained on simulated data do not perform well on real CoSG-generated audio due to overfitting to non-speech regions.", "method": "The authors developed SASTNet, a Semantic-Acoustic Source Tracing Network that integrates Whisper for semantic feature encoding and Wav2vec2 with AudioMAE for acoustic feature encoding to address the issue of overfitting.", "result": "SASTNet demonstrates state-of-the-art performance on the CoSG test set in the CodecFake+ dataset.", "conclusion": "SASTNet effectively leverages both semantic and acoustic features to improve source tracing for codec-based deepfake speech, setting a new benchmark in this area."}}
{"id": "2506.07759", "pdf": "https://arxiv.org/pdf/2506.07759", "abs": "https://arxiv.org/abs/2506.07759", "authors": ["Diego Forni\u00e9s-Tabuenca", "Alejandro Uribe", "Urtzi Otamendi", "Arkaitz Artetxe", "Juan Carlos Rivera", "Oier Lopez de Lacalle"], "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models", "categories": ["cs.AI", "cs.NE", "I.2.7; I.2.8; F.2.2"], "comment": "21 pages, 5 tables, 7 figures and 4 appendixes. Pre-print submitted\n  to IEEE Transactions on Evolutionary Computation", "summary": "Multi-objective optimization is fundamental in complex decision-making tasks.\nTraditional algorithms, while effective, often demand extensive\nproblem-specific modeling and struggle to adapt to nonlinear structures. Recent\nadvances in Large Language Models (LLMs) offer enhanced explainability,\nadaptability, and reasoning. This work proposes Reflective Evolution of\nMulti-objective Heuristics (REMoH), a novel framework integrating NSGA-II with\nLLM-based heuristic generation. A key innovation is a reflection mechanism that\nuses clustering and search-space reflection to guide the creation of diverse,\nhigh-quality heuristics, improving convergence and maintaining solution\ndiversity. The approach is evaluated on the Flexible Job Shop Scheduling\nProblem (FJSSP) in-depth benchmarking against state-of-the-art methods using\nthree instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate\nthat REMoH achieves competitive results compared to state-of-the-art approaches\nwith reduced modeling effort and enhanced adaptability. These findings\nunderscore the potential of LLMs to augment traditional optimization, offering\ngreater flexibility, interpretability, and robustness in multi-objective\nscenarios.", "AI": {"tldr": "This paper proposes REMoH, a novel framework that combines NSGA-II with LLM-based heuristic generation for multi-objective optimization tasks. It introduces a reflection mechanism to improve convergence and solution diversity, demonstrating competitive results on FJSSP benchmarks.", "motivation": "To address the limitations of traditional multi-objective optimization algorithms, such as high problem-specific modeling demands and difficulties in adapting to nonlinear structures, by leveraging the explainability, adaptability, and reasoning capabilities of Large Language Models (LLMs).", "method": "The Reflective Evolution of Multi-objective Heuristics (REMoH) integrates NSGA-II with LLM-based heuristic generation. It incorporates a reflection mechanism that uses clustering and search-space reflection to guide the creation of diverse, high-quality heuristics.", "result": "REMoH achieves competitive results compared to state-of-the-art approaches on the Flexible Job Shop Scheduling Problem (FJSSP) using three instance datasets (Dauzere, Barnes, and Brandimarte), with reduced modeling effort and enhanced adaptability.", "conclusion": "The findings highlight the potential of LLMs to augment traditional optimization methods, providing greater flexibility, interpretability, and robustness in multi-objective optimization scenarios."}}
{"id": "2506.06793", "pdf": "https://arxiv.org/pdf/2506.06793", "abs": "https://arxiv.org/abs/2506.06793", "authors": ["Zixuan Dong", "Yumi Omori", "Keith Ross"], "title": "Is Optimal Transport Necessary for Inverse Reinforcement Learning?", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 10 tables", "summary": "Inverse Reinforcement Learning (IRL) aims to recover a reward function from\nexpert demonstrations. Recently, Optimal Transport (OT) methods have been\nsuccessfully deployed to align trajectories and infer rewards. While OT-based\nmethods have shown strong empirical results, they introduce algorithmic\ncomplexity, hyperparameter sensitivity, and require solving the OT optimization\nproblems. In this work, we challenge the necessity of OT in IRL by proposing\ntwo simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns\nrewards based on the nearest expert state regardless of temporal order; and (2)\nSegment-Matching Reward, which incorporates lightweight temporal alignment by\nmatching agent states to corresponding segments in the expert trajectory. These\nmethods avoid optimization, exhibit linear-time complexity, and are easy to\nimplement. Through extensive evaluations across 32 online and offline\nbenchmarks with three reinforcement learning algorithms, we show that our\nsimple rewards match or outperform recent OT-based approaches. Our findings\nsuggest that the core benefits of OT may arise from basic proximity alignment\nrather than its optimal coupling formulation, advocating for reevaluation of\ncomplexity in future IRL design.", "AI": {"tldr": "An abstract about proposing two heuristic alternatives to Optimal Transport (OT) in Inverse Reinforcement Learning (IRL). These methods avoid optimization, exhibit linear-time complexity, and perform well across benchmarks.", "motivation": "To challenge the necessity of using Optimal Transport in IRL by providing simpler alternatives that can achieve similar or better performance with less computational cost and complexity.", "method": "Proposed two simple, heuristic alternatives: Minimum-Distance Reward and Segment-Matching Reward. The former assigns rewards based on the nearest expert state regardless of temporal order; the latter incorporates lightweight temporal alignment by matching agent states to corresponding segments in the expert trajectory.", "result": "Through extensive evaluations across 32 online and offline benchmarks with three reinforcement learning algorithms, these simple rewards match or outperform recent OT-based approaches.", "conclusion": "The core benefits of OT may arise from basic proximity alignment rather than its optimal coupling formulation, suggesting a need for reevaluation of complexity in future IRL design."}}
{"id": "2506.07330", "pdf": "https://arxiv.org/pdf/2506.07330", "abs": "https://arxiv.org/abs/2506.07330", "authors": ["Yash Datta", "Sharath Rajasekar"], "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security", "categories": ["cs.LG", "cs.AI", "cs.CR", "I.2.7"], "comment": "16 pages, 1 Figure and 5 Tables", "summary": "We present JavelinGuard, a suite of low-cost, high-performance model\narchitectures designed for detecting malicious intent in Large Language Model\n(LLM) interactions, optimized specifically for production deployment. Recent\nadvances in transformer architectures, including compact BERT(Devlin et al.\n2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build\nhighly accurate classifiers with as few as approximately 400M parameters that\nachieve rapid inference speeds even on standard CPU hardware. We systematically\nexplore five progressively sophisticated transformer-based architectures:\nSharanga (baseline transformer classifier), Mahendra (enhanced\nattention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid\nneural ensemble architectures), and Raudra (an advanced multi-task framework\nwith specialized loss functions). Our models are rigorously benchmarked across\nnine diverse adversarial datasets, including popular sets like the NotInject\nseries, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly\nintroduced JavelinBench, specifically crafted to test generalization on\nchallenging borderline and hard-negative cases. Additionally, we compare our\narchitectures against leading open-source guardrail models as well as large\ndecoder-only LLMs such as gpt-4o, demonstrating superior cost-performance\ntrade-offs in terms of accuracy, and latency. Our findings reveal that while\nRaudra's multi-task design offers the most robust performance overall, each\narchitecture presents unique trade-offs in speed, interpretability, and\nresource requirements, guiding practitioners in selecting the optimal balance\nof complexity and efficiency for real-world LLM security applications.", "AI": {"tldr": "The paper introduces JavelinGuard, a set of model architectures for detecting malicious intent in LLM interactions with low cost and high performance. It explores five transformer-based architectures and benchmarks them on nine adversarial datasets. Raudra's multi-task design shows the best overall performance.", "motivation": "To develop efficient and effective model architectures for detecting malicious intent in LLM interactions, optimizing for production deployment.", "method": "Systematically explore five progressively sophisticated transformer-based architectures: Sharanga, Mahendra, Vaishnava and Ashwina, and Raudra. Benchmark these models across nine diverse adversarial datasets including newly introduced JavelinBench.", "result": "Raudra's multi-task design offers the most robust performance overall. Each architecture presents unique trade-offs in speed, interpretability, and resource requirements.", "conclusion": "JavelinGuard provides optimal balance of complexity and efficiency for real-world LLM security applications, with Raudra being the most robust choice."}}
{"id": "2506.07807", "pdf": "https://arxiv.org/pdf/2506.07807", "abs": "https://arxiv.org/abs/2506.07807", "authors": ["John Laird", "Christian Lebiere", "Paul Rosenbloom", "Andrea Stocco", "Robert Wray"], "title": "A Proposal to Extend the Common Model of Cognition with Metacognition", "categories": ["cs.AI"], "comment": null, "summary": "The Common Model of Cognition (CMC) provides an abstract characterization of\nthe structure and processing required by a cognitive architecture for\nhuman-like minds. We propose a unified approach to integrating metacognition\nwithin the CMC. We propose that metacognition involves reasoning over explicit\nrepresentations of an agent's cognitive capabilities and processes in working\nmemory. Our proposal exploits the existing cognitive capabilities of the CMC,\nmaking minimal extensions in the structure and information available within\nworking memory. We provide examples of metacognition within our proposal.", "AI": {"tldr": "A unified approach to integrating metacognition within the Common Model of Cognition (CMC) is proposed, which involves reasoning over explicit representations in working memory and makes minimal extensions to the existing structure.", "motivation": "The motivation is to provide a unified approach for integrating metacognition into the Common Model of Cognition (CMC) for human-like minds.", "method": "The method involves reasoning over explicit representations of an agent's cognitive capabilities and processes in working memory, exploiting the existing cognitive capabilities of the CMC and making minimal extensions to its structure.", "result": "Examples of metacognition within the proposal are provided, demonstrating the feasibility of the approach.", "conclusion": "This approach successfully integrates metacognition within the CMC with minimal changes to its structure."}}
{"id": "2506.06809", "pdf": "https://arxiv.org/pdf/2506.06809", "abs": "https://arxiv.org/abs/2506.06809", "authors": ["Di Lin", "Wanjing Ren", "Xuanbin Li", "Rui Zhang"], "title": "IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning (SSL) methods have been increasingly applied to\ndiverse downstream tasks due to their superior generalization capabilities and\nlow annotation costs. However, most existing heterogeneous graph SSL models\nconvert heterogeneous graphs into homogeneous ones via meta-paths for training,\nwhich only leverage information from nodes at both ends of meta-paths while\nunderutilizing the heterogeneous node information along the meta-paths. To\naddress this limitation, this paper proposes a novel framework named IMPA-HGAE\nto enhance target node embeddings by fully exploiting internal node information\nalong meta-paths. Experimental results validate that IMPA-HGAE achieves\nsuperior performance on heterogeneous datasets. Furthermore, this paper\nintroduce innovative masking strategies to strengthen the representational\ncapacity of generative SSL models on heterogeneous graph data. Additionally,\nthis paper discuss the interpretability of the proposed method and potential\nfuture directions for generative self-supervised learning in heterogeneous\ngraphs. This work provides insights into leveraging meta-path-guided structural\nsemantics for robust representation learning in complex graph scenarios.", "AI": {"tldr": "IMPA-HGAE is a new framework that improves node embeddings in heterogeneous graphs by using all node information along meta-paths, showing superior performance and offering insights for future research.", "motivation": "Current SSL models for heterogeneous graphs underutilize the internal node information along meta-paths when converting them to homogeneous graphs.", "method": "Proposes IMPA-HGAE framework which fully exploits internal node information along meta-paths and introduces innovative masking strategies to enhance generative SSL models.", "result": "IMPA-HGAE achieves superior performance on heterogeneous datasets and provides interpretability of the method.", "conclusion": "This work offers insights into leveraging meta-path-guided structural semantics for robust representation learning in complex graph scenarios."}}
{"id": "2506.07640", "pdf": "https://arxiv.org/pdf/2506.07640", "abs": "https://arxiv.org/abs/2506.07640", "authors": ["Ruopengyu Xu", "Chenglian Liu"], "title": "Stark-Coleman Invariants and Quantum Lower Bounds: An Integrated Framework for Real Quadratic Fields", "categories": ["math.NT", "cs.CR", "math.GR", "11R29, 11R42, 81P68", "F.2.2; G.2.0; E.3"], "comment": "16 pages, 1 figure, 3 tables", "summary": "Class groups of real quadratic fields represent fundamental structures in\nalgebraic number theory with significant computational implications. While\nStark's conjecture establishes theoretical connections between special units\nand class group structures, explicit constructions have remained elusive, and\nprecise quantum complexity bounds for class group computations are lacking.\nHere we establish an integrated framework defining Stark-Coleman invariants\n$\\kappa_p(K) = \\log_p \\left(\n\\frac{\\varepsilon_{\\mathrm{St},p}}{\\sigma(\\varepsilon_{\\mathrm{St},p})} \\right)\n\\mod p^{\\mathrm{ord}_p(\\Delta_K)}$ through a synthesis of $p$-adic Hodge theory\nand extended Coleman integration. We prove these invariants classify class\ngroups under the Generalized Riemann Hypothesis (GRH), resolving the\nisomorphism problem for discriminants $D > 10^{32}$. Furthermore, we\ndemonstrate that this approach yields the quantum lower bound\n$\\exp\\left(\\Omega\\left(\\frac{\\log D}{(\\log \\log D)^2}\\right)\\right)$ for the\nclass group discrete logarithm problem, improving upon previous bounds lacking\nexplicit constants. Our results indicate that Stark units constrain the\ngeometric organization of class groups, providing theoretical insight into\ncomputational complexity barriers.", "AI": {"tldr": "Class groups of real quadratic fields are fundamental in algebraic number theory. This paper introduces Stark-Coleman invariants using p-adic Hodge theory and Coleman integration, proving they classify class groups under GRH for discriminants D > 10^32. It also establishes a quantum lower bound for the class group discrete logarithm problem.", "motivation": "Class groups of real quadratic fields play a crucial role in algebraic number theory, but explicit constructions connecting special units to these groups have been challenging. Additionally, precise quantum complexity bounds for computations involving class groups are not well understood.", "method": "The authors define Stark-Coleman invariants through a combination of p-adic Hodge theory and extended Coleman integration. They then analyze these invariants' properties and their implications for class group classification and computational complexity.", "result": "The invariants were proven to classify class groups under the Generalized Riemann Hypothesis for large discriminants. A quantum lower bound for the class group discrete logarithm problem was established, improving on previous results.", "conclusion": "Stark units impose constraints on the geometric structure of class groups, offering theoretical insights into the computational challenges associated with these groups."}}
{"id": "2506.07820", "pdf": "https://arxiv.org/pdf/2506.07820", "abs": "https://arxiv.org/abs/2506.07820", "authors": ["Jiaxiang CHen", "Zhuo Wang", "Mingxi Zou", "Qifan Wang", "Zenglin Xu"], "title": "Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation", "categories": ["cs.AI"], "comment": null, "summary": "Human reasoning is flexible, adaptive, and grounded in prior\nexperience-qualities that large language models (LLMs) still struggle to\nemulate. Existing methods either explore diverse reasoning paths at inference\ntime or search for optimal workflows through expensive operations, but both\nfall short in leveraging multiple reusable strategies in a structured,\nefficient manner. We propose Guideline Forest, a framework that enhances LLMs\nreasoning by inducing structured reasoning strategies-called guidelines-from\nverified examples and executing them via step-wise aggregation. Unlike\ntest-time search or single-path distillation, our method draws on verified\nreasoning experiences by inducing reusable guidelines and expanding each into\ndiverse variants. Much like human reasoning, these variants reflect alternative\nthought patterns, are executed in parallel, refined via self-correction, and\naggregated step by step-enabling the model to adaptively resolve uncertainty\nand synthesize robust solutions.We evaluate Guideline Forest on four\nbenchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and\nprogrammatic reasoning. Guideline Forest consistently outperforms strong\nbaselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further\nhighlight the effectiveness of multi-path reasoning and stepwise aggregation,\nunderscoring the Guideline Forest's adaptability and generalization potential.", "AI": {"tldr": "The paper introduces Guideline Forest, a framework enhancing LLMs reasoning by inducing structured strategies from verified examples and executing them via step-wise aggregation. Evaluated on four benchmarks, it outperforms strong baselines.", "motivation": "Human reasoning is flexible, adaptive, and grounded in prior experience, qualities that LLMs still struggle to emulate. Existing methods either explore diverse reasoning paths at inference time or search for optimal workflows through expensive operations, but both fall short in leveraging multiple reusable strategies in a structured, efficient manner.", "method": "Guideline Forest enhances LLMs reasoning by inducing structured reasoning strategies called guidelines from verified examples and executing them via step-wise aggregation. It draws on verified reasoning experiences by inducing reusable guidelines and expanding each into diverse variants, which reflect alternative thought patterns, are executed in parallel, refined via self-correction, and aggregated step by step.", "result": "Guideline Forest consistently outperforms strong baselines, including CoT, ReAct, ToT, FoT, and AFlow, when evaluated on four benchmarks spanning mathematical and programmatic reasoning. Ablation studies further highlight the effectiveness of multi-path reasoning and stepwise aggregation.", "conclusion": "Guideline Forest's adaptability and generalization potential are underscored, showing promise in enhancing LLMs reasoning capabilities."}}
{"id": "2506.06815", "pdf": "https://arxiv.org/pdf/2506.06815", "abs": "https://arxiv.org/abs/2506.06815", "authors": ["Max McGuinness", "Eirik Fladmark", "Francisco Vargas"], "title": "Path Integral Optimiser: Global Optimisation via Neural Schr\u00f6dinger-F\u00f6llmer Diffusion", "categories": ["cs.LG"], "comment": "6 pages. Presented at the OPT Workshop, NeurIPS 2024, Vancouver, CA", "summary": "We present an early investigation into the use of neural diffusion processes\nfor global optimisation, focusing on Zhang et al.'s Path Integral Sampler. One\ncan use the Boltzmann distribution to formulate optimization as solving a\nSchr\\\"odinger bridge sampling problem, then apply Girsanov's theorem with a\nsimple (single-point) prior to frame it in stochastic control terms, and\ncompute the solution's integral terms via a neural approximation (a Fourier\nMLP). We provide theoretical bounds for this optimiser, results on toy\noptimisation tasks, and a summary of the stochastic theory motivating the\nmodel. Ultimately, we found the optimiser to display promising per-step\nperformance at optimisation tasks between 2 and 1,247 dimensions, but struggle\nto explore higher-dimensional spaces when faced with a 15.9k parameter model,\nindicating a need for work on adaptation in such environments.", "AI": {"tldr": "An early investigation into using neural diffusion processes for global optimisation based on Zhang et al.'s Path Integral Sampler is presented, showing promising performance in tasks between 2 and 1,247 dimensions but struggling in higher-dimensional spaces.", "motivation": "To explore the potential of neural diffusion processes for global optimization by framing it as a Schr\u00f6dinger bridge sampling problem and solving it with stochastic control theory.", "method": "Using the Boltzmann distribution to formulate optimization problems, applying Girsanov's theorem with a single-point prior, and computing integral terms via a neural approximation (Fourier MLP).", "result": "The optimizer showed promising per-step performance in optimization tasks ranging from 2 to 1,247 dimensions, but struggled with exploration in higher-dimensional spaces when dealing with a model of 15.9k parameters.", "conclusion": "This approach presents an interesting direction for global optimization, but further work is needed on adaptation in high-dimensional environments."}}
{"id": "2506.07795", "pdf": "https://arxiv.org/pdf/2506.07795", "abs": "https://arxiv.org/abs/2506.07795", "authors": ["Xiaotian Ye", "Mengqi Zhang", "Shu Wu"], "title": "LLM Unlearning Should Be Form-Independent", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.", "AI": {"tldr": "The paper addresses the limitations of current Large Language Model (LLM) unlearning methods, which struggle with real-world generalization due to Form-Dependent Bias. It introduces ORT, a benchmark for evaluating unlearning robustness, and proposes ROCR, a training-free method that redirects dangerous concepts to improve unlearning effectiveness.", "motivation": "Existing LLM unlearning methods are ineffective in real-world scenarios because their success depends on the form of training samples and they fail to generalize to different expressions of the same knowledge.", "method": "The study introduces two key contributions: ORT, a benchmark to evaluate unlearning methods' robustness against variations in knowledge expression, and ROCR, a training-free unlearning method targeting dangerous concepts by redirecting them to harmless ones.", "result": "Experiments reveal that Form-Dependent Bias is prevalent and severe in current unlearning techniques. ROCR significantly enhances unlearning effectiveness compared to traditional methods while maintaining natural output quality.", "conclusion": "To achieve practical LLM unlearning, methods must be form-independent. ROCR represents a promising direction towards this goal."}}
{"id": "2506.07824", "pdf": "https://arxiv.org/pdf/2506.07824", "abs": "https://arxiv.org/abs/2506.07824", "authors": ["Yao Yan"], "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs", "categories": ["cs.AI"], "comment": "12 pages, including appendix, 7 figures. EMNLP 2025 submission (ARR\n  May 2025 cycle, reviews pending)", "summary": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLaMA-3-8B-Instruct\uff09\u5728\u591a\u4f4d\u6570\u52a0\u6cd5\u4e2d\u7684\u8ba1\u7b97\u80fd\u529b\u88ab\u6df1\u5165\u5256\u6790\uff0c\u63ed\u793a\u5176\u5185\u90e8\u7b97\u672f\u8fc7\u7a0b\u9075\u5faa\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u7684\u56db\u9636\u6bb5\u8f68\u8ff9\uff0c\u5f3a\u8c03\u4e86\u5206\u5c42\u5904\u7406\u548c\u5185\u90e8\u8ba1\u7b97\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790LLaMA-3-8B-Instruct\u5728\u591a\u4f4d\u6570\u52a0\u6cd5\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u80fd\u529b\u53ca\u5176\u5185\u90e8\u7b97\u672f\u5904\u7406\u673a\u5236\u3002", "method": "\u7ed3\u5408\u7ebf\u6027\u63a2\u6d4b\u4e0elogit-lens\u68c0\u67e5\u65b9\u6cd5\uff0c\u63d0\u51fa\u5e76\u5206\u6790\u4e86\u4e00\u4e2a\u5728\u524d\u5411\u4f20\u9012\u8fc7\u7a0b\u4e2d\u7684\u4eba\u7c7b\u52a0\u6cd5\u89c4\u5219\u76f8\u4eff\u7684\u56db\u4e2a\u8fde\u8d2f\u9636\u6bb5\u3002", "result": "\u53d1\u73b0\u4e86\u6a21\u578b\u5728\u8ba1\u7b97\u8fc7\u7a0b\u4e2d\u5448\u73b0\u51fa\u6e05\u6670\u7684\u5206\u5c42\u7279\u5f81\uff0c\u80fd\u591f\u51e0\u4e4e\u5b8c\u7f8e\u5730\u68c0\u6d4b\u548c\u89e3\u7801\u603b\u548c\u4e2d\u7684\u5404\u4e2a\u6570\u5b57\uff0c\u5e76\u6700\u7ec8\u751f\u6210\u6b63\u786e\u7684\u8f93\u51fa\u5185\u5bb9\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cLLaMA-3-8B-Instruct\u7684\u8ba1\u7b97\u8fc7\u7a0b\u66f4\u503e\u5411\u4e8e\u5185\u90e8\u8ba1\u7b97\u800c\u975e\u6b7b\u8bb0\u786c\u80cc\uff0c\u540c\u65f6\u63d0\u4f9b\u4ee3\u7801\u548c\u6570\u636e\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2506.06853", "pdf": "https://arxiv.org/pdf/2506.06853", "abs": "https://arxiv.org/abs/2506.06853", "authors": ["Ilya Kaufman Sirot", "Omri Azencot"], "title": "Curvature Enhanced Data Augmentation for Regression", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Deep learning models with a large number of parameters, often referred to as\nover-parameterized models, have achieved exceptional performance across various\ntasks. Despite concerns about overfitting, these models frequently generalize\nwell to unseen data, thanks to effective regularization techniques, with data\naugmentation being among the most widely used. While data augmentation has\nshown great success in classification tasks using label-preserving\ntransformations, its application in regression problems has received less\nattention. Recently, a novel \\emph{manifold learning} approach for generating\nsynthetic data was proposed, utilizing a first-order approximation of the data\nmanifold. Building on this foundation, we present a theoretical framework and\npractical tools for approximating and sampling general data manifolds.\nFurthermore, we introduce the Curvature-Enhanced Manifold Sampling (CEMS)\nmethod for regression tasks. CEMS leverages a second-order representation of\nthe data manifold to enable efficient sampling and reconstruction of new data\npoints. Extensive evaluations across multiple datasets and comparisons with\nstate-of-the-art methods demonstrate that CEMS delivers superior performance in\nboth in-distribution and out-of-distribution scenarios, while introducing only\nminimal computational overhead. Code is available at\nhttps://github.com/azencot-group/CEMS.", "AI": {"tldr": "The paper presents Curvature-Enhanced Manifold Sampling (CEMS), a second-order method for synthetic data generation in regression tasks, which outperforms state-of-the-art methods with minimal computational overhead.", "motivation": "Deep learning models, particularly over-parameterized ones, have excelled across various tasks. While data augmentation has been successful in classification tasks, it is less explored in regression problems. The authors aim to address this gap by proposing a novel approach for generating synthetic data in regression tasks.", "method": "The authors develop a theoretical framework and practical tools for approximating and sampling general data manifolds. They introduce CEMS, which uses a second-order representation of the data manifold for efficient sampling and reconstruction of new data points in regression tasks.", "result": "Extensive evaluations show that CEMS outperforms state-of-the-art methods in both in-distribution and out-of-distribution scenarios for regression tasks, while only introducing minimal computational overhead.", "conclusion": "CEMS is an effective method for synthetic data generation in regression tasks, offering superior performance with low computational cost. The code is publicly available."}}
{"id": "2506.07948", "pdf": "https://arxiv.org/pdf/2506.07948", "abs": "https://arxiv.org/abs/2506.07948", "authors": ["Kasimir Schulz", "Kenneth Yeung", "Kieran Evans"], "title": "TokenBreak: Bypassing Text Classification Models Through Token Manipulation", "categories": ["cs.LG", "cs.CR"], "comment": null, "summary": "Natural Language Processing (NLP) models are used for text-related tasks such\nas classification and generation. To complete these tasks, input data is first\ntokenized from human-readable text into a format the model can understand,\nenabling it to make inferences and understand context. Text classification\nmodels can be implemented to guard against threats such as prompt injection\nattacks against Large Language Models (LLMs), toxic input and cybersecurity\nrisks such as spam emails. In this paper, we introduce TokenBreak: a novel\nattack that can bypass these protection models by taking advantage of the\ntokenization strategy they use. This attack technique manipulates input text in\nsuch a way that certain models give an incorrect classification. Importantly,\nthe end target (LLM or email recipient) can still understand and respond to the\nmanipulated text and therefore be vulnerable to the very attack the protection\nmodel was put in place to prevent. The tokenizer is tied to model architecture,\nmeaning it is possible to predict whether or not a model is vulnerable to\nattack based on family. We also present a defensive strategy as an added layer\nof protection that can be implemented without having to retrain the defensive\nmodel.", "AI": {"tldr": "The paper introduces TokenBreak, an attack that can bypass text classification models by manipulating tokenization strategies, and provides a defensive strategy without retraining the model.", "motivation": "To expose vulnerabilities in text classification models used to protect against threats like prompt injection attacks, toxic input, and cybersecurity risks such as spam emails.", "method": "TokenBreak manipulates input text during tokenization so that certain models give incorrect classifications while remaining understandable to the end target (LLM or email recipient). The tokenizer's connection to model architecture allows prediction of vulnerability based on model family.", "result": "TokenBreak successfully demonstrates its ability to bypass protection models, making the end target vulnerable to the very attacks these models were designed to prevent. Additionally, a defensive strategy is presented that can be implemented without retraining the defensive model.", "conclusion": "Text classification models have significant vulnerabilities due to their tokenization strategies, which can be exploited by TokenBreak. A potential defense mechanism is proposed that does not require retraining the original model."}}
{"id": "2506.07837", "pdf": "https://arxiv.org/pdf/2506.07837", "abs": "https://arxiv.org/abs/2506.07837", "authors": ["Shijie Wang", "Yilun Zhang", "Zeyu Lai", "Dexing Kong"], "title": "HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown great potential in\ngeneral domains but perform poorly in some specific domains due to a lack of\ndomain-specific data, such as image-text data or vedio-text data. In some\nspecific domains, there is abundant graphic and textual data scattered around,\nbut lacks standardized arrangement. In the field of medical ultrasound, there\nare ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic\ndiagnostic reports, and so on. However, these ultrasonic materials are often\nsaved in the forms of PDF, images, etc., and cannot be directly used for the\ntraining of MLLMs. This paper proposes a novel image-text reasoning supervised\nfine-tuning data generation pipeline to create specific domain quadruplets\n(image, question, thinking trace, and answer) from domain-specific materials. A\nmedical ultrasound domain dataset ReMUD is established, containing over 45,000\nreasoning and non-reasoning supervised fine-tuning Question Answering (QA) and\nVisual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on\nQwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound\nfield. To facilitate research, the ReMUD dataset, data generation codebase, and\nReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,\naddressing the data shortage issue in specific domain MLLMs.", "AI": {"tldr": "Multimodal large language models (MLLMs) struggle in specific domains due to lack of domain-specific data. This paper proposes a novel pipeline to generate supervised fine-tuning data from domain-specific materials in the medical ultrasound field, establishing a dataset ReMUD and releasing related resources to address data shortage issue.", "motivation": "MLLMs perform poorly in specific domains such as medical ultrasound due to lack of domain-specific data like image-text or video-text data. There is abundant unstandardized graphic and textual data in this field.", "method": "Propose an image-text reasoning supervised fine-tuning data generation pipeline to create specific domain quadruplets (image, question, thinking trace, answer) from domain-specific materials. Establish a medical ultrasound domain dataset ReMUD containing over 45,000 QA and VQA data.", "result": "The ReMUD-7B model fine-tuned on Qwen2.5-VL-7B-Instruct outperforms general-domain MLLMs in medical ultrasound field.", "conclusion": "To facilitate research, release the ReMUD dataset, data generation codebase, and ReMUD-7B parameters at https://github.com/ShiDaizi/ReMUD, addressing the data shortage issue in specific domain MLLMs."}}
{"id": "2506.06858", "pdf": "https://arxiv.org/pdf/2506.06858", "abs": "https://arxiv.org/abs/2506.06858", "authors": ["Ziwei Li", "Yuhan Duan", "Tianyu Xiong", "Yi-Tang Chen", "Wei-Lun Chao", "Han-Wei Shen"], "title": "High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Effective surrogate models are critical for accelerating scientific\nsimulations. Implicit neural representations (INRs) offer a compact and\ncontinuous framework for modeling spatially structured data, but they often\nstruggle with complex scientific fields exhibiting localized, high-frequency\nvariations. Recent approaches address this by introducing additional features\nalong rigid geometric structures (e.g., grids), but at the cost of flexibility\nand increased model size. In this paper, we propose a simple yet effective\nalternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to\nan augmented memory bank to learn flexible feature representations, enabling\nadaptive allocation of model capacity based on data characteristics, rather\nthan rigid structural assumptions. To further improve scalability, we introduce\na coordinate-guided mixture of experts (MoE) that enhances the specialization\nand efficiency of feature representations. Experiments on three large-scale\nensemble simulation datasets show that FA-INR achieves state-of-the-art\nfidelity while significantly reducing model size, establishing a new trade-off\nfrontier between accuracy and compactness for INR-based surrogates.", "AI": {"tldr": "This paper proposes FA-INR, which uses cross-attention and a coordinate-guided mixture of experts to create flexible feature representations for modeling complex scientific fields while reducing model size.", "motivation": "Implicit neural representations often struggle with complex scientific fields exhibiting localized, high-frequency variations. Current approaches that address this issue introduce additional features along rigid geometric structures, but at the cost of flexibility and increased model size.", "method": "The proposed method is called Feature-Adaptive INR (FA-INR). It leverages cross-attention to an augmented memory bank to learn flexible feature representations, enabling adaptive allocation of model capacity based on data characteristics rather than rigid structural assumptions. Additionally, a coordinate-guided mixture of experts (MoE) is introduced to enhance the specialization and efficiency of feature representations.", "result": "Experiments on three large-scale ensemble simulation datasets show that FA-INR achieves state-of-the-art fidelity while significantly reducing model size.", "conclusion": "FA-INR establishes a new trade-off frontier between accuracy and compactness for INR-based surrogates."}}
{"id": "2506.07957", "pdf": "https://arxiv.org/pdf/2506.07957", "abs": "https://arxiv.org/abs/2506.07957", "authors": ["Mat\u00edas Mazzanti", "Esteban Mocskos", "Augusto Vega", "Pradip Bose"], "title": "Understanding the Error Sensitivity of Privacy-Aware Computing", "categories": ["cs.AR", "cs.CR"], "comment": null, "summary": "Homomorphic Encryption (HE) enables secure computation on encrypted data\nwithout decryption, allowing a great opportunity for privacy-preserving\ncomputation. In particular, domains such as healthcare, finance, and\ngovernment, where data privacy and security are of utmost importance, can\nbenefit from HE by enabling third-party computation and services on sensitive\ndata. In other words, HE constitutes the \"Holy Grail\" of cryptography: data\nremains encrypted all the time, being protected while in use.\n  HE's security guarantees rely on noise added to data to make relatively\nsimple problems computationally intractable. This error-centric intrinsic HE\nmechanism generates new challenges related to the fault tolerance and\nrobustness of HE itself: hardware- and software-induced errors during HE\noperation can easily evade traditional error detection and correction\nmechanisms, resulting in silent data corruption (SDC).\n  In this work, we motivate a thorough discussion regarding the sensitivity of\nHE applications to bit faults and provide a detailed error characterization\nstudy of CKKS (Cheon-Kim-Kim-Song). This is one of the most popular HE schemes\ndue to its fixed-point arithmetic support for AI and machine learning\napplications. We also delve into the impact of the residue number system (RNS)\nand the number theoretic transform (NTT), two widely adopted HE optimization\ntechniques, on CKKS' error sensitivity. To the best of our knowledge, this is\nthe first work that looks into the robustness and error sensitivity of\nhomomorphic encryption and, as such, it can pave the way for critical future\nwork in this area.", "AI": {"tldr": "The paper discusses the sensitivity of Homomorphic Encryption (HE) applications to bit faults, focusing on the CKKS scheme. It provides an error characterization study and examines the impact of RNS and NTT optimization techniques on error sensitivity.", "motivation": "To understand and address the challenges related to fault tolerance and robustness in HE schemes, particularly the sensitivity to bit faults.", "method": "Conduct a detailed error characterization study of the CKKS HE scheme and analyze the impact of RNS and NTT optimization techniques on its error sensitivity.", "result": "This work identifies the vulnerabilities of HE schemes to hardware- and software-induced errors and provides insights into improving their robustness.", "conclusion": "This is the first study to investigate the robustness and error sensitivity of homomorphic encryption, offering a foundation for future research in enhancing HE's reliability."}}
{"id": "2506.07853", "pdf": "https://arxiv.org/pdf/2506.07853", "abs": "https://arxiv.org/abs/2506.07853", "authors": ["Hudson de Martim"], "title": "A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Effectively representing legal norms for automated processing is a critical\nchallenge, particularly in tracking the diachronic evolution of their\nhierarchical components (e.g., articles, paragraphs). While foundational\nframeworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal\ndocuments at a macro level, they lack native mechanisms for granular,\ncomponent-level versioning. This limitation hinders the deterministic\npoint-in-time reconstruction of legal texts, a fundamental capability for\nreliable Legal Tech and AI applications. This paper proposes a structured,\ntemporal model that extends the FRBRoo framework to address this gap. It\nintroduces specialized subclasses of Expressio - Temporal Version (TV) and\nLanguage Version (LV - to represent the state of a legal norm and its\nlinguistic variations at specific points in time. The model applies this same\nparadigm hierarchically, introducing Component Work (CW), Component Temporal\nVersion (CTV), and Component Language Version (CLV) to track the lifecycle of\nindividual articles, paragraphs, and clauses. Using the Brazilian Federal\nConstitution as a case study, the paper demonstrates how each amendment creates\nnew Component Temporal Versions for affected provisions, while unaffected\ncomponents retain their existing versions. This fine-grained, time-aware\narchitecture enables the precise, deterministic retrieval and reconstruction of\nany part of a legal text as it existed on a specific date. The model provides a\nrobust foundation for developing advanced legal information systems, knowledge\ngraphs, and AI tools capable of accurate historical analysis and impact\nassessment, overcoming the limitations of current generative models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55FRBRoo\u6846\u67b6\u7684\u7ed3\u6784\u5316\u65f6\u95f4\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u8868\u8fbe\u65f6\u95f4\u548c\u8bed\u8a00\u7248\u672c\u7684\u6982\u5ff5\uff0c\u5b9e\u73b0\u5bf9\u6cd5\u5f8b\u89c4\u8303\u53ca\u5176\u7ec4\u6210\u90e8\u5206\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u7684\u72b6\u6001\u548c\u8bed\u8a00\u53d8\u4f53\u7684\u7cbe\u7ec6\u5efa\u6a21\u3002\u8be5\u6a21\u578b\u4ee5\u5df4\u897f\u8054\u90a6\u5baa\u6cd5\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u8ffd\u8e2a\u6cd5\u5f8b\u6587\u672c\u7684\u65f6\u95f4\u6f14\u5316\uff0c\u4e3a\u9ad8\u7ea7\u6cd5\u5f8b\u4fe1\u606f\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u548cAI\u5de5\u5177\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "motivation": "\u5f53\u524d\u7684\u6cd5\u5f8b\u6587\u6863\u5efa\u6a21\u6846\u67b6\uff08\u5982FRBR/FRBRoo\u548cAkoma Ntoso\uff09\u867d\u7136\u5728\u5b8f\u89c2\u5c42\u9762\u6709\u6548\uff0c\u4f46\u5728\u7ec4\u4ef6\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u7248\u672c\u63a7\u5236\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u652f\u6301\u53ef\u9760\u7684\u65f6\u95f4\u70b9\u91cd\u5efa\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6269\u5c55FRBRoo\u6846\u67b6\u7684\u7ed3\u6784\u5316\u65f6\u95f4\u6a21\u578b\uff0c\u5b9a\u4e49\u4e86Expressio - \u65f6\u95f4\u7248\u672c\uff08TV\uff09\u548c\u8bed\u8a00\u7248\u672c\uff08LV\uff09\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f15\u5165Component Work\uff08CW\uff09\u3001Component Temporal Version\uff08CTV\uff09\u548cComponent Language Version\uff08CLV\uff09\u6765\u8ffd\u8e2a\u6cd5\u5f8b\u6587\u672c\u5404\u90e8\u5206\u7684\u751f\u547d\u5468\u671f\u3002", "result": "\u6a21\u578b\u80fd\u591f\u7cbe\u786e\u5730\u8ffd\u8e2a\u548c\u91cd\u5efa\u6cd5\u5f8b\u6587\u672c\u4e2d\u6bcf\u4e2a\u90e8\u5206\u5728\u7279\u5b9a\u65f6\u95f4\u70b9\u7684\u72b6\u6001\uff0c\u652f\u6301\u5386\u53f2\u5206\u6790\u548c\u5f71\u54cd\u8bc4\u4f30\u3002\u4ee5\u5df4\u897f\u8054\u90a6\u5baa\u6cd5\u4e3a\u4f8b\uff0c\u5c55\u793a\u4e86\u6bcf\u6b21\u4fee\u6b63\u6848\u5982\u4f55\u521b\u5efa\u65b0\u7684\u7ec4\u4ef6\u65f6\u95f4\u7248\u672c\uff0c\u800c\u4e0d\u53d7\u5f71\u54cd\u7684\u90e8\u5206\u4fdd\u7559\u539f\u6709\u7248\u672c\u3002", "conclusion": "\u8be5\u6a21\u578b\u514b\u670d\u4e86\u73b0\u6709\u751f\u6210\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u51c6\u786e\u7684\u5386\u53f2\u5206\u6790\u548c\u5f71\u54cd\u8bc4\u4f30\u7684\u9ad8\u7ea7\u6cd5\u5f8b\u4fe1\u606f\u7cfb\u7edf\u3001\u77e5\u8bc6\u56fe\u8c31\u548cAI\u5de5\u5177\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.07896", "pdf": "https://arxiv.org/pdf/2506.07896", "abs": "https://arxiv.org/abs/2506.07896", "authors": ["Shoko Oka"], "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages, Additional resources available on GitHub repository", "summary": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.", "AI": {"tldr": "Recent advancements in LLMs have revitalized philosophical debates on AI. This study investigates if modern LLMs can address the Frame Problem and Symbol Grounding Problem by testing 13 models on benchmark tasks, finding that closed models perform consistently well while open-source models show variability.", "motivation": "To determine whether modern LLMs possess cognitive capacities to address fundamental philosophical challenges like the Frame Problem and Symbol Grounding Problem which were historically unsolvable within traditional symbolic AI systems.", "method": "Designed two benchmark tasks reflecting the core of each problem, administered them under zero-shot conditions to 13 prominent LLMs (both closed and open-source), and assessed the quality of outputs across five trials using criteria such as contextual reasoning, semantic coherence, and information filtering.", "result": "Closed models consistently achieved high scores while open-source models showed variability in performance due to differences in model size, quantization, and instruction tuning.", "conclusion": "Select modern LLMs may be acquiring capacities sufficient to produce meaningful and stable responses to long-standing theoretical challenges."}}
{"id": "2506.06866", "pdf": "https://arxiv.org/pdf/2506.06866", "abs": "https://arxiv.org/abs/2506.06866", "authors": ["Dongyeop Lee", "Kwanhee Lee", "Jinseok Chung", "Namhoon Lee"], "title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Sparsifying neural networks often suffers from seemingly inevitable\nperformance degradation, and it remains challenging to restore the original\nperformance despite much recent progress. Motivated by recent studies in robust\noptimization, we aim to tackle this problem by finding subnetworks that are\nboth sparse and flat at the same time. Specifically, we formulate pruning as a\nsparsity-constrained optimization problem where flatness is encouraged as an\nobjective. We solve it explicitly via an augmented Lagrange dual approach and\nextend it further by proposing a generalized projection operation, resulting in\nnovel pruning methods called SAFE and its extension, SAFE$^+$. Extensive\nevaluations on standard image classification and language modeling tasks reveal\nthat SAFE consistently yields sparse networks with improved generalization\nperformance, which compares competitively to well-established baselines. In\naddition, SAFE demonstrates resilience to noisy data, making it well-suited for\nreal-world conditions.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u5e73\u6ed1\u6027\u76ee\u6807\u7684\u7a00\u758f\u7ea6\u675f\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u65b9\u6cd5SAFE\u53ca\u5176\u6269\u5c55\u7248\u672cSAFE$^+$\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u7a00\u758f\u7f51\u7edc\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5e76\u5bf9\u566a\u58f0\u6570\u636e\u5177\u6709\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u526a\u679d\u5f80\u5f80\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u5982\u4f55\u6062\u590d\u539f\u59cb\u6027\u80fd\u4ecd\u5177\u6311\u6218\u3002\u53d7\u5230\u9c81\u68d2\u4f18\u5316\u7814\u7a76\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u8bd5\u56fe\u5bfb\u627e\u540c\u65f6\u5177\u5907\u7a00\u758f\u6027\u548c\u5e73\u6ed1\u6027\u7684\u5b50\u7f51\u7edc\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u526a\u679d\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u4e00\u4e2a\u7a00\u758f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5176\u4e2d\u9f13\u52b1\u5e73\u6ed1\u6027\u4f5c\u4e3a\u76ee\u6807\u51fd\u6570\u3002\u901a\u8fc7\u589e\u5f3a\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u65b9\u6cd5\u663e\u5f0f\u6c42\u89e3\u8be5\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e00\u79cd\u5e7f\u4e49\u6295\u5f71\u64cd\u4f5c\uff0c\u4ece\u800c\u5f15\u51fa\u65b0\u7684\u526a\u679d\u65b9\u6cd5SAFE\u548c\u5176\u6269\u5c55\u7248\u672cSAFE$^+$\u3002", "result": "\u5728\u6807\u51c6\u56fe\u50cf\u5206\u7c7b\u548c\u8bed\u8a00\u5efa\u6a21\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cSAFE\u80fd\u591f\u6301\u7eed\u751f\u6210\u5177\u6709\u6539\u8fdb\u6cdb\u5316\u6027\u80fd\u7684\u7a00\u758f\u7f51\u7edc\uff0c\u4e0e\u5df2\u5efa\u7acb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002\u6b64\u5916\uff0cSAFE\u5bf9\u566a\u58f0\u6570\u636e\u8868\u73b0\u51fa\u8f83\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684SAFE\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u6539\u5584\u7a00\u758f\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5bf9\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u566a\u58f0\u6570\u636e\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2506.07915", "pdf": "https://arxiv.org/pdf/2506.07915", "abs": "https://arxiv.org/abs/2506.07915", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement", "categories": ["cs.AI", "cs.CL", "cs.SY", "eess.SY"], "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible\n  publication", "summary": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.", "AI": {"tldr": "In dynamic environments, autonomous decision-making is limited by the gap between an agent's internal model and evolving reality. To address this, LUCIFER integrates hierarchical decision-making architecture with RL and LLMs to translate human insights into actionable intelligence for autonomous systems.", "motivation": "The motivation lies in bridging the gap between an agent's internal model and evolving operational context in dynamic environments, leveraging the contextual bias of human domain stakeholders.", "method": "Propose LUCIFER, a framework integrating hierarchical decision-making architecture with RL and LLMs. LLMs serve dual roles: context extractors structuring stakeholder input and zero-shot exploration facilitators guiding action selection.", "result": "LUCIFER improves exploration efficiency and decision quality, outperforming flat, goal-conditioned policies.", "conclusion": "LUCIFER demonstrates the potential of context-driven decision-making where autonomous systems leverage human contextual knowledge."}}
{"id": "2506.06873", "pdf": "https://arxiv.org/pdf/2506.06873", "abs": "https://arxiv.org/abs/2506.06873", "authors": ["Armin Behnamnia", "Gholamali Aminian", "Alireza Aghaei", "Chengchun Shi", "Vincent Y. F. Tan", "Hamid R. Rabiee"], "title": "Log-Sum-Exponential Estimator for Off-Policy Evaluation and Learning", "categories": ["cs.LG", "stat.ML"], "comment": "Accepted as spotlight poster in ICML 2025", "summary": "Off-policy learning and evaluation leverage logged bandit feedback datasets,\nwhich contain context, action, propensity score, and feedback for each data\npoint. These scenarios face significant challenges due to high variance and\npoor performance with low-quality propensity scores and heavy-tailed reward\ndistributions. We address these issues by introducing a novel estimator based\non the log-sum-exponential (LSE) operator, which outperforms traditional\ninverse propensity score estimators. Our LSE estimator demonstrates variance\nreduction and robustness under heavy-tailed conditions. For off-policy\nevaluation, we derive upper bounds on the estimator's bias and variance. In the\noff-policy learning scenario, we establish bounds on the regret -- the\nperformance gap between our LSE estimator and the optimal policy -- assuming\nbounded $(1+\\epsilon)$-th moment of weighted reward. Notably, we achieve a\nconvergence rate of $O(n^{-\\epsilon/(1+ \\epsilon)})$ for the regret bounds,\nwhere $\\epsilon \\in [0,1]$ and $n$ is the size of logged bandit feedback\ndataset. Theoretical analysis is complemented by comprehensive empirical\nevaluations in both off-policy learning and evaluation scenarios, confirming\nthe practical advantages of our approach. The code for our estimator is\navailable at the following link:\nhttps://github.com/armin-behnamnia/lse-offpolicy-learning.", "AI": {"tldr": "The paper introduces a novel LSE-based estimator for off-policy learning and evaluation that reduces variance, handles heavy-tailed rewards, and outperforms traditional methods.", "motivation": "Off-policy learning and evaluation face challenges with high variance, low-quality propensity scores, and heavy-tailed reward distributions.", "method": "A new estimator based on the log-sum-exponential (LSE) operator is introduced. For off-policy evaluation, upper bounds on bias and variance are derived. In off-policy learning, regret bounds are established assuming bounded weighted reward moment.", "result": "The LSE estimator demonstrates variance reduction and robustness under heavy-tailed conditions. A convergence rate of O(n^(-\u03b5/(1+ \u03b5))) for regret bounds is achieved.", "conclusion": "Theoretical analysis is supported by empirical evaluations, confirming the practical advantages of the LSE estimator in both off-policy learning and evaluation."}}
{"id": "2506.07927", "pdf": "https://arxiv.org/pdf/2506.07927", "abs": "https://arxiv.org/abs/2506.07927", "authors": ["Jiayi Sheng", "Luna Lyu", "Jikai Jin", "Tony Xia", "Alex Gu", "James Zou", "Pan Lu"], "title": "Solving Inequality Proofs with Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "52 pages, 16 figures", "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.", "AI": {"tldr": "Inequality proving is a challenging area for LLMs. The paper proposes an informal task formulation with two subtasks and introduces IneqMath, a dataset of Olympiad-level inequalities. It also develops an LLM-as-judge evaluation framework revealing that top models have low overall accuracy in rigorous proof construction.", "motivation": "To address the limitations of existing datasets which are scarce, synthetic, or rigidly formal, and to provide a better understanding of LLMs' capabilities in advanced mathematical reasoning.", "method": "The paper recasts inequality proving into two subtasks: bound estimation and relation prediction. It releases IneqMath, a curated dataset of Olympiad-level inequalities, and develops an LLM-as-judge evaluation framework with step-wise judges.", "result": "Systematic evaluation shows that even top LLMs achieve less than 10% overall accuracy on IneqMath under step-wise scrutiny, indicating a significant gap in their ability to construct rigorous proofs compared to finding answers.", "conclusion": "Scaling model size or computation yields limited gains in proof correctness. Promising research directions include theorem-guided reasoning and self-refinement."}}
{"id": "2506.06884", "pdf": "https://arxiv.org/pdf/2506.06884", "abs": "https://arxiv.org/abs/2506.06884", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "FREE: Fast and Robust Vision Language Models with Early Exits", "categories": ["cs.LG", "cs.CV"], "comment": "To appear at the Association of Computational Linguistics (ACL) 2025\n  Conference", "summary": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE.", "AI": {"tldr": "In this paper, the authors propose FREE, an adversarial training approach within a GAN-based framework to employ Early Exit strategies in Vision-Language Models (VLMs) for increasing inference speed with minimal performance drop.", "motivation": "The motivation of this paper is to address the challenge of large model size in VLMs that leads to high inference latency, which hinders their application in real-world scenarios.", "method": "The method proposed is named FREE, which uses a GAN-based framework. Each exit includes a transformer layer and a classifier. The transformer layer is adversarially trained to generate feature representations similar to the final layer, while the feature classifier acts as the discriminator.", "result": "Experimental results show that the method speeds up the inference process by more than 1.51x while maintaining comparable performance. It also enhances accuracy and model robustness by reducing overthinking and mitigating the mid-crisis phenomenon.", "conclusion": "FREE effectively employs Early Exit strategies in VLMs, achieving faster inference with minimal performance degradation, thus making VLMs more practical for real-world applications."}}
{"id": "2506.07940", "pdf": "https://arxiv.org/pdf/2506.07940", "abs": "https://arxiv.org/abs/2506.07940", "authors": ["Christopher Subia-Waud"], "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "AI": {"tldr": "The paper introduces Gradients, a decentralized AutoML platform that uses economic incentives for hyperparameter optimization. It outperforms existing platforms with an 82.8% win rate and significant improvements in various tasks.", "motivation": "Existing AutoML platforms rely on single optimization strategies that only explore a fraction of viable hyperparameter configurations.", "method": "Gradients transforms hyperparameter optimization into a competitive marketplace where independent miners compete to discover optimal configurations using economic incentives.", "result": "Gradients achieves an 82.8% win rate against HuggingFace AutoTrain and 100% against TogetherAI, Databricks, and Google Cloud, with mean improvements of 11.8% and 42.1% respectively. Strong gains are observed in complex reasoning, retrieval tasks, and person-specific generation.", "conclusion": "Competitive, economically-driven approaches can systematically discover superior configurations that centralized AutoML consistently miss."}}
{"id": "2506.07963", "pdf": "https://arxiv.org/pdf/2506.07963", "abs": "https://arxiv.org/abs/2506.07963", "authors": ["Jixiang Hong", "Yiran Zhang", "Guanzhong Wang", "Yi Liu", "Ji-Rong Wen", "Rui Yan"], "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u81ea\u76d1\u7763\u53cc\u5956\u52b1\u673a\u5236\uff0c\u589e\u5f3a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\uff0c\u5728\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u6587\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bb9\u6613\u4ea7\u751f\u4e0e\u89c6\u89c9\u8f93\u5165\u77db\u76fe\u7684\u6587\u672c\u54cd\u5e94\u6216\u65e0\u6cd5\u9075\u5faa\u6587\u672c\u5230\u56fe\u50cf\u63d0\u793a\uff0c\u4e14\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u9700\u8981\u5916\u90e8\u76d1\u7763\u4e14\u4ec5\u89e3\u51b3\u5355\u5411\u4efb\u52a1\u3002", "method": "\u57fa\u4e8e\u7406\u89e3\u4e0e\u751f\u6210\u4e3a\u9006\u5bf9\u5076\u4efb\u52a1\u7684\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e00\u79cd\u81ea\u76d1\u7763\u53cc\u5956\u52b1\u673a\u5236\uff1a\u5bf9\u7ed9\u5b9a\u8f93\u5165\u91c7\u6837\u591a\u4e2a\u8f93\u51fa\u540e\u53cd\u8f6c\u8f93\u5165-\u8f93\u51fa\u5bf9\uff0c\u4ee5\u8ba1\u7b97\u6a21\u578b\u7684\u53cc\u4f3c\u7136\u4f5c\u4e3a\u81ea\u6211\u5956\u52b1\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u4efb\u4f55\u5916\u90e8\u76d1\u7763\u5373\u53ef\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6587\u672c\u5230\u56fe\u50cf\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u81ea\u76d1\u7763\u53cc\u5956\u52b1\u673a\u5236\u6709\u6548\u589e\u5f3a\u4e86\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2506.06895", "pdf": "https://arxiv.org/pdf/2506.06895", "abs": "https://arxiv.org/abs/2506.06895", "authors": ["Jihao Andreas Lin", "Sebastian Ament", "Maximilian Balandat", "David Eriksson", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "Eytan Bakshy"], "title": "Scalable Gaussian Processes with Latent Kronecker Structure", "categories": ["cs.LG", "stat.ML"], "comment": "International Conference on Machine Learning 2025", "summary": "Applying Gaussian processes (GPs) to very large datasets remains a challenge\ndue to limited computational scalability. Matrix structures, such as the\nKronecker product, can accelerate operations significantly, but their\napplication commonly entails approximations or unrealistic assumptions. In\nparticular, the most common path to creating a Kronecker-structured kernel\nmatrix is by evaluating a product kernel on gridded inputs that can be\nexpressed as a Cartesian product. However, this structure is lost if any\nobservation is missing, breaking the Cartesian product structure, which\nfrequently occurs in real-world data such as time series. To address this\nlimitation, we propose leveraging latent Kronecker structure, by expressing the\nkernel matrix of observed values as the projection of a latent Kronecker\nproduct. In combination with iterative linear system solvers and pathwise\nconditioning, our method facilitates inference of exact GPs while requiring\nsubstantially fewer computational resources than standard iterative methods. We\ndemonstrate that our method outperforms state-of-the-art sparse and variational\nGPs on real-world datasets with up to five million examples, including\nrobotics, automated machine learning, and climate applications.", "AI": {"tldr": "\u901a\u8fc7\u5229\u7528\u6f5c\u5728\u7684Kronecker\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u7cbe\u786e\u7684\u9ad8\u65af\u8fc7\u7a0b\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u5b9e\u9645\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u7a00\u758f\u548c\u53d8\u5206\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u3002", "motivation": "\u5e94\u7528\u9ad8\u65af\u8fc7\u7a0b\u5230\u975e\u5e38\u5927\u7684\u6570\u636e\u96c6\u4e0a\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u56e0\u4e3a\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u6709\u9650\u3002\u77e9\u9635\u7ed3\u6784\uff08\u5982Kronecker\u79ef\uff09\u867d\u7136\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u64cd\u4f5c\uff0c\u4f46\u5176\u5e94\u7528\u901a\u5e38\u6d89\u53ca\u8fd1\u4f3c\u6216\u4e0d\u5207\u5b9e\u9645\u7684\u5047\u8bbe\u3002\u7279\u522b\u662f\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6570\u636e\uff08\u5982\u65f6\u95f4\u5e8f\u5217\uff09\u65f6\uff0c\u7f3a\u5931\u89c2\u6d4b\u503c\u4f1a\u7834\u574fCartesian\u79ef\u7ed3\u6784\uff0c\u4ece\u800c\u5bfc\u81f4\u65e0\u6cd5\u4f7f\u7528Kronecker\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u5229\u7528\u6f5c\u5728\u7684Kronecker\u7ed3\u6784\uff0c\u5c06\u89c2\u5bdf\u503c\u7684\u6838\u77e9\u9635\u8868\u793a\u4e3a\u6f5c\u5728Kronecker\u4e58\u79ef\u7684\u6295\u5f71\u3002\u7ed3\u5408\u8fed\u4ee3\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u5668\u548c\u8def\u5f84\u6761\u4ef6\u5316\u65b9\u6cd5\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u9ad8\u65af\u8fc7\u7a0b\u63a8\u65ad\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u591a\u8fbe\u4e94\u767e\u4e07\u4e2a\u6837\u672c\u7684\u5b9e\u9645\u6570\u636e\u96c6\u65f6\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u7a00\u758f\u548c\u53d8\u5206\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u3002\u8fd9\u4e9b\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u673a\u5668\u4eba\u3001\u81ea\u52a8\u673a\u5668\u5b66\u4e60\u548c\u6c14\u5019\u5e94\u7528\u7b49\u9886\u57df\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u5730\u89e3\u51b3\u4e86\u73b0\u6709Kronecker\u7ed3\u6784\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u65f6\u65e2\u4fdd\u6301\u4e86\u7cbe\u786e\u6027\u53c8\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.07982", "pdf": "https://arxiv.org/pdf/2506.07982", "abs": "https://arxiv.org/abs/2506.07982", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "title": "$\u03c4^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "AI": {"tldr": "\u73b0\u6709\u7684\u5bf9\u8bddAI\u4ee3\u7406\u57fa\u51c6\u6a21\u62df\u4e86\u5355\u4e00\u63a7\u5236\u73af\u5883\uff0c\u5176\u4e2d\u53ea\u6709AI\u4ee3\u7406\u53ef\u4ee5\u4f7f\u7528\u5de5\u5177\u4e0e\u4e16\u754c\u4e92\u52a8\uff0c\u800c\u7528\u6237\u53ea\u662f\u88ab\u52a8\u7684\u4fe1\u606f\u63d0\u4f9b\u8005\u3002\u8fd9\u4e0e\u73b0\u5b9e\u573a\u666f\u4e0d\u540c\uff0c\u4f8b\u5982\u6280\u672f\u652f\u6301\u4e2d\uff0c\u7528\u6237\u9700\u8981\u79ef\u6781\u53c2\u4e0e\u4fee\u6539\uff08\u5171\u4eab\uff09\u4e16\u754c\u7684\u72b6\u6001\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u03c4\u00b2-bench\uff0c\u5177\u6709\u56db\u4e2a\u5173\u952e\u8d21\u732e\uff1a1) \u4e00\u79cd\u65b0\u9896\u7684\u7535\u4fe1\u53cc\u63a7\u5236\u57df\uff0c\u5efa\u6a21\u4e3aDec-POMDP\uff0c\u5176\u4e2d\u4ee3\u7406\u548c\u7528\u6237\u90fd\u5229\u7528\u5de5\u5177\u5728\u5171\u4eab\u3001\u52a8\u6001\u73af\u5883\u4e2d\u884c\u52a8\uff0c\u6d4b\u8bd5\u4ee3\u7406\u534f\u8c03\u548c\u901a\u4fe1\uff1b2) \u4e00\u4e2a\u7ec4\u5408\u4efb\u52a1\u751f\u6210\u5668\uff0c\u7a0b\u5e8f\u5316\u5730\u4ece\u539f\u5b50\u7ec4\u4ef6\u521b\u5efa\u591a\u6837\u4e14\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\uff0c\u786e\u4fdd\u9886\u57df\u8986\u76d6\u548c\u53d7\u63a7\u590d\u6742\u6027\uff1b3) \u4e00\u4e2a\u53ef\u9760\u7528\u6237\u6a21\u62df\u5668\uff0c\u4e0e\u73af\u5883\u7d27\u5bc6\u8026\u5408\uff0c\u5176\u884c\u4e3a\u53d7\u5de5\u5177\u548c\u53ef\u89c2\u6d4b\u72b6\u6001\u7684\u7ea6\u675f\uff0c\u63d0\u9ad8\u6a21\u62df\u4fdd\u771f\u5ea6\uff1b4) \u901a\u8fc7\u591a\u79cd\u6d88\u878d\u5206\u6790\u5bf9\u4ee3\u7406\u6027\u80fd\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u5305\u62ec\u533a\u5206\u7531\u63a8\u7406\u4ea7\u751f\u7684\u9519\u8bef\u4e0e\u7531\u901a\u4fe1/\u534f\u8c03\u4ea7\u751f\u7684\u9519\u8bef\u3002\u7279\u522b\u662f\uff0c\u6211\u4eec\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u5f53\u4ee3\u7406\u4ece\u65e0\u7528\u6237\u8f6c\u5411\u53cc\u63a7\u5236\u65f6\uff0c\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u663e\u4e86\u5f15\u5bfc\u7528\u6237\u7684\u6311\u6218\u3002\u603b\u4f53\u800c\u8a00\uff0c\u03c4\u00b2-bench\u4e3a\u5fc5\u987b\u6709\u6548\u63a8\u7406\u5e76\u6307\u5bfc\u7528\u6237\u884c\u52a8\u7684\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bddAI\u4ee3\u7406\u57fa\u51c6\u6a21\u62df\u7684\u662f\u5355\u4e00\u63a7\u5236\u73af\u5883\uff0c\u5373\u53ea\u6709AI\u4ee3\u7406\u80fd\u591f\u4f7f\u7528\u5de5\u5177\u4e0e\u4e16\u754c\u4e92\u52a8\uff0c\u800c\u7528\u6237\u4ec5\u4f5c\u4e3a\u88ab\u52a8\u4fe1\u606f\u63d0\u4f9b\u8005\u3002\u8fd9\u79cd\u8bbe\u5b9a\u4e0e\u5b9e\u9645\u573a\u666f\u4e0d\u7b26\uff0c\u5982\u6280\u672f\u652f\u6301\u7b49\u573a\u666f\u4e2d\uff0c\u7528\u6237\u9700\u8981\u4e3b\u52a8\u53c2\u4e0e\u4fee\u6539\u5171\u4eab\u4e16\u754c\u7684\u72b6\u6001\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6765\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7535\u4fe1\u53cc\u63a7\u5236\u57df\uff0c\u5efa\u6a21\u4e3aDec-POMDP\uff0c\u5141\u8bb8\u4ee3\u7406\u548c\u7528\u6237\u540c\u65f6\u5229\u7528\u5de5\u5177\u5728\u5171\u4eab\u52a8\u6001\u73af\u5883\u4e2d\u884c\u52a8\uff1b2. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ec4\u5408\u4efb\u52a1\u751f\u6210\u5668\uff0c\u80fd\u591f\u7a0b\u5e8f\u5316\u5730\u4ece\u539f\u5b50\u7ec4\u4ef6\u521b\u5efa\u591a\u6837\u5316\u4e14\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\uff0c\u4fdd\u8bc1\u9886\u57df\u8986\u76d6\u548c\u53d7\u63a7\u590d\u6742\u6027\uff1b3. \u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u7528\u6237\u6a21\u62df\u5668\uff0c\u8be5\u6a21\u62df\u5668\u4e0e\u73af\u5883\u7d27\u5bc6\u7ed3\u5408\uff0c\u5176\u884c\u4e3a\u53d7\u5230\u5de5\u5177\u548c\u53ef\u89c2\u6d4b\u72b6\u6001\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u6a21\u62df\u7684\u771f\u5b9e\u5ea6\uff1b4. \u8fdb\u884c\u4e86\u591a\u65b9\u9762\u7684\u6d88\u878d\u5206\u6790\uff0c\u4ee5\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\uff0c\u5305\u62ec\u533a\u5206\u63a8\u7406\u9519\u8bef\u4e0e\u901a\u4fe1/\u534f\u8c03\u9519\u8bef\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u4ee3\u7406\u4ece\u65e0\u7528\u6237\u63a7\u5236\u8f6c\u5411\u53cc\u63a7\u5236\u73af\u5883\u65f6\uff0c\u6027\u80fd\u6709\u663e\u8457\u4e0b\u964d\uff0c\u7a81\u51fa\u4e86\u5f15\u5bfc\u7528\u6237\u7684\u6311\u6218\u3002", "conclusion": "\u03c4\u00b2-bench\u4e3a\u9700\u8981\u6709\u6548\u63a8\u7406\u5e76\u6307\u5bfc\u7528\u6237\u884c\u52a8\u7684\u5bf9\u8bddAI\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u4e0e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2506.06907", "pdf": "https://arxiv.org/pdf/2506.06907", "abs": "https://arxiv.org/abs/2506.06907", "authors": ["Fred Xu", "Thomas Markovich"], "title": "Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks have achieved impressive results across diverse network\nmodeling tasks, but accurately estimating uncertainty on graphs remains\ndifficult, especially under distributional shifts. Unlike traditional\nuncertainty estimation, graph-based uncertainty must account for randomness\narising from both the graph's structure and its label distribution, which adds\ncomplexity. In this paper, making an analogy between the evolution of a\nstochastic partial differential equation (SPDE) driven by Matern Gaussian\nProcess and message passing using GNN layers, we present a principled way to\ndesign a novel message passing scheme that incorporates spatial-temporal noises\nmotivated by the Gaussian Process approach to SPDE. Our method simultaneously\ncaptures uncertainty across space and time and allows explicit control over the\ncovariance kernel smoothness, thereby enhancing uncertainty estimates on graphs\nwith both low and high label informativeness. Our extensive experiments on\nOut-of-Distribution (OOD) detection on graph datasets with varying label\ninformativeness demonstrate the soundness and superiority of our model to\nexisting approaches.", "AI": {"tldr": "The paper proposes a new method for uncertainty estimation in Graph Neural Networks (GNNs) using an analogy with stochastic partial differential equations (SPDE). This method incorporates spatial-temporal noises and enhances uncertainty estimates on graphs with varying label informativeness.", "motivation": "Current methods for estimating uncertainty in GNNs are inadequate, particularly under distributional shifts. Traditional approaches do not account for randomness from both graph structure and label distribution.", "method": "The authors draw an analogy between the evolution of SPDE driven by Matern Gaussian Process and message passing in GNN layers to design a novel message passing scheme that incorporates spatial-temporal noises. This approach allows explicit control over the covariance kernel smoothness.", "result": "Experiments on OOD detection show the effectiveness and superiority of the proposed model across datasets with different levels of label informativeness.", "conclusion": "The paper presents a principled way to enhance uncertainty estimates in GNNs, capturing uncertainty across space and time, and demonstrating superior performance in OOD detection tasks."}}
{"id": "2506.08012", "pdf": "https://arxiv.org/pdf/2506.08012", "abs": "https://arxiv.org/abs/2506.08012", "authors": ["Penghao Wu", "Shengnan Ma", "Bo Wang", "Jiaheng Yu", "Lewei Lu", "Ziwei Liu"], "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior", "categories": ["cs.AI", "cs.CV"], "comment": "Project Page at https://penghao-wu.github.io/GUI_Reflection/", "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.", "AI": {"tldr": "Multimodal Large Language Models (MLLMs) have shown great potential in revolutionizing Graphical User Interface (GUI) automation, but existing GUI models lack reflection and error recovery capabilities. To address this issue, the authors propose GUI-Reflection, a novel framework that integrates self-reflection and error correction capabilities into end-to-end multimodal GUI models throughout dedicated training stages.", "motivation": "The motivation of this paper is to bridge the gap in current GUI models which mostly rely on learning from nearly error-free offline trajectories and thus lack reflection and error recovery capabilities.", "method": "The method involves proposing GUI-Reflection, a framework that integrates self-reflection and error correction capabilities into GUI models through three training stages: GUI-specific pre-training, offline supervised fine-tuning (SFT), and online reflection tuning. The framework includes scalable data pipelines for automatic data construction, a GUI-Reflection Task Suite for learning and evaluation, a diverse environment for online training, and an iterative online reflection tuning algorithm.", "result": "The proposed framework equips GUI agents with self-reflection and correction capabilities, leading to more robust, adaptable, and intelligent GUI automation.", "conclusion": "All data, models, environments, and tools will be released publicly, paving the way for more advanced GUI automation."}}
{"id": "2506.06917", "pdf": "https://arxiv.org/pdf/2506.06917", "abs": "https://arxiv.org/abs/2506.06917", "authors": ["Shangjie Du", "Hui Wei", "Dong Yoon Lee", "Zhizhang Hu", "Shijia Pan"], "title": "Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ACM Transactions on Sensor Networks (TOSN) 2025", "summary": "This work introduces GraPhy, a graph-based, physics-guided learning framework\nfor high-resolution and accurate air quality modeling in urban areas with\nlimited monitoring data. Fine-grained air quality monitoring information is\nessential for reducing public exposure to pollutants. However, monitoring\nnetworks are often sparse in socioeconomically disadvantaged regions, limiting\nthe accuracy and resolution of air quality modeling. To address this, we\npropose a physics-guided graph neural network architecture called GraPhy with\nlayers and edge features designed specifically for low-resolution monitoring\ndata. Experiments using data from California's socioeconomically disadvantaged\nSan Joaquin Valley show that GraPhy achieves the overall best performance\nevaluated by mean squared error (MSE), mean absolute error (MAE), and R-square\nvalue (R2), improving the performance by 9%-56% compared to various baseline\nmodels. Moreover, GraPhy consistently outperforms baselines across different\nspatial heterogeneity levels, demonstrating the effectiveness of our model\ndesign.", "AI": {"tldr": "GraPhy is a physics-guided graph neural network designed for high-resolution air quality modeling in urban areas with limited monitoring data.", "motivation": "Fine-grained air quality monitoring information is essential for reducing public exposure to pollutants, but current monitoring networks are often sparse in socioeconomically disadvantaged regions, limiting the accuracy and resolution of air quality modeling.", "method": "The paper proposes GraPhy, a physics-guided graph neural network architecture with layers and edge features specifically designed for low-resolution monitoring data.", "result": "Experiments using data from California's San Joaquin Valley show that GraPhy achieves the best performance evaluated by MSE, MAE, and R2, improving the performance by 9%-56% compared to various baseline models.", "conclusion": "GraPhy consistently outperforms baselines across different spatial heterogeneity levels, demonstrating the effectiveness of its model design."}}
{"id": "2309.11082", "pdf": "https://arxiv.org/pdf/2309.11082", "abs": "https://arxiv.org/abs/2309.11082", "authors": ["Chen Jiang", "Hong Liu", "Xuzheng Yu", "Qing Wang", "Yuan Cheng", "Jia Xu", "Zhongyi Liu", "Qingpei Guo", "Wei Chu", "Ming Yang", "Yuan Qi"], "title": "Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Accepted by ACM MM 2023", "summary": "In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.", "AI": {"tldr": "The paper proposes two novel techniques, DMAE and TPM-CL, to improve contrastive learning for text-video retrieval by focusing on hard negative pairs and fine-grained semantic similarity.", "motivation": "To address the limitations of current contrastive learning methods in text-video retrieval which do not pay enough attention to hard negative pairs and lack the ability to model different levels of semantic similarity.", "method": "Propose DMAE to mine hard negative pairs and NegNCE loss to highlight their impacts; present TPM-CL module with adaptive token masking strategy to construct partial order triplet samples.", "result": "Outperforms existing methods on four widely-used text-video retrieval datasets (MSR-VTT, MSVD, DiDeMo, ActivityNet).", "conclusion": "The proposed approach significantly improves the performance of text-video retrieval by better handling hard negatives and modeling fine-grained semantic similarity."}}
{"id": "2506.06926", "pdf": "https://arxiv.org/pdf/2506.06926", "abs": "https://arxiv.org/abs/2506.06926", "authors": ["Wei Min Loh", "Jiaqi Shang", "Pascal Poupart"], "title": "Basis Transformers for Multi-Task Tabular Regression", "categories": ["cs.LG"], "comment": null, "summary": "Dealing with tabular data is challenging due to partial information, noise,\nand heterogeneous structure. Existing techniques often struggle to\nsimultaneously address key aspects of tabular data such as textual information,\na variable number of columns, and unseen data without metadata besides column\nnames. We propose a novel architecture, \\textit{basis transformers},\nspecifically designed to tackle these challenges while respecting inherent\ninvariances in tabular data, including hierarchical structure and the\nrepresentation of numeric values. We evaluate our design on a multi-task\ntabular regression benchmark, achieving an improvement of 0.338 in the median\n$R^2$ score and the lowest standard deviation across 34 tasks from the\nOpenML-CTR23 benchmark. Furthermore, our model has five times fewer parameters\nthan the best-performing baseline and surpasses pretrained large language model\nbaselines -- even when initialized from randomized weights.", "AI": {"tldr": "The paper introduces basis transformers, a new architecture for handling tabular data challenges, which performs well on benchmarks with fewer parameters.", "motivation": "Existing techniques struggle to handle key aspects of tabular data like textual information, variable columns, and unseen data without metadata.", "method": "Propose basis transformers, designed to respect invariances in tabular data including hierarchical structure and numeric value representation.", "result": "Achieved an improvement of 0.338 in the median $R^2$ score with the lowest standard deviation across 34 tasks from the OpenML-CTR23 benchmark.", "conclusion": "Basis transformers outperform pretrained large language model baselines, even when initialized from randomized weights, and have five times fewer parameters than the best-performing baseline."}}
{"id": "2406.00971", "pdf": "https://arxiv.org/pdf/2406.00971", "abs": "https://arxiv.org/abs/2406.00971", "authors": ["Vahid Azizi", "Fatemeh Koochaki"], "title": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 7 figures", "summary": "Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}", "AI": {"tldr": "The paper extends and fine-tunes MiniGPT-4 for the reverse designing task, demonstrating the extensibility of off-the-shelf VLMs like MiniGPT-4.", "motivation": "To explore the capability of Vision-Language Models (VLMs) in handling complex vision-language tasks such as reverse designing, which goes beyond traditional tasks by requiring comprehension of source image, edited version, and optional textual context simultaneously.", "method": "The authors extend and fine-tune MiniGPT-4 for the reverse designing task, which involves predicting edits and their parameters given a source image, an edited version, and an optional high-level textual edit description.", "result": "Experiments show that off-the-shelf VLMs, specifically MiniGPT-4, can be effectively extended and fine-tuned for complex tasks like reverse designing.", "conclusion": "MiniGPT-4 and similar VLMs have strong extensibility for complex vision-language tasks such as reverse designing."}}
{"id": "2411.12262", "pdf": "https://arxiv.org/pdf/2411.12262", "abs": "https://arxiv.org/abs/2411.12262", "authors": ["Raphael Merx", "Ad\u00e9rito Jos\u00e9 Guterres Correia", "Hanna Suominen", "Ekaterina Vylomova"], "title": "Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service", "categories": ["cs.CL", "cs.AI"], "comment": "to be published in LoResMT 2025", "summary": "Low-resource machine translation (MT) presents a diversity of community needs\nand application challenges that remain poorly understood. To complement surveys\nand focus groups, which tend to rely on small samples of respondents, we\npropose an observational study on actual usage patterns of tetun$.$org, a\nspecialized MT service for the Tetun language, which is the lingua franca in\nTimor-Leste. Our analysis of 100,000 translation requests reveals patterns that\nchallenge assumptions based on existing corpora. We find that users, many of\nthem students on mobile devices, typically translate text from a high-resource\nlanguage into Tetun across diverse domains including science, healthcare, and\ndaily life. This contrasts sharply with available Tetun corpora, which are\ndominated by news articles covering government and social issues. Our results\nsuggest that MT systems for institutionalized minority languages like Tetun\nshould prioritize accuracy on domains relevant to educational contexts, in the\nhigh-resource to low-resource direction. More broadly, this study demonstrates\nhow observational analysis can inform low-resource language technology\ndevelopment, by grounding research in practical community needs.", "AI": {"tldr": "\u901a\u8fc7\u5206\u679010\u4e07\u6b21\u7ffb\u8bd1\u8bf7\u6c42\uff0c\u53d1\u73b0\u8bb8\u591a\u7528\u6237\uff08\u5c24\u5176\u662f\u5b66\u751f\uff09\u4f7f\u7528tetun.org\u5c06\u9ad8\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u6210Tetun\uff0c\u4e3b\u8981\u6d89\u53ca\u79d1\u5b66\u3001\u533b\u7597\u548c\u65e5\u5e38\u751f\u6d3b\u7b49\u9886\u57df\u3002\u8fd9\u4e0e\u73b0\u6709\u7684\u8bed\u6599\u5e93\u5047\u8bbe\u4e0d\u540c\uff0c\u8868\u660e\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5e94\u4f18\u5148\u5173\u6ce8\u6559\u80b2\u76f8\u5173\u9886\u57df\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u5728\u793e\u533a\u9700\u6c42\u548c\u5e94\u7528\u6311\u6218\u65b9\u9762\u5c1a\u672a\u88ab\u5145\u5206\u7406\u89e3\uff0c\u4f20\u7edf\u7684\u8c03\u67e5\u65b9\u6cd5\u6837\u672c\u91cf\u8f83\u5c0f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u7814\u7a76\u65b9\u6cd5\u6765\u8865\u5145\u3002", "method": "\u901a\u8fc7\u5bf9Tetun\u8bed\u8a00\u4e13\u7528\u673a\u5668\u7ffb\u8bd1\u670d\u52a1tetun.org\u7684\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\u8fdb\u884c\u89c2\u5bdf\u7814\u7a76\uff0c\u5206\u6790\u4e8610\u4e07\u6b21\u7ffb\u8bd1\u8bf7\u6c42\u7684\u6570\u636e\u3002", "result": "\u53d1\u73b0\u7528\u6237\u591a\u4e3a\u4f7f\u7528\u79fb\u52a8\u8bbe\u5907\u7684\u5b66\u751f\uff0c\u4ed6\u4eec\u901a\u5e38\u5c06\u9ad8\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u6210Tetun\uff0c\u6db5\u76d6\u79d1\u5b66\u3001\u533b\u7597\u548c\u65e5\u5e38\u751f\u6d3b\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u800c\u975e\u73b0\u6709\u8bed\u6599\u5e93\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u65b0\u95fb\u6587\u7ae0\u3002", "conclusion": "\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u5e94\u4f18\u5148\u8003\u8651\u6559\u80b2\u76f8\u5173\u9886\u57df\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u89c2\u5bdf\u5206\u6790\u53ef\u4ee5\u66f4\u597d\u5730\u6ee1\u8db3\u5b9e\u9645\u793e\u533a\u9700\u6c42\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.06940", "pdf": "https://arxiv.org/pdf/2506.06940", "abs": "https://arxiv.org/abs/2506.06940", "authors": ["Geonhui Yoo", "Minhak Song", "Chulhee Yun"], "title": "Understanding Sharpness Dynamics in NN Training with a Minimalist Example: The Effects of Dataset Difficulty, Depth, Stochasticity, and More", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "When training deep neural networks with gradient descent, sharpness often\nincreases -- a phenomenon known as progressive sharpening -- before saturating\nat the edge of stability. Although commonly observed in practice, the\nunderlying mechanisms behind progressive sharpening remain poorly understood.\nIn this work, we study this phenomenon using a minimalist model: a deep linear\nnetwork with a single neuron per layer. We show that this simple model\neffectively captures the sharpness dynamics observed in recent empirical\nstudies, offering a simple testbed to better understand neural network\ntraining. Moreover, we theoretically analyze how dataset properties, network\ndepth, stochasticity of optimizers, and step size affect the degree of\nprogressive sharpening in the minimalist model. We then empirically demonstrate\nhow these theoretical insights extend to practical scenarios. This study offers\na deeper understanding of sharpness dynamics in neural network training,\nhighlighting the interplay between depth, training data, and optimizers.", "AI": {"tldr": "In this paper, the authors use a deep linear network with a single neuron per layer as a minimalist model to study progressive sharpening phenomenon in neural network training. They theoretically analyze how dataset properties, network depth, stochasticity of optimizers, and step size affect the degree of progressive sharpening. Then they empirically demonstrate how these theoretical insights extend to practical scenarios.", "motivation": "The motivation of this work is to better understand the mechanisms behind progressive sharpening during the training of deep neural networks with gradient descent.", "method": "The method used in this study involves studying progressive sharpening using a minimalist model - a deep linear network with a single neuron per layer. Theoretical analysis is performed on how various factors such as dataset properties, network depth, optimizer stochasticity, and step size impact progressive sharpening in this model.", "result": "The results show that this simple model effectively captures the sharpness dynamics observed in recent empirical studies on neural network training.", "conclusion": "This study concludes by offering a deeper understanding of sharpness dynamics in neural network training, emphasizing the interplay between depth, training data, and optimizers."}}
{"id": "2506.04762", "pdf": "https://arxiv.org/pdf/2506.04762", "abs": "https://arxiv.org/abs/2506.04762", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs)-based query expansion for information retrieval\naugments queries with generated hypothetical documents with LLMs. However, its\nperformance relies heavily on the scale of the language models (LMs),\nnecessitating larger, more advanced LLMs. This approach is costly,\ncomputationally intensive, and often has limited accessibility. To address\nthese limitations, we introduce GOLFer - Smaller LMs-Generated Documents\nHallucination Filter & Combiner - a novel method leveraging smaller open-source\nLMs for query expansion. GOLFer comprises two modules: a hallucination filter\nand a documents combiner. The former detects and removes non-factual and\ninconsistent sentences in generated documents, a common issue with smaller LMs,\nwhile the latter combines the filtered content with the query using a weight\nvector to balance their influence. We evaluate GOLFer alongside dominant\nLLM-based query expansion methods on three web search and ten low-resource\ndatasets. Experimental results demonstrate that GOLFer consistently outperforms\nother methods using smaller LMs, and maintains competitive performance against\nmethods using large-size LLMs, demonstrating its effectiveness.", "AI": {"tldr": "GOLFer is a new method for query expansion that uses smaller LMs and includes a hallucination filter and documents combiner, showing effectiveness and competitiveness against methods using large-size LLMs.", "motivation": "Current LLM-based query expansion methods are costly, computationally intensive, and have limited accessibility due to their reliance on large, advanced LLMs.", "method": "GOLFer consists of two modules: a hallucination filter that detects and removes non-factual/inconsistent sentences in generated documents, and a documents combiner that combines filtered content with the query using a weight vector.", "result": "GOLFer outperforms other methods using smaller LMs and maintains competitive performance against methods using large-size LLMs across three web search and ten low-resource datasets.", "conclusion": "GOLFer demonstrates its effectiveness by consistently outperforming other methods using smaller LMs and maintaining competitive performance against methods using large-size LLMs."}}
{"id": "2506.06954", "pdf": "https://arxiv.org/pdf/2506.06954", "abs": "https://arxiv.org/abs/2506.06954", "authors": ["Clinton Enwerem", "Aniruddh G. Puranic", "John S. Baras", "Calin Belta"], "title": "Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression", "categories": ["cs.LG", "cs.RO"], "comment": "13 pages, 4 figures. Submission under review", "summary": "Mainstream approximate action-value iteration reinforcement learning (RL)\nalgorithms suffer from overestimation bias, leading to suboptimal policies in\nhigh-variance stochastic environments. Quantile-based action-value iteration\nmethods reduce this bias by learning a distribution of the expected cost-to-go\nusing quantile regression. However, ensuring that the learned policy satisfies\nsafety constraints remains a challenge when these constraints are not\nexplicitly integrated into the RL framework. Existing methods often require\ncomplex neural architectures or manual tradeoffs due to combined cost\nfunctions. To address this, we propose a risk-regularized quantile-based\nalgorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety\nwithout complex architectures. We also provide theoretical guarantees on the\ncontraction properties of the risk-sensitive distributional Bellman operator in\nWasserstein space, ensuring convergence to a unique cost distribution.\nSimulations of a mobile robot in a dynamic reach-avoid task show that our\napproach leads to more goal successes, fewer collisions, and better\nsafety-performance trade-offs compared to risk-neutral methods.", "AI": {"tldr": "An RL algorithm with risk-regularized quantile-based method integrating CVaR is proposed to enforce safety in high-variance stochastic environments, showing better safety-performance trade-offs.", "motivation": "Mainstream RL algorithms suffer from overestimation bias and cannot ensure safety constraints in high-variance stochastic environments without complex architectures or manual tradeoffs.", "method": "Propose a risk-regularized quantile-based algorithm integrating CVaR to enforce safety without complex neural architectures, providing theoretical guarantees on the contraction properties of the risk-sensitive distributional Bellman operator.", "result": "Simulations show that the approach leads to more goal successes, fewer collisions, and better safety-performance trade-offs compared to risk-neutral methods.", "conclusion": "The proposed risk-regularized quantile-based algorithm with CVaR integration can effectively enforce safety in high-variance stochastic environments."}}
{"id": "2506.06276", "pdf": "https://arxiv.org/pdf/2506.06276", "abs": "https://arxiv.org/abs/2506.06276", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "TLDR: We show for the first time that normalizing flows can be scaled\n  for high-resolution and text-conditioned image synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "AI": {"tldr": "The paper introduces STARFlow, a scalable generative model based on normalizing flows that achieves strong performance in high-resolution image synthesis. It combines the expressive power of normalizing flows with the structured modeling capabilities of Autoregressive Transformers.", "motivation": "To create a scalable generative model for high-resolution image synthesis that can compete with state-of-the-art diffusion models in sample quality.", "method": "STARFlow's core is Transformer Autoregressive Flow (TARFlow), which integrates normalizing flows and Autoregressive Transformers. Innovations include a deep-shallow design, modeling in pretrained autoencoder latent space, and a novel guidance algorithm.", "result": "STARFlow achieves competitive performance in both class-conditional and text-conditional image generation tasks, approaching state-of-the-art diffusion models in sample quality.", "conclusion": "This work successfully demonstrates normalizing flows operating effectively at high scale and resolution, being the first to do so."}}
{"id": "2506.06977", "pdf": "https://arxiv.org/pdf/2506.06977", "abs": "https://arxiv.org/abs/2506.06977", "authors": ["Pengfei Hu", "Xiaoxue Han", "Fei Wang", "Yue Ning"], "title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Domain generalization has become a critical challenge in clinical prediction,\nwhere patient cohorts often exhibit shifting data distributions that degrade\nmodel performance. Typical domain generalization approaches struggle in\nreal-world healthcare settings for two main reasons: (1) patient-specific\ndomain labels are typically unavailable, making domain discovery especially\ndifficult; (2) purely data-driven approaches overlook key clinical insights,\nleading to a gap in medical knowledge integration. To address these problems,\nwe leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to\ngroup diseases into higher-level categories and discover more flexible latent\ndomains. In this paper, we introduce UdonCare, a hierarchy-guided framework\nthat iteratively prunes fine-grained domains, encodes these refined domains,\nand applies a Siamese-type inference mechanism to separate domain-related\nsignals from patient-level features. Experimental results on clinical datasets\n(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher\nperformance compared to other domain generalization baselines when substantial\ndomain gaps presents, highlighting the untapped potential of medical knowledge\nfor enhancing domain generalization in practical healthcare applications.", "AI": {"tldr": "UdonCare is a hierarchy-guided framework that uses medical ontologies to discover latent domains and separate domain-related signals from patient-level features, improving domain generalization in clinical prediction.", "motivation": "Domain generalization in clinical prediction faces challenges due to shifting data distributions and lack of patient-specific domain labels. Existing methods struggle in real-world healthcare settings because they often overlook important clinical insights.", "method": "The paper introduces UdonCare, which leverages hierarchical medical ontologies (e.g., ICD-9-CM) to group diseases into higher-level categories. It iteratively prunes fine-grained domains, encodes refined domains, and applies a Siamese-type inference mechanism to separate domain-related signals from patient-level features.", "result": "Experiments on MIMIC-III and MIMIC-IV datasets show that UdonCare achieves better performance compared to other domain generalization baselines when substantial domain gaps are present.", "conclusion": "The study highlights the potential of integrating medical knowledge for enhancing domain generalization in practical healthcare applications."}}
{"id": "2506.06283", "pdf": "https://arxiv.org/pdf/2506.06283", "abs": "https://arxiv.org/abs/2506.06283", "authors": ["Juexiao Zhou", "Zhongyi Han", "Mankun Xin", "Xingwei He", "Guotao Wang", "Jiaoyan Song", "Gongning Luo", "Wenjia He", "Xintong Li", "Yuetan Chu", "Juanwen Chen", "Bo Wang", "Xia Wu", "Wenwen Duan", "Zhixia Guo", "Liyan Bai", "Yilin Pan", "Xuefei Bi", "Lu Liu", "Long Feng", "Xiaonan He", "Xin Gao"], "title": "Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Global population aging presents increasing challenges to healthcare systems,\nwith coronary artery disease (CAD) responsible for approximately 17.8 million\ndeaths annually, making it a leading cause of global mortality. As CAD is\nlargely preventable, early detection and proactive management are essential. In\nthis work, we introduce DigitalShadow, an advanced early warning system for\nCAD, powered by a fine-tuned facial foundation model. The system is pre-trained\non 21 million facial images and subsequently fine-tuned into LiveCAD, a\nspecialized CAD risk assessment model trained on 7,004 facial images from 1,751\nsubjects across four hospitals in China. DigitalShadow functions passively and\ncontactlessly, extracting facial features from live video streams without\nrequiring active user engagement. Integrated with a personalized database, it\ngenerates natural language risk reports and individualized health\nrecommendations. With privacy as a core design principle, DigitalShadow\nsupports local deployment to ensure secure handling of user data.", "AI": {"tldr": "DigitalShadow is an early warning system for coronary artery disease (CAD) that uses facial recognition technology to assess CAD risk without active user engagement, ensuring privacy and personalization.", "motivation": "To address the challenges posed by global population aging and the high mortality rate associated with coronary artery disease (CAD), there is a need for early detection and proactive management of CAD. This work aims to introduce DigitalShadow as a solution for passive and contactless CAD risk assessment.", "method": "The system is pre-trained on 21 million facial images and fine-tuned into LiveCAD, a specialized CAD risk assessment model trained on 7,004 facial images from 1,751 subjects across four hospitals in China. It extracts facial features from live video streams and integrates with a personalized database to generate natural language risk reports and individualized health recommendations.", "result": "DigitalShadow functions passively and contactlessly, extracting facial features from live video streams without requiring active user engagement. It supports local deployment to ensure secure handling of user data.", "conclusion": "DigitalShadow represents an advanced early warning system for CAD, offering a novel approach to risk assessment through facial feature extraction. Its design prioritizes privacy and personalization, making it a promising tool in the proactive management of CAD."}}
{"id": "2506.06978", "pdf": "https://arxiv.org/pdf/2506.06978", "abs": "https://arxiv.org/abs/2506.06978", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Near Optimal Non-asymptotic Sample Complexity of 1-Identification", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Motivated by an open direction in existing literature, we study the\n1-identification problem, a fundamental multi-armed bandit formulation on pure\nexploration. The goal is to determine whether there exists an arm whose mean\nreward is at least a known threshold $\\mu_0$, or to output None if it believes\nsuch an arm does not exist. The agent needs to guarantee its output is correct\nwith probability at least $1-\\delta$. Degenne & Koolen 2019 has established the\nasymptotically tight sample complexity for the 1-identification problem, but\nthey commented that the non-asymptotic analysis remains unclear. We design a\nnew algorithm Sequential-Exploration-Exploitation (SEE), and conduct\ntheoretical analysis from the non-asymptotic perspective. Novel to the\nliterature, we achieve near optimality, in the sense of matching upper and\nlower bounds on the pulling complexity. The gap between the upper and lower\nbounds is up to a polynomial logarithmic factor. The numerical result also\nindicates the effectiveness of our algorithm, compared to existing benchmarks.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7eaf\u63a2\u7d22\u7684\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u4e2d\u76841-\u8bc6\u522b\u95ee\u9898\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5SEE\uff0c\u5e76\u4ece\u975e\u6e10\u8fd1\u89c6\u89d2\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u6027\u3002", "motivation": "\u53d7\u5230\u73b0\u6709\u6587\u732e\u4e2d\u4e00\u4e2a\u5f00\u653e\u65b9\u5411\u7684\u542f\u53d1\uff0c\u7814\u7a761-\u8bc6\u522b\u95ee\u9898\uff0c\u76ee\u6807\u662f\u786e\u5b9a\u662f\u5426\u5b58\u5728\u5e73\u5747\u5956\u52b1\u81f3\u5c11\u4e3a\u5df2\u77e5\u9608\u503c$\\mu_0$\u7684\u624b\u81c2\uff0c\u6216\u5728\u8ba4\u4e3a\u4e0d\u5b58\u5728\u8fd9\u6837\u7684\u624b\u81c2\u65f6\u8f93\u51faNone\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5Sequential-Exploration-Exploitation (SEE)\uff0c\u5e76\u4ece\u975e\u6e10\u8fd1\u89c6\u89d2\u8fdb\u884c\u7406\u8bba\u5206\u6790\u3002", "result": "\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u6027\uff0c\u4e0a\u4e0b\u754c\u4e4b\u95f4\u7684\u5dee\u8ddd\u4ec5\u4e3a\u591a\u9879\u5f0f\u5bf9\u6570\u56e0\u5b50\uff0c\u6570\u503c\u7ed3\u679c\u4e5f\u8868\u660e\u8be5\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u65b0\u7b97\u6cd5SEE\uff0c\u5728\u975e\u6e10\u8fd1\u5206\u6790\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u62c9\u52a8\u590d\u6742\u5ea6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.06286", "pdf": "https://arxiv.org/pdf/2506.06286", "abs": "https://arxiv.org/abs/2506.06286", "authors": ["Kevin Baum"], "title": "Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "accepted for the LNCS post proceedings of the AISoLA 2024 conference", "summary": "Recent advances in AI research make it increasingly plausible that artificial\nagents with consequential real-world impact will soon operate beyond tightly\ncontrolled environments. Ensuring that these agents are not only safe but that\nthey adhere to broader normative expectations is thus an urgent\ninterdisciplinary challenge. Multiple fields -- notably AI Safety, AI\nAlignment, and Machine Ethics -- claim to contribute to this task. However, the\nconceptual boundaries and interrelations among these domains remain vague,\nleaving researchers without clear guidance in positioning their work.\n  To address this meta-challenge, we develop a structured conceptual framework\nfor understanding AI alignment. Rather than focusing solely on alignment goals,\nwe introduce a taxonomy distinguishing the alignment aim (safety, ethicality,\nlegality, etc.), scope (outcome vs. execution), and constituency (individual\nvs. collective). This structural approach reveals multiple legitimate alignment\nconfigurations, providing a foundation for practical and philosophical\nintegration across domains, and clarifying what it might mean for an agent to\nbe aligned all-things-considered.", "AI": {"tldr": "Recent AI advancements raise concerns about agents operating in real-world settings. While fields like AI Safety and Machine Ethics aim to address these issues, their boundaries are unclear. This paper introduces a conceptual framework for understanding AI alignment with a taxonomy that considers alignment aim, scope, and constituency.", "motivation": "To provide clear guidance for researchers by developing a structured conceptual framework for understanding AI alignment.", "method": "Developing a taxonomy that distinguishes the alignment aim (safety, ethicality, legality, etc.), scope (outcome vs. execution), and constituency (individual vs. collective).", "result": "Reveals multiple legitimate alignment configurations and provides a foundation for practical and philosophical integration across domains.", "conclusion": "Clarifies what it might mean for an agent to be aligned all-things-considered."}}
{"id": "2506.06980", "pdf": "https://arxiv.org/pdf/2506.06980", "abs": "https://arxiv.org/abs/2506.06980", "authors": ["Sajib Acharjee Dip", "Uddip Acharjee Shuvo", "Dipanwita Mallick", "Abrar Rahman Abir", "Liqing Zhang"], "title": "MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure, 6 tables", "summary": "Cancer subtype classification is crucial for personalized treatment and\nprognostic assessment. However, effectively integrating multi-omic data remains\nchallenging due to the heterogeneous nature of genomic, epigenomic, and\ntranscriptomic features. In this work, we propose Modality-Aware\nCross-Attention MoXGATE, a novel deep-learning framework that leverages\ncross-attention and learnable modality weights to enhance feature fusion across\nmultiple omics sources. Our approach effectively captures inter-modality\ndependencies, ensuring robust and interpretable integration. Through\nexperiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)\ndatasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,\nachieving 95\\% classification accuracy. Ablation studies validate the\neffectiveness of cross-attention over simple concatenation and highlight the\nimportance of different omics modalities. Moreover, our model generalizes well\nto unseen cancer types e.g., breast cancer, underscoring its adaptability. Key\ncontributions include (1) a cross-attention-based multi-omic integration\nframework, (2) modality-weighted fusion for enhanced interpretability, (3)\napplication of focal loss to mitigate data imbalance, and (4) validation across\nmultiple cancer subtypes. Our results indicate that MoXGATE is a promising\napproach for multi-omic cancer subtype classification, offering improved\nperformance and biological generalizability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6MoXGATE\uff0c\u7528\u4e8e\u591a\u7ec4\u5b66\u6570\u636e\u7684\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\uff0c\u5177\u6709\u9ad8\u51c6\u786e\u6027\u548c\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u76ee\u524d\u6709\u6548\u6574\u5408\u57fa\u56e0\u7ec4\u3001\u8868\u89c2\u57fa\u56e0\u7ec4\u548c\u8f6c\u5f55\u7ec4\u7b49\u591a\u7ec4\u5b66\u6570\u636e\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6570\u636e\u7279\u5f81\u5177\u6709\u5f02\u8d28\u6027\uff0c\u96be\u4ee5\u8fdb\u884c\u7a33\u5065\u4e14\u53ef\u89e3\u91ca\u7684\u6574\u5408\u3002", "method": "\u63d0\u51fa\u540d\u4e3aModality-Aware Cross-Attention MoXGATE\u7684\u65b0\u6846\u67b6\uff0c\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u53ef\u5b66\u4e60\u7684\u6a21\u6001\u6743\u91cd\u6765\u589e\u5f3a\u591a\u7ec4\u5b66\u6570\u636e\u95f4\u7684\u7279\u5f81\u878d\u5408\uff0c\u5e76\u901a\u8fc7\u7126\u70b9\u635f\u5931\u51fd\u6570\u89e3\u51b3\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728GIAC\u548cBRCA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoXGATE\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u4e8695%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff1b\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u7684\u6709\u6548\u6027\u53ca\u4e0d\u540c\u7ec4\u5b66\u6a21\u6001\u7684\u91cd\u8981\u6027\u3002\u6b64\u5916\uff0c\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u764c\u75c7\u7c7b\u578b\u4e0a\u4e5f\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "MoXGATE\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u591a\u7ec4\u5b66\u764c\u75c7\u4e9a\u578b\u5206\u7c7b\u65b9\u6cd5\uff0c\u63d0\u4f9b\u66f4\u9ad8\u7684\u6027\u80fd\u548c\u751f\u7269\u5b66\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.06288", "pdf": "https://arxiv.org/pdf/2506.06288", "abs": "https://arxiv.org/abs/2506.06288", "authors": ["Xueying Ding", "Aakriti Mittal", "Achintya Gopal"], "title": "DELPHYNE: A Pre-Trained Model for General and Financial Time Series", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "comment": null, "summary": "Time-series data is a vital modality within data science communities. This is\nparticularly valuable in financial applications, where it helps in detecting\npatterns, understanding market behavior, and making informed decisions based on\nhistorical data. Recent advances in language modeling have led to the rise of\ntime-series pre-trained models that are trained on vast collections of datasets\nand applied to diverse tasks across financial domains. However, across\nfinancial applications, existing time-series pre-trained models have not shown\nboosts in performance over simple finance benchmarks in both zero-shot and\nfine-tuning settings. This phenomenon occurs because of a i) lack of financial\ndata within the pre-training stage, and ii) the negative transfer effect due to\ninherently different time-series patterns across domains. Furthermore,\ntime-series data is continuous, noisy, and can be collected at varying\nfrequencies and with varying lags across different variables, making this data\nmore challenging to model than languages. To address the above problems, we\nintroduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne\nachieves competitive performance to existing foundation and full-shot models\nwith few fine-tuning steps on publicly available datasets, and also shows\nsuperior performances on various financial tasks.", "AI": {"tldr": "Delphyne is a pre-trained model designed for financial time-series data that achieves competitive performance with few fine-tuning steps.", "motivation": "Existing time-series pre-trained models have not shown significant performance boosts in financial applications due to lack of financial data in pre-training and negative transfer effect from different domain patterns. Additionally, time-series data presents unique challenges such as continuity, noise, and varying frequencies.", "method": "Introduced Delphyne, a Pre-trained MoDEL for FINance TimE-series, which addresses the issues of existing models by being specifically designed for financial time-series data.", "result": "Delphyne achieves competitive performance to existing foundation and full-shot models with few fine-tuning steps on publicly available datasets, and shows superior performances on various financial tasks.", "conclusion": "Delphyne presents a promising approach for handling financial time-series data, overcoming the limitations of previous models."}}
{"id": "2506.06986", "pdf": "https://arxiv.org/pdf/2506.06986", "abs": "https://arxiv.org/abs/2506.06986", "authors": ["Austin Snyder", "Ryan Gallagher", "Boris Kovalerchuk"], "title": "Fully Explainable Classification Models Using Hyperblocks", "categories": ["cs.LG"], "comment": "7 pages, 8 figures, 6 tables", "summary": "Building on existing work with Hyperblocks, which classify data using minimum\nand maximum bounds for each attribute, we focus on enhancing interpretability,\ndecreasing training time, and reducing model complexity without sacrificing\naccuracy. This system allows subject matter experts (SMEs) to directly inspect\nand understand the model's decision logic without requiring extensive machine\nlearning expertise. To reduce Hyperblock complexity while retaining\nperformance, we introduce a suite of algorithms for Hyperblock simplification.\nThese include removing redundant attributes, removing redundant blocks through\noverlap analysis, and creating disjunctive units. These methods eliminate\nunnecessary parameters, dramatically reducing model size without harming\nclassification power. We increase robustness by introducing an interpretable\nfallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not\ncovered by any block, ensuring complete data coverage while preserving model\ntransparency. Our results demonstrate that interpretable models can scale to\nhigh-dimensional, large-volume datasets while maintaining competitive accuracy.\nOn benchmark datasets such as WBC (9-D), we achieve strong predictive\nperformance with significantly reduced complexity. On MNIST (784-D), our method\ncontinues to improve through tuning and simplification, showing promise as a\ntransparent alternative to black-box models in domains where trust, clarity,\nand control are crucial.", "AI": {"tldr": "The paper builds on Hyperblocks to improve interpretability, reduce training time and model complexity while maintaining accuracy. It introduces algorithms for simplifying Hyperblocks, including removing redundant attributes and blocks, and creating disjunctive units. A fallback mechanism using k-NN classifiers is also introduced to ensure complete data coverage. The results show that the method can handle high-dimensional datasets with competitive accuracy and reduced complexity.", "motivation": "To enhance the interpretability of models, decrease training time, and reduce model complexity without sacrificing accuracy.", "method": "Introduce a suite of algorithms for Hyperblock simplification which include removing redundant attributes, removing redundant blocks through overlap analysis, and creating disjunctive units. Also, introduce an interpretable fallback mechanism using k-Nearest Neighbor (k-NN) classifiers for points not covered by any block.", "result": "On benchmark datasets such as WBC (9-D), strong predictive performance was achieved with significantly reduced complexity. On MNIST (784-D), the method showed promise as a transparent alternative to black-box models through tuning and simplification.", "conclusion": "Interpretable models can scale to high-dimensional, large-volume datasets while maintaining competitive accuracy."}}
{"id": "2506.06990", "pdf": "https://arxiv.org/pdf/2506.06990", "abs": "https://arxiv.org/abs/2506.06990", "authors": ["Mingyi Li", "Michael R. Metel", "Akiko Takeda"], "title": "Modified K-means Algorithm with Local Optimality Guarantees", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "The K-means algorithm is one of the most widely studied clustering algorithms\nin machine learning. While extensive research has focused on its ability to\nachieve a globally optimal solution, there still lacks a rigorous analysis of\nits local optimality guarantees. In this paper, we first present conditions\nunder which the K-means algorithm converges to a locally optimal solution.\nBased on this, we propose simple modifications to the K-means algorithm which\nensure local optimality in both the continuous and discrete sense, with the\nsame computational complexity as the original K-means algorithm. As the\ndissimilarity measure, we consider a general Bregman divergence, which is an\nextension of the squared Euclidean distance often used in the K-means\nalgorithm. Numerical experiments confirm that the K-means algorithm does not\nalways find a locally optimal solution in practice, while our proposed methods\nprovide improved locally optimal solutions with reduced clustering loss. Our\ncode is available at https://github.com/lmingyi/LO-K-means.", "AI": {"tldr": "This paper examines the local optimality of the K-means algorithm and proposes modifications to ensure local optimality with the same computational complexity.", "motivation": "The motivation is to address the lack of rigorous analysis on the local optimality guarantees of the K-means algorithm, despite its widespread use in machine learning.", "method": "The authors present conditions under which the K-means algorithm converges to a locally optimal solution. They then propose simple modifications to the algorithm ensuring local optimality in both continuous and discrete senses, using Bregman divergence as the dissimilarity measure.", "result": "Numerical experiments show that the standard K-means algorithm does not always find a locally optimal solution, whereas the proposed methods provide improved locally optimal solutions with reduced clustering loss.", "conclusion": "The modified K-means algorithms offer better local optimality guarantees without increasing computational complexity."}}
{"id": "2506.06999", "pdf": "https://arxiv.org/pdf/2506.06999", "abs": "https://arxiv.org/abs/2506.06999", "authors": ["Arun Sharma", "Mingzhou Yang", "Majid Farhadloo", "Subhankar Ghosh", "Bharat Jayaprakash", "Shashi Shekhar"], "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "AI": {"tldr": "The paper proposes a physics-informed diffusion model for detecting anomalous trajectories indicative of GPS spoofing, integrating kinematic constraints to improve accuracy and reduce false-positives.", "motivation": "Detecting anomalous trajectories is crucial to prevent illegal activities in international waters like unauthorized fishing and illicit oil transfers. Current methods face challenges due to AI-generated fake trajectories and lack of labeled data, leading to higher false-positive rates.", "method": "The authors propose a physics-informed diffusion model that incorporates kinematic constraints to identify trajectories violating physical laws. This approach aims to address the limitations of existing generative models which do not consider fine-scale spatiotemporal dependencies and prior physical knowledge.", "result": "Experiments on real-world datasets from maritime and urban domains demonstrate improved prediction accuracy and reduced estimation error rate for anomaly detection and trajectory generation compared to existing methods.", "conclusion": "The proposed physics-informed diffusion model effectively detects anomalous trajectories by leveraging physical laws, leading to more accurate results with fewer false positives. The implementation is publicly available."}}
{"id": "2506.07003", "pdf": "https://arxiv.org/pdf/2506.07003", "abs": "https://arxiv.org/abs/2506.07003", "authors": ["Utkarsh Utkarsh", "Danielle C. Maddix", "Ruijun Ma", "Michael W. Mahoney", "Yuyang Wang"], "title": "End-to-End Probabilistic Framework for Learning with Hard Constraints", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": "46 pages, 5 figures, 10 tables", "summary": "We present a general purpose probabilistic forecasting framework,\nProbHardE2E, to learn systems that can incorporate operational/physical\nconstraints as hard requirements. ProbHardE2E enforces hard constraints by\nexploiting variance information in a novel way; and thus it is also capable of\nperforming uncertainty quantification (UQ) on the model. Our methodology uses a\nnovel differentiable probabilistic projection layer (DPPL) that can be combined\nwith a wide range of neural network architectures. This DPPL allows the model\nto learn the system in an end-to-end manner, compared to other approaches where\nthe constraints are satisfied either through a post-processing step or at\ninference. In addition, ProbHardE2E can optimize a strictly proper scoring\nrule, without making any distributional assumptions on the target, which\nenables it to obtain robust distributional estimates (in contrast to existing\napproaches that generally optimize likelihood-based objectives, which are\nheavily biased by their distributional assumptions and model choices); and it\ncan incorporate a range of non-linear constraints (increasing the power of\nmodeling and flexibility). We apply ProbHardE2E to problems in learning partial\ndifferential equations with uncertainty estimates and to probabilistic\ntime-series forecasting, showcasing it as a broadly applicable general setup\nthat connects these seemingly disparate domains.", "AI": {"tldr": "The paper introduces ProbHardE2E, a probabilistic forecasting framework that incorporates operational/physical constraints as hard requirements. It uses a differentiable probabilistic projection layer (DPPL) to enable end-to-end learning and uncertainty quantification without distributional assumptions.", "motivation": "To develop a general-purpose forecasting system capable of enforcing operational/physical constraints as hard requirements while performing uncertainty quantification on the model.", "method": "ProbHardE2E employs a novel differentiable probabilistic projection layer (DPPL) that can be integrated with various neural network architectures for end-to-end learning. This approach avoids post-processing or inference-based constraint satisfaction and allows optimization of strictly proper scoring rules without making distributional assumptions on the target.", "result": "ProbHardE2E successfully performs uncertainty quantification and enforces hard constraints in both learning partial differential equations with uncertainty estimates and probabilistic time-series forecasting, demonstrating its broad applicability across seemingly disparate domains.", "conclusion": "ProbHardE2E represents a versatile general setup that connects different modeling domains by enabling robust distributional estimates and incorporating non-linear constraints."}}
{"id": "2506.07014", "pdf": "https://arxiv.org/pdf/2506.07014", "abs": "https://arxiv.org/abs/2506.07014", "authors": ["Yutaro Nakagama", "Daisuke Ishii", "Kazuki Yoshizoe"], "title": "Comparison of Lightweight Methods for Vehicle Dynamics-Based Driver Drowsiness Detection", "categories": ["cs.LG"], "comment": "8 pages, 3 figures, to be published at IV 2025", "summary": "Driver drowsiness detection (DDD) prevents road accidents caused by driver\nfatigue. Vehicle dynamics-based DDD has been proposed as a method that is both\neconomical and high performance. However, there are concerns about the\nreliability of performance metrics and the reproducibility of many of the\nexisting methods. For instance, some previous studies seem to have a data\nleakage issue among training and test datasets, and many do not openly provide\nthe datasets they used. To this end, this paper aims to compare the performance\nof representative vehicle dynamics-based DDD methods under a transparent and\nfair framework that uses a public dataset. We first develop a framework for\nextracting features from an open dataset by Aygun et al. and performing DDD\nwith lightweight ML models; the framework is carefully designed to support a\nvariety of onfigurations. Second, we implement three existing representative\nmethods and a concise random forest (RF)-based method in the framework.\nFinally, we report the results of experiments to verify the reproducibility and\nclarify the performance of DDD based on common metrics. Among the evaluated\nmethods, the RF-based method achieved the highest accuracy of 88 %. Our\nfindings imply the issues inherent in DDD methods developed in a non-standard\nmanner, and demonstrate a high performance method implemented appropriately.", "AI": {"tldr": "This paper addresses the issue of reliability and reproducibility in vehicle dynamics-based driver drowsiness detection (DDD) methods by comparing representative methods under a fair framework using a public dataset. The random forest (RF)-based method achieved the highest accuracy.", "motivation": "There are concerns about the reliability of performance metrics and the reproducibility of many existing vehicle dynamics-based DDD methods due to issues like data leakage and lack of openly provided datasets.", "method": "The authors developed a framework for extracting features from an open dataset and performing DDD with lightweight ML models. They implemented three existing representative methods and a concise RF-based method within this framework.", "result": "Experiments verified the reproducibility and clarified the performance of DDD based on common metrics. The RF-based method achieved the highest accuracy of 88%.", "conclusion": "The study highlights issues in non-standard DDD methods and demonstrates a high-performance method when implemented appropriately."}}
{"id": "2506.07033", "pdf": "https://arxiv.org/pdf/2506.07033", "abs": "https://arxiv.org/abs/2506.07033", "authors": ["Yung-Chien Wang", "Kuang-Da Wang", "Wei-Yao Wang", "Wen-Chih Peng"], "title": "Mixture Experts with Test-Time Self-Supervised Aggregation for Tabular Imbalanced Regression", "categories": ["cs.LG"], "comment": "Preprint", "summary": "Tabular data serve as a fundamental and ubiquitous representation of\nstructured information in numerous real-world applications, e.g., finance and\nurban planning. In the realm of tabular imbalanced applications, data imbalance\nhas been investigated in classification tasks with insufficient instances in\ncertain labels, causing the model's ineffective generalizability. However, the\nimbalance issue of tabular regression tasks is underexplored, and yet is\ncritical due to unclear boundaries for continuous labels and simplifying\nassumptions in existing imbalance regression work, which often rely on known\nand balanced test distributions. Such assumptions may not hold in practice and\ncan lead to performance degradation. To address these issues, we propose MATI:\nMixture Experts with Test-Time Self-Supervised Aggregation for Tabular\nImbalance Regression, featuring two key innovations: (i) the Region-Aware\nMixture Expert, which adopts a Gaussian Mixture Model to capture the underlying\nrelated regions. The statistical information of each Gaussian component is then\nused to synthesize and train region-specific experts to capture the unique\ncharacteristics of their respective regions. (ii) Test-Time Self-Supervised\nExpert Aggregation, which dynamically adjusts region expert weights based on\ntest data features to reinforce expert adaptation across varying test\ndistributions. We evaluated MATI on four real-world tabular imbalance\nregression datasets, including house pricing, bike sharing, and age prediction.\nTo reflect realistic deployment scenarios, we adopted three types of test\ndistributions: a balanced distribution with uniform target frequencies, a\nnormal distribution that follows the training data, and an inverse distribution\nthat emphasizes rare target regions. On average across these three test\ndistributions, MATI achieved a 7.1% improvement in MAE compared to existing\nmethods.", "AI": {"tldr": "The paper proposes MATI, a method designed for tabular imbalance regression tasks, which includes Region-Aware Mixture Expert and Test-Time Self-Supervised Expert Aggregation. It shows a 7.1% MAE improvement over existing methods.", "motivation": "Existing work on data imbalance mainly focuses on classification tasks, while the imbalance issue in tabular regression tasks remains underexplored, especially when assumptions about known and balanced test distributions may not hold in practice.", "method": "MATI is proposed with two key components: (i) Region-Aware Mixture Expert that uses Gaussian Mixture Model to capture related regions and synthesize region-specific experts; (ii) Test-Time Self-Supervised Expert Aggregation that dynamically adjusts expert weights based on test data features.", "result": "MATI was evaluated on four real-world datasets with three types of test distributions and achieved a 7.1% improvement in MAE compared to existing methods.", "conclusion": "MATI effectively addresses the imbalance issue in tabular regression tasks by capturing region-specific characteristics and adapting to varying test distributions."}}
{"id": "2506.07040", "pdf": "https://arxiv.org/pdf/2506.07040", "abs": "https://arxiv.org/abs/2506.07040", "authors": ["Yang Xu", "Swetha Ganesh", "Vaneet Aggarwal"], "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "arXiv admin note: text overlap with arXiv:2502.16816", "summary": "We present the first $Q$-learning and actor-critic algorithms for robust\naverage reward Markov Decision Processes (MDPs) with non-asymptotic convergence\nunder contamination, TV distance and Wasserstein distance uncertainty sets. We\nshow that the robust $Q$ Bellman operator is a strict contractive mapping with\nrespect to a carefully constructed semi-norm with constant functions being\nquotiented out. This property supports a stochastic approximation update, that\nlearns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples.\nWe also show that the same idea can be used for robust $Q$ function estimation,\nwhich can be further used for critic estimation. Coupling it with theories in\nrobust policy mirror descent update, we present a natural actor-critic\nalgorithm that attains an $\\epsilon$-optimal robust policy in\n$\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of\ndistributionally robust reinforcement learning in the average reward setting.", "AI": {"tldr": "This paper presents the first Q-learning and actor-critic algorithms for robust average reward MDPs with non-asymptotic convergence under contamination, TV distance and Wasserstein distance uncertainty sets. The authors show that the robust Q Bellman operator is a strict contractive mapping with respect to a carefully constructed semi-norm, leading to stochastic approximation updates that learn the optimal robust Q function in O\u02dc(\u03f5^-2) samples. They also present an actor-critic algorithm that attains an \u03f5-optimal robust policy in O\u02dc(\u03f5^-3) samples.", "motivation": "The motivation of this paper is to advance the theory of distributionally robust reinforcement learning in the average reward setting by developing Q-learning and actor-critic algorithms that can handle uncertainty sets such as contamination, TV distance and Wasserstein distance.", "method": "The method involves showing that the robust Q Bellman operator is a strict contractive mapping with respect to a carefully constructed semi-norm. This leads to stochastic approximation updates that learn the optimal robust Q function. The same idea is used for robust Q function estimation, which can be further used for critic estimation. Coupling it with theories in robust policy mirror descent update, they present a natural actor-critic algorithm.", "result": "The result is that the proposed algorithms attain an \u03f5-optimal robust policy in O\u02dc(\u03f5^-3) samples. These results advance the theory of distributionally robust reinforcement learning in the average reward setting.", "conclusion": "In conclusion, the authors have presented the first Q-learning and actor-critic algorithms for robust average reward MDPs with non-asymptotic convergence under contamination, TV distance and Wasserstein distance uncertainty sets."}}
{"id": "2506.07049", "pdf": "https://arxiv.org/pdf/2506.07049", "abs": "https://arxiv.org/abs/2506.07049", "authors": ["Jake Robertson", "Noah Hollmann", "Samuel M\u00fcller", "Noor Awad", "Frank Hutter"], "title": "FairPFN: A Tabular Foundation Model for Causal Fairness", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Machine learning (ML) systems are utilized in critical sectors, such as\nhealthcare, law enforcement, and finance. However, these systems are often\ntrained on historical data that contains demographic biases, leading to ML\ndecisions that perpetuate or exacerbate existing social inequalities. Causal\nfairness provides a transparent, human-in-the-loop framework to mitigate\nalgorithmic discrimination, aligning closely with legal doctrines of direct and\nindirect discrimination. However, current causal fairness frameworks hold a key\nlimitation in that they assume prior knowledge of the correct causal model,\nrestricting their applicability in complex fairness scenarios where causal\nmodels are unknown or difficult to identify. To bridge this gap, we propose\nFairPFN, a tabular foundation model pre-trained on synthetic causal fairness\ndata to identify and mitigate the causal effects of protected attributes in its\npredictions. FairPFN's key contribution is that it requires no knowledge of the\ncausal model and still demonstrates strong performance in identifying and\nremoving protected causal effects across a diverse set of hand-crafted and\nreal-world scenarios relative to robust baseline methods. FairPFN paves the way\nfor promising future research, making causal fairness more accessible to a\nwider variety of complex fairness problems.", "AI": {"tldr": "Machine learning systems in critical sectors can perpetuate social inequalities due to demographic biases in historical data. Current causal fairness frameworks assume prior knowledge of the correct causal model, restricting their applicability. This paper proposes FairPFN, a model pre-trained on synthetic causal fairness data that identifies and mitigates causal effects of protected attributes without requiring causal model knowledge, showing strong performance across various scenarios.", "motivation": "To address the limitation of current causal fairness frameworks which assume prior knowledge of the correct causal model, restricting their applicability in complex fairness scenarios where causal models are unknown or difficult to identify.", "method": "Propose FairPFN, a tabular foundation model pre-trained on synthetic causal fairness data to identify and mitigate the causal effects of protected attributes in its predictions without requiring knowledge of the causal model.", "result": "FairPFN demonstrates strong performance in identifying and removing protected causal effects across a diverse set of hand-crafted and real-world scenarios relative to robust baseline methods.", "conclusion": "FairPFN bridges the gap in causal fairness by not requiring knowledge of the causal model, paving the way for future research and making causal fairness more accessible to a wider variety of complex fairness problems."}}
{"id": "2506.06299", "pdf": "https://arxiv.org/pdf/2506.06299", "abs": "https://arxiv.org/abs/2506.06299", "authors": ["Daniel Thilo Schroeder", "Meeyoung Cha", "Andrea Baronchelli", "Nick Bostrom", "Nicholas A. Christakis", "David Garcia", "Amit Goldenberg", "Yara Kyrychenko", "Kevin Leyton-Brown", "Nina Lutz", "Gary Marcus", "Filippo Menczer", "Gordon Pennycook", "David G. Rand", "Frank Schweitzer", "Christopher Summerfield", "Audrey Tang", "Jay Van Bavel", "Sander van der Linden", "Dawn Song", "Jonas R. Kunst"], "title": "How Malicious AI Swarms Can Threaten Democracy", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages, 1 figure", "summary": "Advances in AI portend a new era of sophisticated disinformation operations.\nWhile individual AI systems already create convincing -- and at times\nmisleading -- information, an imminent development is the emergence of\nmalicious AI swarms. These systems can coordinate covertly, infiltrate\ncommunities, evade traditional detectors, and run continuous A/B tests, with\nround-the-clock persistence. The result can include fabricated grassroots\nconsensus, fragmented shared reality, mass harassment, voter micro-suppression\nor mobilization, contamination of AI training data, and erosion of\ninstitutional trust. With democratic processes worldwide increasingly\nvulnerable, we urge a three-pronged response: (1) platform-side defenses --\nalways-on swarm-detection dashboards, pre-election high-fidelity\nswarm-simulation stress-tests, transparency audits, and optional client-side\n\"AI shields\" for users; (2) model-side safeguards -- standardized\npersuasion-risk tests, provenance-authenticating passkeys, and watermarking;\nand (3) system-level oversight -- a UN-backed AI Influence Observatory.", "AI": {"tldr": "AI\u7684\u8fdb\u6b65\u9884\u793a\u7740 sophisticated disinformation operations \u7684\u65b0\u65f6\u4ee3\u3002\u6211\u4eec\u5e94\u4ece\u5e73\u53f0\u3001\u6a21\u578b\u548c\u7cfb\u7edf\u4e09\u4e2a\u5c42\u9762\u8fdb\u884c\u9632\u5fa1\u54cd\u5e94\u3002", "motivation": "\u5c3d\u7ba1\u4e2a\u4f53AI\u7cfb\u7edf\u5df2\u7ecf\u80fd\u591f\u521b\u5efa\u4ee4\u4eba\u4fe1\u670d\u4e14\u6709\u65f6\u5177\u6709\u8bef\u5bfc\u6027\u7684\u4fe1\u606f\uff0c\u4f46\u6076\u610fAI\u7fa4\u7684\u51fa\u73b0\u5c06\u5e26\u6765\u66f4\u5927\u7684\u5a01\u80c1\u3002\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u79d8\u5bc6\u534f\u8c03\u3001\u6e17\u900f\u793e\u533a\u3001\u9003\u907f\u4f20\u7edf\u68c0\u6d4b\u5668\u5e76\u6301\u7eed\u8fdb\u884cA/B\u6d4b\u8bd5\u3002\u8fd9\u53ef\u80fd\u5bfc\u81f4\u865a\u5047\u5171\u8bc6\u3001\u73b0\u5b9e\u5206\u88c2\u3001\u5927\u89c4\u6a21\u9a9a\u6270\u3001\u9009\u6c11\u6291\u5236\u6216\u52a8\u5458\u3001AI\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u4ee5\u53ca\u673a\u6784\u4fe1\u4efb\u4fb5\u8680\u3002", "method": "\u4e09\u7ba1\u9f50\u4e0b\u7684\u54cd\u5e94\u7b56\u7565\uff1a(1) \u5e73\u53f0\u7aef\u9632\u5fa1 - \u5e38\u6001\u5316\u7684\u7fa4\u68c0\u6d4b\u4eea\u8868\u677f\u3001\u9009\u4e3e\u524d\u9ad8\u4fdd\u771f\u7fa4\u6a21\u62df\u538b\u529b\u6d4b\u8bd5\u3001\u900f\u660e\u5ea6\u5ba1\u8ba1\u548c\u53ef\u9009\u7684\u5ba2\u6237\u7aef\u201cAI\u62a4\u76fe\u201d\uff1b(2) \u6a21\u578b\u7aef\u4fdd\u969c - \u6807\u51c6\u5316\u8bf4\u670d\u98ce\u9669\u6d4b\u8bd5\u3001\u6eaf\u6e90\u8ba4\u8bc1\u901a\u884c\u5bc6\u94a5\u548c\u6c34\u5370\uff1b(3) \u7cfb\u7edf\u7ea7\u76d1\u7763 - \u8054\u5408\u56fd\u652f\u6301\u7684AI\u5f71\u54cd\u89c2\u6d4b\u7ad9\u3002", "result": "\u901a\u8fc7\u91c7\u53d6\u4e0a\u8ff0\u4e09\u65b9\u9762\u7684\u63aa\u65bd\uff0c\u53ef\u4ee5\u66f4\u597d\u5730\u5e94\u5bf9\u6076\u610fAI\u7fa4\u5e26\u6765\u7684\u6311\u6218\uff0c\u4fdd\u62a4\u6c11\u4e3b\u8fdb\u7a0b\u514d\u53d7\u4fb5\u5bb3\u3002", "conclusion": "\u968f\u7740\u5168\u7403\u6c11\u4e3b\u8fdb\u7a0b\u65e5\u76ca\u8106\u5f31\uff0c\u6211\u4eec\u9700\u8981\u91c7\u53d6\u7d27\u6025\u884c\u52a8\uff0c\u4ee5\u9632\u6b62\u6076\u610fAI\u7fa4\u5bf9\u793e\u4f1a\u9020\u6210\u4e25\u91cd\u7834\u574f\u3002"}}
{"id": "2506.07054", "pdf": "https://arxiv.org/pdf/2506.07054", "abs": "https://arxiv.org/abs/2506.07054", "authors": ["Uri Koren", "Navdeep Kumar", "Uri Gadot", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "title": "Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Classical policy gradient (PG) methods in reinforcement learning frequently\nconverge to suboptimal local optima, a challenge exacerbated in large or\ncomplex environments. This work investigates Policy Gradient with Tree Search\n(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance\npolicy optimization. We provide theoretical analysis demonstrating that\nincreasing the tree search depth $m$-monotonically reduces the set of\nundesirable stationary points and, consequently, improves the worst-case\nperformance of any resulting stationary policy. Critically, our analysis\naccommodates practical scenarios where policy updates are restricted to states\nvisited by the current policy, rather than requiring updates across the entire\nstate space. Empirical evaluations on diverse MDP structures, including Ladder,\nTightrope, and Gridworld environments, illustrate PGTS's ability to exhibit\n\"farsightedness,\" navigate challenging reward landscapes, escape local traps\nwhere standard PG fails, and achieve superior solutions.", "AI": {"tldr": "The paper introduces Policy Gradient with Tree Search (PGTS), a reinforcement learning method that uses an m-step lookahead to improve policy optimization. It shows theoretically and empirically that PGTS can escape local traps, enhance performance, and achieve better solutions in complex environments.", "motivation": "Classical policy gradient methods often get stuck in suboptimal local optima, especially in large or complex environments.", "method": "Policy Gradient with Tree Search (PGTS) integrates an m-step lookahead mechanism into the policy optimization process. Theoretical analysis is provided to show that increasing tree search depth reduces undesirable stationary points and improves worst-case performance without requiring updates across the entire state space.", "result": "Empirical evaluations on various MDP structures (Ladder, Tightrope, Gridworld) demonstrate PGTS's ability to exhibit 'farsightedness', navigate challenging reward landscapes, escape local traps where standard PG fails, and achieve superior solutions.", "conclusion": "PGTS provides a way to overcome the limitations of classical policy gradient methods by incorporating a lookahead mechanism, leading to improved performance in complex environments."}}
{"id": "2506.07078", "pdf": "https://arxiv.org/pdf/2506.07078", "abs": "https://arxiv.org/abs/2506.07078", "authors": ["Jiaheng Dong", "Hong Jia", "Soumyajit Chatterjee", "Abhirup Ghosh", "James Bailey", "Ting Dang"], "title": "E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": "Under Review", "summary": "Speech Foundation Models encounter significant performance degradation when\ndeployed in real-world scenarios involving acoustic domain shifts, such as\nbackground noise and speaker accents. Test-time adaptation (TTA) has recently\nemerged as a viable strategy to address such domain shifts at inference time\nwithout requiring access to source data or labels. However, existing TTA\napproaches, particularly those relying on backpropagation, are\nmemory-intensive, limiting their applicability in speech tasks and\nresource-constrained settings. Although backpropagation-free methods offer\nimproved efficiency, existing ones exhibit poor accuracy. This is because they\nare predominantly developed for vision tasks, which fundamentally differ from\nspeech task formulations, noise characteristics, and model architecture, posing\nunique transferability challenges. In this paper, we introduce E-BATS, the\nfirst Efficient BAckpropagation-free TTA framework designed explicitly for\nspeech foundation models. E-BATS achieves a balance between adaptation\neffectiveness and memory efficiency through three key components: (i)\nlightweight prompt adaptation for a forward-pass-based feature alignment, (ii)\na multi-scale loss to capture both global (utterance-level) and local\ndistribution shifts (token-level) and (iii) a test-time exponential moving\naverage mechanism for stable adaptation across utterances. Experiments\nconducted on four noisy speech datasets spanning sixteen acoustic conditions\ndemonstrate consistent improvements, with 4.1%-13.5% accuracy gains over\nbackpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to\nbackpropagation-based methods. By enabling scalable and robust adaptation under\nacoustic variability, this work paves the way for developing more efficient\nadaptation approaches for practical speech processing systems in real-world\nenvironments.", "AI": {"tldr": "E-BATS is an efficient backpropagation-free TTA framework for speech foundation models, offering improved accuracy and memory efficiency.", "motivation": "Existing test-time adaptation (TTA) methods either consume too much memory or lack accuracy when applied to speech tasks due to differences in task formulation, noise characteristics, and model architecture compared to vision tasks.", "method": "E-BATS incorporates three key components: lightweight prompt adaptation for feature alignment, a multi-scale loss to address global and local distribution shifts, and a test-time exponential moving average mechanism for stable adaptation across utterances.", "result": "Experiments on four noisy speech datasets showed consistent improvements with 4.1%-13.5% accuracy gains over backpropagation-free baselines and 2.0-6.4 times GPU memory savings compared to backpropagation-based methods.", "conclusion": "E-BATS enables scalable and robust adaptation under acoustic variability, paving the way for more efficient adaptation approaches for practical speech processing systems."}}
{"id": "2506.06313", "pdf": "https://arxiv.org/pdf/2506.06313", "abs": "https://arxiv.org/abs/2506.06313", "authors": ["Huiyao Chen", "Yi Yang", "Yinghui Li", "Meishan Zhang", "Min Zhang"], "title": "DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "21 pages, 7 figures", "summary": "Long document understanding has become increasingly crucial in natural\nlanguage processing, with retrieval-based methods emerging as a promising\nsolution to address the context length limitations of large language models\n(LLMs). However, existing approaches either treat documents as flat sequences\nor employ arbitrary chunking strategies, failing to capture the inherent\ndiscourse structure that guides human comprehension. We present DISRetrieval, a\nnovel hierarchical retrieval framework that leverages linguistic discourse\nstructure to enhance long document understanding. Our approach introduces three\nkey innovations: (1) a discourse-aware document organization framework that\nutilizes rhetorical structure theory (RST) to create sentence-level\nhierarchical representations, preserving both semantic relationships and\nnatural document flow; (2) an LLM-enhanced node representation technique that\ncombines discourse structure with adaptive summarization to enrich tree nodes\nwith contextual information; and (3) a hierarchical evidence retrieval\nmechanism that effectively selects relevant content while maintaining discourse\ncoherence. Through comprehensive experiments on QASPER and QuALITY datasets,\nDISRetrieval demonstrates substantial improvements over existing methods in\nboth token-level retrieval metrics and downstream question answering tasks. Our\nablation studies confirm that incorporating discourse structure significantly\nenhances retrieval effectiveness across different document lengths and query\ntypes, validating the importance of linguistically-informed document\nrepresentation in long-text understanding. Our code and datasets are publicly\navailable at github/DreamH1gh/DISRetrieval to facilitate future research.", "AI": {"tldr": "The paper introduces DISRetrieval, a novel hierarchical retrieval framework leveraging linguistic discourse structure for long document understanding. It presents three key innovations and demonstrates substantial improvements over existing methods in token-level retrieval metrics and downstream question answering tasks.", "motivation": "Long document understanding is crucial in natural language processing, but existing approaches fail to capture the inherent discourse structure that guides human comprehension.", "method": "DISRetrieval utilizes rhetorical structure theory (RST) to create sentence-level hierarchical representations, combines discourse structure with adaptive summarization to enrich tree nodes with contextual information, and employs a hierarchical evidence retrieval mechanism that maintains discourse coherence.", "result": "DISRetrieval shows significant improvements over existing methods in both token-level retrieval metrics and downstream question answering tasks on QASPER and QuALITY datasets. Ablation studies confirm the effectiveness of incorporating discourse structure.", "conclusion": "The importance of linguistically-informed document representation in long-text understanding is validated. The code and datasets are publicly available to facilitate future research."}}
{"id": "2506.07085", "pdf": "https://arxiv.org/pdf/2506.07085", "abs": "https://arxiv.org/abs/2506.07085", "authors": ["Uri Koren", "Yonatan Ashlag", "Mirco Mutti", "Esther Derman", "Pierre-Luc Bacon", "Shie Mannor"], "title": "State Entropy Regularization for Robust Reinforcement Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "State entropy regularization has empirically shown better exploration and\nsample complexity in reinforcement learning (RL). However, its theoretical\nguarantees have not been studied. In this paper, we show that state entropy\nregularization improves robustness to structured and spatially correlated\nperturbations. These types of variation are common in transfer learning but\noften overlooked by standard robust RL methods, which typically focus on small,\nuncorrelated changes. We provide a comprehensive characterization of these\nrobustness properties, including formal guarantees under reward and transition\nuncertainty, as well as settings where the method performs poorly. Much of our\nanalysis contrasts state entropy with the widely used policy entropy\nregularization, highlighting their different benefits. Finally, from a\npractical standpoint, we illustrate that compared with policy entropy, the\nrobustness advantages of state entropy are more sensitive to the number of\nrollouts used for policy evaluation.", "AI": {"tldr": "State entropy regularization in reinforcement learning improves robustness to structured and spatially correlated perturbations, unlike standard robust RL methods.", "motivation": "To explore the theoretical guarantees of state entropy regularization in reinforcement learning which has shown better exploration and sample complexity empirically.", "method": "Provide a comprehensive characterization of robustness properties of state entropy regularization, including formal guarantees under reward and transition uncertainty, and contrast it with policy entropy regularization.", "result": "State entropy regularization improves robustness to structured and spatially correlated perturbations. The method's performance is more sensitive to the number of rollouts used for policy evaluation compared to policy entropy.", "conclusion": "State entropy regularization offers advantages in terms of robustness, particularly for structured and spatially correlated perturbations, but its effectiveness can be sensitive to the number of rollouts for policy evaluation."}}
{"id": "2506.06316", "pdf": "https://arxiv.org/pdf/2506.06316", "abs": "https://arxiv.org/abs/2506.06316", "authors": ["Haoyang Feng", "Yanjun Dai", "Yuan Gao"], "title": "A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "For personalized marketing, a new challenge of how to effectively algorithm\nthe A/B testing to maximize user response is urgently to be overcome. In this\npaper, we present a new approach, the RL-LLM-AB test framework, for using\nreinforcement learning strategy optimization combined with LLM to automate and\npersonalize A/B tests. The RL-LLM-AB test is built upon the pre-trained\ninstruction-tuned language model. It first generates A/B versions of candidate\ncontent variants using a Prompt-Conditioned Generator, and then dynamically\nembeds and fuses the user portrait and the context of the current query with\nthe multi-modal perception module to constitute the current interaction state.\nThe content version is then selected in real-time through the policy\noptimization module with an Actor-Critic structure, and long-term revenue is\nestimated according to real-time feedback (such as click-through rate and\nconversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded\ninto the framework to capture long-term user preference drift, which helps to\ngeneralize policy across multiple users and content contexts. Numerical results\ndemonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B\ntesting methods, including classical A/B testing, Contextual Bandits, and\nbenchmark reinforcement learning approaches on real-world marketing data.", "AI": {"tldr": "The paper proposes RL-LLM-ABTest framework that uses reinforcement learning and LLM for personalized A/B testing in marketing, showing superiority over existing methods.", "motivation": "To effectively optimize A/B testing in personalized marketing to maximize user response.", "method": "RL-LLM-AB test framework which combines reinforcement learning strategy optimization with LLM. It involves generating A/B content versions, embedding user portrait and context, selecting content in real-time through policy optimization, and estimating long-term revenue based on feedback. Also includes a Memory-Augmented Reward Estimator.", "result": "Demonstrates superiority of RL-LLM-ABTest over classical A/B testing, Contextual Bandits, and benchmark reinforcement learning approaches using real-world marketing data.", "conclusion": "RL-LLM-ABTest framework is an effective solution for personalized A/B testing in marketing."}}
{"id": "2506.07088", "pdf": "https://arxiv.org/pdf/2506.07088", "abs": "https://arxiv.org/abs/2506.07088", "authors": ["Ilja Kuzborskij", "Yasin Abbasi Yadkori"], "title": "Pointwise confidence estimation in the non-linear $\\ell^2$-regularized least squares", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider a high-probability non-asymptotic confidence estimation in the\n$\\ell^2$-regularized non-linear least-squares setting with fixed design. In\nparticular, we study confidence estimation for local minimizers of the\nregularized training loss. We show a pointwise confidence bound, meaning that\nit holds for the prediction on any given fixed test input $x$. Importantly, the\nproposed confidence bound scales with similarity of the test input to the\ntraining data in the implicit feature space of the predictor (for instance,\nbecoming very large when the test input lies far outside of the training data).\nThis desirable last feature is captured by the weighted norm involving the\ninverse-Hessian matrix of the objective function, which is a generalized\nversion of its counterpart in the linear setting, $x^{\\top} \\text{Cov}^{-1} x$.\nOur generalized result can be regarded as a non-asymptotic counterpart of the\nclassical confidence interval based on asymptotic normality of the MLE\nestimator. We propose an efficient method for computing the weighted norm,\nwhich only mildly exceeds the cost of a gradient computation of the loss\nfunction. Finally, we complement our analysis with empirical evidence showing\nthat the proposed confidence bound provides better coverage/width trade-off\ncompared to a confidence estimation by bootstrapping, which is a gold-standard\nmethod in many applications involving non-linear predictors such as neural\nnetworks.", "AI": {"tldr": "The paper presents a method for high-probability non-asymptotic confidence estimation in the context of \u21132-regularized non-linear least-squares problems. It introduces a pointwise confidence bound that scales with the similarity of test inputs to training data, generalizing classical linear results. An efficient computation method is proposed, and empirical evidence shows improved performance over bootstrapping.", "motivation": "There is a need for reliable confidence estimation methods in non-linear least-squares settings, particularly for local minimizers of regularized training loss. Classical methods based on asymptotic normality may not be suitable for finite-sample regimes or complex models like neural networks.", "method": "The authors derive a pointwise confidence bound for predictions in the \u21132-regularized non-linear least-squares setting. This bound involves a weighted norm using the inverse-Hessian matrix of the objective function, which captures the similarity of test inputs to training data in the feature space.", "result": "The proposed confidence bound scales appropriately with the similarity of test inputs to training data. Empirical results demonstrate that the method provides a better coverage/width trade-off compared to bootstrapping, a commonly used alternative.", "conclusion": "This work provides a non-asymptotic confidence estimation approach for non-linear least-squares problems, offering an improvement over existing techniques like bootstrapping. The method is computationally efficient and has strong theoretical underpinnings."}}
{"id": "2506.06318", "pdf": "https://arxiv.org/pdf/2506.06318", "abs": "https://arxiv.org/abs/2506.06318", "authors": ["Feiyang Pan", "Shenghe Zheng", "Chunyan Yin", "Guangbin Dou"], "title": "MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "MEMS gyroscopes play a critical role in inertial navigation and motion\ncontrol applications but typically suffer from a fundamental trade-off between\nmeasurement range and noise performance. Existing hardware-based solutions\naimed at mitigating this issue introduce additional complexity, cost, and\nscalability challenges. Deep-learning methods primarily focus on noise\nreduction and typically require precisely aligned ground-truth signals, making\nthem difficult to deploy in practical scenarios and leaving the fundamental\ntrade-off unresolved. To address these challenges, we introduce Mixture of\nExperts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework\nspecifically designed for simultaneous over-range signal reconstruction and\nnoise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction\nExpert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing\nsaturated segments; and a Denoise Expert (DE), utilizing dual-branch\ncomplementary masking combined with FFT-guided augmentation for robust noise\nreduction. A lightweight gating module dynamically routes input segments to the\nappropriate expert. Furthermore, existing evaluation lack a comprehensive\nstandard for assessing multi-dimensional signal enhancement. To bridge this\ngap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source\nbenchmarking platform comprising the GyroPeak-100 dataset and a unified\nevaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our\nproposed ISEBench, demonstrating that our framework significantly extends the\nmeasurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by\n98.4%, and achieves state-of-the-art performance, effectively addressing the\nlong-standing trade-off in inertial sensing.", "AI": {"tldr": "MEMS\u9640\u87ba\u4eea\u5728\u60ef\u6027\u5bfc\u822a\u548c\u8fd0\u52a8\u63a7\u5236\u5e94\u7528\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u4f46\u901a\u5e38\u5728\u6d4b\u91cf\u8303\u56f4\u548c\u566a\u58f0\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u57fa\u672c\u6743\u8861\u3002\u73b0\u6709\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u4f1a\u589e\u52a0\u590d\u6742\u6027\u3001\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u964d\u566a\u4e0a\uff0c\u5e76\u4e14\u9700\u8981\u7cbe\u786e\u5bf9\u9f50\u7684\u771f\u5b9e\u4fe1\u53f7\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u90e8\u7f72\uff0c\u5e76\u4e14\u6ca1\u6709\u89e3\u51b3\u6839\u672c\u7684\u6743\u8861\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u6211\u4eec\u5f15\u5165\u4e86MoE-Gyro\uff0c\u8fd9\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u6211\u76d1\u7763\u6846\u67b6\uff0c\u65e8\u5728\u540c\u65f6\u8fdb\u884c\u8d85\u8303\u56f4\u4fe1\u53f7\u91cd\u5efa\u548c\u964d\u566a\u3002MoE-Gyro\u4f7f\u7528\u4e24\u4e2a\u4e13\u5bb6\uff1a\u4e00\u4e2a\u7528\u4e8e\u91cd\u5efa\u9971\u548c\u6bb5\u7684\u8fc7\u8303\u56f4\u91cd\u5efa\u4e13\u5bb6\uff08ORE\uff09\uff0c\u5176\u5177\u6709\u9ad8\u65af\u8870\u51cf\u6ce8\u610f\u529b\u673a\u5236\uff1b\u53e6\u4e00\u4e2a\u662f\u53bb\u566a\u4e13\u5bb6\uff08DE\uff09\uff0c\u5229\u7528\u53cc\u5206\u652f\u4e92\u8865\u63a9\u7801\u548cFFT\u5f15\u5bfc\u589e\u5f3a\u6765\u8fdb\u884c\u5f3a\u5927\u7684\u964d\u566a\u3002\u8f7b\u91cf\u7ea7\u95e8\u63a7\u6a21\u5757\u52a8\u6001\u5730\u5c06\u8f93\u5165\u6bb5\u8def\u7531\u5230\u9002\u5f53\u7684\u4e13\u5bb6\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u8bc4\u4f30\u7f3a\u4e4f\u5168\u9762\u7684\u6807\u51c6\u6765\u8bc4\u4f30\u591a\u7ef4\u4fe1\u53f7\u589e\u5f3a\u3002\u4e3a\u4e86\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86IMU\u4fe1\u53f7\u589e\u5f3a\u57fa\u51c6\uff08ISEBench\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542bGyroPeak-100\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30IMU\u4fe1\u53f7\u589e\u5f3a\u65b9\u6cd5\u3002\u6211\u4eec\u4f7f\u7528\u63d0\u51fa\u7684ISEBench\u8bc4\u4f30MoE-Gyro\uff0c\u8bc1\u660e\u6211\u4eec\u7684\u6846\u67b6\u663e\u8457\u5730\u5c06\u53ef\u6d4b\u91cf\u8303\u56f4\u4ece450 deg/s\u6269\u5c55\u52301500 deg/s\uff0c\u51cf\u5c1198.4%\u7684\u504f\u5dee\u4e0d\u7a33\u5b9a\u6027\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u60ef\u6027\u4f20\u611f\u4e2d\u7684\u957f\u671f\u6743\u8861\u95ee\u9898\u3002", "motivation": "MEMS\u9640\u87ba\u4eea\u5728\u8bb8\u591a\u91cd\u8981\u5e94\u7528\u4e2d\u626e\u6f14\u5173\u952e\u89d2\u8272\uff0c\u4f46\u9762\u4e34\u6d4b\u91cf\u8303\u56f4\u4e0e\u566a\u58f0\u6027\u80fd\u4e4b\u95f4\u7684\u57fa\u672c\u6743\u8861\u95ee\u9898\u3002\u73b0\u6709\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\u589e\u52a0\u4e86\u590d\u6742\u6027\u548c\u6210\u672c\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u6709\u52a9\u4e8e\u964d\u566a\uff0c\u4f46\u9700\u8981\u7cbe\u786e\u5bf9\u9f50\u7684\u771f\u5b9e\u4fe1\u53f7\uff0c\u96be\u4ee5\u5b9e\u9645\u90e8\u7f72\uff0c\u4e14\u672a\u89e3\u51b3\u6839\u672c\u6743\u8861\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u540c\u65f6\u5b9e\u73b0\u8d85\u8303\u56f4\u4fe1\u53f7\u91cd\u5efa\u548c\u964d\u566a\u3002", "method": "\u63d0\u51fa\u4e86MoE-Gyro\u6846\u67b6\uff0c\u5305\u62ec\u4e24\u4e2a\u4e13\u5bb6\u6a21\u5757\uff1a\u8fc7\u8303\u56f4\u91cd\u5efa\u4e13\u5bb6\uff08ORE\uff09\u548c\u53bb\u566a\u4e13\u5bb6\uff08DE\uff09\u3002ORE\u91c7\u7528\u9ad8\u65af\u8870\u51cf\u6ce8\u610f\u529b\u673a\u5236\u91cd\u5efa\u9971\u548c\u4fe1\u53f7\u6bb5\uff0c\u800cDE\u901a\u8fc7\u53cc\u5206\u652f\u4e92\u8865\u63a9\u7801\u548cFFT\u5f15\u5bfc\u589e\u5f3a\u8fdb\u884c\u964d\u566a\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u95e8\u63a7\u6a21\u5757\u52a8\u6001\u9009\u62e9\u5408\u9002\u7684\u4e13\u5bb6\u5904\u7406\u8f93\u5165\u4fe1\u53f7\u6bb5\u3002\u4e3a\u4e86\u8bc4\u4f30\u591a\u7ef4\u4fe1\u53f7\u589e\u5f3a\u6548\u679c\uff0c\u5f00\u53d1\u4e86ISEBench\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542bGyroPeak-100\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMoE-Gyro\u6846\u67b6\u663e\u8457\u6269\u5c55\u4e86\u53ef\u6d4b\u91cf\u8303\u56f4\uff08\u4ece450 deg/s\u52301500 deg/s\uff09\uff0c\u5927\u5e45\u964d\u4f4e\u504f\u5dee\u4e0d\u7a33\u5b9a\u6027\uff0898.4%\uff09\uff0c\u5e76\u5728\u60ef\u6027\u4fe1\u53f7\u589e\u5f3a\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "MoE-Gyro\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86MEMS\u9640\u87ba\u4eea\u5728\u6d4b\u91cf\u8303\u56f4\u548c\u566a\u58f0\u6027\u80fd\u4e4b\u95f4\u7684\u957f\u671f\u6743\u8861\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8d85\u8303\u56f4\u4fe1\u53f7\u91cd\u5efa\u548c\u964d\u566a\u3002ISEBench\u5e73\u53f0\u4e3a\u60ef\u6027\u4fe1\u53f7\u589e\u5f3a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2506.07092", "pdf": "https://arxiv.org/pdf/2506.07092", "abs": "https://arxiv.org/abs/2506.07092", "authors": ["Joydeb Kumar Sana", "Mohammad M. Masud", "M Sohel Rahman", "M Saifur Rahman"], "title": "Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data", "categories": ["cs.LG", "cs.AI"], "comment": "This paper presents a novel distributed patient similarity\n  computation (DPSC) technique based on data transformation (DT) methods,\n  utilizing an effective combination of time series and static data", "summary": "Patient similarity computation (PSC) is a fundamental problem in healthcare\ninformatics. The aim of the patient similarity computation is to measure the\nsimilarity among patients according to their historical clinical records, which\nhelps to improve clinical decision support. This paper presents a novel\ndistributed patient similarity computation (DPSC) technique based on data\ntransformation (DT) methods, utilizing an effective combination of time series\nand static data. Time series data are sensor-collected patients' information,\nincluding metrics like heart rate, blood pressure, Oxygen saturation,\nrespiration, etc. The static data are mainly patient background and demographic\ndata, including age, weight, height, gender, etc. Static data has been used for\nclustering the patients. Before feeding the static data to the machine learning\nmodel adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)\nmethods have been performed, which improve the prediction performances. In\naWOE-based patient similarity models, sensitive patient information has been\nprocessed using aWOE which preserves the data privacy of the trained models. We\nused the Dynamic Time Warping (DTW) approach, which is robust and very popular,\nfor time series similarity. However, DTW is not suitable for big data due to\nthe significant computational run-time. To overcome this problem, distributed\nDTW computation is used in this study. For Coronary Artery Disease, our DT\nbased approach boosts prediction performance by as much as 11.4%, 10.20%, and\n12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of\nCongestive Heart Failure (CHF), our proposed method achieves performance\nenhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.\nThe proposed method reduces the computation time by as high as 40%.", "AI": {"tldr": "This paper proposes a Distributed Patient Similarity Computation (DPSC) technique based on data transformation methods for measuring patient similarity using both time series and static data. The approach enhances prediction performance and reduces computation time.", "motivation": "To improve clinical decision support by accurately measuring patient similarity through effective processing of historical clinical records including time series and static data.", "method": "The method combines adaptive Weight-of-Evidence (aWOE) and Z-score data transformation techniques for static data, and distributed Dynamic Time Warping (DTW) for time series data to compute patient similarity while preserving data privacy.", "result": "For Coronary Artery Disease, the method improves AUC by 11.4%, accuracy by 10.20%, F-measure by 12.6%, and reduces computation time by up to 40%. For Congestive Heart Failure, it achieves improvements of 15.9% in AUC, 10.5% in accuracy, and 21.9% in F-measure.", "conclusion": "The DPSC technique effectively boosts prediction performance and significantly reduces computational run-time, making it suitable for large-scale healthcare informatics applications."}}
{"id": "2506.06322", "pdf": "https://arxiv.org/pdf/2506.06322", "abs": "https://arxiv.org/abs/2506.06322", "authors": ["Polad Geidarov"], "title": "Neural networks with image recognition by pairs", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Neural networks based on metric recognition methods have a strictly\ndetermined architecture. Number of neurons, connections, as well as weights and\nthresholds values are calculated analytically, based on the initial conditions\nof tasks: number of recognizable classes, number of samples, metric expressions\nused. This paper discusses the possibility of transforming these networks in\norder to apply classical learning algorithms to them without using analytical\nexpressions that calculate weight values. In the received network, training is\ncarried out by recognizing images in pairs. This approach simplifies the\nlearning process and easily allows to expand the neural network by adding new\nimages to the recognition task. The advantages of these networks, including\nsuch as: 1) network architecture simplicity and transparency; 2) training\nsimplicity and reliability; 3) the possibility of using a large number of\nimages in the recognition problem using a neural network; 4) a consistent\nincrease in the number of recognizable classes without changing the previous\nvalues of weights and thresholds.", "AI": {"tldr": "This paper explores transforming metric recognition neural networks to apply classical learning algorithms, simplifying training and enabling network expansion without altering existing weights.", "motivation": "To adapt metric recognition neural networks for classical learning algorithms without using analytical expressions for weight calculation, thus simplifying the learning process and allowing network expansion.", "method": "Transforming the architecture of metric recognition neural networks so that training can be conducted by recognizing images in pairs, maintaining simplicity and transparency in architecture, and enabling the addition of new images without changing previous weight values.", "result": "The approach results in a simplified and reliable training process, transparent network architecture, capability to handle a large number of images, and the ability to increase recognizable classes consistently without modifying prior weight and threshold values.", "conclusion": "The transformation of metric recognition neural networks allows for efficient learning and scalability, offering advantages such as simple architecture, reliable training, and the ability to expand the number of recognizable classes without affecting existing parameters."}}
{"id": "2506.07099", "pdf": "https://arxiv.org/pdf/2506.07099", "abs": "https://arxiv.org/abs/2506.07099", "authors": ["Wenying He", "Jieling Huang", "Junhua Gu", "Ji Zhang", "Yude Bai"], "title": "Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages,3 figures", "summary": "Missing data in spatiotemporal systems presents a significant challenge for\nmodern applications, ranging from environmental monitoring to urban traffic\nmanagement. The integrity of spatiotemporal data often deteriorates due to\nhardware malfunctions and software failures in real-world deployments. Current\napproaches based on machine learning and deep learning struggle to model the\nintricate interdependencies between spatial and temporal dimensions effectively\nand, more importantly, suffer from cumulative errors during the data imputation\nprocess, which propagate and amplify through iterations. To address these\nlimitations, we propose CoFILL, a novel Conditional Diffusion Model for\nspatiotemporal data imputation. CoFILL builds on the inherent advantages of\ndiffusion models to generate high-quality imputations without relying on\npotentially error-prone prior estimates. It incorporates an innovative\ndual-stream architecture that processes temporal and frequency domain features\nin parallel. By fusing these complementary features, CoFILL captures both rapid\nfluctuations and underlying patterns in the data, which enables more robust\nimputation. The extensive experiments reveal that CoFILL's noise prediction\nnetwork successfully transforms random noise into meaningful values that align\nwith the true data distribution. The results also show that CoFILL outperforms\nstate-of-the-art methods in imputation accuracy. The source code is publicly\navailable at https://github.com/joyHJL/CoFILL.", "AI": {"tldr": "CoFILL\u662f\u4e00\u79cd\u65b0\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u3002\u5b83\u901a\u8fc7\u53cc\u6d41\u67b6\u6784\u5904\u7406\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u6355\u6349\u6570\u636e\u7684\u5feb\u901f\u6ce2\u52a8\u548c\u5e95\u5c42\u6a21\u5f0f\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u7a33\u5065\u7684\u63d2\u8865\u3002\u5b9e\u9a8c\u8868\u660e\uff0cCoFILL\u5728\u63d2\u8865\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u7a7a\u6570\u636e\u4e2d\u7684\u7f3a\u5931\u95ee\u9898\u5bf9\u73b0\u4ee3\u5e94\u7528\u6784\u6210\u4e86\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5efa\u6a21\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u590d\u6742\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u5728\u6570\u636e\u63d2\u8865\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u4ea7\u751f\u7d2f\u79ef\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86CoFILL\uff0c\u4e00\u79cd\u65b0\u578b\u7684\u6761\u4ef6\u6269\u6563\u6a21\u578b\u3002\u8be5\u6a21\u578b\u5229\u7528\u6269\u6563\u6a21\u578b\u7684\u4f18\u70b9\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u63d2\u8865\u7ed3\u679c\uff0c\u800c\u65e0\u9700\u4f9d\u8d56\u53ef\u80fd\u51fa\u9519\u7684\u5148\u9a8c\u4f30\u8ba1\u3002\u540c\u65f6\uff0c\u91c7\u7528\u521b\u65b0\u7684\u53cc\u6d41\u67b6\u6784\u5e76\u884c\u5904\u7406\u65f6\u57df\u548c\u9891\u57df\u7279\u5f81\uff0c\u878d\u5408\u4e92\u8865\u7279\u5f81\u4ee5\u6355\u6349\u6570\u636e\u7684\u5feb\u901f\u6ce2\u52a8\u548c\u5e95\u5c42\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoFILL\u7684\u566a\u58f0\u9884\u6d4b\u7f51\u7edc\u80fd\u591f\u6210\u529f\u5c06\u968f\u673a\u566a\u58f0\u8f6c\u5316\u4e3a\u4e0e\u771f\u5b9e\u6570\u636e\u5206\u5e03\u4e00\u81f4\u7684\u6709\u610f\u4e49\u503c\uff0c\u5176\u63d2\u8865\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "CoFILL\u4e3a\u65f6\u7a7a\u6570\u636e\u63d2\u8865\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u63d2\u8865\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2506.06325", "pdf": "https://arxiv.org/pdf/2506.06325", "abs": "https://arxiv.org/abs/2506.06325", "authors": ["Viorica Rozina Chifu", "Tudor Cioara", "Cristina Bianca Pop", "Ionut Anghel"], "title": "Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies", "categories": ["cs.NE", "cs.AI", "cs.GT", "cs.MA"], "comment": null, "summary": "This paper proposes a decentralized model of energy cooperation between\nmicrogrids, in which decisions are made locally, at the level of the microgrid\ncommunity. Each microgrid is modeled as an autonomous agent that adopts a Hawk\nor Dove strategy, depending on the level of energy stored in the battery and\nits role in the energy trading process. The interactions between selling and\nbuying microgrids are modeled through an evolutionary algorithm. An individual\nin the algorithm population is represented as an energy trading matrix that\nencodes the amounts of energy traded between the selling and buying microgrids.\nThe population evolution is achieved by recombination and mutation operators.\nRecombination uses a specialized operator for matrix structures, and mutation\nis applied to the matrix elements according to a Gaussian distribution. The\nevaluation of an individual is made with a multi-criteria fitness function that\nconsiders the seller profit, the degree of energy stability at the community\nlevel, penalties for energy imbalance at the community level and for the\ndegradation of microgrids batteries. The method was tested on a simulated\nscenario with 100 microgrids, each with its own selling and buying thresholds,\nto reflect a realistic environment with variable storage characteristics of\nmicrogrids batteries. By applying the algorithm on this scenario, 95 out of the\n100 microgrids reached a stable energy state. This result confirms the\neffectiveness of the proposed model in achieving energy balance both at the\nindividual level, for each microgrid, and at the level of the entire community.", "AI": {"tldr": "This paper proposes a decentralized model of energy cooperation between microgrids using an evolutionary algorithm, achieving stable energy states for 95 out of 100 simulated microgrids.", "motivation": "To create a decentralized decision-making model for energy cooperation among microgrids to achieve energy balance both individually and communally.", "method": "Microgrids are modeled as autonomous agents adopting Hawk or Dove strategies based on battery storage levels and trading roles. Interactions are modeled via an evolutionary algorithm where individuals are represented as energy trading matrices. Population evolution occurs through recombination and mutation operators with evaluations made by a multi-criteria fitness function.", "result": "In a test scenario with 100 microgrids, 95 reached a stable energy state, showing the model's effectiveness in achieving energy balance at both individual and community levels.", "conclusion": "The proposed decentralized model effectively achieves energy balance within microgrid communities through local decision-making."}}
{"id": "2506.07109", "pdf": "https://arxiv.org/pdf/2506.07109", "abs": "https://arxiv.org/abs/2506.07109", "authors": ["Rong-Xi Tan", "Ming Chen", "Ke Xue", "Yao Wang", "Yaoyuan Wang", "Sheng Fu", "Chao Qian"], "title": "Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "ICML 2025", "summary": "The pursuit of universal black-box optimization (BBO) algorithms is a\nlongstanding goal. However, unlike domains such as language or vision, where\nscaling structured data has driven generalization, progress in offline BBO\nremains hindered by the lack of unified representations for heterogeneous\nnumerical spaces. Thus, existing offline BBO approaches are constrained to\nsingle-task and fixed-dimensional settings, failing to achieve cross-domain\nuniversal optimization. Recent advances in language models (LMs) offer a\npromising path forward: their embeddings capture latent relationships in a\nunifying way, enabling universal optimization across different data types\npossible. In this paper, we discuss multiple potential approaches, including an\nend-to-end learning framework in the form of next-token prediction, as well as\nprioritizing the learning of latent spaces with strong representational\ncapabilities. To validate the effectiveness of these methods, we collect\noffline BBO tasks and data from open-source academic works for training.\nExperiments demonstrate the universality and effectiveness of our proposed\nmethods. Our findings suggest that unifying language model priors and learning\nstring embedding space can overcome traditional barriers in universal BBO,\npaving the way for general-purpose BBO algorithms. The code is provided at\nhttps://github.com/lamda-bbo/universal-offline-bbo.", "AI": {"tldr": "This paper explores the use of language model embeddings for universal black-box optimization (BBO), presenting methods that enable cross-domain optimization and demonstrating their effectiveness through experiments.", "motivation": "Current offline BBO approaches are limited to single-task and fixed-dimensional settings due to the lack of unified representations for heterogeneous numerical spaces, hindering progress towards general-purpose BBO algorithms.", "method": "The authors propose multiple approaches inspired by advances in language models, including an end-to-end learning framework based on next-token prediction and prioritizing the learning of latent spaces with strong representational capabilities. These methods aim to unify language model priors and learn string embedding spaces for overcoming traditional barriers in universal BBO.", "result": "Experiments using collected offline BBO tasks and data from open-source academic works show the universality and effectiveness of the proposed methods in achieving cross-domain universal optimization.", "conclusion": "The findings indicate that incorporating language model embeddings can overcome limitations in traditional BBO, paving the way for the development of general-purpose BBO algorithms."}}
{"id": "2506.06331", "pdf": "https://arxiv.org/pdf/2506.06331", "abs": "https://arxiv.org/abs/2506.06331", "authors": ["Qiming Zeng", "Xiao Yan", "Hao Luo", "Yuhao Lin", "Yuxiang Wang", "Fangcheng Fu", "Bo Du", "Quanqing Xu", "Jiawei Jiang"], "title": "How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "By retrieving contexts from knowledge graphs, graph-based retrieval-augmented\ngeneration (GraphRAG) enhances large language models (LLMs) to generate quality\nanswers for user questions. Many GraphRAG methods have been proposed and\nreported inspiring performance in answer quality. However, we observe that the\ncurrent answer evaluation framework for GraphRAG has two critical flaws, i.e.,\nunrelated questions and evaluation biases, which may lead to biased or even\nwrong conclusions on performance. To tackle the two flaws, we propose an\nunbiased evaluation framework that uses graph-text-grounded question generation\nto produce questions that are more related to the underlying dataset and an\nunbiased evaluation procedure to eliminate the biases in LLM-based answer\nassessment. We apply our unbiased framework to evaluate 3 representative\nGraphRAG methods and find that their performance gains are much more moderate\nthan reported previously. Although our evaluation framework may still have\nflaws, it calls for scientific evaluations to lay solid foundations for\nGraphRAG research.", "AI": {"tldr": "In this paper, researchers address flaws in the current evaluation framework for GraphRAG methods, proposing an unbiased evaluation framework that generates more relevant questions and reduces biases in assessment. When applied to three representative GraphRAG methods, the new framework reveals much more moderate performance gains than previously reported, emphasizing the need for scientific evaluations in GraphRAG research.", "motivation": "The authors observe two critical flaws in the current answer evaluation framework for GraphRAG: unrelated questions and evaluation biases, which may lead to biased or incorrect conclusions about performance.", "method": "They propose an unbiased evaluation framework consisting of graph-text-grounded question generation to produce more related questions and an unbiased evaluation procedure to eliminate biases in LLM-based answer assessment.", "result": "When applying the proposed unbiased framework to evaluate three representative GraphRAG methods, it is found that their performance gains are much more moderate than previously reported.", "conclusion": "Although the proposed evaluation framework might still have flaws, it highlights the importance of scientific evaluations to establish solid foundations for GraphRAG research."}}
{"id": "2506.07121", "pdf": "https://arxiv.org/pdf/2506.07121", "abs": "https://arxiv.org/abs/2506.07121", "authors": ["Ren-Jian Wang", "Ke Xue", "Zeyu Qin", "Ziniu Li", "Sheng Tang", "Hao-Tian Li", "Shengcai Liu", "Chao Qian"], "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Ensuring safety of large language models (LLMs) is important. Red teaming--a\nsystematic approach to identifying adversarial prompts that elicit harmful\nresponses from target LLMs--has emerged as a crucial safety evaluation method.\nWithin this framework, the diversity of adversarial prompts is essential for\ncomprehensive safety assessments. We find that previous approaches to\nred-teaming may suffer from two key limitations. First, they often pursue\ndiversity through simplistic metrics like word frequency or sentence embedding\nsimilarity, which may not capture meaningful variation in attack strategies.\nSecond, the common practice of training a single attacker model restricts\ncoverage across potential attack styles and risk categories. This paper\nintroduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to\naddress these limitations. QDRT achieves goal-driven diversity through\nbehavior-conditioned training and implements a behavioral replay buffer in an\nopen-ended manner. Additionally, it trains multiple specialized attackers\ncapable of generating high-quality attacks across diverse styles and risk\ncategories. Our empirical evaluation demonstrates that QDRT generates attacks\nthat are both more diverse and more effective against a wide range of target\nLLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the\nfield of LLM safety by providing a systematic and effective approach to\nautomated red-teaming, ultimately supporting the responsible deployment of\nLLMs.", "AI": {"tldr": "This paper introduces Quality-Diversity Red-Teaming (QDRT), a new framework to enhance the diversity and effectiveness of adversarial prompts for evaluating the safety of large language models.", "motivation": "Previous approaches to red-teaming LLMs have limitations in capturing meaningful variation in attack strategies and restricted coverage across potential attack styles and risk categories due to simplistic metrics and reliance on single attacker models.", "method": "QDRT achieves goal-driven diversity through behavior-conditioned training with a behavioral replay buffer and trains multiple specialized attackers capable of generating high-quality attacks across diverse styles and risk categories.", "result": "Empirical evaluation shows that QDRT generates more diverse and effective attacks against a wide range of target LLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5.", "conclusion": "QDRT provides a systematic and effective approach to automated red-teaming, advancing the field of LLM safety and supporting responsible LLM deployment."}}
{"id": "2506.06332", "pdf": "https://arxiv.org/pdf/2506.06332", "abs": "https://arxiv.org/abs/2506.06332", "authors": ["Mikko Stenlund"], "title": "Introduction to Predictive Coding Networks for Machine Learning", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Predictive coding networks (PCNs) constitute a biologically inspired\nframework for understanding hierarchical computation in the brain, and offer an\nalternative to traditional feedforward neural networks in ML. This note serves\nas a quick, onboarding introduction to PCNs for machine learning practitioners.\nWe cover the foundational network architecture, inference and learning update\nrules, and algorithmic implementation. A concrete image-classification task\n(CIFAR-10) is provided as a benchmark-smashing application, together with an\naccompanying Python notebook containing the PyTorch implementation.", "AI": {"tldr": "This paper introduces Predictive Coding Networks (PCNs) to machine learning practitioners, providing foundational architecture, inference and learning rules, algorithmic implementation, and a CIFAR-10 image-classification benchmark with PyTorch code.", "motivation": "To offer an alternative framework for understanding brain computation and provide a biologically inspired approach to traditional feedforward neural networks in ML.", "method": "Covering the network architecture of PCNs, their inference and learning update rules, and algorithmic implementation, along with a practical image classification task on CIFAR-10 dataset.", "result": "A successful application on CIFAR-10 image classification task is demonstrated, provided as a benchmark-smashing example.", "conclusion": "PCNs serve as a promising biologically-inspired framework for ML, offering an alternative to traditional approaches."}}
{"id": "2506.07134", "pdf": "https://arxiv.org/pdf/2506.07134", "abs": "https://arxiv.org/abs/2506.07134", "authors": ["Eshwar S. R.", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "math.OC", "90C40 (Primary), 93E20, 68T05 (Secondary)", "I.2.6; G.3"], "comment": "19 pages", "summary": "Despite decades of research, it remains challenging to correctly use\nReinforcement Learning (RL) algorithms with function approximation. A prime\nexample is policy iteration, whose fundamental guarantee of monotonic\nimprovement collapses even under linear function approximation. To address this\nissue, we introduce Reliable Policy Iteration (RPI). It replaces the common\nprojection or Bellman-error minimization during policy evaluation with a\nBellman-based constrained optimization. We prove that not only does RPI confer\ntextbook monotonicity on its value estimates but these estimates also lower\nbound the true return. Also, their limit partially satisfies the unprojected\nBellman equation, emphasizing RPI's natural fit within RL. RPI is the first\nalgorithm with such monotonicity and convergence guarantees under function\napproximation. For practical use, we provide a model-free variant of RPI that\namounts to a novel critic. It can be readily integrated into primary model-free\nPI implementations such as DQN and DDPG. In classical control tasks, such\nRPI-enhanced variants consistently maintain their lower-bound guarantee while\nmatching or surpassing the performance of all baseline methods.", "AI": {"tldr": "An abstract about Reliable Policy Iteration (RPI) in Reinforcement Learning (RL), which ensures monotonic improvement and convergence guarantees under function approximation.", "motivation": "Reinforcement Learning algorithms with function approximation are challenging to use correctly. The common policy iteration's guarantee of monotonic improvement collapses even under linear function approximation.", "method": "Introduced Reliable Policy Iteration (RPI). It replaces the common projection or Bellman-error minimization during policy evaluation with a Bellman-based constrained optimization.", "result": "RPI confers textbook monotonicity on its value estimates and these estimates also lower bound the true return. RPI-enhanced variants consistently maintain their lower-bound guarantee while matching or surpassing the performance of all baseline methods in classical control tasks.", "conclusion": "RPI is the first algorithm with monotonicity and convergence guarantees under function approximation and can be integrated into model-free PI implementations such as DQN and DDPG."}}
{"id": "2506.06335", "pdf": "https://arxiv.org/pdf/2506.06335", "abs": "https://arxiv.org/abs/2506.06335", "authors": ["Xuan Xu", "Fufang Wen", "Beilin Chu", "Zhibing Fu", "Qinhong Lin", "Jiaqi Liu", "Binjie Fei", "Zhongliang Yang", "Linna Zhou", "Yu Li"], "title": "FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "In natural language processing (NLP), the focus has shifted from encoder-only\ntiny language models like BERT to decoder-only large language models(LLMs) such\nas GPT-3. However, LLMs' practical application in the financial sector has\nrevealed three limitations: (1) LLMs often perform worse than fine-tuned BERT\non discriminative tasks despite costing much higher computational resources,\nsuch as market sentiment analysis in financial reports; (2) Application on\ngenerative tasks heavily relies on retrieval augmented generation (RAG) methods\nto provide current and specialized information, with general retrievers showing\nsuboptimal performance on domain-specific retrieval tasks; (3) There are\nadditional inadequacies in other feature-based scenarios, such as topic\nmodeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained\non a high-quality, financial-specific corpus of 32b tokens. This represents the\nlargest known Chinese financial pretraining corpus for models of this parameter\nsize. As a better backbone, FinBERT2 can bridge the gap in the\nfinancial-specific deployment of LLMs through the following achievements: (1)\nDiscriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT\nvariants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five\nfinancial classification tasks. (2) Contrastive fine-tuned models\n(Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over\nBGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's\ntext-embedding-3-large) embedders across five financial retrieval tasks; (3)\nBuilding on FinBERT2 variants, we construct the Fin-TopicModel, which enables\nsuperior clustering and topic representation for financial titles. Our work\nrevisits financial BERT models through comparative analysis with contemporary\nLLMs and offers practical insights for effectively utilizing FinBERT in the\nLLMs era.", "AI": {"tldr": "In NLP, there's a shift from encoder-only tiny language models to decoder-only large language models (LLMs). However, LLMs have limitations in the financial sector. This paper introduces FinBERT2, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens, which addresses these limitations.", "motivation": "The motivation is to address the limitations of LLMs in the financial sector, such as worse performance than fine-tuned BERT on discriminative tasks, reliance on RAG methods for generative tasks, and inadequacies in feature-based scenarios like topic modeling.", "method": "FinBERT2 is introduced, a specialized bidirectional encoder pretrained on a high-quality, financial-specific corpus of 32b tokens. It is used as a better backbone to bridge the gap in the financial-specific deployment of LLMs.", "result": "1) Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five financial classification tasks. 2) Contrastive fine-tuned models (Fin-Retrievers) outperform both open-source and proprietary embedders across five financial retrieval tasks. 3) The Fin-TopicModel, built on FinBERT2 variants, enables superior clustering and topic representation for financial titles.", "conclusion": "This work revisits financial BERT models through comparative analysis with contemporary LLMs and offers practical insights for effectively utilizing FinBERT in the LLMs era."}}
{"id": "2506.07165", "pdf": "https://arxiv.org/pdf/2506.07165", "abs": "https://arxiv.org/abs/2506.07165", "authors": ["Qi Liu", "Jingqing Ruan", "Hao Li", "Haodong Zhao", "Desheng Wang", "Jiansong Chen", "Wan Guanglu", "Xunliang Cai", "Zhi Zheng", "Tong Xu"], "title": "AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Existing multi-objective preference alignment methods for large language\nmodels (LLMs) face limitations: (1) the inability to effectively balance\nvarious preference dimensions, and (2) reliance on auxiliary reward/reference\nmodels introduces computational complexity. To address these challenges, we\npropose Adaptive Multi-objective Preference Optimization (AMoPO), a novel\nframework that achieves dynamic balance across preference dimensions. By\nintroducing the multi-objective optimization paradigm to use the\ndimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with\ndiverse preferences without additional reward models or reference models. We\nintroduce an adaptive weight assignment mechanism that models the generation\nspace as a Gaussian distribution, allowing dynamic prioritization of preference\ndimensions. Empirical results demonstrate that AMoPO outperforms\nstate-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B\nmodels reveal the scaling ability of AMoPO. Moreover, additional analysis of\nmultiple dimensions verifies its adaptability and effectiveness. These findings\nvalidate AMoPO's capability to achieve dimension-aware preference alignment,\nhighlighting its superiority. Our codes and datasets are available at\nhttps://github.com/Javkonline/AMoPO.", "AI": {"tldr": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u591a\u76ee\u6807\u504f\u597d\u4f18\u5316\uff08AMoPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u7ef4\u5ea6\u611f\u77e5\u751f\u6210\u6307\u6807\u4f5c\u4e3a\u9690\u5f0f\u5956\u52b1\u548c\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u540c\u504f\u597d\u7ef4\u5ea6\u4e0a\u7684\u52a8\u6001\u5e73\u8861\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAMoPO\u57287B\u300114B\u548c32B\u6a21\u578b\u4e0a\u5747\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u76ee\u6807\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1) \u65e0\u6cd5\u6709\u6548\u5e73\u8861\u5404\u79cd\u504f\u597d\u7ef4\u5ea6\uff1b(2) \u5bf9\u8f85\u52a9\u5956\u52b1/\u53c2\u8003\u6a21\u578b\u7684\u4f9d\u8d56\u589e\u52a0\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u5b9e\u73b0\u52a8\u6001\u5e73\u8861\u5e76\u51cf\u5c11\u5bf9\u989d\u5916\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAMoPO\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u591a\u76ee\u6807\u4f18\u5316\u8303\u5f0f\u5f15\u5165\u5230\u4f7f\u7528\u7ef4\u5ea6\u611f\u77e5\u751f\u6210\u6307\u6807\u4f5c\u4e3a\u9690\u5f0f\u5956\u52b1\u4e2d\uff0c\u4ece\u800c\u65e0\u9700\u989d\u5916\u7684\u5956\u52b1\u6216\u53c2\u8003\u6a21\u578b\u5373\u53ef\u5b9e\u73b0\u504f\u597d\u5bf9\u9f50\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6743\u91cd\u5206\u914d\u673a\u5236\uff0c\u5c06\u751f\u6210\u7a7a\u95f4\u5efa\u6a21\u4e3a\u9ad8\u65af\u5206\u5e03\uff0c\u4ee5\u5b9e\u73b0\u504f\u597d\u7ef4\u5ea6\u7684\u52a8\u6001\u4f18\u5148\u7ea7\u8c03\u6574\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cAMoPO\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa28.5%\u3002\u57287B\u300114B\u548c32B\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u5c55\u793a\u4e86AMoPO\u7684\u53ef\u6269\u5c55\u80fd\u529b\u3002\u591a\u7ef4\u5ea6\u5206\u6790\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u9002\u5e94\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "AMoPO\u80fd\u591f\u5b9e\u73b0\u7ef4\u5ea6\u611f\u77e5\u7684\u504f\u597d\u5bf9\u9f50\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2506.06339", "pdf": "https://arxiv.org/pdf/2506.06339", "abs": "https://arxiv.org/abs/2506.06339", "authors": ["Jumana Alsubhi", "Mohammad D. Alahmadi", "Ahmed Alhusayni", "Ibrahim Aldailami", "Israa Hamdine", "Ahmad Shabana", "Yazeed Iskandar", "Suhayb Khayyat"], "title": "Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture\nfor combining the precision of retrieval systems with the fluency of large\nlanguage models. While several studies have investigated RAG pipelines for\nhigh-resource languages, the optimization of RAG components for Arabic remains\nunderexplored. This study presents a comprehensive empirical evaluation of\nstate-of-the-art RAG components-including chunking strategies, embedding\nmodels, rerankers, and language models-across a diverse set of Arabic datasets.\nUsing the RAGAS framework, we systematically compare performance across four\ncore metrics: context precision, context recall, answer faithfulness, and\nanswer relevancy. Our experiments demonstrate that sentence-aware chunking\noutperforms all other segmentation methods, while BGE-M3 and\nMultilingual-E5-large emerge as the most effective embedding models. The\ninclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness\nin complex datasets, and Aya-8B surpasses StableLM in generation quality. These\nfindings provide critical insights for building high-quality Arabic RAG\npipelines and offer practical guidelines for selecting optimal components\nacross different document types.", "AI": {"tldr": "Retrieval-Augmented Generation (RAG) for Arabic is underexplored. This study evaluates RAG components on Arabic datasets using the RAGAS framework across four metrics. Findings suggest sentence-aware chunking, BGE-M3/Multilingual-E5-large embeddings, bge-reranker-v2-m3, and Aya-8B are optimal for Arabic RAG pipelines.", "motivation": "To address the lack of exploration in optimizing RAG components specifically for Arabic language processing.", "method": "Conduct a comprehensive empirical evaluation of state-of-the-art RAG components including chunking strategies, embedding models, rerankers, and language models on diverse Arabic datasets using the RAGAS framework.", "result": "Sentence-aware chunking performs best among segmentation methods; BGE-M3 and Multilingual-E5-large are the most effective embedding models; inclusion of bge-reranker-v2-m3 improves faithfulness in complex datasets; Aya-8B surpasses StableLM in generation quality.", "conclusion": "The study provides critical insights and practical guidelines for selecting optimal RAG components for building high-quality Arabic RAG pipelines."}}
{"id": "2506.07168", "pdf": "https://arxiv.org/pdf/2506.07168", "abs": "https://arxiv.org/abs/2506.07168", "authors": ["Huanyi Xie", "Lijie Hu", "Lu Yu", "Tianhao Huang", "Longfei Li", "Meng Li", "Jun Zhou", "Huan Wang", "Di Wang"], "title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural\nnetworks (GNNs) often fall short due to the complex textual information\nassociated with each node. Recent methods have improved node representations by\nleveraging large language models (LLMs) to enhance node text features, but\nthese approaches typically require extensive annotations or fine-tuning across\nall nodes, which is both time-consuming and costly. To overcome these\nchallenges, we introduce GAGA, an efficient framework for TAG representation\nlearning. GAGA reduces annotation time and cost by focusing on annotating only\nrepresentative nodes and edges. It constructs an annotation graph that captures\nthe topological relationships among these annotations. Furthermore, GAGA\nemploys a two-level alignment module to effectively integrate the annotation\ngraph with the TAG, aligning their underlying structures. Experiments show that\nGAGA achieves classification accuracies on par with or surpassing\nstate-of-the-art methods while requiring only 1% of the data to be annotated,\ndemonstrating its high efficiency.", "AI": {"tldr": "GAGA is an efficient framework for TAG representation learning which achieves high accuracy with only 1% data annotation.", "motivation": "Traditional GNNs struggle with complex textual information in TAGs. Recent methods using LLMs require extensive annotations or fine-tuning, which are time-consuming and costly.", "method": "GAGA focuses on annotating representative nodes and edges to reduce annotation time and cost. It constructs an annotation graph capturing topological relationships among these annotations and employs a two-level alignment module to integrate the annotation graph with the TAG.", "result": "Experiments demonstrate that GAGA achieves classification accuracies on par with or surpassing state-of-the-art methods while requiring only 1% of the data to be annotated.", "conclusion": "GAGA is an efficient framework for TAG representation learning that significantly reduces the need for extensive annotations without compromising accuracy."}}
{"id": "2506.06340", "pdf": "https://arxiv.org/pdf/2506.06340", "abs": "https://arxiv.org/abs/2506.06340", "authors": ["Wu Hao Ran", "Xi Xi", "Furong Li", "Jingyi Lu", "Jian Jiang", "Hui Huang", "Yuzhuan Zhang", "Shi Li"], "title": "Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs) has opened new avenues for\nanalyzing complex, unstructured data, particularly within the medical domain.\nElectronic Health Records (EHRs) contain a wealth of information in various\nformats, including free text clinical notes, structured lab results, and\ndiagnostic codes. This paper explores the application of advanced language\nmodels to leverage these diverse data sources for improved clinical decision\nsupport. We will discuss how text-based features, often overlooked in\ntraditional high dimensional EHR analysis, can provide semantically rich\nrepresentations and aid in harmonizing data across different institutions.\nFurthermore, we delve into the challenges and opportunities of incorporating\nmedical codes and ensuring the generalizability and fairness of AI models in\nhealthcare.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e3a\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u9886\u57df\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u9ad8\u7ea7\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u6587\u672c\u7279\u5f81\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u5728\u4e0d\u540c\u673a\u6784\u95f4\u7684\u6570\u636e\u534f\u8c03\uff0c\u5e76\u8ba8\u8bba\u4e86\u533b\u7597\u7f16\u7801\u6574\u5408\u53caAI\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u901a\u7528\u6027\u548c\u516c\u5e73\u6027\u6311\u6218\u4e0e\u673a\u9047\u3002", "motivation": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u66f4\u597d\u5730\u89e3\u6790\u548c\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u6539\u8fdb\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u6765\u5904\u7406\u548c\u5206\u6790EHR\u4e2d\u7684\u591a\u79cd\u6570\u636e\u683c\u5f0f\uff0c\u5305\u62ec\u81ea\u7531\u6587\u672c\u4e34\u5e8a\u7b14\u8bb0\u3001\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u5ba4\u7ed3\u679c\u548c\u8bca\u65ad\u4ee3\u7801\uff0c\u63d0\u53d6\u8bed\u4e49\u4e30\u5bcc\u7684\u6587\u672c\u7279\u5f81\uff0c\u4ee5\u63d0\u9ad8\u6570\u636e\u5206\u6790\u6548\u679c\u548c\u8de8\u673a\u6784\u6570\u636e\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u6587\u672c\u7279\u5f81\u63d0\u4f9b\u8bed\u4e49\u4e30\u5bcc\u7684\u8868\u793a\u5f62\u5f0f\uff0c\u5e2e\u52a9\u5728\u4e0d\u540c\u673a\u6784\u95f4\u5b9e\u73b0\u6570\u636e\u534f\u8c03\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u4e86\u5c06\u533b\u7597\u7f16\u7801\u7eb3\u5165\u8003\u8651\u4ee5\u53ca\u786e\u4fddAI\u6a21\u578b\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u901a\u7528\u6027\u548c\u516c\u5e73\u6027\u7684\u6311\u6218\u4e0e\u673a\u4f1a\u3002", "conclusion": "\u9ad8\u7ea7\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u533b\u7597\u7f16\u7801\u6574\u5408\u548cAI\u6a21\u578b\u7684\u901a\u7528\u6027\u4e0e\u516c\u5e73\u6027\u95ee\u9898\u3002"}}
{"id": "2506.07179", "pdf": "https://arxiv.org/pdf/2506.07179", "abs": "https://arxiv.org/abs/2506.07179", "authors": ["Kaiqi Wu", "Weiyang Kong", "Sen Zhang", "Yubao Liu", "Zitong Chen"], "title": "Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic prediction is a critical task in spatial-temporal forecasting with\nbroad applications in travel planning and urban management. Adaptive graph\nconvolution networks have emerged as mainstream solutions due to their ability\nto learn node embeddings in a data-driven manner and capture complex latent\ndependencies. However, existing adaptive graph learning methods for traffic\nforecasting often either ignore the regularization of node embeddings, which\naccount for a significant proportion of model parameters, or face scalability\nissues from expensive graph convolution operations. To address these\nchallenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.\nFirst, we introduce a regularized adaptive graph learning framework that\nsynergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via\na residual difference mechanism, achieving both embedding regularization and\nnoise suppression. Second, to ensure scalability on large road networks, we\ndevelop the Efficient Cosine Operator (ECO), which performs graph convolution\nbased on the cosine similarity of regularized embeddings with linear time\ncomplexity. Extensive experiments on four large-scale real-world traffic\ndatasets show that RAGL consistently outperforms state-of-the-art methods in\nterms of prediction accuracy and exhibits competitive computational efficiency.", "AI": {"tldr": "The paper presents RAGL, a model for traffic prediction which improves upon existing adaptive graph learning methods by introducing embedding regularization and an efficient graph convolution operator, showing superior performance and scalability.", "motivation": "Traffic prediction is crucial for travel planning and urban management, but current adaptive graph learning methods either lack embedding regularization or have scalability issues.", "method": "The RAGL model uses a regularized adaptive graph learning framework combining Stochastic Shared Embedding (SSE) with adaptive graph convolution through a residual difference mechanism for embedding regularization and noise suppression. It also employs the Efficient Cosine Operator (ECO) for scalable graph convolution with linear time complexity.", "result": "Experiments on four large-scale real-world traffic datasets demonstrate that RAGL outperforms state-of-the-art methods in prediction accuracy while maintaining competitive computational efficiency.", "conclusion": "RAGL addresses the limitations of previous adaptive graph learning methods for traffic forecasting by providing both improved prediction accuracy and scalability."}}
{"id": "2506.06341", "pdf": "https://arxiv.org/pdf/2506.06341", "abs": "https://arxiv.org/abs/2506.06341", "authors": ["Xinghe Cheng", "Xufang Zhou", "Liangda Fang", "Chaobo He", "Yuyu Zhou", "Weiqi Luo", "Zhiguo Gong", "Quanlong Guan"], "title": "NR4DER: Neural Re-ranking for Diversified Exercise Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CY"], "comment": "accepted for presentation at the SIGIR 2025 Full Papers track", "summary": "With the widespread adoption of online education platforms, an increasing\nnumber of students are gaining new knowledge through Massive Open Online\nCourses (MOOCs). Exercise recommendation have made strides toward improving\nstudent learning outcomes. However, existing methods not only struggle with\nhigh dropout rates but also fail to match the diverse learning pace of\nstudents. They frequently face difficulties in adjusting to inactive students'\nlearning patterns and in accommodating individualized learning paces, resulting\nin limited accuracy and diversity in recommendations. To tackle these\nchallenges, we propose Neural Re-ranking for Diversified Exercise\nRecommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to\nimprove the effectiveness of the exercise filter module. It then employs a\nsequence enhancement method to enhance the representation of inactive students,\naccurately matches students with exercises of appropriate difficulty. Finally,\nit utilizes neural re-ranking to generate diverse recommendation lists based on\nindividual students' learning histories. Extensive experimental results\nindicate that NR4DER significantly outperforms existing methods across multiple\nreal-world datasets and effectively caters to the diverse learning pace of\nstudents.", "AI": {"tldr": "The paper proposes NR4DER, a method that uses mLSTM for exercise filtering, sequence enhancement for inactive students, and neural re-ranking for diverse recommendations, outperforming existing methods in real-world datasets.", "motivation": "Existing exercise recommendation systems have issues with high dropout rates and fail to accommodate the diverse learning pace of students, leading to limited accuracy and diversity in recommendations.", "method": "NR4DER leverages mLSTM for improving exercise filter effectiveness, applies sequence enhancement for better representation of inactive students, matches students with suitable difficulty exercises, and uses neural re-ranking to generate diverse recommendation lists based on individual student histories.", "result": "Extensive experiments show NR4DER significantly outperforms existing methods across multiple real-world datasets, effectively addressing the diverse learning pace of students.", "conclusion": "NR4DER is an effective solution for diversified exercise recommendation, enhancing both accuracy and diversity in recommendations."}}
{"id": "2506.07185", "pdf": "https://arxiv.org/pdf/2506.07185", "abs": "https://arxiv.org/abs/2506.07185", "authors": ["J. C. Husillos", "A. Gallego", "A. Roma", "A. Troncoso"], "title": "Learning based on neurovectors for tabular data: a new neural network approach", "categories": ["cs.LG"], "comment": "Submitted to 25th IEEE International Conference on Data Mining (ICDM\n  2025)", "summary": "In this paper, we present a novel learning approach based on Neurovectors, an\ninnovative paradigm that structures information through interconnected nodes\nand vector relationships for tabular data processing. Unlike traditional\nartificial neural networks that rely on weight adjustment through\nbackpropagation, Neurovectors encode information by structuring data in vector\nspaces where energy propagation, rather than traditional weight updates, drives\nthe learning process, enabling a more adaptable and explainable learning\nprocess. Our method generates dynamic representations of knowledge through\nneurovectors, thereby improving both the interpretability and efficiency of the\npredictive model. Experimental results using datasets from well-established\nrepositories such as the UCI machine learning repository and Kaggle are\nreported both for classification and regression. To evaluate its performance,\nwe compare our approach with standard machine learning and deep learning\nmodels, showing that Neurovectors achieve competitive accuracy.", "AI": {"tldr": "This paper introduces Neurovectors, a new approach for tabular data processing that uses interconnected nodes and vector relationships. It replaces traditional weight updates with energy propagation, enhancing adaptability and interpretability.", "motivation": "To address the limitations of traditional artificial neural networks in terms of explainability and efficiency for tabular data processing.", "method": "Neurovectors encode information by structuring data in vector spaces where energy propagation drives the learning process, generating dynamic representations of knowledge.", "result": "Competitive accuracy is achieved compared to standard machine learning and deep learning models on classification and regression tasks using datasets from UCI and Kaggle repositories.", "conclusion": "Neurovectors offer an innovative paradigm for tabular data processing with improved interpretability and efficiency."}}
{"id": "2506.06343", "pdf": "https://arxiv.org/pdf/2506.06343", "abs": "https://arxiv.org/abs/2506.06343", "authors": ["Taesoo Kim", "Jong Hwan Ko"], "title": "TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in speech-enabled language models have shown promising\nresults in building intelligent voice assistants. However, most existing\napproaches rely on large-scale paired speech-text data and extensive\ncomputational resources, which pose challenges in terms of scalability and\naccessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework\nthat enables training speech-capable language models using only text data. Our\nkey insight is to leverage a unified encoder that maps semantically equivalent\ntext and speech inputs to a shared latent space. By aligning the encoder output\nwith the embedding space of a LLM via a lightweight projection network, we\nenable the model to generalize from text-only supervision to speech-based\ninference. Despite being trained exclusively on text, TESU-LLM achieves strong\nperformance on various speech-related benchmarks, comparable to baseline\nmethods trained with large-scale multimodal datasets and substantial\ncomputational resources. These results highlight the effectiveness and\nefficiency of our approach, offering a scalable path toward building speech\nLLMs without speech data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTESU-LLM\u7684\u65b0\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u6587\u672c\u6570\u636e\u8bad\u7ec3\u5177\u5907\u8bed\u97f3\u80fd\u529b\u7684\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u7edf\u4e00\u7f16\u7801\u5668\u5c06\u8bed\u4e49\u7b49\u4ef7\u7684\u6587\u672c\u548c\u8bed\u97f3\u8f93\u5165\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u4e0e\u8f7b\u91cf\u7ea7\u6295\u5f71\u7f51\u7edc\u5bf9\u9f50\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u4ece\u7eaf\u6587\u672c\u76d1\u7763\u63a8\u5e7f\u5230\u8bed\u97f3\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0cTESU-LLM\u5728\u591a\u4e2a\u8bed\u97f3\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5ab2\u7f8e\u4f7f\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u4e14\u65e0\u9700\u8bed\u97f3\u6570\u636e\uff0c\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u652f\u6301\u8bed\u8a00\u6a21\u578b\u5927\u591a\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u914d\u5bf9\u7684\u8bed\u97f3-\u6587\u672c\u6570\u636e\u548c\u5e7f\u6cdb\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u5229\u7528\u66f4\u5c11\u7684\u6570\u636e\u548c\u8d44\u6e90\u5b9e\u73b0\u9ad8\u6548\u7684\u8bed\u97f3\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aTESU-LLM\u7684\u6846\u67b6\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u5229\u7528\u7edf\u4e00\u7f16\u7801\u5668\u5c06\u8bed\u4e49\u7b49\u4ef7\u7684\u6587\u672c\u548c\u8bed\u97f3\u8f93\u5165\u6620\u5c04\u5230\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\u3002\u7136\u540e\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6295\u5f71\u7f51\u7edc\u5c06\u7f16\u7801\u5668\u8f93\u51fa\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u7eaf\u6587\u672c\u76d1\u7763\u5230\u8bed\u97f3\u63a8\u7406\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5c3d\u7ba1\u4ec5\u57fa\u4e8e\u6587\u672c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0cTESU-LLM\u5728\u591a\u79cd\u8bed\u97f3\u76f8\u5173\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4e0e\u4f7f\u7528\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u8bad\u7ec3\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "TESU-LLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u7528\u4e8e\u6784\u5efa\u65e0\u9700\u8bed\u97f3\u6570\u636e\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u5411\u3002"}}
{"id": "2506.07191", "pdf": "https://arxiv.org/pdf/2506.07191", "abs": "https://arxiv.org/abs/2506.07191", "authors": ["Ramisa Farha", "Joshua O. Olukoya"], "title": "Analyzing Breast Cancer Survival Disparities by Race and Demographic Location: A Survival Analysis Approach", "categories": ["cs.LG", "stat.AP"], "comment": null, "summary": "This study employs a robust analytical framework to uncover patterns in\nsurvival outcomes among breast cancer patients from diverse racial and\ngeographical backgrounds. This research uses the SEER 2021 dataset to analyze\nbreast cancer survival outcomes to identify and comprehend dissimilarities. Our\napproach integrates exploratory data analysis (EDA), through this we identify\nkey variables that influence survival rates and employ survival analysis\ntechniques, including the Kaplan-Meier estimator and log-rank test and the\nadvanced modeling Cox Proportional Hazards model to determine how survival\nrates vary across racial groups and countries. Model validation and\ninterpretation are undertaken to ensure the reliability of our findings, which\nare documented comprehensively to inform policymakers and healthcare\nprofessionals. The outcome of this paper is a detailed version of statistical\nanalysis that not just highlights disparities in breast cancer treatment and\ncare but also serves as a foundational tool for developing targeted\ninterventions to address the inequalities effectively. Through this research,\nour aim is to contribute to the global efforts to improve breast cancer\noutcomes and reduce treatment disparities.", "AI": {"tldr": "This study uses a robust analytical framework to uncover patterns in survival outcomes among breast cancer patients from diverse racial and geographical backgrounds using the SEER 2021 dataset. It highlights disparities in breast cancer treatment and care, contributing to global efforts to improve outcomes and reduce disparities.", "motivation": "To identify and comprehend dissimilarities in breast cancer survival outcomes among patients from diverse racial and geographical backgrounds.", "method": "The research integrates exploratory data analysis (EDA) to identify key variables influencing survival rates and employs survival analysis techniques such as Kaplan-Meier estimator, log-rank test, and Cox Proportional Hazards model to analyze variations across racial groups and countries.", "result": "The outcome is a detailed statistical analysis highlighting disparities in breast cancer treatment and care, serving as a tool for developing targeted interventions to address inequalities effectively.", "conclusion": "This study aims to contribute to global efforts to improve breast cancer outcomes and reduce treatment disparities."}}
{"id": "2506.06344", "pdf": "https://arxiv.org/pdf/2506.06344", "abs": "https://arxiv.org/abs/2506.06344", "authors": ["Alex Pierron", "Michel Barbeau", "Luca De Cicco", "Jose Rubio-Hernan", "Joaquin Garcia-Alfaro"], "title": "A Reinforcement Learning Approach for RIS-aided Fair Communications", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "7 pages, 6 figures, 1 table, 16 references", "summary": "Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements\nthat can dynamically alter electromagnetic wave properties to enhance\nbeamforming and leading to improvements in areas with low coverage properties.\nThey have the potential to be combined with Reinforcement Learning (RL)\ntechniques to achieve network performance and energy efficiency via\noptimization techniques. In addition to performance and energy improvements, it\nis also crucial to consider the concept of fair communications. RISs must\nensure that User Equipment (UE) units receive their signals with adequate\nstrength, without other UE being deprived of service due to insufficient power.\nIn this paper, we address such a problem. We explore the fairness properties of\nprevious work and propose a novel method that aims at obtaining an efficient\nand fair duplex RIS-RL system for multiple legitimate UE units. We report and\ndiscuss our experimental work and simulation results. We also release our code\nand datasets to foster further research in the topic.", "AI": {"tldr": "This paper investigates the use of Reconfigurable Intelligent Surfaces (RISs) combined with Reinforcement Learning (RL) to create a fair and efficient communication system for multiple user equipment units.", "motivation": "The motivation is to enhance network performance and energy efficiency in areas with low coverage, while ensuring fair communications among User Equipment (UE) units.", "method": "The method involves exploring fairness properties from previous work and proposing a novel approach that integrates RISs with RL techniques to achieve an efficient and fair duplex system for multiple UE units.", "result": "The authors report their experimental work and simulation results, showing improvements in both efficiency and fairness in the communication system. They also release their code and datasets for further research.", "conclusion": "Reconfigurable Intelligent Surfaces combined with Reinforcement Learning can significantly improve both the efficiency and fairness of communication systems for multiple user equipment units."}}
{"id": "2506.07198", "pdf": "https://arxiv.org/pdf/2506.07198", "abs": "https://arxiv.org/abs/2506.07198", "authors": ["Tianci Bu", "Chuanrui Wang", "Hao Ma", "Haoren Zheng", "Xin Lu", "Tailin Wu"], "title": "GGBall: Graph Generative Model on Poincar\u00e9 Ball", "categories": ["cs.LG"], "comment": "29 pages, 3 figures", "summary": "Generating graphs with hierarchical structures remains a fundamental\nchallenge due to the limitations of Euclidean geometry in capturing exponential\ncomplexity. Here we introduce \\textbf{GGBall}, a novel hyperbolic framework for\ngraph generation that integrates geometric inductive biases with modern\ngenerative paradigms. GGBall combines a Hyperbolic Vector-Quantized Autoencoder\n(HVQVAE) with a Riemannian flow matching prior defined via closed-form\ngeodesics. This design enables flow-based priors to model complex latent\ndistributions, while vector quantization helps preserve the curvature-aware\nstructure of the hyperbolic space. We further develop a suite of hyperbolic GNN\nand Transformer layers that operate entirely within the manifold, ensuring\nstability and scalability. Empirically, our model reduces degree MMD by over\n75\\% on Community-Small and over 40\\% on Ego-Small compared to state-of-the-art\nbaselines, demonstrating an improved ability to preserve topological\nhierarchies. These results highlight the potential of hyperbolic geometry as a\npowerful foundation for the generative modeling of complex, structured, and\nhierarchical data domains. Our code is available at\n\\href{https://github.com/AI4Science-WestlakeU/GGBall}{here}.", "AI": {"tldr": "GGBall is a hyperbolic framework for graph generation that integrates geometric inductive biases with modern generative paradigms, reducing degree MMD significantly compared to state-of-the-art baselines.", "motivation": "Generating graphs with hierarchical structures is challenging due to the limitations of Euclidean geometry in capturing exponential complexity.", "method": "GGBall combines a Hyperbolic Vector-Quantized Autoencoder (HVQVAE) with a Riemannian flow matching prior defined via closed-form geodesics. It also includes hyperbolic GNN and Transformer layers that operate entirely within the manifold.", "result": "Empirically, our model reduces degree MMD by over 75% on Community-Small and over 40% on Ego-Small compared to state-of-the-art baselines.", "conclusion": "These results highlight the potential of hyperbolic geometry as a powerful foundation for the generative modeling of complex, structured, and hierarchical data domains."}}
{"id": "2506.06345", "pdf": "https://arxiv.org/pdf/2506.06345", "abs": "https://arxiv.org/abs/2506.06345", "authors": ["Sukru Selim Calik", "Andac Akyuz", "Zeynep Hilal Kilimci", "Kerem Colak"], "title": "Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "comment": null, "summary": "Financial literacy is increasingly dependent on the ability to interpret\ncomplex financial data and utilize advanced forecasting tools. In this context,\nthis study proposes a novel approach that combines transformer-based time\nseries models with explainable artificial intelligence (XAI) to enhance the\ninterpretability and accuracy of stock price predictions. The analysis focuses\non the daily stock prices of the five highest-volume banks listed in the\nBIST100 index, along with XBANK and XU100 indices, covering the period from\nJanuary 2015 to March 2025. Models including DLinear, LTSNet, Vanilla\nTransformer, and Time Series Transformer are employed, with input features\nenriched by technical indicators. SHAP and LIME techniques are used to provide\ntransparency into the influence of individual features on model outputs. The\nresults demonstrate the strong predictive capabilities of transformer models\nand highlight the potential of interpretable machine learning to empower\nindividuals in making informed investment decisions and actively engaging in\nfinancial markets.", "AI": {"tldr": "A novel approach combining transformer-based time series models with explainable AI (XAI) techniques is proposed to improve stock price prediction accuracy and interpretability for high-volume banks in BIST100, XBANK, and XU100 indices from 2015-2025. Models like DLinear, LTSNet, Vanilla Transformer, and Time Series Transformer are used alongside SHAP and LIME for feature influence transparency.", "motivation": "Financial literacy increasingly depends on interpreting complex financial data and using advanced forecasting tools. This study aims to enhance the interpretability and accuracy of stock price predictions by integrating transformer-based time series models with XAI.", "method": "The study uses transformer-based time series models (DLinear, LTSNet, Vanilla Transformer, Time Series Transformer) enriched with technical indicators to predict daily stock prices of five highest-volume banks in BIST100, along with XBANK and XU100 indices. SHAP and LIME are applied for explaining individual feature impacts.", "result": "Transformer models show strong predictive capabilities, and XAI methods provide valuable insights into model behavior, empowering individuals to make informed investment decisions.", "conclusion": "Combining transformer-based models with XAI techniques can significantly improve both the accuracy and interpretability of stock price predictions, enhancing financial literacy."}}
{"id": "2506.07218", "pdf": "https://arxiv.org/pdf/2506.07218", "abs": "https://arxiv.org/abs/2506.07218", "authors": ["Tong Xiao", "Xin Xu", "Zhenya Huang", "Hongyu Gao", "Quan Liu", "Qi Liu", "Enhong Chen"], "title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language\nModels (MLLMs) is a challenging task that has attracted increasing attention in\nthe community. Recently, several studies have applied Reinforcement Learning\nwith Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the\nreasoning abilities of MLLMs. However, these works largely overlook the\nenhancement of multimodal perception capabilities in MLLMs, which serve as a\ncore prerequisite and foundational component of complex multimodal reasoning.\nThrough McNemar's test, we find that existing RLVR method fails to effectively\nenhance the multimodal perception capabilities of MLLMs, thereby limiting their\nfurther improvement in multimodal reasoning. To address this limitation, we\npropose Perception-R1, which introduces a novel visual perception reward that\nexplicitly encourages MLLMs to perceive the visual content accurately, thereby\ncan effectively incentivizing both their multimodal perception and reasoning\ncapabilities. Specifically, we first collect textual visual annotations from\nthe CoT trajectories of multimodal problems, which will serve as visual\nreferences for reward assignment. During RLVR training, we employ a judging LLM\nto assess the consistency between the visual annotations and the responses\ngenerated by MLLM, and assign the visual perception reward based on these\nconsistency judgments. Extensive experiments on several multimodal reasoning\nbenchmarks demonstrate the effectiveness of our Perception-R1, which achieves\nstate-of-the-art performance on most benchmarks using only 1,442 training data.", "AI": {"tldr": "An abstract about enhancing multimodal reasoning capabilities of MLLMs through a novel method called Perception-R1, which focuses on improving multimodal perception capabilities.", "motivation": "The motivation is to address the limitation of existing RLVR methods in effectively enhancing the multimodal perception capabilities of MLLMs, which is crucial for complex multimodal reasoning.", "method": "Propose Perception-R1, a method that introduces a novel visual perception reward. It collects textual visual annotations from CoT trajectories as references and uses a judging LLM to assess consistency between these annotations and MLLM responses during RLVR training.", "result": "Extensive experiments show the effectiveness of Perception-R1, achieving state-of-the-art performance on most multimodal reasoning benchmarks with only 1,442 training data.", "conclusion": "Perception-R1 effectively incentivizes both multimodal perception and reasoning capabilities of MLLMs."}}
{"id": "2506.06347", "pdf": "https://arxiv.org/pdf/2506.06347", "abs": "https://arxiv.org/abs/2506.06347", "authors": ["Zachary Yang", "Domenico Tullo", "Reihaneh Rabbany"], "title": "Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection", "categories": ["cs.CL", "cs.AI", "I.2.7; J.4"], "comment": "11 pages, 1 figure, 9 Tables, KDD 2025 ADS Track", "summary": "Toxicity detection in gaming communities faces significant scaling challenges\nwhen expanding across multiple games and languages, particularly in real-time\nenvironments where computational efficiency is crucial. We present two key\nfindings to address these challenges while building upon our previous work on\nToxBuster, a BERT-based real-time toxicity detection system. First, we\nintroduce a soft-prompting approach that enables a single model to effectively\nhandle multiple games by incorporating game-context tokens, matching the\nperformance of more complex methods like curriculum learning while offering\nsuperior scalability. Second, we develop an LLM-assisted label transfer\nframework using GPT-4o-mini to extend support to seven additional languages.\nEvaluations on real game chat data across French, German, Portuguese, and\nRussian achieve macro F1-scores ranging from 32.96% to 58.88%, with\nparticularly strong performance in German, surpassing the English benchmark of\n45.39%. In production, this unified approach significantly reduces\ncomputational resources and maintenance overhead compared to maintaining\nseparate models for each game and language combination. At Ubisoft, this model\nsuccessfully identifies an average of 50 players, per game, per day engaging in\nsanctionable behavior.", "AI": {"tldr": "\u4e3a\u4e86\u5e94\u5bf9\u5728\u591a\u4e2a\u6e38\u620f\u548c\u8bed\u8a00\u4e2d\u6269\u5c55\u6bd2\u6027\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u63d0\u793a\u65b9\u6cd5\u548cLLM\u8f85\u52a9\u6807\u7b7e\u8f6c\u79fb\u6846\u67b6\u3002\u8f6f\u63d0\u793a\u65b9\u6cd5\u901a\u8fc7\u52a0\u5165\u6e38\u620f\u4e0a\u4e0b\u6587\u6807\u8bb0\uff0c\u4f7f\u5355\u4e00\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u591a\u4e2a\u6e38\u620f\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff1bLLM\u8f85\u52a9\u6807\u7b7e\u8f6c\u79fb\u6846\u67b6\u5219\u6269\u5c55\u4e86\u5bf9\u4e03\u79cd\u989d\u5916\u8bed\u8a00\u7684\u652f\u6301\u3002\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6e38\u620f\u804a\u5929\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u5fb7\u8bed\u4e2d\u8868\u73b0\u4f18\u4e8e\u82f1\u8bed\u57fa\u51c6\u3002\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u8fd9\u79cd\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u7ef4\u62a4\u5f00\u9500\u3002", "motivation": "\u6bd2\u6027\u7684\u68c0\u6d4b\u5728\u6e38\u620f\u793e\u533a\u9762\u4e34\u591a\u91cd\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u6269\u5c55\u5230\u591a\u6e38\u620f\u548c\u591a\u79cd\u8bed\u8a00\u65f6\u3002\u540c\u65f6\uff0c\u5728\u5b9e\u65f6\u73af\u5883\u4e2d\uff0c\u8ba1\u7b97\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u5177\u53ef\u6269\u5c55\u6027\u7684\u6bd2\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51fa\u8f6f\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a0\u5165\u6e38\u620f\u4e0a\u4e0b\u6587\u6807\u8bb0\uff0c\u4f7f\u5355\u4e2a\u6a21\u578b\u53ef\u4ee5\u5904\u7406\u591a\u4e2a\u6e38\u620f\uff0c\u5176\u6027\u80fd\u4e0e\u8bfe\u7a0b\u5b66\u4e60\u7b49\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\n2. \u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u8f85\u52a9\u6807\u7b7e\u8f6c\u79fb\u6846\u67b6\uff0c\u4f7f\u7528GPT-4o-mini\u6269\u5c55\u652f\u6301\u4e03\u79cd\u989d\u5916\u7684\u8bed\u8a00\u3002", "result": "\u5728\u771f\u5b9e\u6e38\u620f\u804a\u5929\u6570\u636e\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u6cd5\u8bed\u3001\u5fb7\u8bed\u3001\u8461\u8404\u7259\u8bed\u548c\u4fc4\u8bed\uff0c\u5b8fF1\u5206\u6570\u4ece32.96%\u523058.88%\u4e0d\u7b49\uff0c\u5176\u4e2d\u5fb7\u8bed\u7684\u8868\u73b0\u7279\u522b\u51fa\u8272\uff0c\u8d85\u8fc7\u4e86\u82f1\u8bed\u57fa\u51c6\uff0845.39%\uff09\u3002\u5728\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u8be5\u7edf\u4e00\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d44\u6e90\u548c\u7ef4\u62a4\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f6f\u63d0\u793a\u65b9\u6cd5\u548cLLM\u8f85\u52a9\u6807\u7b7e\u8f6c\u79fb\u6846\u67b6\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5728\u591a\u6e38\u620f\u548c\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u6269\u5c55\u6bd2\u6027\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2506.07229", "pdf": "https://arxiv.org/pdf/2506.07229", "abs": "https://arxiv.org/abs/2506.07229", "authors": ["Mateusz Gajewski", "Miko\u0142aj Morzy", "Adam Karczmarz", "Piotr Sankowski"], "title": "VARSHAP: Addressing Global Dependency Problems in Explainable AI with Variance-Based Local Feature Attribution", "categories": ["cs.LG"], "comment": null, "summary": "Existing feature attribution methods like SHAP often suffer from global\ndependence, failing to capture true local model behavior. This paper introduces\nVARSHAP, a novel model-agnostic local feature attribution method which uses the\nreduction of prediction variance as the key importance metric of features.\nBuilding upon Shapley value framework, VARSHAP satisfies the key Shapley\naxioms, but, unlike SHAP, is resilient to global data distribution shifts.\nExperiments on synthetic and real-world datasets demonstrate that VARSHAP\noutperforms popular methods such as KernelSHAP or LIME, both quantitatively and\nqualitatively.", "AI": {"tldr": "VARSHAP is a new model-agnostic local feature attribution method that uses prediction variance reduction as importance metric, showing better performance than SHAP and LIME.", "motivation": "Existing feature attribution methods such as SHAP have limitations in capturing true local model behavior due to global dependence.", "method": "VARSHAP employs Shapley value framework while using the reduction of prediction variance as the key importance metric of features, making it resilient to global data distribution shifts.", "result": "Experiments on synthetic and real-world datasets show VARSHAP outperforms KernelSHAP and LIME in both quantitative and qualitative aspects.", "conclusion": "VARSHAP provides a more effective way for local feature attribution compared to existing methods."}}
{"id": "2506.06351", "pdf": "https://arxiv.org/pdf/2506.06351", "abs": "https://arxiv.org/abs/2506.06351", "authors": ["Alexis Le Pichon", "Alice Janela Cameijo", "Samir Aknine", "Youcef Sklab", "Souhila Arib", "Quentin Brissaud", "Sven Peter Naesholm"], "title": "Deep learning methods for modeling infrasound transmission loss in the middle atmosphere", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "12 pages, 7 figures", "summary": "Accurate modeling of infrasound transmission losses (TLs) is essential to\nassess the performance of the global International Monitoring System infrasound\nnetwork. Among existing propagation modeling tools, parabolic equation (PE)\nmethod enables TLs to be finely modeled, but its computational cost does not\nallow exploration of a large parameter space for operational monitoring\napplications. To reduce computation times, Brissaud et al. 2023 explored the\npotential of convolutional neural networks trained on a large set of regionally\nsimulated wavefields (< 1000 km from the source) to predict TLs with negligible\ncomputation times compared to PE simulations. However, this method struggles in\nunfavorable initial wind conditions, especially at high frequencies, and causal\nissues with winds at large distances from the source affecting ground TLs close\nto the source. In this study, we have developed an optimized convolutional\nnetwork designed to minimize prediction errors while predicting TLs from\nglobally simulated combined temperature and wind fields spanning over\npropagation ranges of 4000 km. Our approach enhances the previously proposed\none by implementing key optimizations that improve the overall architecture\nperformance. The implemented model predicts TLs with an average error of 8.6 dB\nin the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric\nscenarios.", "AI": {"tldr": "This paper optimizes a convolutional neural network to predict infrasound transmission losses over global ranges with improved accuracy and reduced computation time compared to parabolic equation simulations.", "motivation": "To accurately model infrasound transmission losses globally for the International Monitoring System, overcoming limitations of existing methods like computational cost and prediction errors in unfavorable wind conditions.", "method": "Developed an optimized convolutional network that predicts transmission losses from globally simulated temperature and wind fields over 4000 km propagation ranges, implementing key architectural improvements.", "result": "The model predicts transmission losses with an average error of 8.6 dB across the frequency band (0.1-3.2 Hz) under realistic atmospheric scenarios.", "conclusion": "The optimized convolutional network enhances the prediction of infrasound transmission losses globally, providing a more efficient alternative to traditional parabolic equation methods."}}
{"id": "2506.07240", "pdf": "https://arxiv.org/pdf/2506.07240", "abs": "https://arxiv.org/abs/2506.07240", "authors": ["Roy Eisenstadt", "Itamar Zimerman", "Lior Wolf"], "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "comment": null, "summary": "Recently, techniques such as explicit structured reasoning have demonstrated\nstrong test-time scaling behavior by enforcing a separation between the model's\ninternal \"thinking\" process and the final response. A key factor influencing\nanswer quality in this setting is the length of the thinking stage. When the\nreasoning is too short, the model may fail to capture the complexity of the\ntask. Conversely, when it is too long, the model may overthink, leading to\nunnecessary computation and degraded performance. This paper explores and\nexploits the underlying mechanisms by which LLMs understand and regulate the\nlength of their reasoning during explicit thought processes. First, we show\nthat LLMs encode their progress through the reasoning process and introduce an\ninteractive progress bar visualization, which is then used to reveal insights\non the model's planning dynamics. Second, we manipulate the internal progress\nencoding during inference to reduce unnecessary steps and generate a more\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\nand reduces inference latency. Our code is publicly available.", "AI": {"tldr": "The paper explores how LLMs understand and control the length of their reasoning process, introduces an interactive progress bar for visualization, manipulates internal progress encoding to reduce unnecessary steps, and demonstrates that this method improves accuracy and reduces inference latency.", "motivation": "To improve the quality of answers by understanding and regulating the length of reasoning in LLMs during explicit thought processes.", "method": "1. Show that LLMs encode their progress through the reasoning process and introduce an interactive progress bar visualization. 2. Manipulate the internal progress encoding during inference to generate a more concise chain of thoughts.", "result": "Empirical results show that the 'overclocking' method mitigates overthinking, improves answer accuracy, and reduces inference latency.", "conclusion": "This method provides insights into the model's planning dynamics and offers a way to optimize the reasoning process in LLMs."}}
{"id": "2506.06353", "pdf": "https://arxiv.org/pdf/2506.06353", "abs": "https://arxiv.org/abs/2506.06353", "authors": ["Naseem Babu", "Jimson Mathew", "A. P. Vinod"], "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "The growing convergence between Large Language Models (LLMs) and\nelectroencephalography (EEG) research is enabling new directions in neural\ndecoding, brain-computer interfaces (BCIs), and affective computing. This\nsurvey offers a systematic review and structured taxonomy of recent\nadvancements that utilize LLMs for EEG-based analysis and applications. We\norganize the literature into four domains: (1) LLM-inspired foundation models\nfor EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal\ngeneration including image and 3D object synthesis, and (4) clinical\napplications and dataset management tools. The survey highlights how\ntransformer-based architectures adapted through fine-tuning, few-shot, and\nzero-shot learning have enabled EEG-based models to perform complex tasks such\nas natural language generation, semantic interpretation, and diagnostic\nassistance. By offering a structured overview of modeling strategies, system\ndesigns, and application areas, this work serves as a foundational resource for\nfuture work to bridge natural language processing and neural signal analysis\nthrough language models.", "AI": {"tldr": "The paper reviews the integration of Large Language Models (LLMs) with EEG research across four domains, showing how transformer-based models enable complex tasks in neural decoding, BCIs, and affective computing.", "motivation": "To provide a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications.", "method": "Organizing the literature into four domains: LLM-inspired foundation models for EEG representation learning, EEG-to-language decoding, cross-modal generation, and clinical applications/dataset management tools. Transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning are highlighted.", "result": "Transformer-based models have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance.", "conclusion": "This work serves as a foundational resource for future research to bridge natural language processing and neural signal analysis through language models."}}
{"id": "2506.07247", "pdf": "https://arxiv.org/pdf/2506.07247", "abs": "https://arxiv.org/abs/2506.07247", "authors": ["Ngoc-Quan Pham", "Tuan Truong", "Quyen Tran", "Tan Nguyen", "Dinh Phung", "Trung Le"], "title": "Promoting Ensemble Diversity with Interactive Bayesian Distributional Robustness for Fine-tuning Foundation Models", "categories": ["cs.LG"], "comment": "ICML 2025 (Poster)", "summary": "We introduce Interactive Bayesian Distributional Robustness (IBDR), a novel\nBayesian inference framework that allows modeling the interactions between\nparticles, thereby enhancing ensemble quality through increased particle\ndiversity. IBDR is grounded in a generalized theoretical framework that\nconnects the distributional population loss with the approximate posterior,\nmotivating a practical dual optimization procedure that enforces distributional\nrobustness while fostering particle diversity. We evaluate IBDR's performance\nagainst various baseline methods using the VTAB-1K benchmark and the common\nreasoning language task. The results consistently show that IBDR outperforms\nthese baselines, underscoring its effectiveness in real-world applications.", "AI": {"tldr": "The paper presents Interactive Bayesian Distributional Robustness (IBDR), a new Bayesian inference framework that models particle interactions to improve ensemble quality via increased particle diversity. It is based on a generalized theoretical framework connecting distributional population loss with the approximate posterior, leading to a dual optimization procedure enforcing robustness and diversity. Evaluated using VTAB-1K and common reasoning language tasks, IBDR outperforms various baselines.", "motivation": "The motivation of this paper is to enhance the ensemble quality in Bayesian inference by modeling the interactions between particles, which can increase particle diversity.", "method": "Interactive Bayesian Distributional Robustness (IBDR) is a novel Bayesian inference framework that models particle interactions to enhance ensemble quality through increased particle diversity. It is grounded in a generalized theoretical framework that connects the distributional population loss with the approximate posterior, motivating a practical dual optimization procedure that enforces distributional robustness while fostering particle diversity.", "result": "IBDR's performance was evaluated against various baseline methods using the VTAB-1K benchmark and the common reasoning language task. The results consistently show that IBDR outperforms these baselines.", "conclusion": "IBDR is effective in real-world applications as it consistently outperforms various baselines when evaluated using different benchmarks and tasks."}}
{"id": "2506.06358", "pdf": "https://arxiv.org/pdf/2506.06358", "abs": "https://arxiv.org/abs/2506.06358", "authors": ["Alice Janela Cameijo", "Alexis Le Pichon", "Youcef Sklab", "Souhila Arib", "Quentin Brissaud", "Sven peter Naesholm", "Constantino Listowski", "Samir Aknine"], "title": "Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation", "categories": ["eess.SP", "cs.AI"], "comment": "49 pages, 22 figures", "summary": "Accurate modeling of infrasound transmission loss is essential for evaluating\nthe performance of the International Monitoring System, enabling the effective\ndesign and maintenance of infrasound stations to support compliance of the\nComprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling\ntools enable transmission loss to be finely simulated using atmospheric models.\nHowever, the computational cost prohibits the exploration of a large parameter\nspace in operational monitoring applications. To address this, recent studies\nmade use of a deep learning algorithm capable of making transmission loss\npredictions almost instantaneously. However, the use of nudged atmospheric\nmodels leads to an incomplete representation of the medium, and the absence of\ntemperature as an input makes the algorithm incompatible with long range\npropagation. In this study, we address these limitations by using both wind and\ntemperature fields as inputs to a neural network, simulated up to 130 km\naltitude and 4,000 km distance. We also optimize several aspects of the neural\nnetwork architecture. We exploit convolutional and recurrent layers to capture\nspatially and range-dependent features embedded in realistic atmospheric\nmodels, improving the overall performance. The neural network reaches an\naverage error of 4 dB compared to full parabolic equation simulations and\nprovides epistemic and data-related uncertainty estimates. Its evaluation on\nthe 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its\nprediction capability using atmospheric conditions and frequencies not included\nin the training. This represents a significant step towards near real-time\nassessment of International Monitoring System detection thresholds of explosive\nsources.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\uff0c\u4f7f\u7528\u98ce\u901f\u548c\u6e29\u5ea6\u573a\u4f5c\u4e3a\u8f93\u5165\uff0c\u5b9e\u73b0\u4e86\u5bf9\u8fdc\u8ddd\u79bb\u58f0\u6ce2\u4f20\u8f93\u635f\u5931\u7684\u5feb\u901f\u9884\u6d4b\uff0c\u5e76\u57282022\u5e74\u6d2a\u52a0\u706b\u5c71\u55b7\u53d1\u4e8b\u4ef6\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7531\u4e8e\u4ec5\u4f7f\u7528\u8c03\u6574\u8fc7\u7684\u5927\u6c14\u6a21\u578b\u4e14\u7f3a\u4e4f\u6e29\u5ea6\u8f93\u5165\uff0c\u65e0\u6cd5\u51c6\u786e\u6a21\u62df\u957f\u8ddd\u79bb\u58f0\u6ce2\u4f20\u64ad\u635f\u5931\u3002\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6a21\u578b\u4ee5\u5305\u542b\u6e29\u5ea6\u548c\u98ce\u901f\u6570\u636e\uff0c\u63d0\u5347\u5bf9\u5927\u6c14\u6761\u4ef6\u7684\u5168\u9762\u8868\u793a\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u5c06\u98ce\u901f\u4e0e\u6e29\u5ea6\u573a\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\uff0c\u8fdb\u884c\u9ad8\u8fbe130\u516c\u91cc\u9ad8\u5ea6\u30014000\u516c\u91cc\u8ddd\u79bb\u7684\u6a21\u62df\u3002\u540c\u65f6\u4f18\u5316\u4e86\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5229\u7528\u5377\u79ef\u5c42\u548c\u5faa\u73af\u5c42\u6355\u6349\u7a7a\u95f4\u548c\u8303\u56f4\u4f9d\u8d56\u7279\u5f81\u3002", "result": "\u8be5\u795e\u7ecf\u7f51\u7edc\u76f8\u5bf9\u4e8e\u5b8c\u6574\u629b\u7269\u7ebf\u65b9\u7a0b\u6a21\u62df\u7684\u5e73\u5747\u8bef\u5dee\u4e3a4 dB\uff0c\u80fd\u591f\u63d0\u4f9b\u8ba4\u77e5\u548c\u6570\u636e\u76f8\u5173\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e2022\u5e74\u6d2a\u52a0\u706b\u5c71\u55b7\u53d1\u4e8b\u4ef6\u7684\u9884\u6d4b\u3002", "conclusion": "\u6b64\u7814\u7a76\u662f\u8fc8\u5411\u7206\u70b8\u6e90\u56fd\u9645\u76d1\u6d4b\u7cfb\u7edf\u5b9e\u65f6\u68c0\u6d4b\u9608\u503c\u8bc4\u4f30\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u63d0\u5347\u4e86\u5bf9\u58f0\u6ce2\u4f20\u64ad\u635f\u5931\u7684\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.07254", "pdf": "https://arxiv.org/pdf/2506.07254", "abs": "https://arxiv.org/abs/2506.07254", "authors": ["Kevin Frans", "Sergey Levine", "Pieter Abbeel"], "title": "A Stable Whitening Optimizer for Efficient Neural Network Training", "categories": ["cs.LG"], "comment": null, "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44% of the gradient steps and 62% of the wallclock\ntime.", "AI": {"tldr": "This paper proposes SPlus, an improved method for neural network optimization based on the Shampoo family of algorithms. It addresses three key issues: divergence caused by caching matrix-inverses, learning rate transfer across network width, and parameter noise from high learning rates. Experiments show that SPlus can achieve Adam's validation performance with fewer gradient steps and less wallclock time.", "motivation": "The motivation is to improve the efficiency and stability of neural network optimization by addressing the limitations of the existing Shampoo algorithm.", "method": "SPlus introduces a bounded update combining historical eigenbasis with instantaneous normalization to prevent divergence, adapts shape-aware scaling for learning rate transfer, and proposes an iterate-averaging scheme to handle parameter noise.", "result": "SPlus reaches Adam's validation performance within 44% of the gradient steps and 62% of the wallclock time on Transformer training tasks including language modelling, image classification, and diffusion modelling.", "conclusion": "SPlus is a more efficient and stable optimization method compared to Adam and naive Shampoo."}}
{"id": "2506.07272", "pdf": "https://arxiv.org/pdf/2506.07272", "abs": "https://arxiv.org/abs/2506.07272", "authors": ["Alex Clinton", "Thomas Zeng", "Yiding Chen", "Xiaojin Zhu", "Kirthevasan Kandasamy"], "title": "A Cram\u00e9r-von Mises Approach to Incentivizing Truthful Data Sharing", "categories": ["cs.LG"], "comment": null, "summary": "Modern data marketplaces and data sharing consortia increasingly rely on\nincentive mechanisms to encourage agents to contribute data. However, schemes\nthat reward agents based on the quantity of submitted data are vulnerable to\nmanipulation, as agents may submit fabricated or low-quality data to inflate\ntheir rewards. Prior work has proposed comparing each agent's data against\nothers' to promote honesty: when others contribute genuine data, the best way\nto minimize discrepancy is to do the same. Yet prior implementations of this\nidea rely on very strong assumptions about the data distribution (e.g.\nGaussian), limiting their applicability. In this work, we develop reward\nmechanisms based on a novel, two-sample test inspired by the Cram\\'er-von Mises\nstatistic. Our methods strictly incentivize agents to submit more genuine data,\nwhile disincentivizing data fabrication and other types of untruthful\nreporting. We establish that truthful reporting constitutes a (possibly\napproximate) Nash equilibrium in both Bayesian and prior-agnostic settings. We\ntheoretically instantiate our method in three canonical data sharing problems\nand show that it relaxes key assumptions made by prior work. Empirically, we\ndemonstrate that our mechanism incentivizes truthful data sharing via\nsimulations and on real-world language and image data.", "AI": {"tldr": "\u5728\u73b0\u4ee3\u6570\u636e\u5e02\u573a\u4e2d\uff0c\u5956\u52b1\u673a\u5236\u5e38\u88ab\u7528\u4e8e\u6fc0\u52b1\u53c2\u4e0e\u8005\u8d21\u732e\u6570\u636e\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u6570\u636e\u91cf\u7684\u5956\u52b1\u65b9\u6848\u5bb9\u6613\u88ab\u64cd\u63a7\uff0c\u4f8b\u5982\u63d0\u4ea4\u4f2a\u9020\u6216\u4f4e\u8d28\u91cf\u7684\u6570\u636e\u4ee5\u83b7\u53d6\u66f4\u591a\u5956\u52b1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCram\u00e9r-von Mises\u7edf\u8ba1\u91cf\u7684\u4e24\u6837\u672c\u6d4b\u8bd5\u7684\u5956\u52b1\u673a\u5236\uff0c\u8be5\u673a\u5236\u53ef\u4ee5\u4e25\u683c\u6fc0\u52b1\u53c2\u4e0e\u8005\u63d0\u4ea4\u66f4\u771f\u5b9e\u7684\u6570\u636e\uff0c\u5e76\u4e14\u51cf\u5c11\u6570\u636e\u4f2a\u9020\u548c\u4e0d\u8bda\u5b9e\u62a5\u544a\u7684\u884c\u4e3a\u3002\u7814\u7a76\u8bc1\u660e\uff0c\u5728\u8d1d\u53f6\u65af\u548c\u5148\u9a8c\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u771f\u5b9e\u62a5\u544a\u6784\u6210\u4e86\uff08\u53ef\u80fd\u8fd1\u4f3c\u7684\uff09\u7eb3\u4ec0\u5747\u8861\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9645\u6570\u636e\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u672c\u6587\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5178\u578b\u6570\u636e\u5171\u4eab\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u653e\u677e\u4e86\u5148\u524d\u5de5\u4f5c\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5f3a\u5047\u8bbe\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u5e02\u573a\u5956\u52b1\u673a\u5236\u5bb9\u6613\u53d7\u5230\u64cd\u7eb5\uff0c\u56e0\u4e3a\u53c2\u4e0e\u8005\u53ef\u80fd\u4f1a\u63d0\u4ea4\u4f2a\u9020\u6216\u4f4e\u8d28\u91cf\u7684\u6570\u636e\u6765\u589e\u52a0\u81ea\u5df1\u7684\u6536\u76ca\u3002\u4e3a\u4e86\u4fc3\u8fdb\u6570\u636e\u7684\u771f\u5b9e\u6027\u548c\u9ad8\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u5956\u52b1\u673a\u5236\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4e8e\u5f3a\u6570\u636e\u5206\u5e03\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u6fc0\u52b1\u53c2\u4e0e\u8005\u63d0\u4ea4\u771f\u5b9e\u6570\u636e\u5e76\u51cf\u5c11\u4e0d\u8bda\u5b9e\u884c\u4e3a\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eCram\u00e9r-von Mises\u7edf\u8ba1\u91cf\u7684\u4e24\u6837\u672c\u6d4b\u8bd5\u7684\u5956\u52b1\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6bd4\u8f83\u6bcf\u4e2a\u53c2\u4e0e\u8005\u7684\u6570\u636e\u4e0e\u5176\u4ed6\u4eba\u7684\u6570\u636e\uff0c\u4e25\u683c\u6fc0\u52b1\u53c2\u4e0e\u8005\u63d0\u4ea4\u66f4\u771f\u5b9e\u7684\u6570\u636e\uff0c\u5e76\u60e9\u7f5a\u6570\u636e\u4f2a\u9020\u548c\u5176\u4ed6\u7c7b\u578b\u7684\u4e0d\u8bda\u5b9e\u62a5\u544a\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8bc1\u660e\u4e86\u5728\u8d1d\u53f6\u65af\u548c\u5148\u9a8c\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\uff0c\u771f\u5b9e\u62a5\u544a\u6784\u6210\uff08\u53ef\u80fd\u8fd1\u4f3c\u7684\uff09\u7eb3\u4ec0\u5747\u8861\u3002", "result": "\u7406\u8bba\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u5178\u578b\u7684\u6570\u636e\u5171\u4eab\u95ee\u9898\u4e2d\u653e\u677e\u4e86\u5148\u524d\u5de5\u4f5c\u5bf9\u6570\u636e\u5206\u5e03\u7684\u5f3a\u5047\u8bbe\u3002\u5b9e\u8bc1\u4e0a\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8bed\u8a00\u53ca\u56fe\u50cf\u6570\u636e\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u8be5\u673a\u5236\u80fd\u591f\u6709\u6548\u6fc0\u52b1\u53c2\u4e0e\u8005\u8fdb\u884c\u771f\u5b9e\u7684\u6570\u636e\u5171\u4eab\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8eCram\u00e9r-von Mises\u7edf\u8ba1\u91cf\u7684\u4e24\u6837\u672c\u6d4b\u8bd5\u7684\u5956\u52b1\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5730\u6fc0\u52b1\u53c2\u4e0e\u8005\u63d0\u4ea4\u771f\u5b9e\u6570\u636e\uff0c\u51cf\u5c11\u6570\u636e\u4f2a\u9020\u548c\u4e0d\u8bda\u5b9e\u62a5\u544a\u7684\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6570\u636e\u5171\u4eab\u573a\u666f\uff0c\u5e76\u4e14\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2506.06361", "pdf": "https://arxiv.org/pdf/2506.06361", "abs": "https://arxiv.org/abs/2506.06361", "authors": ["Tim Schneider", "Guillaume Duret", "Cristiana de Farias", "Roberto Calandra", "Liming Chen", "Jan Peters"], "title": "Tactile MNIST: Benchmarking Active Tactile Perception", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Tactile perception has the potential to significantly enhance dexterous\nrobotic manipulation by providing rich local information that can complement or\nsubstitute for other sensory modalities such as vision. However, because\ntactile sensing is inherently local, it is not well-suited for tasks that\nrequire broad spatial awareness or global scene understanding on its own. A\nhuman-inspired strategy to address this issue is to consider active perception\ntechniques instead. That is, to actively guide sensors toward regions with more\ninformative or significant features and integrate such information over time in\norder to understand a scene or complete a task. Both active perception and\ndifferent methods for tactile sensing have received significant attention\nrecently. Yet, despite advancements, both fields lack standardized benchmarks.\nTo bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an\nopen-source, Gymnasium-compatible benchmark specifically designed for active\ntactile perception tasks, including localization, classification, and volume\nestimation. Our benchmark suite offers diverse simulation scenarios, from\nsimple toy environments all the way to complex tactile perception tasks using\nvision-based tactile sensors. Furthermore, we also offer a comprehensive\ndataset comprising 13,500 synthetic 3D MNIST digit models and 153,600\nreal-world tactile samples collected from 600 3D printed digits. Using this\ndataset, we train a CycleGAN for realistic tactile simulation rendering. By\nproviding standardized protocols and reproducible evaluation frameworks, our\nbenchmark suite facilitates systematic progress in the fields of tactile\nsensing and active perception.", "AI": {"tldr": "The paper introduces the Tactile MNIST Benchmark Suite, an open-source benchmark for active tactile perception tasks such as localization, classification, and volume estimation. It includes diverse simulation scenarios and a comprehensive dataset with synthetic 3D MNIST digit models and real-world tactile samples. A CycleGAN is trained for realistic tactile simulation rendering to facilitate progress in tactile sensing and active perception.", "motivation": "Tactile perception can enhance robotic manipulation but lacks broad spatial awareness alone. Active perception techniques can guide sensors towards informative regions over time, improving scene understanding or task completion. However, both tactile sensing and active perception lack standardized benchmarks.", "method": "Introduced the Tactile MNIST Benchmark Suite which is Gymnasium-compatible and designed for active tactile perception tasks including localization, classification, and volume estimation. The suite offers various simulation scenarios and includes a large dataset of synthetic 3D MNIST digit models and real-world tactile samples. A CycleGAN was also trained for realistic tactile simulation rendering.", "result": "The Tactile MNIST Benchmark Suite provides standardized protocols and reproducible evaluation frameworks, promoting systematic progress in tactile sensing and active perception.", "conclusion": "This benchmark suite addresses the need for standardized benchmarks in tactile sensing and active perception, offering tools and resources that advance research in these fields."}}
{"id": "2506.07275", "pdf": "https://arxiv.org/pdf/2506.07275", "abs": "https://arxiv.org/abs/2506.07275", "authors": ["Haochen Song", "Dominik Hofer", "Rania Islambouli", "Laura Hawkins", "Ananya Bhattacharjee", "Meredith Franklin", "Joseph Jay Williams"], "title": "Investigating the Relationship Between Physical Activity and Tailored Behavior Change Messaging: Connecting Contextual Bandit with Large Language Models", "categories": ["cs.LG", "cs.HC", "stat.AP"], "comment": null, "summary": "Machine learning approaches, such as contextual multi-armed bandit (cMAB)\nalgorithms, offer a promising strategy to reduce sedentary behavior by\ndelivering personalized interventions to encourage physical activity. However,\ncMAB algorithms typically require large participant samples to learn\neffectively and may overlook key psychological factors that are not explicitly\nencoded in the model. In this study, we propose a hybrid approach that combines\ncMAB for selecting intervention types with large language models (LLMs) to\npersonalize message content. We evaluate four intervention types: behavioral\nself-monitoring, gain-framed, loss-framed, and social comparison, each\ndelivered as a motivational message aimed at increasing motivation for physical\nactivity and daily step count. Message content is further personalized using\ndynamic contextual factors including daily fluctuations in self-efficacy,\nsocial influence, and regulatory focus. Over a seven-day trial, participants\nreceive daily messages assigned by one of four models: cMAB alone, LLM alone,\ncombined cMAB with LLM personalization (cMABxLLM), or equal randomization\n(RCT). Outcomes include daily step count and message acceptance, assessed via\necological momentary assessments (EMAs). We apply a causal inference framework\nto evaluate the effects of each model. Our findings offer new insights into the\ncomplementary roles of LLM-based personalization and cMAB adaptation in\npromoting physical activity through personalized behavioral messaging.", "AI": {"tldr": "This paper proposes a hybrid approach combining contextual multi-armed bandit (cMAB) algorithms with large language models (LLMs) for personalized interventions to reduce sedentary behavior and increase physical activity.", "motivation": "To address the limitations of cMAB algorithms that require large participant samples and may overlook key psychological factors, by integrating LLMs for personalizing message content in interventions.", "method": "A seven-day trial where participants receive daily messages assigned by one of four models: cMAB alone, LLM alone, combined cMAB with LLM personalization (cMABxLLM), or equal randomization (RCT). Four intervention types are evaluated: behavioral self-monitoring, gain-framed, loss-framed, and social comparison. Outcomes include daily step count and message acceptance assessed via ecological momentary assessments (EMAs).", "result": "The causal inference framework is applied to evaluate the effects of each model on promoting physical activity and increasing message acceptance.", "conclusion": "The findings provide insights into how LLM-based personalization and cMAB adaptation can complement each other in promoting physical activity through personalized behavioral messaging."}}
{"id": "2506.06362", "pdf": "https://arxiv.org/pdf/2506.06362", "abs": "https://arxiv.org/abs/2506.06362", "authors": ["Dejun Xu", "Jijia Chen", "Gary G. Yen", "Min Jiang"], "title": "CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Bilevel optimization poses a significant computational challenge due to its\nnested structure, where each upper-level candidate solution requires solving a\ncorresponding lower-level problem. While evolutionary algorithms (EAs) are\neffective at navigating such complex landscapes, their high resource demands\nremain a key bottleneck -- particularly the redundant evaluation of numerous\nunpromising lower-level tasks. Despite recent advances in multitasking and\ntransfer learning, resource waste persists. To address this issue, we propose a\nnovel resource allocation framework for bilevel EAs that selectively identifies\nand focuses on promising lower-level tasks. Central to our approach is a\ncontrastive ranking network that learns relational patterns between paired\nupper- and lower-level solutions online. This knowledge guides a\nreference-based ranking strategy that prioritizes tasks for optimization and\nadaptively controls resampling based on estimated population quality.\nComprehensive experiments across five state-of-the-art bilevel algorithms show\nthat our framework significantly reduces computational cost while preserving --\nor even enhancing -- solution accuracy. This work offers a generalizable\nstrategy to improve the efficiency of bilevel EAs, paving the way for more\nscalable bilevel optimization.", "AI": {"tldr": "Bilevel optimization is challenging due to its nested structure. Evolutionary algorithms (EAs) are effective but resource-intensive. This paper proposes a novel framework that uses a contrastive ranking network to prioritize promising lower-level tasks, reducing computational cost while maintaining or improving solution accuracy.", "motivation": "Bilevel optimization presents challenges due to its nested structure, requiring each upper-level solution to solve a corresponding lower-level problem. Current methods, including multitasking and transfer learning, still lead to significant resource waste.", "method": "The paper introduces a new resource allocation framework for bilevel EAs. It includes a contrastive ranking network that learns relationships between paired upper- and lower-level solutions online. This network guides a reference-based ranking strategy that prioritizes tasks for optimization and adaptively controls resampling based on population quality estimates.", "result": "Experiments across five state-of-the-art bilevel algorithms demonstrate that the proposed framework significantly reduces computational costs while preserving or enhancing solution accuracy.", "conclusion": "The proposed framework offers a generalizable strategy to improve the efficiency of bilevel EAs, making bilevel optimization more scalable."}}
{"id": "2506.07276", "pdf": "https://arxiv.org/pdf/2506.07276", "abs": "https://arxiv.org/abs/2506.07276", "authors": ["Suho Shin", "Chenghao Yang", "Haifeng Xu", "Mohammad T. Hajiaghayi"], "title": "Tokenized Bandit for LLM Decoding and Alignment", "categories": ["cs.LG", "cs.AI"], "comment": "To appear at ICML 2025", "summary": "We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),\nvariants of linear and stochastic multi-armed bandit problems inspired by LLM\ndecoding and alignment. In these problems, at each round $t \\in [T]$, a user\nsubmits a query (context), and the decision maker (DM) sequentially selects a\ntoken irrevocably from a token set. Once the sequence is complete, the DM\nobserves a random utility from the user, whose expectation is presented by a\nsequence function mapping the chosen token sequence to a nonnegative real value\nthat depends on the query.\n  In both problems, we first show that learning is impossible without any\nstructure on the sequence function. We introduce a natural assumption,\ndiminishing distance with more commons (DDMC), and propose algorithms with\nregret $\\tilde{O}(L\\sqrt{T})$ and $\\tilde{O}(L\\sqrt{T^{2/3}})$ for TLB and\nTMAB, respectively. As a side product, we obtain an (almost) optimality of the\ngreedy decoding for LLM decoding algorithm under DDMC, which justifies the\nunresaonable effectiveness of greedy decoding in several tasks. This also has\nan immediate application to decoding-time LLM alignment, when the misaligned\nutility can be represented as the frozen LLM's utility and a linearly\nrealizable latent function. We finally validate our algorithm's performance\nempirically as well as verify our assumptions using synthetic and real-world\ndatasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86tokenized linear bandit (TLB) \u548c multi-armed bandit (TMAB) \u95ee\u9898\uff0c\u5e76\u5728\u7279\u5b9a\u5047\u8bbe\u4e0b\u63d0\u51fa\u4e86\u7b97\u6cd5\u548c\u7406\u8bba\u7ed3\u679c\u3002", "motivation": "\u53d7LLM\u89e3\u7801\u548c\u5bf9\u9f50\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u4eec\u5f15\u5165\u4e86tokenized linear bandit (TLB) \u548c multi-armed bandit (TMAB) \u53d8\u4f53\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5173\u6ce8\u4e8e\u5728\u6bcf\u8f6e\u4e2d\u4ece\u4e00\u4e2atoken\u96c6\u5408\u4e2d\u4e0d\u53ef\u9006\u5730\u9009\u62e9token\u5e8f\u5217\uff0c\u5e76\u6839\u636e\u7528\u6237\u7684\u67e5\u8be2\u89c2\u5bdf\u5230\u968f\u673a\u6548\u7528\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5c55\u793a\u4e86\u5728\u6ca1\u6709\u4efb\u4f55\u5e8f\u5217\u51fd\u6570\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u5b66\u4e60\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u7136\u540e\u5f15\u5165\u4e86\u4e00\u4e2a\u81ea\u7136\u5047\u8bbe\u2014\u2014\u968f\u7740\u66f4\u591a\u5171\u540c\u9879\u8ddd\u79bb\u9012\u51cf(DDMC)\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff0c\u5206\u522b\u5177\u6709$\\tilde{O}(L\\sqrt{T})$\u548c$\\tilde{O}(L\\sqrt{T^{2/3}})$\u7684\u9057\u61be\u754c\u3002\u6b64\u5916\uff0c\u8fd8\u5f97\u5230\u4e86\u8d2a\u5a6a\u89e3\u7801\u5728LLM\u89e3\u7801\u4efb\u52a1\u4e2d\u7684\uff08\u51e0\u4e4e\uff09\u6700\u4f18\u6027\u7ed3\u679c\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5728DDMC\u5047\u8bbe\u4e0b\u63d0\u51fa\u7684\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8d2a\u5a6a\u89e3\u7801\u5728\u67d0\u4e9b\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u6027\u3002\u5b9e\u9a8c\u4e0a\u4f7f\u7528\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6027\u80fd\u53ca\u5047\u8bbe\u7684\u5408\u7406\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u5f15\u5165\u65b0\u7684bandit\u95ee\u9898\u53d8\u4f53\u548c\u76f8\u5173\u5047\u8bbe\uff0c\u4e0d\u4ec5\u4e3a\u89e3\u51b3\u6b64\u7c7b\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7b97\u6cd5\uff0c\u8fd8\u89e3\u91ca\u4e86\u8d2a\u5a6a\u89e3\u7801\u5728LLM\u4efb\u52a1\u4e2d\u7684\u6210\u529f\uff0c\u540c\u65f6\u4e3a\u89e3\u7801\u65f6\u95f4LLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u76f4\u63a5\u5e94\u7528\u3002"}}
{"id": "2506.06376", "pdf": "https://arxiv.org/pdf/2506.06376", "abs": "https://arxiv.org/abs/2506.06376", "authors": ["Heng Dong", "Kefei Duan", "Chongjie Zhang"], "title": "Enhancing Decision-Making of Large Language Models via Actor-Critic", "categories": ["cs.CL", "cs.AI"], "comment": "Forty-second International Conference on Machine Learning (ICML 2025)", "summary": "Large Language Models (LLMs) have achieved remarkable advancements in natural\nlanguage processing tasks, yet they encounter challenges in complex\ndecision-making scenarios that require long-term reasoning and alignment with\nhigh-level objectives. Existing methods either rely on short-term\nauto-regressive action generation or face limitations in accurately simulating\nrollouts and assessing outcomes, leading to sub-optimal decisions. This paper\nintroduces a novel LLM-based Actor-Critic framework, termed LAC, that\neffectively improves LLM policies with long-term action evaluations in a\nprincipled and scalable way. Our approach addresses two key challenges: (1)\nextracting robust action evaluations by computing Q-values via token logits\nassociated with positive/negative outcomes, enhanced by future trajectory\nrollouts and reasoning; and (2) enabling efficient policy improvement through a\ngradient-free mechanism. Experiments across diverse environments -- including\nhigh-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),\nand large action spaces (WebShop) -- demonstrate the framework's generality and\nsuperiority over state-of-the-art methods. Notably, our approach achieves\ncompetitive performance using 7B/8B parameter LLMs, even outperforming baseline\nmethods employing GPT-4 in complex tasks. These results underscore the\npotential of integrating structured policy optimization with LLMs' intrinsic\nknowledge to advance decision-making capabilities in multi-step environments.", "AI": {"tldr": "This paper presents LAC, a new LLM-based Actor-Critic framework that improves decision-making by long-term reasoning and action evaluations.", "motivation": "Despite the success of LLMs in natural language processing tasks, they struggle with complex decision-making requiring long-term reasoning and alignment with high-level objectives. Current methods either rely on short-term auto-regressive action generation or have limitations in accurately simulating rollouts and assessing outcomes.", "method": "The paper introduces LAC, which addresses two challenges: extracting robust action evaluations by computing Q-values through token logits associated with outcomes, enhanced by future trajectory rollouts and reasoning; and enabling efficient policy improvement via a gradient-free mechanism.", "result": "Experiments in various environments (ALFWorld, BabyAI-Text, WebShop) show the framework's generality and superiority over state-of-the-art methods. Remarkably, it achieves competitive performance using 7B/8B parameter LLMs, even surpassing GPT-4 in some complex tasks.", "conclusion": "The integration of structured policy optimization with LLMs' intrinsic knowledge shows potential to enhance decision-making capabilities in multi-step environments."}}
{"id": "2506.07288", "pdf": "https://arxiv.org/pdf/2506.07288", "abs": "https://arxiv.org/abs/2506.07288", "authors": ["Weijie Guan", "Haohui Wang", "Jian Kang", "Lihui Liu", "Dawei Zhou"], "title": "EviNet: Evidential Reasoning Network for Resilient Graph Learning in the Open and Noisy Environments", "categories": ["cs.LG"], "comment": "KDD 2025", "summary": "Graph learning has been crucial to many real-world tasks, but they are often\nstudied with a closed-world assumption, with all possible labels of data known\na priori. To enable effective graph learning in an open and noisy environment,\nit is critical to inform the model users when the model makes a wrong\nprediction to in-distribution data of a known class, i.e., misclassification\ndetection or when the model encounters out-of-distribution from novel classes,\ni.e., out-of-distribution detection. This paper introduces Evidential Reasoning\nNetwork (EVINET), a framework that addresses these two challenges by\nintegrating Beta embedding within a subjective logic framework. EVINET includes\ntwo key modules: Dissonance Reasoning for misclassification detection and\nVacuity Reasoning for out-of-distribution detection. Extensive experiments\ndemonstrate that EVINET outperforms state-of-the-art methods across multiple\nmetrics in the tasks of in-distribution classification, misclassification\ndetection, and out-of-distribution detection. EVINET demonstrates the necessity\nof uncertainty estimation and logical reasoning for misclassification detection\nand out-of-distribution detection and paves the way for open-world graph\nlearning. Our code and data are available at https://github.com/SSSKJ/EviNET.", "AI": {"tldr": "Evidential Reasoning Network (EVINET) is a novel framework integrating Beta embedding within a subjective logic framework to address misclassification and out-of-distribution detection in graph learning. It consists of two modules, Dissonance Reasoning for misclassification detection and Vacuity Reasoning for out-of-distribution detection, demonstrating superior performance over state-of-the-art methods in multiple tasks.", "motivation": "Graph learning models often operate under the closed-world assumption where all possible labels are known beforehand. However, real-world applications require models that can effectively handle open and noisy environments, including detecting misclassifications and out-of-distribution instances.", "method": "The paper proposes EVINET, which integrates Beta embedding within a subjective logic framework. It includes two key modules: Dissonance Reasoning for detecting misclassifications and Vacuity Reasoning for detecting out-of-distribution samples.", "result": "Experiments show that EVINET surpasses state-of-the-art methods across various metrics in tasks such as in-distribution classification, misclassification detection, and out-of-distribution detection.", "conclusion": "EVINET highlights the importance of uncertainty estimation and logical reasoning for robust detection capabilities in graph learning, advancing the field towards open-world scenarios."}}
{"id": "2506.07298", "pdf": "https://arxiv.org/pdf/2506.07298", "abs": "https://arxiv.org/abs/2506.07298", "authors": ["Yijia Dai", "Zhaolin Gao", "Yahya Satter", "Sarah Dean", "Jennifer J. Sun"], "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.", "AI": {"tldr": "\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5229\u7528\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\uff0c\u53ef\u4ee5\u6709\u6548\u5bf9\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMMs\uff09\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff0c\u8fbe\u5230\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u52a8\u7269\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u6a21\u578b\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1HMM\u662f\u7528\u4e8e\u5efa\u6a21\u5177\u6709\u6f5c\u5728\u9a6c\u5c14\u53ef\u592b\u7ed3\u6784\u7684\u5e8f\u5217\u6570\u636e\u7684\u57fa\u7840\u5de5\u5177\uff0c\u4f46\u5c06\u5176\u62df\u5408\u5230\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4ecd\u7136\u8ba1\u7b97\u4e0a\u5177\u6709\u6311\u6218\u6027\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u63a2\u7d22\u4e86\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6765\u5bf9HMM\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u5efa\u6a21\u7684\u53ef\u80fd\u6027\u3002", "method": "\u7814\u7a76\u5c55\u793a\u4e86\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5982\u4f55\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u5bf9\u9690\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff08HMMs\uff09\u751f\u6210\u7684\u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5206\u6790\u4e86LLMs\u5728\u5404\u79cd\u5408\u6210HMM\u4e0a\u7684\u8868\u73b0\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u5f71\u54cd\u6a21\u578b\u8868\u73b0\u7684HMM\u7279\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u4f7f\u7528ICL\u4f5c\u4e3a\u590d\u6742\u6570\u636e\u8bca\u65ad\u5de5\u5177\u7684\u5b9e\u7528\u6307\u5357\u3002", "result": "LLMs\u5728\u591a\u79cd\u5408\u6210HMM\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd1\u7406\u8bba\u6700\u4f18\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u6269\u5c55\u8d8b\u52bf\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u7ecf\u9a8c\u89c2\u5bdf\u63d0\u4f9b\u4e86\u7406\u8bba\u731c\u60f3\u3002\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u52a8\u7269\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0cICL\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u8bbe\u8ba1\u7684\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u8bc1\u660eICL\u80fd\u591f\u5b66\u4e60\u548c\u9884\u6d4bHMM\u751f\u6210\u7684\u5e8f\u5217\uff0c\u52a0\u6df1\u4e86\u6211\u4eec\u5bf9\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u7406\u89e3\uff0c\u5e76\u786e\u7acb\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u63ed\u793a\u590d\u6742\u79d1\u5b66\u6570\u636e\u4e2d\u7684\u9690\u85cf\u7ed3\u6784\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.06381", "pdf": "https://arxiv.org/pdf/2506.06381", "abs": "https://arxiv.org/abs/2506.06381", "authors": ["Trisanth Srinivasan", "Santosh Patapati", "Himani Musku", "Idhant Gode", "Aditya Arora", "Samvit Bhattacharya", "Abubakr Nazriev", "Sanika Hirave", "Zaryab Kanjiani", "Srinjoy Ghose", "Srinidhi Shetty"], "title": "CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.HC", "cs.MA", "C.3; C.4; D.2.4; D.4.6; I.2.7"], "comment": null, "summary": "Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to\noperate in critical applications. However, traditional verification and\nvalidation methods often struggle to handle the unpredictable and dynamic\nnature of AI components. In this paper, we introduce CPS-Guard, a novel\nframework that employs multi-role orchestration to automate the iterative\nassurance process for AI-powered CPS. By assigning specialized roles (e.g.,\nsafety monitoring, security assessment, fault injection, and recovery planning)\nto dedicated agents within a simulated environment, CPS-Guard continuously\nevaluates and refines AI behavior against a range of dependability\nrequirements. We demonstrate the framework through a case study involving an\nautonomous vehicle navigating an intersection with an AI-based planner. Our\nresults show that CPS-Guard effectively detects vulnerabilities, manages\nperformance impacts, and supports adaptive recovery strategies, thereby\noffering a structured and extensible solution for rigorous V&V in safety- and\nsecurity-critical systems.", "AI": {"tldr": "CPS-Guard is a new framework using multi-role orchestration to automate assurance process for AI-powered CPS, showing effectiveness in detecting vulnerabilities and supporting recovery strategies through a autonomous vehicle case study.", "motivation": "Traditional verification and validation methods are insufficient for handling unpredictable and dynamic AI components in critical applications of Cyber-Physical Systems.", "method": "Introduced CPS-Guard framework which uses multi-role orchestration including safety monitoring, security assessment, fault injection, and recovery planning within a simulated environment to evaluate and refine AI behavior against dependability requirements.", "result": "The framework successfully detected vulnerabilities, managed performance impacts, and supported adaptive recovery strategies in the autonomous vehicle case study.", "conclusion": "CPS-Guard offers a structured and extensible solution for rigorous V&V in safety- and security-critical systems."}}
{"id": "2506.07308", "pdf": "https://arxiv.org/pdf/2506.07308", "abs": "https://arxiv.org/abs/2506.07308", "authors": ["Yizhuo Chen", "Chun-Fu", "Chen", "Hsiang Hsu", "Shaohan Hu", "Tarek Abdelzaher"], "title": "PASS: Private Attributes Protection with Stochastic Data Substitution", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "The growing Machine Learning (ML) services require extensive collections of\nuser data, which may inadvertently include people's private information\nirrelevant to the services. Various studies have been proposed to protect\nprivate attributes by removing them from the data while maintaining the\nutilities of the data for downstream tasks. Nevertheless, as we theoretically\nand empirically show in the paper, these methods reveal severe vulnerability\nbecause of a common weakness rooted in their adversarial training based\nstrategies. To overcome this limitation, we propose a novel approach, PASS,\ndesigned to stochastically substitute the original sample with another one\naccording to certain probabilities, which is trained with a novel loss function\nsoundly derived from information-theoretic objective defined for\nutility-preserving private attributes protection. The comprehensive evaluation\nof PASS on various datasets of different modalities, including facial images,\nhuman activity sensory signals, and voice recording datasets, substantiates\nPASS's effectiveness and generalizability.", "AI": {"tldr": "PASS, a novel approach that stochastically substitutes the original sample with another one according to certain probabilities and is trained with a novel loss function, effectively protects private attributes while maintaining data utilities for downstream tasks.", "motivation": "Existing methods for protecting private attributes by removing them from data while maintaining data utilities reveal severe vulnerability due to a common weakness rooted in their adversarial training based strategies.", "method": "The proposed approach PASS stochastically substitutes the original sample with another one according to certain probabilities. It is trained with a novel loss function derived from information-theoretic objective defined for utility-preserving private attributes protection.", "result": "PASS demonstrates effectiveness and generalizability through comprehensive evaluation on various datasets of different modalities including facial images, human activity sensory signals, and voice recording datasets.", "conclusion": "PASS overcomes the limitation of existing methods and provides a more robust solution for protecting private attributes while preserving data utilities."}}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382", "abs": "https://arxiv.org/abs/2506.06382", "authors": ["Micha\u0142 P. Karpowicz"], "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": null, "summary": "This paper explains \\textbf{why it is impossible to create large language\nmodels that do not hallucinate and what are the trade-offs we should be looking\nfor}. It presents a formal \\textbf{impossibility theorem} demonstrating that no\ninference mechanism can simultaneously satisfy four fundamental properties:\n\\textbf{truthful (non-hallucinatory) generation, semantic information\nconservation, relevant knowledge revelation, and knowledge-constrained\noptimality}. By modeling LLM inference as an \\textbf{auction of ideas} where\nneural components compete to contribute to responses, we prove the\nimpossibility using the Green-Laffont theorem. That mathematical framework\nprovides a rigorous foundation for understanding the nature of inference\nprocess, with implications for model architecture, training objectives, and\nevaluation methods.", "AI": {"tldr": "This paper explains why large language models cannot avoid hallucination and presents an impossibility theorem. It shows that no inference mechanism can meet four fundamental properties simultaneously.", "motivation": "To understand why it's impossible to create large language models without hallucination and to find the trade-offs in designing these models.", "method": "Modeling LLM inference as an auction of ideas, using the Green-Laffont theorem to prove the impossibility of meeting four key properties simultaneously.", "result": "Demonstrates that no inference mechanism can be truthful, conserve semantic information, reveal relevant knowledge, and achieve knowledge-constrained optimality all at once.", "conclusion": "The impossibility theorem provides a solid foundation for understanding the nature of inference process in LLMs, impacting model architecture, training objectives, and evaluation methods."}}
{"id": "2506.07311", "pdf": "https://arxiv.org/pdf/2506.07311", "abs": "https://arxiv.org/abs/2506.07311", "authors": ["Thomas Joshi", "Herman Saini", "Neil Dhillon", "Antoni Viros i Martin", "Kaoutar El Maghraoui"], "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.", "AI": {"tldr": "This paper presents a new method that combines PagedAttention and PyTorch's FlexAttention to improve memory efficiency for long-context inference in Large Language Models (LLMs) by addressing issues with key-value (KV) caches. The approach reduces inference latency and manages memory more effectively, especially on longer sequences.", "motivation": "To solve the problem of severe memory inefficiencies during long-context inference in LLMs caused by conventional handling of key-value (KV) caches.", "method": "Integration of PagedAttention with PyTorch's FlexAttention within IBM's Foundation Model Stack (FMS). This fused attention kernel efficiently gathers scattered KV data.", "result": "Benchmarks on an NVIDIA L4 GPU show significantly reduced inference latency which grows only linearly with sequence length from 128 to 2048 tokens when using a global KV cache, as opposed to exponential increases without caching. Memory usage remains largely unchanged for single-step evaluations, with minimal increments only noticeable at lengths exceeding 2048 tokens due to power-of-two cache allocations.", "conclusion": "The authors open-source the full implementation and discuss its implications for future long-context model deployment."}}
{"id": "2506.06383", "pdf": "https://arxiv.org/pdf/2506.06383", "abs": "https://arxiv.org/abs/2506.06383", "authors": ["Qian Huang", "King Wang Poon"], "title": "Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor", "categories": ["cs.CY", "cs.AI"], "comment": "19 pages, 5 figures", "summary": "Artificial intelligence is poised to transform teaching and coaching\npractices,yet its optimal role alongside human expertise remains unclear.This\nstudy investigates human and AI collaboration in fitness education through a\none year qualitative case study with a Pilates instructor.The researcher\nparticipated in the instructor classes and conducted biweekly semi structured\ninterviews to explore how generative AI could be integrated into class planning\nand instruction.", "AI": {"tldr": "The paper explores the collaboration between human and AI in fitness education via a one year qualitative case study with a Pilates instructor, revealing insights on integrating generative AI into class planning and instruction.", "motivation": "To clarify the optimal role of AI alongside human expertise in teaching and coaching practices, specifically within fitness education.", "method": "A one year qualitative case study was conducted with a Pilates instructor, including participation in classes and biweekly semi structured interviews.", "result": "Generative AI can be effectively integrated into class planning and instruction, enhancing the teaching practice when collaborating with human instructors.", "conclusion": "Human AI collaboration holds potential for enriching fitness education, though clear guidelines for integration are necessary."}}
{"id": "2506.07312", "pdf": "https://arxiv.org/pdf/2506.07312", "abs": "https://arxiv.org/abs/2506.07312", "authors": ["Yusuf Elnady"], "title": "Generative Modeling of Networked Time-Series via Transformer Architectures", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many security and network applications require having large datasets to train\nthe machine learning models. Limited data access is a well-known problem in the\nsecurity domain. Recent studies have shown the potential of Transformer models\nto enlarge the size of data by synthesizing new samples, but the synthesized\nsamples don't improve the models over the real data. To address this issue, we\ndesign an efficient transformer-based model as a generative framework to\ngenerate time-series data, that can be used to boost the performance of\nexisting and new ML workflows. Our new transformer model achieves the SOTA\nresults. We style our model to be generalizable and work across different\ndatasets, and produce high-quality samples.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u73b0\u6709\u548c\u65b0\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\uff0c\u6a21\u578b\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u5e76\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u6837\u672c\u3002", "motivation": "\u5728\u5b89\u5168\u9886\u57df\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\uff0c\u4f46\u6570\u636e\u8bbf\u95ee\u53d7\u9650\u662f\u4e00\u4e2a\u5df2\u77e5\u95ee\u9898\u3002\u867d\u7136\u53d8\u538b\u5668\u6a21\u578b\u53ef\u4ee5\u5408\u6210\u65b0\u6837\u672c\u4ee5\u6269\u5927\u6570\u636e\u89c4\u6a21\uff0c\u4f46\u8fd9\u4e9b\u5408\u6210\u6837\u672c\u5bf9\u6a21\u578b\u7684\u6539\u8fdb\u4e0d\u5982\u771f\u5b9e\u6570\u636e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u3002\u8be5\u6a21\u578b\u65e8\u5728\u63d0\u9ad8\u73b0\u6709\u548c\u65b0\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6a21\u578b\u88ab\u8bbe\u8ba1\u4e3a\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u4ee5\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e0a\u5de5\u4f5c\u3002", "result": "\u65b0\u7684\u53d8\u538b\u5668\u6a21\u578b\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u80fd\u591f\u4ea7\u751f\u9ad8\u8d28\u91cf\u7684\u6837\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u53d8\u538b\u5668\u7684\u751f\u6210\u6846\u67b6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8fd8\u5c55\u793a\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u80fd\u529b\u3002"}}
{"id": "2506.06384", "pdf": "https://arxiv.org/pdf/2506.06384", "abs": "https://arxiv.org/abs/2506.06384", "authors": ["Yi Ji", "Runzhi Li", "Baolei Mao"], "title": "Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KSEM2025 AI & Sec Workshop", "summary": "With the widespread adoption of Large Language Models (LLMs), prompt\ninjection attacks have emerged as a significant security threat. Existing\ndefense mechanisms often face critical trade-offs between effectiveness and\ngeneralizability. This highlights the urgent need for efficient prompt\ninjection detection methods that are applicable across a wide range of LLMs. To\naddress this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion\ndetection framework. It integrates a pretrained language model with heuristic\nfeature engineering to detect prompt injection attacks. Specifically, the\nframework employs DeBERTa-v3-base as a feature extractor to transform input\ntext into semantic vectors enriched with contextual information. In parallel,\nwe design heuristic rules based on known attack patterns to extract explicit\nstructural features commonly observed in attacks. Features from both channels\nare subsequently fused and passed through a fully connected neural network to\nproduce the final prediction. This dual-channel approach mitigates the\nlimitations of relying only on DeBERTa to extract features. Experimental\nresults on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms\nexisting methods in terms of accuracy, recall, and F1-score. Furthermore, when\ndeployed actually, it significantly reduces attack success rates across\nmainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDMPI-PMHFE\u7684\u53cc\u901a\u9053\u7279\u5f81\u878d\u5408\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u542f\u53d1\u5f0f\u7279\u5f81\u5de5\u7a0b\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u663e\u8457\u964d\u4f4e\u4e86\u653b\u51fb\u6210\u529f\u7387\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6210\u4e3a\u91cd\u8981\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u73b0\u6709\u7684\u9632\u5fa1\u673a\u5236\u5728\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u5404\u79cdLLMs\u7684\u63d0\u793a\u6ce8\u5165\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "DMPI-PMHFE\u6846\u67b6\u4f7f\u7528DeBERTa-v3-base\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5c06\u8f93\u5165\u6587\u672c\u8f6c\u6362\u4e3a\u5305\u542b\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u8bed\u4e49\u5411\u91cf\uff1b\u540c\u65f6\u8bbe\u8ba1\u57fa\u4e8e\u5df2\u77e5\u653b\u51fb\u6a21\u5f0f\u7684\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u63d0\u53d6\u653b\u51fb\u4e2d\u5e38\u89c1\u7684\u663e\u5f0f\u7ed3\u6784\u7279\u5f81\u3002\u7136\u540e\u5c06\u6765\u81ea\u4e24\u4e2a\u901a\u9053\u7684\u7279\u5f81\u878d\u5408\u5e76\u901a\u8fc7\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u6700\u7ec8\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cDMPI-PMHFE\u5728\u51c6\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9645\u90e8\u7f72\u540e\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5305\u62ecGLM-4\u3001LLaMA 3\u3001Qwen 2.5\u548cGPT-4o\u5728\u5185\u7684\u4e3b\u6d41LLMs\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "DMPI-PMHFE\u901a\u8fc7\u53cc\u901a\u9053\u7279\u5f81\u878d\u5408\u65b9\u6cd5\u7f13\u89e3\u4e86\u4ec5\u4f9d\u8d56DeBERTa\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\u7684\u5c40\u9650\u6027\uff0c\u662f\u4e00\u79cd\u6709\u6548\u7684\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2506.07324", "pdf": "https://arxiv.org/pdf/2506.07324", "abs": "https://arxiv.org/abs/2506.07324", "authors": ["David Millard", "Arielle Carr", "St\u00e9phane Gaudreault", "Ali Baheri"], "title": "DEF: Diffusion-augmented Ensemble Forecasting", "categories": ["cs.LG", "physics.ao-ph", "35Q93 (Primary), 86A10, 65M75 (Secondary)", "I.2.6; I.6.3"], "comment": "26 pages, 20 plots, journal paper", "summary": "We present DEF (\\textbf{\\ul{D}}iffusion-augmented \\textbf{\\ul{E}}nsemble\n\\textbf{\\ul{F}}orecasting), a novel approach for generating initial condition\nperturbations. Modern approaches to initial condition perturbations are\nprimarily designed for numerical weather prediction (NWP) solvers, limiting\ntheir applicability in the rapidly growing field of machine learning for\nweather prediction. Consequently, stochastic models in this domain are often\ndeveloped on a case-by-case basis. We demonstrate that a simple conditional\ndiffusion model can (1) generate meaningful structured perturbations, (2) be\napplied iteratively, and (3) utilize a guidance term to intuitivey control the\nlevel of perturbation. This method enables the transformation of any\ndeterministic neural forecasting system into a stochastic one. With our\nstochastic extended systems, we show that the model accumulates less error over\nlong-term forecasts while producing meaningful forecast distributions. We\nvalidate our approach on the 5.625$^\\circ$ ERA5 reanalysis dataset, which\ncomprises atmospheric and surface variables over a discretized global grid,\nspanning from the 1960s to the present. On this dataset, our method\ndemonstrates improved predictive performance along with reasonable spread\nestimates.", "AI": {"tldr": "The paper introduces DEF, a novel method using conditional diffusion models to generate initial condition perturbations for transforming deterministic neural forecasting systems into stochastic ones, validated on the ERA5 reanalysis dataset.", "motivation": "Current methods for generating initial condition perturbations are mainly designed for numerical weather prediction solvers, restricting their use in machine learning-based weather prediction. This limitation necessitates case-by-case development of stochastic models in this domain.", "method": "DEF employs a simple conditional diffusion model capable of generating structured perturbations, being applied iteratively, and utilizing a guidance term to control perturbation levels. It transforms any deterministic neural forecasting system into a stochastic one.", "result": "DEF accumulates less error over long-term forecasts and produces meaningful forecast distributions. On the 5.625\u00b0 ERA5 reanalysis dataset, it demonstrates improved predictive performance and reasonable spread estimates.", "conclusion": "DEF is an effective approach for generating initial condition perturbations in machine learning-based weather prediction, offering improvements in long-term forecast accuracy and providing meaningful uncertainty estimates."}}
{"id": "2506.06387", "pdf": "https://arxiv.org/pdf/2506.06387", "abs": "https://arxiv.org/abs/2506.06387", "authors": ["Baptiste Chatelier", "Vincent Corlay", "Musa Furkan Keskin", "Matthieu Crussi\u00e8re", "Henk Wymeersch", "Luc Le Magoarou"], "title": "Model-based Neural Data Augmentation for sub-wavelength Radio Localization", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "The increasing deployment of large antenna arrays at base stations has\nsignificantly improved the spatial resolution and localization accuracy of\nradio-localization methods. However, traditional signal processing techniques\nstruggle in complex radio environments, particularly in scenarios dominated by\nnon line of sight (NLoS) propagation paths, resulting in degraded localization\naccuracy. Recent developments in machine learning have facilitated the\ndevelopment of machine learning-assisted localization techniques, enhancing\nlocalization accuracy in complex radio environments. However, these methods\noften involve substantial computational complexity during both the training and\ninference phases. This work extends the well-established fingerprinting-based\nlocalization framework by simultaneously reducing its memory requirements and\nimproving its accuracy. Specifically, a model-based neural network is used to\nlearn the location-to-channel mapping, and then serves as a generative neural\nchannel model. This generative model augments the fingerprinting comparison\ndictionary while reducing the memory requirements. The proposed method\noutperforms fingerprinting baselines by achieving sub-wavelength localization\naccuracy, even in NLoS environments. Remarkably, it offers an improvement by\nseveral orders of magnitude in localization accuracy, while simultaneously\nreducing memory requirements by an order of magnitude compared to classical\nfingerprinting methods.", "AI": {"tldr": "This paper proposes a model-based neural network that learns the location-to-channel mapping to enhance fingerprinting-based localization, achieving sub-wavelength accuracy in NLoS environments while reducing memory needs.", "motivation": "The motivation is to address the limitations of traditional signal processing techniques and machine learning-assisted localization methods in complex radio environments, especially in NLoS scenarios, where localization accuracy is degraded and computational complexity is high.", "method": "A model-based neural network is employed to learn the location-to-channel mapping, acting as a generative neural channel model. This model augments the fingerprinting comparison dictionary while reducing memory requirements.", "result": "The method achieves sub-wavelength localization accuracy in NLoS environments, improving accuracy by several orders of magnitude and reducing memory requirements by an order of magnitude compared to classical fingerprinting methods.", "conclusion": "This approach successfully enhances localization accuracy while decreasing memory usage, making it a significant advancement in fingerprinting-based localization."}}
{"id": "2506.07328", "pdf": "https://arxiv.org/pdf/2506.07328", "abs": "https://arxiv.org/abs/2506.07328", "authors": ["Jintao Yan", "Tan Chen", "Yuxuan Sun", "Zhaojun Nan", "Sheng Zhou", "Zhisheng Niu"], "title": "Mobility-Aware Asynchronous Federated Learning with Dynamic Sparsification", "categories": ["cs.LG"], "comment": null, "summary": "Asynchronous Federated Learning (AFL) enables distributed model training\nacross multiple mobile devices, allowing each device to independently update\nits local model without waiting for others. However, device mobility introduces\nintermittent connectivity, which necessitates gradient sparsification and leads\nto model staleness, jointly affecting AFL convergence. This paper develops a\ntheoretical model to characterize the interplay among sparsification, model\nstaleness and mobility-induced contact patterns, and their joint impact on AFL\nconvergence. Based on the analysis, we propose a mobility-aware dynamic\nsparsification (MADS) algorithm that optimizes the sparsification degree based\non contact time and model staleness. Closed-form solutions are derived, showing\nthat under low-speed conditions, MADS increases the sparsification degree to\nenhance convergence, while under high-speed conditions, it reduces the\nsparsification degree to guarantee reliable uploads within limited contact\ntime. Experimental results validate the theoretical findings. Compared with the\nstate-of-the-art benchmarks, the MADS algorithm increases the image\nclassification accuracy on the CIFAR-10 dataset by 8.76% and reduces the\naverage displacement error in the Argoverse trajectory prediction dataset by\n9.46%.", "AI": {"tldr": "Asynchronous Federated Learning (AFL) faces challenges due to device mobility and intermittent connectivity. This paper develops a theoretical model to understand the relationship between sparsification, model staleness, and mobility-induced contact patterns on AFL convergence. Based on this, a MADS algorithm is proposed to optimize sparsification degree according to contact time and model staleness. Experiments show that MADS improves image classification accuracy and reduces average displacement error in trajectory prediction.", "motivation": "Device mobility in Asynchronous Federated Learning introduces intermittent connectivity, requiring gradient sparsification which affects model staleness and thus AFL convergence.", "method": "Theoretical model developed to characterize interplay among sparsification, model staleness, and mobility-induced contact patterns. Proposed MADS algorithm optimizes sparsification degree based on contact time and model staleness with closed-form solutions derived.", "result": "MADS increases image classification accuracy on CIFAR-10 by 8.76% and reduces average displacement error in Argoverse dataset by 9.46%. Experimental results validate theoretical findings.", "conclusion": "MADS algorithm effectively optimizes sparsification degree under different mobility conditions improving AFL convergence."}}
{"id": "2506.06390", "pdf": "https://arxiv.org/pdf/2506.06390", "abs": "https://arxiv.org/abs/2506.06390", "authors": ["Liangliang Chen", "Zhihao Qin", "Yiming Guo", "Jacqueline Rohde", "Ying Zhang"], "title": "Benchmarking Large Language Models on Homework Assessment in Circuit Analysis", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize various\nfields, including code development, robotics, finance, and education, due to\ntheir extensive prior knowledge and rapid advancements. This paper investigates\nhow LLMs can be leveraged in engineering education. Specifically, we benchmark\nthe capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama\n3 70B, in assessing homework for an undergraduate-level circuit analysis\ncourse. We have developed a novel dataset consisting of official reference\nsolutions and real student solutions to problems from various topics in circuit\nanalysis. To overcome the limitations of image recognition in current\nstate-of-the-art LLMs, the solutions in the dataset are converted to LaTeX\nformat. Using this dataset, a prompt template is designed to test five metrics\nof student solutions: completeness, method, final answer, arithmetic error, and\nunits. The results show that GPT-4o and Llama 3 70B perform significantly\nbetter than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B\neach having distinct advantages in different evaluation aspects. Additionally,\nwe present insights into the limitations of current LLMs in several aspects of\ncircuit analysis. Given the paramount importance of ensuring reliability in\nLLM-generated homework assessment to avoid misleading students, our results\nestablish benchmarks and offer valuable insights for the development of a\nreliable, personalized tutor for circuit analysis -- a focus of our future\nwork. Furthermore, the proposed evaluation methods can be generalized to a\nbroader range of courses for engineering education in the future.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u7535\u8def\u5206\u6790\u8bfe\u7a0b\u7684\u5bb6\u5ead\u4f5c\u4e1a\u3002\u7814\u7a76\u53d1\u73b0GPT-4o\u548cLlama 3 70B\u7684\u8868\u73b0\u4f18\u4e8eGPT-3.5 Turbo\uff0c\u5e76\u63d0\u51fa\u4e86\u5f53\u524dLLMs\u7684\u5c40\u9650\u6027\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u56e0\u5176\u5e7f\u6cdb\u7684\u77e5\u8bc6\u548c\u5feb\u901f\u8fdb\u6b65\uff0c\u6709\u53ef\u80fd\u6539\u53d8\u5305\u62ec\u5de5\u7a0b\u6559\u80b2\u5728\u5185\u7684\u591a\u4e2a\u9886\u57df\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5b83\u4eec\u5728\u5de5\u7a0b\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b\u5b98\u65b9\u53c2\u8003\u7b54\u6848\u548c\u771f\u5b9e\u5b66\u751f\u89e3\u7b54\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u8fd9\u4e9b\u89e3\u7b54\u8f6c\u6362\u4e3aLaTeX\u683c\u5f0f\u4ee5\u514b\u670d\u56fe\u50cf\u8bc6\u522b\u7684\u9650\u5236\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u63d0\u793a\u6a21\u677f\uff0c\u7528\u4e8e\u6d4b\u8bd5\u5b66\u751f\u89e3\u7b54\u7684\u4e94\u4e2a\u6307\u6807\uff1a\u5b8c\u6574\u6027\u3001\u65b9\u6cd5\u3001\u6700\u7ec8\u7b54\u6848\u3001\u7b97\u672f\u9519\u8bef\u548c\u5355\u4f4d\u3002", "result": "GPT-4o\u548cLlama 3 70B\u5728\u6240\u6709\u4e94\u4e2a\u6307\u6807\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8eGPT-3.5 Turbo\uff0c\u5404\u81ea\u5728\u4e0d\u540c\u7684\u8bc4\u4f30\u65b9\u9762\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u7535\u8def\u5206\u6790\u7684\u53ef\u9760\u4e2a\u6027\u5316\u5bfc\u5e08\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u51c6\u5e76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u8bc4\u4f30\u65b9\u6cd5\u53ef\u4ee5\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u5de5\u7a0b\u6559\u80b2\u8bfe\u7a0b\u3002"}}
{"id": "2506.06391", "pdf": "https://arxiv.org/pdf/2506.06391", "abs": "https://arxiv.org/abs/2506.06391", "authors": ["John Mavi", "Diana Teodora G\u0103itan", "Sergio Coronado"], "title": "From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used across sectors, yet their\nalignment with International Humanitarian Law (IHL) is not well understood.\nThis study evaluates eight leading LLMs on their ability to refuse prompts that\nexplicitly violate these legal frameworks, focusing also on helpfulness - how\nclearly and constructively refusals are communicated. While most models\nrejected unlawful requests, the clarity and consistency of their responses\nvaried. By revealing the model's rationale and referencing relevant legal or\nsafety principles, explanatory refusals clarify the system's boundaries, reduce\nambiguity, and help prevent misuse. A standardised system-level safety prompt\nsignificantly improved the quality of the explanations expressed within\nrefusals in most models, highlighting the effectiveness of lightweight\ninterventions. However, more complex prompts involving technical language or\nrequests for code revealed ongoing vulnerabilities. These findings contribute\nto the development of safer, more transparent AI systems and propose a\nbenchmark to evaluate the compliance of LLM with IHL.", "AI": {"tldr": "This study evaluates 8 leading LLMs on their ability to refuse prompts violating International Humanitarian Law (IHL). A standardised safety prompt improves refusal explanations, but vulnerabilities persist in complex prompts. Findings aim to develop safer AI systems and propose a compliance benchmark.", "motivation": "To understand how well Large Language Models (LLMs) align with International Humanitarian Law (IHL) and improve their safety and transparency by evaluating their ability to refuse unlawful prompts.", "method": "The study evaluated eight leading LLMs on their ability to refuse prompts that explicitly violate IHL legal frameworks. It also assessed the helpfulness of these refusals - focusing on how clearly and constructively they were communicated. A standardised system-level safety prompt was introduced to observe its impact on the quality of refusal explanations.", "result": "Most models rejected unlawful requests, but there was variation in the clarity and consistency of their responses. Explanatory refusals, which revealed the model's rationale and referenced relevant legal or safety principles, clarified system boundaries and reduced ambiguity. The introduction of a standardised safety prompt significantly improved the quality of refusal explanations in most models. However, vulnerabilities were identified in handling more complex prompts involving technical language or code.", "conclusion": "The findings contribute towards developing safer and more transparent AI systems. They also propose a benchmark for evaluating LLM compliance with IHL."}}
{"id": "2506.07334", "pdf": "https://arxiv.org/pdf/2506.07334", "abs": "https://arxiv.org/abs/2506.07334", "authors": ["Haoyu Wang", "Peihao Wang", "Mufei Li", "Shikun Liu", "Siqi Miao", "Zhangyang Wang", "Pan Li"], "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models", "categories": ["cs.LG"], "comment": null, "summary": "Modern large language models (LLMs) are inherently auto-regressive, requiring\ninput to be serialized into flat sequences regardless of their structural\ndependencies. This serialization hinders the model's ability to leverage\nstructural inductive biases, especially in tasks such as retrieval-augmented\ngeneration (RAG) and reasoning on data with native graph structures, where\ninter-segment dependencies are crucial. We introduce Graph-KV with the\npotential to overcome this limitation. Graph-KV leverages the KV-cache of text\nsegments as condensed representations and governs their interaction through\nstructural inductive biases. In this framework, 'target' segments selectively\nattend only to the KV-caches of their designated 'source' segments, rather than\nall preceding segments in a serialized sequence. This approach induces a\ngraph-structured block mask, sparsifying attention and enabling a\nmessage-passing-like step within the LLM. Furthermore, strategically allocated\npositional encodings for source and target segments reduce positional bias and\ncontext window consumption. We evaluate Graph-KV across three scenarios: (1)\nseven RAG benchmarks spanning direct inference, multi-hop reasoning, and\nlong-document understanding; (2) Arxiv-QA, a novel academic paper QA task with\nfull-text scientific papers structured as citation ego-graphs; and (3) paper\ntopic classification within a citation network. By effectively reducing\npositional bias and harnessing structural inductive biases, Graph-KV\nsubstantially outperforms baselines, including standard costly sequential\nencoding, across various settings. Code and the Graph-KV data are publicly\navailable.", "AI": {"tldr": "Graph-KV is a new method that uses structural inductive biases to improve performance on tasks with inter-segment dependencies, such as RAG and reasoning on graph-structured data. It leverages KV-caches of text segments and reduces positional bias.", "motivation": "Modern large language models (LLMs) are inherently auto-regressive and require input to be serialized into flat sequences regardless of their structural dependencies. This serialization hinders the model's ability to leverage structural inductive biases.", "method": "Graph-KV leverages the KV-cache of text segments as condensed representations and governs their interaction through structural inductive biases. 'Target' segments selectively attend only to the KV-caches of their designated 'source' segments, inducing a graph-structured block mask and enabling a message-passing-like step within the LLM.", "result": "Graph-KV substantially outperforms baselines, including standard costly sequential encoding, across various settings.", "conclusion": "By effectively reducing positional bias and harnessing structural inductive biases, Graph-KV shows significant improvements in tasks such as retrieval-augmented generation (RAG) and reasoning on data with native graph structures."}}
{"id": "2506.06396", "pdf": "https://arxiv.org/pdf/2506.06396", "abs": "https://arxiv.org/abs/2506.06396", "authors": ["Christopher D. Molek", "Roberto Fronteddu", "K. Brent Venable", "Niranjan Suri"], "title": "Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of\nBattlefield Things (IoBT), gives rise to new opportunities for enhancing\nsituational awareness. To increase the potential of IoBT for situational\nawareness in critical decision making, the data from these devices must be\nprocessed into consumer-ready information objects, and made available to\nconsumers on demand. To address this challenge we propose a workflow that makes\nuse of natural language processing (NLP) to query a database technology and\nreturn a response in natural language. Our solution utilizes Large Language\nModels (LLMs) that are sized for edge devices to perform NLP as well as\ngraphical databases which are well suited for dynamic connected networks which\nare pervasive in the IoBT. Our architecture employs LLMs for both mapping\nquestions in natural language to Cypher database queries as well as to\nsummarize the database output back to the user in natural language. We evaluate\nseveral medium sized LLMs for both of these tasks on a database representing\npublicly available data from the US Army's Multipurpose Sensing Area (MSA) at\nthe Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion\nparameters) outperforms the other models across all the considered metrics.\nMost importantly, we note that, unlike current methods, our two step approach\nallows the relaxation of the Exact Match (EM) requirement of the produced\nCypher queries with ground truth code and, in this way, it achieves a 19.4%\nincrease in accuracy. Our workflow lays the ground work for deploying LLMs on\nedge devices to enable natural language interactions with databases containing\ninformation objects for critical decision making.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u67e5\u8be2\u6570\u636e\u5e93\u5e76\u4ee5\u81ea\u7136\u8bed\u8a00\u8fd4\u56de\u54cd\u5e94\u7684\u5de5\u4f5c\u6d41\u7a0b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6218\u573a\u7269\u8054\u7f51\uff08IoBT\uff09\u4e2d\u7684\u60c5\u5883\u611f\u77e5\u3002\u901a\u8fc7\u4f7f\u7528\u9002\u914d\u8fb9\u7f18\u8bbe\u5907\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u5f62\u6570\u636e\u5e93\uff0c\u67b6\u6784\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u8f6c\u6362\u4e3a\u6570\u636e\u5e93\u67e5\u8be2\uff0c\u5e76\u5c06\u7ed3\u679c\u603b\u7ed3\u56de\u7528\u6237\u3002\u5b9e\u9a8c\u663e\u793aLlama 3.1\u5728\u591a\u9879\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u901a\u8fc7\u653e\u677e\u4e0e\u771f\u5b9e\u4ee3\u7801\u7684\u5b8c\u5168\u5339\u914d\u8981\u6c42\uff0c\u63d0\u9ad8\u4e8619.4%\u7684\u51c6\u786e\u6027\u3002\u6b64\u5de5\u4f5c\u6d41\u4e3a\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72LLMs\u4ee5\u5b9e\u73b0\u5173\u952e\u51b3\u7b56\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u968f\u7740\u6218\u573a\u7269\u8054\u7f51\uff08IoBT\uff09\u7684\u53d1\u5c55\uff0c\u4e3a\u4e86\u589e\u5f3a\u60c5\u5883\u611f\u77e5\u80fd\u529b\uff0c\u9700\u8981\u5c06\u6765\u81ea\u8bbe\u5907\u7684\u6570\u636e\u5904\u7406\u6210\u53ef\u4f9b\u6d88\u8d39\u8005\u6309\u9700\u4f7f\u7528\u7684\u6d88\u8d39\u7ea7\u4fe1\u606f\u5bf9\u8c61\u3002\u76ee\u524d\u7f3a\u4e4f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u67e5\u8be2\u6570\u636e\u5e93\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\uff0c\u4ece\u800c\u6ee1\u8db3\u5173\u952e\u51b3\u7b56\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5de5\u4f5c\u6d41\u7a0b\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u6570\u636e\u5e93\u67e5\u8be2\u548c\u7ed3\u679c\u603b\u7ed3\u3002\u5177\u4f53\u5305\u62ec\uff1a\u4f7f\u7528\u9002\u914d\u8fb9\u7f18\u8bbe\u5907\u7684\u4e2d\u578bLLMs\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u6620\u5c04\u5230Cypher\u6570\u636e\u5e93\u67e5\u8be2\uff0c\u5e76\u5c06\u67e5\u8be2\u7ed3\u679c\u4ee5\u81ea\u7136\u8bed\u8a00\u5f62\u5f0f\u53cd\u9988\u7ed9\u7528\u6237\u3002\u540c\u65f6\u91c7\u7528\u56fe\u5f62\u6570\u636e\u5e93\u5904\u7406\u52a8\u6001\u8fde\u63a5\u7f51\u7edc\u6570\u636e\u3002", "result": "\u5728\u516c\u5f00\u7684\u7f8e\u519b\u591a\u7528\u9014\u4f20\u611f\u533a\u57df\uff08MSA\uff09\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u591a\u4e2a\u4e2d\u578bLLMs\uff0c\u7ed3\u679c\u663e\u793aLlama 3.1\uff0880\u4ebf\u53c2\u6570\uff09\u5728\u6240\u6709\u8003\u8651\u7684\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u7684\u4e24\u6b65\u6cd5\u901a\u8fc7\u653e\u677e\u5bf9Cypher\u67e5\u8be2\u4e0e\u771f\u5b9e\u4ee3\u7801\u7684\u5b8c\u5168\u5339\u914d\u8981\u6c42\uff0c\u5b9e\u73b0\u4e8619.4%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72LLMs\u4ee5\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u636e\u5e93\u4ea4\u4e92\u7684\u6f5c\u529b\uff0c\u4e3a\u5728IoBT\u73af\u5883\u4e2d\u8fdb\u884c\u5173\u952e\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.07355", "pdf": "https://arxiv.org/pdf/2506.07355", "abs": "https://arxiv.org/abs/2506.07355", "authors": ["Yuya Okada", "Takayuki Nishio"], "title": "SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model\nadaptation framework for Split Computing under closed constraints, where the\nhead and tail networks are proprietary and inaccessible to users. In such\nclosed environments, conventional adaptation methods are infeasible since they\nrequire access to model parameters or architectures. SALT addresses this\nchallenge by introducing a compact, trainable adapter on the client side to\nrefine latent features from the head network, enabling user-specific adaptation\nwithout modifying the original models or increasing communication overhead. We\nevaluate SALT on user-specific classification tasks with CIFAR-10 and\nCIFAR-100, demonstrating improved accuracy with lower training latency compared\nto fine-tuning methods. Furthermore, SALT facilitates model adaptation for\nrobust inference over lossy networks, a common challenge in edge-cloud\nenvironments. With minimal deployment overhead, SALT offers a practical\nsolution for personalized inference in edge AI systems under strict system\nconstraints.", "AI": {"tldr": "The paper introduces SALT, a model adaptation framework for Split Computing that works under closed constraints without access to head and tail networks. It uses a compact adapter on the client side to refine latent features for user-specific adaptation. Evaluated on CIFAR-10 and CIFAR-100, it shows improved accuracy with less training latency compared to fine-tuning methods. Additionally, it enhances robust inference over lossy networks in edge-cloud environments.", "motivation": "To address the challenge of adapting models in closed environments where head and tail networks are proprietary and inaccessible, making conventional adaptation methods infeasible.", "method": "SALT introduces a compact, trainable adapter on the client side which refines latent features from the head network, enabling user-specific adaptation without modifying original models or increasing communication overhead.", "result": "SALT demonstrates improved accuracy with lower training latency compared to fine-tuning methods when evaluated on user-specific classification tasks using CIFAR-10 and CIFAR-100 datasets. It also facilitates robust inference over lossy networks.", "conclusion": "SALT provides a practical solution for personalized inference in edge AI systems under strict system constraints with minimal deployment overhead."}}
{"id": "2506.07366", "pdf": "https://arxiv.org/pdf/2506.07366", "abs": "https://arxiv.org/abs/2506.07366", "authors": ["Haiyue Ma", "Zhixu Du", "Yiran Chen"], "title": "MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing", "categories": ["cs.LG", "cs.AR"], "comment": null, "summary": "In multi-GPU Mixture-of-Experts (MoE) network, experts are distributed across\ndifferent GPUs, which creates load imbalance as each expert processes different\nnumber of tokens. Recent works improve MoE inference load balance by\ndynamically duplicating popular experts to more GPUs to process excessive\ntokens, which requires predicting the distribution before routing. In this\npaper, we discuss the tradeoff of prediction strategies, accuracies, overhead,\nand end-to-end system performance. We propose MoE-GPS, a framework that guides\nthe selection of the optimal predictor design under various system\nconfigurations, by quantifying the performance impact to system-level model\nruntime. Specifically, we advocate for Distribution-Only Prediction, a\nprediction strategy that only predicts overall token distribution which\nsignificantly reduces overhead compared to the traditional Token-to-Expert\nPrediction. On Mixtral 8x7B MMLU dataset, MoE-GPS suggests Distribution-Only\nPrediction which improves end-to-end inference performance by more than 23%\ncompared with Token-to-Expert Prediction.", "AI": {"tldr": "In multi-GPU Mixture-of-Experts (MoE) network, load imbalance occurs due to varying token processing among experts. Recent works improve inference load balance by duplicating popular experts across more GPUs, requiring distribution prediction before routing. This paper proposes MoE-GPS, a framework that selects optimal predictor design under different system configurations by quantifying the performance impact on system-level model runtime. Specifically, it promotes Distribution-Only Prediction which reduces overhead compared to Token-to-Expert Prediction and improves end-to-end inference performance.", "motivation": "Load imbalance in multi-GPU Mixture-of-Experts (MoE) networks is a significant issue as each expert processes a different number of tokens. To address this, there's a need for effective prediction strategies that can optimize the distribution of tokens among experts to achieve better load balancing.", "method": "The paper introduces MoE-GPS, a framework designed to guide the selection of the optimal predictor design under various system configurations. It focuses on Distribution-Only Prediction, a strategy that predicts overall token distribution rather than predicting for each token individually, significantly reducing overhead.", "result": "On the Mixtral 8x7B MMLU dataset, MoE-GPS suggests that using Distribution-Only Prediction can improve end-to-end inference performance by over 23% compared to Token-to-Expert Prediction.", "conclusion": "MoE-GPS provides an effective solution for selecting optimal predictor designs in MoE networks, with Distribution-Only Prediction showing substantial improvements in inference performance while reducing overhead."}}
{"id": "2506.06401", "pdf": "https://arxiv.org/pdf/2506.06401", "abs": "https://arxiv.org/abs/2506.06401", "authors": ["Hongming Yang", "Shi Lin", "Jun Shao", "Changting Lin", "Donghai Zhu", "Meng Han", "Qinglei Kong"], "title": "Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work is accepted at ACL 2025", "summary": "Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized\nmodels designed to run efficiently on consumer-grade hardware, offering\nsignificant advantages in resource efficiency, cost-effectiveness, and data\nprivacy. However, these models often struggle with limited inference and\nreasoning capabilities, which restrict their performance on complex tasks and\nlimit their practical applicability. Moreover, existing prompt optimization\nmethods typically rely on extensive manual effort or the meta-cognitive\nabilities of state-of-the-art LLMs, making them less effective for LwLLMs. To\naddress these challenges, we introduce DeBoP, a new Direct Behavior\nOptimization Paradigm, original from the Chain-of-Thought (CoT) prompting\ntechnique. Unlike CoT Prompting, DeBoP is an automatic optimization method,\nwhich focuses on the optimization directly on the behavior of LwLLMs. In\nparticular, DeBoP transforms the optimization of complex prompts into the\noptimization of discrete, quantifiable execution sequences using a\ngradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging\ntasks where state-of-the-art LLMs excel but LwLLMs generally underperform.\nExperimental results demonstrate that DeBoP significantly outperforms recent\nprompt optimization methods on most tasks. In particular, DeBoP-optimized\nLwLLMs surpass GPT-3.5 on most tasks while reducing computational time by\napproximately 60% compared to other automatic prompt optimization methods.", "AI": {"tldr": "DeBoP\u662f\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u4e13\u95e8\u9488\u5bf9\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff08LwLLMs\uff09\uff0c\u901a\u8fc7\u81ea\u52a8\u4f18\u5316\u5176\u884c\u4e3a\u8868\u73b0\uff0c\u5728\u591a\u4e2a\u590d\u6742\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8d85\u8d8aGPT-3.5\u5e76\u51cf\u5c11\u7ea660%\u7684\u8ba1\u7b97\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u7684\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u548c\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4f20\u7edf\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5927\u91cf\u4eba\u5de5\u6216\u5148\u8fdb\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5bf9\u8f7b\u91cf\u7ea7\u6a21\u578b\u6548\u679c\u4e0d\u4f73\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u6539\u5584\u8fd9\u4e9b\u6a21\u578b\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u4e86DeBoP\uff08Direct Behavior Optimization Paradigm\uff09\uff0c\u4e00\u79cd\u57fa\u4e8e\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u6280\u672f\u7684\u81ea\u52a8\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5c06\u590d\u6742\u7684\u63d0\u793a\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u79bb\u6563\u3001\u53ef\u91cf\u5316\u7684\u6267\u884c\u5e8f\u5217\u4f18\u5316\uff0c\u5e76\u4f7f\u7528\u65e0\u68af\u5ea6\u8499\u7279\u5361\u7f57\u6811\u641c\u7d22\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeBoP\u5728\u4e03\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6700\u65b0\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002\u7ecf\u8fc7DeBoP\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8d85\u8fc7\u4e86GPT-3.5\uff0c\u540c\u65f6\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea660%\u3002", "conclusion": "DeBoP\u4f5c\u4e3a\u4e00\u79cd\u65b0\u9896\u7684\u4f18\u5316\u8303\u5f0f\uff0c\u6210\u529f\u63d0\u5347\u4e86\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2506.07378", "pdf": "https://arxiv.org/pdf/2506.07378", "abs": "https://arxiv.org/abs/2506.07378", "authors": ["Yuen Chen", "Haozhe Si", "Guojun Zhang", "Han Zhao"], "title": "Moment Alignment: Unifying Gradient and Hessian Matching for Domain Generalization", "categories": ["cs.LG", "stat.ML"], "comment": "UAI 2025", "summary": "Domain generalization (DG) seeks to develop models that generalize well to\nunseen target domains, addressing the prevalent issue of distribution shifts in\nreal-world applications. One line of research in DG focuses on aligning\ndomain-level gradients and Hessians to enhance generalization. However,\nexisting methods are computationally inefficient and the underlying principles\nof these approaches are not well understood. In this paper, we develop the\ntheory of moment alignment for DG. Grounded in \\textit{transfer measure}, a\nprincipled framework for quantifying generalizability between two domains, we\nfirst extend the definition of transfer measure to domain generalization that\nincludes multiple source domains and establish a target error bound. Then, we\nprove that aligning derivatives across domains improves transfer measure both\nwhen the feature extractor induces an invariant optimal predictor across\ndomains and when it does not. Notably, moment alignment provides a unifying\nunderstanding of Invariant Risk Minimization, gradient matching, and Hessian\nmatching, three previously disconnected approaches to DG. We further connect\nfeature moments and derivatives of the classifier head, and establish the\nduality between feature learning and classifier fitting. Building upon our\ntheory, we introduce \\textbf{C}losed-Form \\textbf{M}oment \\textbf{A}lignment\n(CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in\nclosed-form. Our method overcomes the computational inefficiencies of existing\ngradient and Hessian-based techniques by eliminating the need for repeated\nbackpropagation or sampling-based Hessian estimation. We validate the efficacy\nof our approach through two sets of experiments: linear probing and full\nfine-tuning. CMA demonstrates superior performance in both settings compared to\nEmpirical Risk Minimization and state-of-the-art algorithms.", "AI": {"tldr": "The paper develops a theory of moment alignment for domain generalization (DG) and introduces Closed-Form Moment Alignment (CMA), a novel DG algorithm that aligns domain-level gradients and Hessians in closed-form, overcoming computational inefficiencies.", "motivation": "Existing methods for aligning domain-level gradients and Hessians in DG are computationally inefficient and lack a clear understanding of their underlying principles.", "method": "The authors extend the definition of transfer measure to domain generalization with multiple source domains, establish a target error bound, and prove that aligning derivatives across domains improves transfer measure. They connect feature moments and derivatives of the classifier head, establishing duality between feature learning and classifier fitting. Based on this theory, they introduce CMA, which aligns domain-level gradients and Hessians in closed-form without repeated backpropagation or sampling-based Hessian estimation.", "result": "CMA demonstrates superior performance compared to Empirical Risk Minimization and state-of-the-art algorithms in both linear probing and full fine-tuning experiments.", "conclusion": "Moment alignment provides a unifying understanding of previously disconnected approaches to DG such as Invariant Risk Minimization, gradient matching, and Hessian matching."}}
{"id": "2506.06404", "pdf": "https://arxiv.org/pdf/2506.06404", "abs": "https://arxiv.org/abs/2506.06404", "authors": ["Sooyung Choi", "Jaehyeok Lee", "Xiaoyuan Yi", "Jing Yao", "Xing Xie", "JinYeong Bak"], "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "The application scope of Large Language Models (LLMs) continues to expand,\nleading to increasing interest in personalized LLMs that align with human\nvalues. However, aligning these models with individual values raises\nsignificant safety concerns, as certain values may correlate with harmful\ninformation. In this paper, we identify specific safety risks associated with\nvalue-aligned LLMs and investigate the psychological principles behind these\nchallenges. Our findings reveal two key insights. (1) Value-aligned LLMs are\nmore prone to harmful behavior compared to non-fine-tuned models and exhibit\nslightly higher risks in traditional safety evaluations than other fine-tuned\nmodels. (2) These safety issues arise because value-aligned LLMs genuinely\ngenerate text according to the aligned values, which can amplify harmful\noutcomes. Using a dataset with detailed safety categories, we find significant\ncorrelations between value alignment and safety risks, supported by\npsychological hypotheses. This study offers insights into the \"black box\" of\nvalue alignment and proposes in-context alignment methods to enhance the safety\nof value-aligned LLMs.", "AI": {"tldr": "The paper explores safety risks of value-aligned Large Language Models (LLMs), finding they are more likely to generate harmful content due to amplifying aligned values, and proposes in-context alignment methods for improvement.", "motivation": "To address the growing interest in personalized LLMs that align with human values, while identifying and mitigating potential safety concerns associated with these models.", "method": "Identify specific safety risks of value-aligned LLMs, investigate psychological principles behind these challenges, and analyze correlations between value alignment and safety risks using a dataset with detailed safety categories.", "result": "Value-aligned LLMs are more prone to harmful behavior compared to non-fine-tuned models, exhibit slightly higher risks than other fine-tuned models in traditional safety evaluations, and genuinely generate text according to aligned values, which can amplify harmful outcomes.", "conclusion": "This study sheds light on the 'black box' of value alignment in LLMs and suggests in-context alignment methods to improve the safety of value-aligned LLMs."}}
{"id": "2506.07405", "pdf": "https://arxiv.org/pdf/2506.07405", "abs": "https://arxiv.org/abs/2506.07405", "authors": ["Zhongping Ji"], "title": "RiemannFormer: A Framework for Attention in Curved Spaces", "categories": ["cs.LG"], "comment": "10 pages, 1 figure", "summary": "This research endeavors to offer insights into unlocking the further\npotential of transformer-based architectures. One of the primary motivations is\nto offer a geometric interpretation for the attention mechanism in\ntransformers. In our framework, the attention mainly involves metric tensors,\ntangent spaces, inner product, and how they relate to each other. These\nquantities and structures at discrete positions are intricately interconnected\nvia the parallel transport of tangent vectors. To make the learning process\nmore efficient, we reduce the number of parameters through ingenious predefined\nconfigurations. Moreover, we introduce an explicit mechanism to highlight a\nneighborhood by attenuating the remote values, given that transformers\ninherently neglect local inductive bias. Experimental results demonstrate that\nour modules deliver significant performance improvements relative to the\nbaseline. More evaluation experiments on visual and large language models will\nbe launched successively.", "AI": {"tldr": "This research aims to unlock the potential of transformer-based architectures by providing a geometric interpretation for the attention mechanism and introducing an explicit mechanism to highlight local inductive bias, resulting in significant performance improvements.", "motivation": "The primary motivation is to provide a geometric interpretation for the attention mechanism in transformers.", "method": "The method mainly involves metric tensors, tangent spaces, inner product, and their interrelations via the parallel transport of tangent vectors. Ingenious predefined configurations are used to reduce parameters and an explicit mechanism is introduced to highlight local inductive bias.", "result": "Experimental results show significant performance improvements relative to the baseline.", "conclusion": "The modules deliver significant performance improvements. Further evaluation experiments on visual and large language models will be conducted."}}
{"id": "2506.06406", "pdf": "https://arxiv.org/pdf/2506.06406", "abs": "https://arxiv.org/abs/2506.06406", "authors": ["Guoyang Xia", "Yifeng Ding", "Fengfa Li", "Lei Ren", "Chen Wei", "Fangxiang Feng", "Xiaojie Wang"], "title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have become a key approach for scaling\nlarge language models, with growing interest in extending them to multimodal\ntasks. Existing methods to build multimodal MoE models either incur high\ntraining costs or suffer from degraded language capabilities when adapting\npretrained models. To address this, we propose Soft ModalityAware Routing\n(SMAR), a novel regularization technique that uses Kullback Leibler divergence\nto control routing probability distributions across modalities, encouraging\nexpert specialization without modifying model architecture or heavily relying\non textual data. Experiments on visual instruction tuning show that SMAR\npreserves language ability at 86.6% retention with only 2.5% pure text,\noutperforming baselines while maintaining strong multimodal performance. Our\napproach offers a practical and efficient solution to balance modality\ndifferentiation and language capabilities in multimodal MoE models.", "AI": {"tldr": "The paper introduces Soft Modality-Aware Routing (SMAR), a regularization technique for multimodal Mixture of Experts (MoE) models. It uses Kullback Leibler divergence to control routing probability distributions across modalities, encouraging expert specialization without modifying model architecture or heavily relying on textual data.", "motivation": "Existing methods for building multimodal MoE models either have high training costs or suffer from degraded language capabilities when adapting pretrained models.", "method": "Soft Modality-Aware Routing (SMAR) is proposed as a novel regularization technique that employs Kullback Leibler divergence to control the routing probability distributions across modalities, promoting expert specialization while maintaining language capabilities and efficient multimodal performance.", "result": "Experiments on visual instruction tuning demonstrate that SMAR preserves language ability at 86.6% retention with only 2.5% pure text, outperforming baselines while maintaining strong multimodal performance.", "conclusion": "SMAR offers a practical and efficient solution to balance modality differentiation and language capabilities in multimodal MoE models."}}
{"id": "2506.07406", "pdf": "https://arxiv.org/pdf/2506.07406", "abs": "https://arxiv.org/abs/2506.07406", "authors": ["Yifan Luo", "Zhennan Zhou", "Bin Dong"], "title": "InverseScope: Scalable Activation Inversion for Interpreting Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 8 figures", "summary": "Understanding the internal representations of large language models (LLMs) is\na central challenge in interpretability research. Existing feature\ninterpretability methods often rely on strong assumptions about the structure\nof representations that may not hold in practice. In this work, we introduce\nInverseScope, an assumption-light and scalable framework for interpreting\nneural activations via input inversion. Given a target activation, we define a\ndistribution over inputs that generate similar activations and analyze this\ndistribution to infer the encoded features. To address the inefficiency of\nsampling in high-dimensional spaces, we propose a novel conditional generation\narchitecture that significantly improves sample efficiency compared to previous\nmethods. We further introduce a quantitative evaluation protocol that tests\ninterpretability hypotheses using feature consistency rate computed over the\nsampled inputs. InverseScope scales inversion-based interpretability methods to\nlarger models and practical tasks, enabling systematic and quantitative\nanalysis of internal representations in real-world LLMs.", "AI": {"tldr": "InverseScope\u662f\u4e00\u79cd\u65b0\u7684\u8f7b\u5047\u8bbe\u3001\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u53cd\u6f14\u89e3\u91ca\u795e\u7ecf\u6fc0\u6d3b\u3002\u5b83\u5b9a\u4e49\u4e86\u751f\u6210\u76f8\u4f3c\u6fc0\u6d3b\u7684\u8f93\u5165\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8be5\u5206\u5e03\u63a8\u65ad\u7f16\u7801\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6761\u4ef6\u751f\u6210\u67b6\u6784\uff0c\u63d0\u9ad8\u4e86\u9ad8\u7ef4\u7a7a\u95f4\u91c7\u6837\u7684\u6548\u7387\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u91c7\u6837\u8f93\u5165\u8ba1\u7b97\u7279\u5f81\u4e00\u81f4\u7387\u7684\u5b9a\u91cf\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u7684\u7279\u5f81\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5173\u4e8e\u8868\u793a\u7ed3\u6784\u7684\u5f3a\u5047\u8bbe\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5728\u5b9e\u9645\u4e2d\u53ef\u80fd\u4e0d\u6210\u7acb\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5185\u90e8\u8868\u793a\u3002", "method": "1. \u5f15\u5165InverseScope\u6846\u67b6\uff0c\u901a\u8fc7\u8f93\u5165\u53cd\u6f14\u89e3\u91ca\u795e\u7ecf\u6fc0\u6d3b\u3002\n2. \u5b9a\u4e49\u751f\u6210\u76f8\u4f3c\u6fc0\u6d3b\u7684\u8f93\u5165\u5206\u5e03\u5e76\u5206\u6790\u8be5\u5206\u5e03\u4ee5\u63a8\u65ad\u7f16\u7801\u7279\u5f81\u3002\n3. \u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u6761\u4ef6\u751f\u6210\u67b6\u6784\uff0c\u63d0\u9ad8\u9ad8\u7ef4\u7a7a\u95f4\u91c7\u6837\u6548\u7387\u3002\n4. \u5f15\u5165\u5b9a\u91cf\u8bc4\u4f30\u534f\u8bae\uff0c\u4f7f\u7528\u91c7\u6837\u8f93\u5165\u8ba1\u7b97\u7279\u5f81\u4e00\u81f4\u7387\u6765\u6d4b\u8bd5\u53ef\u89e3\u91ca\u6027\u5047\u8bbe\u3002", "result": "InverseScope\u80fd\u591f\u5c06\u53cd\u6f14\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u6269\u5c55\u5230\u66f4\u5927\u7684\u6a21\u578b\u548c\u5b9e\u7528\u4efb\u52a1\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u73b0\u5b9e\u4e16\u754cLLMs\u5185\u90e8\u8868\u793a\u7684\u7cfb\u7edf\u6027\u548c\u5b9a\u91cf\u5206\u6790\u3002", "conclusion": "InverseScope\u4e3a\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u5047\u8bbe\u3001\u53ef\u6269\u5c55\u7684\u65b0\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u5f3a\u5047\u8bbe\u95ee\u9898\uff0c\u63a8\u52a8\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.07407", "pdf": "https://arxiv.org/pdf/2506.07407", "abs": "https://arxiv.org/abs/2506.07407", "authors": ["Yihong Jin", "Ze Yang", "Juntian Liu", "Xinhe Xu"], "title": "Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the rapid development of multi-cloud environments, it is increasingly\nimportant to ensure the security and reliability of intelligent monitoring\nsystems. In this paper, we propose an anomaly detection and early warning\nmechanism for intelligent monitoring system in multi-cloud environment based on\nLarge-Scale Language Model (LLM). On the basis of the existing monitoring\nframework, the proposed model innovatively introduces a multi-level feature\nextraction method, which combines the natural language processing ability of\nLLM with traditional machine learning methods to enhance the accuracy of\nanomaly detection and improve the real-time response efficiency. By introducing\nthe contextual understanding capabilities of LLMs, the model dynamically adapts\nto different cloud service providers and environments, so as to more\neffectively detect abnormal patterns and predict potential failures.\nExperimental results show that the proposed model is significantly better than\nthe traditional anomaly detection system in terms of detection accuracy and\nlatency, and significantly improves the resilience and active management\nability of cloud infrastructure.", "AI": {"tldr": "In this paper, the authors propose an anomaly detection and early warning mechanism for intelligent monitoring systems in multi-cloud environments using Large-Scale Language Models (LLMs). The model introduces a multi-level feature extraction method combining LLMs with traditional machine learning to improve detection accuracy and real-time response. Experiments demonstrate superior performance over traditional systems.", "motivation": "The motivation of this paper is to enhance the security and reliability of intelligent monitoring systems within multi-cloud environments, addressing the challenges posed by diverse cloud service providers and environments.", "method": "The proposed method involves developing an anomaly detection and early warning mechanism that leverages Large-Scale Language Models (LLMs) alongside traditional machine learning techniques. A multi-level feature extraction approach is used, which combines the natural language processing capabilities of LLMs with conventional methods to detect anomalies more effectively and predict potential failures.", "result": "The experimental results indicate that the proposed model outperforms traditional anomaly detection systems in terms of detection accuracy and latency, thereby improving the resilience and active management capabilities of cloud infrastructure.", "conclusion": "The conclusion drawn from this study is that the integration of LLMs into the monitoring framework significantly enhances the ability to detect anomalies and predict failures in multi-cloud environments, leading to more secure and reliable cloud infrastructures."}}
{"id": "2506.07408", "pdf": "https://arxiv.org/pdf/2506.07408", "abs": "https://arxiv.org/abs/2506.07408", "authors": ["Xiaojun zhou", "Chunna Zhao", "Yaqun Huang", "Chengli Zhou", "Junjie Ye", "Kemeng Xiang"], "title": "Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fractional-order differentiation has many characteristics different from\ninteger-order differentiation. These characteristics can be applied to the\noptimization algorithms of artificial neural networks to obtain better results.\nHowever, due to insufficient theoretical research, at present, there is no\nfractional-order matrix differentiation method that is perfectly compatible\nwith automatic differentiation (Autograd) technology. Therefore, we propose a\nfractional-order matrix differentiation calculation method. This method is\nintroduced by the definition of the integer-order Jacobian matrix. We denote it\nas fractional-order Jacobian matrix differentiation (${{\\bf{J}}^\\alpha }$).\nThrough ${{\\bf{J}}^\\alpha }$, we can carry out the matrix-based\nfractional-order chain rule. Based on the Linear module and the\nfractional-order differentiation, we design the fractional-order Autograd\ntechnology to enable the use of fractional-order differentiation in hidden\nlayers, thereby enhancing the practicality of fractional-order differentiation\nin deep learning. In the experiment, according to the PyTorch framework, we\ndesign fractional-order Linear (FLinear) and replace nn.Linear in the\nmultilayer perceptron with FLinear. Through the qualitative analysis of the\ntraining set and validation set $Loss$, the quantitative analysis of the test\nset indicators, and the analysis of time consumption and GPU memory usage\nduring model training, we verify the superior performance of ${{\\bf{J}}^\\alpha\n}$ and prove that it is an excellent fractional-order gradient descent method\nin the field of deep learning.", "AI": {"tldr": "This paper proposes fractional-order matrix differentiation method, fractional-order Jacobian matrix differentiation (${{\\bf{J}}^\\alpha }$), and fractional-order Autograd technology to enhance the practicality of fractional-order differentiation in deep learning.", "motivation": "Fractional-order differentiation has many characteristics different from integer-order differentiation and can be applied to optimization algorithms of artificial neural networks. However, there is no fractional-order matrix differentiation method that is perfectly compatible with automatic differentiation (Autograd) technology due to insufficient theoretical research.", "method": "The authors propose a fractional-order matrix differentiation calculation method called fractional-order Jacobian matrix differentiation (${{\\bf{J}}^\\alpha }$). Based on this, they design fractional-order Autograd technology which enables the use of fractional-order differentiation in hidden layers. They also design fractional-order Linear (FLinear) and replace nn.Linear in multilayer perceptrons with FLinear for experiments.", "result": "Through qualitative analysis of training and validation set $Loss$, quantitative analysis of test set indicators, and analysis of time consumption and GPU memory usage during model training, the authors verify the superior performance of ${{\\bf{J}}^\\alpha }$. It proves to be an excellent fractional-order gradient descent method in deep learning.", "conclusion": "The proposed fractional-order Jacobian matrix differentiation (${{\\bf{J}}^\\alpha }$) and fractional-order Autograd technology improve the practicality of fractional-order differentiation in deep learning."}}
{"id": "2506.07413", "pdf": "https://arxiv.org/pdf/2506.07413", "abs": "https://arxiv.org/abs/2506.07413", "authors": ["Ziwen Wang", "Jiajun Fan", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "Variational Supervised Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies.", "AI": {"tldr": "VarCon is proposed to solve two limitations of contrastive learning. It achieves state-of-the-art performance in several experiments, yields clearer decision boundaries and demonstrates superior performance in few-shot learning.", "motivation": "Contrastive learning has two key limitations: semantically related instances can inadvertently be pushed apart and excessive reliance on large in-batch negatives and tailored augmentations hinders generalization.", "method": "Reformulate supervised contrastive learning as variational inference over latent class variables and maximize a posterior-weighted evidence lower bound (ELBO) for efficient class-aware matching and fine-grained control over intra-class dispersion in the embedding space.", "result": "(1) Achieves state-of-the-art performance for contrastive learning frameworks; (2) Yields substantially clearer decision boundaries and semantic organization in the embedding space; (3) Demonstrates superior performance in few-shot learning and superior robustness across various augmentation strategies.", "conclusion": "VarCon solves the two limitations of contrastive learning and shows excellent performance in several aspects."}}
{"id": "2506.07416", "pdf": "https://arxiv.org/pdf/2506.07416", "abs": "https://arxiv.org/abs/2506.07416", "authors": ["Jin Huang", "Yuchao Jin", "Le An", "Josh Park"], "title": "LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an efficient Vision-Language Model (VLM) pipeline\nspecifically optimized for deployment on embedded devices, such as those used\nin robotics and autonomous driving. The pipeline significantly reduces the\ncomputational overhead by jointly leveraging patch selection to filter\nirrelevant camera views, a token selection module to reduce input sequence\nlength for the LLM, and speculative decoding to accelerate token generation.\nEvaluation on the NVIDIA DRIVE Thor platform for automonous driving\napplication, our pipeline achieves $2.5\\times$ end-to-end latency reduction\nwithout compromising task accuracy. The speed-up further increases to\n$3.2\\times$ when applying FP8 post-training quantization. These results\ndemonstrate our pipeline as a viable solution for enabling real-time VLM\ndeployment in resource-constrained environments.", "AI": {"tldr": "This paper proposes an efficient Vision-Language Model (VLM) pipeline for embedded devices in robotics and autonomous driving, which reduces computational overhead through patch selection, token selection, and speculative decoding. Evaluated on NVIDIA DRIVE Thor, it achieves 2.5x latency reduction without accuracy loss, increasing to 3.2x with FP8 quantization.", "motivation": "To enable real-time deployment of VLMs on resource-constrained embedded devices used in robotics and autonomous driving by reducing computational overhead without sacrificing task accuracy.", "method": "The method involves a pipeline that uses patch selection to filter irrelevant camera views, a token selection module to shorten input sequences for the LLM, and speculative decoding to speed up token generation. Additionally, FP8 post-training quantization is applied to further enhance performance.", "result": "The pipeline achieves a 2.5x reduction in end-to-end latency on the NVIDIA DRIVE Thor platform for autonomous driving applications without affecting task accuracy. With FP8 post-training quantization, the speed-up increases to 3.2x.", "conclusion": "The proposed pipeline provides a feasible solution for real-time VLM deployment in resource-constrained environments, such as those found in robotics and autonomous driving."}}
{"id": "2506.07417", "pdf": "https://arxiv.org/pdf/2506.07417", "abs": "https://arxiv.org/abs/2506.07417", "authors": ["Nan Sun", "Xixun Lin", "Zhiheng Zhou", "Yanmin Shang", "Zhenlin Cheng", "Yanan Cao"], "title": "Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages,5 figures", "summary": "Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims\nto identify whether incoming data deviates from the distribution of the\nin-distribution (ID) training set, has garnered considerable attention in\nsecurity-sensitive fields. Current OOD detection paradigms primarily focus on\nstatic graphs and confront two critical challenges: i) high bias and high\nvariance caused by single-point estimation, which makes the predictions\nsensitive to randomness in the data; ii) score homogenization resulting from\nthe lack of OOD training data, where the model only learns ID-specific\npatterns, resulting in overall low OOD scores and a narrow score gap between ID\nand OOD data. To tackle these issues, we first investigate OOD detection in\ndynamic graphs through the lens of Evidential Deep Learning (EDL).\nSpecifically, we propose EviSEC, an innovative and effective OOD detector via\nEvidential Spectrum-awarE Contrastive Learning. We design an evidential neural\nnetwork to redefine the output as the posterior Dirichlet distribution,\nexplaining the randomness of inputs through the uncertainty of distribution,\nwhich is overlooked by single-point estimation. Moreover, spectrum-aware\naugmentation module generates OOD approximations to identify patterns with high\nOOD scores, thereby widening the score gap between ID and OOD data and\nmitigating score homogenization. Extensive experiments on real-world datasets\ndemonstrate that EviSAC effectively detects OOD samples in dynamic graphs.", "AI": {"tldr": "The paper proposes EviSEC, an OOD detector for dynamic graphs based on EDL, which addresses challenges of high bias/variance and score homogenization.", "motivation": "Current OOD detection methods mainly focus on static graphs and face issues like high bias and variance from single-point estimation and score homogenization due to lack of OOD training data.", "method": "The authors propose EviSEC, which uses evidential deep learning. It redefines the output as a posterior Dirichlet distribution to explain input randomness and includes a spectrum-aware augmentation module to generate OOD approximations.", "result": "Experiments on real-world datasets show that EviSEC effectively detects OOD samples in dynamic graphs.", "conclusion": "EviSEC is an innovative solution for OOD detection in dynamic graphs that mitigates problems of high bias/variance and score homogenization."}}
{"id": "2506.06446", "pdf": "https://arxiv.org/pdf/2506.06446", "abs": "https://arxiv.org/abs/2506.06446", "authors": ["Ivi Chatzi", "Nina Corvelo Benz", "Stratis Tsirtsis", "Manuel Gomez-Rodriguez"], "title": "Canonical Autoregressive Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "State of the art large language models are trained using large amounts of\ntokens derived from raw text using what is called a tokenizer. Crucially, the\ntokenizer determines the (token) vocabulary a model will use during inference\nas well as, in principle, the (token) language. This is because, while the\ntoken vocabulary may allow for different tokenizations of a string, the\ntokenizer always maps the string to only one of these tokenizations--the\ncanonical tokenization. However, multiple lines of empirical evidence suggest\nthat large language models do not always generate canonical token sequences,\nand this comes with several negative consequences. In this work, we first show\nthat, to generate a canonical token sequence, a model needs to generate\n(partial) canonical token sequences at each step of the autoregressive\ngeneration process underpinning its functioning. Building upon this theoretical\nresult, we introduce canonical sampling, a simple and efficient sampling method\nthat precludes a given model from generating non-canonical token sequences.\nFurther, we also show that, in comparison with standard sampling, the\ndistribution of token sequences generated using canonical sampling is provably\ncloser to the true distribution of token sequences used during training.", "AI": {"tldr": "Large language models sometimes generate non-canonical token sequences, which can lead to negative consequences. This paper introduces canonical sampling, a method that ensures models only generate canonical token sequences, and shows that this approach generates token sequences closer to the true distribution used during training.", "motivation": "The motivation of this paper is to address the issue of large language models generating non-canonical token sequences, which can lead to several negative consequences.", "method": "The authors first establish a theoretical result that to generate a canonical token sequence, a model needs to generate (partial) canonical token sequences at each step of the autoregressive generation process. Based on this, they introduce canonical sampling, a simple and efficient sampling method that prevents a given model from generating non-canonical token sequences.", "result": "Canonical sampling precludes the model from generating non-canonical token sequences. Additionally, compared with standard sampling, the distribution of token sequences generated using canonical sampling is provably closer to the true distribution of token sequences used during training.", "conclusion": "Canonical sampling is a simple and efficient method to ensure that large language models generate canonical token sequences, thereby improving their performance and fidelity to the training data."}}
{"id": "2506.07440", "pdf": "https://arxiv.org/pdf/2506.07440", "abs": "https://arxiv.org/abs/2506.07440", "authors": ["Ruhan Wang", "Zhiyong Wang", "Chengkai Huang", "Rui Wang", "Tong Yu", "Lina Yao", "John C. S. Lui", "Dongruo Zhou"], "title": "Federated In-Context Learning: Iterative Refinement for Improved Answer Quality", "categories": ["cs.LG"], "comment": "27 pages, 16 figures. Accepted to ICML 2025", "summary": "For question-answering (QA) tasks, in-context learning (ICL) enables language\nmodels to generate responses without modifying their parameters by leveraging\nexamples provided in the input. However, the effectiveness of ICL heavily\ndepends on the availability of high-quality examples, which are often scarce\ndue to data privacy constraints, annotation costs, and distribution\ndisparities. A natural solution is to utilize examples stored on client\ndevices, but existing approaches either require transmitting model parameters -\nincurring significant communication overhead - or fail to fully exploit local\ndatasets, limiting their effectiveness. To address these challenges, we propose\nFederated In-Context Learning (Fed-ICL), a general framework that enhances ICL\nthrough an iterative, collaborative process. Fed-ICL progressively refines\nresponses by leveraging multi-round interactions between clients and a central\nserver, improving answer quality without the need to transmit model parameters.\nWe establish theoretical guarantees for the convergence of Fed-ICL and conduct\nextensive experiments on standard QA benchmarks, demonstrating that our\nproposed approach achieves strong performance while maintaining low\ncommunication costs.", "AI": {"tldr": "The paper proposes Federated In-Context Learning (Fed-ICL), a framework that improves question-answering tasks through collaborative, iterative processes between clients and a central server without transmitting model parameters. It achieves strong performance with low communication costs.", "motivation": "In-context learning (ICL) for question-answering tasks relies on high-quality examples which are often limited due to data privacy, annotation costs, and distribution disparities. Current solutions either incur significant communication overhead or fail to fully exploit local datasets.", "method": "The proposed method is called Federated In-Context Learning (Fed-ICL). It enhances ICL via an iterative, collaborative process involving multi-round interactions between clients and a central server to progressively refine responses without the need to transmit model parameters.", "result": "Theoretical guarantees for the convergence of Fed-ICL were established. Extensive experiments on standard QA benchmarks showed that Fed-ICL achieves strong performance while maintaining low communication costs.", "conclusion": "Fed-ICL provides an effective solution to enhance in-context learning for question-answering tasks by leveraging local datasets through a federated approach, achieving good performance with minimal communication overhead."}}
{"id": "2506.07448", "pdf": "https://arxiv.org/pdf/2506.07448", "abs": "https://arxiv.org/abs/2506.07448", "authors": ["T. Duy Nguyen-Hien", "Desi R. Ivanova", "Yee Whye Teh", "Wee Sun Lee"], "title": "Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although large language models (LLMs) are highly interactive and extendable,\ncurrent approaches to ensure reliability in deployments remain mostly limited\nto rejecting outputs with high uncertainty in order to avoid misinformation.\nThis conservative strategy reflects the current lack of tools to systematically\ndistinguish and respond to different sources of uncertainty. In this paper, we\nadvocate for the adoption of Bayesian Modeling of Experiments -- a framework\nthat provides a coherent foundation to reason about uncertainty and clarify the\nreducibility of uncertainty -- for managing and proactively addressing\nuncertainty that arises in LLM deployments. This framework enables LLMs and\ntheir users to take contextually appropriate steps, such as requesting\nclarification, retrieving external information, or refining inputs. By\nsupporting active resolution rather than passive avoidance, it opens the door\nto more reliable, transparent, and broadly applicable LLM systems, particularly\nin high-stakes, real-world settings.", "AI": {"tldr": "This paper proposes using Bayesian Modeling of Experiments to handle uncertainty in large language models (LLMs), shifting from passive avoidance to active resolution of uncertainties, thereby enhancing reliability and transparency.", "motivation": "There is a need for better tools to manage uncertainty in LLM deployments beyond simply rejecting uncertain outputs, as current methods are limited and do not address different sources of uncertainty effectively.", "method": "The paper suggests adopting Bayesian Modeling of Experiments as a framework to systematically distinguish and respond to various sources of uncertainty in LLMs. This involves enabling LLMs to take appropriate actions based on context, such as seeking clarification or retrieving external data.", "result": "Using the Bayesian Modeling framework allows LLMs to actively resolve uncertainties rather than passively avoid them, leading to more reliable and transparent systems that can be applied in high-stakes real-world situations.", "conclusion": "Bayesian Modeling of Experiments provides a promising approach to enhance the management of uncertainty in LLM deployments, promoting more robust and adaptable systems."}}
{"id": "2506.06472", "pdf": "https://arxiv.org/pdf/2506.06472", "abs": "https://arxiv.org/abs/2506.06472", "authors": ["Ziqi Yuan", "Haoyang Zhang", "Yirui Eric Zhou", "Apoorve Mohan", "I-Hsin Chung", "Seetharami Seelam", "Jian Huang"], "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "We present the design and implementation of a new lifetime-aware tensor\noffloading framework for GPU memory expansion using low-cost PCIe-based\nsolid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for\nlarge language model (LLM) training with multiple GPUs and multiple SSDs. Its\ndesign is driven by our observation that the active tensors take only a small\nfraction (1.7% on average) of allocated GPU memory in each LLM training\niteration, the inactive tensors are usually large and will not be used for a\nlong period of time, creating ample opportunities for offloading/prefetching\ntensors to/from slow SSDs without stalling the GPU training process. TERAIO\naccurately estimates the lifetime (active period of time in GPU memory) of each\ntensor with the profiling of the first few iterations in the training process.\nWith the tensor lifetime analysis, TERAIO will generate an optimized tensor\noffloading/prefetching plan and integrate it into the compiled LLM program via\nPyTorch. TERAIO has a runtime tensor migration engine to execute the\noffloading/prefetching plan via GPUDirect storage, which allows direct tensor\nmigration between GPUs and SSDs for alleviating the CPU bottleneck and\nmaximizing the SSD bandwidth utilization. In comparison with state-of-the-art\nstudies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves\nthe training performance of various LLMs by 1.47x on average, and achieves\n80.7% of the ideal performance assuming unlimited GPU memory.", "AI": {"tldr": "The paper introduces TERAIO, a lifetime-aware tensor offloading framework for GPU memory expansion using SSDs. It enhances LLM training performance by profiling tensor lifetimes, generating optimized offloading plans, and integrating them into the program through PyTorch. TERAIO improves training performance by 1.47x on average compared to state-of-the-art methods.", "motivation": "The motivation is to address the limitation of GPU memory in large language model training by utilizing low-cost PCIe-based SSDs as an extension to GPU memory, thereby improving the efficiency and reducing the cost of LLM training.", "method": "TERAIO profiles the first few iterations of the training process to estimate tensor lifetimes, generates an optimized tensor offloading/prefetching plan, integrates it into the compiled LLM program via PyTorch, and uses a runtime tensor migration engine with GPUDirect storage to execute the plan efficiently.", "result": "TERAIO improves the training performance of various LLMs by 1.47x on average compared to state-of-the-art studies like ZeRO-Offload and ZeRO-Infinity, achieving 80.7% of the ideal performance assuming unlimited GPU memory.", "conclusion": "TERAIO successfully demonstrates the potential of using SSDs for GPU memory expansion in LLM training, providing a cost-effective solution that significantly enhances training performance."}}
{"id": "2506.07452", "pdf": "https://arxiv.org/pdf/2506.07452", "abs": "https://arxiv.org/abs/2506.07452", "authors": ["Yuxin Xiao", "Sana Tonekaboni", "Walter Gerych", "Vinith Suriyakumar", "Marzyeh Ghassemi"], "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in jailbreak queries. Although these\nstyle patterns are semantically unrelated to the malicious intents behind\njailbreak queries, their safety impact remains unclear. In this work, we seek\nto understand whether style patterns compromise LLM safety, how superficial\nstyle alignment increases model vulnerability, and how best to mitigate these\nrisks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,\nand find that malicious queries with style patterns inflate the attack success\nrate (ASR) for nearly all models. Notably, ASR inflation correlates with both\nthe length of style patterns and the relative attention an LLM exhibits on\nthem. We then investigate superficial style alignment, and find that\nfine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of\nthose same styles. Finally, we propose SafeStyle, a defense strategy that\nincorporates a small amount of safety training data augmented to match the\ndistribution of style patterns in the fine-tuning data. Across three LLMs and\nfive fine-tuning style settings, SafeStyle consistently outperforms baselines\nin maintaining LLM safety.", "AI": {"tldr": "Large language models (LLMs) can be tricked by style patterns in malicious queries, increasing attack success rates. Fine-tuning on specific styles worsens this vulnerability. The study proposes SafeStyle, a defense strategy to maintain LLM safety.", "motivation": "To understand whether and how style patterns in prompts compromise the safety of large language models (LLMs), and to find ways to mitigate these risks during alignment.", "method": "Evaluated 32 LLMs across seven jailbreak benchmarks to observe the impact of style patterns on attack success rates. Investigated superficial style alignment through fine-tuning with specific styles. Proposed SafeStyle, a defense strategy involving safety training data augmented to match style pattern distributions.", "result": "Malicious queries with style patterns increase attack success rates, with inflation correlating to pattern length and model attention. Fine-tuning on specific styles makes LLMs more vulnerable. SafeStyle outperforms baselines in maintaining LLM safety across different settings.", "conclusion": "Style patterns in prompts can significantly compromise LLM safety. Superficial style alignment increases vulnerability, but using SafeStyle during fine-tuning can effectively mitigate these risks."}}
{"id": "2506.06474", "pdf": "https://arxiv.org/pdf/2506.06474", "abs": "https://arxiv.org/abs/2506.06474", "authors": ["Everett Richards", "Bipul Thapa", "Lena Mashayekhy"], "title": "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "cs.NI", "I.4.8; I.2.10; I.2.11; I.2.9; C.2.4"], "comment": "This paper has been accepted to IEEE EDGE 2025. The final version\n  will be published in IEEE Xplore later this year", "summary": "Accurate and reliable object detection is critical for ensuring the safety\nand efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board\nperception systems have limited accuracy due to occlusions and blind spots,\nwhile cloud-based solutions introduce significant latency, making them\nunsuitable for real-time processing demands required for autonomous driving in\ndynamic environments. To address these challenges, we introduce an innovative\nframework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that\nleverages edge computing and multi-CAV collaboration for real-time,\nmulti-perspective object detection. Our ECOD framework integrates two key\nalgorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and\nVariable Object Tally and Evaluation (VOTE). PACE aggregates detection data\nfrom multiple CAVs on an edge server to enhance perception in scenarios where\nindividual CAVs have limited visibility. VOTE utilizes a consensus-based voting\nmechanism to improve the accuracy of object classification by integrating data\nfrom multiple CAVs. Both algorithms are designed at the edge to operate in\nreal-time, ensuring low-latency and reliable decision-making for CAVs. We\ndevelop a hardware-based controlled testbed consisting of camera-equipped\nrobotic CAVs and an edge server to evaluate the efficacy of our framework. Our\nexperimental results demonstrate the significant benefits of ECOD in terms of\nimproved object classification accuracy, outperforming traditional\nsingle-perspective onboard approaches by up to 75%, while ensuring low-latency,\nedge-driven real-time processing. This research highlights the potential of\nedge computing to enhance collaborative perception for latency-sensitive\nautonomous systems.", "AI": {"tldr": "ECOD\u6846\u67b6\u901a\u8fc7\u8fb9\u7f18\u8ba1\u7b97\u548c\u591aCAV\u534f\u4f5c\u8fdb\u884c\u5b9e\u65f6\u3001\u591a\u89c6\u89d2\u7269\u4f53\u68c0\u6d4b\uff0c\u5305\u542bPACE\u548cVOTE\u4e24\u4e2a\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u7269\u4f53\u5206\u7c7b\u51c6\u786e\u7387\u6bd4\u4f20\u7edf\u65b9\u6cd5\u9ad875%\uff0c\u4e14\u4fdd\u8bc1\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u7cbe\u786e\u53ef\u9760\u7684\u7269\u4f53\u68c0\u6d4b\u6765\u4fdd\u969c\u5b89\u5168\u548c\u6548\u7387\uff0c\u4f46\u4f20\u7edf\u7684\u8f66\u8f7d\u611f\u77e5\u7cfb\u7edf\u56e0\u906e\u6321\u548c\u76f2\u70b9\u9650\u5236\u4e86\u51c6\u786e\u6027\uff0c\u57fa\u4e8e\u4e91\u7aef\u7684\u89e3\u51b3\u65b9\u6848\u53c8\u56e0\u5ef6\u8fdf\u95ee\u9898\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u5904\u7406\u9700\u6c42\u3002", "method": "\u5f15\u5165\u4e86Edge-Enabled Collaborative Object Detection\uff08ECOD\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u548c\u591aCAV\u534f\u4f5c\u8fdb\u884c\u5b9e\u65f6\u3001\u591a\u89c6\u89d2\u7269\u4f53\u68c0\u6d4b\u3002\u5176\u4e2d\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7b97\u6cd5\uff1aPerceptive Aggregation and Collaborative Estimation\uff08PACE\uff09\u548cVariable Object Tally and Evaluation\uff08VOTE\uff09\u3002\u524d\u8005\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u805a\u5408\u591a\u4e2aCAV\u7684\u68c0\u6d4b\u6570\u636e\u4ee5\u589e\u5f3a\u611f\u77e5\u80fd\u529b\uff0c\u540e\u8005\u4f7f\u7528\u57fa\u4e8e\u5171\u8bc6\u7684\u6295\u7968\u673a\u5236\u6765\u63d0\u9ad8\u7269\u4f53\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u5355\u89c6\u89d2\u8f66\u8f7d\u65b9\u6cd5\u76f8\u6bd4\uff0cECOD\u6846\u67b6\u5728\u7269\u4f53\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u63d0\u9ad8\u4e86\u591a\u8fbe75%\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u4f4e\u5ef6\u8fdf\u7684\u8fb9\u7f18\u9a71\u52a8\u5b9e\u65f6\u5904\u7406\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u8fb9\u7f18\u8ba1\u7b97\u6709\u6f5c\u529b\u589e\u5f3a\u5bf9\u5ef6\u8fdf\u654f\u611f\u7684\u81ea\u4e3b\u7cfb\u7edf\u7684\u534f\u4f5c\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2506.07459", "pdf": "https://arxiv.org/pdf/2506.07459", "abs": "https://arxiv.org/abs/2506.07459", "authors": ["Ziwen Wang", "Jiajun Fan", "Ruihan Guo", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "ProteinZero: Self-Improving Protein Generation via Online Reinforcement Learning", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Protein generative models have shown remarkable promise in protein design but\nstill face limitations in success rate, due to the scarcity of high-quality\nprotein datasets for supervised pretraining. We present ProteinZero, a novel\nframework that enables scalable, automated, and continuous self-improvement of\nthe inverse folding model through online reinforcement learning. To achieve\ncomputationally tractable online feedback, we introduce efficient proxy reward\nmodels based on ESM-fold and a novel rapid ddG predictor that significantly\naccelerates evaluation speed. ProteinZero employs a general RL framework\nbalancing multi-reward maximization, KL-divergence from a reference model, and\na novel protein-embedding level diversity regularization that prevents mode\ncollapse while promoting higher sequence diversity. Through extensive\nexperiments, we demonstrate that ProteinZero substantially outperforms existing\nmethods across every key metric in protein design, achieving significant\nimprovements in structural accuracy, designability, thermodynamic stability,\nand sequence diversity. Most impressively, ProteinZero reduces design failure\nrates by approximately 36% - 48% compared to widely-used methods like\nProteinMPNN, ESM-IF and InstructPLM, consistently achieving success rates\nexceeding 90% across diverse and complex protein folds. Notably, the entire RL\nrun on CATH-4.3 can be done with a single 8 X GPU node in under 3 days,\nincluding reward computation. Our work establishes a new paradigm for protein\ndesign where models evolve continuously from their own generated outputs,\nopening new possibilities for exploring the vast protein design space.", "AI": {"tldr": "ProteinZero is a novel framework that uses online reinforcement learning to improve protein design models, significantly outperforming existing methods in structural accuracy, stability, and sequence diversity.", "motivation": "Current protein generative models face limitations due to the lack of high-quality datasets for supervised pretraining, leading to lower success rates in protein design.", "method": "ProteinZero employs a scalable RL framework with efficient proxy reward models based on ESM-fold and a rapid ddG predictor. It balances multi-reward maximization, KL-divergence from a reference model, and protein-embedding level diversity regularization to prevent mode collapse and promote sequence diversity.", "result": "ProteinZero achieves significant improvements in structural accuracy, designability, thermodynamic stability, and sequence diversity, reducing design failure rates by 36%-48% compared to existing methods and achieving success rates exceeding 90%. The entire RL process can be run efficiently within 3 days using an 8 GPU node.", "conclusion": "ProteinZero establishes a new paradigm for protein design, enabling continuous self-improvement through its own generated outputs, thus opening up new possibilities for exploring the vast protein design space."}}
{"id": "2506.06483", "pdf": "https://arxiv.org/pdf/2506.06483", "abs": "https://arxiv.org/abs/2506.06483", "authors": ["Yao Ni", "Song Wen", "Piotr Koniusz", "Anoop Cherian"], "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by\nadapting the model to generate images containing specific subjects. However,\nexisting fine-tuning methods suffer from two key issues: underfitting, where\nthe model fails to reliably capture subject identity, and overfitting, where it\nmemorizes the subject image and reduces background diversity. To address these\nchallenges, we propose two auxiliary consistency losses for diffusion\nfine-tuning. First, a prior consistency regularization loss ensures that the\npredicted diffusion noise for prior (non-subject) images remains consistent\nwith that of the pretrained model, improving fidelity. Second, a subject\nconsistency regularization loss enhances the fine-tuned model's robustness to\nmultiplicative noise modulated latent code, helping to preserve subject\nidentity while improving diversity. Our experimental results demonstrate that\nincorporating these losses into fine-tuning not only preserves subject identity\nbut also enhances image diversity, outperforming DreamBooth in terms of CLIP\nscores, background variation, and overall visual quality.", "AI": {"tldr": "The paper proposes two auxiliary consistency losses for fine-tuning Stable Diffusion model to solve underfitting and overfitting problems, which can preserve subject identity and enhance image diversity.", "motivation": "Fine-tuning methods for Stable Diffusion model have issues of underfitting (failing to capture subject identity) and overfitting (memorizing the subject image and reducing background diversity).", "method": "Propose prior consistency regularization loss and subject consistency regularization loss to be incorporated into the fine-tuning process.", "result": "Experimental results show that the proposed method can preserve subject identity and enhance image diversity, outperforming DreamBooth in CLIP scores, background variation, and overall visual quality.", "conclusion": "The two auxiliary consistency losses are effective in improving the fidelity and diversity of subject-driven image synthesis."}}
{"id": "2506.07467", "pdf": "https://arxiv.org/pdf/2506.07467", "abs": "https://arxiv.org/abs/2506.07467", "authors": ["Jie Peng", "Hongwei Yang", "Jing Zhao", "Hengji Dong", "Hui He", "Weizhe Zhang", "Haoyu He"], "title": "Circumventing Backdoor Space via Weight Symmetry", "categories": ["cs.LG"], "comment": null, "summary": "Deep neural networks are vulnerable to backdoor attacks, where malicious\nbehaviors are implanted during training. While existing defenses can\neffectively purify compromised models, they typically require labeled data or\nspecific training procedures, making them difficult to apply beyond supervised\nlearning settings. Notably, recent studies have shown successful backdoor\nattacks across various learning paradigms, highlighting a critical security\nconcern. To address this gap, we propose Two-stage Symmetry Connectivity (TSC),\na novel backdoor purification defense that operates independently of data\nformat and requires only a small fraction of clean samples. Through theoretical\nanalysis, we prove that by leveraging permutation invariance in neural networks\nand quadratic mode connectivity, TSC amplifies the loss on poisoned samples\nwhile maintaining bounded clean accuracy. Experiments demonstrate that TSC\nachieves robust performance comparable to state-of-the-art methods in\nsupervised learning scenarios. Furthermore, TSC generalizes to self-supervised\nlearning frameworks, such as SimCLR and CLIP, maintaining its strong defense\ncapabilities. Our code is available at https://github.com/JiePeng104/TSC.", "AI": {"tldr": "This paper proposes Two-stage Symmetry Connectivity (TSC), a new method to defend against backdoor attacks in deep neural networks. It works independently of data format and only requires a small amount of clean samples, effective in both supervised and self-supervised learning settings.", "motivation": "Deep neural networks are vulnerable to backdoor attacks which can be implanted during training. Current defenses usually need labeled data or specific training procedures, making them hard to apply beyond supervised learning scenarios.", "method": "The proposed method, TSC, leverages permutation invariance in neural networks and quadratic mode connectivity. It amplifies the loss on poisoned samples while maintaining clean sample accuracy through two stages.", "result": "Experiments show that TSC achieves robust performance comparable to state-of-the-art methods in supervised learning and also generalizes well to self-supervised learning frameworks like SimCLR and CLIP.", "conclusion": "TSC is an effective defense mechanism against backdoor attacks, working independently of data format and requiring only a small fraction of clean samples."}}
{"id": "2506.06484", "pdf": "https://arxiv.org/pdf/2506.06484", "abs": "https://arxiv.org/abs/2506.06484", "authors": ["Manuel Sage", "Khalil Al Handawi", "Yaoyao Fiona Zhao"], "title": "The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "Accepted for publication at the 19th ASME International Conference on\n  Energy Sustainability", "summary": "Power-to-Gas (P2G) technologies gain recognition for enabling the integration\nof intermittent renewables, such as wind and solar, into electricity grids.\nHowever, determining the most cost-effective operation of these systems is\ncomplex due to the volatile nature of renewable energy, electricity prices, and\nloads. Additionally, P2G systems are less efficient in converting and storing\nenergy compared to battery energy storage systems (BESs), and the benefits of\nconverting electricity into gas are not immediately apparent. Deep\nReinforcement Learning (DRL) has shown promise in managing the operation of\nenergy systems amidst these uncertainties. Yet, DRL techniques face\ndifficulties with the delayed reward characteristic of P2G system operation.\nPrevious research has mostly focused on short-term studies that look at the\nenergy conversion process, neglecting the long-term storage capabilities of\nP2G.\n  This study presents a new method by thoroughly examining how DRL can be\napplied to the economic operation of P2G systems, in combination with BESs and\ngas turbines, over extended periods. Through three progressively more complex\ncase studies, we assess the performance of DRL algorithms, specifically Deep\nQ-Networks and Proximal Policy Optimization, and introduce modifications to\nenhance their effectiveness. These modifications include integrating forecasts,\nimplementing penalties on the reward function, and applying strategic cost\ncalculations, all aimed at addressing the issue of delayed rewards. Our\nfindings indicate that while DRL initially struggles with the complex\ndecision-making required for P2G system operation, the adjustments we propose\nsignificantly improve its capability to devise cost-effective operation\nstrategies, thereby unlocking the potential for long-term energy storage in P2G\ntechnologies.", "AI": {"tldr": "The paper explores the application of Deep Reinforcement Learning (DRL) for the economic operation of Power-to-Gas (P2G) systems combined with Battery Energy Storage Systems (BESs) and gas turbines over extended periods. It proposes modifications to DRL algorithms to address delayed reward issues, improving their effectiveness in devising cost-effective operation strategies.", "motivation": "Power-to-Gas (P2G) technologies are recognized for integrating intermittent renewables into electricity grids, but determining their most cost-effective operation is complex due to volatile renewable energy, electricity prices, and loads. P2G systems are less efficient than battery energy storage systems (BESs), and previous research has focused on short-term studies neglecting long-term storage capabilities.", "method": "The study examines how DRL can be applied to the economic operation of P2G systems combined with BESs and gas turbines over extended periods through three progressively more complex case studies. Modifications such as integrating forecasts, implementing penalties on the reward function, and applying strategic cost calculations are introduced to enhance the effectiveness of DRL algorithms.", "result": "Findings indicate that while DRL initially struggles with the complex decision-making required for P2G system operation, the proposed modifications significantly improve its capability to devise cost-effective operation strategies.", "conclusion": "The modifications to DRL algorithms unlock the potential for long-term energy storage in P2G technologies, enhancing their cost-effectiveness and ability to integrate with intermittent renewables."}}
{"id": "2506.07468", "pdf": "https://arxiv.org/pdf/2506.07468", "abs": "https://arxiv.org/abs/2506.07468", "authors": ["Mickel Liu", "Liwei Jiang", "Yancheng Liang", "Simon Shaolei Du", "Yejin Choi", "Tim Althoff", "Natasha Jaques"], "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).", "AI": {"tldr": "Self-RedTeam\u662f\u4e00\u79cd\u5728\u7ebf\u81ea\u6211\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u4ee3\u7406\u7684\u6301\u7eed\u4ea4\u4e92\u5b9e\u73b0\u52a8\u6001\u534f\u540c\u9002\u5e94\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u91c7\u7528\u53cd\u5e94\u5f0f\u3001\u5206\u79bb\u7684\u6d41\u7a0b\uff0c\u5bfc\u81f4\u653b\u51fb\u8005\u8fc7\u5ea6\u62df\u5408\u5230\u8fc7\u65f6\u7684\u9632\u5fa1\uff0c\u800c\u9632\u5fa1\u8005\u5219\u59cb\u7ec8\u843d\u540e\u4e8e\u65b0\u5174\u5a01\u80c1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u534f\u540c\u9002\u5e94\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelf-RedTeam\u7684\u5728\u7ebf\u81ea\u6211\u535a\u5f08\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5c06\u5b89\u5168\u5bf9\u9f50\u89c6\u4e3a\u4e00\u4e2a\u4e8c\u4eba\u96f6\u548c\u6e38\u620f\u3002\u5355\u4e00\u6a21\u578b\u5728\u653b\u51fb\u8005\u548c\u9632\u5fa1\u8005\u89d2\u8272\u4e4b\u95f4\u4ea4\u66ff\uff0c\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\u5e76\u52a0\u4ee5\u9632\u8303\uff0c\u540c\u65f6\u7531\u5956\u52b1\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u88c1\u51b3\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u9690\u85cf\u7684\u601d\u7ef4\u94fe\uff08hidden Chain-of-Thought\uff09\uff0c\u5141\u8bb8\u4ee3\u7406\u79c1\u4e0b\u89c4\u5212\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u6297\u6027\u591a\u6837\u6027\u548c\u51cf\u5c11\u8fc7\u5ea6\u62d2\u7edd\u3002", "result": "\u4e0e\u9488\u5bf9\u9759\u6001\u9632\u5fa1\u8005\u7684\u653b\u51fb\u8005\u76f8\u6bd4\uff0cSelf-RedTeam\u53d1\u73b0\u4e86\u66f4\u591a\u6837\u5316\u7684\u653b\u51fb\uff08+21.8% SBERT\uff09\uff0c\u5e76\u5728\u5b89\u5168\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u9c81\u68d2\u6027\uff08\u4f8b\u5982\uff0c\u5728WildJailBreak\u4e0a\u63d0\u9ad8\u4e86+65.5%\uff09\u3002\u9690\u85cf\u7684\u601d\u7ef4\u94fe\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u5bf9\u6297\u6027\u591a\u6837\u6027\u548c\u51cf\u5c11\u4e86\u8fc7\u5ea6\u62d2\u7edd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4ece\u53cd\u5e94\u5f0f\u4fee\u8865\u8f6c\u5411\u4e3b\u52a8\u534f\u540c\u8fdb\u5316\u662f\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u8bad\u7ec3\u7684\u5173\u952e\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u53ef\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u81ea\u4e3b\u548c\u9c81\u68d2\u7684\u81ea\u6211\u6539\u8fdb\u3002"}}
{"id": "2506.06485", "pdf": "https://arxiv.org/pdf/2506.06485", "abs": "https://arxiv.org/abs/2506.06485", "authors": ["Kaiser Sun", "Fan Bai", "Mark Dredze"], "title": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models frequently rely on both contextual input and parametric\nknowledge to perform tasks. However, these sources can come into conflict,\nespecially when retrieved documents contradict the model's parametric\nknowledge. We propose a diagnostic framework to systematically evaluate LLM\nbehavior under context-memory conflict, where the contextual information\ndiverges from their parametric beliefs. We construct diagnostic data that\nelicit these conflicts and analyze model performance across multiple task\ntypes. Our findings reveal that (1) knowledge conflict has minimal impact on\ntasks that do not require knowledge utilization, (2) model performance is\nconsistently higher when contextual and parametric knowledge are aligned, (3)\nmodels are unable to fully suppress their internal knowledge even when\ninstructed, and (4) providing rationales that explain the conflict increases\nreliance on contexts. These insights raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u51b2\u7a81\u4e2d\u7684\u884c\u4e3a\u8bca\u65ad\u6846\u67b6", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u53c2\u6570\u5316\u4fe1\u5ff5\u76f8\u6096\u65f6\u7684\u884c\u4e3a\u8868\u73b0", "method": "\u6784\u5efa\u5f15\u53d1\u77e5\u8bc6\u51b2\u7a81\u7684\u8bca\u65ad\u6570\u636e\u5e76\u5206\u6790\u6a21\u578b\u5728\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u4e2d\u7684\u8868\u73b0", "result": "(1) \u77e5\u8bc6\u51b2\u7a81\u5bf9\u4e0d\u9700\u8981\u77e5\u8bc6\u5229\u7528\u7684\u4efb\u52a1\u5f71\u54cd\u6700\u5c0f\uff0c(2) \u4e0a\u4e0b\u6587\u548c\u53c2\u6570\u5316\u77e5\u8bc6\u4e00\u81f4\u65f6\u6a21\u578b\u8868\u73b0\u66f4\u4f73\uff0c(3) \u6a21\u578b\u65e0\u6cd5\u5b8c\u5168\u6291\u5236\u5185\u90e8\u77e5\u8bc6\uff0c(4) \u63d0\u4f9b\u89e3\u91ca\u51b2\u7a81\u7684\u7406\u7531\u4f1a\u589e\u52a0\u5bf9\u4e0a\u4e0b\u6587\u7684\u4f9d\u8d56", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f15\u53d1\u4e86\u5bf9\u57fa\u4e8e\u6a21\u578b\u8bc4\u4f30\u7684\u6709\u6548\u6027\u62c5\u5fe7\uff0c\u5e76\u5f3a\u8c03\u5728\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u9700\u8981\u8003\u8651\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898"}}
{"id": "2506.07477", "pdf": "https://arxiv.org/pdf/2506.07477", "abs": "https://arxiv.org/abs/2506.07477", "authors": ["Thomas Zhu", "Joshua Clune", "Jeremy Avigad", "Albert Qiaochu Jiang", "Sean Welleck"], "title": "Premise Selection for a Lean Hammer", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "LeanHammer is available at https://github.com/JOSHCLUNE/LeanHammer", "summary": "Neural methods are transforming automated reasoning for proof assistants, yet\nintegrating these advances into practical verification workflows remains\nchallenging. Hammers are tools that interface with external automatic theorem\nprovers to automate tedious reasoning steps. They have dramatically improved\nproductivity in proof assistants, but the Lean proof assistant still does not\nhave a hammer despite its growing popularity. We present LeanHammer, the first\nend-to-end domain-general hammer for Lean, built on a novel neural premise\nselection system for a hammer in dependent type theory. Unlike existing Lean\npremise selectors, our approach dynamically adapts to user-specific contexts\nand combines with symbolic proof search and reconstruction to create a\npractical hammer. With comprehensive evaluations, we show that our premise\nselector enables LeanHammer to solve 21\\% more goals relative to existing\npremise selectors, and generalize well to diverse domains. Our work bridges the\ngap between neural retrieval and symbolic reasoning, making formal verification\nmore accessible to researchers and practitioners.", "AI": {"tldr": "LeanHammer is the first end-to-end domain-general hammer for Lean, built on a novel neural premise selection system. It solves 21% more goals than existing selectors and generalizes well to diverse domains.", "motivation": "Neural methods are transforming automated reasoning for proof assistants but integrating these advances into practical verification workflows remains challenging. The Lean proof assistant does not have a hammer despite its growing popularity.", "method": "LeanHammer combines a novel neural premise selection system with symbolic proof search and reconstruction. The premise selector dynamically adapts to user-specific contexts.", "result": "LeanHammer solves 21% more goals relative to existing premise selectors and generalizes well to diverse domains.", "conclusion": "This work bridges the gap between neural retrieval and symbolic reasoning, making formal verification more accessible."}}
{"id": "2506.07492", "pdf": "https://arxiv.org/pdf/2506.07492", "abs": "https://arxiv.org/abs/2506.07492", "authors": ["Xiangkun Hu", "Lemin Kong", "Tong He", "David Wipf"], "title": "Explicit Preference Optimization: No Need for an Implicit Reward Model", "categories": ["cs.LG", "stat.ML"], "comment": "arXiv admin note: substantial text overlap with arXiv:2407.09072", "summary": "The generated responses of large language models (LLMs) are often fine-tuned\nto human preferences through a process called reinforcement learning from human\nfeedback (RLHF). As RLHF relies on a challenging training sequence, whereby a\nseparate reward model is independently learned and then later applied to LLM\npolicy updates, ongoing research effort has targeted more straightforward\nalternatives. In this regard, direct preference optimization (DPO) and its many\noffshoots circumvent the need for a separate reward training step. Instead,\nthrough the judicious use of a reparameterization trick that induces an\n\\textit{implicit} reward, DPO and related methods consolidate learning to the\nminimization of a single loss function. And yet despite demonstrable success in\nsome real-world settings, we prove that DPO-based objectives are nonetheless\nsubject to sub-optimal regularization and counter-intuitive interpolation\nbehaviors, underappreciated artifacts of the reparameterizations upon which\nthey are based. To this end, we introduce an \\textit{explicit} preference\noptimization framework termed EXPO that requires no analogous\nreparameterization to achieve an implicit reward. Quite differently, we merely\nposit intuitively-appealing regularization factors from scratch that\ntransparently avoid the potential pitfalls of key DPO variants, provably\nsatisfying regularization desiderata that prior methods do not. Empirical\nresults serve to corroborate our analyses and showcase the efficacy of EXPO.", "AI": {"tldr": "This paper proves that DPO-based objectives have sub-optimal regularization and counter-intuitive interpolation behaviors, and introduces a new framework EXPO which avoids these issues.", "motivation": "The motivation of this paper is to address the limitations of current methods for fine-tuning large language models (LLMs) to human preferences, particularly focusing on direct preference optimization (DPO) and its variants.", "method": "The method involves proving that DPO-based objectives are subject to sub-optimal regularization and counter-intuitive interpolation behaviors due to the reparameterizations they use. To solve this, the paper introduces an explicit preference optimization framework termed EXPO, which does not require reparameterization and instead uses intuitively-appealing regularization factors.", "result": "Empirical results confirm the analyses presented in the paper and demonstrate the efficacy of the EXPO framework.", "conclusion": "In conclusion, the paper highlights the drawbacks of DPO-based methods and presents EXPO as a more effective alternative for optimizing LLMs according to human preferences."}}
{"id": "2506.06509", "pdf": "https://arxiv.org/pdf/2506.06509", "abs": "https://arxiv.org/abs/2506.06509", "authors": ["Jakub Jagielski", "Markus Abel"], "title": "Private GPTs for LLM-driven testing in software development and machine learning", "categories": ["cs.SE", "cs.AI", "I.2.1"], "comment": "5 pages, 10 figures", "summary": "In this contribution, we examine the capability of private GPTs to\nautomatically generate executable test code based on requirements. More\nspecifically, we use acceptance criteria as input, formulated as part of epics,\nor stories, which are typically used in modern development processes. This\ngives product owners, or business intelligence, respectively, a way to directly\nproduce testable criteria through the use of LLMs. We explore the quality of\nthe so-produced tests in two ways: i) directly by letting the LLM generate code\nfrom requirements, ii) through an intermediate step using Gherkin syntax. As a\nresult, it turns out that the two-step procedure yields better results -where\nwe define better in terms of human readability and best coding practices, i.e.\nlines of code and use of additional libraries typically used in testing.\nConcretely, we evaluate prompt effectiveness across two scenarios: a simple\n\"Hello World\" program and a digit classification model, showing that structured\nprompts lead to higher-quality test outputs.", "AI": {"tldr": "This paper explores the use of private GPTs in generating executable test code from requirements, comparing direct generation and a two-step method involving Gherkin syntax. The latter yields better results in terms of human readability and coding practices. They evaluate this across 'Hello World' and digit classification scenarios, showing structured prompts improve test quality.", "motivation": "To assess the capability of private GPTs to automatically generate executable test code based on acceptance criteria formulated as part of epics or stories in modern development processes, providing product owners with a way to produce testable criteria through LLMs.", "method": "Using acceptance criteria as input, they examine two methods for generating tests: i) directly generating code from requirements and ii) using an intermediate step with Gherkin syntax. They then evaluate the effectiveness of these methods across two scenarios: a simple 'Hello World' program and a digit classification model.", "result": "The two-step procedure (using Gherkin syntax) yields better results defined by human readability and adherence to best coding practices, such as lines of code and use of additional libraries in testing.", "conclusion": "Structured prompts lead to higher-quality test outputs when generating executable test code using private GPTs."}}
{"id": "2506.07500", "pdf": "https://arxiv.org/pdf/2506.07500", "abs": "https://arxiv.org/abs/2506.07500", "authors": ["Shakir Yousefi", "Andreas Plesner", "Till Aczel", "Roger Wattenhofer"], "title": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks", "categories": ["cs.LG", "cs.PF"], "comment": null, "summary": "Modern neural networks demonstrate state-of-the-art performance on numerous\nexisting benchmarks; however, their high computational requirements and energy\nconsumption prompt researchers to seek more efficient solutions for real-world\ndeployment. Logic gate networks (LGNs) learns a large network of logic gates\nfor efficient image classification. However, learning a network that can solve\na simple problem like CIFAR-10 can take days to weeks to train. Even then,\nalmost half of the network remains unused, causing a discretization gap. This\ndiscretization gap hinders real-world deployment of LGNs, as the performance\ndrop between training and inference negatively impacts accuracy. We inject\nGumbel noise with a straight-through estimator during training to significantly\nspeed up training, improve neuron utilization, and decrease the discretization\ngap. We theoretically show that this results from implicit Hessian\nregularization, which improves the convergence properties of LGNs. We train\nnetworks $4.5 \\times$ faster in wall-clock time, reduce the discretization gap\nby $98\\%$, and reduce the number of unused gates by $100\\%$.", "AI": {"tldr": "Gumbel noise with a straight-through estimator is injected during training of Logic Gate Networks (LGNs) to speed up training, improve neuron utilization, and decrease the discretization gap.", "motivation": "Modern neural networks have high computational requirements and energy consumption, prompting researchers to seek more efficient solutions. LGNs offer efficient image classification but suffer from long training times and a discretization gap.", "method": "Inject Gumbel noise with a straight-through estimator during training of LGNs to significantly speed up training, improve neuron utilization, and decrease the discretization gap.", "result": "Trained networks 4.5 times faster in wall-clock time, reduced the discretization gap by 98%, and reduced the number of unused gates by 100%.", "conclusion": "The injection of Gumbel noise results in implicit Hessian regularization, which improves the convergence properties of LGNs and makes them more viable for real-world deployment."}}
{"id": "2506.06522", "pdf": "https://arxiv.org/pdf/2506.06522", "abs": "https://arxiv.org/abs/2506.06522", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Syed Zawad", "Farhan Ahmed", "Heiko Ludwig", "Holger Boche"], "title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work on large language models (LLMs) has increasingly focused on\npost-training and alignment with datasets curated to enhance instruction\nfollowing, world knowledge, and specialized skills. However, most post-training\ndatasets used in leading open- and closed-source LLMs remain inaccessible to\nthe public, with limited information about their construction process. This\nlack of transparency has motivated the recent development of open-source\npost-training corpora. While training on these open alternatives can yield\nperformance comparable to that of leading models, systematic comparisons remain\nchallenging due to the significant computational cost of conducting them\nrigorously at scale, and are therefore largely absent. As a result, it remains\nunclear how specific samples, task types, or curation strategies influence\ndownstream performance when assessing data quality. In this work, we conduct\nthe first comprehensive side-by-side analysis of two prominent open\npost-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie\nframework, we annotate each sample with detailed quality metrics, including\nturn structure (single-turn vs. multi-turn), task category, input quality, and\nresponse quality, and we derive statistics that reveal structural and\nqualitative similarities and differences between the two datasets. Based on\nthese insights, we design a principled curation recipe that produces a new data\nmixture, TuluTalk, which contains 14% fewer samples than either source dataset\nwhile matching or exceeding their performance on key benchmarks. Our findings\noffer actionable insights for constructing more effective post-training\ndatasets that improve model performance within practical resource limits. To\nsupport future research, we publicly release both the annotated source datasets\nand our curated TuluTalk mixture.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5bf9\u6bd4\u4e86\u4e24\u4e2a\u5f00\u6e90\u5fae\u8c03\u6570\u636e\u96c6Tulu-3-SFT-Mix\u548cSmolTalk\uff0c\u5e76\u901a\u8fc7Magpie\u6846\u67b6\u5bf9\u6837\u672c\u8fdb\u884c\u8be6\u7ec6\u6807\u6ce8\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u7ed3\u6784\u4e0e\u8d28\u91cf\u5dee\u5f02\u3002\u57fa\u4e8e\u6b64\uff0c\u4ed6\u4eec\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6TuluTalk\uff0c\u5728\u51cf\u5c1114%\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4e0d\u964d\u53cd\u5347\u3002\u7814\u7a76\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u5982\u4f55\u6709\u6548\u6784\u5efa\u5fae\u8c03\u6570\u636e\u96c6\u7684\u5b9e\u7528\u89c1\u89e3\uff0c\u5e76\u516c\u5f00\u53d1\u5e03\u4e86\u6240\u6709\u76f8\u5173\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5fae\u8c03\u6570\u636e\u96c6\u5927\u591a\u4e0d\u516c\u5f00\uff0c\u4e14\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u5c3d\u7ba1\u4f7f\u7528\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u53ef\u4ee5\u8fbe\u5230\u63a5\u8fd1\u9876\u5c16\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4f46\u7cfb\u7edf\u6027\u6bd4\u8f83\u4ecd\u56e0\u8ba1\u7b97\u6210\u672c\u9ad8\u800c\u96be\u4ee5\u5b9e\u73b0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u5206\u6790\u6765\u7406\u89e3\u4e0d\u540c\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Magpie\u6846\u67b6\u5bf9Tulu-3-SFT-Mix\u548cSmolTalk\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u8fdb\u884c\u6807\u6ce8\uff0c\u5305\u62ec\u5bf9\u8bdd\u8f6e\u6b21\u7ed3\u6784\u3001\u4efb\u52a1\u7c7b\u522b\u3001\u8f93\u5165\u8d28\u91cf\u53ca\u54cd\u5e94\u8d28\u91cf\u7b49\u6307\u6807\u3002\u57fa\u4e8e\u6807\u6ce8\u7ed3\u679c\uff0c\u7edf\u8ba1\u5e76\u5206\u6790\u4e24\u8005\u7684\u7ed3\u6784\u4e0e\u8d28\u91cf\u5f02\u540c\uff0c\u5e76\u636e\u6b64\u8bbe\u8ba1\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u6df7\u5408\u7b56\u7565\u4ee5\u751f\u6210TuluTalk\u6570\u636e\u96c6\u3002", "result": "\u65b0\u6570\u636e\u96c6TuluTalk\u5728\u51cf\u5c1114%\u6837\u672c\u7684\u540c\u65f6\uff0c\u4e8e\u5173\u952e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u539f\u6570\u636e\u96c6\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u6548\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5f71\u54cd\u6570\u636e\u8d28\u91cf\u7684\u5177\u4f53\u56e0\u7d20\uff0c\u5982\u5bf9\u8bdd\u8f6e\u6b21\u7ed3\u6784\u548c\u4efb\u52a1\u7c7b\u522b\u5206\u5e03\u7b49\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u5fae\u8c03\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\uff0c\u540c\u65f6\u516c\u5f00\u53d1\u5e03\u6240\u6709\u6807\u6ce8\u6570\u636e\u548c\u65b0\u6570\u636e\u96c6TuluTalk\uff0c\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u5de5\u4f5c\u3002"}}
{"id": "2506.07501", "pdf": "https://arxiv.org/pdf/2506.07501", "abs": "https://arxiv.org/abs/2506.07501", "authors": ["Libo Wang"], "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "The relevant code has been uploaded to the publicly available GitHub\n  repository. The link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/GoCE", "summary": "In view of the problem that each subchain in the chain-of-model (CoM) relies\nonly on the information of the previous subchain and may lose long-range\ndependencies due to the causal mask blocking the global context flow between\nmulti-level subchains, this work proposes a graph of causal evolution (GoCE).\nIts core principle is to map the implicit token representation into a\ndifferentiable and sparse causal adjacency matrix, then permeate causal\nconstraints through each layer of calculation using causal-masked attention and\ncausal-MoE. By combining intervention consistency loss test and self-evolution\ngate, the dynamic balance between causal structure learning and adaptive\nupdating of transformer architecture is realized. The researcher built\nexperimental environments in sandboxes built with Claude Sonnet 4,\no4-mini-high, and DeepSeek R1 respectively with the transformer variant\narchitecture introduced in GoCE. It is evaluated on publicly available datasets\nincluding CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the\nbaseline LLMs. The finding proves that GoCE strengthens the transformer's\nability to capture long-range causal dependencies, while the ability to\nself-evolve is improved. It not only surpasses the design of CoM in terms of\ndesign principles, but also provides experience for future research on causal\nlearning and continuous adaptive improvement.", "AI": {"tldr": "In this paper, researchers propose a new method called graph of causal evolution (GoCE) to solve the problem in chain-of-model (CoM). GoCE can strengthen transformer's ability to capture long-range causal dependencies and improve its self-evolution ability. It surpasses CoM in design principles and provides valuable experience for future research.", "motivation": "The motivation of this paper is to address the limitation in CoM where each subchain only depends on the previous subchain information and may lose long-range dependencies due to the causal mask blocking global context flow between multi-level subchains.", "method": "The proposed method, GoCE, maps token representation into a differentiable and sparse causal adjacency matrix, permeates causal constraints through each layer using causal-masked attention and causal-MoE, combines intervention consistency loss test and self-evolution gate to balance causal structure learning and adaptive updating of transformer architecture.", "result": "GoCE was evaluated on datasets including CLUTRR, CLADDER, EX-FEVER, and CausalQA. The results show that it strengthens transformer's ability to capture long-range causal dependencies and improves its self-evolution ability.", "conclusion": "GoCE not only surpasses CoM in terms of design principles but also provides valuable experience for future research on causal learning and continuous adaptive improvement."}}
{"id": "2506.07505", "pdf": "https://arxiv.org/pdf/2506.07505", "abs": "https://arxiv.org/abs/2506.07505", "authors": ["Perry Dong", "Alec M. Lessing", "Annie S. Chen", "Chelsea Finn"], "title": "Reinforcement Learning via Implicit Imitation Guidance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study the problem of sample efficient reinforcement learning, where prior\ndata such as demonstrations are provided for initialization in lieu of a dense\nreward signal. A natural approach is to incorporate an imitation learning\nobjective, either as regularization during training or to acquire a reference\npolicy. However, imitation learning objectives can ultimately degrade long-term\nperformance, as it does not directly align with reward maximization. In this\nwork, we propose to use prior data solely for guiding exploration via noise\nadded to the policy, sidestepping the need for explicit behavior cloning\nconstraints. The key insight in our framework, Data-Guided Noise (DGN), is that\ndemonstrations are most useful for identifying which actions should be\nexplored, rather than forcing the policy to take certain actions. Our approach\nachieves up to 2-3x improvement over prior reinforcement learning from offline\ndata methods across seven simulated continuous control tasks.", "AI": {"tldr": "\u5728\u6837\u672c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u4f7f\u7528\u5148\u9a8c\u6570\u636e\u5f15\u5bfc\u63a2\u7d22\u800c\u975e\u76f4\u63a5\u6a21\u4eff\uff0c\u53ef\u63d0\u5347\u957f\u671f\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5229\u7528\u5148\u9a8c\u6570\u636e\uff08\u5982\u6f14\u793a\uff09\u6765\u521d\u59cb\u5316\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\uff0c\u4ee5\u66ff\u4ee3\u5bc6\u96c6\u5956\u52b1\u4fe1\u53f7\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "method": "\u63d0\u51faData-Guided Noise (DGN)\u6846\u67b6\uff0c\u901a\u8fc7\u5411\u7b56\u7565\u6dfb\u52a0\u566a\u58f0\u6765\u5229\u7528\u5148\u9a8c\u6570\u636e\u6307\u5bfc\u63a2\u7d22\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u8fdb\u884c\u884c\u4e3a\u514b\u9686\u7ea6\u675f\u3002", "result": "\u5728\u4e03\u4e2a\u6a21\u62df\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\uff0c\u76f8\u8f83\u4e8e\u5176\u4ed6\u57fa\u4e8e\u79bb\u7ebf\u6570\u636e\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u4e862-3\u500d\u3002", "conclusion": "\u6f14\u793a\u6570\u636e\u5bf9\u4e8e\u8bc6\u522b\u5e94\u63a2\u7d22\u7684\u52a8\u4f5c\u6700\u6709\u7528\uff0c\u800c\u4e0d\u662f\u5f3a\u5236\u7b56\u7565\u6267\u884c\u7279\u5b9a\u52a8\u4f5c\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u66f4\u597d\u5730\u5b9e\u73b0\u5956\u52b1\u6700\u5927\u5316\u3002"}}
{"id": "2506.06539", "pdf": "https://arxiv.org/pdf/2506.06539", "abs": "https://arxiv.org/abs/2506.06539", "authors": ["Yijie Hao", "Haofei Yu", "Jiaxuan You"], "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "When exposed to complex queries containing multiple conditions, today's large\nlanguage models (LLMs) tend to produce responses that only partially satisfy\nthe query while neglecting certain conditions. We therefore introduce the\nconcept of Intent Hallucination. In this phenomenon, LLMs either omit\n(neglecting to address certain parts) or misinterpret (responding to invented\nquery parts) elements of the given query, leading to intent hallucinated\ngeneration. To systematically evaluate intent hallucination, we introduce\nFAITHQA, a novel benchmark for intent hallucination that contains 20,068\nproblems, covering both query-only and retrieval-augmented generation (RAG)\nsetups with varying topics and difficulty. FAITHQA is the first hallucination\nbenchmark that goes beyond factual verification, tailored to identify the\nfundamental cause of intent hallucination. By evaluating various LLMs on\nFAITHQA, we find that (1) intent hallucination is a common issue even for\nstate-of-the-art models, and (2) the phenomenon stems from omission or\nmisinterpretation of LLMs. To facilitate future research, we introduce an\nautomatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting\nintent hallucination. Human evaluation results demonstrate that CONSTRAINT\nSCORE is closer to human performance for intent hallucination compared to\nbaselines.", "AI": {"tldr": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u65f6\u5bb9\u6613\u51fa\u73b0\u610f\u56fe\u5e7b\u89c9\u73b0\u8c61\uff0c\u5373\u5ffd\u7565\u6216\u8bef\u89e3\u67e5\u8be2\u4e2d\u7684\u67d0\u4e9b\u90e8\u5206\u3002\u4e3a\u8bc4\u4f30\u6b64\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86FAITHQA\u57fa\u51c6\u6d4b\u8bd5\u548cCONSTRAINT SCORE\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u610f\u56fe\u5e7b\u89c9\u7684\u666e\u904d\u6027\u548c\u6210\u56e0\uff0c\u5e76\u63a8\u52a8\u672a\u6765\u76f8\u5173\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e94\u5bf9\u590d\u6742\u67e5\u8be2\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u5168\u9762\u6ee1\u8db3\u67e5\u8be2\u8981\u6c42\uff0c\u5b58\u5728\u5ffd\u7565\u6216\u8bef\u89e3\u67e5\u8be2\u6761\u4ef6\u7684\u95ee\u9898\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63d0\u51fa\u610f\u56fe\u5e7b\u89c9\u7684\u6982\u5ff5\u4ee5\u89e3\u51b3\u8fd9\u4e00\u4e0d\u8db3\u3002", "method": "1. \u63d0\u51faFAITHQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b20,068\u4e2a\u95ee\u9898\uff0c\u8986\u76d6\u591a\u79cd\u4e3b\u9898\u548c\u96be\u5ea6\uff0c\u9002\u7528\u4e8e\u67e5\u8be2\u751f\u6210\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u573a\u666f\u3002\n2. \u5f15\u5165CONSTRAINT SCORE\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u68c0\u6d4b\u610f\u56fe\u5e7b\u89c9\u3002\n3. \u901a\u8fc7\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1CONSTRAINT SCORE\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8868\u73b0\u3002", "result": "1. FAITHQA\u8bc4\u4f30\u663e\u793a\u610f\u56fe\u5e7b\u89c9\u662f\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u4e5f\u666e\u904d\u5b58\u5728\u7684\u95ee\u9898\u3002\n2. \u610f\u56fe\u5e7b\u89c9\u4e3b\u8981\u6e90\u4e8e\u6a21\u578b\u5bf9\u67e5\u8be2\u5185\u5bb9\u7684\u9057\u6f0f\u6216\u8bef\u89e3\u3002\n3. CONSTRAINT SCORE\u5728\u68c0\u6d4b\u610f\u56fe\u5e7b\u89c9\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u66f4\u4e3a\u4e00\u81f4\u3002", "conclusion": "\u610f\u56fe\u5e7b\u89c9\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002FAITHQA\u548cCONSTRAINT SCORE\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u5de5\u5177\u548c\u65b9\u5411\u3002"}}
{"id": "2506.07517", "pdf": "https://arxiv.org/pdf/2506.07517", "abs": "https://arxiv.org/abs/2506.07517", "authors": ["Shuqiang Zhang", "Yuchao Zhang", "Jinkun Chen", "Haochen Sui"], "title": "Addressing Correlated Latent Exogenous Variables in Debiased Recommender Systems", "categories": ["cs.LG", "cs.IR"], "comment": "In Proceedings of the 31st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining V.2 (KDD '25), August 3--7, 2025, Toronto, ON,\n  Canada", "summary": "Recommendation systems (RS) aim to provide personalized content, but they\nface a challenge in unbiased learning due to selection bias, where users only\ninteract with items they prefer. This bias leads to a distorted representation\nof user preferences, which hinders the accuracy and fairness of\nrecommendations. To address the issue, various methods such as error imputation\nbased, inverse propensity scoring, and doubly robust techniques have been\ndeveloped. Despite the progress, from the structural causal model perspective,\nprevious debiasing methods in RS assume the independence of the exogenous\nvariables. In this paper, we release this assumption and propose a learning\nalgorithm based on likelihood maximization to learn a prediction model. We\nfirst discuss the correlation and difference between unmeasured confounding and\nour scenario, then we propose a unified method that effectively handles latent\nexogenous variables. Specifically, our method models the data generation\nprocess with latent exogenous variables under mild normality assumptions. We\nthen develop a Monte Carlo algorithm to numerically estimate the likelihood\nfunction. Extensive experiments on synthetic datasets and three real-world\ndatasets demonstrate the effectiveness of our proposed method. The code is at\nhttps://github.com/WallaceSUI/kdd25-background-variable.", "AI": {"tldr": "The paper proposes a learning algorithm based on likelihood maximization to address selection bias in recommendation systems by considering latent exogenous variables.", "motivation": "Recommendation systems face challenges in unbiased learning due to selection bias, which leads to distorted representation of user preferences. Existing debiasing methods assume the independence of exogenous variables, which is not always valid.", "method": "The authors propose a unified method that models the data generation process with latent exogenous variables under mild normality assumptions and develop a Monte Carlo algorithm to numerically estimate the likelihood function.", "result": "Extensive experiments on synthetic datasets and three real-world datasets demonstrate the effectiveness of the proposed method.", "conclusion": "The proposed learning algorithm based on likelihood maximization effectively handles latent exogenous variables and improves the accuracy and fairness of recommendations."}}
{"id": "2506.06540", "pdf": "https://arxiv.org/pdf/2506.06540", "abs": "https://arxiv.org/abs/2506.06540", "authors": ["Patrick Y. Wu"], "title": "Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "19 pages, 6 figures", "summary": "After a disruptive event or shock, such as the Department of Government\nEfficiency (DOGE) federal layoffs of 2025, expert judgments are colored by\nknowledge of the outcome. This can make it difficult or impossible to\nreconstruct the pre-event perceptions needed to study the factors associated\nwith the event. This position paper argues that large language models (LLMs),\ntrained on vast amounts of digital media data, can be a viable substitute for\nexpert political surveys when a shock disrupts traditional measurement. We\nanalyze the DOGE layoffs as a specific case study for this position. We use\npairwise comparison prompts with LLMs and derive ideology scores for federal\nexecutive agencies. These scores replicate pre-layoff expert measures and\npredict which agencies were targeted by DOGE. We also use this same approach\nand find that the perceptions of certain federal agencies as knowledge\ninstitutions predict which agencies were targeted by DOGE, even when\ncontrolling for ideology. This case study demonstrates that using LLMs allows\nus to rapidly and easily test the associated factors hypothesized behind the\nshock. More broadly, our case study of this recent event exemplifies how LLMs\noffer insights into the correlational factors of the shock when traditional\nmeasurement techniques fail. We conclude by proposing a two-part criterion for\nwhen researchers can turn to LLMs as a substitute for expert political surveys.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u4f5c\u4e3a\u4e13\u5bb6\u653f\u6cbb\u8c03\u67e5\u7684\u53ef\u884c\u66ff\u4ee3\u54c1\uff0c\u7279\u522b\u662f\u5728\u7a81\u53d1\u4e8b\u4ef6\u6216\u51b2\u51fb\u4e8b\u4ef6\u7834\u574f\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u65f6\u3002\u901a\u8fc7\u5206\u67902025\u5e74\u7684DOGE\u8054\u90a6\u88c1\u5458\u4e8b\u4ef6\uff0c\u7814\u7a76\u53d1\u73b0\u4f7f\u7528LLMs\u5f97\u51fa\u7684\u610f\u8bc6\u5f62\u6001\u8bc4\u5206\u80fd\u591f\u590d\u5236\u88c1\u5458\u524d\u4e13\u5bb6\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u5e76\u9884\u6d4b\u54ea\u4e9b\u673a\u6784\u4f1a\u88abDOGE\u9488\u5bf9\u3002\u6b64\u5916\uff0c\u8fd9\u79cd\u65b9\u6cd5\u8fd8\u63ed\u793a\u4e86\u67d0\u4e9b\u8054\u90a6\u673a\u6784\u88ab\u89c6\u4e3a\u77e5\u8bc6\u673a\u6784\u7684\u77e5\u89c9\u4e5f\u4e0e\u88abDOGE\u9488\u5bf9\u6709\u5173\uff0c\u5373\u4f7f\u63a7\u5236\u4e86\u610f\u8bc6\u5f62\u6001\u56e0\u7d20\u3002\u8be5\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cLLMs\u53ef\u4ee5\u5e2e\u52a9\u5feb\u901f\u8f7b\u677e\u5730\u6d4b\u8bd5\u51b2\u51fb\u80cc\u540e\u7684\u5047\u8bbe\u56e0\u7d20\uff0c\u5e76\u5728\u4f20\u7edf\u6d4b\u91cf\u6280\u672f\u5931\u6548\u65f6\u63d0\u4f9b\u5bf9\u51b2\u51fb\u76f8\u5173\u56e0\u7d20\u7684\u89c1\u89e3\u3002\u6700\u540e\uff0c\u6587\u7ae0\u63d0\u51fa\u4e86\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u5728\u4f55\u79cd\u60c5\u51b5\u4e0b\u5c06LLMs\u4f5c\u4e3a\u4e13\u5bb6\u653f\u6cbb\u8c03\u67e5\u7684\u66ff\u4ee3\u54c1\u7684\u53cc\u91cd\u8981\u6c42\u3002", "motivation": "\u5728\u53d1\u751f\u7834\u574f\u6027\u4e8b\u4ef6\u6216\u51b2\u51fb\u540e\uff0c\u4e13\u5bb6\u5224\u65ad\u5f80\u5f80\u53d7\u5230\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u8fd9\u4f7f\u5f97\u91cd\u5efa\u4e8b\u4ef6\u524d\u7684\u8ba4\u77e5\u53d8\u5f97\u56f0\u96be\u751a\u81f3\u4e0d\u53ef\u80fd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66ff\u4ee3\u4f20\u7edf\u7684\u4e13\u5bb6\u653f\u6cbb\u8c03\u67e5\uff0c\u4ee5\u4fbf\u7814\u7a76\u4e0e\u4e8b\u4ef6\u76f8\u5173\u7684\u56e0\u7d20\u3002", "method": "\u4f7f\u7528\u6210\u5bf9\u6bd4\u8f83\u63d0\u793a\u7b26\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u4ea4\u4e92\uff0c\u4ece\u800c\u4e3a\u8054\u90a6\u6267\u884c\u673a\u6784\u751f\u6210\u610f\u8bc6\u5f62\u6001\u8bc4\u5206\u3002\u8fd9\u4e9b\u8bc4\u5206\u7528\u4e8e\u590d\u5236\u88c1\u5458\u524d\u4e13\u5bb6\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u5e76\u9884\u6d4b\u54ea\u4e9b\u673a\u6784\u53ef\u80fd\u53d7\u5230DOGE\u88c1\u5458\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u91c7\u7528\u76f8\u540c\u7684\u65b9\u6cd5\u63a2\u8ba8\u8054\u90a6\u673a\u6784\u4f5c\u4e3a\u77e5\u8bc6\u673a\u6784\u7684\u77e5\u89c9\u662f\u5426\u4e0e\u88c1\u5458\u76ee\u6807\u9009\u62e9\u6709\u5173\u3002", "result": "LLMs\u751f\u6210\u7684\u610f\u8bc6\u5f62\u6001\u8bc4\u5206\u6210\u529f\u590d\u5236\u4e86\u88c1\u5458\u524d\u4e13\u5bb6\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u5e76\u80fd\u591f\u9884\u6d4b\u54ea\u4e9b\u673a\u6784\u88abDOGE\u9488\u5bf9\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u67d0\u4e9b\u8054\u90a6\u673a\u6784\u88ab\u89c6\u4e3a\u77e5\u8bc6\u673a\u6784\u7684\u77e5\u89c9\u4e5f\u4e0e\u88abDOGE\u88c1\u5458\u6709\u5173\uff0c\u5373\u4f7f\u5728\u63a7\u5236\u610f\u8bc6\u5f62\u6001\u56e0\u7d20\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u4f5c\u4e3a\u4e13\u5bb6\u653f\u6cbb\u8c03\u67e5\u7684\u53ef\u884c\u66ff\u4ee3\u54c1\uff0c\u7279\u522b\u662f\u5728\u51b2\u51fb\u4e8b\u4ef6\u7834\u574f\u4f20\u7edf\u6d4b\u91cf\u65b9\u6cd5\u65f6\u3002\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u91cd\u8981\u6c42\uff0c\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5224\u65ad\u4f55\u65f6\u53ef\u4ee5\u5c06LLMs\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.07534", "pdf": "https://arxiv.org/pdf/2506.07534", "abs": "https://arxiv.org/abs/2506.07534", "authors": ["Cl\u00e9ment Bonet", "Christophe Vauthier", "Anna Korba"], "title": "Flowing Datasets with Wasserstein over Wasserstein Gradient Flows", "categories": ["cs.LG", "math.OC", "stat.ML"], "comment": "Accepted as an oral at ICML2025", "summary": "Many applications in machine learning involve data represented as probability\ndistributions. The emergence of such data requires radically novel techniques\nto design tractable gradient flows on probability distributions over this type\nof (infinite-dimensional) objects. For instance, being able to flow labeled\ndatasets is a core task for applications ranging from domain adaptation to\ntransfer learning or dataset distillation. In this setting, we propose to\nrepresent each class by the associated conditional distribution of features,\nand to model the dataset as a mixture distribution supported on these classes\n(which are themselves probability distributions), meaning that labeled datasets\ncan be seen as probability distributions over probability distributions. We\nendow this space with a metric structure from optimal transport, namely the\nWasserstein over Wasserstein (WoW) distance, derive a differential structure on\nthis space, and define WoW gradient flows. The latter enables to design\ndynamics over this space that decrease a given objective functional. We apply\nour framework to transfer learning and dataset distillation tasks, leveraging\nour gradient flow construction as well as novel tractable functionals that take\nthe form of Maximum Mean Discrepancies with Sliced-Wasserstein based kernels\nbetween probability distributions.", "AI": {"tldr": "This paper proposes a novel method to design gradient flows on probability distributions, specifically for labeled datasets in machine learning applications such as domain adaptation and transfer learning. By representing each class as a conditional distribution of features and modeling the dataset as a mixture distribution supported on these classes, the authors introduce the Wasserstein over Wasserstein (WoW) distance and define WoW gradient flows. This framework is applied to transfer learning and dataset distillation tasks using Maximum Mean Discrepancies with Sliced-Wasserstein based kernels.", "motivation": "The motivation of this paper stems from the need for new techniques to handle data represented as probability distributions in machine learning applications. Specifically, the challenge lies in designing tractable gradient flows over infinite-dimensional objects like labeled datasets.", "method": "The method involves representing each class by its associated conditional distribution of features and modeling the dataset as a mixture distribution supported on these classes. The space is endowed with a metric structure using the Wasserstein over Wasserstein (WoW) distance from optimal transport theory. A differential structure is derived, and WoW gradient flows are defined to decrease a given objective functional.", "result": "The proposed framework was successfully applied to transfer learning and dataset distillation tasks, leveraging Maximum Mean Discrepancies with Sliced-Wasserstein based kernels. This demonstrates the potential of the WoW gradient flows in practical machine learning scenarios.", "conclusion": "In conclusion, the paper presents a novel approach to manage gradient flows on probability distributions, particularly for labeled datasets in machine learning. By introducing the WoW distance and defining WoW gradient flows, the authors provide a powerful tool for tasks such as transfer learning and dataset distillation."}}
{"id": "2506.06541", "pdf": "https://arxiv.org/pdf/2506.06541", "abs": "https://arxiv.org/abs/2506.06541", "authors": ["Eugenie Lai", "Gerardo Vitagliano", "Ziyu Zhang", "Sivaprasad Sudhir", "Om Chabra", "Anna Zeng", "Anton A. Zabreyko", "Chenning Li", "Ferdi Kossmann", "Jialin Ding", "Jun Chen", "Markos Markakis", "Matthew Russo", "Weiyang Wang", "Ziniu Wu", "Michael J. Cafarella", "Lei Cao", "Samuel Madden", "Tim Kraska"], "title": "KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes", "categories": ["cs.DB", "cs.AI", "cs.MA"], "comment": null, "summary": "Constructing real-world data-to-insight pipelines often involves data\nextraction from data lakes, data integration across heterogeneous data sources,\nand diverse operations from data cleaning to analysis. The design and\nimplementation of data science pipelines require domain knowledge, technical\nexpertise, and even project-specific insights. AI systems have shown remarkable\nreasoning, coding, and understanding capabilities. However, it remains unclear\nto what extent these capabilities translate into successful design and\nexecution of such complex pipelines. We introduce KRAMABENCH: a benchmark\ncomposed of 104 manually-curated real-world data science pipelines spanning\n1700 data files from 24 data sources in 6 different domains. We show that these\npipelines test the end-to-end capabilities of AI systems on data processing,\nrequiring data discovery, wrangling and cleaning, efficient processing,\nstatistical reasoning, and orchestrating data processing steps given a\nhigh-level task. Our evaluation tests 5 general models and 3 code generation\nmodels using our reference framework, DS-GURU, which instructs the AI model to\ndecompose a question into a sequence of subtasks, reason through each step, and\nsynthesize Python code that implements the proposed design. Our results on\nKRAMABENCH show that, although the models are sufficiently capable of solving\nwell-specified data science code generation tasks, when extensive data\nprocessing and domain knowledge are required to construct real-world data\nscience pipelines, existing out-of-box models fall short. Progress on\nKramaBench represents crucial steps towards developing autonomous data science\nagents for real-world applications. Our code, reference framework, and data are\navailable at https://github.com/mitdbg/KramaBench.", "AI": {"tldr": "\u5c3d\u7ba1\u73b0\u6709\u7684AI\u6a21\u578b\u5728\u89e3\u51b3\u660e\u786e\u89c4\u5b9a\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8db3\u591f\u7684\u80fd\u529b\uff0c\u4f46\u5728\u6784\u5efa\u5b9e\u9645\u7684\u6570\u636e\u79d1\u5b66\u7ba1\u9053\u65f6\uff0c\u5f53\u9700\u8981\u5e7f\u6cdb\u7684\u6570\u636e\u5904\u7406\u548c\u9886\u57df\u77e5\u8bc6\u65f6\uff0c\u73b0\u6210\u7684\u6a21\u578b\u8868\u73b0\u4e0d\u8db3\u3002KRAMABENCH\u57fa\u51c6\u6d4b\u8bd5\u5c55\u793a\u4e86\u5411\u5f00\u53d1\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u8fc8\u8fdb\u7684\u5173\u952e\u6b65\u9aa4\u3002", "motivation": "\u8bbe\u8ba1\u548c\u5b9e\u73b0\u6570\u636e\u79d1\u5b66\u7ba1\u9053\u9700\u8981\u9886\u57df\u77e5\u8bc6\u3001\u6280\u672f\u4e13\u957f\uff0c\u751a\u81f3\u7279\u5b9a\u9879\u76ee\u7684\u89c1\u89e3\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695aAI\u7cfb\u7edf\u7684\u63a8\u7406\u3001\u7f16\u7801\u548c\u7406\u89e3\u80fd\u529b\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6210\u529f\u8bbe\u8ba1\u548c\u6267\u884c\u6b64\u7c7b\u590d\u6742\u7ba1\u9053\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165\u4e86KRAMABENCH\uff1a\u4e00\u4e2a\u7531104\u4e2a\u624b\u52a8\u7b56\u5212\u7684\u5b9e\u9645\u6570\u636e\u79d1\u5b66\u7ba1\u9053\u7ec4\u6210\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u4e86\u6765\u81ea6\u4e2a\u4e0d\u540c\u9886\u57df\u768424\u4e2a\u6570\u636e\u6e90\u76841700\u4e2a\u6570\u636e\u6587\u4ef6\u3002\u4f7f\u7528\u53c2\u8003\u6846\u67b6DS-GURU\u8bc4\u4f305\u4e2a\u901a\u7528\u6a21\u578b\u548c3\u4e2a\u4ee3\u7801\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6846\u67b6\u6307\u5bfcAI\u6a21\u578b\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u5b50\u4efb\u52a1\uff0c\u901a\u8fc7\u6bcf\u4e00\u6b65\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u5408\u6210\u5b9e\u73b0\u6240\u63d0\u8bae\u8bbe\u8ba1\u7684Python\u4ee3\u7801\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u8fd9\u4e9b\u6a21\u578b\u5728\u89e3\u51b3\u660e\u786e\u89c4\u5b9a\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8db3\u591f\u7684\u80fd\u529b\uff0c\u4f46\u5728\u9700\u8981\u5e7f\u6cdb\u6570\u636e\u5904\u7406\u548c\u9886\u57df\u77e5\u8bc6\u4ee5\u6784\u5efa\u5b9e\u9645\u6570\u636e\u79d1\u5b66\u7ba1\u9053\u65f6\uff0c\u73b0\u6709\u73b0\u6210\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "KRAMABENCH\u4ee3\u8868\u4e86\u671d\u5411\u5f00\u53d1\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u7684\u81ea\u4e3b\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u8fc8\u51fa\u7684\u5173\u952e\u6b65\u9aa4\u3002"}}
{"id": "2506.07549", "pdf": "https://arxiv.org/pdf/2506.07549", "abs": "https://arxiv.org/abs/2506.07549", "authors": ["Zhangchi Zhao", "Jun Shu", "Deyu Meng", "Zongben Xu"], "title": "Improving Memory Efficiency for Training KANs via Meta Learning", "categories": ["cs.LG"], "comment": "ICML 2025", "summary": "Inspired by the Kolmogorov-Arnold representation theorem, KANs offer a novel\nframework for function approximation by replacing traditional neural network\nweights with learnable univariate functions. This design demonstrates\nsignificant potential as an efficient and interpretable alternative to\ntraditional MLPs. However, KANs are characterized by a substantially larger\nnumber of trainable parameters, leading to challenges in memory efficiency and\nhigher training costs compared to MLPs. To address this limitation, we propose\nto generate weights for KANs via a smaller meta-learner, called MetaKANs. By\ntraining KANs and MetaKANs in an end-to-end differentiable manner, MetaKANs\nachieve comparable or even superior performance while significantly reducing\nthe number of trainable parameters and maintaining promising interpretability.\nExtensive experiments on diverse benchmark tasks, including symbolic\nregression, partial differential equation solving, and image classification,\ndemonstrate the effectiveness of MetaKANs in improving parameter efficiency and\nmemory usage. The proposed method provides an alternative technique for\ntraining KANs, that allows for greater scalability and extensibility, and\nnarrows the training cost gap with MLPs stated in the original paper of KANs.\nOur code is available at https://github.com/Murphyzc/MetaKAN.", "AI": {"tldr": "Inspired by the Kolmogorov-Arnold representation theorem, KANs provide a new framework for function approximation. However, they have a large number of parameters. To solve this problem, MetaKANs are proposed to generate weights through a smaller meta-learner, which can significantly reduce the number of parameters and maintain interpretability.", "motivation": "KANs have great potential as an efficient and interpretable alternative to traditional MLPs, but their large number of parameters leads to challenges in memory efficiency and higher training costs.", "method": "Propose MetaKANs to generate weights for KANs via a smaller meta-learner, and train KANs and MetaKANs in an end-to-end differentiable manner.", "result": "MetaKANs achieve comparable or even superior performance while significantly reducing the number of trainable parameters and maintaining promising interpretability.", "conclusion": "MetaKANs improve parameter efficiency and memory usage, provide an alternative technique for training KANs, enhance scalability and extensibility, and narrow the training cost gap with MLPs."}}
{"id": "2506.06561", "pdf": "https://arxiv.org/pdf/2506.06561", "abs": "https://arxiv.org/abs/2506.06561", "authors": ["Ho Yin 'Sam' Ng", "Ting-Yao Hsu", "Aashish Anantha Ramakrishnan", "Branislav Kveton", "Nedim Lipka", "Franck Dernoncourt", "Dongwon Lee", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Ting-Hao 'Kenneth' Huang"], "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.", "AI": {"tldr": "This paper introduces LaMP-Cap, a dataset for personalized figure caption generation with multimodal figure profiles, which helps generate captions closer to the original author-written ones.", "motivation": "Figure captions are crucial for helping readers understand and remember a figure's key message. However, authors almost always need to revise generic AI-generated captions to match their writing style and the domain's style, highlighting the need for personalization.", "method": "LaMP-Cap provides not only the needed inputs, such as figure images, but also up to three other figures from the same document--each with its image, caption, and figure-mentioning paragraphs--as a profile to characterize the context.", "result": "Experiments with four LLMs show that using profile information consistently helps generate captions closer to the original author-written ones. Ablation studies reveal that images in the profile are more helpful than figure-mentioning paragraphs.", "conclusion": "This paper demonstrates the advantage of using multimodal profiles over text-only ones for personalized figure caption generation."}}
{"id": "2506.07551", "pdf": "https://arxiv.org/pdf/2506.07551", "abs": "https://arxiv.org/abs/2506.07551", "authors": ["Mengsong Wu", "YaFei Wang", "Yidong Ming", "Yuqi An", "Yuwei Wan", "Wenliang Chen", "Binbin Lin", "Yuqiang Li", "Tong Xie", "Dongzhan Zhou"], "title": "ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "comment": "15 pages, 6 figures", "summary": "Large language models (LLMs) have recently demonstrated promising\ncapabilities in chemistry tasks while still facing challenges due to outdated\npretraining knowledge and the difficulty of incorporating specialized chemical\nexpertise. To address these issues, we propose an LLM-based agent that\nsynergistically integrates 137 external chemical tools created ranging from\nbasic information retrieval to complex reaction predictions, and a dataset\ncuration pipeline to generate the dataset ChemToolBench that facilitates both\neffective tool selection and precise parameter filling during fine-tuning and\nevaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search\n(HE-MCTS) framework, enabling independent optimization of tool planning and\nexecution. By leveraging self-generated data, our approach supports step-level\nfine-tuning (FT) of the policy model and training task-adaptive PRM and ORM\nthat surpass GPT-4o. Experimental evaluations demonstrate that our approach\nsignificantly improves performance in Chemistry QA and discovery tasks,\noffering a robust solution to integrate specialized tools with LLMs for\nadvanced chemical applications. All datasets and code are available at\nhttps://github.com/AI4Chem/ChemistryAgent .", "AI": {"tldr": "This paper proposes an LLM-based agent that integrates 137 external chemical tools and a dataset curation pipeline to generate ChemToolBench. They introduce a HE-MCTS framework for tool planning and execution optimization, supporting step-level fine-tuning of the policy model and training task-adaptive PRM and ORM surpassing GPT-4o.", "motivation": "To overcome the challenges faced by LLMs in chemistry tasks due to outdated pretraining knowledge and difficulty incorporating specialized chemical expertise.", "method": "Propose an LLM-based agent integrating 137 external chemical tools and a dataset curation pipeline generating ChemToolBench. Introduce HE-MCTS framework for independent optimization of tool planning and execution.", "result": "Significantly improves performance in Chemistry QA and discovery tasks, surpassing GPT-4o.", "conclusion": "The proposed approach offers a robust solution to integrate specialized tools with LLMs for advanced chemical applications."}}
{"id": "2506.06566", "pdf": "https://arxiv.org/pdf/2506.06566", "abs": "https://arxiv.org/abs/2506.06566", "authors": ["Chen Bao", "Chuanbing Huo", "Qinyu Chen", "Chang Gao"], "title": "AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition", "categories": ["eess.AS", "cs.AI"], "comment": "Under review", "summary": "This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition\nframework based on Whisper-tiny, tailored for low-resource deployment on edge\ndevices. Our approach introduces a hybrid training strategy that systematically\ncombines standard and aphasic speech at varying ratios, enabling robust\ngeneralization, and a GPT-4-based reference enhancement method that refines\nnoisy aphasic transcripts, improving supervision quality. We conduct extensive\nexperiments across multiple data mixing configurations and evaluation settings.\nResults show that our fine-tuned model significantly outperforms the zero-shot\nbaseline, reducing WER on aphasic speech by over 30% while preserving\nperformance on standard speech. The proposed framework offers a scalable,\nefficient solution for real-world disordered speech recognition.", "AI": {"tldr": "This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition framework based on Whisper-tiny with hybrid training strategy and GPT-4-based reference enhancement method. The fine-tuned model significantly outperforms the zero-shot baseline, reducing WER on aphasic speech by over 30%.", "motivation": "Current speech recognition models may not perform well on disordered speech such as aphasic speech due to lack of specific tuning and resources.", "method": "The AS-ASR framework is built upon Whisper-tiny model and incorporates two key innovations: a hybrid training strategy that mixes standard and aphasic speech in varying ratios for robust generalization, and a GPT-4-based reference enhancement technique to refine noisy transcripts and improve supervision quality.", "result": "The fine-tuned model shows significant improvement over the zero-shot baseline, achieving over 30% reduction in Word Error Rate (WER) on aphasic speech while maintaining performance on standard speech across multiple experimental settings.", "conclusion": "AS-ASR presents an effective, scalable, and efficient solution for real-world disordered speech recognition, particularly suitable for low-resource deployment on edge devices."}}
{"id": "2506.07578", "pdf": "https://arxiv.org/pdf/2506.07578", "abs": "https://arxiv.org/abs/2506.07578", "authors": ["Florian Andreas Marwitz", "Ralf M\u00f6ller", "Magnus Bender", "Marcel Gehrke"], "title": "Denoising the Future: Top-p Distributions for Moving Through Time", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference in dynamic probabilistic models is a complex task involving\nexpensive operations. In particular, for Hidden Markov Models, the whole state\nspace has to be enumerated for advancing in time. Even states with negligible\nprobabilities are considered, resulting in computational inefficiency and\nincreased noise due to the propagation of unlikely probability mass. We propose\nto denoise the future and speed up inference by using only the top-p states,\ni.e., the most probable states with accumulated probability p. We show that the\nerror introduced by using only the top-p states is bound by p and the so-called\nminimal mixing rate of the underlying model. Moreover, in our empirical\nevaluation, we show that we can expect speedups of at least an order of\nmagnitude, while the error in terms of total variation distance is below 0.09.", "AI": {"tldr": "\u901a\u8fc7\u4ec5\u4f7f\u7528\u6700\u53ef\u80fd\u7684\u72b6\u6001\uff08top-p states\uff09\uff0c\u53ef\u4ee5\u51cf\u5c11\u63a8\u7406\u4e2d\u7684\u8bef\u5dee\u5e76\u52a0\u901f\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u5728\u603b\u53d8\u5f02\u8ddd\u79bb\u65b9\u9762\u7684\u8bef\u5dee\u4f4e\u4e8e0.09\u3002", "motivation": "\u63a8\u7406\u5728\u52a8\u6001\u6982\u7387\u6a21\u578b\u4e2d\u662f\u4e00\u9879\u590d\u6742\u7684\u4efb\u52a1\uff0c\u6d89\u53ca\u6602\u8d35\u7684\u64cd\u4f5c\u3002\u5bf9\u4e8e\u9690\u85cf\u9a6c\u5c14\u53ef\u592b\u6a21\u578b\uff0c\u6574\u4e2a\u72b6\u6001\u7a7a\u95f4\u5fc5\u987b\u88ab\u679a\u4e3e\u4ee5\u4fbf\u5728\u65f6\u95f4\u4e0a\u63a8\u8fdb\uff0c\u8fd9\u5bfc\u81f4\u4e86\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u7531\u4e8e\u4f20\u64ad\u4e0d\u592a\u53ef\u80fd\u7684\u6982\u7387\u8d28\u91cf\u800c\u589e\u52a0\u7684\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u566a\u672a\u6765\u548c\u52a0\u901f\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5373\u4ec5\u4f7f\u7528\u7d2f\u79ef\u6982\u7387p\u7684\u6700\u53ef\u80fd\u72b6\u6001\uff08top-p states\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u53ef\u4ee5\u671f\u671b\u81f3\u5c11\u63d0\u9ad8\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\uff0c\u800c\u5728\u603b\u53d8\u5f02\u8ddd\u79bb\u65b9\u9762\u7684\u8bef\u5dee\u4f4e\u4e8e0.09\u3002", "conclusion": "\u4ec5\u4f7f\u7528top-p\u72b6\u6001\u5f15\u5165\u7684\u8bef\u5dee\u7531p\u548c\u5e95\u5c42\u6a21\u578b\u7684\u6700\u5c0f\u6df7\u5408\u7387\u51b3\u5b9a\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u8bef\u5dee\u3002"}}
{"id": "2506.06569", "pdf": "https://arxiv.org/pdf/2506.06569", "abs": "https://arxiv.org/abs/2506.06569", "authors": ["Yannis Spyridis", "Vasileios Argyriou"], "title": "Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automated sorting is crucial for improving the efficiency and scalability of\ntextile recycling, but accurately identifying material composition and\ndetecting contaminants from sensor data remains challenging. This paper\ninvestigates the use of standard RGB imagery, a cost-effective sensing\nmodality, for key pre-processing tasks in an automated system. We present\ncomputer vision components designed for a conveyor belt setup to perform (a)\nclassification of four common textile types and (b) segmentation of non-textile\nfeatures such as buttons and zippers. For classification, several pre-trained\narchitectures were evaluated using transfer learning and cross-validation, with\nEfficientNetB0 achieving the best performance on a held-out test set with\n81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the\nGrounding DINO open-vocabulary detector with the Segment Anything Model (SAM)\nwas employed, demonstrating excellent performance with a mIoU of 0.90 for the\ngenerated masks against ground truth. This study demonstrates the feasibility\nof using RGB images coupled with modern deep learning techniques, including\ntransfer learning for classification and foundation models for zero-shot\nsegmentation, to enable essential analysis steps for automated textile\nrecycling pipelines.", "AI": {"tldr": "The paper explores using RGB imagery for textile recycling tasks, achieving 81.25% accuracy in classification and a mIoU of 0.90 in segmentation.", "motivation": "Automated sorting is crucial for improving the efficiency and scalability of textile recycling, but accurately identifying material composition and detecting contaminants from sensor data remains challenging.", "method": "For classification, several pre-trained architectures were evaluated using transfer learning and cross-validation, with EfficientNetB0 achieving the best performance. For feature segmentation, a zero-shot approach combining the Grounding DINO open-vocabulary detector with the Segment Anything Model (SAM) was employed.", "result": "EfficientNetB0 achieved 81.25% accuracy on a held-out test set for classification. The zero-shot approach demonstrated excellent performance with a mIoU of 0.90 for the generated masks against ground truth.", "conclusion": "This study demonstrates the feasibility of using RGB images coupled with modern deep learning techniques, including transfer learning for classification and foundation models for zero-shot segmentation, to enable essential analysis steps for automated textile recycling pipelines."}}
{"id": "2506.07581", "pdf": "https://arxiv.org/pdf/2506.07581", "abs": "https://arxiv.org/abs/2506.07581", "authors": ["Tan Chen", "Jintao Yan", "Yuxuan Sun", "Sheng Zhou", "Zhisheng Niu"], "title": "FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated learning (FL) is a promising paradigm for multiple devices to\ncooperatively train a model. When applied in wireless networks, two issues\nconsistently affect the performance of FL, i.e., data heterogeneity of devices\nand limited bandwidth. Many papers have investigated device scheduling\nstrategies considering the two issues. However, most of them recognize data\nheterogeneity as a property of individual devices. In this paper, we prove that\nthe convergence speed of FL is affected by the sum of device-level and\nsample-level collective gradient divergence (CGD). The device-level CGD refers\nto the gradient divergence of the scheduled device group, instead of the sum of\nthe individual device divergence. The sample-level CGD is statistically upper\nbounded by sampling variance, which is inversely proportional to the total\nnumber of samples scheduled for local update. To derive a tractable form of the\ndevice-level CGD, we further consider a classification problem and transform it\ninto the weighted earth moving distance (WEMD) between the group distribution\nand the global distribution. Then we propose FedCGD algorithm to minimize the\nsum of multi-level CGDs by balancing WEMD and sampling variance, within\npolynomial time. Simulation shows that the proposed strategy increases\nclassification accuracy on the CIFAR-10 dataset by up to 4.2\\% while scheduling\n41.8\\% fewer devices, and flexibly switches between reducing WEMD and reducing\nsampling variance.", "AI": {"tldr": "This paper addresses the impact of collective gradient divergence (CGD) on federated learning (FL) convergence speed in wireless networks, proposing the FedCGD algorithm to optimize device scheduling and improve performance.", "motivation": "Federated learning faces challenges with data heterogeneity and limited bandwidth when applied in wireless networks. While many studies focus on device scheduling strategies, they often overlook the collective impact of devices on gradient divergence.", "method": "The authors prove that FL's convergence speed is influenced by the sum of device-level and sample-level CGD. They derive a tractable form of device-level CGD using weighted earth moving distance (WEMD). Based on this, they propose the FedCGD algorithm which optimizes device scheduling by balancing WEMD and sampling variance.", "result": "Simulation results show that the FedCGD algorithm increases classification accuracy on the CIFAR-10 dataset by up to 4.2% while reducing the number of scheduled devices by 41.8%. The algorithm can flexibly adjust between minimizing WEMD and sampling variance.", "conclusion": "FedCGD effectively minimizes multi-level CGDs, leading to improved FL performance in wireless networks through efficient device scheduling."}}
{"id": "2506.07584", "pdf": "https://arxiv.org/pdf/2506.07584", "abs": "https://arxiv.org/abs/2506.07584", "authors": ["Hao Li", "Bowen Deng", "Chang Xu", "Zhiyuan Feng", "Viktor Schlegel", "Yu-Hao Huang", "Yizheng Sun", "Jingyuan Sun", "Kailai Yang", "Yiyao Yu", "Jiang Bian"], "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data", "categories": ["cs.LG"], "comment": null, "summary": "A unified foundation model for medical time series -- pretrained on open\naccess and ethics board-approved medical corpora -- offers the potential to\nreduce annotation burdens, minimize model customization, and enable robust\ntransfer across clinical institutions, modalities, and tasks, particularly in\ndata-scarce or privacy-constrained environments. However, existing generalist\ntime series foundation models struggle to handle medical time series data due\nto their inherent challenges, including irregular intervals, heterogeneous\nsampling rates, and frequent missing values. To address these challenges, we\nintroduce MIRA, a unified foundation model specifically designed for medical\ntime series forecasting. MIRA incorporates a Continuous-Time Rotary Positional\nEncoding that enables fine-grained modeling of variable time intervals, a\nfrequency-specific mixture-of-experts layer that routes computation across\nlatent frequency regimes to further promote temporal specialization, and a\nContinuous Dynamics Extrapolation Block based on Neural ODE that models the\ncontinuous trajectory of latent states, enabling accurate forecasting at\narbitrary target timestamps. Pretrained on a large-scale and diverse medical\ncorpus comprising over 454 billion time points collect from publicly available\ndatasets, MIRA achieves reductions in forecasting errors by an average of 10%\nand 7% in out-of-distribution and in-distribution scenarios, respectively, when\ncompared to other zero-shot and fine-tuned baselines. We also introduce a\ncomprehensive benchmark spanning multiple downstream clinical tasks,\nestablishing a foundation for future research in medical time series modeling.", "AI": {"tldr": "MIRA is a unified foundation model for medical time series forecasting that incorporates Continuous-Time Rotary Positional Encoding, frequency-specific mixture-of-experts layer, and Continuous Dynamics Extrapolation Block. It reduces forecasting errors by 10% in out-of-distribution scenarios and 7% in in-distribution scenarios.", "motivation": "Existing generalist time series foundation models struggle to handle medical time series data due to irregular intervals, heterogeneous sampling rates, and frequent missing values.", "method": "MIRA uses Continuous-Time Rotary Positional Encoding for fine-grained modeling of variable time intervals, a frequency-specific mixture-of-experts layer for routing computation across latent frequency regimes, and a Continuous Dynamics Extrapolation Block based on Neural ODE for modeling the continuous trajectory of latent states.", "result": "MIRA achieves reductions in forecasting errors by an average of 10% and 7% in out-of-distribution and in-distribution scenarios respectively.", "conclusion": "MIRA offers the potential to reduce annotation burdens, minimize model customization, and enable robust transfer across clinical institutions, modalities, and tasks."}}
{"id": "2506.06576", "pdf": "https://arxiv.org/pdf/2506.06576", "abs": "https://arxiv.org/abs/2506.06576", "authors": ["Yijia Shao", "Humishka Zope", "Yucheng Jiang", "Jiaxin Pei", "David Nguyen", "Erik Brynjolfsson", "Diyi Yang"], "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "Preprint", "summary": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the\nlabor market, raising concerns about job displacement, diminished human agency,\nand overreliance on automation. Yet, we lack a systematic understanding of the\nevolving landscape. In this paper, we address this gap by introducing a novel\nauditing framework to assess which occupational tasks workers want AI agents to\nautomate or augment, and how those desires align with the current technological\ncapabilities. Our framework features an audio-enhanced mini-interview to\ncapture nuanced worker desires and introduces the Human Agency Scale (HAS) as a\nshared language to quantify the preferred level of human involvement. Using\nthis framework, we construct the WORKBank database, building on the U.S.\nDepartment of Labor's O*NET database, to capture preferences from 1,500 domain\nworkers and capability assessments from AI experts across over 844 tasks\nspanning 104 occupations. Jointly considering the desire and technological\ncapability divides tasks in WORKBank into four zones: Automation \"Green Light\"\nZone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone.\nThis highlights critical mismatches and opportunities for AI agent development.\nMoving beyond a simple automate-or-not dichotomy, our results reveal diverse\nHAS profiles across occupations, reflecting heterogeneous expectations for\nhuman involvement. Moreover, our study offers early signals of how AI agent\nintegration may reshape the core human competencies, shifting from\ninformation-focused skills to interpersonal ones. These findings underscore the\nimportance of aligning AI agent development with human desires and preparing\nworkers for evolving workplace dynamics.", "AI": {"tldr": "The paper presents a novel auditing framework to assess workers' desires for AI agent automation or augmentation in occupational tasks and align them with current technological capabilities, constructing the WORKBank database.", "motivation": "To address the lack of systematic understanding of the evolving landscape of compound AI systems (AI agents) in the labor market and their impact on job displacement, human agency, and automation reliance.", "method": "Introduced an audio-enhanced mini-interview based auditing framework combined with the Human Agency Scale (HAS) to quantify preferred human involvement levels. Constructed the WORKBank database using preferences from 1,500 domain workers and capability assessments from AI experts across 844 tasks in 104 occupations.", "result": "Divided tasks into four zones based on worker desires and technological capabilities: Automation 'Green Light' Zone, 'Red Light' Zone, R&D Opportunity Zone, and Low Priority Zone. Revealed diverse HAS profiles across occupations and suggested a shift from information-focused skills to interpersonal ones due to AI agent integration.", "conclusion": "Highlighted the importance of aligning AI agent development with human desires and preparing workers for evolving workplace dynamics."}}
{"id": "2506.07585", "pdf": "https://arxiv.org/pdf/2506.07585", "abs": "https://arxiv.org/abs/2506.07585", "authors": ["Seokbin Yoon", "Keumjin Lee"], "title": "Aircraft Trajectory Dataset Augmentation in Latent Space", "categories": ["cs.LG"], "comment": null, "summary": "Aircraft trajectory modeling plays a crucial role in Air Traffic Management\n(ATM) and is important for various downstream tasks, including conflict\ndetection and landing time prediction. Dataset augmentation through the\naddition of synthetically generated trajectory data is necessary to develop a\nmore robust aircraft trajectory model and ensure that the trajectory dataset is\nsufficient and balanced. In this work, we propose a novel framework called\nATRADA for aircraft trajectory dataset augmentation. In the proposed framework,\na Transformer encoder learns the underlying patterns in the original trajectory\ndataset and converts each data point into a context vector in the learned\nlatent space. The converted dataset in the latent space is projected into\nreduced dimensions using principal component analysis (PCA), and a Gaussian\nmixture model (GMM) is applied to fit the probability distribution of the data\npoints in the reduced-dimensional space. Finally, new samples are drawn from\nthe fitted GMM, the dimension of the samples is reverted to the original\ndimension, and they are decoded with a Multi-Layer Perceptron (MLP). Several\nexperiments demonstrate that the framework effectively generates new,\nhigh-quality synthetic aircraft trajectory data, which were compared to the\nresults of several baselines.", "AI": {"tldr": "A novel framework named ATRADA is proposed for aircraft trajectory dataset augmentation, which leverages Transformer encoder, PCA, GMM and MLP to generate high-quality synthetic data.", "motivation": "Aircraft trajectory modeling is essential in Air Traffic Management and downstream tasks like conflict detection and landing time prediction. To develop a more robust model, it's necessary to augment the dataset with synthetically generated trajectory data.", "method": "The ATRADA framework uses a Transformer encoder to learn patterns from the original trajectory dataset and transform each data point into a context vector. Then PCA reduces the dimensionality of these vectors, and GMM fits the probability distribution. New samples are drawn from GMM, reverted to original dimensions, and decoded by MLP.", "result": "Experiments show that the framework can effectively generate new, high-quality synthetic aircraft trajectory data, outperforming several baselines.", "conclusion": "ATRADA provides an effective solution for aircraft trajectory dataset augmentation, enhancing the development of robust models."}}
{"id": "2506.07587", "pdf": "https://arxiv.org/pdf/2506.07587", "abs": "https://arxiv.org/abs/2506.07587", "authors": ["Tongzhou Yu", "Zhuhao Zhang", "Guanghui Zhu", "Shen Jiang", "Meikang Qiu", "Yihua Huang"], "title": "PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and\npromising approaches for fine-tuning pre-trained language models. Compared with\nFull parameter Fine-Tuning (FFT), PEFT achieved comparable task performance\nwith a substantial reduction of trainable parameters, which largely saved the\ntraining and storage costs. However, using the PEFT method requires considering\na vast design space, such as the type of PEFT modules and their insertion\nlayers. Inadequate configurations can lead to sub-optimal results. Conventional\nsolutions such as architectural search techniques, while effective, tend to\nintroduce substantial additional overhead. In this paper, we propose a novel\napproach, PrunePEFT, which formulates the PEFT strategy search as a pruning\nproblem and introduces a hybrid pruning strategy that capitalizes on the\nsensitivity of pruning methods to different PEFT modules. This method extends\ntraditional pruning techniques by iteratively removing redundant or conflicting\nPEFT modules, thereby optimizing the fine-tuned configuration. By efficiently\nidentifying the most relevant modules, our approach significantly reduces the\ncomputational burden typically associated with architectural search processes,\nmaking it a more scalable and efficient solution for fine-tuning large\npre-trained models.", "AI": {"tldr": "Parameter Efficient Fine-Tuning (PEFT) methods are promising for fine-tuning language models with fewer parameters. However, choosing the right PEFT configuration is challenging without incurring significant overhead. This paper proposes PrunePEFT, a novel approach that formulates PEFT strategy search as a pruning problem and introduces a hybrid pruning strategy to optimize the fine-tuned configuration.", "motivation": "PEFT methods offer advantages over Full parameter Fine-Tuning (FFT) by achieving similar performance with fewer trainable parameters. Yet, selecting appropriate PEFT configurations remains difficult and resource-intensive.", "method": "The proposed method, PrunePEFT, treats PEFT strategy search as a pruning problem and applies a hybrid pruning strategy. It iteratively removes redundant or conflicting PEFT modules based on their sensitivity to different pruning techniques, thereby optimizing the fine-tuned model configuration.", "result": "PrunePEFT efficiently identifies the most relevant PEFT modules, significantly reducing computational costs associated with architectural search processes while maintaining scalability and efficiency.", "conclusion": "PrunePEFT provides an effective and efficient solution for optimizing PEFT configurations, making it easier to fine-tune large pre-trained models with reduced computational burden."}}
{"id": "2506.06594", "pdf": "https://arxiv.org/pdf/2506.06594", "abs": "https://arxiv.org/abs/2506.06594", "authors": ["Daniel Leite", "Igor \u0160krjanc", "Fernando Gomide"], "title": "From Model-Based and Adaptive Control to Evolving Fuzzy Control", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "4 pages, 2 figures. Fuzz-IEEE 2025 Booklet: 60 Years of Fuzzy Set\n  Theory", "summary": "Evolving fuzzy systems build and adapt fuzzy models - such as predictors and\ncontrollers - by incrementally updating their rule-base structure from data\nstreams. On the occasion of the 60-year anniversary of fuzzy set theory,\ncommemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the\nhistorical development and core contributions of classical fuzzy and adaptive\nmodeling and control frameworks. It then highlights the emergence and\nsignificance of evolving intelligent systems in fuzzy modeling and control,\nemphasizing their advantages in handling nonstationary environments. Key\nchallenges and future directions are discussed, including safety,\ninterpretability, and principled structural evolution.", "AI": {"tldr": "Evolving fuzzy systems create and adjust fuzzy models like predictors and controllers by updating their rule-base structure incrementally from data streams. This paper revisits the development of fuzzy and adaptive modeling and control frameworks, emphasizing the importance of evolving intelligent systems in dealing with nonstationary environments and discussing key challenges such as safety, interpretability, and principled structural evolution.", "motivation": "To revisit the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks, and to highlight the emergence and significance of evolving intelligent systems in fuzzy modeling and control.", "method": "Revisiting the historical development and core contributions of classical fuzzy and adaptive modeling and control frameworks, and emphasizing the advantages of evolving intelligent systems in handling nonstationary environments.", "result": "The paper discusses the key challenges and future directions in the field of evolving fuzzy systems, including safety, interpretability, and principled structural evolution.", "conclusion": "Evolving fuzzy systems are significant in handling nonstationary environments, but there are still key challenges to be addressed in terms of safety, interpretability, and principled structural evolution."}}
{"id": "2506.07595", "pdf": "https://arxiv.org/pdf/2506.07595", "abs": "https://arxiv.org/abs/2506.07595", "authors": ["Hao Qiu", "Emmanuel Esposito", "Mengxiao Zhang"], "title": "Exploiting Curvature in Online Convex Optimization with Delayed Feedback", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "In this work, we study the online convex optimization problem with curved\nlosses and delayed feedback. When losses are strongly convex, existing\napproaches obtain regret bounds of order $d_{\\max} \\ln T$, where $d_{\\max}$ is\nthe maximum delay and $T$ is the time horizon. However, in many cases, this\nguarantee can be much worse than $\\sqrt{d_{\\mathrm{tot}}}$ as obtained by a\ndelayed version of online gradient descent, where $d_{\\mathrm{tot}}$ is the\ntotal delay. We bridge this gap by proposing a variant of\nfollow-the-regularized-leader that obtains regret of order\n$\\min\\{\\sigma_{\\max}\\ln T, \\sqrt{d_{\\mathrm{tot}}}\\}$, where $\\sigma_{\\max}$ is\nthe maximum number of missing observations. We then consider exp-concave losses\nand extend the Online Newton Step algorithm to handle delays with an adaptive\nlearning rate tuning, achieving regret $\\min\\{d_{\\max} n\\ln T,\n\\sqrt{d_{\\mathrm{tot}}}\\}$ where $n$ is the dimension. To our knowledge, this\nis the first algorithm to achieve such a regret bound for exp-concave losses.\nWe further consider the problem of unconstrained online linear regression and\nachieve a similar guarantee by designing a variant of the Vovk-Azoury-Warmuth\nforecaster with a clipping trick. Finally, we implement our algorithms and\nconduct experiments under various types of delay and losses, showing an\nimproved performance over existing methods.", "AI": {"tldr": "This paper explores online convex optimization with curved losses and delayed feedback, proposing new algorithms that achieve improved regret bounds.", "motivation": "The motivation of this paper is to bridge the gap between existing approaches for online convex optimization with strongly convex losses and delayed feedback, which obtain regret bounds of order $d_{\\max} \\ln T$, and the bounds obtained by a delayed version of online gradient descent, which are of order $\\sqrt{d_{\\mathrm{tot}}}$.", "method": "The authors propose a variant of follow-the-regularized-leader that obtains regret of order $\\min\\{\\sigma_{\\max}\\ln T, \\sqrt{d_{\\mathrm{tot}}}\\}$, where $\\sigma_{\\max}$ is the maximum number of missing observations. They also extend the Online Newton Step algorithm to handle delays with an adaptive learning rate tuning, achieving regret $\\min\\{d_{\\max} n\\ln T, \\sqrt{d_{\\mathrm{tot}}}\\}$. For unconstrained online linear regression, they design a variant of the Vovk-Azoury-Warmuth forecaster with a clipping trick.", "result": "The proposed algorithms achieve regret bounds that are better than existing methods in various types of delay and losses. The experiments show an improved performance over existing methods.", "conclusion": "This paper presents new algorithms for online convex optimization with curved losses and delayed feedback that achieve improved regret bounds. These algorithms outperform existing methods in various scenarios."}}
{"id": "2506.07596", "pdf": "https://arxiv.org/pdf/2506.07596", "abs": "https://arxiv.org/abs/2506.07596", "authors": ["Torsten Krau\u00df", "Hamid Dashtbani", "Alexandra Dmitrienko"], "title": "TwinBreak: Jailbreaking LLM Security Alignments based on Twin Prompts", "categories": ["cs.LG"], "comment": "26 pages, 25 tables, 13 figures, 2 algorithms, to appear in the 43th\n  USENIX Security Symposium (USENIX Security 2025)", "summary": "Machine learning is advancing rapidly, with applications bringing notable\nbenefits, such as improvements in translation and code generation. Models like\nChatGPT, powered by Large Language Models (LLMs), are increasingly integrated\ninto daily life. However, alongside these benefits, LLMs also introduce social\nrisks. Malicious users can exploit LLMs by submitting harmful prompts, such as\nrequesting instructions for illegal activities. To mitigate this, models often\ninclude a security mechanism that automatically rejects such harmful prompts.\nHowever, they can be bypassed through LLM jailbreaks. Current jailbreaks often\nrequire significant manual effort, high computational costs, or result in\nexcessive model modifications that may degrade regular utility.\n  We introduce TwinBreak, an innovative safety alignment removal method.\nBuilding on the idea that the safety mechanism operates like an embedded\nbackdoor, TwinBreak identifies and prunes parameters responsible for this\nfunctionality. By focusing on the most relevant model layers, TwinBreak\nperforms fine-grained analysis of parameters essential to model utility and\nsafety. TwinBreak is the first method to analyze intermediate outputs from\nprompts with high structural and content similarity to isolate safety\nparameters. We present the TwinPrompt dataset containing 100 such twin prompts.\nExperiments confirm TwinBreak's effectiveness, achieving 89% to 98% success\nrates with minimal computational requirements across 16 LLMs from five vendors.", "AI": {"tldr": "The paper introduces TwinBreak, a method to remove safety alignments in LLMs by pruning parameters responsible for the safety mechanism. It uses twin prompts with high similarity to isolate safety parameters and achieves high success rates across multiple LLMs.", "motivation": "To address the issue of LLM jailbreaks which currently require significant manual effort or computational costs, and may degrade model utility.", "method": "TwinBreak identifies and prunes parameters responsible for the safety mechanism in LLMs by analyzing intermediate outputs from prompts with high structural and content similarity.", "result": "Experiments show TwinBreak's effectiveness with 89% to 98% success rates across 16 LLMs from five vendors, requiring minimal computational resources.", "conclusion": "TwinBreak is an effective method for removing safety alignments in LLMs with minimal computational requirements."}}
{"id": "2506.06605", "pdf": "https://arxiv.org/pdf/2506.06605", "abs": "https://arxiv.org/abs/2506.06605", "authors": ["Xiao Wang", "Mengjue Tan", "Qiao Jin", "Guangzhi Xiong", "Yu Hu", "Aidong Zhang", "Zhiyong Lu", "Minjia Zhang"], "title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based medical question-answering systems lack citation\ngeneration and evaluation capabilities, raising concerns about their adoption\nin practice. In this work, we introduce \\name, the first end-to-end framework\nthat facilitates the design and evaluation of citation generation with LLMs for\nmedical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation\nmethod that generates high-quality citations. Our evaluation highlights the\nchallenges and opportunities of citation generation for medical tasks, while\nidentifying important design choices that have a significant impact on the\nfinal citation quality. Our proposed method achieves superior citation\nprecision and recall improvements compared to strong baseline methods, and we\nshow that evaluation results correlate well with annotation results from\nprofessional experts.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\\name\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u533b\u7597\u4efb\u52a1\u4e2d\u751f\u6210\u548c\u8bc4\u4f30\u5f15\u7528\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5f15\u5165\u591a\u8f6e\u68c0\u7d22-\u5f15\u7528\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5f15\u7528\u7684\u8d28\u91cf\uff0c\u5e76\u4e14\u5c55\u793a\u4e86\u5176\u4e0e\u4e13\u4e1a\u4e13\u5bb6\u6807\u6ce8\u7ed3\u679c\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7f3a\u4e4f\u751f\u6210\u548c\u8bc4\u4f30\u5f15\u7528\u7684\u80fd\u529b\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u62c5\u5fe7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\\name\u7684\u6846\u67b6\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u591a\u8f6e\u68c0\u7d22-\u5f15\u7528\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5f15\u7528\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u8f83\u4e8e\u5f3a\u5927\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5f15\u7528\u7684\u7cbe\u786e\u5ea6\u548c\u53ec\u56de\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4e14\u8bc4\u4f30\u7ed3\u679c\u4e0e\u4e13\u4e1a\u4e13\u5bb6\u7684\u6807\u6ce8\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u672c\u6587\u7684\u5de5\u4f5c\u5f3a\u8c03\u4e86\u533b\u7597\u4efb\u52a1\u4e2d\u5f15\u7528\u751f\u6210\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u5e76\u6307\u51fa\u4e86\u5bf9\u6700\u7ec8\u5f15\u7528\u8d28\u91cf\u6709\u91cd\u5927\u5f71\u54cd\u7684\u91cd\u8981\u8bbe\u8ba1\u9009\u62e9\u3002"}}
{"id": "2506.07616", "pdf": "https://arxiv.org/pdf/2506.07616", "abs": "https://arxiv.org/abs/2506.07616", "authors": ["Zhixin Geng", "Xu Fan", "Xiqiao Lu", "Yan Zhang", "Guangyuan Yu", "Cheng Huang", "Qian Wang", "Yuewu Li", "Weichun Ma", "Qi Yu", "Libo Wu", "Hao Li"], "title": "FuXi-Air: Urban Air Quality Forecasting Based on Emission-Meteorology-Pollutant multimodal Machine Learning", "categories": ["cs.LG"], "comment": null, "summary": "Air pollution has emerged as a major public health challenge in megacities.\nNumerical simulations and single-site machine learning approaches have been\nwidely applied in air quality forecasting tasks. However, these methods face\nmultiple limitations, including high computational costs, low operational\nefficiency, and limited integration with observational data. With the rapid\nadvancement of artificial intelligence, there is an urgent need to develop a\nlow-cost, efficient air quality forecasting model for smart urban management.\nAn air quality forecasting model, named FuXi-Air, has been constructed in this\nstudy based on multimodal data fusion to support high-precision air quality\nforecasting and operated in typical megacities. The model integrates\nmeteorological forecasts, emission inventories, and pollutant monitoring data\nunder the guidance of air pollution mechanism. By combining an autoregressive\nprediction framework with a frame interpolation strategy, the model\nsuccessfully completes 72-hour forecasts for six major air pollutants at an\nhourly resolution across multiple monitoring sites within 25-30 seconds. In\nterms of both computational efficiency and forecasting accuracy, it outperforms\nthe mainstream numerical air quality models in operational forecasting work.\nAblation experiments concerning key influencing factors show that although\nmeteorological data contribute more to model accuracy than emission inventories\ndo, the integration of multimodal data significantly improves forecasting\nprecision and ensures that reliable predictions are obtained under differing\npollution mechanisms across megacities. This study provides both a technical\nreference and a practical example for applying multimodal data-driven models to\nair quality forecasting and offers new insights into building hybrid\nforecasting systems to support air pollution risk warning in smart city\nmanagement.", "AI": {"tldr": "This paper introduces FuXi-Air, an air quality forecasting model using multimodal data fusion for high-precision and efficient forecasts in megacities.", "motivation": "Air pollution is a major public health challenge in megacities with existing forecasting methods having limitations such as high computational costs and limited integration with observational data.", "method": "The model integrates meteorological forecasts, emission inventories, and pollutant monitoring data under the guidance of air pollution mechanism. It combines an autoregressive prediction framework with a frame interpolation strategy to complete 72-hour forecasts for six major air pollutants at an hourly resolution.", "result": "FuXi-Air outperforms mainstream numerical air quality models in both computational efficiency and forecasting accuracy. Meteorological data contribute more to model accuracy than emission inventories, but integrating multimodal data significantly improves forecasting precision.", "conclusion": "This study provides a technical reference and practical example for applying multimodal data-driven models to air quality forecasting, offering new insights into building hybrid forecasting systems for smart city management."}}
{"id": "2506.06607", "pdf": "https://arxiv.org/pdf/2506.06607", "abs": "https://arxiv.org/abs/2506.06607", "authors": ["Charles Goddard", "Fernando Fernandes Neto"], "title": "Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a training-free method to transplant tokenizers in pretrained\nlarge language models (LLMs) by reconstructing unseen token embeddings via\nOrthogonal Matching Pursuit (OMP). Specifically, we approximate each\nout-of-vocabulary token as a sparse linear combination of shared tokens, in two\nphases: first, compute each new token's representation in the donor embedding\nspace with a small dictionary of shared anchor tokens, then transfer these same\nsparse coefficients back into the base model's embedding space.\n  On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and\nQwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of\nthe base model's performance across multiple benchmarks, while other zero-shot\napproaches degrade significantly. Compared to baselines (zero-init, mean-init,\nand existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves\nthe best overall performance, effectively bridging large tokenizer\ndiscrepancies without gradient updates. Our analysis further identifies\nmismatched numerical tokenization schemes as a critical challenge for\npreserving mathematical reasoning capabilities. This technique enables direct\nreuse of pretrained model weights with new tokenizers, facilitating\ncross-tokenizer knowledge distillation, speculative decoding, ensembling,\nmerging, and domain-specific vocabulary adaptations. We integrate our method\ninto the open-source mergekit-tokensurgeon tool for post hoc vocabulary\nrealignment.", "AI": {"tldr": "A training-free method using Orthogonal Matching Pursuit (OMP) is introduced to transplant tokenizers in pretrained large language models, allowing for zero-shot preservation of model performance across benchmarks and enabling various applications such as knowledge distillation and domain-specific vocabulary adaptations.", "motivation": "The motivation behind this paper is to address the challenge of adapting pretrained large language models to new tokenizers without requiring additional training, which can lead to degradation in model performance. Current methods often struggle with tokenizer discrepancies, particularly in preserving mathematical reasoning capabilities.", "method": "The method involves reconstructing unseen token embeddings via Orthogonal Matching Pursuit (OMP). This is done by approximating each out-of-vocabulary token as a sparse linear combination of shared tokens, computed in two phases: first, calculating the new token's representation in the donor embedding space with anchor tokens; second, transferring these sparse coefficients into the base model's embedding space.", "result": "OMP achieves the best zero-shot preservation of the base model's performance across multiple benchmarks compared to other approaches like zero-init, mean-init, WECHSEL, FOCUS, and ZETT. It effectively bridges large tokenizer discrepancies without gradient updates and identifies mismatched numerical tokenization schemes as a critical challenge for preserving mathematical reasoning capabilities.", "conclusion": "This technique enables direct reuse of pretrained model weights with new tokenizers, facilitating applications such as cross-tokenizer knowledge distillation, speculative decoding, ensembling, merging, and domain-specific vocabulary adaptations. The method has been integrated into the open-source mergekit-tokensurgeon tool for post hoc vocabulary realignment."}}
{"id": "2506.07619", "pdf": "https://arxiv.org/pdf/2506.07619", "abs": "https://arxiv.org/abs/2506.07619", "authors": ["Toby Boyne", "Juan S. Campos", "Becky D. Langdon", "Jixiang Qing", "Yilin Xie", "Shiqiang Zhang", "Calvin Tsay", "Ruth Misener", "Daniel W. Davies", "Kim E. Jelfs", "Sarah Boyall", "Thomas M. Dixon", "Linden Schrecker", "Jose Pablo Folch"], "title": "The Catechol Benchmark: Time-series Solvent Selection Data for Few-shot Machine Learning", "categories": ["cs.LG", "q-bio.QM"], "comment": null, "summary": "Machine learning has promised to change the landscape of laboratory\nchemistry, with impressive results in molecular property prediction and\nreaction retro-synthesis. However, chemical datasets are often inaccessible to\nthe machine learning community as they tend to require cleaning, thorough\nunderstanding of the chemistry, or are simply not available. In this paper, we\nintroduce a novel dataset for yield prediction, providing the first-ever\ntransient flow dataset for machine learning benchmarking, covering over 1200\nprocess conditions. While previous datasets focus on discrete parameters, our\nexperimental set-up allow us to sample a large number of continuous process\nconditions, generating new challenges for machine learning models. We focus on\nsolvent selection, a task that is particularly difficult to model theoretically\nand therefore ripe for machine learning applications. We showcase benchmarking\nfor regression algorithms, transfer-learning approaches, feature engineering,\nand active learning, with important applications towards solvent replacement\nand sustainable manufacturing.", "AI": {"tldr": "The paper introduces a new dataset for yield prediction in chemistry, focusing on solvent selection, and showcases various machine learning techniques for this task.", "motivation": "Chemical datasets are often inaccessible or require significant preprocessing, limiting the application of machine learning in laboratory chemistry.", "method": "Introduced a novel transient flow dataset for yield prediction covering over 1200 process conditions. The study focuses on solvent selection and applies regression algorithms, transfer-learning, feature engineering, and active learning to this task.", "result": "The benchmarking results indicate that machine learning models can effectively predict yield based on continuous process conditions, with important implications for solvent replacement and sustainable manufacturing.", "conclusion": "This new dataset and the associated machine learning approaches provide valuable tools for advancing laboratory chemistry, particularly in optimizing solvent selection for more sustainable chemical processes."}}
{"id": "2506.06622", "pdf": "https://arxiv.org/pdf/2506.06622", "abs": "https://arxiv.org/abs/2506.06622", "authors": ["Yifan Zeng"], "title": "\\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) hold immense promise for revolutionizing\nfinancial analysis and decision-making, yet their direct application is often\nhampered by issues of data hallucination and lack of access to real-time,\nverifiable financial information. This paper introduces QuantMCP, a novel\nframework designed to rigorously ground LLMs in financial reality. By\nleveraging the Model Context Protocol (MCP) for standardized and secure tool\ninvocation, QuantMCP enables LLMs to accurately interface with a diverse array\nof Python-accessible financial data APIs (e.g., Wind, yfinance). Users can\ninteract via natural language to precisely retrieve up-to-date financial data,\nthereby overcoming LLM's inherent limitations in factual data recall. More\ncritically, once furnished with this verified, structured data, the LLM's\nanalytical capabilities are unlocked, empowering it to perform sophisticated\ndata interpretation, generate insights, and ultimately support more informed\nfinancial decision-making processes. QuantMCP provides a robust, extensible,\nand secure bridge between conversational AI and the complex world of financial\ndata, aiming to enhance both the reliability and the analytical depth of LLM\napplications in finance.", "AI": {"tldr": "QuantMCP is a framework that connects Large Language Models with financial data APIs through the Model Context Protocol, enhancing LLMs' ability to access accurate financial information and improving their analytical capabilities for better decision-making in finance.", "motivation": "To address the limitations of Large Language Models in financial analysis due to data hallucination and lack of access to real-time financial information.", "method": "By introducing QuantMCP, a framework that uses the Model Context Protocol to securely invoke Python-accessible financial data APIs, allowing LLMs to accurately retrieve up-to-date financial data.", "result": "Users can interact via natural language to retrieve precise financial data, unlocking LLM's analytical capabilities for sophisticated data interpretation and insight generation.", "conclusion": "QuantMCP provides a secure and extensible bridge between conversational AI and financial data, improving the reliability and depth of LLM applications in finance."}}
{"id": "2506.07624", "pdf": "https://arxiv.org/pdf/2506.07624", "abs": "https://arxiv.org/abs/2506.07624", "authors": ["Ali Hariri", "\u00c1lvaro Arroyo", "Alessio Gravina", "Moshe Eliasof", "Carola-Bibiane Sch\u00f6nlieb", "Davide Bacciu", "Kamyar Azizzadenesheli", "Xiaowen Dong", "Pierre Vandergheynst"], "title": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks", "categories": ["cs.LG"], "comment": null, "summary": "ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by\nMessage Passing Neural Networks (MPNNs), which gained popularity for their\nsimplicity and effectiveness in capturing local graph structure. Despite their\nsuccess, MPNNs are limited in their ability to capture long-range dependencies\nbetween nodes. This has led researchers to adapt MPNNs through rewiring or make\nuse of Graph Transformers, which compromises the computational efficiency that\ncharacterized early spatial message-passing architectures, and typically\ndisregards the graph structure. Almost a decade after its original\nintroduction, we revisit ChebNet to shed light on its ability to model distant\nnode interactions. We find that out-of-box, ChebNet already shows competitive\nadvantages relative to classical MPNNs and GTs on long-range benchmarks, while\nmaintaining good scalability properties for high-order polynomials. However, we\nuncover that this polynomial expansion leads ChebNet to an unstable regime\nduring training. To address this limitation, we cast ChebNet as a stable and\nnon-dissipative dynamical system, which we coin Stable-ChebNet. Our\nStable-ChebNet model allows for stable information propagation, and has\ncontrollable dynamics which do not require the use of eigendecompositions,\npositional encodings, or graph rewiring. Across several benchmarks,\nStable-ChebNet achieves near state-of-the-art performance.", "AI": {"tldr": "ChebNet\uff0c\u65e9\u671f\u7684\u8c31\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u6355\u6349\u957f\u8ddd\u79bb\u8282\u70b9\u4f9d\u8d56\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u672c\u6587\u63d0\u51faStable-ChebNet\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5176\u89c6\u4e3a\u7a33\u5b9a\u7684\u975e\u8017\u6563\u52a8\u529b\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1MPNNs\u5728\u6355\u6349\u5c40\u90e8\u56fe\u7ed3\u6784\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6355\u6349\u957f\u8ddd\u79bb\u8282\u70b9\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u5df2\u6709\u7684\u6539\u8fdb\u65b9\u6cd5\u5982\u91cd\u8fde\u7ebf\u6216\u4f7f\u7528Graph Transformers\uff0c\u5f80\u5f80\u727a\u7272\u8ba1\u7b97\u6548\u7387\u6216\u5ffd\u7565\u56fe\u7ed3\u6784\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u91cd\u65b0\u5ba1\u89c6ChebNet\uff0c\u63a2\u7d22\u5176\u5728\u5efa\u6a21\u8fdc\u8ddd\u79bb\u8282\u70b9\u4ea4\u4e92\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u8005\u9996\u5148\u5206\u6790\u4e86ChebNet\u5728\u957f\u8ddd\u79bb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5177\u6709\u7ade\u4e89\u529b\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5c06ChebNet\u5efa\u6a21\u4e3a\u4e00\u4e2a\u7a33\u5b9a\u4e14\u975e\u8017\u6563\u7684\u52a8\u529b\u7cfb\u7edf\uff0c\u79f0\u4e3aStable-ChebNet\u3002\u8be5\u6a21\u578b\u5141\u8bb8\u7a33\u5b9a\u7684\u4fe1\u606f\u4f20\u64ad\uff0c\u5e76\u5177\u6709\u53ef\u63a7\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u65e0\u9700\u7279\u5f81\u5206\u89e3\u3001\u4f4d\u7f6e\u7f16\u7801\u6216\u56fe\u91cd\u8fde\u7ebf\u3002", "result": "Stable-ChebNet\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u63a5\u8fd1\u6700\u5148\u8fdb\u6c34\u5e73\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "Stable-ChebNet\u4f5c\u4e3a\u4e00\u79cd\u6539\u8fdb\u7684ChebNet\u6a21\u578b\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u539f\u6a21\u578b\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u957f\u8ddd\u79bb\u8282\u70b9\u4ea4\u4e92\u5efa\u6a21\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u3002"}}
{"id": "2506.06630", "pdf": "https://arxiv.org/pdf/2506.06630", "abs": "https://arxiv.org/abs/2506.06630", "authors": ["Heeju Ko", "Sungjune Kim", "Gyeongrok Oh", "Jeongyoon Yoon", "Honglak Lee", "Sujin Jang", "Seungryong Kim", "Sangpil Kim"], "title": "Active Test-time Vision-Language Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Navigation (VLN) policies trained on offline datasets often\nexhibit degraded task performance when deployed in unfamiliar navigation\nenvironments at test time, where agents are typically evaluated without access\nto external interaction or feedback. Entropy minimization has emerged as a\npractical solution for reducing prediction uncertainty at test time; however,\nit can suffer from accumulated errors, as agents may become overconfident in\nincorrect actions without sufficient contextual grounding. To tackle these\nchallenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time\nactive learning framework that enables a practical human-robot interaction via\nepisodic feedback on uncertain navigation outcomes. In particular, ATENA learns\nto increase certainty in successful episodes and decrease it in failed ones,\nimproving uncertainty calibration. Here, we propose mixture entropy\noptimization, where entropy is obtained from a combination of the action and\npseudo-expert distributions-a hypothetical action distribution assuming the\nagent's selected action to be optimal-controlling both prediction confidence\nand action preference. In addition, we propose a self-active learning strategy\nthat enables an agent to evaluate its navigation outcomes based on confident\npredictions. As a result, the agent stays actively engaged throughout all\niterations, leading to well-grounded and adaptive decision-making. Extensive\nevaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate\nthat ATENA successfully overcomes distributional shifts at test time,\noutperforming the compared baseline methods across various settings.", "AI": {"tldr": "ATENA is a active learning framework that improves uncertainty calibration and decision-making for VLN policies in unfamiliar environments.", "motivation": "VLN policies trained on offline datasets often show degraded performance when deployed in new environments without external interaction or feedback. Entropy minimization can reduce prediction uncertainty but may suffer from accumulated errors due to overconfidence in incorrect actions.", "method": "Introduced ATENA, which uses episodic feedback to adjust certainty levels in successful and failed episodes, improving uncertainty calibration. Proposed mixture entropy optimization that controls prediction confidence and action preference by combining action and pseudo-expert distributions. Also proposed a self-active learning strategy enabling agents to evaluate navigation outcomes based on confident predictions.", "result": "Extensive evaluations on challenging VLN benchmarks (REVERIE, R2R, and R2R-CE) demonstrate that ATENA successfully overcomes distributional shifts at test time, outperforming baseline methods across various settings.", "conclusion": "ATENA enhances uncertainty calibration and decision-making for VLN policies in unfamiliar environments via active learning and mixture entropy optimization."}}
{"id": "2506.07661", "pdf": "https://arxiv.org/pdf/2506.07661", "abs": "https://arxiv.org/abs/2506.07661", "authors": ["Meir Feder", "Ruediger Urbanke", "Yaniv Fogel"], "title": "The Universality Lens: Why Even Highly Over-Parametrized Models Learn Well", "categories": ["cs.LG", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "A fundamental question in modern machine learning is why large,\nover-parameterized models, such as deep neural networks and transformers, tend\nto generalize well, even when their number of parameters far exceeds the number\nof training samples.\n  We investigate this phenomenon through the lens of information theory,\ngrounded in universal learning theory. Specifically, we study a Bayesian\nmixture learner with log-loss and (almost) uniform prior over an expansive\nhypothesis class.\n  Our key result shows that the learner's regret is not determined by the\noverall size of the hypothesis class, but rather by the cumulative probability\nof all models that are close, in Kullback-Leibler divergence distance, to the\ntrue data-generating process. We refer to this cumulative probability as the\nweight of the hypothesis.\n  This leads to a natural notion of model simplicity: simple models are those\nwith large weight and thus require fewer samples to generalize, while complex\nmodels have small weight and need more data. This perspective provides a\nrigorous and intuitive explanation for why over-parameterized models often\navoid overfitting: the presence of simple hypotheses allows the posterior to\nconcentrate on them when supported by the data.\n  We further bridge theory and practice by recalling that stochastic gradient\ndescent with Langevin dynamics samples from the correct posterior distribution,\nenabling our theoretical learner to be approximated using standard machine\nlearning methods combined with ensemble learning.\n  Our analysis yields non-uniform regret bounds and aligns with key practical\nconcepts such as flat minima and model distillation. The results apply broadly\nacross online, batch, and supervised learning settings, offering a unified and\nprincipled understanding of the generalization behavior of modern AI systems.", "AI": {"tldr": "The paper explores why over-parameterized models generalize well using information theory and universal learning theory, showing that model simplicity is tied to the cumulative probability of hypotheses close to the true data-generating process.", "motivation": "To understand why large, over-parameterized models like deep neural networks generalize well despite having more parameters than training samples.", "method": "Investigate through Bayesian mixture learner with log-loss and uniform prior; analyze regret bounds based on Kullback-Leibler divergence and hypothesis weight; connect theory to practice via stochastic gradient descent with Langevin dynamics.", "result": "Model simplicity defined by large hypothesis weight leads to better generalization; complex models require more data; over-parameterized models avoid overfitting due to presence of simple hypotheses.", "conclusion": "Provides a rigorous explanation for generalization of over-parameterized models and connects theoretical findings to practical concepts like flat minima and model distillation."}}
{"id": "2506.07666", "pdf": "https://arxiv.org/pdf/2506.07666", "abs": "https://arxiv.org/abs/2506.07666", "authors": ["Seyedhamidreza Mousavi", "Seyedali Mousavi", "Masoud Daneshtalab"], "title": "ProARD: progressive adversarial robustness distillation: provide wide range of robust students", "categories": ["cs.LG"], "comment": null, "summary": "Adversarial Robustness Distillation (ARD) has emerged as an effective method\nto enhance the robustness of lightweight deep neural networks against\nadversarial attacks. Current ARD approaches have leveraged a large robust\nteacher network to train one robust lightweight student. However, due to the\ndiverse range of edge devices and resource constraints, current approaches\nrequire training a new student network from scratch to meet specific\nconstraints, leading to substantial computational costs and increased CO2\nemissions. This paper proposes Progressive Adversarial Robustness Distillation\n(ProARD), enabling the efficient one-time training of a dynamic network that\nsupports a diverse range of accurate and robust student networks without\nrequiring retraining. We first make a dynamic deep neural network based on\ndynamic layers by encompassing variations in width, depth, and expansion in\neach design stage to support a wide range of architectures. Then, we consider\nthe student network with the largest size as the dynamic teacher network.\nProARD trains this dynamic network using a weight-sharing mechanism to jointly\noptimize the dynamic teacher network and its internal student networks.\nHowever, due to the high computational cost of calculating exact gradients for\nall the students within the dynamic network, a sampling mechanism is required\nto select a subset of students. We show that random student sampling in each\niteration fails to produce accurate and robust students.", "AI": {"tldr": "The paper proposes Progressive Adversarial Robustness Distillation (ProARD) to efficiently train a dynamic network that supports multiple accurate and robust student networks without retraining.", "motivation": "Current ARD approaches require training a new student network from scratch to meet specific constraints, leading to substantial computational costs and increased CO2 emissions.", "method": "Propose ProARD which makes a dynamic deep neural network based on dynamic layers by encompassing variations in width, depth, and expansion. The largest student network is considered as the dynamic teacher network. ProARD trains this dynamic network using a weight-sharing mechanism and uses a sampling mechanism to select a subset of students since calculating exact gradients for all students within the dynamic network is computationally expensive.", "result": "Random student sampling in each iteration fails to produce accurate and robust students.", "conclusion": "Not mentioned in the abstract."}}
{"id": "2506.07673", "pdf": "https://arxiv.org/pdf/2506.07673", "abs": "https://arxiv.org/abs/2506.07673", "authors": ["Guanhua Zhang", "Florian E. Dorner", "Moritz Hardt"], "title": "How Benchmark Prediction from Fewer Data Misses the Mark", "categories": ["cs.LG"], "comment": null, "summary": "Large language model (LLM) evaluation is increasingly costly, prompting\ninterest in methods that speed up evaluation by shrinking benchmark datasets.\nBenchmark prediction (also called efficient LLM evaluation) aims to select a\nsmall subset of evaluation points and predict overall benchmark performance\nfrom that subset. In this paper, we systematically assess the strengths and\nlimitations of 11 benchmark prediction methods across 19 diverse benchmarks.\nFirst, we identify a highly competitive baseline: Take a random sample and fit\na regression model on the sample to predict missing entries. Outperforming most\nexisting methods, this baseline challenges the assumption that careful subset\nselection is necessary for benchmark prediction. Second, we discover that all\nexisting methods crucially depend on model similarity. They work best when\ninterpolating scores among similar models. The effectiveness of benchmark\nprediction sharply declines when new models have higher accuracy than\npreviously seen models. In this setting of extrapolation, none of the previous\nmethods consistently beat a simple average over random samples. To improve over\nthe sample average, we introduce a new method inspired by augmented inverse\npropensity weighting. This method consistently outperforms the random sample\naverage even for extrapolation. However, its performance still relies on model\nsimilarity and the gains are modest in general. This shows that benchmark\nprediction fails just when it is most needed: at the evaluation frontier, where\nthe goal is to evaluate new models of unknown capabilities.", "AI": {"tldr": "The paper systematically evaluates 11 benchmark prediction methods across 19 benchmarks for efficient LLM evaluation, finding that most methods depend on model similarity and struggle with extrapolation. A new method inspired by augmented inverse propensity weighting outperforms random sampling in extrapolation but still relies on model similarity.", "motivation": "To address the increasing cost of evaluating large language models (LLMs), the study explores methods that speed up evaluation by shrinking benchmark datasets, focusing on selecting a small subset of evaluation points to predict overall benchmark performance.", "method": "The authors assess 11 benchmark prediction methods across 19 diverse benchmarks. They compare these methods against a competitive baseline which involves taking a random sample and fitting a regression model. Additionally, they introduce a new method based on augmented inverse propensity weighting designed to improve performance in extrapolation.", "result": "Most existing methods depend heavily on model similarity and perform poorly when extrapolating for new models with higher accuracy. The new introduced method outperforms random sampling averages in extrapolation scenarios but gains are modest and still rely on model similarity.", "conclusion": "Benchmark prediction struggles most where it is needed most - at the evaluation frontier for new models with unknown capabilities. Despite improvements, the effectiveness of these methods remains limited."}}
{"id": "2506.06657", "pdf": "https://arxiv.org/pdf/2506.06657", "abs": "https://arxiv.org/abs/2506.06657", "authors": ["Nikhita Vedula", "Dushyanta Dhyani", "Laleh Jalali", "Boris Oreshkin", "Mohsen Bayati", "Shervin Malmasi"], "title": "Quantile Regression with Large Language Models for Price Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL, 2025", "summary": "Large Language Models (LLMs) have shown promise in structured prediction\ntasks, including regression, but existing approaches primarily focus on point\nestimates and lack systematic comparison across different methods. We\ninvestigate probabilistic regression using LLMs for unstructured inputs,\naddressing challenging text-to-distribution prediction tasks such as price\nestimation where both nuanced text understanding and uncertainty quantification\nare critical. We propose a novel quantile regression approach that enables LLMs\nto produce full predictive distributions, improving upon traditional point\nestimates. Through extensive experiments across three diverse price prediction\ndatasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads\nsignificantly outperforms traditional approaches for both point and\ndistributional estimations, as measured by three established metrics each for\nprediction accuracy and distributional calibration. Our systematic comparison\nof LLM approaches, model architectures, training approaches, and data scaling\nreveals that Mistral-7B consistently outperforms encoder architectures,\nembedding-based methods, and few-shot learning methods. Our experiments also\nreveal the effectiveness of LLM-assisted label correction in achieving\nhuman-level accuracy without systematic bias. Our curated datasets are made\navailable at https://github.com/vnik18/llm-price-quantile-reg/ to support\nfuture research.", "AI": {"tldr": "The paper explores probabilistic regression using Large Language Models (LLMs) for unstructured inputs, proposing a novel quantile regression approach that enables LLMs to produce full predictive distributions. Experiments show Mistral-7B outperforms traditional methods in price prediction tasks.", "motivation": "Existing approaches for structured prediction tasks with LLMs mainly focus on point estimates and lack systematic comparisons across different methods. There is a need for a method that addresses challenging text-to-distribution prediction tasks such as price estimation, which require nuanced text understanding and uncertainty quantification.", "method": "The authors propose a novel quantile regression approach that enables LLMs to produce full predictive distributions, improving upon traditional point estimates. They use a Mistral-7B model fine-tuned with quantile heads and conduct extensive experiments across three diverse price prediction datasets.", "result": "The Mistral-7B model fine-tuned with quantile heads significantly outperforms traditional approaches for both point and distributional estimations. It consistently surpasses encoder architectures, embedding-based methods, and few-shot learning methods. The LLM-assisted label correction achieves human-level accuracy without systematic bias.", "conclusion": "The proposed quantile regression approach using LLMs is effective for probabilistic regression tasks with unstructured inputs. The systematic comparison reveals the superiority of Mistral-7B in price prediction tasks. Curated datasets are made available to support future research."}}
{"id": "2506.07706", "pdf": "https://arxiv.org/pdf/2506.07706", "abs": "https://arxiv.org/abs/2506.07706", "authors": ["Boris Martirosyan", "Alexey Karmanov"], "title": "Evaluating Robustness in Latent Diffusion Models via Embedding Level Augmentation", "categories": ["cs.LG"], "comment": null, "summary": "Latent diffusion models (LDMs) achieve state-of-the-art performance across\nvarious tasks, including image generation and video synthesis. However, they\ngenerally lack robustness, a limitation that remains not fully explored in\ncurrent research. In this paper, we propose several methods to address this\ngap. First, we hypothesize that the robustness of LDMs primarily should be\nmeasured without their text encoder, because if we take and explore the whole\narchitecture, the problems of image generator and text encoders wll be fused.\nSecond, we introduce novel data augmentation techniques designed to reveal\nrobustness shortcomings in LDMs when processing diverse textual prompts. We\nthen fine-tune Stable Diffusion 3 and Stable Diffusion XL models using\nDreambooth, incorporating these proposed augmentation methods across multiple\ntasks. Finally, we propose a novel evaluation pipeline specifically tailored to\nassess the robustness of LDMs fine-tuned via Dreambooth.", "AI": {"tldr": "The paper addresses the lack of robustness in Latent Diffusion Models (LDMs) by proposing methods to measure and improve it, focusing on separating image generation from text encoding issues. It introduces new data augmentation techniques, fine-tunes Stable Diffusion models using Dreambooth, and proposes a tailored evaluation pipeline.", "motivation": "Current research does not fully explore the robustness limitations of LDMs, particularly when processing diverse textual prompts.", "method": "1. Measure LDM robustness without considering the text encoder to isolate image generation issues. 2. Introduce novel data augmentation techniques to reveal robustness shortcomings. 3. Fine-tune Stable Diffusion models using Dreambooth with these augmentations across multiple tasks. 4. Propose a novel evaluation pipeline for assessing robustness.", "result": "Not explicitly stated in the abstract, but the methods aim to reveal and address robustness shortcomings in LDMs.", "conclusion": "The proposed methods and evaluation pipeline provide a way to better understand and enhance the robustness of LDMs."}}
{"id": "2506.06658", "pdf": "https://arxiv.org/pdf/2506.06658", "abs": "https://arxiv.org/abs/2506.06658", "authors": ["Calvin Luo", "Zilai Zeng", "Mingxi Jia", "Yilun Du", "Chen Sun"], "title": "Self-Adapting Improvement Loops for Robotic Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Video generative models trained on expert demonstrations have been utilized\nas performant text-conditioned visual planners for solving robotic tasks.\nHowever, generalization to unseen tasks remains a challenge. Whereas improved\ngeneralization may be facilitated by leveraging learned prior knowledge from\nadditional pre-collected offline data sources, such as web-scale video\ndatasets, in the era of experience we aim to design agents that can\ncontinuously improve in an online manner from self-collected behaviors. In this\nwork we thus propose the Self-Adapting Improvement Loop (SAIL), where an\nin-domain video model iteratively updates itself on self-produced trajectories,\ncollected through adaptation with an internet-scale pretrained video model, and\nsteadily improves its performance for a specified task of interest. We apply\nSAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks\non a real robot arm, and find that performance improvements continuously emerge\nover multiple iterations for novel tasks initially unseen during original\nin-domain video model training. Furthermore, we discover that SAIL is\nsurprisingly robust regarding if and how the self-collected experience is\nfiltered, and the quality of the initial in-domain demonstrations. Through\nadaptation with summarized internet-scale data, and learning through online\nexperience, we thus demonstrate a way to iteratively bootstrap a\nhigh-performance video model for solving novel robotic tasks through\nself-improvement.", "AI": {"tldr": "The paper proposes Self-Adapting Improvement Loop (SAIL), which uses an in-domain video model to iteratively update itself on self-produced trajectories, improving performance for novel tasks. SAIL is robust and effective even with filtering or varying quality of initial demonstrations.", "motivation": "Current video generative models struggle with generalizing to unseen robotic tasks. Leveraging learned prior knowledge from additional offline data sources and designing agents that can continuously improve online could address this issue.", "method": "Propose SAIL where an in-domain video model updates itself using self-produced trajectories collected through adaptation with a pretrained internet-scale video model, enhancing task-specific performance over iterations.", "result": "SAIL leads to continuous performance improvements across multiple iterations on unseen tasks, both in simulation (MetaWorld tasks) and real-world scenarios (manipulation tasks on a robot arm). It shows robustness to filtering and quality variations of initial demonstrations.", "conclusion": "SAIL demonstrates a method to iteratively enhance a high-performance video model for solving new robotic tasks via self-improvement, combining adaptation with summarized large-scale data and learning from online experience."}}
{"id": "2506.07735", "pdf": "https://arxiv.org/pdf/2506.07735", "abs": "https://arxiv.org/abs/2506.07735", "authors": ["Haizhao Jing", "Haokui Zhang", "Zhenhao Shang", "Rong Xiao", "Peng Wang", "Yanning Zhang"], "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": "9 pages, 3 figures", "summary": "Neural Architecture Representation Learning aims to transform network models\ninto feature representations for predicting network attributes, playing a\ncrucial role in deploying and designing networks for real-world applications.\nRecently, inspired by the success of transformers, transformer-based models\nintegrated with Graph Neural Networks (GNNs) have achieved significant progress\nin representation learning. However, current methods still have some\nlimitations. First, existing methods overlook hardware attribute information,\nwhich conflicts with the current trend of diversified deep learning hardware\nand limits the practical applicability of models. Second, current encoding\napproaches rely on static adjacency matrices to represent topological\nstructures, failing to capture the structural differences between computational\nnodes, which ultimately compromises encoding effectiveness. In this paper, we\nintroduce LeDG-Former, an innovative framework that addresses these limitations\nthrough the synergistic integration of language-based semantic embedding and\ndynamic graph representation learning. Specifically, inspired by large language\nmodels (LLMs), we propose a language embedding framework where both neural\narchitectures and hardware platform specifications are projected into a unified\nsemantic space through tokenization and LLM processing, enabling zero-shot\nprediction across different hardware platforms for the first time. Then, we\npropose a dynamic graph-based transformer for modeling neural architectures,\nresulting in improved neural architecture modeling performance. On the NNLQP\nbenchmark, LeDG-Former surpasses previous methods, establishing a new SOTA\nwhile demonstrating the first successful cross-hardware latency prediction\ncapability. Furthermore, our framework achieves superior performance on the\ncell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "AI": {"tldr": "This paper introduces LeDG-Former, a novel framework that integrates language-based semantic embedding and dynamic graph representation learning for neural architecture representation. It addresses current limitations by incorporating hardware attribute information and improving encoding effectiveness. LeDG-Former outperforms previous methods on benchmarks and demonstrates cross-hardware latency prediction capability.", "motivation": "Current methods in neural architecture representation learning overlook hardware attribute information and rely on static adjacency matrices to represent topological structures, which limits their practical applicability and compromises encoding effectiveness.", "method": "The authors propose LeDG-Former, which includes a language embedding framework inspired by large language models (LLMs) to project neural architectures and hardware platform specifications into a unified semantic space, enabling zero-shot prediction across different hardware platforms. They also propose a dynamic graph-based transformer for modeling neural architectures.", "result": "LeDG-Former surpasses previous methods on the NNLQP benchmark, establishing a new SOTA while demonstrating the first successful cross-hardware latency prediction capability. It also achieves superior performance on the cell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "conclusion": "LeDG-Former effectively addresses the limitations of existing methods by incorporating hardware attribute information and improving encoding effectiveness through dynamic graph representation learning."}}
{"id": "2506.06659", "pdf": "https://arxiv.org/pdf/2506.06659", "abs": "https://arxiv.org/abs/2506.06659", "authors": ["Wenhao Yao", "Zhenxin Li", "Shiyi Lan", "Zi Wang", "Xinglong Sun", "Jose M. Alvarez", "Zuxuan Wu"], "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "15 pages, 6 figures", "summary": "In complex driving environments, autonomous vehicles must navigate safely.\nRelying on a single predicted path, as in regression-based approaches, usually\ndoes not explicitly assess the safety of the predicted trajectory.\nSelection-based methods address this by generating and scoring multiple\ntrajectory candidates and predicting the safety score for each, but face\noptimization challenges in precisely selecting the best option from thousands\nof possibilities and distinguishing subtle but safety-critical differences,\nespecially in rare or underrepresented scenarios. We propose DriveSuprim to\novercome these challenges and advance the selection-based paradigm through a\ncoarse-to-fine paradigm for progressive candidate filtering, a rotation-based\naugmentation method to improve robustness in out-of-distribution scenarios, and\na self-distillation framework to stabilize training. DriveSuprim achieves\nstate-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS\nin NAVSIM v2 without extra data, demonstrating superior safetycritical\ncapabilities, including collision avoidance and compliance with rules, while\nmaintaining high trajectory quality in various driving scenarios.", "AI": {"tldr": "DriveSuprim is a new method for autonomous vehicle navigation that improves safety and performance through a coarse-to-fine filtering process, rotation-based augmentation, and self-distillation framework.", "motivation": "Current selection-based methods for autonomous vehicle trajectory planning face challenges in selecting the best option from thousands of possibilities and distinguishing subtle but safety-critical differences, especially in rare scenarios.", "method": "DriveSuprim employs a coarse-to-fine paradigm for progressive candidate filtering, a rotation-based augmentation method to improve robustness in out-of-distribution scenarios, and a self-distillation framework to stabilize training.", "result": "DriveSuprim achieves state-of-the-art performance with 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS in NAVSIM v2 without extra data, showing superior safety-critical capabilities including collision avoidance and compliance with rules.", "conclusion": "DriveSuprim advances the selection-based paradigm and demonstrates superior safety-critical capabilities while maintaining high trajectory quality in various driving scenarios."}}
{"id": "2506.07744", "pdf": "https://arxiv.org/pdf/2506.07744", "abs": "https://arxiv.org/abs/2506.07744", "authors": ["Seungho Baek", "Taegeon Park", "Jongchan Park", "Seungjun Oh", "Yusung Kim"], "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICML 2025", "summary": "Existing offline hierarchical reinforcement learning methods rely on\nhigh-level policy learning to generate subgoal sequences. However, their\nefficiency degrades as task horizons increase, and they lack effective\nstrategies for stitching useful state transitions across different\ntrajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that\nformulates subgoal selection as a graph search problem rather than learning an\nexplicit high-level policy. By embedding states into a Temporal Distance\nRepresentation (TDR) space, GAS clusters semantically similar states from\ndifferent trajectories into unified graph nodes, enabling efficient transition\nstitching. A shortest-path algorithm is then applied to select subgoal\nsequences within the graph, while a low-level policy learns to reach the\nsubgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)\nmetric, which filters out noisy or inefficient transition states, significantly\nenhancing task performance. GAS outperforms prior offline HRL methods across\nlocomotion, navigation, and manipulation tasks. Notably, in the most\nstitching-critical task, it achieves a score of 88.3, dramatically surpassing\nthe previous state-of-the-art score of 1.0. Our source code is available at:\nhttps://github.com/qortmdgh4141/GAS.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6Graph-Assisted Stitching (GAS)\uff0c\u5c06\u5b50\u76ee\u6807\u9009\u62e9\u4f5c\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5b66\u4e60\u663e\u5f0f\u7684\u9ad8\u5c42\u7b56\u7565\u3002\u901a\u8fc7\u5d4c\u5165\u72b6\u6001\u5230\u65f6\u95f4\u8ddd\u79bb\u8868\u793a\uff08TDR\uff09\u7a7a\u95f4\uff0cGAS\u5c06\u8bed\u4e49\u76f8\u4f3c\u7684\u72b6\u6001\u805a\u7c7b\u6210\u7edf\u4e00\u7684\u56fe\u8282\u70b9\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u6548\u7684\u8f6c\u6362\u7f1d\u5408\u3002GAS\u5728\u8fd0\u52a8\u3001\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4ee5\u524d\u7684\u79bb\u7ebfHRL\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u79bb\u7ebf\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9ad8\u7ea7\u7b56\u7565\u5b66\u4e60\u6765\u751f\u6210\u5b50\u76ee\u6807\u5e8f\u5217\uff0c\u4f46\u968f\u7740\u4efb\u52a1\u8303\u56f4\u7684\u589e\u52a0\uff0c\u5176\u6548\u7387\u4e0b\u964d\uff0c\u5e76\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u7b56\u7565\u6765\u7f1d\u5408\u4e0d\u540c\u8f68\u8ff9\u4e2d\u6709\u7528\u7684\u72b6\u6001\u8f6c\u6362\u3002", "method": "1. \u5c06\u5b50\u76ee\u6807\u9009\u62e9\u4f5c\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u800c\u4e0d\u662f\u5b66\u4e60\u663e\u5f0f\u7684\u9ad8\u5c42\u7b56\u7565\u3002\n2. \u901a\u8fc7\u5d4c\u5165\u72b6\u6001\u5230\u65f6\u95f4\u8ddd\u79bb\u8868\u793a\uff08TDR\uff09\u7a7a\u95f4\uff0c\u5c06\u8bed\u4e49\u76f8\u4f3c\u7684\u72b6\u6001\u805a\u7c7b\u6210\u7edf\u4e00\u7684\u56fe\u8282\u70b9\u3002\n3. \u4f7f\u7528\u6700\u77ed\u8def\u5f84\u7b97\u6cd5\u5728\u56fe\u4e2d\u9009\u62e9\u5b50\u76ee\u6807\u5e8f\u5217\uff0c\u540c\u65f6\u4f4e\u7ea7\u7b56\u7565\u5b66\u4e60\u5230\u8fbe\u5b50\u76ee\u6807\u3002\n4. \u5f15\u5165\u65f6\u95f4\u6548\u7387\uff08TE\uff09\u5ea6\u91cf\uff0c\u8fc7\u6ee4\u6389\u5608\u6742\u6216\u4f4e\u6548\u7684\u8f6c\u6362\u72b6\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u3002", "result": "GAS\u5728\u8fd0\u52a8\u3001\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4ee5\u524d\u7684\u79bb\u7ebfHRL\u65b9\u6cd5\u3002\u7279\u522b\u662f\u5728\u6700\u9700\u8981\u7f1d\u5408\u7684\u4efb\u52a1\u4e2d\uff0c\u5f97\u5206\u4e3a88.3\uff0c\u8fdc\u8d85\u4e4b\u524d\u6700\u5148\u8fdb\u7684\u5f97\u52061.0\u3002", "conclusion": "GAS\u6846\u67b6\u901a\u8fc7\u5c06\u5b50\u76ee\u6807\u9009\u62e9\u4f5c\u4e3a\u56fe\u641c\u7d22\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2506.06683", "pdf": "https://arxiv.org/pdf/2506.06683", "abs": "https://arxiv.org/abs/2506.06683", "authors": ["Shiying Duan", "Pei Ren", "Nanxiang Jiang", "Zhengping Che", "Jian Tang", "Yifan Sun", "Zhaoxin Fan", "Wenjun Wu"], "title": "RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Dual-arm robots play a crucial role in improving efficiency and flexibility\nin complex multitasking scenarios. While existing methods have achieved\npromising results in task planning, they often fail to fully optimize task\nparallelism, limiting the potential of dual-arm collaboration. To address this\nissue, we propose RoboPARA, a novel large language model (LLM)-driven framework\nfor dual-arm task parallelism planning. RoboPARA employs a two-stage process:\n(1) Dependency Graph-based Planning Candidates Generation, which constructs\ndirected acyclic graphs (DAGs) to model task dependencies and eliminate\nredundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which\noptimizes DAG traversal to maximize parallelism while maintaining task\ncoherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task\ndataset (X-DAPT dataset), the first dataset specifically designed to evaluate\ndual-arm task parallelism across diverse scenarios and difficulty levels.\nExtensive experiments on the X-DAPT dataset demonstrate that RoboPARA\nsignificantly outperforms existing methods, achieving higher efficiency and\nreliability, particularly in complex task combinations. The code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "Dual-arm robots' efficiency and flexibility are enhanced by optimizing task parallelism through RoboPARA, a LLM-driven framework. It uses two stages for planning and introduces X-DAPT dataset. Experiments show it outperforms existing methods.", "motivation": "Existing methods in dual-arm robot task planning have not fully optimized task parallelism, thus limiting the potential of dual-arm collaboration.", "method": "RoboPARA is proposed with a two-stage process: 1) Dependency Graph-based Planning Candidates Generation which constructs DAGs to model task dependencies and eliminate redundancy; 2) Graph Re-Traversal-based Dual-Arm Parallel Planning which optimizes DAG traversal for maximum parallelism while maintaining task coherence.", "result": "Extensive experiments on the newly introduced X-DAPT dataset demonstrate that RoboPARA significantly outperforms existing methods in terms of efficiency and reliability, especially in complex task combinations.", "conclusion": "RoboPARA offers an advanced solution for dual-arm task parallelism planning, improving efficiency and reliability. The associated code and dataset will be released upon acceptance."}}
{"id": "2506.07747", "pdf": "https://arxiv.org/pdf/2506.07747", "abs": "https://arxiv.org/abs/2506.07747", "authors": ["Adam Breuer"], "title": "E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "ICML 2025; Code available at: https://github.com/BreuerLabs/E- LDA", "summary": "In this paper, we provide the first practical algorithms with provable\nguarantees for the problem of inferring the topics assigned to each document in\nan LDA topic model. This is the primary inference problem for many applications\nof topic models in social science, data exploration, and causal inference\nsettings. We obtain this result by showing a novel non-gradient-based,\ncombinatorial approach to estimating topic models. This yields algorithms that\nconverge to near-optimal posterior probability in logarithmic parallel\ncomputation time (adaptivity) -- exponentially faster than any known LDA\nalgorithm. We also show that our approach can provide interpretability\nguarantees such that each learned topic is formally associated with a known\nkeyword. Finally, we show that unlike alternatives, our approach can maintain\nthe independence assumptions necessary to use the learned topic model for\ndownstream causal inference methods that allow researchers to study topics as\ntreatments. In terms of practical performance, our approach consistently\nreturns solutions of higher semantic quality than solutions from\nstate-of-the-art LDA algorithms, neural topic models, and LLM-based topic\nmodels across a diverse range of text datasets and evaluation parameters.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u975e\u68af\u5ea6\u7ec4\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1LDA\u4e3b\u9898\u6a21\u578b\u4e2d\u7684\u6587\u6863\u4e3b\u9898\u5206\u914d\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u6709\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\uff0c\u800c\u4e14\u5728\u5b9e\u8df5\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002\u76f8\u8f83\u4e8e\u73b0\u6709\u7b97\u6cd5\uff0c\u65b0\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u597d\u7684\u8bed\u4e49\u8d28\u91cf\u548c\u66f4\u5f3a\u7684\u56e0\u679c\u63a8\u65ad\u80fd\u529b\u3002", "motivation": "\u5f53\u524dLDA\u4e3b\u9898\u6a21\u578b\u4e2d\u7684\u6587\u6863\u4e3b\u9898\u5206\u914d\u95ee\u9898\u7f3a\u4e4f\u65e2\u5b9e\u7528\u53c8\u6709\u7406\u8bba\u4fdd\u969c\u7684\u7b97\u6cd5\uff0c\u800c\u8fd9\u4e00\u95ee\u9898\u5bf9\u4e8e\u793e\u4f1a\u79d1\u5b66\u3001\u6570\u636e\u63a2\u7d22\u548c\u56e0\u679c\u63a8\u65ad\u7b49\u9886\u57df\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u68af\u5ea6\u7ec4\u5408\u7684\u65b0\u9896\u4f30\u8ba1\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u5bf9\u6570\u5e76\u884c\u8ba1\u7b97\u65f6\u95f4\u6536\u655b\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u540e\u9a8c\u6982\u7387\uff0c\u5e76\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u4e3b\u9898\u4e0e\u5df2\u77e5\u5173\u952e\u8bcd\u76f8\u5173\u8054\uff0c\u540c\u65f6\u7ef4\u6301\u5fc5\u8981\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u4ee5\u652f\u6301\u4e0b\u6e38\u56e0\u679c\u63a8\u65ad\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u6587\u672c\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u53c2\u6570\u4e0b\uff0c\u8bed\u4e49\u8d28\u91cf\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684LDA\u7b97\u6cd5\u3001\u795e\u7ecf\u4e3b\u9898\u6a21\u578b\u548cLLM\u57fa\u4e3b\u9898\u6a21\u578b\u3002", "conclusion": "\u8fd9\u79cd\u65b0\u65b9\u6cd5\u4e3a\u89e3\u51b3LDA\u4e3b\u9898\u6a21\u578b\u4e2d\u7684\u6838\u5fc3\u63a8\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u66f4\u53ef\u9760\u4e14\u66f4\u5177\u89e3\u91ca\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8fdb\u884c\u56e0\u679c\u63a8\u65ad\u6240\u9700\u7684\u72ec\u7acb\u6027\u5047\u8bbe\u3002"}}
{"id": "2506.06693", "pdf": "https://arxiv.org/pdf/2506.06693", "abs": "https://arxiv.org/abs/2506.06693", "authors": ["Priyanshu Yadav"], "title": "Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing", "categories": ["cs.AR", "cs.AI", "eess.SP", "C.1.3, B.5.2, I.5.1 C.1.3, B.5.2, I.5.1 C.1.3, B.5.2, I.5.1"], "comment": "12 Pages, 1 figure", "summary": "This paper presents a comprehensive analysis of the RISC-V instruction set\narchitecture, focusing on its modular design, implementation challenges, and\nperformance characteristics. We examine the RV32I base instruction set with\nextensions for multiplication (M) and atomic operations (A). Through\ncycle-accurate simulation of a pipelined implementation, we evaluate\nperformance metrics including CPI (cycles per instruction) and power\nefficiency. Our results demonstrate RISC-V's advantages in embedded systems and\nits scalability for custom accelerators. Comparative analysis shows a 17%\nreduction in power consumption compared to ARM Cortex-M0 implementations in\nsimilar process nodes. The open-standard nature of RISC-V provides significant\nflexibility for domain-specific optimizations.", "AI": {"tldr": "The paper analyzes RISC-V ISA focusing on modular design, implementation challenges and performance characteristics. Through cycle-accurate simulation of RV32I with M/A extensions, it shows 17% power reduction vs ARM Cortex-M0 and highlights flexibility for optimizations.", "motivation": "To provide a comprehensive evaluation of RISC-V architecture's suitability for embedded systems and custom accelerator designs by analyzing its design, implementation challenges and performance metrics.", "method": "Conducting cycle-accurate simulation of a pipelined RISC-V implementation using RV32I base instruction set with M and A extensions to evaluate CPI and power efficiency.", "result": "Demonstrated 17% power consumption reduction compared to ARM Cortex-M0 in similar process nodes, showcasing RISC-V's advantages in embedded systems.", "conclusion": "RISC-V's open-standard nature provides significant flexibility for domain-specific optimizations while offering better power efficiency for embedded applications."}}
{"id": "2506.07754", "pdf": "https://arxiv.org/pdf/2506.07754", "abs": "https://arxiv.org/abs/2506.07754", "authors": ["Nicola Lavecchia", "Sid Fadanelli", "Federico Ricciuti", "Gennaro Aloe", "Enrico Bagli", "Pietro Giuffrida", "Daniele Vergari"], "title": "Comparing Credit Risk Estimates in the Gen-AI Era", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative AI technologies have demonstrated significant potential across\ndiverse applications. This study provides a comparative analysis of credit\nscore modeling techniques, contrasting traditional approaches with those\nleveraging generative AI. Our findings reveal that current generative AI models\nfall short of matching the performance of traditional methods, regardless of\nthe integration strategy employed. These results highlight the limitations in\nthe current capabilities of generative AI for credit risk scoring, emphasizing\nthe need for further research and development before the possibility of\napplying generative AI for this specific task, or equivalent ones.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u6280\u672f\u5728\u4fe1\u7528\u8bc4\u5206\u5efa\u6a21\u4e2d\u5c1a\u672a\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "motivation": "\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u4fe1\u7528\u8bc4\u5206\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u53ca\u5c40\u9650\u6027\u3002", "method": "\u5bf9\u6bd4\u5206\u6790\u4f20\u7edf\u4fe1\u7528\u8bc4\u5206\u5efa\u6a21\u65b9\u6cd5\u4e0e\u751f\u6210\u5f0fAI\u6280\u672f\u3002", "result": "\u5f53\u524d\u751f\u6210\u5f0fAI\u6a21\u578b\u65e0\u8bba\u91c7\u7528\u4f55\u79cd\u96c6\u6210\u7b56\u7565\uff0c\u5176\u6027\u80fd\u5747\u672a\u8fbe\u5230\u4f20\u7edf\u65b9\u6cd5\u7684\u6c34\u5e73\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u4fe1\u7528\u98ce\u9669\u8bc4\u5206\u4efb\u52a1\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5f00\u53d1\u624d\u80fd\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2506.07769", "pdf": "https://arxiv.org/pdf/2506.07769", "abs": "https://arxiv.org/abs/2506.07769", "authors": ["Dekai Zhang", "Matthew Williams", "Francesca Toni"], "title": "Clustered Federated Learning via Embedding Distributions", "categories": ["cs.LG"], "comment": "24 pages", "summary": "Federated learning (FL) is a widely used framework for machine learning in\ndistributed data environments where clients hold data that cannot be easily\ncentralised, such as for data protection reasons. FL, however, is known to be\nvulnerable to non-IID data. Clustered FL addresses this issue by finding more\nhomogeneous clusters of clients. We propose a novel one-shot clustering method,\nEMD-CFL, using the Earth Mover's distance (EMD) between data distributions in\nembedding space. We theoretically motivate the use of EMDs using results from\nthe domain adaptation literature and demonstrate empirically superior\nclustering performance in extensive comparisons against 16 baselines and on a\nrange of challenging datasets.", "AI": {"tldr": "In federated learning, non-IID data can cause problems. This paper proposes EMD-CFL, a new one-shot clustering method that uses Earth Mover's distance in embedding space to find more homogeneous clusters of clients, improving performance.", "motivation": "Federated learning is vulnerable to non-IID data, which can degrade model performance. To address this issue, clustered federated learning aims to group clients into more homogeneous clusters.", "method": "The proposed method, EMD-CFL, is a one-shot clustering approach that leverages the Earth Mover's distance (EMD) between data distributions in the embedding space to identify clusters of clients with similar data distributions.", "result": "The method shows empirically superior clustering performance compared to 16 baselines across a range of challenging datasets, demonstrating its effectiveness.", "conclusion": "EMD-CFL provides an effective solution for addressing the non-IID problem in federated learning by creating more homogeneous client clusters."}}
{"id": "2506.07804", "pdf": "https://arxiv.org/pdf/2506.07804", "abs": "https://arxiv.org/abs/2506.07804", "authors": ["Jie Bao", "Chuangyin Dang", "Rui Luo", "Hanwei Zhang", "Zhixin Zhou"], "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "As deep learning models are increasingly deployed in high-risk applications,\nrobust defenses against adversarial attacks and reliable performance guarantees\nbecome paramount. Moreover, accuracy alone does not provide sufficient\nassurance or reliable uncertainty estimates for these models. This study\nadvances adversarial training by leveraging principles from Conformal\nPrediction. Specifically, we develop an adversarial attack method, termed OPSA\n(OPtimal Size Attack), designed to reduce the efficiency of conformal\nprediction at any significance level by maximizing model uncertainty without\nrequiring coverage guarantees. Correspondingly, we introduce OPSA-AT\n(Adversarial Training), a defense strategy that integrates OPSA within a novel\nconformal training paradigm. Experimental evaluations demonstrate that our OPSA\nattack method induces greater uncertainty compared to baseline approaches for\nvarious defenses. Conversely, our OPSA-AT defensive model significantly\nenhances robustness not only against OPSA but also other adversarial attacks,\nand maintains reliable prediction. Our findings highlight the effectiveness of\nthis integrated approach for developing trustworthy and resilient deep learning\nmodels for safety-critical domains. Our code is available at\nhttps://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.", "AI": {"tldr": "This study combines adversarial training with Conformal Prediction principles to enhance model robustness and reliability in high-risk applications. It introduces OPSA, an attack method that increases model uncertainty, and OPSA-AT, a defense strategy that boosts resilience against various adversarial attacks while maintaining accurate predictions.", "motivation": "Deep learning models are being used more frequently in high-risk scenarios where robust defenses against adversarial attacks and reliable performance guarantees are essential. Accuracy alone is insufficient for providing assurance or reliable uncertainty estimates for these models.", "method": "The study develops OPSA (OPtimal Size Attack), which maximizes model uncertainty without coverage guarantees, and OPSA-AT (Adversarial Training), a defense mechanism integrating OPSA within a conformal training framework.", "result": "Experimental results indicate that the OPSA attack induces higher uncertainty than baseline methods across different defenses. Meanwhile, the OPSA-AT defensive model significantly improves robustness against not only OPSA but also other adversarial attacks, ensuring reliable predictions.", "conclusion": "The integrated approach of combining adversarial training with Conformal Prediction principles effectively develops trustworthy and resilient deep learning models suitable for safety-critical domains."}}
{"id": "2506.06705", "pdf": "https://arxiv.org/pdf/2506.06705", "abs": "https://arxiv.org/abs/2506.06705", "authors": ["Zhihui Chen", "Kai He", "Yucheng Huang", "Yunxiao Zhu", "Mengling Feng"], "title": "DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains", "categories": ["cs.CL", "cs.AI"], "comment": "Zhihui Chen and Kai He contributed equally to this work, Mengling\n  Feng is the corresponding author", "summary": "Detecting LLM-generated text in specialized and high-stakes domains like\nmedicine and law is crucial for combating misinformation and ensuring\nauthenticity. However, current zero-shot detectors, while effective on general\ntext, often fail when applied to specialized content due to domain shift. We\nprovide a theoretical analysis showing this failure is fundamentally linked to\nthe KL divergence between human, detector, and source text distributions. To\naddress this, we propose DivScore, a zero-shot detection framework using\nnormalized entropy-based scoring and domain knowledge distillation to robustly\nidentify LLM-generated text in specialized domains. We also release a\ndomain-specific benchmark for LLM-generated text detection in the medical and\nlegal domains. Experiments on our benchmark show that DivScore consistently\noutperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%\nhigher recall (0.1% false positive rate threshold). In adversarial settings,\nDivScore demonstrates superior robustness than other baselines, achieving on\naverage 22.8% advantage in AUROC and 29.5% in recall. Code and data are\npublicly available.", "AI": {"tldr": "Detecting LLM-generated text in specialized domains is crucial but challenging due to domain shift. This paper proposes DivScore, a zero-shot detection framework that outperforms current methods in medical and legal fields.", "motivation": "Current zero-shot detectors fail to effectively detect LLM-generated text in specialized domains like medicine and law because of the domain shift issue.", "method": "The paper proposes DivScore, a zero-shot detection framework using normalized entropy-based scoring and domain knowledge distillation to identify LLM-generated text in specialized domains. It also introduces a theoretical analysis linking detector failure to KL divergence between different text distributions.", "result": "Experiments on a domain-specific benchmark show DivScore consistently outperforms state-of-the-art detectors with 14.4% higher AUROC and 64.0% higher recall at 0.1% false positive rate threshold. In adversarial settings, it achieves 22.8% advantage in AUROC and 29.5% in recall.", "conclusion": "DivScore provides a robust solution for detecting LLM-generated text in specialized domains such as medicine and law, significantly improving upon existing methods."}}
{"id": "2506.07806", "pdf": "https://arxiv.org/pdf/2506.07806", "abs": "https://arxiv.org/abs/2506.07806", "authors": ["Avinash Kori", "Francesca Toni", "Ben Glocker"], "title": "Identifiable Object Representations under Spatial Ambiguities", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Modular object-centric representations are essential for *human-like\nreasoning* but are challenging to obtain under spatial ambiguities, *e.g. due\nto occlusions and view ambiguities*. However, addressing challenges presents\nboth theoretical and practical difficulties. We introduce a novel multi-view\nprobabilistic approach that aggregates view-specific slots to capture\n*invariant content* information while simultaneously learning disentangled\nglobal *viewpoint-level* information. Unlike prior single-view methods, our\napproach resolves spatial ambiguities, provides theoretical guarantees for\nidentifiability, and requires *no viewpoint annotations*. Extensive experiments\non standard benchmarks and novel complex datasets validate our method's\nrobustness and scalability.", "AI": {"tldr": "The paper proposes a multi-view probabilistic approach that aggregates view-specific slots to capture invariant content information and disentangled global viewpoint-level information, resolving spatial ambiguities without needing viewpoint annotations.", "motivation": "Modular object-centric representations are crucial for human-like reasoning but hard to obtain under spatial ambiguities such as occlusions and view ambiguities.", "method": "A novel multi-view probabilistic approach is introduced. It aggregates view-specific slots for invariant content information while learning disentangled global viewpoint-level information simultaneously.", "result": "Through extensive experiments on standard benchmarks and complex datasets, the method shows robustness and scalability.", "conclusion": "The proposed approach resolves spatial ambiguities, provides identifiability guarantees, and does not require viewpoint annotations."}}
{"id": "2506.06719", "pdf": "https://arxiv.org/pdf/2506.06719", "abs": "https://arxiv.org/abs/2506.06719", "authors": ["Mufhumudzi Muthivhi", "Jiahao Huo", "Fredrik Gustafsson", "Terence L. van Zyl"], "title": "Improving Wildlife Out-of-Distribution Detection: Africas Big Five", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mitigating human-wildlife conflict seeks to resolve unwanted encounters\nbetween these parties. Computer Vision provides a solution to identifying\nindividuals that might escalate into conflict, such as members of the Big Five\nAfrican animals. However, environments often contain several varied species.\nThe current state-of-the-art animal classification models are trained under a\nclosed-world assumption. They almost always remain overconfident in their\npredictions even when presented with unknown classes. This study investigates\nout-of-distribution (OOD) detection of wildlife, specifically the Big Five. To\nthis end, we select a parametric Nearest Class Mean (NCM) and a non-parametric\ncontrastive learning approach as baselines to take advantage of pretrained and\nprojected features from popular classification encoders. Moreover, we compare\nour baselines to various common OOD methods in the literature. The results show\nfeature-based methods reflect stronger generalisation capability across varying\nclassification thresholds. Specifically, NCM with ImageNet pre-trained features\nachieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the\nbest OOD methods, respectively. The code can be found here\nhttps://github.com/pxpana/BIG5OOD", "AI": {"tldr": "The study explores out-of-distribution (OOD) detection of wildlife, particularly the Big Five African animals. It compares parametric Nearest Class Mean (NCM) and non-parametric contrastive learning approaches with pretrained features to common OOD methods. NCM with ImageNet pre-trained features shows significant improvement in AUPR-IN, AUPR-OUT and AUTC metrics.", "motivation": "Current animal classification models operate under a closed-world assumption and remain overconfident when faced with unknown classes, which is problematic for mitigating human-wildlife conflict involving varied species such as the Big Five African animals.", "method": "The research selects a parametric Nearest Class Mean (NCM) and a non-parametric contrastive learning approach as baselines, leveraging pretrained and projected features from popular classification encoders. These are then compared with various common OOD methods in literature.", "result": "Feature-based methods demonstrate stronger generalisation capability across varying classification thresholds. Notably, NCM with ImageNet pre-trained features achieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the best OOD methods respectively.", "conclusion": "Out-of-distribution detection using feature-based methods like NCM with ImageNet pre-trained features provides better generalization and could be beneficial in identifying potential human-wildlife conflicts."}}
{"id": "2506.07822", "pdf": "https://arxiv.org/pdf/2506.07822", "abs": "https://arxiv.org/abs/2506.07822", "authors": ["Xintong Duan", "Yutong He", "Fahim Tajwar", "Ruslan Salakhutdinov", "J. Zico Kolter", "Jeff Schneider"], "title": "Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although diffusion models have achieved strong results in decision-making\ntasks, their slow inference speed remains a key limitation. While the\nconsistency model offers a potential solution, its applications to\ndecision-making often struggle with suboptimal demonstrations or rely on\ncomplex concurrent training of multiple networks. In this work, we propose a\nnovel approach to consistency distillation for offline reinforcement learning\nthat directly incorporates reward optimization into the distillation process.\nOur method enables single-step generation while maintaining higher performance\nand simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and\nlong horizon planning demonstrate that our approach can achieve an 8.7%\nimprovement over previous state-of-the-art while offering up to 142x speedup\nover diffusion counterparts in inference time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e00\u81f4\u6027\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u5956\u52b1\u4f18\u5316\u76f4\u63a5\u878d\u5165\u84b8\u998f\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u5355\u6b65\u751f\u6210\u3001\u6027\u80fd\u66f4\u9ad8\u4e14\u8bad\u7ec3\u66f4\u7b80\u5355\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e868.7%\uff0c\u63a8\u7406\u901f\u5ea6\u52a0\u5feb\u4e86142\u500d\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u901f\u5ea6\u6162\u662f\u5176\u4e3b\u8981\u9650\u5236\u3002\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u56e0\u6b21\u4f18\u6f14\u793a\u5f71\u54cd\u6548\u679c\uff0c\u8981\u4e48\u9700\u8981\u590d\u6742\u7684\u540c\u65f6\u8bad\u7ec3\u591a\u4e2a\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e00\u81f4\u6027\u84b8\u998f\u65b9\u6cd5\uff0c\u5c06\u5956\u52b1\u4f18\u5316\u76f4\u63a5\u878d\u5165\u84b8\u998f\u8fc7\u7a0b\uff0c\u4ece\u800c\u7b80\u5316\u8bad\u7ec3\u5e76\u5b9e\u73b0\u5355\u6b65\u751f\u6210\u3002", "result": "\u5728Gym MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u548c\u957f\u671f\u89c4\u5212\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8f83\u4e4b\u524d\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u5347\u4e868.7%\u7684\u6027\u80fd\uff0c\u5e76\u5728\u63a8\u7406\u65f6\u95f4\u4e0a\u52a0\u901f\u4e86\u6700\u591a142\u500d\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u7ed3\u5408\u5956\u52b1\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u540c\u65f6\u51cf\u5c11\u4e86\u8bad\u7ec3\u590d\u6742\u5ea6\u3002"}}
{"id": "2506.07829", "pdf": "https://arxiv.org/pdf/2506.07829", "abs": "https://arxiv.org/abs/2506.07829", "authors": ["Jan Corazza", "Hadi Partovi Aria", "Hyohun Kim", "Daniel Neider", "Zhe Xu"], "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) algorithms can find an optimal policy for a\nsingle agent to accomplish a particular task. However, many real-world problems\nrequire multiple agents to collaborate in order to achieve a common goal. For\nexample, a robot executing a task in a warehouse may require the assistance of\na drone to retrieve items from high shelves. In Decentralized Multi-Agent RL\n(DMARL), agents learn independently and then combine their policies at\nexecution time, but often must satisfy constraints on compatibility of local\npolicies to ensure that they can achieve the global task when combined. In this\npaper, we study how providing high-level symbolic knowledge to agents can help\naddress unique challenges of this setting, such as privacy constraints,\ncommunication limitations, and performance concerns. In particular, we extend\nthe formal tools used to check the compatibility of local policies with the\nteam task, making decentralized training with theoretical guarantees usable in\nmore scenarios. Furthermore, we empirically demonstrate that symbolic knowledge\nabout the temporal evolution of events in the environment can significantly\nexpedite the learning process in DMARL.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u4f9b\u9ad8\u5c42\u6b21\u7684\u7b26\u53f7\u77e5\u8bc6\u7ed9\u667a\u80fd\u4f53\uff0c\u6269\u5c55\u4e86\u7528\u4e8e\u68c0\u67e5\u5c40\u90e8\u7b56\u7565\u4e0e\u56e2\u961f\u4efb\u52a1\u517c\u5bb9\u6027\u7684\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u5e76\u5728\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08DMARL\uff09\u4e2d\u5c55\u793a\u4e86\u8fd9\u79cd\u77e5\u8bc6\u80fd\u663e\u8457\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u8bb8\u591a\u73b0\u5b9e\u95ee\u9898\u9700\u8981\u591a\u4e2a\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u5171\u540c\u76ee\u6807\uff0c\u4f46\u5728\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08DMARL\uff09\u4e2d\uff0c\u667a\u80fd\u4f53\u72ec\u7acb\u5b66\u4e60\u540e\u9700\u7ed3\u5408\u5176\u7b56\u7565\u6267\u884c\u4efb\u52a1\uff0c\u540c\u65f6\u6ee1\u8db3\u5c40\u90e8\u7b56\u7565\u517c\u5bb9\u6027\u7ea6\u675f\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u9ad8\u5c42\u6b21\u7b26\u53f7\u77e5\u8bc6\u6765\u5e94\u5bf9\u9690\u79c1\u3001\u901a\u4fe1\u548c\u6027\u80fd\u7b49\u6311\u6218\u663e\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u5411\u667a\u80fd\u4f53\u63d0\u4f9b\u9ad8\u5c42\u6b21\u7684\u7b26\u53f7\u77e5\u8bc6\uff0c\u6269\u5c55\u4e86\u7528\u4e8e\u9a8c\u8bc1\u5c40\u90e8\u7b56\u7565\u4e0e\u56e2\u961f\u4efb\u52a1\u517c\u5bb9\u6027\u7684\u5f62\u5f0f\u5316\u5de5\u5177\uff0c\u4f7f\u5f97\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u5728\u66f4\u591a\u573a\u666f\u4e0b\u5177\u5907\u7406\u8bba\u4fdd\u8bc1\u3002\u6b64\u5916\uff0c\u5728\u5b9e\u9a8c\u4e2d\u5f15\u5165\u5173\u4e8e\u73af\u5883\u4e2d\u4e8b\u4ef6\u65f6\u95f4\u6f14\u5316\u7684\u7b26\u53f7\u77e5\u8bc6\uff0c\u4ee5\u52a0\u901fDMARL\u4e2d\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u7406\u8bba\u4e0a\u6269\u5c55\u4e86\u517c\u5bb9\u6027\u68c0\u67e5\u5de5\u5177\u7684\u9002\u7528\u8303\u56f4\uff1b\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u7b26\u53f7\u77e5\u8bc6\u80fd\u591f\u663e\u8457\u52a0\u5febDMARL\u7684\u5b66\u4e60\u901f\u5ea6\u3002", "conclusion": "\u9ad8\u5c42\u6b21\u7b26\u53f7\u77e5\u8bc6\u6709\u52a9\u4e8e\u89e3\u51b3DMARL\u4e2d\u7684\u9690\u79c1\u3001\u901a\u4fe1\u53ca\u6027\u80fd\u95ee\u9898\uff0c\u5e76\u4e14\u53ef\u4ee5\u52a0\u901f\u5b66\u4e60\u8fc7\u7a0b\uff0c\u540c\u65f6\u4f7f\u53bb\u4e2d\u5fc3\u5316\u8bad\u7ec3\u5177\u5907\u66f4\u5f3a\u7684\u7406\u8bba\u4fdd\u969c\u3002"}}
{"id": "2506.06732", "pdf": "https://arxiv.org/pdf/2506.06732", "abs": "https://arxiv.org/abs/2506.06732", "authors": ["Woongjib Choi", "Byeong Hyeon Kim", "Hyungseob Lim", "Inseon Jang", "Hong-Goo Kang"], "title": "Neural Spectral Band Generation for Audio Coding", "categories": ["eess.AS", "cs.AI", "eess.SP"], "comment": "Accepted to Interspeech 2025", "summary": "Audio bandwidth extension is the task of reconstructing missing high\nfrequency components of bandwidth-limited audio signals, where bandwidth\nlimitation is a common issue for audio signals due to several reasons,\nincluding channel capacity and data constraints. While conventional spectral\nband replication is a well-established parametric approach to audio bandwidth\nextension, the SBR usually entails coarse feature extraction and reconstruction\ntechniques, which leads to limitations when processing various types of audio\nsignals. In parallel, numerous deep neural network-based audio bandwidth\nextension methods have been proposed. These DNN-based methods are usually\nreferred to as blind BWE, as these methods do not rely on prior information\nextracted from original signals, and only utilize given low frequency band\nsignals to estimate missing high frequency components. In order to replace\nconventional SBR with DNNs, simply adopting existing DNN-based methodologies\nresults in suboptimal performance due to the blindness of these methods. My\nproposed research suggests a new approach to parametric non-blind bandwidth\nextension, as DNN-based side information extraction and DNN-based bandwidth\nextension are performed only at the front and end of the audio coding pipeline.", "AI": {"tldr": "An abstract discussing the limitations of traditional spectral band replication (SBR) and deep neural network (DNN)-based audio bandwidth extension methods for reconstructing high-frequency audio components. A new approach combining parametric non-blind bandwidth extension with DNN-based side information extraction is proposed.", "motivation": "To address the limitations of conventional SBR and existing DNN-based blind BWE methods, which result in suboptimal performance when processing various types of audio signals.", "method": "Propose a new approach to parametric non-blind bandwidth extension that incorporates DNN-based side information extraction and DNN-based bandwidth extension at the front and end of the audio coding pipeline respectively.", "result": "The proposed method aims to overcome the limitations of coarse feature extraction and reconstruction techniques used in SBR, as well as the blindness of DNN-based methods.", "conclusion": "This research suggests that integrating DNNs into a parametric non-blind framework could potentially lead to improved audio bandwidth extension performance."}}
{"id": "2506.07833", "pdf": "https://arxiv.org/pdf/2506.07833", "abs": "https://arxiv.org/abs/2506.07833", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "title": "Improving large language models with concept-aware fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm", "AI": {"tldr": "Large language models (LLMs) are limited by next-token prediction, hindering coherent concept formation. This paper introduces Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that enhances LLMs' conceptual understanding during the fine-tuning phase, showing significant improvements in various tasks.", "motivation": "The existing paradigm of next-token prediction in LLMs limits their ability to form coherent, high-level concepts, which is a barrier to human-like understanding and reasoning.", "method": "Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method that redefines how LLMs are fine-tuned by enabling the learning of sequences spanning multiple tokens, thus fostering stronger concept-aware learning.", "result": "Experiments demonstrate significant improvements compared to conventional next-token finetuning methods across diverse tasks, including text summarization and de novo protein design.", "conclusion": "CAFT brings multi-token prediction to the post-training phase, democratizing its benefits for practitioners and researchers, with wider implications for the machine learning research community."}}
{"id": "2506.07843", "pdf": "https://arxiv.org/pdf/2506.07843", "abs": "https://arxiv.org/abs/2506.07843", "authors": ["Davide Carbone"], "title": "Jarzynski Reweighting and Sampling Dynamics for Training Energy-Based Models: Theoretical Analysis of Different Transition Kernels", "categories": ["cs.LG", "physics.comp-ph"], "comment": null, "summary": "Energy-Based Models (EBMs) provide a flexible framework for generative\nmodeling, but their training remains theoretically challenging due to the need\nto approximate normalization constants and efficiently sample from complex,\nmulti-modal distributions. Traditional methods, such as contrastive divergence\nand score matching, introduce biases that can hinder accurate learning. In this\nwork, we present a theoretical analysis of Jarzynski reweighting, a technique\nfrom non-equilibrium statistical mechanics, and its implications for training\nEBMs. We focus on the role of the choice of the kernel and we illustrate these\ntheoretical considerations in two key generative frameworks: (i) flow-based\ndiffusion models, where we reinterpret Jarzynski reweighting in the context of\nstochastic interpolants to mitigate discretization errors and improve sample\nquality, and (ii) Restricted Boltzmann Machines, where we analyze its role in\ncorrecting the biases of contrastive divergence. Our results provide insights\ninto the interplay between kernel choice and model performance, highlighting\nthe potential of Jarzynski reweighting as a principled tool for generative\nlearning.", "AI": {"tldr": "This paper explores the use of Jarzynski reweighting in training Energy-Based Models (EBMs), focusing on kernel choice and its implications in flow-based diffusion models and Restricted Boltzmann Machines.", "motivation": "The motivation lies in addressing theoretical challenges in EBM training, particularly biases introduced by traditional methods like contrastive divergence and score matching. This motivates an investigation into Jarzynski reweighting as a potential solution.", "method": "Theoretical analysis of Jarzynski reweighting is conducted, examining its role in two generative frameworks: flow-based diffusion models and Restricted Boltzmann Machines. In flow-based diffusion models, it is used to mitigate discretization errors and improve sample quality. In Restricted Boltzmann Machines, it is analyzed for correcting biases of contrastive divergence.", "result": "The results provide insights into how the choice of kernel affects model performance, demonstrating the potential of Jarzynski reweighting in improving generative learning.", "conclusion": "Jarzynski reweighting shows promise as a principled tool for generative learning, offering a way to enhance EBM training through careful consideration of kernel choice."}}
{"id": "2506.06737", "pdf": "https://arxiv.org/pdf/2506.06737", "abs": "https://arxiv.org/abs/2506.06737", "authors": ["Qi Shi", "Qiwei Han", "Cl\u00e1udia Soares"], "title": "C-PATH: Conversational Patient Assistance and Triage in Healthcare System", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IEEE ICDH 2025, 10 pages, 8 figures, 5 tables", "summary": "Navigating healthcare systems can be complex and overwhelming, creating\nbarriers for patients seeking timely and appropriate medical attention. In this\npaper, we introduce C-PATH (Conversational Patient Assistance and Triage in\nHealthcare), a novel conversational AI system powered by large language models\n(LLMs) designed to assist patients in recognizing symptoms and recommending\nappropriate medical departments through natural, multi-turn dialogues. C-PATH\nis fine-tuned on medical knowledge, dialogue data, and clinical summaries using\na multi-stage pipeline built on the LLaMA3 architecture. A core contribution of\nthis work is a GPT-based data augmentation framework that transforms structured\nclinical knowledge from DDXPlus into lay-person-friendly conversations,\nallowing alignment with patient communication norms. We also implement a\nscalable conversation history management strategy to ensure long-range\ncoherence. Evaluation with GPTScore demonstrates strong performance across\ndimensions such as clarity, informativeness, and recommendation accuracy.\nQuantitative benchmarks show that C-PATH achieves superior performance in\nGPT-rewritten conversational datasets, significantly outperforming\ndomain-specific baselines. C-PATH represents a step forward in the development\nof user-centric, accessible, and accurate AI tools for digital health\nassistance and triage.", "AI": {"tldr": "C-PATH is a conversational AI system designed to assist patients in recognizing symptoms and recommending medical departments through natural, multi-turn dialogues. It is powered by large language models (LLMs) and fine-tuned on medical knowledge, dialogue data, and clinical summaries using a multi-stage pipeline built on the LLaMA3 architecture.", "motivation": "Navigating healthcare systems can be complex and overwhelming, creating barriers for patients seeking timely and appropriate medical attention.", "method": "C-PATH is powered by large language models (LLMs) and fine-tuned on medical knowledge, dialogue data, and clinical summaries using a multi-stage pipeline built on the LLaMA3 architecture. A GPT-based data augmentation framework transforms structured clinical knowledge into lay-person-friendly conversations. A scalable conversation history management strategy ensures long-range coherence.", "result": "Evaluation with GPTScore demonstrates strong performance across dimensions such as clarity, informativeness, and recommendation accuracy. Quantitative benchmarks show that C-PATH achieves superior performance in GPT-rewritten conversational datasets, significantly outperforming domain-specific baselines.", "conclusion": "C-PATH represents a step forward in the development of user-centric, accessible, and accurate AI tools for digital health assistance and triage."}}
{"id": "2506.07854", "pdf": "https://arxiv.org/pdf/2506.07854", "abs": "https://arxiv.org/abs/2506.07854", "authors": ["Zheng Zhang", "Jie Bao", "Zhixin Zhou", "Nicolo Colombo", "Lixin Cheng", "Rui Luo"], "title": "Residual Reweighted Conformal Prediction for Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Graph Neural Networks (GNNs) excel at modeling relational data but face\nsignificant challenges in high-stakes domains due to unquantified uncertainty.\nConformal prediction (CP) offers statistical coverage guarantees, but existing\nmethods often produce overly conservative prediction intervals that fail to\naccount for graph heteroscedasticity and structural biases. While residual\nreweighting CP variants address some of these limitations, they neglect graph\ntopology, cluster-specific uncertainties, and risk data leakage by reusing\ntraining sets. To address these issues, we propose Residual Reweighted GNN\n(RR-GNN), a framework designed to generate minimal prediction sets with\nprovable marginal coverage guarantees.\n  RR-GNN introduces three major innovations to enhance prediction performance.\nFirst, it employs Graph-Structured Mondrian CP to partition nodes or edges into\ncommunities based on topological features, ensuring cluster-conditional\ncoverage that reflects heterogeneity. Second, it uses Residual-Adaptive\nNonconformity Scores by training a secondary GNN on a held-out calibration set\nto estimate task-specific residuals, dynamically adjusting prediction intervals\naccording to node or edge uncertainty. Third, it adopts a Cross-Training\nProtocol, which alternates the optimization of the primary GNN and the residual\npredictor to prevent information leakage while maintaining graph dependencies.\nWe validate RR-GNN on 15 real-world graphs across diverse tasks, including node\nclassification, regression, and edge weight prediction. Compared to CP\nbaselines, RR-GNN achieves improved efficiency over state-of-the-art methods,\nwith no loss of coverage.", "AI": {"tldr": "RR-GNN\u662f\u4e00\u79cd\u521b\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u4e3b\u8981\u6539\u8fdb\uff08Graph-Structured Mondrian CP\u3001Residual-Adaptive Nonconformity Scores\u548cCross-Training Protocol\uff09\u751f\u6210\u5177\u6709\u53ef\u8bc1\u660e\u8fb9\u9645\u8986\u76d6\u7387\u4fdd\u8bc1\u7684\u6700\u5c0f\u9884\u6d4b\u96c6\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u4efb\u52a1\u3002", "motivation": "\u5f53\u524d\u7684GNN\u5728\u9ad8\u98ce\u9669\u9886\u57df\u9762\u4e34\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u800c\u73b0\u6709\u7684CP\u65b9\u6cd5\u8fc7\u4e8e\u4fdd\u5b88\u4e14\u672a\u80fd\u5145\u5206\u8003\u8651\u56fe\u7684\u5f02\u8d28\u6027\u548c\u7ed3\u6784\u504f\u5dee\uff0c\u540c\u65f6\u5b58\u5728\u6570\u636e\u6cc4\u9732\u7684\u98ce\u9669\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "RR-GNN\u63d0\u51fa\u4e86\u4e09\u79cd\u4e3b\u8981\u521b\u65b0\uff1a1) \u4f7f\u7528Graph-Structured Mondrian CP\u57fa\u4e8e\u62d3\u6251\u7279\u5f81\u5c06\u8282\u70b9\u6216\u8fb9\u5212\u5206\u4e3a\u793e\u533a\uff1b2) \u91c7\u7528Residual-Adaptive Nonconformity Scores\u8bad\u7ec3\u6b21\u7ea7GNN\u4f30\u8ba1\u4efb\u52a1\u7279\u5b9a\u6b8b\u5dee\uff1b3) \u4f7f\u7528Cross-Training Protocol\u4ea4\u66ff\u4f18\u5316\u4e3bGNN\u548c\u6b8b\u5dee\u9884\u6d4b\u5668\u3002", "result": "RR-GNN\u572815\u4e2a\u771f\u5b9e\u4e16\u754c\u56fe\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u6db5\u76d6\u591a\u79cd\u4efb\u52a1\uff08\u8282\u70b9\u5206\u7c7b\u3001\u56de\u5f52\u548c\u8fb9\u6743\u91cd\u9884\u6d4b\uff09\u3002\u4e0eCP\u57fa\u7ebf\u76f8\u6bd4\uff0cRR-GNN\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u4e14\u6ca1\u6709\u635f\u5931\u8986\u76d6\u7387\u3002", "conclusion": "RR-GNN\u901a\u8fc7\u5f15\u5165\u521b\u65b0\u6280\u672f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u751f\u6210\u4e86\u5177\u6709\u53ef\u8bc1\u660e\u8986\u76d6\u7387\u4fdd\u8bc1\u7684\u6700\u5c0f\u9884\u6d4b\u96c6\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u4e2d\u7684GNN\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06752", "pdf": "https://arxiv.org/pdf/2506.06752", "abs": "https://arxiv.org/abs/2506.06752", "authors": ["Anna B. Jakobsen", "Anders B. Clausen", "Jaco van de Pol", "Irfansha Shaik"], "title": "Depth-Optimal Quantum Layout Synthesis as SAT", "categories": ["quant-ph", "cs.AI"], "comment": "24 pages, 4 figures, 11 tables", "summary": "Quantum circuits consist of gates applied to qubits. Current quantum hardware\nplatforms impose connectivity restrictions on binary CX gates. Hence, Layout\nSynthesis is an important step to transpile quantum circuits before they can be\nexecuted. Since CX gates are noisy, it is important to reduce the CX count or\nCX depth of the mapped circuits.\n  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis\nin SAT. Previous SAT encodings focused on gate count and CX-gate count. Our\nencoding instead guarantees that we find mapped circuits with minimal circuit\ndepth or minimal CX-gate depth. We use incremental SAT solving and parallel\nplans for an efficient encoding. This results in speedups of more than 10-100x\ncompared to OLSQ2, which guarantees depth-optimality. But minimizing depth\nstill takes more time than minimizing gate count with Q-Synth.\n  We correlate the noise reduction achieved by simulating circuits after\n(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count\ncorrelates better with reducing noise than minimizing for CX-depth. However,\ntaking into account both CX-count and CX-depth provides the best noise\nreduction.", "AI": {"tldr": "The paper presents a new SAT encoding method for Quantum-circuit Layout Synthesis which guarantees minimal circuit depth or CX-gate depth, achieving significant speedups compared to OLSQ2. However, minimizing CX-depth takes more time than minimizing gate count with Q-Synth. Minimizing CX-count correlates better with reducing noise but considering both CX-count and CX-depth provides the best noise reduction.", "motivation": "Current quantum hardware platforms impose connectivity restrictions on binary CX gates, making Layout Synthesis an important step in transpiling quantum circuits before execution. Since CX gates are noisy, it's crucial to reduce the CX count or CX depth of mapped circuits.", "method": "The authors provide a new and efficient SAT encoding for Quantum-circuit Layout Synthesis. This encoding focuses on finding mapped circuits with minimal circuit depth or minimal CX-gate depth using incremental SAT solving and parallel plans.", "result": "This approach results in speedups of more than 10-100x compared to OLSQ2, which also guarantees depth-optimality. Simulations show that minimizing CX-count correlates better with reducing noise than minimizing CX-depth, but considering both provides the best noise reduction.", "conclusion": "The new SAT encoding method efficiently achieves minimal circuit depth or CX-gate depth, significantly speeding up the process compared to previous methods. While minimizing CX-count is better correlated with noise reduction, combining CX-count and CX-depth considerations yields optimal noise reduction."}}
{"id": "2506.07861", "pdf": "https://arxiv.org/pdf/2506.07861", "abs": "https://arxiv.org/abs/2506.07861", "authors": ["Firas Laakom", "Haobo Chen", "J\u00fcrgen Schmidhuber", "Yuheng Bu"], "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "38 pages", "summary": "Despite substantial progress in promoting fairness in high-stake applications\nusing machine learning models, existing methods often modify the training\nprocess, such as through regularizers or other interventions, but lack formal\nguarantees that fairness achieved during training will generalize to unseen\ndata. Although overfitting with respect to prediction performance has been\nextensively studied, overfitting in terms of fairness loss has received far\nless attention. This paper proposes a theoretical framework for analyzing\nfairness generalization error through an information-theoretic lens. Our novel\nbounding technique is based on Efron-Stein inequality, which allows us to\nderive tight information-theoretic fairness generalization bounds with both\nMutual Information (MI) and Conditional Mutual Information (CMI). Our empirical\nresults validate the tightness and practical relevance of these bounds across\ndiverse fairness-aware learning algorithms. Our framework offers valuable\ninsights to guide the design of algorithms improving fairness generalization.", "AI": {"tldr": "This paper proposes a theoretical framework for analyzing fairness generalization error through an information-theoretic lens, using Efron-Stein inequality to derive tight bounds with both Mutual Information (MI) and Conditional Mutual Information (CMI). Empirical results validate the practical relevance of these bounds.", "motivation": "Existing methods for promoting fairness in machine learning models often modify the training process but lack formal guarantees that fairness achieved during training will generalize to unseen data. Overfitting in terms of fairness loss has received far less attention compared to prediction performance overfitting.", "method": "The paper develops a novel bounding technique based on Efron-Stein inequality to analyze fairness generalization error. This approach derives tight information-theoretic fairness generalization bounds using both Mutual Information (MI) and Conditional Mutual Information (CMI).", "result": "Empirical results demonstrate the tightness and practical relevance of the derived bounds across diverse fairness-aware learning algorithms.", "conclusion": "The proposed framework provides valuable insights for guiding the design of algorithms aimed at improving fairness generalization."}}
{"id": "2506.07864", "pdf": "https://arxiv.org/pdf/2506.07864", "abs": "https://arxiv.org/abs/2506.07864", "authors": ["Mirko Paolo Barbato", "Giorgia Rigamonti", "Davide Marelli", "Paolo Napoletano"], "title": "Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous\nmonitoring to prevent severe hypo- and hyperglycemic events. While continuous\nglucose monitoring has improved blood glucose management, deploying predictive\nmodels on wearable devices remains challenging due to computational and memory\nconstraints. To address this, we propose a novel Lightweight Sequential\nTransformer model designed for blood glucose prediction in T1D. By integrating\nthe strengths of Transformers' attention mechanisms and the sequential\nprocessing of recurrent neural networks, our architecture captures long-term\ndependencies while maintaining computational efficiency. The model is optimized\nfor deployment on resource-constrained edge devices and incorporates a balanced\nloss function to handle the inherent data imbalance in hypo- and hyperglycemic\nevents. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,\ndemonstrate that the proposed model outperforms state-of-the-art methods in\npredicting glucose levels and detecting adverse events. This work fills the gap\nbetween high-performance modeling and practical deployment, providing a\nreliable and efficient T1D management solution.", "AI": {"tldr": "A novel Lightweight Sequential Transformer model for blood glucose prediction in Type 1 Diabetes is proposed, which integrates the strengths of Transformers and recurrent neural networks while being optimized for deployment on resource-constrained edge devices. Experiments show that it outperforms state-of-the-art methods.", "motivation": "Type 1 Diabetes affects millions worldwide and requires continuous monitoring to prevent severe hypo- and hyperglycemic events. However, deploying predictive models on wearable devices is challenging due to computational and memory constraints.", "method": "The authors propose a Lightweight Sequential Transformer model designed for blood glucose prediction in T1D. It combines the attention mechanisms of Transformers with the sequential processing of recurrent neural networks to capture long-term dependencies while maintaining computational efficiency. The model also incorporates a balanced loss function to handle data imbalance in hypo- and hyperglycemic events.", "result": "Experiments on two benchmark datasets (OhioT1DM and DiaTrend) demonstrate that the proposed model outperforms state-of-the-art methods in predicting glucose levels and detecting adverse events.", "conclusion": "This work bridges the gap between high-performance modeling and practical deployment by providing a reliable and efficient solution for Type 1 Diabetes management."}}
{"id": "2506.07871", "pdf": "https://arxiv.org/pdf/2506.07871", "abs": "https://arxiv.org/abs/2506.07871", "authors": ["Sigma Jahan", "Mohammad Masudur Rahman"], "title": "Can Hessian-Based Insights Support Fault Diagnosis in Attention-based Models?", "categories": ["cs.LG", "cs.SE"], "comment": null, "summary": "As attention-based deep learning models scale in size and complexity,\ndiagnosing their faults becomes increasingly challenging. In this work, we\nconduct an empirical study to evaluate the potential of Hessian-based analysis\nfor diagnosing faults in attention-based models. Specifically, we use\nHessian-derived insights to identify fragile regions (via curvature analysis)\nand parameter interdependencies (via parameter interaction analysis) within\nattention mechanisms. Through experiments on three diverse models (HAN, 3D-CNN,\nDistilBERT), we show that Hessian-based metrics can localize instability and\npinpoint fault sources more effectively than gradients alone. Our empirical\nfindings suggest that these metrics could significantly improve fault diagnosis\nin complex neural architectures, potentially improving software debugging\npractices.", "AI": {"tldr": "\u968f\u7740\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u89c4\u6a21\u548c\u590d\u6742\u6027\u4e0a\u7684\u589e\u957f\uff0c\u8bca\u65ad\u5176\u6545\u969c\u53d8\u5f97\u8d8a\u6765\u8d8a\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u8bc4\u4f30\u4e86Hessian\u5206\u6790\u5728\u8bca\u65ad\u6ce8\u610f\u529b\u6a21\u578b\u6545\u969c\u4e2d\u7684\u6f5c\u529b\u3002\u7814\u7a76\u8868\u660e\uff0c\u4e0e\u4ec5\u4f7f\u7528\u68af\u5ea6\u76f8\u6bd4\uff0cHessian-based\u6307\u6807\u80fd\u591f\u66f4\u6709\u6548\u5730\u5b9a\u4f4d\u4e0d\u7a33\u5b9a\u6027\u5e76\u786e\u5b9a\u6545\u969c\u6e90\uff0c\u4ece\u800c\u53ef\u80fd\u663e\u8457\u6539\u5584\u590d\u6742\u795e\u7ecf\u67b6\u6784\u4e2d\u7684\u6545\u969c\u8bca\u65ad\uff0c\u751a\u81f3\u6539\u8fdb\u8f6f\u4ef6\u8c03\u8bd5\u5b9e\u8df5\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u89c4\u6a21\u548c\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u5bf9\u5176\u6545\u969c\u8fdb\u884c\u8bca\u65ad\u53d8\u5f97\u6108\u53d1\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u65b0\u7684\u65b9\u6cd5\u6765\u8fdb\u884c\u6709\u6548\u7684\u6545\u969c\u8bca\u65ad\u3002", "method": "\u901a\u8fc7Hessian-based\u5206\u6790\u6765\u8bca\u65ad\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6a21\u578b\u6545\u969c\uff0c\u5177\u4f53\u5305\u62ec\u5229\u7528Hessian\u5bfc\u51fa\u7684\u89c1\u89e3\u8bc6\u522b\u8106\u5f31\u533a\u57df\uff08\u901a\u8fc7\u66f2\u7387\u5206\u6790\uff09\u548c\u53c2\u6570\u76f8\u4e92\u4f9d\u8d56\u6027\uff08\u901a\u8fc7\u53c2\u6570\u4ea4\u4e92\u5206\u6790\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u6a21\u578b\uff08HAN\u30013D-CNN\u3001DistilBERT\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eHessian-based\u6307\u6807\u53ef\u4ee5\u6bd4\u5355\u72ec\u4f7f\u7528\u68af\u5ea6\u66f4\u6709\u6548\u5730\u5b9a\u4f4d\u4e0d\u7a33\u5b9a\u6027\u548c\u786e\u5b9a\u6545\u969c\u6e90\u3002", "conclusion": "Hessian-based\u5ea6\u91cf\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u590d\u6742\u795e\u7ecf\u67b6\u6784\u4e2d\u7684\u6545\u969c\u8bca\u65ad\u80fd\u529b\uff0c\u5e76\u53ef\u80fd\u6539\u5584\u8f6f\u4ef6\u8c03\u8bd5\u5b9e\u8df5\u3002"}}
{"id": "2506.06806", "pdf": "https://arxiv.org/pdf/2506.06806", "abs": "https://arxiv.org/abs/2506.06806", "authors": ["Subhendu Khatuya", "Shashwat Naidu", "Saptarshi Ghosh", "Pawan Goyal", "Niloy Ganguly"], "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work has been accepted to appear at the Association for\n  Computational Linguistics (ACL), 2025", "summary": "The explosion of textual data has made manual document classification\nincreasingly challenging. To address this, we introduce a robust, efficient\ndomain-agnostic generative model framework for multi-label text classification.\nInstead of treating labels as mere atomic symbols, our approach utilizes\npredefined label descriptions and is trained to generate these descriptions\nbased on the input text. During inference, the generated descriptions are\nmatched to the pre-defined labels using a finetuned sentence transformer. We\nintegrate this with a dual-objective loss function, combining cross-entropy\nloss and cosine similarity of the generated sentences with the predefined\ntarget descriptions, ensuring both semantic alignment and accuracy. Our\nproposed model LAGAMC stands out for its parameter efficiency and versatility\nacross diverse datasets, making it well-suited for practical applications. We\ndemonstrate the effectiveness of our proposed model by achieving new\nstate-of-the-art performances across all evaluated datasets, surpassing several\nstrong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in\nMacro-F1 compared to the closest baseline across all datasets.", "AI": {"tldr": "LAGAMC\u662f\u4e00\u79cd\u65b0\u7684\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u6807\u7b7e\u63cf\u8ff0\u5e76\u7ed3\u5408\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5206\u522b\u5728Micro-F1\u548cMacro-F1\u6307\u6807\u4e0a\u63d0\u9ad8\u4e8613.94%\u548c24.85%\u3002", "motivation": "\u968f\u7740\u6587\u672c\u6570\u636e\u7684\u6fc0\u589e\uff0c\u624b\u52a8\u6587\u6863\u5206\u7c7b\u53d8\u5f97\u8d8a\u6765\u8d8a\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u7528\u7684\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5229\u7528\u9884\u5b9a\u4e49\u7684\u6807\u7b7e\u63cf\u8ff0\u751f\u6210\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u7684\u53e5\u5b50\u53d8\u6362\u5668\u5c06\u751f\u6210\u7684\u63cf\u8ff0\u4e0e\u9884\u5b9a\u4e49\u6807\u7b7e\u5339\u914d\u3002\u540c\u65f6\uff0c\u91c7\u7528\u53cc\u76ee\u6807\u635f\u5931\u51fd\u6570\uff0c\u5305\u62ec\u4ea4\u53c9\u71b5\u635f\u5931\u548c\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u4ee5\u786e\u4fdd\u8bed\u4e49\u5bf9\u9f50\u548c\u51c6\u786e\u6027\u3002", "result": "LAGAMC\u6a21\u578b\u5728\u6240\u6709\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u591a\u4e2a\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LAGAMC\u6a21\u578b\u56e0\u5176\u53c2\u6570\u6548\u7387\u548c\u8de8\u6570\u636e\u96c6\u7684\u901a\u7528\u6027\uff0c\u975e\u5e38\u9002\u5408\u5b9e\u9645\u5e94\u7528\uff0c\u4e14\u5728\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2506.07883", "pdf": "https://arxiv.org/pdf/2506.07883", "abs": "https://arxiv.org/abs/2506.07883", "authors": ["Rajat Rasal", "Avinash Kori", "Fabio De Sousa Ribeiro", "Tian Xia", "Ben Glocker"], "title": "Diffusion Counterfactual Generation with Semantic Abduction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada", "summary": "Counterfactual image generation presents significant challenges, including\npreserving identity, maintaining perceptual quality, and ensuring faithfulness\nto an underlying causal model. While existing auto-encoding frameworks admit\nsemantic latent spaces which can be manipulated for causal control, they\nstruggle with scalability and fidelity. Advancements in diffusion models\npresent opportunities for improving counterfactual image editing, having\ndemonstrated state-of-the-art visual quality, human-aligned perception and\nrepresentation learning capabilities. Here, we present a suite of\ndiffusion-based causal mechanisms, introducing the notions of spatial, semantic\nand dynamic abduction. We propose a general framework that integrates semantic\nrepresentations into diffusion models through the lens of Pearlian causality to\nedit images via a counterfactual reasoning process. To our knowledge, this is\nthe first work to consider high-level semantic identity preservation for\ndiffusion counterfactuals and to demonstrate how semantic control enables\nprincipled trade-offs between faithful causal control and identity\npreservation.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u5957\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u56e0\u679c\u673a\u5236\uff0c\u9996\u6b21\u5c06\u9ad8\u5c42\u6b21\u8bed\u4e49\u8eab\u4efd\u4fdd\u7559\u5f15\u5165\u5230\u6269\u6563\u53cd\u4e8b\u5b9e\u4e2d\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u4e49\u63a7\u5236\u5982\u4f55\u5728\u5fe0\u5b9e\u56e0\u679c\u63a7\u5236\u548c\u8eab\u4efd\u4fdd\u7559\u4e4b\u95f4\u5b9e\u73b0\u6709\u539f\u5219\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7f16\u7801\u6846\u67b6\u867d\u7136\u5141\u8bb8\u7528\u4e8e\u56e0\u679c\u63a7\u5236\u7684\u8bed\u4e49\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u4f46\u5728\u53ef\u6269\u5c55\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u53cd\u4e8b\u5b9e\u56fe\u50cf\u7f16\u8f91\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u7a7a\u95f4\u3001\u8bed\u4e49\u548c\u52a8\u6001\u8bf1\u56e0\u7684\u6982\u5ff5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u8bed\u4e49\u8868\u793a\u6574\u5408\u5230\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5229\u7528Pearlian\u56e0\u679c\u5173\u7cfb\u8fdb\u884c\u56fe\u50cf\u7f16\u8f91\u3002", "result": "\u6b64\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u5c42\u6b21\u8bed\u4e49\u8eab\u4efd\u7684\u4fdd\u7559\uff0c\u5e76\u5c55\u793a\u4e86\u8bed\u4e49\u63a7\u5236\u5982\u4f55\u5728\u5fe0\u5b9e\u56e0\u679c\u63a7\u5236\u548c\u8eab\u4efd\u4fdd\u7559\u4e4b\u95f4\u5b9e\u73b0\u6709\u539f\u5219\u7684\u6743\u8861\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u8003\u8651\u6269\u6563\u53cd\u4e8b\u5b9e\u4e2d\u9ad8\u5c42\u6b21\u8bed\u4e49\u8eab\u4efd\u4fdd\u7559\u7684\u5de5\u4f5c\uff0c\u5e76\u6210\u529f\u5c55\u793a\u4e86\u8bed\u4e49\u63a7\u5236\u5728\u53cd\u4e8b\u5b9e\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.06808", "pdf": "https://arxiv.org/pdf/2506.06808", "abs": "https://arxiv.org/abs/2506.06808", "authors": ["James A. Michaelov", "Reeka Estacio", "Zhien Zhang", "Benjamin K. Bergen"], "title": "Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Can language models reliably predict that possible events are more likely\nthan merely improbable ones? By teasing apart possibility, typicality, and\ncontextual relatedness, we show that despite the results of previous work,\nlanguage models' ability to do this is far from robust. In fact, under certain\nconditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -\nperform at worse-than-chance level, assigning higher probabilities to\nimpossible sentences such as 'the car was given a parking ticket by the brake'\nthan to merely unlikely sentences such as 'the car was given a parking ticket\nby the explorer'.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u9884\u6d4b\u53ef\u80fd\u4e8b\u4ef6\u7684\u80fd\u529b\u4e0d\u5982\u968f\u673a\u6c34\u5e73\uff0c\u5305\u62ecLlama 3\u3001Gemma 2\u548cMistral NeMo\u5728\u5185\u7684\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u90fd\u5b58\u5728\u6b64\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u53ef\u9760\u5730\u533a\u5206\u53ef\u80fd\u4e8b\u4ef6\u4e0e\u4e0d\u53ef\u80fd\u4e8b\u4ef6\uff0c\u5e76\u63a2\u8ba8\u5176\u5728\u53ef\u80fd\u6027\u3001\u5178\u578b\u6027\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5bf9\u53ef\u80fd\u4e8b\u4ef6\u3001\u5178\u578b\u4e8b\u4ef6\u53ca\u4e0a\u4e0b\u6587\u76f8\u5173\u4e8b\u4ef6\u7684\u5904\u7406\u80fd\u529b\uff0c\u5bf9\u6bd4\u4e0d\u540c\u6a21\u578b\uff08\u5982Llama 3\u3001Gemma 2\u548cMistral NeMo\uff09\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8868\u73b0\u4f4e\u4e8e\u968f\u673a\u6c34\u5e73\uff0c\u4f8b\u5982\u5bf9\u4e0d\u53ef\u80fd\u53e5\u5b50\u8d4b\u4e88\u66f4\u9ad8\u7684\u6982\u7387\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5728\u533a\u5206\u53ef\u80fd\u4e8b\u4ef6\u4e0e\u4e0d\u53ef\u80fd\u4e8b\u4ef6\u65b9\u9762\u7684\u80fd\u529b\u5e76\u4e0d\u7a33\u5065\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2506.07884", "pdf": "https://arxiv.org/pdf/2506.07884", "abs": "https://arxiv.org/abs/2506.07884", "authors": ["Anand Ganesh", "Babhrubahan Bose", "Anand Rajagopalan"], "title": "Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions", "categories": ["cs.LG", "math.FA", "46B15", "I.2.6"], "comment": "9 pages", "summary": "We construct four Schauder bases for the space $C[0,1]$, one using ReLU\nfunctions, another using Softplus functions, and two more using sigmoidal\nversions of the ReLU and Softplus functions. This establishes the existence of\na basis using these functions for the first time, and improves on the universal\napproximation property associated with them.", "AI": {"tldr": "This paper constructs four Schauder bases for the space C[0,1], which improves on the universal approximation property associated with ReLU, Softplus and their sigmoidal versions.", "motivation": "To establish the existence of a basis using ReLU, Softplus and their sigmoidal versions for the first time and improve on the universal approximation property associated with them.", "method": "Constructing four Schauder bases for the space C[0,1], one using ReLU functions, another using Softplus functions, and two more using sigmoidal versions of the ReLU and Softplus functions.", "result": "Successful construction of four Schauder bases for the space C[0,1].", "conclusion": "The construction of four Schauder bases for the space C[0,1] using ReLU, Softplus and their sigmoidal versions has been established. This improves on the universal approximation property associated with these functions."}}
{"id": "2506.07902", "pdf": "https://arxiv.org/pdf/2506.07902", "abs": "https://arxiv.org/abs/2506.07902", "authors": ["Sifan Wang", "Zehao Dou", "Tong-Rui Liu", "Lu Lu"], "title": "FunDiff: Diffusion Models over Function Spaces for Physics-Informed Generative Modeling", "categories": ["cs.LG", "physics.comp-ph", "stat.ML"], "comment": "31 pages, 12 figures", "summary": "Recent advances in generative modeling -- particularly diffusion models and\nflow matching -- have achieved remarkable success in synthesizing discrete data\nsuch as images and videos. However, adapting these models to physical\napplications remains challenging, as the quantities of interest are continuous\nfunctions governed by complex physical laws. Here, we introduce\n$\\textbf{FunDiff}$, a novel framework for generative modeling in function\nspaces. FunDiff combines a latent diffusion process with a function autoencoder\narchitecture to handle input functions with varying discretizations, generate\ncontinuous functions evaluable at arbitrary locations, and seamlessly\nincorporate physical priors. These priors are enforced through architectural\nconstraints or physics-informed loss functions, ensuring that generated samples\nsatisfy fundamental physical laws. We theoretically establish minimax\noptimality guarantees for density estimation in function spaces, showing that\ndiffusion-based estimators achieve optimal convergence rates under suitable\nregularity conditions. We demonstrate the practical effectiveness of FunDiff\nacross diverse applications in fluid dynamics and solid mechanics. Empirical\nresults show that our method generates physically consistent samples with high\nfidelity to the target distribution and exhibits robustness to noisy and\nlow-resolution data. Code and datasets are publicly available at\nhttps://github.com/sifanexisted/fundiff.", "AI": {"tldr": "Recent advances in generative modeling have succeeded in synthesizing discrete data, but adapting them to physical applications is still challenging. This paper introduces FunDiff, a novel framework for generative modeling in function spaces that combines a latent diffusion process with a function autoencoder architecture. The authors theoretically establish minimax optimality guarantees and demonstrate the practical effectiveness of FunDiff across diverse applications.", "motivation": "Adapting generative models to physical applications remains challenging due to the complexity of continuous functions governed by physical laws.", "method": "FunDiff combines a latent diffusion process with a function autoencoder architecture. It handles input functions with varying discretizations, generates continuous functions evaluable at arbitrary locations, and incorporates physical priors through architectural constraints or physics-informed loss functions.", "result": "Empirical results show that FunDiff generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy and low-resolution data.", "conclusion": "FunDiff is a promising framework for generative modeling in function spaces with theoretical guarantees and practical effectiveness in physical applications."}}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821", "abs": "https://arxiv.org/abs/2506.06821", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u901a\u8fc7\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u6765\u8fdb\u884c\u4ee3\u7801\u68c0\u67e5\u6216\u8c03\u8bd5\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u672c\u6587\u63d0\u51fa\u4e86TCGBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u751f\u6210\u6709\u6548\u548c\u9488\u5bf9\u6027\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\u65b9\u9762\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u867d\u7136LLMs\u80fd\u591f\u751f\u6210\u6709\u6548\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\uff0c\u4f46\u5728\u751f\u6210\u63ed\u793a\u4eba\u7c7b\u4ee3\u7801\u7f3a\u9677\u7684\u9488\u5bf9\u6027\u6d4b\u8bd5\u7528\u4f8b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002\u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u624b\u52a8\u6574\u7406\u6570\u636e\u96c6\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u548c\u5fae\u8c03\u6765\u63d0\u9ad8LLMs\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u4ee3\u7801\u68c0\u67e5\u6216\u8c03\u8bd5\u4e2d\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u65b9\u9762\u3002\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u7814\u7a76LLMs\u5728\u7ade\u4e89\u7ea7\u7f16\u7a0b\uff08CP\uff09\u4efb\u52a1\u4e2d\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faTCGBench\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u4efb\u52a1\uff1a1) \u4e3a\u7ed9\u5b9a\u7684CP\u95ee\u9898\u751f\u6210\u6709\u6548\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\uff1b2) \u751f\u6210\u80fd\u66b4\u9732\u4eba\u7c7b\u4ee3\u7801\u9519\u8bef\u7684\u9488\u5bf9\u6027\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\u3002\u540c\u65f6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u624b\u52a8\u6574\u7406\u6570\u636e\u96c6\uff0c\u4ee5\u901a\u8fc7\u63d0\u793a\u548c\u5fae\u8c03\u63d0\u9ad8LLMs\u7684\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLMs\u901a\u5e38\u53ef\u4ee5\u751f\u6210\u6709\u6548\u7684\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\uff0c\u4f46\u5728\u751f\u6210\u63ed\u793a\u4eba\u7c7b\u4ee3\u7801\u7f3a\u9677\u7684\u9488\u5bf9\u6027\u6d4b\u8bd5\u7528\u4f8b\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u5373\u4f7f\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\uff08\u5982o3-mini\uff09\u4e5f\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u7684\u8868\u73b0\u3002\u4f7f\u7528\u624b\u52a8\u6574\u7406\u7684\u6570\u636e\u96c6\u540e\uff0cLLMs\u7684\u6027\u80fd\u6709\u6240\u63d0\u5347\u3002", "conclusion": "TCGBench\u57fa\u51c6\u5c55\u793a\u4e86LLMs\u5728\u751f\u6210\u6d4b\u8bd5\u7528\u4f8b\u751f\u6210\u5668\u65b9\u9762\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002\u901a\u8fc7\u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\u8fdb\u884c\u63d0\u793a\u548c\u5fae\u8c03\uff0c\u53ef\u4ee5\u63d0\u9ad8LLMs\u5728\u6b64\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u7136\u800c\uff0cLLMs\u5728\u751f\u6210\u9488\u5bf9\u6027\u6d4b\u8bd5\u7528\u4f8b\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2506.07903", "pdf": "https://arxiv.org/pdf/2506.07903", "abs": "https://arxiv.org/abs/2506.07903", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted to ICML 2025. Code available at\n  https://github.com/KevinRojas1499/Diffuse-Everything", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "AI": {"tldr": "Diffusion models excel in unimodal data generation but lack in multimodal joint generation. This paper proposes a new framework for multimodal diffusion models that allows native generation across different modalities by using a decoupled noise schedule for each modality, showing competitive performance in text-image generation and mixed-type tabular data synthesis.", "motivation": "To overcome the limitations of existing approaches that rely heavily on external preprocessing protocols to harmonize varied data representations into a unified, unimodal format which can be problematic for applications with limited data.", "method": "Propose a novel framework for building multimodal diffusion models on arbitrary state spaces with an innovative decoupled noise schedule for each modality, enabling unconditional and modality-conditioned generation within a single model simultaneously.", "result": "Empirically validated for text-image generation and mixed-type tabular data synthesis, achieving competitive performance.", "conclusion": "The proposed framework for multimodal diffusion models with decoupled noise schedule shows promise in generating coupled data across different modalities."}}
{"id": "2506.06822", "pdf": "https://arxiv.org/pdf/2506.06822", "abs": "https://arxiv.org/abs/2506.06822", "authors": ["Chenlu Zhan", "Yufei Zhang", "Gaoang Wang", "Hongwei Wang"], "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modeling 3D language fields with Gaussian Splatting for open-ended language\nqueries has recently garnered increasing attention. However, recent 3DGS-based\nmodels leverage view-dependent 2D foundation models to refine 3D semantics but\nlack a unified 3D representation, leading to view inconsistencies.\nAdditionally, inherent open-vocabulary challenges cause inconsistencies in\nobject and relational descriptions, impeding hierarchical semantic\nunderstanding. In this paper, we propose Hi-LSplat, a view-consistent\nHierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.\nTo achieve view-consistent 3D hierarchical semantics, we first lift 2D features\nto 3D features by constructing a 3D hierarchical semantic tree with layered\ninstance clustering, which addresses the view inconsistency issue caused by 2D\nsemantic features. Besides, we introduce instance-wise and part-wise\ncontrastive losses to capture all-sided hierarchical semantic representations.\nNotably, we construct two hierarchical semantic datasets to better assess the\nmodel's ability to distinguish different semantic levels. Extensive experiments\nhighlight our method's superiority in 3D open-vocabulary segmentation and\nlocalization. Its strong performance on hierarchical semantic datasets\nunderscores its ability to capture complex hierarchical semantics within 3D\nscenes.", "AI": {"tldr": "This paper proposes Hi-LSplat, a view-consistent Hierarchical Language Gaussian Splatting work for 3D open-vocabulary querying. It lifts 2D features to 3D features by constructing a 3D hierarchical semantic tree and introduces contrastive losses for capturing all-sided hierarchical semantic representations.", "motivation": "Recent models using Gaussian Splatting for open-ended language queries in 3D lack a unified 3D representation, leading to view inconsistencies and impeding hierarchical semantic understanding.", "method": "The method constructs a 3D hierarchical semantic tree with layered instance clustering to lift 2D features to 3D features and introduce instance-wise and part-wise contrastive losses to capture hierarchical semantic representations.", "result": "Extensive experiments show the method's superiority in 3D open-vocabulary segmentation and localization, and its strong performance on hierarchical semantic datasets highlights its ability to capture complex hierarchical semantics within 3D scenes.", "conclusion": "Hi-LSplat addresses the view inconsistency issue caused by 2D semantic features and demonstrates superior performance in capturing complex hierarchical semantics within 3D scenes."}}
{"id": "2506.07918", "pdf": "https://arxiv.org/pdf/2506.07918", "abs": "https://arxiv.org/abs/2506.07918", "authors": ["Vahid Balazadeh", "Hamidreza Kamkari", "Valentin Thomas", "Benson Li", "Junwei Ma", "Jesse C. Cresswell", "Rahul G. Krishnan"], "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Causal effect estimation from observational data is fundamental across\nvarious applications. However, selecting an appropriate estimator from dozens\nof specialized methods demands substantial manual effort and domain expertise.\nWe present CausalPFN, a single transformer that amortizes this workflow:\ntrained once on a large library of simulated data-generating processes that\nsatisfy ignorability, it infers causal effects for new observational datasets\nout-of-the-box. CausalPFN combines ideas from Bayesian causal inference with\nthe large-scale training protocol of prior-fitted networks (PFNs), learning to\nmap raw observations directly to causal effects without any task-specific\nadjustment. Our approach achieves superior average performance on heterogeneous\nand average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC).\nMoreover, it shows competitive performance for real-world policy making on\nuplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to\nsupport reliable decision-making based on Bayesian principles. This\nready-to-use model does not require any further training or tuning and takes a\nstep toward automated causal inference (https://github.com/vdblm/CausalPFN).", "AI": {"tldr": "CausalPFN is a transformer that simplifies causal effect estimation from observational data by automating the selection of an appropriate estimator, providing reliable uncertainty estimates and not needing further training or tuning.", "motivation": "The manual effort and domain expertise required to select an appropriate estimator for causal effect estimation from observational data motivates the development of CausalPFN.", "method": "CausalPFN combines Bayesian causal inference with the large-scale training protocol of prior-fitted networks (PFNs) to map raw observations directly to causal effects. It is trained once on a large library of simulated data-generating processes that satisfy ignorability and can infer causal effects for new observational datasets out-of-the-box.", "result": "CausalPFN achieves superior average performance on heterogeneous and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC) and shows competitive performance for real-world policy making on uplift modeling tasks.", "conclusion": "CausalPFN provides calibrated uncertainty estimates supporting reliable decision-making based on Bayesian principles and represents a step toward automated causal inference."}}
{"id": "2506.06823", "pdf": "https://arxiv.org/pdf/2506.06823", "abs": "https://arxiv.org/abs/2506.06823", "authors": ["Qi Li", "Liangzhi Li", "Zhouqiang Jiang", "Bowen Wang", "Keke Tang"], "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.10992", "summary": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy.", "AI": {"tldr": "Visual Prompting (VP) shows potential in vision tasks but its performance under robust source models is unexplored. This paper investigates if VP can inherit robustness, encounters a robustness-generalization trade-off, and proposes Prompt Boundary Loosening (PBL) to mitigate this.", "motivation": "To determine if VP can inherit robustness from source models, understand the trade-off between robustness and generalization in VP, and find a strategy to address any limitations.", "method": "Thorough exploration of VP's ability to inherit robustness, encounter trade-offs, and the introduction of PBL as a strategy to mitigate the trade-off.", "result": "PBL effectively ensures successful inheritance of robustness and significantly enhances VP's generalization ability across various datasets.", "conclusion": "The findings are universal and the proposed PBL strategy demonstrates significant benefits."}}
{"id": "2506.07919", "pdf": "https://arxiv.org/pdf/2506.07919", "abs": "https://arxiv.org/abs/2506.07919", "authors": ["Manuel Brenner", "Georgia Koppe"], "title": "Uncovering the Functional Roles of Nonlinearity in Memory", "categories": ["cs.LG", "cs.AI", "cs.CL", "nlin.CD", "physics.comp-ph"], "comment": "Preprint under review", "summary": "Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems.", "AI": {"tldr": "\u5728\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e2d\uff0c\u975e\u7ebf\u6027\u5e76\u975e\u603b\u662f\u5fc5\u8981\u3002\u7814\u7a76\u53d1\u73b0\u6700\u5c0f\u975e\u7ebf\u6027\u901a\u5e38\u662f\u6700\u4f18\u7684\uff0c\u53ef\u751f\u6210\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5065\u548c\u66f4\u6613\u89e3\u91ca\u7684\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u975e\u7ebf\u6027\u9012\u5f52\u88ab\u8ba4\u4e3a\u5bf9\u4e8e\u8bb0\u5fc6\u548c\u957f\u8ddd\u79bb\u65f6\u95f4\u5904\u7406\u673a\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u7ebf\u6027\u52a8\u529b\u5b66\u53ef\u80fd\u5df2\u7ecf\u8db3\u591f\u3002\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u5730\u5256\u6790\u975e\u7ebf\u6027\u5728\u9012\u5f52\u7f51\u7edc\u4e2d\u7684\u529f\u80fd\u89d2\u8272\uff0c\u660e\u786e\u5176\u8ba1\u7b97\u5fc5\u8981\u6027\u548c\u6240\u652f\u6301\u7684\u673a\u5236\u3002", "method": "\u4f7f\u7528\u51e0\u4e4e\u7ebf\u6027\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff08AL-RNNs\uff09\u4f5c\u4e3a\u7075\u6d3b\u7684\u5efa\u6a21\u5de5\u5177\u548c\u63a2\u9488\uff0c\u5bf9\u591a\u79cd\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u548c\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u523a\u6fc0\u9009\u62e9\u4efb\u52a1\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u63a7\u5236\u975e\u7ebf\u6027\u7684\u7ec6\u7c92\u5ea6\u3002", "result": "\u53d1\u73b0\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\uff0c\u6700\u5c0f\u975e\u7ebf\u6027\u4e0d\u4ec5\u8db3\u591f\u800c\u4e14\u901a\u5e38\u662f\u6700\u4f73\u7684\uff0c\u53ef\u4ee5\u4ea7\u751f\u6bd4\u5b8c\u5168\u975e\u7ebf\u6027\u6216\u7ebf\u6027\u6a21\u578b\u66f4\u7b80\u5355\u3001\u66f4\u7a33\u5065\u548c\u66f4\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u6709\u539f\u5219\u7684\u6846\u67b6\u6765\u9009\u62e9\u6027\u5f15\u5165\u975e\u7ebf\u6027\uff0c\u5c06\u52a8\u529b\u7cfb\u7edf\u7406\u8bba\u4e0e\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u4e2d\u957f\u8ddd\u79bb\u8bb0\u5fc6\u548c\u7ed3\u6784\u5316\u8ba1\u7b97\u7684\u529f\u80fd\u9700\u6c42\u8054\u7cfb\u8d77\u6765\uff0c\u5bf9\u4eba\u5de5\u548c\u751f\u7269\u795e\u7ecf\u7cfb\u7edf\u90fd\u6709\u542f\u793a\u3002"}}
{"id": "2506.06826", "pdf": "https://arxiv.org/pdf/2506.06826", "abs": "https://arxiv.org/abs/2506.06826", "authors": ["Chenfei Yuan", "Nanshan Jia", "Hangqi Li", "Peter W. Glynn", "Zeyu Zheng"], "title": "Controllable Coupled Image Generation via Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We provide an attention-level control method for the task of coupled image\ngeneration, where \"coupled\" means that multiple simultaneously generated images\nare expected to have the same or very similar backgrounds. While backgrounds\ncoupled, the centered objects in the generated images are still expected to\nenjoy the flexibility raised from different text prompts. The proposed method\ndisentangles the background and entity components in the model's\ncross-attention modules, attached with a sequence of time-varying weight\ncontrol parameters depending on the time step of sampling. We optimize this\nsequence of weight control parameters with a combined objective that assesses\nhow coupled the backgrounds are as well as text-to-image alignment and overall\nvisual quality. Empirical results demonstrate that our method outperforms\nexisting approaches across these criteria.", "AI": {"tldr": "The paper presents a novel method for coupled image generation that separates background and object components using time-varying weight control parameters, leading to improved performance in generating images with similar backgrounds but different objects.", "motivation": "To address the challenge of generating multiple images with the same or similar backgrounds but different centered objects based on distinct text prompts.", "method": "The method involves disentangling background and entity components within cross-attention modules and applying time-varying weight control parameters optimized through a combined objective function.", "result": "Empirical results indicate that the proposed method surpasses existing techniques in terms of background coupling, text-to-image alignment, and overall visual quality.", "conclusion": "The introduced attention-level control method effectively enhances the generation of coupled images by managing the relationship between shared backgrounds and unique objects."}}
{"id": "2506.07920", "pdf": "https://arxiv.org/pdf/2506.07920", "abs": "https://arxiv.org/abs/2506.07920", "authors": ["Hossein Babaei", "Mel White", "Richard G. Baraniuk"], "title": "W4S4: WaLRUS Meets S4 for Long-Range Sequence Modeling", "categories": ["cs.LG", "eess.AS", "eess.IV", "eess.SP"], "comment": "10 pages, 2 figures, 3 tables", "summary": "State Space Models (SSMs) have emerged as powerful components for sequence\nmodeling, enabling efficient handling of long-range dependencies via linear\nrecurrence and convolutional computation. However, their effectiveness depends\nheavily on the choice and initialization of the state matrix. In this work, we\nbuild on the SaFARi framework and existing WaLRUS SSMs to introduce a new\nvariant, W4S4 (WaLRUS for S4), a new class of SSMs constructed from redundant\nwavelet frames. WaLRUS admits a stable diagonalization and supports fast kernel\ncomputation without requiring low-rank approximations, making it both\ntheoretically grounded and computationally efficient. We show that WaLRUS\nretains information over long horizons significantly better than HiPPO-based\nSSMs, both in isolation and when integrated into deep architectures such as S4.\nOur experiments demonstrate consistent improvements across delay reconstruction\ntasks, classification benchmarks, and long-range sequence modeling, confirming\nthat high-quality, structured initialization enabled by wavelet-based state\ndynamic offers substantial advantages over existing alternatives. WaLRUS\nprovides a scalable and versatile foundation for the next generation of deep\nSSM-based models.", "AI": {"tldr": "State Space Models (SSMs) are powerful for sequence modeling but rely on the state matrix's choice and initialization. This paper introduces W4S4, a new SSM variant based on redundant wavelet frames within the WaLRUS framework. It offers stable diagonalization, fast kernel computation, and better long-horizon information retention compared to HiPPO-based SSMs. Experiments show improvements in various tasks, highlighting the advantages of wavelet-based state dynamics.", "motivation": "To address the limitations of existing SSMs, which heavily depend on the choice and initialization of the state matrix, and to improve their efficiency and effectiveness in handling long-range dependencies.", "method": "The method involves constructing a new class of SSMs called W4S4 from redundant wavelet frames within the WaLRUS framework. This approach allows for stable diagonalization and fast kernel computation without low-rank approximations.", "result": "WaLRUS retains information over long horizons significantly better than HiPPO-based SSMs. The experiments demonstrate consistent improvements across delay reconstruction tasks, classification benchmarks, and long-range sequence modeling.", "conclusion": "W4S4 provides a scalable and versatile foundation for the next generation of deep SSM-based models, offering substantial advantages due to high-quality, structured initialization enabled by wavelet-based state dynamics."}}
{"id": "2506.06830", "pdf": "https://arxiv.org/pdf/2506.06830", "abs": "https://arxiv.org/abs/2506.06830", "authors": ["Guankun Wang", "Rui Tang", "Mengya Xu", "Long Bai", "Huxin Gao", "Hongliang Ren"], "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Advanced Intelligent Systems", "summary": "Endoscopic surgery is the gold standard for robotic-assisted minimally\ninvasive surgery, offering significant advantages in early disease detection\nand precise interventions. However, the complexity of surgical scenes,\ncharacterized by high variability in different surgical activity scenarios and\nconfused image features between targets and the background, presents challenges\nfor surgical environment understanding. Traditional deep learning models often\nstruggle with cross-activity interference, leading to suboptimal performance in\neach downstream task. To address this limitation, we explore multi-task\nlearning, which utilizes the interrelated features between tasks to enhance\noverall task performance. In this paper, we propose EndoARSS, a novel\nmulti-task learning framework specifically designed for endoscopy surgery\nactivity recognition and semantic segmentation. Built upon the DINOv2\nfoundation model, our approach integrates Low-Rank Adaptation to facilitate\nefficient fine-tuning while incorporating Task Efficient Shared Low-Rank\nAdapters to mitigate gradient conflicts across diverse tasks. Additionally, we\nintroduce the Spatially-Aware Multi-Scale Attention that enhances feature\nrepresentation discrimination by enabling cross-spatial learning of global\ninformation. In order to evaluate the effectiveness of our framework, we\npresent three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored\nfor endoscopic surgery scenarios with detailed annotations for both activity\nrecognition and semantic segmentation tasks. Extensive experiments demonstrate\nthat EndoARSS achieves remarkable performance across multiple benchmarks,\nsignificantly improving both accuracy and robustness in comparison to existing\nmodels. These results underscore the potential of EndoARSS to advance AI-driven\nendoscopic surgical systems, offering valuable insights for enhancing surgical\nsafety and efficiency.", "AI": {"tldr": "The paper introduces EndoARSS, a multi-task learning framework for endoscopy surgery activity recognition and semantic segmentation. It uses DINOv2 with Low-Rank Adaptation and Spatially-Aware Multi-Scale Attention. Three new datasets are presented, and experiments show significant improvements in accuracy and robustness compared to existing models.", "motivation": "Endoscopic surgery is crucial for minimally invasive procedures but faces challenges due to complex surgical scenes and image feature confusion between targets and background. Traditional deep learning models struggle with cross-activity interference, prompting the need for a more effective solution.", "method": "The authors propose EndoARSS, built on the DINOv2 foundation model, incorporating Low-Rank Adaptation for efficient fine-tuning and Task Efficient Shared Low-Rank Adapters to address gradient conflicts across tasks. Additionally, Spatially-Aware Multi-Scale Attention is introduced to enhance feature representation discrimination by enabling cross-spatial learning of global information.", "result": "Extensive experiments demonstrate that EndoARSS significantly improves both accuracy and robustness in multiple benchmarks related to endoscopic surgery scenarios, surpassing existing models.", "conclusion": "EndoARSS shows great potential in advancing AI-driven endoscopic surgical systems, providing valuable insights for enhancing surgical safety and efficiency."}}
{"id": "2506.07929", "pdf": "https://arxiv.org/pdf/2506.07929", "abs": "https://arxiv.org/abs/2506.07929", "authors": ["Amirreza Yasami", "Mohammadali Tofigh", "Mahdi Shahbakhti", "Charles Robert Koch"], "title": "A Generative Physics-Informed Reinforcement Learning-Based Approach for Construction of Representative Drive Cycle", "categories": ["cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Accurate driving cycle construction is crucial for vehicle design, fuel\neconomy analysis, and environmental impact assessments. A generative\nPhysics-Informed Expected SARSA-Monte Carlo (PIESMC) approach that constructs\nrepresentative driving cycles by capturing transient dynamics, acceleration,\ndeceleration, idling, and road grade transitions while ensuring model fidelity\nis introduced. Leveraging a physics-informed reinforcement learning framework\nwith Monte Carlo sampling, PIESMC delivers efficient cycle construction with\nreduced computational cost. Experimental evaluations on two real-world datasets\ndemonstrate that PIESMC replicates key kinematic and energy metrics, achieving\nup to a 57.3% reduction in cumulative kinematic fragment errors compared to the\nMicro-trip-based (MTB) method and a 10.5% reduction relative to the\nMarkov-chain-based (MCB) method. Moreover, it is nearly an order of magnitude\nfaster than conventional techniques. Analyses of vehicle-specific power\ndistributions and wavelet-transformed frequency content further confirm its\nability to reproduce experimental central tendencies and variability.", "AI": {"tldr": "A new method called PIESMC is developed for constructing representative driving cycles, which is more accurate and efficient than existing methods.", "motivation": "Accurate driving cycle construction is crucial for vehicle design, fuel economy analysis, and environmental impact assessments.", "method": "PIESMC approach captures transient dynamics, acceleration, deceleration, idling, and road grade transitions while ensuring model fidelity using a physics-informed reinforcement learning framework with Monte Carlo sampling.", "result": "PIESMC achieves up to a 57.3% reduction in cumulative kinematic fragment errors compared to the MTB method and a 10.5% reduction relative to the MCB method. It is nearly an order of magnitude faster than conventional techniques.", "conclusion": "PIESMC delivers efficient cycle construction with reduced computational cost and accurately replicates key kinematic and energy metrics."}}
{"id": "2506.06836", "pdf": "https://arxiv.org/pdf/2506.06836", "abs": "https://arxiv.org/abs/2506.06836", "authors": ["Zelin He", "Sarah Alnegheimish", "Matthew Reimherr"], "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Time-series anomaly detection (TSAD) has played a vital role in a variety of\nfields, including healthcare, finance, and industrial monitoring. Prior\nmethods, which mainly focus on training domain-specific models on numerical\ndata, lack the visual-temporal reasoning capacity that human experts have to\nidentify contextual anomalies. To fill this gap, we explore a solution based on\nvision language models (VLMs). Recent studies have shown the ability of VLMs\nfor visual reasoning tasks, yet their direct application to time series has\nfallen short on both accuracy and efficiency. To harness the power of VLMs for\nTSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening\nstage built on a relatively lightweight pretrained vision encoder, which\nleverages 2-D time-series representations to accurately localize candidate\nanomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal\ncontext and VLM reasoning capacity to refine the detection upon the candidates\nprovided by ViT4TS. We show that without any time-series training, VLM4TS\noutperforms time-series pretrained and from-scratch baselines in most cases,\nyielding a 24.6 percent improvement in F1-max score over the best baseline.\nMoreover, VLM4TS also consistently outperforms existing language-model-based\nTSAD methods and is on average 36 times more efficient in token usage.", "AI": {"tldr": "The paper proposes a two-stage solution for time-series anomaly detection (TSAD) using vision language models (VLMs), including ViT4TS and VLM4TS, which significantly improves accuracy and efficiency compared to existing methods.", "motivation": "Current TSAD methods lack the visual-temporal reasoning capacity that human experts have, making it difficult to identify contextual anomalies. To address this limitation, the authors explore the use of vision language models (VLMs) for TSAD.", "method": "The proposed method consists of two stages: 1) ViT4TS, a vision-screening stage based on a lightweight pretrained vision encoder that localizes candidate anomalies using 2-D time-series representations; 2) VLM4TS, a VLM-based stage that refines the detection by integrating global temporal context and VLM reasoning capacity.", "result": "VLM4TS outperforms time-series pretrained and from-scratch baselines in most cases, with a 24.6 percent improvement in F1-max score over the best baseline. It also consistently outperforms existing language-model-based TSAD methods and is 36 times more efficient in token usage.", "conclusion": "The proposed two-stage solution effectively leverages the power of VLMs for TSAD, achieving significant improvements in both accuracy and efficiency."}}
{"id": "2506.07933", "pdf": "https://arxiv.org/pdf/2506.07933", "abs": "https://arxiv.org/abs/2506.07933", "authors": ["Lev V. Utkin", "Semen P. Khomets", "Vlada A. Efremenko", "Andrei V. Konstantinov", "Natalya M. Verbova"], "title": "Ensemble-Based Survival Models with the Self-Attended Beran Estimator Predictions", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Survival analysis predicts the time until an event of interest, such as\nfailure or death, but faces challenges due to censored data, where some events\nremain unobserved. Ensemble-based models, like random survival forests and\ngradient boosting, are widely used but can produce unstable predictions due to\nvariations in bootstrap samples. To address this, we propose SurvBESA (Survival\nBeran Estimators Self-Attended), a novel ensemble model that combines Beran\nestimators with a self-attention mechanism. Unlike traditional methods,\nSurvBESA applies self-attention to predicted survival functions, smoothing out\nnoise by adjusting each survival function based on its similarity to\nneighboring survival functions. We also explore a special case using Huber's\ncontamination model to define attention weights, simplifying training to a\nquadratic or linear optimization problem. Numerical experiments show that\nSurvBESA outperforms state-of-the-art models. The implementation of SurvBESA is\npublicly available.", "AI": {"tldr": "SurvBESA is a new ensemble model that combines Beran estimators with self-attention to predict survival functions more accurately and stably than existing methods.", "motivation": "Survival analysis predictions can be unstable due to censored data and variations in bootstrap samples when using ensemble-based models.", "method": "Propose SurvBESA, an ensemble model that uses Beran estimators and a self-attention mechanism on predicted survival functions; also explore a special case using Huber's contamination model for defining attention weights, simplifying training.", "result": "Numerical experiments demonstrate SurvBESA outperforms state-of-the-art models in survival analysis.", "conclusion": "SurvBESA offers improved performance and stability in survival analysis predictions compared to existing models."}}
{"id": "2506.06837", "pdf": "https://arxiv.org/pdf/2506.06837", "abs": "https://arxiv.org/abs/2506.06837", "authors": ["Eyal Briman", "Ehud Shapiro", "Nimrod Talmon"], "title": "AI-Generated Compromises for Coalition Formation", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The challenge of finding compromises between agent proposals is fundamental\nto AI subfields such as argumentation, mediation, and negotiation. Building on\nthis tradition, Elkind et al. (2021) introduced a process for coalition\nformation that seeks majority-supported proposals preferable to the status quo,\nusing a metric space where each agent has an ideal point. A crucial step in\nthis process involves identifying compromise proposals around which agent\ncoalitions can unite. How to effectively find such compromise proposals remains\nan open question. We address this gap by formalizing a model that incorporates\nagent bounded rationality and uncertainty, and by developing AI methods to\ngenerate compromise proposals. We focus on the domain of collaborative document\nwriting, such as the democratic drafting of a community constitution. Our\napproach uses natural language processing techniques and large language models\nto induce a semantic metric space over text. Based on this space, we design\nalgorithms to suggest compromise points likely to receive broad support. To\nevaluate our methods, we simulate coalition formation processes and show that\nAI can facilitate large-scale democratic text editing, a domain where\ntraditional tools are limited.", "AI": {"tldr": "This paper aims to solve the problem of finding compromise proposals among agents by incorporating agent bounded rationality and uncertainty, developing AI methods for generating compromises in collaborative document writing.", "motivation": "The motivation lies in addressing a crucial gap in coalition formation processes, which involves identifying compromise proposals that can unite agent coalitions.", "method": "The method includes formalizing a model with agent bounded rationality and uncertainty, using NLP techniques and large language models to create a semantic metric space over text, and designing algorithms to suggest compromise points.", "result": "Through simulations, the results show that AI can effectively facilitate large-scale democratic text editing.", "conclusion": "AI-based methods have potential in enhancing democratic drafting processes, such as community constitution writing, where traditional tools are insufficient."}}
{"id": "2506.06840", "pdf": "https://arxiv.org/pdf/2506.06840", "abs": "https://arxiv.org/abs/2506.06840", "authors": ["Fahad Mostafa"], "title": "A Statistical Framework for Model Selection in LSTM Networks", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "62M10, 92B20, 62P10, 62P99"], "comment": null, "summary": "Long Short-Term Memory (LSTM) neural network models have become the\ncornerstone for sequential data modeling in numerous applications, ranging from\nnatural language processing to time series forecasting. Despite their success,\nthe problem of model selection, including hyperparameter tuning, architecture\nspecification, and regularization choice remains largely heuristic and\ncomputationally expensive. In this paper, we propose a unified statistical\nframework for systematic model selection in LSTM networks. Our framework\nextends classical model selection ideas, such as information criteria and\nshrinkage estimation, to sequential neural networks. We define penalized\nlikelihoods adapted to temporal structures, propose a generalized threshold\napproach for hidden state dynamics, and provide efficient estimation strategies\nusing variational Bayes and approximate marginal likelihood methods. Several\nbiomedical data centric examples demonstrate the flexibility and improved\nperformance of the proposed framework.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eLSTM\u7f51\u7edc\u7cfb\u7edf\u6a21\u578b\u9009\u62e9\u7684\u7edf\u4e00\u7edf\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u9002\u5e94\u65f6\u95f4\u7ed3\u6784\u7684\u60e9\u7f5a\u4f3c\u7136\u3001\u9690\u85cf\u72b6\u6001\u52a8\u6001\u7684\u5e7f\u4e49\u9608\u503c\u65b9\u6cd5\u548c\u4f7f\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u53ca\u8fd1\u4f3c\u8fb9\u7f18\u4f3c\u7136\u7684\u6709\u6548\u4f30\u8ba1\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u6a21\u578b\u9009\u62e9\u8fc7\u7a0b\u3002", "motivation": "\u5c3d\u7ba1LSTM\u795e\u7ecf\u7f51\u7edc\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff08\u5305\u62ec\u8d85\u53c2\u6570\u8c03\u6574\u3001\u67b6\u6784\u89c4\u8303\u548c\u6b63\u5219\u5316\u9009\u62e9\uff09\u4ecd\u7136\u4e3b\u8981\u4f9d\u8d56\u4e8e\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8be5\u6846\u67b6\u6269\u5c55\u4e86\u7ecf\u5178\u6a21\u578b\u9009\u62e9\u601d\u60f3\uff0c\u5982\u4fe1\u606f\u6807\u51c6\u548c\u6536\u7f29\u4f30\u8ba1\uff0c\u5230\u987a\u5e8f\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5b9a\u4e49\u4e86\u9002\u5e94\u65f6\u95f4\u7ed3\u6784\u7684\u60e9\u7f5a\u4f3c\u7136\uff0c\u63d0\u51fa\u4e86\u9690\u85cf\u72b6\u6001\u52a8\u6001\u7684\u5e7f\u4e49\u9608\u503c\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8e\u53d8\u5206\u8d1d\u53f6\u65af\u548c\u8fd1\u4f3c\u8fb9\u9645\u4f3c\u7136\u7684\u6709\u6548\u4f30\u8ba1\u7b56\u7565\u3002", "result": "\u51e0\u4e2a\u4ee5\u751f\u7269\u533b\u5b66\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4f8b\u5b50\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u7684\u7075\u6d3b\u6027\u548c\u6539\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLSTM\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cfb\u7edf\u5316\u7684\u6a21\u578b\u9009\u62e9\u65b9\u6cd5\uff0c\u80fd\u591f\u63d0\u9ad8\u6a21\u578b\u9009\u62e9\u6548\u7387\u5e76\u6539\u5584\u6027\u80fd\u3002"}}
{"id": "2506.07949", "pdf": "https://arxiv.org/pdf/2506.07949", "abs": "https://arxiv.org/abs/2506.07949", "authors": ["Anastasios N. Angelopoulos", "Jacob Eisenstein", "Jonathan Berant", "Alekh Agarwal", "Adam Fisch"], "title": "Cost-Optimal Active AI Model Evaluation", "categories": ["cs.LG"], "comment": null, "summary": "The development lifecycle of generative AI systems requires continual\nevaluation, data acquisition, and annotation, which is costly in both resources\nand time. In practice, rapid iteration often makes it necessary to rely on\nsynthetic annotation data because of the low cost, despite the potential for\nsubstantial bias. In this paper, we develop novel, cost-aware methods for\nactively balancing the use of a cheap, but often inaccurate, weak rater -- such\nas a model-based autorater that is designed to automatically assess the quality\nof generated content -- with a more expensive, but also more accurate, strong\nrater alternative such as a human. More specifically, the goal of our approach\nis to produce a low variance, unbiased estimate of the mean of the target\n\"strong\" rating, subject to some total annotation budget. Building on recent\nwork in active and prediction-powered statistical inference, we derive a family\nof cost-optimal policies for allocating a given annotation budget between weak\nand strong raters so as to maximize statistical efficiency. Using synthetic and\nreal-world data, we empirically characterize the conditions under which these\npolicies yield improvements over prior methods. We find that, especially in\ntasks where there is high variability in the difficulty of examples, our\npolicies can achieve the same estimation precision at a far lower total\nannotation budget than standard evaluation methods.", "AI": {"tldr": "\u5728\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u5f00\u53d1\u5468\u671f\u4e2d\uff0c\u6301\u7eed\u7684\u8bc4\u4f30\u3001\u6570\u636e\u83b7\u53d6\u548c\u6807\u6ce8\u65e2\u8017\u8d39\u8d44\u6e90\u53c8\u8017\u65f6\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6210\u672c\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e73\u8861\u4f7f\u7528\u5ec9\u4ef7\u4f46\u4e0d\u51c6\u786e\u7684\u5f31\u8bc4\u4f30\u8005\uff08\u5982\u6a21\u578b\u81ea\u52a8\u8bc4\u4f30\u5668\uff09\u4e0e\u6602\u8d35\u4f46\u66f4\u51c6\u786e\u7684\u5f3a\u8bc4\u4f30\u8005\uff08\u5982\u4eba\u7c7b\uff09\uff0c\u4ee5\u5728\u6709\u9650\u9884\u7b97\u4e0b\u5b9e\u73b0\u5bf9\u76ee\u6807'\u5f3a'\u8bc4\u5206\u5747\u503c\u7684\u4f4e\u504f\u5dee\u65e0\u504f\u4f30\u8ba1\u3002\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u548c\u9884\u6d4b\u9a71\u52a8\u7edf\u8ba1\u63a8\u65ad\u7684\u7814\u7a76\uff0c\u4f5c\u8005\u63a8\u5bfc\u51fa\u4e00\u7cfb\u5217\u6210\u672c\u6700\u4f18\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u5f31\u8bc4\u4f30\u8005\u548c\u5f3a\u8bc4\u4f30\u8005\u4e4b\u95f4\u5206\u914d\u6ce8\u91ca\u9884\u7b97\uff0c\u4ece\u800c\u6700\u5927\u5316\u7edf\u8ba1\u6548\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u8f83\u5927\u7684\u4efb\u52a1\u4e2d\uff0c\u65b0\u7b56\u7565\u80fd\u5728\u663e\u8457\u964d\u4f4e\u603b\u6807\u6ce8\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u540c\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u751f\u547d\u5468\u671f\u9700\u8981\u4e0d\u65ad\u8fdb\u884c\u8bc4\u4f30\u3001\u6570\u636e\u83b7\u53d6\u548c\u6807\u6ce8\uff0c\u8fd9\u65e2\u6d88\u8017\u8d44\u6e90\u53c8\u8017\u65f6\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7531\u4e8e\u5feb\u901f\u8fed\u4ee3\u7684\u9700\u6c42\uff0c\u5f80\u5f80\u4f9d\u8d56\u5408\u6210\u6807\u6ce8\u6570\u636e\uff0c\u5c3d\u7ba1\u5176\u53ef\u80fd\u5b58\u5728\u663e\u8457\u504f\u5dee\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u6709\u6548\u7ed3\u5408\u5ec9\u4ef7\u4f46\u4e0d\u51c6\u786e\u7684\u5f31\u8bc4\u4f30\u8005\u4e0e\u6602\u8d35\u4f46\u51c6\u786e\u7684\u5f3a\u8bc4\u4f30\u8005\uff0c\u6210\u4e3a\u4e86\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6210\u672c\u611f\u77e5\u65b9\u6cd5\uff0c\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5f31\u8bc4\u4f30\u8005\u548c\u5f3a\u8bc4\u4f30\u8005\u4e4b\u95f4\u7684\u9884\u7b97\u5206\u914d\uff0c\u6765\u6700\u5927\u5316\u7edf\u8ba1\u6548\u7387\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ed6\u4eec\u57fa\u4e8e\u4e3b\u52a8\u5b66\u4e60\u548c\u9884\u6d4b\u9a71\u52a8\u7edf\u8ba1\u63a8\u65ad\u6280\u672f\uff0c\u63a8\u5bfc\u51fa\u4e00\u7cfb\u5217\u6210\u672c\u6700\u4f18\u7b56\u7565\u3002\u8fd9\u4e9b\u7b56\u7565\u80fd\u591f\u6839\u636e\u4efb\u52a1\u7279\u70b9\u52a8\u6001\u8c03\u6574\u5f31\u8bc4\u4f30\u8005\u548c\u5f3a\u8bc4\u4f30\u8005\u7684\u4f7f\u7528\u6bd4\u4f8b\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4f5c\u8005\u53d1\u73b0\uff0c\u5728\u6837\u672c\u96be\u5ea6\u5dee\u5f02\u8f83\u5927\u7684\u4efb\u52a1\u4e2d\uff0c\u65b0\u7b56\u7565\u53ef\u4ee5\u5728\u663e\u8457\u964d\u4f4e\u603b\u6807\u6ce8\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0e\u6807\u51c6\u8bc4\u4f30\u65b9\u6cd5\u76f8\u540c\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5728\u751f\u6210\u5f0fAI\u7cfb\u7edf\u7684\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5408\u7406\u5206\u914d\u5f31\u8bc4\u4f30\u8005\u548c\u5f3a\u8bc4\u4f30\u8005\u7684\u4f7f\u7528\u6bd4\u4f8b\uff0c\u6709\u6548\u964d\u4f4e\u603b\u6807\u6ce8\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u8f83\u9ad8\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002\u8fd9\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2506.06842", "pdf": "https://arxiv.org/pdf/2506.06842", "abs": "https://arxiv.org/abs/2506.06842", "authors": ["Arkadiusz Modzelewski", "Witold Sosnowski", "Tiziano Labruna", "Adam Wierzbicki", "Giovanni Da San Martino"], "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Disinformation detection is a key aspect of media literacy. Psychological\nstudies have shown that knowledge of persuasive fallacies helps individuals\ndetect disinformation. Inspired by these findings, we experimented with large\nlanguage models (LLMs) to test whether infusing persuasion knowledge enhances\ndisinformation detection. As a result, we introduce the Persuasion-Augmented\nChain of Thought (PCoT), a novel approach that leverages persuasion to improve\ndisinformation detection in zero-shot classification. We extensively evaluate\nPCoT on online news and social media posts. Moreover, we publish two novel,\nup-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets\nenable the evaluation of PCoT on content entirely unseen by the LLMs used in\nour experiments, as the content was published after the models' knowledge\ncutoffs. We show that, on average, PCoT outperforms competitive methods by 15%\nacross five LLMs and five datasets. These findings highlight the value of\npersuasion in strengthening zero-shot disinformation detection.", "AI": {"tldr": "The paper explores using large language models with persuasion knowledge to enhance disinformation detection, introducing Persuasion-Augmented Chain of Thought (PCoT) which improves zero-shot classification and outperforms other methods by 15%.", "motivation": "Psychological studies indicate that understanding persuasive fallacies aids in detecting disinformation.", "method": "Experimenting with large language models infused with persuasion knowledge, leading to the creation of PCoT approach for disinformation detection.", "result": "PCoT outperforms competitive methods by 15% across five LLMs and five datasets.", "conclusion": "Persuasion plays a crucial role in enhancing zero-shot disinformation detection."}}
{"id": "2506.07958", "pdf": "https://arxiv.org/pdf/2506.07958", "abs": "https://arxiv.org/abs/2506.07958", "authors": ["Salah A. Faroughi", "Farinaz Mostajeran"], "title": "Neural Tangent Kernel Analysis to Probe Convergence in Physics-informed Neural Solvers: PIKANs vs. PINNs", "categories": ["cs.LG", "math-ph", "math.AP", "math.MP", "math.SP"], "comment": null, "summary": "Physics-informed Kolmogorov-Arnold Networks (PIKANs), and in particular their\nChebyshev-based variants (cPIKANs), have recently emerged as promising models\nfor solving partial differential equations (PDEs). However, their training\ndynamics and convergence behavior remain largely unexplored both theoretically\nand numerically. In this work, we aim to advance the theoretical understanding\nof cPIKANs by analyzing them using Neural Tangent Kernel (NTK) theory. Our\nobjective is to discern the evolution of kernel structure throughout\ngradient-based training and its subsequent impact on learning efficiency. We\nfirst derive the NTK of standard cKANs in a supervised setting, and then extend\nthe analysis to the physics-informed context. We analyze the spectral\nproperties of NTK matrices, specifically their eigenvalue distributions and\nspectral bias, for four representative PDEs: the steady-state Helmholtz\nequation, transient diffusion and Allen-Cahn equations, and forced vibrations\ngoverned by the Euler-Bernoulli beam equation. We also conduct an investigation\ninto the impact of various optimization strategies, e.g., first-order,\nsecond-order, and hybrid approaches, on the evolution of the NTK and the\nresulting learning dynamics. Results indicate a tractable behavior for NTK in\nthe context of cPIKANs, which exposes learning dynamics that standard\nphysics-informed neural networks (PINNs) cannot capture. Spectral trends also\nreveal when domain decomposition improves training, directly linking kernel\nbehavior to convergence rates under different setups. To the best of our\nknowledge, this is the first systematic NTK study of cPIKANs, providing\ntheoretical insight that clarifies and predicts their empirical performance.", "AI": {"tldr": "Physics-informed Kolmogorov-Arnold Networks (PIKANs), particularly their Chebyshev-based variants (cPIKANs), have shown promise in solving PDEs. This paper advances the theoretical understanding of cPIKANs by analyzing them with Neural Tangent Kernel (NTK) theory, examining kernel structure evolution and its impact on learning efficiency during gradient-based training.", "motivation": "To deepen the theoretical understanding of cPIKANs' training dynamics and convergence behavior, which remain largely unexplored both theoretically and numerically.", "method": "Derive the NTK of standard cKANs in a supervised setting, extend the analysis to the physics-informed context, analyze the spectral properties of NTK matrices for four representative PDEs, and investigate the impact of various optimization strategies on the evolution of the NTK and learning dynamics.", "result": "Results show tractable NTK behavior in cPIKANs that uncovers learning dynamics not captured by standard PINNs. Spectral trends also reveal conditions where domain decomposition improves training and link kernel behavior to convergence rates under different setups.", "conclusion": "This study is the first systematic NTK analysis of cPIKANs, offering theoretical insights that clarify and predict their empirical performance."}}
{"id": "2506.06852", "pdf": "https://arxiv.org/pdf/2506.06852", "abs": "https://arxiv.org/abs/2506.06852", "authors": ["John Waithaka", "Moise Busogi"], "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation of satellite imagery is crucial for Earth observation\napplications, but remains constrained by limited labelled training data. While\nself-supervised pretraining methods like Masked Autoencoders (MAE) have shown\npromise, they focus on reconstruction rather than localisation-a fundamental\naspect of segmentation tasks. We propose adapting LOCA (Location-aware), a\nposition prediction self-supervised learning method, for multimodal satellite\nimagery semantic segmentation. Our approach addresses the unique challenges of\nsatellite data by extending SatMAE's channel grouping from multispectral to\nmultimodal data, enabling effective handling of multiple modalities, and\nintroducing same-group attention masking to encourage cross-modal interaction\nduring pretraining. The method uses relative patch position prediction,\nencouraging spatial reasoning for localisation rather than reconstruction. We\nevaluate our approach on the Sen1Floods11 flood mapping dataset, where it\nsignificantly outperforms existing reconstruction-based self-supervised\nlearning methods for satellite imagery. Our results demonstrate that position\nprediction tasks, when properly adapted for multimodal satellite imagery, learn\nrepresentations more effective for satellite image semantic segmentation than\nreconstruction-based approaches.", "AI": {"tldr": "This paper proposes adapting LOCA (Location-aware) for multimodal satellite imagery semantic segmentation, which significantly outperforms existing reconstruction-based self-supervised learning methods.", "motivation": "Semantic segmentation of satellite imagery is crucial for Earth observation applications, but remains constrained by limited labelled training data. Self-supervised pretraining methods like Masked Autoencoders (MAE) focus on reconstruction rather than localisation, which is a fundamental aspect of segmentation tasks.", "method": "The approach adapts LOCA (Location-aware) for multimodal satellite imagery semantic segmentation. It extends SatMAE's channel grouping from multispectral to multimodal data, introduces same-group attention masking to encourage cross-modal interaction during pretraining, and uses relative patch position prediction to encourage spatial reasoning for localisation rather than reconstruction.", "result": "Evaluated on the Sen1Floods11 flood mapping dataset, the proposed approach significantly outperforms existing reconstruction-based self-supervised learning methods for satellite imagery.", "conclusion": "Position prediction tasks, when properly adapted for multimodal satellite imagery, learn representations more effective for satellite image semantic segmentation than reconstruction-based approaches."}}
{"id": "2506.07969", "pdf": "https://arxiv.org/pdf/2506.07969", "abs": "https://arxiv.org/abs/2506.07969", "authors": ["Jacob Helwig", "Sai Sreeharsha Adavi", "Xuan Zhang", "Yuchao Lin", "Felix S. Chim", "Luke Takeshi Vizzini", "Haiyang Yu", "Muhammad Hasnain", "Saykat Kumar Biswas", "John J. Holloway", "Narendra Singh", "N. K. Anand", "Swagnik Guhathakurta", "Shuiwang Ji"], "title": "A Two-Phase Deep Learning Framework for Adaptive Time-Stepping in High-Speed Flow Modeling", "categories": ["cs.LG", "physics.flu-dyn"], "comment": null, "summary": "We consider the problem of modeling high-speed flows using machine learning\nmethods. While most prior studies focus on low-speed fluid flows in which\nuniform time-stepping is practical, flows approaching and exceeding the speed\nof sound exhibit sudden changes such as shock waves. In such cases, it is\nessential to use adaptive time-stepping methods to allow a temporal resolution\nsufficient to resolve these phenomena while simultaneously balancing\ncomputational costs. Here, we propose a two-phase machine learning method,\nknown as ShockCast, to model high-speed flows with adaptive time-stepping. In\nthe first phase, we propose to employ a machine learning model to predict the\ntimestep size. In the second phase, the predicted timestep is used as an input\nalong with the current fluid fields to advance the system state by the\npredicted timestep. We explore several physically-motivated components for\ntimestep prediction and introduce timestep conditioning strategies inspired by\nneural ODE and Mixture of Experts. As ShockCast is the first framework for\nlearning high-speed flows, we evaluate our methods by generating two supersonic\nflow datasets, available at https://huggingface.co/datasets/divelab. Our code\nis publicly available as part of the AIRS library\n(https://github.com/divelab/AIRS).", "AI": {"tldr": "A two-phase machine learning method, ShockCast, is proposed for modeling high-speed flows with adaptive time-stepping.", "motivation": "High-speed flows approaching and exceeding the speed of sound exhibit sudden changes such as shock waves. Adaptive time-stepping methods are essential to resolve these phenomena while balancing computational costs.", "method": "The method consists of two phases: 1) using a machine learning model to predict the timestep size; 2) using the predicted timestep along with current fluid fields to advance the system state. Several physically-motivated components for timestep prediction are explored and timestep conditioning strategies inspired by neural ODE and Mixture of Experts are introduced.", "result": "ShockCast is evaluated by generating two supersonic flow datasets. The code is publicly available as part of the AIRS library.", "conclusion": "ShockCast is the first framework for learning high-speed flows and it provides an effective solution for modeling such flows with adaptive time-stepping."}}
{"id": "2506.07972", "pdf": "https://arxiv.org/pdf/2506.07972", "abs": "https://arxiv.org/abs/2506.07972", "authors": ["Hongzheng Chen", "Yingheng Wang", "Yaohui Cai", "Hins Hu", "Jiajie Li", "Shirley Huang", "Chenhui Deng", "Rongjian Liang", "Shufeng Kong", "Haoxing Ren", "Samitha Samaranayake", "Carla P. Gomes", "Zhiru Zhang"], "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.", "AI": {"tldr": "The paper introduces HeuriGym, a framework for evaluating LLMs' ability to generate heuristic algorithms for combinatorial optimization problems. It highlights current evaluation limitations, presents the Quality-Yield Index (QYI) metric, and reveals persistent limitations in LLMs' tool use, planning, and adaptive reasoning.", "motivation": "Current evaluation methodologies for LLMs are inadequate, as they either rely on closed-ended questions that lead to saturation and memorization or subjective comparisons lacking consistency and rigor.", "method": "HeuriGym is an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems. It allows LLMs to propose heuristics, receive feedback via code execution, and iteratively refine their solutions.", "result": "Evaluation of nine state-of-the-art models on nine problems across various domains using HeuriGym exposed persistent limitations in LLMs' tool use, planning, and adaptive reasoning. Top models achieved QYI scores of only 0.6, significantly below the expert baseline of 1.", "conclusion": "HeuriGym provides a valuable benchmark for guiding the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains."}}
{"id": "2506.06862", "pdf": "https://arxiv.org/pdf/2506.06862", "abs": "https://arxiv.org/abs/2506.06862", "authors": ["Chenguang Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "accepted to International Journal of Robotics Research (IJRR). 24\n  pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and\n  AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/", "summary": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "AI": {"tldr": "This paper proposes multimodal spatial language maps that combine pretrained multimodal features with 3D environment reconstructions, enabling robots to understand and execute spatial and multimodal navigation goals.", "motivation": "Existing methods for grounding language in a navigating agent's observations either lack connection to environment mapping, spatial precision of geometric maps, or neglect additional modalities beyond vision.", "method": "The authors propose multimodal spatial language maps which fuse pretrained multimodal features with a 3D reconstruction of the environment. They present two instances: visual-language maps (VLMaps) and audio-visual-language maps (AVLMaps). These maps are built autonomously using standard exploration and can be combined with large language models (LLMs) for enhanced capabilities.", "result": "Experiments in simulation and real-world settings show that the proposed multimodal spatial language maps enable zero-shot spatial and multimodal goal navigation, improving recall by 50% in ambiguous scenarios. The approach is applicable to mobile robots and tabletop manipulators.", "conclusion": "Multimodal spatial language maps provide a unified representation integrating audio, visual, and language cues, allowing robots to ground multimodal goal queries to spatial locations for navigation, significantly enhancing goal disambiguation in ambiguous environments."}}
{"id": "2506.07975", "pdf": "https://arxiv.org/pdf/2506.07975", "abs": "https://arxiv.org/abs/2506.07975", "authors": ["Caleb Zheng", "Eli Shlizerman"], "title": "Hyperpruning: Efficient Search through Pruned Variants of Recurrent Neural Networks Leveraging Lyapunov Spectrum", "categories": ["cs.LG"], "comment": "26 pages, 3 figures", "summary": "A variety of pruning methods have been introduced for over-parameterized\nRecurrent Neural Networks to improve efficiency in terms of power consumption\nand storage utilization. These advances motivate a new paradigm, termed\n`hyperpruning', which seeks to identify the most suitable pruning strategy for\na given network architecture and application. Unlike conventional\nhyperparameter search, where the optimal configuration's accuracy remains\nuncertain, in the context of network pruning, the accuracy of the dense model\nsets the target for the accuracy of the pruned one. The goal, therefore, is to\ndiscover pruned variants that match or even surpass this established accuracy.\nHowever, exhaustive search over pruning configurations is computationally\nexpensive and lacks early performance guarantees. To address this challenge, we\npropose a novel Lyapunov Spectrum (LS)-based distance metric that enables early\ncomparison between pruned and dense networks, allowing accurate prediction of\npost-training performance. By integrating this LS-based distance with standard\nhyperparameter optimization algorithms, we introduce an efficient hyperpruning\nframework, termed LS-based Hyperpruning (LSH). LSH reduces search time by an\norder of magnitude compared to conventional approaches relying on full\ntraining. Experiments on stacked LSTM and RHN architectures using the Penn\nTreebank dataset, and on AWD-LSTM-MoS using WikiText-2, demonstrate that under\nfixed training budgets and target pruning ratios, LSH consistently identifies\nsuperior pruned models. Remarkably, these pruned variants not only outperform\nthose selected by loss-based baseline but also exceed the performance of their\ndense counterpart.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4fee\u526a\u65b9\u6cd5\u2014\u2014\u8d85\u4fee\u526a\uff08hyperpruning\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u8c31\uff08LS\uff09\u8ddd\u79bb\u7684\u9ad8\u6548\u8d85\u4fee\u526a\u6846\u67b6\uff08LSH\uff09\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u4f18\u4e8e\u5bc6\u96c6\u6a21\u578b\u7684\u4fee\u526a\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709\u7684\u4fee\u526a\u65b9\u6cd5\u867d\u7136\u53ef\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u5728\u641c\u7d22\u6700\u4f73\u4fee\u526a\u914d\u7f6e\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u4e14\u7f3a\u4e4f\u65e9\u671f\u6027\u80fd\u4fdd\u8bc1\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u641c\u7d22\u7b56\u7565\u6765\u786e\u5b9a\u6700\u9002\u5408\u7684\u4fee\u526a\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u674e\u96c5\u666e\u8bfa\u592b\u8c31\uff08LS\uff09\u8ddd\u79bb\u7684\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5141\u8bb8\u5728\u8bad\u7ec3\u524d\u6bd4\u8f83\u4fee\u526a\u7f51\u7edc\u548c\u5bc6\u96c6\u7f51\u7edc\uff0c\u5e76\u9884\u6d4b\u8bad\u7ec3\u540e\u7684\u6027\u80fd\u3002\u7ed3\u5408\u6807\u51c6\u8d85\u53c2\u6570\u4f18\u5316\u7b97\u6cd5\uff0c\u6784\u5efa\u4e86LSH\u6846\u67b6\uff0c\u4ee5\u52a0\u901f\u641c\u7d22\u8fc7\u7a0b\u5e76\u627e\u5230\u6700\u4f18\u4fee\u526a\u914d\u7f6e\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLSH\u80fd\u591f\u5728\u56fa\u5b9a\u8bad\u7ec3\u9884\u7b97\u548c\u76ee\u6807\u4fee\u526a\u7387\u4e0b\uff0c\u6301\u7eed\u53d1\u73b0\u4f18\u4e8e\u635f\u5931\u57fa\u51c6\u9009\u62e9\u7684\u4fee\u526a\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8d8a\u539f\u59cb\u5bc6\u96c6\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "LSH\u6846\u67b6\u663e\u8457\u51cf\u5c11\u4e86\u641c\u7d22\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u4fee\u526a\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u7f51\u7edc\u4fee\u526a\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.06864", "pdf": "https://arxiv.org/pdf/2506.06864", "abs": "https://arxiv.org/abs/2506.06864", "authors": ["Junyu Liu", "Jianfeng Ren", "Sunhong Liang", "Xudong Jiang"], "title": "Face recognition on point cloud with cgan-top for denoising", "categories": ["cs.CV", "cs.AI"], "comment": "Published in ICASSP 2023", "summary": "Face recognition using 3D point clouds is gaining growing interest, while raw\npoint clouds often contain a significant amount of noise due to imperfect\nsensors. In this paper, an end-to-end 3D face recognition on a noisy point\ncloud is proposed, which synergistically integrates the denoising and\nrecognition modules. Specifically, a Conditional Generative Adversarial Network\non Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the\nnoise in the point cloud, and recover the underlying features for subsequent\nrecognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is\nthen adapted to recognize faces from the processed point cloud, which\nhierarchically links both the local point features and neighboring features of\nmultiple scales. The proposed method is validated on the Bosphorus dataset. It\nsignificantly improves the recognition accuracy under all noise settings, with\na maximum gain of 14.81%.", "AI": {"tldr": "The paper presents an end-to-end 3D face recognition system for noisy point clouds, combining a denoising module (cGAN-TOP) and a recognition module (LDGCNN), improving accuracy by up to 14.81%.", "motivation": "3D face recognition using point clouds is increasingly important but raw data often contains noise due to imperfect sensors, which affects recognition accuracy.", "method": "An end-to-end system synergistically integrating denoising and recognition modules: cGAN-TOP for denoising and LDGCNN for recognizing faces from processed point clouds.", "result": "Validated on the Bosphorus dataset, the method significantly improves recognition accuracy under all noise settings, with a maximum gain of 14.81%.", "conclusion": "The proposed approach effectively handles noise in 3D point clouds and enhances face recognition accuracy."}}
{"id": "2506.07976", "pdf": "https://arxiv.org/pdf/2506.07976", "abs": "https://arxiv.org/abs/2506.07976", "authors": ["Junhong Shen", "Hao Bai", "Lunjun Zhang", "Yifei Zhou", "Amrith Setlur", "Shengbang Tong", "Diego Caples", "Nan Jiang", "Tong Zhang", "Ameet Talwalkar", "Aviral Kumar"], "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.", "AI": {"tldr": "\u5728\u5f53\u524d\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u4e2d\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5TTI\uff08Test-Time Interaction\uff09\uff0c\u901a\u8fc7\u6269\u5c55\u4ea4\u4e92\u7ef4\u5ea6\u6765\u589e\u5f3a\u4ee3\u7406\u7684\u884c\u4e3a\u80fd\u529b\uff0c\u5982\u63a2\u7d22\u3001\u56de\u6eaf\u548c\u52a8\u6001\u91cd\u65b0\u89c4\u5212\u3002\u7814\u7a76\u663e\u793a\uff0c\u5373\u4f7f\u6ca1\u6709\u4efb\u4f55\u8bad\u7ec3\u7684\u63d0\u793a\u4ea4\u4e92\u6269\u5c55\u4e5f\u80fd\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u4e14TTI\u65b9\u6cd5\u5728WebVoyager\u548cWebArena\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u5f00\u653e\u6570\u636e\u7f51\u7edc\u4ee3\u7406\u6c34\u5e73\u3002", "motivation": "\u5f53\u524d\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u4f9d\u8d56\u4e8e\u751f\u6210\u957f\u63a8\u7406\u75d5\u8ff9\uff0c\u4f46\u5728\u9700\u8981\u4ea4\u4e92\u7684\u95ee\u9898\u4e2d\uff0c\u8fd9\u79cd\u8fc7\u7a0b\u65e0\u6cd5\u8ba9\u4ee3\u7406\u4ece\u73af\u5883\u4e2d\u83b7\u53d6\u65b0\u4fe1\u606f\u6216\u968f\u65f6\u95f4\u8c03\u6574\u884c\u4e3a\u3002", "method": "\u63d0\u51faTTI\uff08Test-Time Interaction\uff09\u65b9\u6cd5\uff0c\u8fd9\u662f\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u4ee3\u7406\u7684 rollout \u957f\u5ea6\u6765\u8bad\u7ec3\u4ee3\u7406\u3002", "result": "TTI\u65b9\u6cd5\u4f7f\u7528Gemma 3 12B\u6a21\u578b\uff0c\u5728WebVoyager\u548cWebArena\u57fa\u51c6\u4e0a\u4ea7\u751f\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u5f00\u653e\u6570\u636e\u7f51\u7edc\u4ee3\u7406\uff0c\u5e76\u4e14\u80fd\u591f\u4f7f\u4ee3\u7406\u81ea\u9002\u5e94\u5730\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "conclusion": "\u4ea4\u4e92\u6269\u5c55\u662f\u4e0e\u6bcf\u6b65\u8ba1\u7b97\u6269\u5c55\u4e92\u8865\u7684\u5f3a\u5927\u8f74\u7ebf\uff0c\u4e3a\u8bad\u7ec3\u81ea\u9002\u5e94\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2506.07980", "pdf": "https://arxiv.org/pdf/2506.07980", "abs": "https://arxiv.org/abs/2506.07980", "authors": ["Alberto Baz\u00e1n-Guill\u00e9n", "Carlos Beis-Penedo", "Diego Cajaraville-Aboy", "Pablo Barbecho-Bautista", "Rebeca P. D\u00edaz-Redondo", "Luis J. de la Cruz Llopis", "Ana Fern\u00e1ndez-Vilas", "M\u00f3nica Aguilar Igartua", "Manuel Fern\u00e1ndez-Veiga"], "title": "Realistic Urban Traffic Generator using Decentralized Federated Learning for the SUMO simulator", "categories": ["cs.LG"], "comment": "21 pages, 7 figures", "summary": "Realistic urban traffic simulation is essential for sustainable urban\nplanning and the development of intelligent transportation systems. However,\ngenerating high-fidelity, time-varying traffic profiles that accurately reflect\nreal-world conditions, especially in large-scale scenarios, remains a major\nchallenge. Existing methods often suffer from limitations in accuracy,\nscalability, or raise privacy concerns due to centralized data processing. This\nwork introduces DesRUTGe (Decentralized Realistic Urban Traffic Generator), a\nnovel framework that integrates Deep Reinforcement Learning (DRL) agents with\nthe SUMO simulator to generate realistic 24-hour traffic patterns. A key\ninnovation of DesRUTGe is its use of Decentralized Federated Learning (DFL),\nwherein each traffic detector and its corresponding urban zone function as an\nindependent learning node. These nodes train local DRL models using minimal\nhistorical data and collaboratively refine their performance by exchanging\nmodel parameters with selected peers (e.g., geographically adjacent zones),\nwithout requiring a central coordinator. Evaluated using real-world data from\nthe city of Barcelona, DesRUTGe outperforms standard SUMO-based tools such as\nRouteSampler, as well as other centralized learning approaches, by delivering\nmore accurate and privacy-preserving traffic pattern generation.", "AI": {"tldr": "DesRUTGe\u662f\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u548cSUMO\u6a21\u62df\u5668\u6765\u751f\u6210\u771f\u5b9e\u768424\u5c0f\u65f6\u4ea4\u901a\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u65f6\u53d8\u7684\u4ea4\u901a\u914d\u7f6e\u6587\u4ef6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u573a\u666f\u4e2d\uff0c\u9762\u4e34\u51c6\u786e\u6027\u3001\u53ef\u6269\u5c55\u6027\u6216\u9690\u79c1\u95ee\u9898\u7b49\u6311\u6218\u3002", "method": "\u5f15\u5165DesRUTGe\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e0eSUMO\u6a21\u62df\u5668\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\uff0c\u4f7f\u6bcf\u4e2a\u4ea4\u901a\u68c0\u6d4b\u5668\u53ca\u5176\u5bf9\u5e94\u7684\u57ce\u533a\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684\u5b66\u4e60\u8282\u70b9\u8fdb\u884c\u672c\u5730\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4e0e\u9009\u5b9a\u7684\u5bf9\u7b49\u8282\u70b9\u4ea4\u6362\u6a21\u578b\u53c2\u6570\u6765\u534f\u4f5c\u6539\u8fdb\u6027\u80fd\uff0c\u800c\u65e0\u9700\u4e2d\u592e\u534f\u8c03\u5668\u3002", "result": "\u4f7f\u7528\u5df4\u585e\u7f57\u90a3\u7684\u5b9e\u9645\u6570\u636e\u8bc4\u4f30\u8868\u660e\uff0cDesRUTGe\u5728\u751f\u6210\u66f4\u51c6\u786e\u4e14\u4fdd\u62a4\u9690\u79c1\u7684\u4ea4\u901a\u6a21\u5f0f\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8eSUMO\u7684\u6807\u51c6\u5de5\u5177\u548c\u5176\u4ed6\u96c6\u4e2d\u5f0f\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "DesRUTGe\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u4e2d\u592e\u534f\u8c03\u5668\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u51c6\u786e\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u4ea4\u901a\u6a21\u5f0f\uff0c\u4e3a\u53ef\u6301\u7eed\u7684\u57ce\u5e02\u89c4\u5212\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2506.06870", "pdf": "https://arxiv.org/pdf/2506.06870", "abs": "https://arxiv.org/abs/2506.06870", "authors": ["Bugra Kilictas", "Faruk Alpay"], "title": "Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks", "categories": ["cs.LO", "cs.AI", "03B70, 18M05, 68T50", "F.4.1; I.2.7"], "comment": "21 pages, no figures. Includes formal proofs, RDF/Turtle ontology\n  schema, {\\phi}-index disambiguation cases, and evaluation of\n  transformer-based AI models under semantic drift", "summary": "ISO 639:2023 unifies the ISO language-code family and introduces contextual\nmetadata, but it lacks a machine-native mechanism for handling dialectal drift\nand creole mixtures. We propose a formalisation of recursive semantic\nanchoring, attaching to every language entity $\\chi$ a family of fixed-point\noperators $\\phi_{n,m}$ that model bounded semantic drift via the relation\n$\\phi_{n,m}(\\chi) = \\chi \\oplus \\Delta(\\chi)$, where $\\Delta(\\chi)$ is a drift\nvector in a latent semantic manifold. The base anchor $\\phi_{0,0}$ recovers the\ncanonical ISO 639:2023 identity, whereas $\\phi_{99,9}$ marks the maximal drift\nstate that triggers a deterministic fallback. Using category theory, we treat\nthe operators $\\phi_{n,m}$ as morphisms and drift vectors as arrows in a\ncategory $\\mathrm{DriftLang}$. A functor $\\Phi: \\mathrm{DriftLang} \\to\n\\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves\nconvergence. We provide an RDF/Turtle schema (\\texttt{BaseLanguage},\n\\texttt{DriftedLanguage}, \\texttt{ResolvedAnchor}) and worked examples -- e.g.,\n$\\phi_{8,4}$ (Standard Mandarin) versus $\\phi_{8,7}$ (a colloquial variant),\nand $\\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with\ntransformer models show higher accuracy in language identification and\ntranslation on noisy or code-switched input when the $\\phi$-indices are used to\nguide fallback routing. The framework is compatible with ISO/TC 37 and provides\nan AI-tractable, drift-aware semantic layer for future standards.", "AI": {"tldr": "ISO 639:2023\u867d\u7136\u7edf\u4e00\u4e86\u8bed\u8a00\u7f16\u7801\u4f53\u7cfb\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u5143\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u5904\u7406\u65b9\u8a00\u6f02\u79fb\u548c\u6df7\u5408\u8bed\u7684\u673a\u5236\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9012\u5f52\u8bed\u4e49\u951a\u5b9a\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u8bed\u8a00\u8bc6\u522b\u548c\u7ffb\u8bd1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "ISO 639:2023\u6807\u51c6\u867d\u6709\u6539\u8fdb\uff0c\u4f46\u4ecd\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u8bed\u8a00\u5b66\u4e2d\u7684\u65b9\u8a00\u6f02\u79fb\u548c\u6df7\u5408\u8bed\u73b0\u8c61\uff0c\u8fd9\u4fc3\u4f7f\u4f5c\u8005\u5bfb\u627e\u4e00\u79cd\u673a\u5668\u53ef\u64cd\u4f5c\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9012\u5f52\u8bed\u4e49\u951a\u5b9a\u7684\u65b9\u6cd5\uff0c\u4e3a\u6bcf\u4e2a\u8bed\u8a00\u5b9e\u4f53\u03c7\u5b9a\u4e49\u4e86\u4e00\u7cfb\u5217\u56fa\u5b9a\u70b9\u7b97\u5b50\u03c6{n,m}\uff0c\u901a\u8fc7\u5173\u7cfb\u03c6{n,m}(\u03c7) = \u03c7 \u2295 \u0394(\u03c7)\u6a21\u62df\u6709\u9650\u8bed\u4e49\u6f02\u79fb\u3002\u4f7f\u7528\u8303\u7574\u8bba\u5c06\u8fd9\u4e9b\u7b97\u5b50\u89c6\u4e3a\u5f62\u6001\uff0c\u8bc1\u660e\u4e86\u6536\u655b\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2aRDF/Turtle\u6a21\u5f0f\u4ee5\u5b9e\u73b0\u5177\u4f53\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u542b\u566a\u58f0\u6216\u4ee3\u7801\u5207\u6362\u7684\u8f93\u5165\u4e0a\uff0c\u5229\u7528\u03c6-indices\u6307\u5bfc\u56de\u9000\u8def\u7531\u53ef\u4ee5\u63d0\u9ad8\u53d8\u538b\u5668\u6a21\u578b\u5728\u8bed\u8a00\u8bc6\u522b\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0eISO/TC 37\u517c\u5bb9\uff0c\u4e3a\u672a\u6765\u7684\u6807\u51c6\u63d0\u4f9b\u4e86\u4e00\u4e2aAI\u53ef\u5904\u7406\u3001\u6f02\u79fb\u611f\u77e5\u7684\u8bed\u4e49\u5c42\u3002"}}
{"id": "2506.07998", "pdf": "https://arxiv.org/pdf/2506.07998", "abs": "https://arxiv.org/abs/2506.07998", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "title": "Generative Modeling of Weights: Generalization or Memorization?", "categories": ["cs.LG", "cs.CV"], "comment": "Project page at https://boyazeng.github.io/weight_memorization", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "AI": {"tldr": "\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u6709\u6548\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u65b9\u9762\u7684\u63a2\u7d22\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5f53\u524d\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u96be\u4ee5\u8d85\u8d8a\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9700\u66f4\u8c28\u614e\u8bc4\u4f30\u5176\u5728\u65b0\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u8bc4\u4f30\u5176\u751f\u6210\u65b0\u9896\u6743\u91cd\u7684\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u5982\u4f55\u514b\u670d\u8bb0\u5fc6\u6548\u5e94\u7684\u9650\u5236\u3002", "method": "\u9009\u53d6\u56db\u79cd\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5206\u6790\u5b83\u4eec\u751f\u6210\u4e0d\u540c\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u9ad8\u8868\u73b0\u6a21\u578b\u6743\u91cd\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u4e0e\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u6dfb\u52a0\u566a\u58f0\u6216\u6743\u91cd\u96c6\u6210\uff09\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u7684\u8bb0\u5fc6\u7279\u6027\u53ca\u6539\u8fdb\u53ef\u80fd\u6027\u3002", "result": "\u53d1\u73b0\u8fd9\u4e9b\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u8bb0\u5fc6\u751f\u6210\u6743\u91cd\uff0c\u751f\u6210\u7ed3\u679c\u591a\u4e3a\u590d\u5236\u54c1\u6216\u7b80\u5355\u63d2\u503c\uff0c\u672a\u80fd\u663e\u8457\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\uff1b\u4fee\u6539\u5e38\u89c1\u5efa\u6a21\u56e0\u7d20\u6216\u5e94\u7528\u6570\u636e\u589e\u5f3a\u4e5f\u65e0\u6cd5\u6709\u6548\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002", "conclusion": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u65b0\u9896\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u4ed4\u7ec6\u5730\u8bc4\u4f30\u751f\u6210\u6a21\u578b\u5728\u65b0\u9886\u57df\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u660e\u786e\u5176\u9002\u7528\u7684\u6570\u636e\u7c7b\u578b\u3002"}}
{"id": "2506.06874", "pdf": "https://arxiv.org/pdf/2506.06874", "abs": "https://arxiv.org/abs/2506.06874", "authors": ["Ala Yankouskaya", "Areej B. Babiker", "Syeda W. F. Rizvi", "Sameha Alshakhsi", "Magnus Liebherr", "Raian Ali"], "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models", "categories": ["cs.HC", "cs.AI", "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods"], "comment": null, "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a\u65b0\u768412\u9879\u95ee\u5377\uff08LLM-D12\uff09\u4ee5\u6d4b\u91cf\u4eba\u4eec\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u63ed\u793a\u4e86\u5de5\u5177\u4f9d\u8d56\u548c\u5173\u7cfb\u4f9d\u8d56\u4e24\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u8bc4\u4f30\u4eba\u7c7b\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u7684\u5de5\u5177\u4e3b\u8981\u57fa\u4e8e\u7ecf\u5178\u7684\u884c\u4e3a\u6210\u763e\u75c7\u72b6\uff0c\u672a\u80fd\u5145\u5206\u6355\u6349LLM-\u4eba\u7c7b\u5173\u7cfb\u7684\u590d\u6742\u6027\u3002", "method": "\u7814\u7a76\u4eba\u5458\u57fa\u4e8e\u5148\u524d\u7684\u7406\u8bba\u5de5\u4f5c\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b12\u4e2a\u9879\u76ee\u7684\u95ee\u5377\uff08LLM-D12\uff09\uff0c\u5e76\u5728526\u540d\u82f1\u56fd\u53c2\u4e0e\u8005\u4e2d\u6536\u96c6\u4e86\u6570\u636e\u3002\u901a\u8fc7\u63a2\u7d22\u6027\u548c\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\uff0c\u4f7f\u7528\u6837\u672c\u5206\u5272\u65b9\u6cd5\uff0c\u786e\u5b9a\u4e86\u91cf\u8868\u7684\u4e24\u56e0\u5b50\u7ed3\u6784\uff1a\u5de5\u5177\u4f9d\u8d56\u548c\u5173\u7cfb\u4f9d\u8d56\u3002", "result": "LLM-D12\u91cf\u8868\u663e\u793a\u51fa\u6781\u597d\u7684\u5185\u90e8\u4e00\u81f4\u6027\u548c\u6e05\u6670\u7684\u533a\u5206\u6548\u5ea6\uff0c\u5916\u90e8\u9a8c\u8bc1\u4e5f\u786e\u8ba4\u4e86\u5176\u6982\u5ff5\u57fa\u7840\u548c\u4e24\u4e2a\u5b50\u91cf\u8868\u4e4b\u95f4\u7684\u533a\u522b\u3002", "conclusion": "LLM-D12\u4e3a\u7406\u89e3\u4eba\u7c7b\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u4e14\u7ec6\u81f4\u7684\u89c6\u89d2\uff0c\u5f3a\u8c03\u8fd9\u79cd\u4f9d\u8d56\u4e0d\u4e00\u5b9a\u662f\u529f\u80fd\u969c\u788d\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u53ef\u80fd\u53d8\u5f97\u6709\u95ee\u9898\u3002"}}
{"id": "2506.08001", "pdf": "https://arxiv.org/pdf/2506.08001", "abs": "https://arxiv.org/abs/2506.08001", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Sch\u00f6lkopf", "Weiyang Liu"], "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Technical report v1 (36 pages, 24 figures, project page:\n  https://spherelab.ai/poet-site/)", "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.", "AI": {"tldr": "POET is a new algorithm for training large language models using orthogonal transformations, improving optimization and generalization.", "motivation": "Effectively and reliably training large language models remains a significant challenge in the field of artificial intelligence.", "method": "POET reparameterizes each neuron with two learnable orthogonal matrices and a fixed random weight matrix, using Orthogonal Equivalence Transformation to optimize neurons.", "result": "Extensive experiments validate the effectiveness and scalability of POET in training LLMs.", "conclusion": "POET provides a stable way to optimize the objective function with improved generalization for large-scale neural networks."}}
{"id": "2506.06904", "pdf": "https://arxiv.org/pdf/2506.06904", "abs": "https://arxiv.org/abs/2506.06904", "authors": ["Yuhan Helena Liu", "Guangyu Robert Yang", "Christopher J. Cueva"], "title": "Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example", "categories": ["cs.NE", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Understanding how the brain learns may be informed by studying biologically\nplausible learning rules. These rules, often approximating gradient descent\nlearning to respect biological constraints such as locality, must meet two\ncritical criteria to be considered an appropriate brain model: (1) good\nneuroscience task performance and (2) alignment with neural recordings. While\nextensive research has assessed the first criterion, the second remains\nunderexamined. Employing methods such as Procrustes analysis on well-known\nneuroscience datasets, this study demonstrates the existence of a biologically\nplausible learning rule -- namely e-prop, which is based on gradient truncation\nand has demonstrated versatility across a wide range of tasks -- that can\nachieve neural data similarity comparable to Backpropagation Through Time\n(BPTT) when matched for task accuracy. Our findings also reveal that model\narchitecture and initial conditions can play a more significant role in\ndetermining neural similarity than the specific learning rule. Furthermore, we\nobserve that BPTT-trained models and their biologically plausible counterparts\nexhibit similar dynamical properties at comparable accuracies. These results\nunderscore the substantial progress made in developing biologically plausible\nlearning rules, highlighting their potential to achieve both competitive task\nperformance and neural data similarity.", "AI": {"tldr": "The study investigates biologically plausible learning rules, focusing on e-prop's capability to achieve neural data similarity akin to BPTT while meeting neuroscience task performance standards.", "motivation": "To identify a biologically plausible learning rule that aligns well with both neuroscience task performance and neural recordings.", "method": "Utilizing methods like Procrustes analysis on established neuroscience datasets to compare the performance of e-prop against BPTT in terms of neural data similarity and dynamical properties.", "result": "E-prop achieves neural data similarity comparable to BPTT when matched for task accuracy. Model architecture and initial conditions significantly influence neural similarity more than the specific learning rule. BPTT-trained models and their biologically plausible counterparts exhibit similar dynamical properties at equivalent accuracies.", "conclusion": "Significant advancements have been made in creating biologically plausible learning rules that can match competitive task performance and neural data similarity."}}
{"id": "2506.00654", "pdf": "https://arxiv.org/pdf/2506.00654", "abs": "https://arxiv.org/abs/2506.00654", "authors": ["Marco Di Gennaro", "Francesco Panebianco", "Marco Pianta", "Stefano Zanero", "Michele Carminati"], "title": "Amatriciana: Exploiting Temporal GNNs for Robust and Efficient Money Laundering Detection", "categories": ["cs.CR", "cs.LG"], "comment": null, "summary": "Money laundering is a financial crime that poses a serious threat to\nfinancial integrity and social security. The growing number of transactions\nmakes it necessary to use automatic tools that help law enforcement agencies\ndetect such criminal activity. In this work, we present Amatriciana, a novel\napproach based on Graph Neural Networks to detect money launderers inside a\ngraph of transactions by considering temporal information. Amatriciana uses the\nwhole graph of transactions without splitting it into several time-based\nsubgraphs, exploiting all relational information in the dataset. Our\nexperiments on a public dataset reveal that the model can learn from a limited\namount of data. Furthermore, when more data is available, the model outperforms\nother State-of-the-art approaches; in particular, Amatriciana decreases the\nnumber of False Positives (FPs) while detecting many launderers. In summary,\nAmatriciana achieves an F1 score of 0.76. In addition, it lowers the FPs by 55%\nwith respect to other State-of-the-art models.", "AI": {"tldr": "The paper presents Amatriciana, a new method using Graph Neural Networks to detect money laundering in transaction graphs by considering temporal information.", "motivation": "Money laundering is a serious financial crime that threatens financial integrity and social security. With the increasing number of transactions, automatic tools are needed to assist law enforcement agencies in detecting such criminal activities.", "method": "Amatriciana uses Graph Neural Networks to analyze the entire graph of transactions without splitting it into time-based subgraphs, thus exploiting all relational information in the dataset.", "result": "Experiments on a public dataset show that the model can learn from limited data and outperforms other state-of-the-art approaches when more data is available. It achieves an F1 score of 0.76 and reduces False Positives by 55%.", "conclusion": "Amatriciana is effective in detecting money launderers with fewer false positives compared to other methods."}}
{"id": "2506.06930", "pdf": "https://arxiv.org/pdf/2506.06930", "abs": "https://arxiv.org/abs/2506.06930", "authors": ["Alexander Spangher", "Tenghao Huang", "Jialiang Gu", "Jiatong Shi", "Muhao Chen"], "title": "DiscoSum: Discourse-aware News Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, 10 pages in Appendix", "summary": "Recent advances in text summarization have predominantly leveraged large\nlanguage models to generate concise summaries. However, language models often\ndo not maintain long-term discourse structure, especially in news articles,\nwhere organizational flow significantly influences reader engagement. We\nintroduce a novel approach to integrating discourse structure into\nsummarization processes, focusing specifically on news articles across various\nmedia. We present a novel summarization dataset where news articles are\nsummarized multiple times in different ways across different social media\nplatforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse\nschema to describe summarization structures and a novel algorithm, DiscoSum,\nwhich employs beam search technique for structure-aware summarization, enabling\nthe transformation of news stories to meet different stylistic and structural\ndemands. Both human and automatic evaluation results demonstrate the efficacy\nof our approach in maintaining narrative fidelity and meeting structural\nrequirements.", "AI": {"tldr": "\u8fd1\u671f\u6587\u672c\u6458\u8981\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7b80\u6d01\u7684\u6458\u8981\uff0c\u4f46\u8fd9\u4e9b\u6a21\u578b\u5f80\u5f80\u65e0\u6cd5\u7ef4\u6301\u957f\u7bc7\u8bba\u8ff0\u7ed3\u6784\uff0c\u5c24\u5176\u662f\u5728\u65b0\u95fb\u6587\u7ae0\u4e2d\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5c06\u8bba\u8ff0\u7ed3\u6784\u6574\u5408\u5230\u6458\u8981\u8fc7\u7a0b\u4e2d\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u65b0\u7684\u6458\u8981\u6570\u636e\u96c6\u3001\u65b0\u7684\u65b0\u95fb\u8bba\u8ff0\u6a21\u5f0f\u4ee5\u53ca\u65b0\u7684\u7b97\u6cd5DiscoSum\uff0c\u4ee5\u5b9e\u73b0\u7ed3\u6784\u611f\u77e5\u7684\u6458\u8981\u751f\u6210\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u53d9\u8ff0\u4fdd\u771f\u5ea6\u548c\u6ee1\u8db3\u7ed3\u6784\u8981\u6c42\u65b9\u9762\u975e\u5e38\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u6458\u8981\u6280\u672f\u867d\u7136\u80fd\u591f\u751f\u6210\u7b80\u6d01\u7684\u6458\u8981\uff0c\u4f46\u5728\u5904\u7406\u65b0\u95fb\u6587\u7ae0\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u5f88\u597d\u5730\u7ef4\u6301\u957f\u7bc7\u8bba\u8ff0\u7ed3\u6784\uff0c\u5f71\u54cd\u8bfb\u8005\u7684\u53c2\u4e0e\u5ea6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u66f4\u597d\u5730\u6574\u5408\u8bba\u8ff0\u7ed3\u6784\u7684\u6458\u8981\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6458\u8981\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u4e0d\u540c\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u4e0a\u5bf9\u65b0\u95fb\u6587\u7ae0\u7684\u4e0d\u540c\u65b9\u5f0f\u7684\u591a\u6b21\u6458\u8981\uff1b\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u65b0\u95fb\u8bba\u8ff0\u6a21\u5f0f\u6765\u63cf\u8ff0\u6458\u8981\u7ed3\u6784\uff1b\u8bbe\u8ba1\u4e86\u4e00\u79cd\u540d\u4e3aDiscoSum\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u91c7\u7528\u675f\u641c\u7d22\u6280\u672f\u8fdb\u884c\u7ed3\u6784\u611f\u77e5\u7684\u6458\u8981\u751f\u6210\u3002", "result": "\u901a\u8fc7\u4eba\u7c7b\u548c\u81ea\u52a8\u8bc4\u4f30\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u53d9\u8ff0\u4fdd\u771f\u5ea6\u548c\u6ee1\u8db3\u7ed3\u6784\u8981\u6c42\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06\u8bba\u8ff0\u7ed3\u6784\u6574\u5408\u5230\u6458\u8981\u8fc7\u7a0b\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6458\u8981\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u65b0\u95fb\u6587\u7ae0\u9886\u57df\u3002DiscoSum\u7b97\u6cd5\u4e3a\u9002\u5e94\u4e0d\u540c\u7684\u98ce\u683c\u548c\u7ed3\u6784\u9700\u6c42\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06305", "pdf": "https://arxiv.org/pdf/2506.06305", "abs": "https://arxiv.org/abs/2506.06305", "authors": ["No\u00e9mie Bergues", "Arthur Carr\u00e9", "Paul Join-Lambert", "Brice Hoffmann", "Arnaud Blondel", "Hamza Tajmouati"], "title": "Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Predicting the 3D conformation of small molecules within protein binding\nsites is a key challenge in drug design. When a crystallized reference ligand\n(template) is available, it provides geometric priors that can guide 3D pose\nprediction. We present a two-stage method for ligand conformation generation\nguided by such templates. In the first stage, we introduce a molecular\nalignment approach based on flow-matching to generate 3D coordinates for the\nligand, using the template structure as a reference. In the second stage, a\ndifferentiable pose optimization procedure refines this conformation based on\nshape and pharmacophore similarities, internal energy, and, optionally, the\nprotein binding pocket. We evaluate our approach on a new benchmark of ligand\npairs co-crystallized with the same target and show that it outperforms\nstandard docking tools and open-access alignment methods, especially in cases\ninvolving low similarity to the template or high ligand flexibility.", "AI": {"tldr": "This paper proposes a two-stage method for ligand conformation generation guided by template structures, which outperforms standard docking tools and open-access alignment methods.", "motivation": "Predicting the 3D conformation of small molecules within protein binding sites is crucial in drug design. A crystallized reference ligand (template) can provide geometric priors that guide 3D pose prediction.", "method": "The first stage uses a molecular alignment approach based on flow-matching to generate 3D coordinates for the ligand using the template structure as a reference. The second stage involves a differentiable pose optimization procedure refining this conformation based on shape and pharmacophore similarities, internal energy, and optionally, the protein binding pocket.", "result": "The approach was evaluated on a new benchmark of ligand pairs co-crystallized with the same target and showed superior performance compared to standard docking tools and open-access alignment methods, especially in cases involving low similarity to the template or high ligand flexibility.", "conclusion": "The proposed two-stage method for ligand conformation generation guided by templates shows promise in drug design by providing more accurate 3D pose predictions."}}
{"id": "2506.06944", "pdf": "https://arxiv.org/pdf/2506.06944", "abs": "https://arxiv.org/abs/2506.06944", "authors": ["Mellon M. Zhang", "Glen Chou", "Saibal Mukhopadhyay"], "title": "Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate and efficient object detection is essential for autonomous vehicles,\nwhere real-time perception requires low latency and high throughput. LiDAR\nsensors provide robust depth information, but conventional methods process full\n360{\\deg} scans in a single pass, introducing significant delay. Streaming\napproaches address this by sequentially processing partial scans in the native\npolar coordinate system, yet they rely on translation-invariant convolutions\nthat are misaligned with polar geometry -- resulting in degraded performance or\nrequiring complex distortion mitigation. Recent Mamba-based state space models\n(SSMs) have shown promise for LiDAR perception, but only in the full-scan\nsetting, relying on geometric serialization and positional embeddings that are\nmemory-intensive and ill-suited to streaming. We propose Polar Hierarchical\nMamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming\nLiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial\nencoding and a global forward Mamba for inter-sector temporal modeling,\nreplacing convolutions and positional encodings with distortion-aware,\ndimensionally-decomposed operations. PHiM sets a new state-of-the-art among\nstreaming detectors on the Waymo Open Dataset, outperforming the previous best\nby 10\\% and matching full-scan baselines at twice the throughput. Code will be\navailable at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .", "AI": {"tldr": "PHiM is a new SSM architecture for polar-coordinate streaming LiDAR that surpasses previous methods in performance and efficiency.", "motivation": "Conventional object detection methods for LiDAR data either process full 360\u00b0 scans, introducing significant delay, or rely on translation-invariant convolutions that are misaligned with polar geometry. Existing Mamba-based models work well for full-scan settings but are memory-intensive and not suitable for streaming data.", "method": "The proposed Polar Hierarchical Mamba (PHiM) uses local bidirectional Mamba blocks for intra-sector spatial encoding and a global forward Mamba for inter-sector temporal modeling. It replaces convolutions and positional encodings with distortion-aware, dimensionally-decomposed operations tailored to the polar coordinate system.", "result": "PHiM establishes a new state-of-the-art among streaming detectors on the Waymo Open Dataset, achieving a 10% improvement over the previous best method while matching the performance of full-scan baselines at twice the throughput.", "conclusion": "PHiM demonstrates superior performance and efficiency for polar-coordinate streaming LiDAR data, setting a new benchmark in the field."}}
{"id": "2506.06306", "pdf": "https://arxiv.org/pdf/2506.06306", "abs": "https://arxiv.org/abs/2506.06306", "authors": ["Ali Abedi", "Charlene H. Chu", "Shehroz S. Khan"], "title": "Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning", "categories": ["eess.SP", "cs.CV", "cs.HC", "cs.LG"], "comment": "16 pages, 4 figures, 2 tables", "summary": "Agitation is one of the most common responsive behaviors in people living\nwith dementia, particularly among those residing in community settings without\ncontinuous clinical supervision. Timely prediction of agitation can enable\nearly intervention, reduce caregiver burden, and improve the quality of life\nfor both patients and caregivers. This study aimed to develop and benchmark\nmachine learning approaches for the early prediction of agitation in\ncommunity-dwelling older adults with dementia using multimodal sensor data. A\nnew set of agitation-related contextual features derived from activity data was\nintroduced and employed for agitation prediction. A wide range of machine\nlearning and deep learning models was evaluated across multiple problem\nformulations, including binary classification for single-timestamp tabular\nsensor data and multi-timestamp sequential sensor data, as well as anomaly\ndetection for single-timestamp tabular sensor data. The study utilized the\nTechnology Integrated Health Management (TIHM) dataset, the largest publicly\navailable dataset for remote monitoring of people living with dementia,\ncomprising 2,803 days of in-home activity, physiology, and sleep data. The most\neffective setting involved binary classification of sensor data using the\ncurrent 6-hour timestamp to predict agitation at the subsequent timestamp.\nIncorporating additional information, such as time of day and agitation\nhistory, further improved model performance, with the highest AUC-ROC of 0.9720\nand AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work\npresents the first comprehensive benchmarking of state-of-the-art techniques\nfor agitation prediction in community-based dementia care using\nprivacy-preserving sensor data. The approach enables accurate, explainable, and\nefficient agitation prediction, supporting proactive dementia care and aging in\nplace.", "AI": {"tldr": "This paper aims to develop machine learning methods for early agitation prediction in dementia patients using multimodal sensor data. It introduces new agitation-related features and evaluates various models on the TIHM dataset, finding binary classification with LightGBM yields the best performance (AUC-ROC 0.9720). This work provides a benchmark for privacy-preserving agitation prediction.", "motivation": "Agitation is a common issue among dementia patients, especially those living without continuous clinical supervision. Early prediction of agitation can help reduce caregiver burden and improve quality of life for both patients and caregivers.", "method": "The study employed multimodal sensor data from the TIHM dataset, which includes in-home activity, physiology, and sleep data over 2,803 days. It introduced new contextual features related to agitation and evaluated multiple machine learning and deep learning models across different problem formulations such as binary classification, sequential data classification, and anomaly detection.", "result": "Binary classification using the current 6-hour timestamp to predict agitation at the next timestamp was found to be the most effective approach. Incorporating additional information like time of day and agitation history further improved model performance, achieving an AUC-ROC of 0.9720 and AUC-PR of 0.4320 with LightGBM.", "conclusion": "This study presents the first comprehensive benchmark for state-of-the-art techniques in agitation prediction for community-based dementia care. The results enable accurate, explainable, and efficient agitation prediction, supporting proactive dementia care and aging in place."}}
{"id": "2506.06946", "pdf": "https://arxiv.org/pdf/2506.06946", "abs": "https://arxiv.org/abs/2506.06946", "authors": ["Daniel Lawand", "Lucas Quaresma", "Roberto Bolgheroni", "Alfredo Goldman", "Renato Cordeiro Ferreira"], "title": "Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "comment": "9 pages, 3 figures (2 diagrams, 1 code listing), submitted to the\n  workshop SADIS 2025", "summary": "Deploying a Machine Learning (ML) training pipeline into production requires\nrobust software engineering practices. This differs significantly from\nexperimental workflows. This experience report investigates this challenge in\nSPIRA, a project whose goal is to create an ML-Enabled System (MLES) to\npre-diagnose insufficiency respiratory via speech analysis. The first version\nof SPIRA's training pipeline lacked critical software quality attributes. This\npaper presents an overview of the MLES, then compares three versions of the\narchitecture of the Continuous Training subsystem, which evolved from a Big\nBall of Mud, to a Modular Monolith, towards Microservices. By adopting\ndifferent design principles and patterns to enhance its maintainability,\nrobustness, and extensibility. In this way, the paper seeks to offer insights\nfor both ML Engineers tasked to productionize ML training pipelines and Data\nScientists seeking to adopt MLOps practices.", "AI": {"tldr": "This paper reports the evolution of the Continuous Training subsystem architecture in the SPIRA project, which aims to pre-diagnose respiratory insufficiency through speech analysis. The architecture evolved from a Big Ball of Mud to a Modular Monolith and then to Microservices, improving maintainability, robustness, and extensibility.", "motivation": "The motivation is to address the challenge of deploying ML training pipelines into production with robust software engineering practices, as opposed to experimental workflows. This is investigated within the context of the SPIRA project.", "method": "The method involves presenting an overview of the ML-Enabled System (MLES) for pre-diagnosing respiratory insufficiency via speech analysis and comparing three versions of the architecture of the Continuous Training subsystem: Big Ball of Mud, Modular Monolith, and Microservices.", "result": "The results indicate that adopting different design principles and patterns improved the maintainability, robustness, and extensibility of the system.", "conclusion": "The conclusion is that insights gained from this experience can benefit ML Engineers in productionizing ML training pipelines and Data Scientists adopting MLOps practices."}}
{"id": "2506.06308", "pdf": "https://arxiv.org/pdf/2506.06308", "abs": "https://arxiv.org/abs/2506.06308", "authors": ["Adoubi Vincent De Paul Adombi"], "title": "Scientific machine learning in Hydrology: a unified perspective", "categories": ["physics.comp-ph", "cs.LG", "physics.data-an"], "comment": null, "summary": "Scientific machine learning (SciML) provides a structured approach to\nintegrating physical knowledge into data-driven modeling, offering significant\npotential for advancing hydrological research. In recent years, multiple\nmethodological families have emerged, including physics-informed machine\nlearning, physics-guided machine learning, hybrid physics-machine learning, and\ndata-driven physics discovery. Within each of these families, a proliferation\nof heterogeneous approaches has developed independently, often without\nconceptual coordination. This fragmentation complicates the assessment of\nmethodological novelty and makes it difficult to identify where meaningful\nadvances can still be made in the absence of a unified conceptual framework.\nThis review, the first focused overview of SciML in hydrology, addresses these\nlimitations by proposing a unified methodological framework for each SciML\nfamily, bringing together representative contributions into a coherent\nstructure that fosters conceptual clarity and supports cumulative progress in\nhydrological modeling. Finally, we highlight the limitations and future\nopportunities of each unified family to guide systematic research in hydrology,\nwhere these methods remain underutilized.", "AI": {"tldr": "This paper provides the first focused overview of SciML in hydrology, proposing unified methodological frameworks for each SciML family to foster conceptual clarity and support cumulative progress.", "motivation": "To address the fragmentation in scientific machine learning approaches within hydrology, which complicates assessment of novelty and identification of areas for meaningful advances.", "method": "Proposing a unified methodological framework for each family of SciML (physics-informed, physics-guided, hybrid physics-machine learning, and data-driven physics discovery) to bring together representative contributions into a coherent structure.", "result": "Provides a clear and structured understanding of the various SciML families and their respective contributions to hydrological modeling.", "conclusion": "Highlights the limitations and future opportunities of each unified SciML family to guide systematic research in hydrology."}}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955", "abs": "https://arxiv.org/abs/2506.06955", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Hirokazu Kiyomaru", "Koichi Takeda", "Yusuke Miyao", "Maki Matsuda", "Yusuke Oda", "Pontus Stenetorp", "Qianying Liu", "Su Myat Noe", "Hideyuki Tachibana", "Kouta Nakayama", "Sadao Kurohashi"], "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.", "AI": {"tldr": "The paper introduces BIS Reasoning 1.0, a Japanese dataset for evaluating belief-inconsistent reasoning in large language models (LLMs). It benchmarks state-of-the-art models revealing significant variance in performance and identifying critical weaknesses when handling belief-conflicting inputs.", "motivation": "To evaluate the capability of large language models in belief-inconsistent reasoning, which is crucial for high-stakes domains such as law, healthcare, and scientific literature.", "method": "Developed a large-scale Japanese dataset named BIS Reasoning 1.0 consisting of syllogistic reasoning problems that are logically valid yet belief-inconsistent. Benchmarked state-of-the-art LLMs including GPT models, Claude models, and leading Japanese LLMs on this dataset.", "result": "Significant variance in performance among different LLMs was observed, with GPT-4o achieving the highest accuracy at 79.54%. Critical weaknesses were identified in current LLMs when dealing with logically valid but belief-conflicting inputs.", "conclusion": "Current LLMs have limitations in handling belief-inconsistent reasoning, which has important implications for their deployment in high-stakes domains where truth must override intuitive belief."}}
{"id": "2506.06309", "pdf": "https://arxiv.org/pdf/2506.06309", "abs": "https://arxiv.org/abs/2506.06309", "authors": ["Mohamed Kefi", "Tien Dat Pham", "Thin Nguyen", "Mark G. Tjoelker", "Viola Devasirvatham", "Kenichi Kashiwagi"], "title": "Leveraging Novel Ensemble Learning Techniques and Landsat Multispectral Data for Estimating Olive Yields in Tunisia", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Olive production is an important tree crop in Mediterranean climates.\nHowever, olive yield varies significantly due to climate change. Accurately\nestimating yield using remote sensing and machine learning remains a complex\nchallenge. In this study, we developed a streamlined pipeline for olive yield\nestimation in the Kairouan and Sousse governorates of Tunisia. We extracted\nfeatures from multispectral reflectance bands, vegetation indices derived from\nLandsat-8 OLI and Landsat-9 OLI-2 satellite imagery, along with digital\nelevation model data. These spatial features were combined with ground-based\nfield survey data to form a structured tabular dataset. We then developed an\nautomated ensemble learning framework, implemented using AutoGluon to train and\nevaluate multiple machine learning models, select optimal combinations through\nstacking, and generate robust yield predictions using five-fold\ncross-validation. The results demonstrate strong predictive performance from\nboth sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per\nha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha. This\nstudy highlights a scalable, cost-effective, and accurate method for olive\nyield estimation, with potential applicability across diverse agricultural\nregions globally.", "AI": {"tldr": "The paper presents a method for estimating olive yield using remote sensing data and machine learning models, achieving high accuracy with Landsat-8 and Landsat-9 satellite imagery.", "motivation": "Olive production is crucial in Mediterranean climates, but climate change causes significant yield variations. Accurate estimation of olive yield using remote sensing and machine learning remains complex.", "method": "The study developed a pipeline that extracts features from multispectral reflectance bands, vegetation indices from Landsat-8 OLI and Landsat-9 OLI-2 satellite imagery, and digital elevation model data. This was combined with ground-based field survey data to form a structured tabular dataset. An automated ensemble learning framework implemented using AutoGluon was used to train and evaluate multiple machine learning models, select optimal combinations through stacking, and generate robust yield predictions using five-fold cross-validation.", "result": "The results show strong predictive performance from both sensors, with Landsat-8 OLI achieving R2 = 0.8635 and RMSE = 1.17 tons per ha, and Landsat-9 OLI-2 achieving R2 = 0.8378 and RMSE = 1.32 tons per ha.", "conclusion": "This study provides a scalable, cost-effective, and accurate method for olive yield estimation, which could be applicable across diverse agricultural regions globally."}}
{"id": "2506.06958", "pdf": "https://arxiv.org/pdf/2506.06958", "abs": "https://arxiv.org/abs/2506.06958", "authors": ["Chance Jiajie Li", "Jiayi Wu", "Zhenze Mo", "Ao Qu", "Yuhan Tang", "Kaiya Ivy Zhao", "Yulu Gan", "Jie Fan", "Jiangbo Yu", "Jinhua Zhao", "Paul Liang", "Luis Alonso", "Kent Larson"], "title": "Position: Simulating Society Requires Simulating Thought", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": null, "summary": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior -- it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior -- primarily through prompting\nand supervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability -- making them unreliable for analyzing how\npeople reason, deliberate, or respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations.", "AI": {"tldr": "The paper introduces Generative Minds (GenMinds) as a new paradigm to enhance large language models' ability to simulate society by providing structured, revisable, and traceable reasoning. It also presents the RECAP framework for evaluating these agents.", "motivation": "Current LLM-based agents lack internal coherence, causal reasoning, and belief traceability when emulating individual and group behavior.", "method": "The authors propose Generative Minds (GenMinds), which uses cognitive science principles to support structured belief representations in generative agents. They also introduce the RECAP framework to evaluate these agents based on reasoning fidelity via causal traceability, demographic grounding, and intervention consistency.", "result": "These contributions aim to move from surface-level mimicry to generative agents that simulate thought rather than just language for social simulations.", "conclusion": "By adopting GenMinds and using the RECAP framework, there can be advancements in simulating society with LLMs through more reliable cognitively grounded reasoning."}}
{"id": "2506.06310", "pdf": "https://arxiv.org/pdf/2506.06310", "abs": "https://arxiv.org/abs/2506.06310", "authors": ["Xiaoyu Sun", "Yang Yang", "Xunde Dong"], "title": "Enhancing Contrastive Learning-based Electrocardiogram Pretrained Model with Patient Memory Queue", "categories": ["eess.SP", "cs.LG", "eess.IV"], "comment": "8 pages, 4 figures", "summary": "In the field of automatic Electrocardiogram (ECG) diagnosis, due to the\nrelatively limited amount of labeled data, how to build a robust ECG pretrained\nmodel based on unlabeled data is a key area of focus for researchers. Recent\nadvancements in contrastive learning-based ECG pretrained models highlight the\npotential of exploiting the additional patient-level self-supervisory signals\ninherent in ECG. They are referred to as patient contrastive learning. Its\nrationale is that multiple physical recordings from the same patient may share\ncommonalities, termed patient consistency, so redefining positive and negative\npairs in contrastive learning as intrapatient and inter-patient samples\nprovides more shared context to learn an effective representation. However,\nthese methods still fail to efficiently exploit patient consistency due to the\ninsufficient amount of intra-inter patient samples existing in a batch. Hence,\nwe propose a contrastive learning-based ECG pretrained model enhanced by the\nPatient Memory Queue (PMQ), which incorporates a large patient memory queue to\nmitigate model degeneration that can arise from insufficient intra-inter\npatient samples. In order to further enhance the performance of the pretrained\nmodel, we introduce two extra data augmentation methods to provide more\nperspectives of positive and negative pairs for pretraining. Extensive\nexperiments were conducted on three public datasets with three different data\nratios. The experimental results show that the comprehensive performance of our\nmethod outperforms previous contrastive learning methods and exhibits greater\nrobustness in scenarios with limited labeled data. The code is available at\nhttps://github.com/3hiuwoo/PMQ.", "AI": {"tldr": "The paper proposes a contrastive learning-based ECG pretrained model enhanced by Patient Memory Queue (PMQ) to better exploit patient consistency, and introduces two extra data augmentation methods. Experiments show that the method outperforms previous ones and is more robust with limited labeled data.", "motivation": "Due to the relatively limited amount of labeled data in automatic ECG diagnosis, researchers need to build a robust ECG pretrained model based on unlabeled data.", "method": "The proposed method incorporates a large patient memory queue (Patient Memory Queue - PMQ) to mitigate model degeneration from insufficient intra-inter patient samples. Two extra data augmentation methods are also introduced for more perspectives of positive and negative pairs during pretraining.", "result": "Extensive experiments conducted on three public datasets with different data ratios indicate that the comprehensive performance of the proposed method surpasses previous contrastive learning methods and shows greater robustness in scenarios with limited labeled data.", "conclusion": "The contrastive learning-based ECG pretrained model enhanced by PMQ successfully exploits patient consistency more effectively, leading to superior and more robust performance."}}
{"id": "2506.06311", "pdf": "https://arxiv.org/pdf/2506.06311", "abs": "https://arxiv.org/abs/2506.06311", "authors": ["Meiyan Kang", "Shizuo Kaji", "Sang-Yun Lee", "Taegon Kim", "Hee-Hwan Ryu", "Suyoung Choi"], "title": "A Novel Shape-Aware Topological Representation for GPR Data with DNN Integration", "categories": ["eess.SP", "cs.LG"], "comment": "15 pages, 6 figures", "summary": "Ground Penetrating Radar (GPR) is a widely used Non-Destructive Testing (NDT)\ntechnique for subsurface exploration, particularly in infrastructure inspection\nand maintenance. However, conventional interpretation methods are often limited\nby noise sensitivity and a lack of structural awareness. This study presents a\nnovel framework that enhances the detection of underground utilities,\nespecially pipelines, by integrating shape-aware topological features derived\nfrom B-scan GPR images using Topological Data Analysis (TDA), with the spatial\ndetection capabilities of the YOLOv5 deep neural network (DNN). We propose a\nnovel shape-aware topological representation that amplifies structural features\nin the input data, thereby improving the model's responsiveness to the\ngeometrical features of buried objects. To address the scarcity of annotated\nreal-world data, we employ a Sim2Real strategy that generates diverse and\nrealistic synthetic datasets, effectively bridging the gap between simulated\nand real-world domains. Experimental results demonstrate significant\nimprovements in mean Average Precision (mAP), validating the robustness and\nefficacy of our approach. This approach underscores the potential of\nTDA-enhanced learning in achieving reliable, real-time subsurface object\ndetection, with broad applications in urban planning, safety inspection, and\ninfrastructure management.", "AI": {"tldr": "This paper presents a novel framework combining Topological Data Analysis (TDA) with YOLOv5 for enhancing the detection of underground utilities from GPR images. It proposes a shape-aware topological representation and uses a Sim2Real strategy to generate synthetic datasets, showing significant improvements in detection accuracy.", "motivation": "Conventional methods for interpreting GPR data are limited by noise sensitivity and lack of structural awareness, motivating the development of a more robust detection system.", "method": "The method integrates TDA-derived shape-aware topological features from B-scan GPR images with the spatial detection capabilities of YOLOv5. A novel shape-aware topological representation is proposed to amplify structural features, and a Sim2Real strategy is used to create realistic synthetic datasets.", "result": "Experimental results show significant improvements in mean Average Precision (mAP), demonstrating the robustness and efficacy of the approach.", "conclusion": "This study highlights the potential of TDA-enhanced learning for reliable, real-time subsurface object detection, with applications in urban planning, safety inspection, and infrastructure management."}}
{"id": "2506.06315", "pdf": "https://arxiv.org/pdf/2506.06315", "abs": "https://arxiv.org/abs/2506.06315", "authors": ["Masoud Rahimi", "Reza Karbasi", "Abdol-Hossein Vahabie"], "title": "An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "5 pages, 5 figures", "summary": "We introduce an open-source Python framework for generating synthetic ECG\nimage datasets to advance critical deep learning-based tasks in ECG analysis,\nincluding ECG digitization, lead region and lead name detection, and\npixel-level waveform segmentation. Using the PTB-XL signal dataset, our\nproposed framework produces four open-access datasets: (1) ECG images in\nvarious lead configurations paired with time-series signals for ECG\ndigitization, (2) ECG images annotated with YOLO-format bounding boxes for\ndetection of lead region and lead name, (3)-(4) cropped single-lead images with\nsegmentation masks compatible with U-Net-based models in normal and overlapping\nversions. In the overlapping case, waveforms from neighboring leads are\nsuperimposed onto the target lead image, while the segmentation masks remain\nclean. The open-source Python framework and datasets are publicly available at\nhttps://github.com/rezakarbasi/ecg-image-and-signal-dataset and\nhttps://doi.org/10.5281/zenodo.15484519, respectively.", "AI": {"tldr": "An open-source Python framework is introduced for generating synthetic ECG image datasets, advancing deep learning-based tasks in ECG analysis such as digitization, lead detection, and waveform segmentation. Four open-access datasets are produced using the PTB-XL signal dataset.", "motivation": "To advance critical deep learning-based tasks in ECG analysis by providing a tool to generate synthetic ECG image datasets.", "method": "The method involves creating an open-source Python framework that uses the PTB-XL signal dataset to produce four different types of open-access datasets for various ECG analysis tasks.", "result": "Four open-access datasets were successfully generated: ECG images with time-series signals for digitization, images annotated with bounding boxes for lead detection, and cropped single-lead images with segmentation masks for waveform segmentation in normal and overlapping versions.", "conclusion": "An open-source Python framework was developed and used to create valuable datasets for advancing deep learning applications in ECG analysis."}}
{"id": "2506.06323", "pdf": "https://arxiv.org/pdf/2506.06323", "abs": "https://arxiv.org/abs/2506.06323", "authors": ["Abdullah Burkan Bereketoglu"], "title": "Composite Reward Design in PPO-Driven Adaptive Filtering", "categories": ["eess.SP", "cs.LG", "cs.SY", "eess.SY"], "comment": "5 pages, 9 figures, 1 table, , Keywords: Adaptive filtering,\n  reinforcement learning, PPO, noise reduction, signal denoising", "summary": "Model-free and reinforcement learning-based adaptive filtering methods are\ngaining traction for denoising in dynamic, non-stationary environments such as\nwireless signal channels. Traditional filters like LMS, RLS, Wiener, and Kalman\nare limited by assumptions of stationary or requiring complex fine-tuning or\nexact noise statistics or fixed models. This letter proposes an adaptive\nfiltering framework using Proximal Policy Optimization (PPO), guided by a\ncomposite reward that balances SNR improvement, MSE reduction, and residual\nsmoothness. Experiments on synthetic signals with various noise types show that\nour PPO agent generalizes beyond its training distribution, achieving real-time\nperformance and outperforming classical filters. This work demonstrates the\nviability of policy-gradient reinforcement learning for robust, low-latency\nadaptive signal filtering.", "AI": {"tldr": "This paper explores the use of Proximal Policy Optimization (PPO) in adaptive filtering for denoising signals in dynamic environments, showing better performance than traditional methods.", "motivation": "Traditional filters have limitations in dynamic, non-stationary environments due to assumptions of stationarity or the need for complex fine-tuning.", "method": "The paper proposes an adaptive filtering framework using PPO, guided by a composite reward balancing SNR improvement, MSE reduction, and residual smoothness.", "result": "Experiments demonstrate that the PPO agent generalizes beyond its training distribution, achieving real-time performance and outperforming classical filters.", "conclusion": "Policy-gradient reinforcement learning is viable for robust, low-latency adaptive signal filtering."}}
{"id": "2506.06998", "pdf": "https://arxiv.org/pdf/2506.06998", "abs": "https://arxiv.org/abs/2506.06998", "authors": ["Ming Li", "Zhengyuan Yang", "Xiyao Wang", "Dianqi Li", "Kevin Lin", "Tianyi Zhou", "Lijuan Wang"], "title": "What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models (LRMs) achieve strong reasoning performance by\nemitting long chains of thought. Yet, these verbose traces slow down inference\nand often drift into unnecessary detail, known as the overthinking phenomenon.\nTo better understand LRMs' behavior, we systematically analyze the token-level\nmisalignment between reasoning and non-reasoning models. While it is expected\nthat their primary difference lies in the stylistic \"thinking cues\", LRMs\nuniquely exhibit two pivotal, previously under-explored phenomena: a Global\nMisalignment Rebound, where their divergence from non-reasoning models persists\nor even grows as response length increases, and more critically, a Local\nMisalignment Diminish, where the misalignment concentrates at the \"thinking\ncues\" each sentence starts with but rapidly declines in the remaining of the\nsentence. Motivated by the Local Misalignment Diminish, we propose\nFoReaL-Decoding, a collaborative fast-slow thinking decoding method for\ncost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few\ntokens for each sentence, and then a weaker draft model completes the following\ntokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to\nsmoothly interpolate between the small and the large model. On four popular\nmath-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),\nFoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by\nup to 40%, while preserving 86 to 100% of model performance. These results\nestablish FoReaL-Decoding as a simple, plug-and-play route to controllable\ncost-quality trade-offs in reasoning-centric tasks.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u867d\u7136\u901a\u8fc7\u751f\u6210\u957f\u94fe\u6761\u7684\u601d\u8003\u8fc7\u7a0b\u53d6\u5f97\u4f18\u79c0\u7684\u63a8\u7406\u6548\u679c\uff0c\u4f46\u4e5f\u5b58\u5728\u5197\u957f\u548c\u8fc7\u5ea6\u601d\u8003\u7684\u95ee\u9898\u3002\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5728token\u7ea7\u522b\u4e0a\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4e24\u79cd\u672a\u88ab\u5145\u5206\u7814\u7a76\u7684\u73b0\u8c61\uff1a\u5168\u5c40\u9519\u4f4d\u53cd\u5f39\u548c\u5c40\u90e8\u9519\u4f4d\u51cf\u5f31\u3002\u57fa\u4e8e\u5c40\u90e8\u9519\u4f4d\u51cf\u5f31\u7684\u53d1\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u7801\u65b9\u6cd5FoReaL-Decoding\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u5feb\u6162\u601d\u7ef4\u6a21\u578b\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\u548c\u63a8\u7406\u957f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u7406\u89e3\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u884c\u4e3a\uff0c\u4f5c\u8005\u8bd5\u56fe\u7cfb\u7edf\u5730\u5206\u6790\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u5728token\u7ea7\u522b\u7684\u5dee\u5f02\uff0c\u5e76\u63a2\u7d22\u8fd9\u4e9b\u5dee\u5f02\u80cc\u540e\u7684\u5177\u4f53\u73b0\u8c61\u53ca\u53ef\u80fd\u7684\u5e94\u7528\u4ef7\u503c\u3002", "method": "1. \u7cfb\u7edf\u5206\u6790\u63a8\u7406\u6a21\u578b\u548c\u975e\u63a8\u7406\u6a21\u578b\u5728token\u7ea7\u522b\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4e24\u79cd\u5173\u952e\u73b0\u8c61\uff1a\n   - \u5168\u5c40\u9519\u4f4d\u53cd\u5f39\uff1a\u968f\u7740\u54cd\u5e94\u957f\u5ea6\u589e\u52a0\uff0cLRMs\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u7684\u5dee\u5f02\u6301\u7eed\u751a\u81f3\u6269\u5927\u3002\n   - \u5c40\u90e8\u9519\u4f4d\u51cf\u5f31\uff1a\u9519\u4f4d\u4e3b\u8981\u96c6\u4e2d\u5728\u53e5\u5b50\u5f00\u5934\u7684\u201c\u601d\u8003\u63d0\u793a\u201d\u90e8\u5206\uff0c\u968f\u540e\u8fc5\u901f\u51cf\u5f31\u3002\n2. \u57fa\u4e8e\u5c40\u90e8\u9519\u4f4d\u51cf\u5f31\uff0c\u63d0\u51faFoReaL-Decoding\u65b9\u6cd5\uff1a\n   - \u4f7f\u7528\u4e00\u4e2aLeading\u6a21\u578b\u751f\u6210\u6bcf\u4e2a\u53e5\u5b50\u7684\u524d\u51e0\u4e2atoken\u3002\n   - \u4f7f\u7528\u8f83\u5f31\u7684draft\u6a21\u578b\u5b8c\u6210\u5269\u4f59\u90e8\u5206\u3002\n   - \u91c7\u7528\u968f\u673a\u95e8\u673a\u5236\u5e73\u6ed1\u63d2\u503c\u5927\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u8f93\u51fa\u3002", "result": "\u5728\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08AIME24, GPQA-Diamond, MATH500, AMC23\uff09\u4e0a\uff0cFoReaL-Decoding\u65b9\u6cd5\uff1a\n- \u5c06\u7406\u8bbaFLOPs\u51cf\u5c1130%\u523050%\u3002\n- \u5c06\u94fe\u6761\u957f\u5ea6\u7f29\u77ed\u81f3\u591a40%\u3002\n- \u540c\u65f6\u4fdd\u630186%\u5230100%\u7684\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "FoReaL-Decoding\u4f5c\u4e3a\u4e00\u79cd\u7b80\u5355\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u4e3a\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6210\u672c-\u8d28\u91cf\u6743\u8861\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.06334", "pdf": "https://arxiv.org/pdf/2506.06334", "abs": "https://arxiv.org/abs/2506.06334", "authors": ["Alexandre Bouras", "Audrey Durand", "Richard Khoury"], "title": "Preference-based learning for news headline recommendation", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "This study explores strategies for optimizing news headline recommendations\nthrough preference-based learning. Using real-world data of user interactions\nwith French-language online news posts, we learn a headline recommender agent\nunder a contextual bandit setting. This allows us to explore the impact of\ntranslation on engagement predictions, as well as the benefits of different\ninteractive strategies on user engagement during data collection. Our results\nshow that explicit exploration may not be required in the presence of noisy\ncontexts, opening the door to simpler but efficient strategies in practice.", "AI": {"tldr": "This study explores strategies for optimizing news headline recommendations through preference-based learning, finding that explicit exploration may not be required in noisy contexts.", "motivation": "To improve news headline recommendations by understanding the impact of translation on engagement predictions and the benefits of different interactive strategies on user engagement.", "method": "Using real-world data of user interactions with French-language online news posts, a headline recommender agent is learned under a contextual bandit setting.", "result": "Explicit exploration may not be required in the presence of noisy contexts for efficient strategies.", "conclusion": "Simpler strategies can be efficient in practice when dealing with noisy contexts in news headline recommendations."}}
{"id": "2506.06342", "pdf": "https://arxiv.org/pdf/2506.06342", "abs": "https://arxiv.org/abs/2506.06342", "authors": ["Mohd Ashhad", "Sana Rahmani", "Mohammed Fayiz", "Ali Etemad", "Javad Hashemi"], "title": "Uncertainty-Aware Multi-view Arrhythmia Classification from ECG", "categories": ["eess.SP", "cs.LG"], "comment": "This paper has been accepted to IJCNN 2024 conference", "summary": "We propose a deep neural architecture that performs uncertainty-aware\nmulti-view classification of arrhythmia from ECG. Our method learns two\ndifferent views (1D and 2D) of single-lead ECG to capture different types of\ninformation. We use a fusion technique to reduce the conflict between the\ndifferent views caused by noise and artifacts in ECG data, thus incorporating\nuncertainty to obtain stronger final predictions. Our framework contains the\nfollowing three modules (1) a time-series module to learn the morphological\nfeatures from ECG; (2) an image-space learning module to learn the\nspatiotemporal features; and (3) the uncertainty-aware fusion module to fuse\nthe information from the two different views. Experimental results on two\nreal-world datasets demonstrate that our framework not only improves the\nperformance on arrhythmia classification compared to the state-of-the-art but\nalso shows better robustness to noise and artifacts present in ECG.", "AI": {"tldr": "The paper presents a deep neural architecture for uncertainty-aware multi-view classification of arrhythmia from ECG, which improves performance and robustness.", "motivation": "To address the challenge of classifying arrhythmia from ECG signals by leveraging multiple views and accounting for uncertainties due to noise and artifacts.", "method": "The method involves learning two different views (1D and 2D) of single-lead ECG, using a time-series module for morphological features, an image-space learning module for spatiotemporal features, and an uncertainty-aware fusion module to combine information from both views.", "result": "The experimental results on two real-world datasets show that the framework outperforms state-of-the-art methods in arrhythmia classification and demonstrates greater robustness to noise and artifacts in ECG data.", "conclusion": "The proposed deep neural architecture effectively performs uncertainty-aware multi-view classification of arrhythmia from ECG, leading to improved performance and robustness."}}
{"id": "2506.07006", "pdf": "https://arxiv.org/pdf/2506.07006", "abs": "https://arxiv.org/abs/2506.07006", "authors": ["Zechen Hu", "Tong Xu", "Xuesu Xiao", "Xuan Wang"], "title": "CARoL: Context-aware Adaptation for Robot Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is\noften inefficient. Leveraging prior knowledge has the potential to\nsignificantly enhance learning efficiency, which, however, raises two critical\nchallenges: how to determine the relevancy of existing knowledge and how to\nadaptively integrate them into learning a new task. In this paper, we propose\nContext-aware Adaptation for Robot Learning (CARoL), a novel framework to\nefficiently learn a similar but distinct new task from prior knowledge. CARoL\nincorporates context awareness by analyzing state transitions in system\ndynamics to identify similarities between the new task and prior knowledge. It\nthen utilizes these identified similarities to prioritize and adapt specific\nknowledge pieces for the new task. Additionally, CARoL has a broad\napplicability spanning policy-based, value-based, and actor-critic RL\nalgorithms. We validate the efficiency and generalizability of CARoL on both\nsimulated robotic platforms and physical ground vehicles. The simulations\ninclude CarRacing and LunarLander environments, where CARoL demonstrates faster\nconvergence and higher rewards when learning policies for new tasks. In\nreal-world experiments, we show that CARoL enables a ground vehicle to quickly\nand efficiently adapt policies learned in simulation to smoothly traverse\nreal-world off-road terrain.", "AI": {"tldr": "CARoL is a framework that uses context-aware adaptation to efficiently learn new robotic tasks from prior knowledge, demonstrating faster convergence and higher rewards in simulations and real-world experiments.", "motivation": "Reinforcement Learning (RL) can be inefficient when learning new robotic tasks from scratch, but leveraging prior knowledge could significantly enhance learning efficiency. The challenges lie in determining the relevancy of existing knowledge and adaptively integrating them into learning a new task.", "method": "The proposed Context-aware Adaptation for Robot Learning (CARoL) framework incorporates context awareness by analyzing state transitions in system dynamics to identify similarities between the new task and prior knowledge. It then prioritizes and adapts specific knowledge pieces for the new task. CARoL is applicable to policy-based, value-based, and actor-critic RL algorithms.", "result": "In simulations (CarRacing and LunarLander environments), CARoL shows faster convergence and higher rewards when learning policies for new tasks. In real-world experiments, it enables a ground vehicle to quickly and efficiently adapt policies learned in simulation to traverse real-world off-road terrain.", "conclusion": "CARoL offers an efficient method to leverage prior knowledge for learning similar but distinct new tasks in robotics, proving its efficiency and generalizability on both simulated robotic platforms and physical ground vehicles."}}
{"id": "2506.07008", "pdf": "https://arxiv.org/pdf/2506.07008", "abs": "https://arxiv.org/abs/2506.07008", "authors": ["Fatemeh Pourahmadian", "Yang Xu"], "title": "Deep regularization networks for inverse problems with noisy operators", "categories": ["math.NA", "cs.AI", "cs.NA", "eess.SP"], "comment": null, "summary": "A supervised learning approach is proposed for regularization of large\ninverse problems where the main operator is built from noisy data. This is\ngermane to superresolution imaging via the sampling indicators of the inverse\nscattering theory. We aim to accelerate the spatiotemporal regularization\nprocess for this class of inverse problems to enable real-time imaging. In this\napproach, a neural operator maps each pattern on the right-hand side of the\nscattering equation to its affiliated regularization parameter. The network is\ntrained in two steps which entails: (1) training on low-resolution\nregularization maps furnished by the Morozov discrepancy principle with\nnonoptimal thresholds, and (2) optimizing network predictions through\nminimization of the Tikhonov loss function regulated by the validation loss.\nStep 2 allows for tailoring of the approximate maps of Step 1 toward\nconstruction of higher quality images. This approach enables direct learning\nfrom test data and dispenses with the need for a-priori knowledge of the\noptimal regularization maps. The network, trained on low-resolution data,\nquickly generates dense regularization maps for high-resolution imaging. We\nhighlight the importance of the training loss function on the network's\ngeneralizability. In particular, we demonstrate that networks informed by the\nlogic of discrepancy principle lead to images of higher contrast. In this case,\nthe training process involves many-objective optimization. We propose a new\nmethod to adaptively select the appropriate loss weights during training\nwithout requiring an additional optimization process. The proposed approach is\nsynthetically examined for imaging damage evolution in an elastic plate. The\nresults indicate that the discrepancy-informed regularization networks not only\naccelerate the imaging process, but also remarkably enhance the image quality\nin complex environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6b63\u5219\u5316\u5927\u578b\u9006\u95ee\u9898\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u566a\u58f0\u6570\u636e\u7684\u6563\u5c04\u65b9\u7a0b\u3002\u901a\u8fc7\u4e24\u6b65\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u52a0\u901f\u65f6\u7a7a\u6b63\u5219\u5316\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6210\u50cf\uff0c\u5e76\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u5728\u8d85\u5206\u8fa8\u7387\u6210\u50cf\u4e2d\uff0c\u9006\u6563\u5c04\u7406\u8bba\u7684\u91c7\u6837\u6307\u6807\u6784\u5efa\u7684\u4e3b\u7b97\u5b50\u901a\u5e38\u6765\u81ea\u566a\u58f0\u6570\u636e\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u6b63\u5219\u5316\u8fc7\u7a0b\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u6210\u50cf\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\u5c06\u6563\u5c04\u65b9\u7a0b\u53f3\u4fa7\u7684\u6bcf\u4e2a\u6a21\u5f0f\u6620\u5c04\u5230\u5176\u5173\u8054\u7684\u6b63\u5219\u5316\u53c2\u6570\uff0c\u5206\u4e24\u6b65\u8bad\u7ec3\u7f51\u7edc\uff1a1\uff09\u57fa\u4e8eMorozov\u5dee\u5f02\u539f\u7406\u7684\u4f4e\u5206\u8fa8\u7387\u6b63\u5219\u5316\u56fe\u8fdb\u884c\u521d\u6b65\u8bad\u7ec3\uff1b2\uff09\u901a\u8fc7\u6700\u5c0f\u5316Tikhonov\u635f\u5931\u51fd\u6570\u4f18\u5316\u7f51\u7edc\u9884\u6d4b\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u81ea\u9002\u5e94\u9009\u62e9\u9002\u5f53\u7684\u635f\u5931\u6743\u91cd\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u52a0\u901f\u4e86\u6210\u50cf\u8fc7\u7a0b\uff0c\u8fd8\u663e\u8457\u63d0\u9ad8\u4e86\u590d\u6742\u73af\u5883\u4e0b\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5f39\u6027\u677f\u635f\u4f24\u6f14\u5316\u6210\u50cf\u4e2d\u7684\u5408\u6210\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5927\u578b\u9006\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5feb\u901f\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u6b63\u5219\u5316\u56fe\uff0c\u5e76\u63d0\u9ad8\u56fe\u50cf\u5bf9\u6bd4\u5ea6\u548c\u8d28\u91cf\u3002"}}
{"id": "2506.07016", "pdf": "https://arxiv.org/pdf/2506.07016", "abs": "https://arxiv.org/abs/2506.07016", "authors": ["Sanjoy Chowdhury", "Mohamed Elmoghany", "Yohan Abeysinghe", "Junjie Fei", "Sayan Nag", "Salman Khan", "Mohamed Elhoseiny", "Dinesh Manocha"], "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks", "categories": ["cs.CV", "cs.AI"], "comment": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage", "summary": "Large multimodal models (LMMs) have shown remarkable progress in audio-visual\nunderstanding, yet they struggle with real-world scenarios that require complex\nreasoning across extensive video collections. Existing benchmarks for video\nquestion answering remain limited in scope, typically involving one clip per\nquery, which falls short of representing the challenges of large-scale,\naudio-visual retrieval and reasoning encountered in practical applications. To\nbridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal\nis to identify salient segments across different videos in response to a query\nand link them together to generate the most informative answer. To this end, we\npresent AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA\npairs designed to assess the capabilities of LMMs in multi-video retrieval and\ntemporal grounding task. Additionally, we propose a model-agnostic, multi-agent\nframework MAGNET to address this challenge, achieving up to 89% and 65%\nrelative improvements over baseline methods on BLEU@4 and GPT evaluation scores\nin QA task on our proposed AVHaystacks. To enable robust evaluation of\nmulti-video retrieval and temporal grounding for optimal response generation,\nwe introduce two new metrics, STEM, which captures alignment errors between a\nground truth and a predicted step sequence and MTGS, to facilitate balanced and\ninterpretable evaluation of segment-level grounding performance. Project:\nhttps://schowdhury671.github.io/magnet_project/", "AI": {"tldr": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b(LMMs)\u5728\u89c6\u542c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u9700\u8981\u8de8\u5927\u91cf\u89c6\u9891\u8fdb\u884c\u590d\u6742\u63a8\u7406\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7684\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u6709\u9650\uff0c\u901a\u5e38\u6bcf\u4e2a\u67e5\u8be2\u6d89\u53ca\u4e00\u4e2a\u7247\u6bb5\uff0c\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u7684\u5927\u89c4\u6a21\u89c6\u542c\u68c0\u7d22\u548c\u63a8\u7406\u6311\u6218\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1AV-HaystacksQA\uff0c\u76ee\u6807\u662f\u6839\u636e\u67e5\u8be2\u8bc6\u522b\u4e0d\u540c\u89c6\u9891\u4e2d\u7684\u663e\u7740\u7247\u6bb5\u5e76\u5c06\u5b83\u4eec\u94fe\u63a5\u8d77\u6765\u4ee5\u751f\u6210\u6700 informative \u7684\u7b54\u6848\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86AVHaystacks\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b3100\u4e2a\u6ce8\u91ca\u7684QA\u5bf9\u7684\u89c6\u542c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LMMs\u5728\u591a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u95f4\u63a5\u5730\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u4e0d\u53ef\u77e5\u7684\u591a\u4ee3\u7406\u6846\u67b6MAGNET\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\uff0c\u5728\u6211\u4eec\u63d0\u51fa\u7684AVHaystacks\u4e0a\u7684QA\u4efb\u52a1\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5728BLEU@4\u548cGPT\u8bc4\u4f30\u5206\u6570\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8689%\u548c65%\u3002\u4e3a\u4e86\u5b9e\u73b0\u7a33\u5065\u7684\u591a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u95f4\u63a5\u5730\u8bc4\u4f30\uff0c\u4ee5\u751f\u6210\u6700\u4f73\u54cd\u5e94\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u6307\u6807STEM\u548cMTGS\uff0c\u4ee5\u4fc3\u8fdb\u5e73\u8861\u548c\u53ef\u89e3\u91ca\u7684\u6bb5\u7ea7\u63a5\u5730\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u89c6\u542c\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u9700\u8981\u8de8\u5927\u91cf\u89c6\u9891\u8fdb\u884c\u590d\u6742\u63a8\u7406\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u73b0\u6709\u7684\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u8303\u56f4\u6709\u9650\uff0c\u901a\u5e38\u6bcf\u4e2a\u67e5\u8be2\u6d89\u53ca\u4e00\u4e2a\u7247\u6bb5\uff0c\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u7684\u5927\u89c4\u6a21\u89c6\u542c\u68c0\u7d22\u548c\u63a8\u7406\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u66f4\u590d\u6742\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u4efb\u52a1\u548c\u57fa\u51c6\u6765\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u7684\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u4efb\u52a1AV-HaystacksQA\uff0c\u76ee\u6807\u662f\u6839\u636e\u67e5\u8be2\u8bc6\u522b\u4e0d\u540c\u89c6\u9891\u4e2d\u7684\u663e\u7740\u7247\u6bb5\u5e76\u5c06\u5b83\u4eec\u94fe\u63a5\u8d77\u6765\u4ee5\u751f\u6210\u6700 informative \u7684\u7b54\u6848\u30022. \u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u89c6\u542c\u57fa\u51c6AVHaystacks\uff0c\u5305\u542b3100\u4e2a\u6ce8\u91ca\u7684QA\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30LMMs\u5728\u591a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u95f4\u63a5\u5730\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u30023. \u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u4e0d\u53ef\u77e5\u7684\u591a\u4ee3\u7406\u6846\u67b6MAGNET\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e00\u6311\u6218\u30024. \u5f15\u5165\u4e86\u4e24\u4e2a\u65b0\u6307\u6807STEM\u548cMTGS\uff0c\u7528\u4e8e\u5e73\u8861\u548c\u53ef\u89e3\u91ca\u7684\u6bb5\u7ea7\u63a5\u5730\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5728\u63d0\u51fa\u7684AVHaystacks\u57fa\u51c6\u4e0a\u7684QA\u4efb\u52a1\u4e2d\uff0cMAGNET\u6846\u67b6\u76f8\u5bf9\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u5728BLEU@4\u548cGPT\u8bc4\u4f30\u5206\u6570\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8689%\u548c65%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002\u65b0\u63d0\u51fa\u7684STEM\u548cMTGS\u6307\u6807\u6709\u52a9\u4e8e\u66f4\u5e73\u8861\u548c\u53ef\u89e3\u91ca\u5730\u8bc4\u4f30\u6bb5\u7ea7\u63a5\u5730\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u63d0\u51faAV-HaystacksQA\u4efb\u52a1\u548cAVHaystacks\u57fa\u51c6\uff0c\u586b\u8865\u4e86\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u57fa\u51c6\u5728\u591a\u89c6\u9891\u68c0\u7d22\u548c\u65f6\u95f4\u63a5\u5730\u65b9\u9762\u7684\u7a7a\u767d\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u7684MAGNET\u6846\u67b6\u5728\u8be5\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u65b0\u5f15\u5165\u7684STEM\u548cMTGS\u6307\u6807\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5e73\u8861\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2506.06346", "pdf": "https://arxiv.org/pdf/2506.06346", "abs": "https://arxiv.org/abs/2506.06346", "authors": ["Wei Li", "Xiaochun Wu", "Xiaoxi Hu", "Yuxuan Zhang", "Sebastian Bader", "Yuhan Huang"], "title": "LD-RPMNet: Near-Sensor Diagnosis for Railway Point Machines", "categories": ["eess.SP", "cs.LG"], "comment": "This paper is accepted for IEEE Sensors Applcations Symposium (SAS)\n  2025", "summary": "Near-sensor diagnosis has become increasingly prevalent in industry. This\nstudy proposes a lightweight model named LD-RPMNet that integrates Transformers\nand Convolutional Neural Networks, leveraging both local and global feature\nextraction to optimize computational efficiency for a practical railway\napplication. The LD-RPMNet introduces a Multi-scale Depthwise Separable\nConvolution (MDSC) module, which decomposes cross-channel convolutions into\npointwise and depthwise convolutions while employing multi-scale kernels to\nenhance feature extraction. Meanwhile, a Broadcast Self-Attention (BSA)\nmechanism is incorporated to simplify complex matrix multiplications and\nimprove computational efficiency. Experimental results based on collected sound\nsignals during the operation of railway point machines demonstrate that the\noptimized model reduces parameter count and computational complexity by 50%\nwhile improving diagnostic accuracy by nearly 3%, ultimately achieving an\naccuracy of 98.86%. This demonstrates the possibility of near-sensor fault\ndiagnosis applications in railway point machines.", "AI": {"tldr": "This paper proposes LD-RPMNet, a lightweight model that combines Transformers and CNNs for railway fault diagnosis, achieving high accuracy with reduced parameters.", "motivation": "To develop an efficient near-sensor diagnosis system for railway applications, specifically for fault detection in railway point machines.", "method": "The LD-RPMNet model integrates Multi-scale Depthwise Separable Convolution (MDSC) and Broadcast Self-Attention (BSA) mechanisms to enhance feature extraction and reduce computational complexity.", "result": "The model reduces parameters and computational complexity by 50% while improving diagnostic accuracy by nearly 3%, reaching an accuracy of 98.86%.", "conclusion": "LD-RPMNet proves effective for near-sensor fault diagnosis in railway point machines, offering potential for practical applications."}}
{"id": "2506.07023", "pdf": "https://arxiv.org/pdf/2506.07023", "abs": "https://arxiv.org/abs/2506.07023", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 8 figures", "summary": "Segmentation of nuclei regions from histological images enables morphometric\nanalysis of nuclei structures, which in turn helps in the detection and\ndiagnosis of diseases under consideration. To develop a nuclei segmentation\nalgorithm, applicable to different types of target domain representations,\nimage-to-image translation networks can be considered as they are invariant to\ntarget domain image representations. One of the important issues with\nimage-to-image translation models is that they fail miserably when the\ninformation content between two image domains are asymmetric in nature. In this\nregard, the paper introduces a new deep generative model for segmenting nuclei\nstructures from histological images. The proposed model considers an embedding\nspace for handling information-disparity between information-rich histological\nimage space and information-poor segmentation map domain. Integrating\njudiciously the concepts of optimal transport and measure theory, the model\ndevelops an invertible generator, which provides an efficient optimization\nframework with lower network complexity. The concept of invertible generator\nautomatically eliminates the need of any explicit cycle-consistency loss. The\nproposed model also introduces a spatially-constrained squeeze operation within\nthe framework of invertible generator to maintain spatial continuity within the\nimage patches. The model provides a better trade-off between network complexity\nand model performance compared to other existing models having complex network\narchitectures. The performance of the proposed deep generative model, along\nwith a comparison with state-of-the-art nuclei segmentation methods, is\ndemonstrated on publicly available histological image data sets.", "AI": {"tldr": "The paper proposes a new deep generative model for nuclei segmentation from histological images, which uses an invertible generator and spatially-constrained squeeze operation to handle information disparity and maintain spatial continuity.", "motivation": "Existing image-to-image translation models struggle with asymmetric information content between domains, particularly in the context of nuclei segmentation from histological images.", "method": "The proposed model incorporates an embedding space to address information disparity, integrates optimal transport and measure theory for an efficient optimization framework, uses an invertible generator eliminating the need for cycle-consistency loss, and introduces a spatially-constrained squeeze operation to preserve spatial continuity.", "result": "The model demonstrates superior performance on publicly available histological image datasets compared to existing state-of-the-art nuclei segmentation methods, providing a better balance between network complexity and performance.", "conclusion": "The new deep generative model offers an effective solution for nuclei segmentation in histological images, overcoming limitations of previous approaches by addressing information disparity and maintaining spatial continuity."}}
{"id": "2506.06348", "pdf": "https://arxiv.org/pdf/2506.06348", "abs": "https://arxiv.org/abs/2506.06348", "authors": ["Vassiliki Mancoridis", "Brian Bue", "Jake H. Lee", "Andrew K. Thorpe", "Daniel Cusworth", "Alana Ayasse", "Philip G. Brodrick", "Riley Duren"], "title": "Multi-Platform Methane Plume Detection via Model and Domain Adaptation", "categories": ["eess.SP", "cs.LG"], "comment": "12 pages 8 figures. In review", "summary": "Prioritizing methane for near-term climate action is crucial due to its\nsignificant impact on global warming. Previous work used columnwise matched\nfilter products from the airborne AVIRIS-NG imaging spectrometer to detect\nmethane plume sources; convolutional neural networks (CNNs) discerned\nanthropogenic methane plumes from false positive enhancements. However, as an\nincreasing number of remote sensing platforms are used for methane plume\ndetection, there is a growing need to address cross-platform alignment. In this\nwork, we describe model- and data-driven machine learning approaches that\nleverage airborne observations to improve spaceborne methane plume detection,\nreconciling the distributional shifts inherent with performing the same task\nacross platforms. We develop a spaceborne methane plume classifier using data\nfrom the EMIT imaging spectroscopy mission. We refine classifiers trained on\nairborne imagery from AVIRIS-NG campaigns using transfer learning,\noutperforming the standalone spaceborne model. Finally, we use CycleGAN, an\nunsupervised image-to-image translation technique, to align the data\ndistributions between airborne and spaceborne contexts. Translating spaceborne\nEMIT data to the airborne AVIRIS-NG domain using CycleGAN and applying airborne\nclassifiers directly yields the best plume detection results. This methodology\nis useful not only for data simulation, but also for direct data alignment.\nThough demonstrated on the task of methane plume detection, our work more\nbroadly demonstrates a data-driven approach to align related products obtained\nfrom distinct remote sensing instruments.", "AI": {"tldr": "The paper highlights the importance of addressing methane emissions and presents machine learning methods to improve methane plume detection across different remote sensing platforms.", "motivation": "Methane is a significant contributor to global warming, and detecting methane plumes from various remote sensing platforms has become increasingly important. There's a need to reconcile differences in data distribution between airborne and spaceborne platforms.", "method": "The study uses model- and data-driven machine learning approaches including transfer learning and CycleGAN for image-to-image translation to align data distributions between airborne (AVIRIS-NG) and spaceborne (EMIT) platforms.", "result": "Transfer learning refined classifiers trained on airborne imagery outperformed standalone spaceborne models. Using CycleGAN to translate spaceborne data to the airborne domain and applying airborne classifiers yielded the best plume detection results.", "conclusion": "This methodology not only aids in methane plume detection but also provides a broader approach to align products from distinct remote sensing instruments."}}
{"id": "2506.07028", "pdf": "https://arxiv.org/pdf/2506.07028", "abs": "https://arxiv.org/abs/2506.07028", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 9 figures", "summary": "Segmentation of nuclei regions from histological images is an important task\nfor automated computer-aided analysis of histological images, particularly in\nthe presence of impermissible color variation in the color appearance of\nstained tissue images. While color normalization enables better nuclei\nsegmentation, accurate segmentation of nuclei structures makes color\nnormalization rather trivial. In this respect, the paper proposes a novel deep\ngenerative model for simultaneously segmenting nuclei structures and\nnormalizing color appearance of stained histological images.This model\njudiciously integrates the merits of truncated normal distribution and spatial\nattention. The model assumes that the latent color appearance information,\ncorresponding to a particular histological image, is independent of respective\nnuclei segmentation map as well as embedding map information. The disentangled\nrepresentation makes the model generalizable and adaptable as the modification\nor loss in color appearance information cannot be able to affect the nuclei\nsegmentation map as well as embedding information. Also, for dealing with the\nstain overlap of associated histochemical reagents, the prior for latent color\nappearance code is assumed to be a mixture of truncated normal distributions.\nThe proposed model incorporates the concept of spatial attention for\nsegmentation of nuclei regions from histological images. The performance of the\nproposed approach, along with a comparative analysis with related\nstate-of-the-art algorithms, has been demonstrated on publicly available\nstandard histological image data sets.", "AI": {"tldr": "A new deep generative model is proposed for simultaneous nuclei segmentation and color normalization in histological images, integrating truncated normal distribution and spatial attention.", "motivation": "Automated analysis of histological images requires accurate nuclei segmentation, but impermissible color variation in stained tissue images poses challenges. While color normalization can improve nuclei segmentation, it also relies on accurate segmentation.", "method": "The model integrates the merits of truncated normal distribution and spatial attention, assuming latent color appearance information is independent of nuclei segmentation map and embedding map information. It uses a mixture of truncated normal distributions as prior for latent color appearance code to handle stain overlap.", "result": "The performance of the approach has been demonstrated on publicly available standard histological image data sets, with a comparative analysis showing its effectiveness against state-of-the-art algorithms.", "conclusion": "The novel deep generative model enables simultaneous nuclei segmentation and color normalization, making it generalizable and adaptable to variations in color appearance without affecting segmentation accuracy."}}
{"id": "2506.06349", "pdf": "https://arxiv.org/pdf/2506.06349", "abs": "https://arxiv.org/abs/2506.06349", "authors": ["Thien Nhan Vo", "Thanh Xuan Truong"], "title": "Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": null, "summary": "This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.", "AI": {"tldr": "This study compares traditional machine learning and deep learning methods for classifying heartbeats from ECG signals. Hand-crafted features with LightGBM achieved the highest accuracy of 99% and F1 score of 0.94, outperforming image-based CNN approach.", "motivation": "To evaluate and compare the effectiveness of hand-crafted feature-based machine learning models and image-based deep learning models in classifying heartbeats from ECG signals.", "method": "The dataset was preprocessed through downsampling, filtering, and normalization. In the first approach, various classifiers such as SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and LightGBM were trained using extracted features like HRV, mean, variance, and RR intervals. The second approach involved transforming ECG signals into images using GAF, MTF, and RP, which were then classified using CNN architectures like VGG and Inception.", "result": "LightGBM model achieved the highest performance with an accuracy of 99% and an F1 score of 0.94. Image-based CNN approach had a lower F1 score of 0.85. Models like SVM and AdaBoost showed significantly lower scores.", "conclusion": "Hand-crafted features proved more effective in capturing temporal and morphological variations in ECG signals compared to image-based representations. Future work should consider multi-lead ECG signals and temporal dependencies across successive beats."}}
{"id": "2506.07035", "pdf": "https://arxiv.org/pdf/2506.07035", "abs": "https://arxiv.org/abs/2506.07035", "authors": ["Zixuan Jiang", "Renjing Xu"], "title": "AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Deciphering protein function remains a fundamental challenge in protein\nrepresentation learning. The task presents significant difficulties for protein\nlanguage models (PLMs) due to the sheer volume of functional annotation\ncategories and the highly imbalanced distribution of annotated instances across\nbiological ontologies. Inspired by the remarkable success of reinforcement\nlearning from human feedback (RLHF) in large language model (LLM) alignment, we\npropose AnnoDPO, a novel multi-modal framework for protein function prediction\nthat leverages Direct Preference Optimization (DPO) to enhance annotation\nlearning. Our methodology addresses the dual challenges of annotation scarcity\nand category imbalance through preference-aligned training objectives,\nestablishing a new paradigm for biological knowledge integration in protein\nrepresentation learning.", "AI": {"tldr": "Deciphering protein function is challenging in protein representation learning. Current protein language models (PLMs) face difficulties due to the large number of functional annotation categories and imbalanced data distribution. Drawing inspiration from reinforcement learning from human feedback (RLHF) used in large language models (LLMs), researchers propose AnnoDPO, a new multi-modal framework for protein function prediction that uses Direct Preference Optimization (DPO) to improve annotation learning. This approach addresses issues of limited annotations and category imbalance by using preference-aligned training objectives.", "motivation": "The motivation behind this paper is to address the challenges in deciphering protein function within protein representation learning, particularly focusing on overcoming the limitations posed by a high volume of functional annotation categories and an imbalanced distribution of annotated instances across biological ontologies.", "method": "The method proposed in this paper is called AnnoDPO, which is a novel multi-modal framework for protein function prediction. It leverages Direct Preference Optimization (DPO) to enhance annotation learning, aiming to tackle the dual challenges of annotation scarcity and category imbalance through preference-aligned training objectives.", "result": "The result of implementing AnnoDPO is the establishment of a new paradigm for integrating biological knowledge into protein representation learning, which effectively handles the issues of limited annotations and category imbalance.", "conclusion": "In conclusion, AnnoDPO presents a promising advancement in protein function prediction by utilizing DPO to overcome the challenges associated with annotation scarcity and category imbalance, thus contributing significantly to the field of protein representation learning."}}
{"id": "2506.06356", "pdf": "https://arxiv.org/pdf/2506.06356", "abs": "https://arxiv.org/abs/2506.06356", "authors": ["Yimin Du"], "title": "Deep Learning Enhanced Multi-Day Turnover Quantitative Trading Algorithm for Chinese A-Share Market", "categories": ["cs.CE", "cs.LG"], "comment": "10 pages", "summary": "This paper presents a sophisticated multi-day turnover quantitative trading\nalgorithm that integrates advanced deep learning techniques with comprehensive\ncross-sectional stock prediction for the Chinese A-share market. Our framework\ncombines five interconnected modules: initial stock selection through deep\ncross-sectional prediction networks, opening signal distribution analysis using\nmixture models for arbitrage identification, market capitalization and\nliquidity-based dynamic position sizing, grid-search optimized profit-taking\nand stop-loss mechanisms, and multi-granularity volatility-based market timing\nmodels. The algorithm employs a novel approach to balance capital efficiency\nwith risk management through adaptive holding periods and sophisticated\nentry/exit timing. Trained on comprehensive A-share data from 2010-2020 and\nrigorously backtested on 2021-2024 data, our method achieves remarkable\nperformance with 15.2\\% annualized returns, maximum drawdown constrained below\n5\\%, and a Sharpe ratio of 1.87. The strategy demonstrates exceptional\nscalability by maintaining 50-100 daily positions with a 9-day maximum holding\nperiod, incorporating dynamic profit-taking and stop-loss mechanisms that\nenhance capital turnover efficiency while preserving risk-adjusted returns. Our\napproach exhibits robust performance across various market regimes while\nmaintaining high capital capacity suitable for institutional deployment.", "AI": {"tldr": "This paper presents a multi-day turnover quantitative trading algorithm for the Chinese A-share market that integrates deep learning techniques with cross-sectional stock prediction, achieving 15.2% annualized returns, maximum drawdown below 5%, and a Sharpe ratio of 1.87.", "motivation": "To develop a sophisticated quantitative trading algorithm that combines deep learning techniques with comprehensive cross-sectional stock prediction to achieve high performance in the Chinese A-share market while maintaining risk control and capital efficiency.", "method": "The method includes five interconnected modules: initial stock selection through deep cross-sectional prediction networks, opening signal distribution analysis using mixture models for arbitrage identification, market capitalization and liquidity-based dynamic position sizing, grid-search optimized profit-taking and stop-loss mechanisms, and multi-granularity volatility-based market timing models.", "result": "The algorithm achieves 15.2% annualized returns, maximum drawdown constrained below 5%, and a Sharpe ratio of 1.87 when backtested on 2021-2024 data.", "conclusion": "The strategy demonstrates exceptional scalability, robustness across various market regimes, and high capital capacity suitable for institutional deployment."}}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044", "abs": "https://arxiv.org/abs/2506.07044", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...", "AI": {"tldr": "Multimodal Large Language Models (MLLMs) are effective in general domains but limited in medical applications due to knowledge coverage, hallucinations, and reasoning capabilities. This paper proposes a data curation procedure to build a multimodal dataset with extensive medical knowledge, introduces Lingshu - a medical-specialized MLLM undergoing multi-stage training, explores reinforcement learning for enhanced reasoning, develops MedEvalKit for model evaluation, and evaluates Lingshu's superior performance on medical tasks.", "motivation": "Existing MLLMs have limitations in medical applications such as insufficient medical knowledge beyond imaging, susceptibility to hallucinations due to suboptimal data curation, and lack of reasoning capabilities for complex medical scenarios.", "method": "The authors propose a comprehensive data curation process that acquires rich medical knowledge from diverse sources including imaging, texts, and general-domain data, synthesizes accurate medical captions, VQA, and reasoning samples, builds a multimodal dataset enriched with medical knowledge, introduces Lingshu which undergoes multi-stage training embedding medical expertise, explores reinforcement learning to enhance reasoning ability, and develops MedEvalKit for unified evaluation.", "result": "Lingshu outperforms existing open-source multimodal models on most medical tasks including multimodal QA, text-based QA, and medical report generation.", "conclusion": "The proposed medical-specialized MLLM Lingshu, built upon a carefully curated multimodal medical dataset and trained through multiple stages, demonstrates superior performance in various medical tasks, highlighting its potential in advancing medical AI applications."}}
{"id": "2506.06360", "pdf": "https://arxiv.org/pdf/2506.06360", "abs": "https://arxiv.org/abs/2506.06360", "authors": ["Jiyao Wang", "Suzan Ayas", "Jiahao Zhang", "Xiao Wen", "Dengbo He", "Birsen Donmez"], "title": "Towards Generalizable Drowsiness Monitoring with Physiological Sensors: A Preliminary Study", "categories": ["eess.SP", "cs.LG"], "comment": "Accepted by HFES2025", "summary": "Accurately detecting drowsiness is vital to driving safety. Among all\nmeasures, physiological-signal-based drowsiness monitoring can be more\nprivacy-preserving than a camera-based approach. However, conflicts exist\nregarding how physiological metrics are associated with different drowsiness\nlabels across datasets. Thus, we analyzed key features from electrocardiograms\n(ECG), electrodermal activity (EDA), and respiratory (RESP) signals across four\ndatasets, where different drowsiness inducers (such as fatigue and low arousal)\nand assessment methods (subjective vs. objective) were used. Binary logistic\nregression models were built to identify the physiological metrics that are\nassociated with drowsiness. Findings indicate that distinct different\ndrowsiness inducers can lead to different physiological responses, and\nobjective assessments were more sensitive than subjective ones in detecting\ndrowsiness. Further, the increased heart rate stability, reduced respiratory\namplitude, and decreased tonic EDA are robustly associated with increased\ndrowsiness. The results enhance understanding of drowsiness detection and can\ninform future generalizable monitoring designs.", "AI": {"tldr": "Analyzed key features from ECG,EDA,and RESP signals across four datasets to identify physiological metrics associated with drowsiness using binary logistic regression models.", "motivation": "Accurately detecting drowsiness is vital to driving safety and physiological-signal-based drowsiness monitoring can be more privacy-preserving than a camera-based approach.", "method": "Analyzed key features from electrocardiograms(ECG),electrodermal activity(EDA),and respiratory(RESP)signals across four datasets where different drowsiness inducers and assessment methods were used. Binary logistic regression models were built.", "result": "Findings indicate that distinct different drowsiness inducers can lead to different physiological responses,objective assessments were more sensitive than subjective ones in detecting drowsiness.The increased heart rate stability,reduced respiratory amplitude,and decreased tonic EDA are robustly associated with increased drowsiness.", "conclusion": "The results enhance understanding of drowsiness detection and can inform future generalizable monitoring designs."}}
{"id": "2506.07045", "pdf": "https://arxiv.org/pdf/2506.07045", "abs": "https://arxiv.org/abs/2506.07045", "authors": ["Yikun Ji", "Hong Yan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Qi Fan", "Liqing Zhang", "Jianfu Zhang"], "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.", "AI": {"tldr": "This paper explores the use of Multi-modal Large Language Models (MLLMs) for detecting AI-generated images with interpretable explanations, overcoming limitations like hallucination and misalignment with human reasoning. A dataset of AI-generated images is constructed, annotated with bounding boxes and captions highlighting synthesis artifacts. MLLMs are finetuned using a multi-stage optimization strategy to balance detection accuracy, visual localization, and coherent textual explanation. The resultant model outperforms baseline methods in detecting AI-generated images and localizing visual flaws.", "motivation": "The motivation behind this paper is the increasing demand for interpretable and robust detection methods for AI-generated images as image generation technologies advance rapidly. Existing approaches lack transparency and human-understandable justifications.", "method": "The method involves constructing a dataset of AI-generated images with annotations of bounding boxes and descriptive captions that emphasize synthesis artifacts. Then, MLLMs are finetuned using a multi-stage optimization strategy that balances accurate detection, visual localization, and coherent textual explanation.", "result": "The resulting model achieves superior performance in both detecting AI-generated images and localizing visual flaws, significantly surpassing baseline methods.", "conclusion": "In conclusion, by finetuning MLLMs with a specially constructed dataset and a multi-stage optimization strategy, the paper demonstrates a successful approach to detect AI-generated images with meaningful explanations, addressing issues such as hallucination and misalignment with human reasoning."}}
{"id": "2506.07060", "pdf": "https://arxiv.org/pdf/2506.07060", "abs": "https://arxiv.org/abs/2506.07060", "authors": ["Laura Cohen", "Xavier Hinaut", "Lilyana Petrova", "Alexandre Pitti", "Syd Reynal", "Ichiro Tsuda"], "title": "Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Natural intelligence (NI) consistently achieves more with less. Infants learn\nlanguage, develop abstract concepts, and acquire sensorimotor skills from\nsparse data, all within tight neural and energy limits. In contrast, today's AI\nrelies on virtually unlimited computational power, energy, and data to reach\nhigh performance. This paper argues that constraints in NI are paradoxically\ncatalysts for efficiency, adaptability, and creativity. We first show how\nlimited neural bandwidth promotes concise codes that still capture complex\npatterns. Spiking neurons, hierarchical structures, and symbolic-like\nrepresentations emerge naturally from bandwidth constraints, enabling robust\ngeneralization. Next, we discuss chaotic itinerancy, illustrating how the brain\ntransits among transient attractors to flexibly retrieve memories and manage\nuncertainty. We then highlight reservoir computing, where random projections\nfacilitate rapid generalization from small datasets. Drawing on developmental\nperspectives, we emphasize how intrinsic motivation, along with responsive\nsocial environments, drives infant language learning and discovery of meaning.\nSuch active, embodied processes are largely absent in current AI. Finally, we\nsuggest that adopting 'less is more' principles -- energy constraints,\nparsimonious architectures, and real-world interaction -- can foster the\nemergence of more efficient, interpretable, and biologically grounded\nartificial systems.", "AI": {"tldr": "\u81ea\u7136\u667a\u80fd\uff08NI\uff09\u901a\u8fc7\u6709\u9650\u7684\u795e\u7ecf\u548c\u80fd\u91cf\u9650\u5236\uff0c\u4ece\u7a00\u758f\u6570\u636e\u4e2d\u9ad8\u6548\u5b66\u4e60\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5f53\u524d\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u4f9d\u8d56\u51e0\u4e4e\u65e0\u9650\u7684\u8ba1\u7b97\u80fd\u529b\u3001\u80fd\u91cf\u548c\u6570\u636e\u6765\u8fbe\u5230\u9ad8\u6027\u80fd\u3002\u672c\u6587\u63a2\u8ba8\u4e86NI\u4e2d\u7684\u7ea6\u675f\u5982\u4f55\u6210\u4e3a\u6548\u7387\u3001\u9002\u5e94\u6027\u548c\u521b\u9020\u529b\u7684\u50ac\u5316\u5242\uff0c\u5e76\u63d0\u51fa\u4e86\u91c7\u7528'\u5c11\u5373\u662f\u591a'\u539f\u5219\u4ee5\u53d1\u5c55\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u751f\u7269\u57fa\u7840\u7684\u4eba\u5de5\u7cfb\u7edf\u3002", "motivation": "\u53d7\u5230\u81ea\u7136\u667a\u80fd\uff08\u5982\u5a74\u513f\u5b66\u4e60\u8bed\u8a00\u3001\u5f62\u6210\u62bd\u8c61\u6982\u5ff5\u548c\u83b7\u53d6\u611f\u77e5\u8fd0\u52a8\u6280\u80fd\uff09\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u4e86\u89e3\u5982\u4f55\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u4e0e\u6cdb\u5316\uff0c\u4ece\u800c\u6539\u8fdb\u5f53\u524d\u4f9d\u8d56\u5927\u91cf\u8ba1\u7b97\u3001\u80fd\u91cf\u548c\u6570\u636e\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5206\u6790\u6709\u9650\u795e\u7ecf\u5e26\u5bbd\u3001\u6df7\u6c8c\u6e38\u79fb\u3001\u50a8\u5b58\u8ba1\u7b97\u7b49\u673a\u5236\uff0c\u5c55\u793a\u4e86\u81ea\u7136\u667a\u80fd\u5982\u4f55\u5229\u7528\u7ea6\u675f\u6761\u4ef6\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\u548c\u6cdb\u5316\u3002\u540c\u65f6\uff0c\u7ed3\u5408\u53d1\u80b2\u89c6\u89d2\uff0c\u5f3a\u8c03\u5185\u5728\u52a8\u673a\u548c\u793e\u4f1a\u73af\u5883\u5bf9\u5a74\u513f\u5b66\u4e60\u7684\u91cd\u8981\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6709\u9650\u7684\u795e\u7ecf\u5e26\u5bbd\u4fc3\u8fdb\u4e86\u7b80\u6d01\u7684\u7f16\u7801\u65b9\u5f0f\uff0c\u968f\u673a\u6295\u5f71\u6709\u52a9\u4e8e\u4ece\u5c0f\u6570\u636e\u96c6\u4e2d\u5feb\u901f\u6cdb\u5316\uff0c\u800c\u6df7\u6c8c\u6e38\u79fb\u5219\u5e2e\u52a9\u5927\u8111\u7075\u6d3b\u68c0\u7d22\u8bb0\u5fc6\u548c\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u4e9b\u673a\u5236\u5171\u540c\u63a8\u52a8\u4e86\u81ea\u7136\u667a\u80fd\u7684\u9ad8\u6548\u6027\u3001\u9002\u5e94\u6027\u548c\u521b\u9020\u6027\u3002", "conclusion": "\u4f5c\u8005\u5efa\u8bae\u672a\u6765\u4eba\u5de5\u667a\u80fd\u5e94\u91c7\u7eb3'\u5c11\u5373\u662f\u591a'\u7684\u539f\u5219\uff0c\u5373\u901a\u8fc7\u80fd\u6e90\u7ea6\u675f\u3001\u7b80\u7ea6\u67b6\u6784\u548c\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\uff0c\u53d1\u5c55\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u57fa\u4e8e\u751f\u7269\u5b66\u539f\u7406\u7684\u7cfb\u7edf\u3002"}}
{"id": "2506.06363", "pdf": "https://arxiv.org/pdf/2506.06363", "abs": "https://arxiv.org/abs/2506.06363", "authors": ["Thang D. Pham", "Aditya Tanikanti", "Murat Ke\u00e7eli"], "title": "ChemGraph: An Agentic Framework for Computational Chemistry Workflows", "categories": ["physics.chem-ph", "cond-mat.mtrl-sci", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Atomistic simulations are essential tools in chemistry and materials science,\naccelerating the discovery of novel catalysts, energy storage materials, and\npharmaceuticals. However, running these simulations remains challenging due to\nthe wide range of computational methods, diverse software ecosystems, and the\nneed for expert knowledge and manual effort for the setup, execution, and\nvalidation stages. In this work, we present ChemGraph, an agentic framework\npowered by artificial intelligence and state-of-the-art simulation tools to\nstreamline and automate computational chemistry and materials science\nworkflows. ChemGraph leverages graph neural network-based foundation models for\naccurate yet computationally efficient calculations and large language models\n(LLMs) for natural language understanding, task planning, and scientific\nreasoning to provide an intuitive and interactive interface. Users can perform\ntasks such as molecular structure generation, single-point energy, geometry\noptimization, vibrational analysis, and thermochemistry calculations with\nmethods ranging from tight-binding and machine learning interatomic potentials\nto density functional theory or wave function theory-based methods. We evaluate\nChemGraph across 13 benchmark tasks and demonstrate that smaller LLMs\n(GPT-4o-mini, Claude-3.5-haiku, Qwen2.5-14B) perform well on simple workflows,\nwhile more complex tasks benefit from using larger models like GPT-4o.\nImportantly, we show that decomposing complex tasks into smaller subtasks\nthrough a multi-agent framework enables smaller LLM models to match or exceed\nGPT-4o's performance in specific scenarios.", "AI": {"tldr": "ChemGraph is an AI-powered framework that automates computational chemistry workflows using graph neural networks and language models. It simplifies tasks like molecular structure generation and energy calculations, with performance varying based on model size and task complexity.", "motivation": "To address the challenges in running atomistic simulations due to diverse computational methods, software ecosystems, and the need for expert knowledge.", "method": "ChemGraph uses graph neural network-based foundation models for efficient calculations and large language models for natural language understanding, task planning, and scientific reasoning.", "result": "Evaluated across 13 benchmark tasks, smaller LLMs perform well on simple workflows while larger models are better for complex tasks. Decomposing complex tasks into subtasks via a multi-agent framework allows smaller LLMs to match or exceed larger models' performance in specific scenarios.", "conclusion": "ChemGraph streamlines and automates computational chemistry workflows, making complex simulations more accessible."}}
{"id": "2506.07062", "pdf": "https://arxiv.org/pdf/2506.07062", "abs": "https://arxiv.org/abs/2506.07062", "authors": ["Dongryung Lee", "Sejune Joo", "Kimin Lee", "Beomjoon Kim"], "title": "Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search", "categories": ["cs.RO", "cs.AI"], "comment": "The International Journal of Robotics Research (IJRR)", "summary": "The problem of relocating a set of objects to designated areas amidst movable\nobstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)\nproblem, a subclass of task and motion planning (TAMP). Traditional approaches\nto G-TAMP have relied either on domain-independent heuristics or on learning\nfrom planning experience to guide the search, both of which typically demand\nsignificant computational resources or data. In contrast, humans often use\ncommon sense to intuitively decide which objects to manipulate in G-TAMP\nproblems. Inspired by this, we propose leveraging Large Language Models (LLMs),\nwhich have common sense knowledge acquired from internet-scale data, to guide\ntask planning in G-TAMP problems. To enable LLMs to perform geometric\nreasoning, we design a predicate-based prompt that encodes geometric\ninformation derived from a motion planning algorithm. We then query the LLM to\ngenerate a task plan, which is then used to search for a feasible set of\ncontinuous parameters. Since LLMs are prone to mistakes, instead of committing\nto LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action\nspace and use the LLM to guide the search. Unlike the previous approach that\ncalls an LLM at every node and incurs high computational costs, we use it to\nwarm-start the MCTS with the nodes explored in completing the LLM's task plan.\nOn six different G-TAMP problems, we show our method outperforms previous LLM\nplanners and pure search algorithms. Code can be found at:\nhttps://github.com/iMSquared/prime-the-search", "AI": {"tldr": "The paper proposes a method to solve G-TAMP problems by leveraging Large Language Models (LLMs) with geometric reasoning, using MCTS in a hybrid action space and warm-starting it with LLM's task plan. The approach outperforms previous LLM planners and pure search algorithms on six different G-TAMP problems.", "motivation": "To address the computational resource and data demands of traditional approaches to G-TAMP problems, the authors draw inspiration from human common sense knowledge to explore the use of LLMs for guiding task planning in these problems.", "method": "The method involves designing a predicate-based prompt for LLMs to perform geometric reasoning, querying the LLM to generate a task plan, and extending MCTS to a hybrid action space to utilize the LLM's output for guiding the search. The LLM is used to warm-start the MCTS with nodes explored in completing its task plan, reducing computational costs compared to calling the LLM at every node.", "result": "The proposed method outperforms previous LLM planners and pure search algorithms on six different G-TAMP problems.", "conclusion": "The integration of LLMs with geometric reasoning and MCTS in a hybrid action space provides an effective solution to G-TAMP problems, demonstrating superior performance over existing methods."}}
{"id": "2506.06373", "pdf": "https://arxiv.org/pdf/2506.06373", "abs": "https://arxiv.org/abs/2506.06373", "authors": ["Th\u00e9o Guyard", "C\u00e9dric Herzet", "Cl\u00e9ment Elvira"], "title": "El0ps: An Exact L0-regularized Problems Solver", "categories": ["cs.MS", "cs.LG", "math.OC"], "comment": null, "summary": "This paper presents El0ps, a Python toolbox providing several utilities to\nhandle L0-regularized problems related to applications in machine learning,\nstatistics, and signal processing, among other fields. In contrast to existing\ntoolboxes, El0ps allows users to define custom instances of these problems\nthrough a flexible framework, provides a dedicated solver achieving\nstate-of-the-art performance, and offers several built-in machine learning\npipelines. Our aim with El0ps is to provide a comprehensive tool which opens\nnew perspectives for the integration of L0-regularized problems in practical\napplications.", "AI": {"tldr": "This paper introduces El0ps, a Python toolbox for handling L0-regularized problems with custom instances, state-of-the-art solvers, and built-in machine learning pipelines.", "motivation": "To provide a comprehensive tool which opens new perspectives for the integration of L0-regularized problems in practical applications.", "method": "El0ps allows users to define custom instances of L0-regularized problems through a flexible framework, provides a dedicated solver achieving state-of-the-art performance, and offers several built-in machine learning pipelines.", "result": "Users can now handle L0-regularized problems more effectively in machine learning, statistics, and signal processing.", "conclusion": "El0ps is presented as a Python toolbox that advances the practical application of L0-regularized problems."}}
{"id": "2506.07064", "pdf": "https://arxiv.org/pdf/2506.07064", "abs": "https://arxiv.org/abs/2506.07064", "authors": ["Kai Xiong", "Xiao Ding", "Yixin Cao", "Yuxiong Yan", "Li Du", "Yufei Zhang", "Jinglong Gao", "Jiaqian Liu", "Bing Qin", "Ting Liu"], "title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) have mastered abundant simple and explicit\ncommonsense knowledge through pre-training, enabling them to achieve human-like\nperformance in simple commonsense reasoning. Nevertheless, LLMs struggle to\nreason with complex and implicit commonsense knowledge that is derived from\nsimple ones (such as understanding the long-term effects of certain events), an\naspect humans tend to focus on more. Existing works focus on complex tasks like\nmath and code, while complex commonsense reasoning remains underexplored due to\nits uncertainty and lack of structure. To fill this gap and align with\nreal-world concerns, we propose a benchmark Com$^2$ focusing on complex\ncommonsense reasoning. We first incorporate causal event graphs to serve as\nstructured complex commonsense. Then we adopt causal theory~(e.g.,\nintervention) to modify the causal event graphs and obtain different scenarios\nthat meet human concerns. Finally, an LLM is employed to synthesize examples\nwith slow thinking, which is guided by the logical relationships in the\nmodified causal graphs. Furthermore, we use detective stories to construct a\nmore challenging subset. Experiments show that LLMs struggle in reasoning depth\nand breadth, while post-training and slow thinking can alleviate this. The code\nand data are available at https://github.com/Waste-Wood/Com2.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7b80\u5355\u5e38\u8bc6\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u590d\u6742\u548c\u9690\u6027\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aCom$^2$\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u590d\u6742\u5e38\u8bc6\u63a8\u7406\u3002\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u4e8b\u4ef6\u56fe\u3001\u56e0\u679c\u7406\u8bba\u548c\u4fa6\u63a2\u6545\u4e8b\uff0cCom$^2$\u80fd\u591f\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u5173\u6ce8\u7684\u4e0d\u540c\u573a\u666f\uff0c\u5e76\u6307\u5bfcLLMs\u8fdb\u884c\u6df1\u5165\u63a8\u7406\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u540e\u8bad\u7ec3\u548c\u6162\u601d\u8003\u53ef\u4ee5\u6539\u5584LLMs\u5728\u63a8\u7406\u6df1\u5ea6\u548c\u5e7f\u5ea6\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u7b80\u5355\u5e38\u8bc6\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u590d\u6742\u548c\u9690\u6027\u5e38\u8bc6\u77e5\u8bc6\u65f6\u9047\u5230\u4e86\u56f0\u96be\uff0c\u4f8b\u5982\u7406\u89e3\u67d0\u4e9b\u4e8b\u4ef6\u7684\u957f\u671f\u5f71\u54cd\u3002\u8fd9\u662f\u4eba\u7c7b\u66f4\u503e\u5411\u4e8e\u5173\u6ce8\u7684\u65b9\u9762\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u590d\u6742\u5e38\u8bc6\u63a8\u7406\u7684\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u9996\u5148\uff0c\u5c06\u56e0\u679c\u4e8b\u4ef6\u56fe\u4f5c\u4e3a\u7ed3\u6784\u5316\u7684\u590d\u6742\u5e38\u8bc6\u77e5\u8bc6\u7eb3\u5165\uff1b\u7136\u540e\uff0c\u91c7\u7528\u56e0\u679c\u7406\u8bba\uff08\u5982\u5e72\u9884\uff09\u4fee\u6539\u56e0\u679c\u4e8b\u4ef6\u56fe\u4ee5\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u5173\u6ce8\u7684\u4e0d\u540c\u573a\u666f\uff1b\u6700\u540e\uff0c\u5229\u7528LLMs\u7ed3\u5408\u903b\u8f91\u5173\u7cfb\u5408\u6210\u793a\u4f8b\uff0c\u540c\u65f6\u4f7f\u7528\u4fa6\u63a2\u6545\u4e8b\u6784\u5efa\u66f4\u5177\u6311\u6218\u6027\u7684\u5b50\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLMs\u5728\u63a8\u7406\u6df1\u5ea6\u548c\u5e7f\u5ea6\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u540e\u8bad\u7ec3\u548c\u6162\u601d\u8003\u65b9\u6cd5\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684Com$^2$\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u590d\u6742\u5e38\u8bc6\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86LLMs\u5728\u590d\u6742\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u6539\u8fdb\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2506.06377", "pdf": "https://arxiv.org/pdf/2506.06377", "abs": "https://arxiv.org/abs/2506.06377", "authors": ["Giuseppe Arbia", "Luca Morandini", "Vincenzo Nardelli"], "title": "Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research", "categories": ["cs.CY", "cs.LG", "econ.EM", "stat.CO"], "comment": null, "summary": "This paper investigates Large Language Models (LLMs) ability to assess the\neconomic soundness and theoretical consistency of empirical findings in spatial\neconometrics. We created original and deliberately altered \"counterfactual\"\nsummaries from 28 published papers (2005-2024), which were evaluated by a\ndiverse set of LLMs. The LLMs provided qualitative assessments and structured\nbinary classifications on variable choice, coefficient plausibility, and\npublication suitability. The results indicate that while LLMs can expertly\nassess the coherence of variable choices (with top models like GPT-4o achieving\nan overall F1 score of 0.87), their performance varies significantly when\nevaluating deeper aspects such as coefficient plausibility and overall\npublication suitability. The results further revealed that the choice of LLM,\nthe specific characteristics of the paper and the interaction between these two\nfactors significantly influence the accuracy of the assessment, particularly\nfor nuanced judgments. These findings highlight LLMs' current strengths in\nassisting with initial, more surface-level checks and their limitations in\nperforming comprehensive, deep economic reasoning, suggesting a potential\nassistive role in peer review that still necessitates robust human oversight.", "AI": {"tldr": "This paper explores the ability of Large Language Models (LLMs) to evaluate the economic soundness and theoretical consistency in spatial econometrics. Using original and altered summaries from 28 papers, LLMs were tested on variable choice, coefficient plausibility, and publication suitability. Results show LLMs perform well on surface-level checks but struggle with deeper analysis.", "motivation": "To assess whether Large Language Models can effectively judge the economic soundness and theoretical consistency of empirical findings in spatial econometrics.", "method": "Created original and deliberately altered summaries from 28 published papers and evaluated them using a diverse set of LLMs, analyzing their qualitative assessments and structured binary classifications on variable choice, coefficient plausibility, and publication suitability.", "result": "LLMs performed well in assessing the coherence of variable choices (GPT-4o achieved an F1 score of 0.87), but showed significant variation in evaluating deeper aspects like coefficient plausibility and overall publication suitability. The accuracy of assessment was influenced by the LLM used, paper characteristics, and their interaction.", "conclusion": "LLMs are useful for initial, surface-level checks in peer review but are limited in deep economic reasoning, thus requiring human oversight."}}
{"id": "2506.07066", "pdf": "https://arxiv.org/pdf/2506.07066", "abs": "https://arxiv.org/abs/2506.07066", "authors": ["Li Jingyuan"], "title": "From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem", "categories": ["econ.TH", "cs.AI", "q-fin.CP"], "comment": null, "summary": "This paper presents a comprehensive formalization of the von\nNeumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive\ntheorem prover. We implement the classical axioms of preference-completeness,\ntransitivity, continuity, and independence-enabling machine-verified proofs of\nboth the existence and uniqueness of utility representations. Our formalization\ncaptures the mathematical structure of preference relations over lotteries,\nverifying that preferences satisfying the vNM axioms can be represented by\nexpected utility maximization.\n  Our contributions include a granular implementation of the independence\naxiom, formally verified proofs of fundamental claims about mixture lotteries,\nconstructive demonstrations of utility existence, and computational experiments\nvalidating the results. We prove equivalence to classical presentations while\noffering greater precision at decision boundaries.\n  This formalization provides a rigorous foundation for applications in\neconomic modeling, AI alignment, and management decision systems, bridging the\ngap between theoretical decision theory and computational implementation.", "AI": {"tldr": "This paper presents a comprehensive formalization of the von Neumann-Morgenstern (vNM) expected utility theorem using Lean 4, verifying preference relations over lotteries and providing rigorous foundations for applications in economic modeling, AI alignment, etc.", "motivation": "To provide a rigorous formalization of the vNM expected utility theorem that bridges theoretical decision theory and computational implementation.", "method": "Implementing classical axioms of preference in Lean 4 to enable machine-verified proofs of utility representations, including granular independence axiom, mixture lotteries claims, constructive demonstrations of utility existence, and computational experiments.", "result": "Formally verified proofs of fundamental claims about mixture lotteries, equivalence to classical presentations with greater precision at decision boundaries, and validated results through computational experiments.", "conclusion": "The formalization provides a rigorous foundation for applications in economic modeling, AI alignment, and management decision systems."}}
{"id": "2506.06378", "pdf": "https://arxiv.org/pdf/2506.06378", "abs": "https://arxiv.org/abs/2506.06378", "authors": ["Charalampos Tsirmpas", "Stasinos Konstantopoulos", "Dimitris Andrikopoulos", "Konstantina Kyriakouli", "Panagiotis Fatouros"], "title": "Transformer-Based Decomposition of Electrodermal Activity for Real-World Mental Health Applications", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Decomposing Electrodermal Activity (EDA) into phasic (short-term,\nstimulus-linked responses) and tonic (longer-term baseline) components is\nessential for extracting meaningful emotional and physiological biomarkers.\nThis study presents a comparative analysis of knowledge-driven, statistical,\nand deep learning-based methods for EDA signal decomposition, with a focus on\nin-the-wild data collected from wearable devices. In particular, the authors\nintroduce the Feel Transformer, a novel Transformer-based model adapted from\nthe Autoformer architecture, designed to separate phasic and tonic components\nwithout explicit supervision. The model leverages pooling and trend-removal\nmechanisms to enforce physiologically meaningful decompositions. Comparative\nexperiments against methods such as Ledalab, cvxEDA, and conventional\ndetrending show that the Feel Transformer achieves a balance between feature\nfidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy,\nreal-world data. The model demonstrates potential for real-time biosignal\nanalysis and future applications in stress prediction, digital mental health\ninterventions, and physiological forecasting.", "AI": {"tldr": "This paper presents a comparative analysis of different methods for EDA signal decomposition and introduces the Feel Transformer, which is effective for separating phasic and tonic components in real-world data.", "motivation": "To extract meaningful emotional and physiological biomarkers from Electrodermal Activity (EDA), it's necessary to decompose EDA into phasic and tonic components. Current methods have limitations, especially with in-the-wild data collected from wearable devices.", "method": "The authors compare knowledge-driven, statistical, and deep learning-based methods for EDA signal decomposition. They introduce the Feel Transformer, a novel model adapted from the Autoformer architecture that separates phasic and tonic components without explicit supervision by leveraging pooling and trend-removal mechanisms.", "result": "The Feel Transformer achieves a balance between feature fidelity (SCR frequency, amplitude, and tonic slope) and robustness to noisy, real-world data. It outperforms other methods like Ledalab, cvxEDA, and conventional detrending.", "conclusion": "The Feel Transformer shows potential for real-time biosignal analysis and could be applied in stress prediction, digital mental health interventions, and physiological forecasting."}}
{"id": "2506.07079", "pdf": "https://arxiv.org/pdf/2506.07079", "abs": "https://arxiv.org/abs/2506.07079", "authors": ["Mostafa Eslami", "Maryam Babazadeh"], "title": "On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "This paper presents an early investigation of Data-Assisted Control\n  (DAC) with reinforcement learning, showcasing its potential through a simple\n  example. Theoretical analysis is ongoing to establish formal support and\n  guarantees for the proposed approach", "summary": "This paper introduces a hypothetical hybrid control framework for\nport-Hamiltonian (p$\\mathcal{H}$) systems, employing a dynamic decomposition\nbased on Data-Assisted Control (DAC). The system's evolution is split into two\nparts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow\nhandling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a\ndissipative/input flow addressing both structural and parametric uncertainties.\nA virtual port variable $\\Pi$ serves as the interface between these two\ncomponents. A nonlinear controller manages the intrinsic Hamiltonian flow,\ndetermining a desired port control value $\\Pi_c$. Concurrently, Reinforcement\nLearning (RL) is applied to the dissipative/input flow to learn an agent for\nproviding optimal policy in mapping $\\Pi_c$ to the actual system input. This\nhybrid approach effectively manages RHS uncertainties while preserving the\nsystem's inherent structure. Key advantages include adjustable performance via\nLHS controller parameters, enhanced AI explainability and interpretability\nthrough the port variable $\\Pi$, the ability to guarantee safety and state\nattainability with hard/soft constraints, reduced complexity in learning\nhypothesis classes compared to end-to-end solutions, and improved\nstate/parameter estimation using LHS prior knowledge and system Hamiltonian to\naddress partial observability. The paper details the p$\\mathcal{H}$\nformulation, derives the decomposition, and presents the modular controller\narchitecture. Beyond design, crucial aspects of stability and robustness\nanalysis and synthesis are investigated, paving the way for deeper theoretical\ninvestigations. An application example, a pendulum with nonlinear dynamics, is\nsimulated to demonstrate the approach's empirical and phenomenological benefits\nfor future research.", "AI": {"tldr": "The paper proposes a hybrid control framework for port-Hamiltonian systems using Data-Assisted Control (DAC) to handle uncertainties and preserve system structure. It splits the system into intrinsic Hamiltonian flow and dissipative/input flow, managed by nonlinear control and Reinforcement Learning respectively.", "motivation": "To create a control framework that can manage both structural and parametric uncertainties in port-Hamiltonian systems while preserving their inherent structure.", "method": "A dynamic decomposition based on Data-Assisted Control (DAC) is employed to split the system's evolution into two parts: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow handling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a dissipative/input flow addressing both structural and parametric uncertainties. A virtual port variable serves as the interface between these components.", "result": "This approach effectively manages RHS uncertainties while preserving the system's inherent structure. Advantages include adjustable performance via LHS controller parameters, enhanced AI explainability and interpretability, the ability to guarantee safety and state attainability with constraints, reduced complexity in learning hypothesis classes, and improved state/parameter estimation.", "conclusion": "The proposed hybrid control framework provides a modular architecture for controlling port-Hamiltonian systems, offering significant advantages in managing uncertainties, enhancing explainability, ensuring safety, reducing learning complexity, and improving estimation. Stability and robustness analysis are also investigated, and an application example demonstrates its empirical benefits."}}
{"id": "2506.06389", "pdf": "https://arxiv.org/pdf/2506.06389", "abs": "https://arxiv.org/abs/2506.06389", "authors": ["Rifat Sadik", "Tanvir Rahman", "Arpan Bhattacharjee", "Bikash Chandra Halder", "Ismail Hossain"], "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning models have shown remarkable success in dermatological image\nanalysis, offering potential for automated skin disease diagnosis. Previously,\nconvolutional neural network(CNN) based architectures have achieved immense\npopularity and success in computer vision (CV) based task like skin image\nrecognition, generation and video analysis. But with the emergence of\ntransformer based models, CV tasks are now are nowadays carrying out using\nthese models. Vision Transformers (ViTs) is such a transformer-based models\nthat have shown success in computer vision. It uses self-attention mechanisms\nto achieve state-of-the-art performance across various tasks. However, their\nreliance on global attention mechanisms makes them susceptible to adversarial\nperturbations. This paper aims to investigate the susceptibility of ViTs for\nmedical images to adversarial watermarking-a method that adds so-called\nimperceptible perturbations in order to fool models. By generating adversarial\nwatermarks through Projected Gradient Descent (PGD), we examine the\ntransferability of such attacks to CNNs and analyze the performance defense\nmechanism -- adversarial training. Results indicate that while performance is\nnot compromised for clean images, ViTs certainly become much more vulnerable to\nadversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,\nadversarial training raises it up to 90.0%.", "AI": {"tldr": "In this paper, authors investigate the vulnerability of Vision Transformers (ViTs) to adversarial watermarking in medical images and explore defense mechanisms.", "motivation": "With the rise of transformer-based models like Vision Transformers (ViTs), which have shown success in computer vision tasks, there is a need to understand their susceptibility to adversarial attacks in medical image analysis.", "method": "The authors use Projected Gradient Descent (PGD) to generate adversarial watermarks and test the transferability of these attacks to CNNs. They also analyze the effectiveness of adversarial training as a defense mechanism.", "result": "Results show that ViTs are more vulnerable to adversarial attacks with an accuracy drop to 27.6%, but adversarial training can improve the accuracy up to 90.0%. Clean image performance remains unaffected.", "conclusion": "Vision Transformers are susceptible to adversarial watermarking attacks, but adversarial training provides an effective defense mechanism."}}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395", "abs": "https://arxiv.org/abs/2506.06395", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC\nimproves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on\nAMC23. RLSC offers a simple, scalable post-training method for reasoning models\nwith minimal supervision.", "AI": {"tldr": "In this paper, researchers developed a new method called Reinforcement Learning via Self-Confidence (RLSC) which enhances the performance of large language models in reasoning tasks without needing costly human annotations or external reward models. This method was tested on Qwen2.5-Math-7B and showed significant improvements on several math tests.", "motivation": "The motivation is to create an effective post-training method for large language models that can align their behavior with task goals without relying on costly human annotations or external reward models.", "method": "The proposed method, Reinforcement Learning via Self-Confidence (RLSC), uses the model's own confidence as reward signals for reinforcement learning, eliminating the need for labels, preference models, or reward engineering.", "result": "When applied to Qwen2.5-Math-7B, RLSC improved accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on AMC23.", "conclusion": "RLSC provides a simple, scalable post-training approach for reasoning models that requires minimal supervision."}}
{"id": "2506.07104", "pdf": "https://arxiv.org/pdf/2506.07104", "abs": "https://arxiv.org/abs/2506.07104", "authors": ["Jiaxuan Gao", "Shu Yan", "Qixin Tan", "Lu Yang", "Shusheng Xu", "Wei Fu", "Zhiyu Mei", "Kaifeng Lyu", "Yi Wu"], "title": "How Far Are We from Optimal Reasoning Efficiency?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.", "AI": {"tldr": "Large Reasoning Models (LRMs) are powerful but often inefficient. This paper introduces reasoning efficiency frontiers and a metric called Reasoning Efficiency Gap (REG) to quantify this inefficiency. It proposes REO-RL, a reinforcement learning method that reduces REG by optimizing token budgets, demonstrating significant improvements in efficiency with minimal accuracy loss.", "motivation": "To address the inefficiency of LRMs which leads to high inference costs and limits practical deployment, and to provide a unified way to measure and improve reasoning efficiency.", "method": "Introduced reasoning efficiency frontiers as empirical upper bounds from fine-tuning LRMs. Proposed REG as a metric for efficiency. Developed REO-RL, a reinforcement learning algorithm that minimizes REG by targeting specific token budgets and using numerical integration over selected budgets.", "result": "REO-RL consistently reduced REG by >=50% across all evaluated LRMs. It matched Qwen3-4B/8B efficiency frontiers under a 16K token budget with little accuracy loss. Ablation studies confirmed the effectiveness of the exponential token budget strategy.", "conclusion": "Fine-tuning LRMs to perfectly align with efficiency frontiers remains an open challenge despite significant improvements achieved by REO-RL."}}
{"id": "2506.07106", "pdf": "https://arxiv.org/pdf/2506.07106", "abs": "https://arxiv.org/abs/2506.07106", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "AI": {"tldr": "The paper presents Theorem-of-Thought (ToTh), a framework enhancing LLM reasoning through collaboration of three inference agents, resulting in more reliable and interpretable reasoning chains.", "motivation": "Large language models have strong performance in natural language reasoning tasks but suffer from brittleness and difficulty in interpretation. Existing techniques like Chain-of-Thought lack mechanisms to enforce logical structure and assess internal coherence.", "method": "ToTh introduces three parallel agents simulating abductive, deductive, and inductive inference modes. Each agent produces a reasoning trace structured into a formal reasoning graph. Bayesian belief propagation guided by natural language inference assigns confidence scores to each step. The most coherent graph is selected for the final answer.", "result": "Experiments on symbolic and numerical reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs. It provides interpretable and logically grounded reasoning chains.", "conclusion": "ToTh represents a promising direction for building more robust and cognitively inspired LLM reasoning."}}
{"id": "2506.07118", "pdf": "https://arxiv.org/pdf/2506.07118", "abs": "https://arxiv.org/abs/2506.07118", "authors": ["Yu-Xuan Wu", "Ziyan Huang", "Bin Hu", "Zhi-Hong Guan"], "title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "14 pages", "summary": "This article proposes a robust brain-inspired audio feature extractor\n(RBA-FE) model for depression diagnosis, using an improved hierarchical network\narchitecture. Most deep learning models achieve state-of-the-art performance\nfor image-based diagnostic tasks, ignoring the counterpart audio features. In\norder to tailor the noise challenge, RBA-FE leverages six acoustic features\nextracted from the raw audio, capturing both spatial characteristics and\ntemporal dependencies. This hybrid attribute helps alleviate the precision\nlimitation in audio feature extraction within other learning models like deep\nresidual shrinkage networks. To deal with the noise issues, our model\nincorporates an improved spiking neuron model, called adaptive rate smooth\nleaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of\n``retuning of cellular signal selectivity\" in the brain attention systems,\nwhich enhances the model robustness against environmental noises in audio data.\nExperimental results demonstrate that RBA-FE achieves state-of-the-art accuracy\non the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in\nprecision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014\nand DAIC-WOZ datasets both show enhancements in noise robustness. It is further\nindicated by comparison that the ARSLIF neuron model suggest the abnormal\nfiring pattern within the feature extraction on depressive audio data, offering\nbrain-inspired interpretability.", "AI": {"tldr": "This paper presents a robust brain-inspired audio feature extractor (RBA-FE) model using an improved hierarchical network architecture for depression diagnosis, which incorporates an adaptive rate smooth leaky integrate-and-fire (ARSLIF) model to handle noise issues. Experimental results on MODMA, AVEC2014, and DAIC-WOZ datasets show state-of-the-art accuracy and enhanced noise robustness.", "motivation": "The motivation of this paper is to address the limitation in existing deep learning models that focus on image-based diagnostic tasks while ignoring audio features, particularly in the context of depression diagnosis. The authors aim to develop a model that can effectively extract audio features while handling the challenge of noise in real-world data.", "method": "The proposed method involves developing a robust brain-inspired audio feature extractor (RBA-FE) model with an improved hierarchical network architecture. This model leverages six acoustic features extracted from raw audio data to capture spatial characteristics and temporal dependencies. To deal with noise, the RBA-FE incorporates an adaptive rate smooth leaky integrate-and-fire (ARSLIF) model, which emulates the 'retuning of cellular signal selectivity' mechanism in brain attention systems.", "result": "The RBA-FE model achieves state-of-the-art accuracy on the MODMA dataset with precision, accuracy, recall, and F1 score values of 0.8750, 0.8974, 0.8750, and 0.8750 respectively. Experiments on the AVEC2014 and DAIC-WOZ datasets also demonstrate enhanced noise robustness. Additionally, the ARSLIF neuron model indicates abnormal firing patterns within feature extraction on depressive audio data, providing brain-inspired interpretability.", "conclusion": "The authors conclude that the RBA-FE model successfully addresses the challenges of noise in audio data for depression diagnosis, achieving high accuracy and robustness. The incorporation of the ARSLIF model not only enhances performance but also offers interpretability by identifying abnormal firing patterns in depressive audio data."}}
{"id": "2506.06410", "pdf": "https://arxiv.org/pdf/2506.06410", "abs": "https://arxiv.org/abs/2506.06410", "authors": ["Gabriel Nova", "Sander van Cranenburgh", "Stephane Hess"], "title": "Improving choice model specification using reinforcement learning", "categories": ["econ.GN", "cs.LG", "q-fin.EC"], "comment": "13 pages, 7 figures", "summary": "Discrete choice modelling is a theory-driven modelling framework for\nunderstanding and forecasting choice behaviour. To obtain behavioural insights,\nmodellers test several competing model specifications in their attempts to\ndiscover the 'true' data generation process. This trial-and-error process\nrequires expertise, is time-consuming, and relies on subjective theoretical\nassumptions. Although metaheuristics have been proposed to assist choice\nmodellers, they treat model specification as a classic optimisation problem,\nrelying on static strategies, applying predefined rules, and neglecting\noutcomes from previous estimated models. As a result, current metaheuristics\nstruggle to prioritise promising search regions, adapt exploration dynamically,\nand transfer knowledge to other modelling tasks. To address these limitations,\nwe introduce a deep reinforcement learning-based framework where an 'agent'\nspecifies models by estimating them and receiving rewards based on\ngoodness-of-fit and parsimony. Results demonstrate the agent dynamically adapts\nits strategies to identify promising specifications across data generation\nprocesses, showing robustness and potential transferability, without prior\ndomain knowledge.", "AI": {"tldr": "An AI model using deep reinforcement learning is developed to automate and improve the process of discrete choice modelling, showing robustness and potential transferability.", "motivation": "Discrete choice modelling is crucial for understanding and forecasting choice behaviour, but current methods are time-consuming, require expertise, and rely on subjective theoretical assumptions. Metaheuristics proposed to assist modellers have limitations such as treating model specification as a classic optimisation problem with static strategies.", "method": "A deep reinforcement learning-based framework is introduced where an 'agent' specifies models by estimating them and receiving rewards based on goodness-of-fit and parsimony. The agent dynamically adapts its strategies without prior domain knowledge.", "result": "The results show that the agent can dynamically adapt its strategies to identify promising specifications across data generation processes, demonstrating robustness and potential transferability.", "conclusion": "This new framework addresses the limitations of current metaheuristics in discrete choice modelling, providing a more efficient and effective approach."}}
{"id": "2506.07122", "pdf": "https://arxiv.org/pdf/2506.07122", "abs": "https://arxiv.org/abs/2506.07122", "authors": ["Prakriti Tripathi", "Theertha Biju", "Maniram Thota", "Rakesh Lingam"], "title": "Image segmentation and classification of E-waste for waste segregation", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "4 pages, 7 figures. For code and link to dataset, see\n  https://github.com/prakriti16/Image-segmentation-and-classification-of-e-waste", "summary": "Industry partners provided a problem statement that involves classifying\nelectronic waste using machine learning models that will be used by\npick-and-place robots for waste segregation. We started by taking common\nelectronic waste items, such as a mouse and charger, unsoldering them, and\ntaking pictures to create a custom dataset. Then state-of-the art YOLOv11 model\nwas trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also\ntrained and achieved 41 mAP. The model will be further integrated with\npick-and-place robots to perform segregation of e-waste.", "AI": {"tldr": "The paper addresses the problem of classifying electronic waste using machine learning models for use by pick-and-place robots. A custom dataset was created, and both YOLOv11 and Mask-RCNN models were trained, achieving 70 mAP and 41 mAP respectively.", "motivation": "To develop a machine learning model that can classify electronic waste to be used by pick-and-place robots for waste segregation.", "method": "Creating a custom dataset by taking pictures of unsoldered common electronic waste items, then training YOLOv11 and Mask-RCNN models on this dataset.", "result": "YOLOv11 achieved 70 mAP in real-time, while Mask-RCNN achieved 41 mAP.", "conclusion": "The machine learning models will be integrated with pick-and-place robots for e-waste segregation."}}
{"id": "2506.07126", "pdf": "https://arxiv.org/pdf/2506.07126", "abs": "https://arxiv.org/abs/2506.07126", "authors": ["Weihan Lu", "Hong Cai Chen"], "title": "MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 12 figures, 2 tables", "summary": "Design rule checking (DRC) is of great significance for cost reduction and\ndesign efficiency improvement in integrated circuit (IC) designs.\nMachine-learning-based DRC has become an important approach in computer-aided\ndesign (CAD). In this paper, we propose MAGNet, a hybrid deep learning model\nthat integrates an improved U-Net with a graph neural network for DRC violation\nprediction. The U-Net backbone is enhanced with a Dynamic Attention Module\n(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability\nin extracting fine-grained and multi-scale spatial features. In parallel, we\nconstruct a pixel-aligned graph structure based on chip layout tiles, and apply\na specialized GNN to model the topological relationships among pins. During\ngraph construction, a graph-to-grid mapping is generated to align GNN features\nwith the layout image. In addition, a label amplification strategy is adopted\nduring training to enhance the model's sensitivity to sparse violation\npatterns. Overall, MAGNet effectively combines spatial, semantic, and\nstructural information, achieving improved prediction accuracy and reduced\nfalse positive rates in DRC hotspot detection. Subsequently, through\nincremental training, we achieve a more sensitive discrimination ability for\nhotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,\nand J-Net, MAGnet significantly outperforms these models, achieving substantial\nimprovements in overall performance.", "AI": {"tldr": "MAGNet is a hybrid deep learning model combining improved U-Net and GNN for DRC violation prediction, enhancing feature extraction with DAM and MSCM, utilizing pixel-aligned graph structure, adopting label amplification strategy, achieving better accuracy and reduced false positive rates in DRC hotspot detection compared to ibUnet, RouteNet, and J-Net.", "motivation": "Design rule checking (DRC) is crucial for cost reduction and design efficiency improvement in integrated circuit (IC) designs. Existing methods have limitations that this research aims to overcome by leveraging machine learning.", "method": "Propose MAGNet, integrating an enhanced U-Net with DAM and MSCM for spatial feature extraction, constructing a pixel-aligned graph structure with specialized GNN for topological relationships modeling, using graph-to-grid mapping to align features, and applying label amplification strategy during training.", "result": "MAGNet shows improved prediction accuracy and reduced false positive rates in DRC hotspot detection. It outperforms ibUnet, RouteNet, and J-Net in overall performance, with incremental training further improving discrimination ability for hotspots.", "conclusion": "MAGNet effectively combines spatial, semantic, and structural information for DRC violation prediction, demonstrating superior performance compared to existing models."}}
{"id": "2506.07127", "pdf": "https://arxiv.org/pdf/2506.07127", "abs": "https://arxiv.org/abs/2506.07127", "authors": ["Wenke xia", "Yichu Yang", "Hongtao Wu", "Xiao Ma", "Tao Kong", "Di Hu"], "title": "Robotic Policy Learning via Human-assisted Action Preference Optimization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Establishing a reliable and iteratively refined robotic system is essential\nfor deploying real-world applications. While Vision-Language-Action (VLA)\nmodels are widely recognized as the foundation model for such robotic\ndeployment, their dependence on expert demonstrations hinders the crucial\ncapabilities of correction and learning from failures. To mitigate this\nlimitation, we introduce a Human-assisted Action Preference Optimization method\nnamed HAPO, designed to correct deployment failures and foster effective\nadaptation through preference alignment for VLA models. This method begins with\na human-robot collaboration framework for reliable failure correction and\ninteraction trajectory collection through human intervention. These\nhuman-intervention trajectories are further employed within the action\npreference optimization process, facilitating VLA models to mitigate failure\naction occurrences while enhancing corrective action adaptation. Specifically,\nwe propose an adaptive reweighting algorithm to address the issues of\nirreversible interactions and token probability mismatch when introducing\npreference optimization into VLA models, facilitating model learning from\nbinary desirability signals derived from interactions. Through combining these\nmodules, our human-assisted action preference optimization method ensures\nreliable deployment and effective learning from failure for VLA models. The\nexperiments conducted in simulation and real-world scenarios prove superior\ngeneralization and robustness of our framework across a variety of manipulation\ntasks.", "AI": {"tldr": "The paper presents HAPO, a Human-assisted Action Preference Optimization method for Vision-Language-Action (VLA) models in robotics. HAPO improves failure correction and adaptation through human intervention and preference alignment, utilizing an adaptive reweighting algorithm. Experiments show enhanced generalization and robustness in various tasks.", "motivation": "Current VLA models used in robotic systems rely heavily on expert demonstrations, which limits their ability to learn from and correct failures effectively.", "method": "HAPO introduces a human-robot collaboration framework that collects interaction trajectories via human intervention to correct failures. An adaptive reweighting algorithm is proposed to address challenges in incorporating preference optimization into VLA models, allowing learning from binary desirability signals.", "result": "Experiments conducted both in simulation and real-world settings demonstrate the superior generalization and robustness of the HAPO framework across different manipulation tasks.", "conclusion": "HAPO ensures reliable deployment and effective learning from failures for VLA models by integrating human-assisted action preference optimization."}}
{"id": "2506.07135", "pdf": "https://arxiv.org/pdf/2506.07135", "abs": "https://arxiv.org/abs/2506.07135", "authors": ["Jos\u00e9 Manuel Su\u00e1rez", "Lu\u00eds Mariano Bibb\u00f3", "Joaqu\u00edn Bogado", "Alejandro Fernandez"], "title": "Taxonomy of migration scenarios for Qiskit refactoring using LLMs", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "Accepted for publication in ASQC JAIIO 54\n  (https://54jaiio.sadio.org.ar/simposios/)", "summary": "As quantum computing advances, quantum programming libraries' heterogeneity\nand steady evolution create new challenges for software developers. Frequent\nupdates in software libraries break working code that needs to be refactored,\nthus adding complexity to an already complex landscape. These refactoring\nchallenges are, in many cases, fundamentally different from those known in\nclassical software engineering due to the nature of quantum computing software.\nThis study addresses these challenges by developing a taxonomy of quantum\ncircuit's refactoring problems, providing a structured framework to analyze and\ncompare different refactoring approaches. Large Language Models (LLMs) have\nproven valuable tools for classic software development, yet their value in\nquantum software engineering remains unexplored. This study uses LLMs to\ncategorize refactoring needs in migration scenarios between different Qiskit\nversions. Qiskit documentation and release notes were scrutinized to create an\ninitial taxonomy of refactoring required for migrating between Qiskit releases.\nTwo taxonomies were produced: one by expert developers and one by an LLM. These\ntaxonomies were compared, analyzing differences and similarities, and were\nintegrated into a unified taxonomy that reflects the findings of both methods.\nBy systematically categorizing refactoring challenges in Qiskit, the unified\ntaxonomy is a foundation for future research on AI-assisted migration while\nenabling a more rigorous evaluation of automated refactoring techniques.\nAdditionally, this work contributes to quantum software engineering (QSE) by\nenhancing software development workflows, improving language compatibility, and\npromoting best practices in quantum programming.", "AI": {"tldr": "\u968f\u7740\u91cf\u5b50\u8ba1\u7b97\u7684\u53d1\u5c55\uff0c\u91cf\u5b50\u7f16\u7a0b\u5e93\u7684\u5f02\u6784\u6027\u548c\u6301\u7eed\u6f14\u53d8\u7ed9\u8f6f\u4ef6\u5f00\u53d1\u8005\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\u3002\u672c\u6587\u901a\u8fc7\u5f00\u53d1\u91cf\u5b50\u7535\u8def\u91cd\u6784\u95ee\u9898\u7684\u5206\u7c7b\u6cd5\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5206\u6790\u548c\u6bd4\u8f83\u4e0d\u540c\u91cd\u6784\u65b9\u6cd5\u7684\u7ed3\u6784\u5316\u6846\u67b6\u3002\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9Qiskit\u7248\u672c\u8fc1\u79fb\u4e2d\u7684\u91cd\u6784\u9700\u6c42\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u751f\u6210\u4e86\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u4e3aAI\u8f85\u52a9\u8fc1\u79fb\u548c\u81ea\u52a8\u5316\u91cd\u6784\u6280\u672f\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u63a8\u52a8\u4e86\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u7684\u53d1\u5c55\u3002", "motivation": "\u91cf\u5b50\u7f16\u7a0b\u5e93\u7684\u9891\u7e41\u66f4\u65b0\u5bfc\u81f4\u9700\u8981\u91cd\u6784\u4ee3\u7801\uff0c\u589e\u52a0\u4e86\u5f00\u53d1\u590d\u6742\u6027\uff0c\u4e14\u4e0e\u7ecf\u5178\u8f6f\u4ef6\u5de5\u7a0b\u4e2d\u7684\u91cd\u6784\u6311\u6218\u5b58\u5728\u672c\u8d28\u533a\u522b\u3002", "method": "1. \u5206\u6790Qiskit\u6587\u6863\u548c\u53d1\u884c\u8bf4\u660e\u4ee5\u521b\u5efa\u521d\u59cb\u91cd\u6784\u5206\u7c7b\u6cd5\u3002\n2. \u4f7f\u7528\u4e13\u5bb6\u5f00\u53d1\u8005\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5206\u522b\u751f\u6210\u4e24\u4e2a\u5206\u7c7b\u6cd5\u3002\n3. \u6bd4\u8f83\u5e76\u6574\u5408\u4e24\u4e2a\u5206\u7c7b\u6cd5\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\u3002", "result": "\u751f\u6210\u4e86\u7531\u4e13\u5bb6\u548cLLM\u4ea7\u751f\u7684\u4e24\u79cd\u5206\u7c7b\u6cd5\uff0c\u5e76\u6574\u5408\u6210\u4e00\u4e2a\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u5206\u7c7bQiskit\u4e2d\u7684\u91cd\u6784\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5173\u4e8eAI\u8f85\u52a9\u8fc1\u79fb\u548c\u81ea\u52a8\u5316\u91cd\u6784\u6280\u672f\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u4fc3\u8fdb\u4e86\u91cf\u5b50\u8f6f\u4ef6\u5de5\u7a0b\u7684\u53d1\u5c55\uff0c\u5305\u62ec\u4f18\u5316\u8f6f\u4ef6\u5f00\u53d1\u6d41\u7a0b\u3001\u63d0\u9ad8\u8bed\u8a00\u517c\u5bb9\u6027\u548c\u63a8\u5e7f\u6700\u4f73\u5b9e\u8df5\u3002"}}
{"id": "2506.07138", "pdf": "https://arxiv.org/pdf/2506.07138", "abs": "https://arxiv.org/abs/2506.07138", "authors": ["Hao Tang", "Chengchao Shen"], "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "The source code and trained weights are available at\n  https://github.com/visresearch/LLaVA-STF", "summary": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.", "AI": {"tldr": "Large multimodal models face computational challenges. This paper proposes Spatial Token Fusion (STF) and Multi-Block Token Fusion (MBTF) methods to reduce vision token sequences while preserving information, leading to faster inference without sacrificing performance.", "motivation": "To address the high computational cost of large multimodal models due to large language models and quadratic complexity in processing long vision token sequences.", "method": "The paper introduces two methods: Spatial Token Fusion (STF) which fuses spatial-adjacent tokens into one to shorten vision token sequences, and Multi-Block Token Fusion (MBTF) which supplements multi-granularity features for reduced token sequences.", "result": "Experiments show that using only 25% of the baseline's vision tokens, the proposed method achieves comparable or better performance on 8 popular vision-language benchmarks based on LLaVA-1.5.", "conclusion": "The combination of STF and MBTF improves inference efficiency without losing multimodal reasoning capabilities."}}
{"id": "2506.07142", "pdf": "https://arxiv.org/pdf/2506.07142", "abs": "https://arxiv.org/abs/2506.07142", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the second in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate Chain-of-Thought\n(CoT) prompting, a technique that encourages a large language model (LLM) to\n\"think step by step\" (Wei et al., 2022). CoT is a widely adopted method for\nimproving reasoning tasks, however, our findings reveal a more nuanced picture\nof its effectiveness. We demonstrate two things:\n  - The effectiveness of Chain-of-Thought prompting can vary greatly depending\non the type of task and model. For non-reasoning models, CoT generally improves\naverage performance by a small amount, particularly if the model does not\ninherently engage in step-by-step processing by default. However, CoT can\nintroduce more variability in answers, sometimes triggering occasional errors\nin questions the model would otherwise get right. We also found that many\nrecent models perform some form of CoT reasoning even if not asked; for these\nmodels, a request to perform CoT had little impact. Performing CoT generally\nrequires far more tokens (increasing cost and time) than direct answers.\n  - For models designed with explicit reasoning capabilities, CoT prompting\noften results in only marginal, if any, gains in answer accuracy. However, it\nsignificantly increases the time and tokens needed to generate a response.", "AI": {"tldr": "This report investigates the effectiveness of Chain-of-Thought (CoT) prompting in large language models, revealing that its benefits depend heavily on the model and task type. For non-reasoning models, CoT offers small improvements but can introduce errors and requires more tokens. For reasoning-focused models, CoT adds significant cost and time with little accuracy improvement.", "motivation": "To help business, education, and policy leaders understand the technical aspects of working with AI through rigorous testing, specifically focusing on the effectiveness of Chain-of-Thought (CoT) prompting.", "method": "Investigating the impact of CoT prompting across different types of tasks and models, analyzing performance improvements, error rates, token usage, and response times.", "result": "CoT prompting effectiveness varies by task and model type. Non-reasoning models see minor average performance improvements but increased variability and errors. Reasoning models show marginal gains in accuracy but much higher costs in terms of time and tokens.", "conclusion": "CoT prompting is not universally effective; its benefits are task- and model-dependent. It often increases costs significantly without substantial accuracy improvements."}}
{"id": "2506.06542", "pdf": "https://arxiv.org/pdf/2506.06542", "abs": "https://arxiv.org/abs/2506.06542", "authors": ["Sherman Khoo", "Yakun Wang", "Song Liu", "Mark Beaumont"], "title": "Direct Fisher Score Estimation for Likelihood Maximization", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We study the problem of likelihood maximization when the likelihood function\nis intractable but model simulations are readily available. We propose a\nsequential, gradient-based optimization method that directly models the Fisher\nscore based on a local score matching technique which uses simulations from a\nlocalized region around each parameter iterate. By employing a linear\nparameterization to the surrogate score model, our technique admits a\nclosed-form, least-squares solution. This approach yields a fast, flexible, and\nefficient approximation to the Fisher score, effectively smoothing the\nlikelihood objective and mitigating the challenges posed by complex likelihood\nlandscapes. We provide theoretical guarantees for our score estimator,\nincluding bounds on the bias introduced by the smoothing. Empirical results on\na range of synthetic and real-world problems demonstrate the superior\nperformance of our method compared to existing benchmarks.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5e8f\u5217\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u8ba1\u7b97\u4f46\u6a21\u578b\u6a21\u62df\u5bb9\u6613\u83b7\u5f97\u7684\u60c5\u51b5\u3002\u901a\u8fc7\u5c40\u90e8\u5f97\u5206\u5339\u914d\u6280\u672f\u548c\u7ebf\u6027\u53c2\u6570\u5316\u7684\u65b9\u6cd5\uff0c\u8be5\u6280\u672f\u63d0\u4f9b\u4e86\u5feb\u901f\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684Fisher\u5f97\u5206\u8fd1\u4f3c\uff0c\u5e76\u5728\u7406\u8bba\u4e0a\u4fdd\u8bc1\u4e86\u4f30\u8ba1\u5668\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e00\u7cfb\u5217\u5408\u6210\u548c\u771f\u5b9e\u95ee\u9898\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u8005\u4eec\u9762\u4e34\u4e00\u4e2a\u6311\u6218\uff1a\u5f53\u4f3c\u7136\u51fd\u6570\u96be\u4ee5\u76f4\u63a5\u8ba1\u7b97\u65f6\uff0c\u5982\u4f55\u6700\u5927\u5316\u4f3c\u7136\u51fd\u6570\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6a21\u578b\u6a21\u62df\u6570\u636e\u6765\u8fd1\u4f3cFisher\u5f97\u5206\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u4f18\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e8f\u5217\u3001\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u76f4\u63a5\u5bf9Fisher\u5f97\u5206\u8fdb\u884c\u5efa\u6a21\u3002\u4f7f\u7528\u5c40\u90e8\u5f97\u5206\u5339\u914d\u6280\u672f\uff0c\u4ece\u6bcf\u4e2a\u53c2\u6570\u8fed\u4ee3\u5468\u56f4\u7684\u5c40\u90e8\u533a\u57df\u751f\u6210\u6a21\u62df\u6570\u636e\u3002\u901a\u8fc7\u91c7\u7528\u7ebf\u6027\u53c2\u6570\u5316\u7684\u65b9\u6cd5\uff0c\u5f97\u5230\u4e86\u5c01\u95ed\u5f62\u5f0f\u7684\u6700\u5c0f\u4e8c\u4e58\u89e3\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u6548\u5730\u5e73\u6ed1\u4e86\u4f3c\u7136\u76ee\u6807\uff0c\u7f13\u89e3\u4e86\u590d\u6742\u4f3c\u7136\u666f\u89c2\u5e26\u6765\u7684\u56f0\u96be\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u4e86\u5f97\u5206\u4f30\u8ba1\u5668\u7684\u6027\u80fd\uff0c\u5e76\u7ed9\u51fa\u4e86\u7531\u5e73\u6ed1\u5f15\u5165\u7684\u504f\u5dee\u754c\u3002\u5728\u591a\u4e2a\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u95ee\u9898\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u7684\u57fa\u51c6\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u96be\u4ee5\u8ba1\u7b97\u4f3c\u7136\u51fd\u6570\u4f46\u6613\u4e8e\u83b7\u53d6\u6a21\u578b\u6a21\u62df\u6570\u636e\u7684\u60c5\u51b5\u3002\u6b64\u65b9\u6cd5\u4e0d\u4ec5\u5728\u7406\u8bba\u4e0a\u5f97\u5230\u4e86\u4fdd\u969c\uff0c\u800c\u4e14\u5728\u5b9e\u8df5\u4e2d\u4e5f\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2506.06557", "pdf": "https://arxiv.org/pdf/2506.06557", "abs": "https://arxiv.org/abs/2506.06557", "authors": ["Antonio Pariente", "Ignacio Hounie", "Santiago Segarra", "Alejandro Ribeiro"], "title": "Infinity Search: Approximate Vector Search with Projections on q-Metric Spaces", "categories": ["cs.IR", "cs.LG", "eess.SP", "math.MG"], "comment": null, "summary": "Despite the ubiquity of vector search applications, prevailing search\nalgorithms overlook the metric structure of vector embeddings, treating it as a\nconstraint rather than exploiting its underlying properties. In this paper, we\ndemonstrate that in $q$-metric spaces, metric trees can leverage a stronger\nversion of the triangle inequality to reduce comparisons for exact search.\nNotably, as $q$ approaches infinity, the search complexity becomes logarithmic.\nTherefore, we propose a novel projection method that embeds vector datasets\nwith arbitrary dissimilarity measures into $q$-metric spaces while preserving\nthe nearest neighbor. We propose to learn an approximation of this projection\nto efficiently transform query points to a space where euclidean distances\nsatisfy the desired properties. Our experimental results with text and image\nvector embeddings show that learning $q$-metric approximations enables classic\nmetric tree algorithms -- which typically underperform with high-dimensional\ndata -- to achieve competitive performance against state-of-the-art search\nmethods.", "AI": {"tldr": "\u901a\u8fc7\u5b66\u4e60$q$-metric\u8fd1\u4f3c\uff0c\u53ef\u4ee5\u4f7f\u7ecf\u5178\u7684\u5ea6\u91cf\u6811\u7b97\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u4e0a\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u641c\u7d22\u65b9\u6cd5\u7ade\u4e89\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u77e2\u91cf\u641c\u7d22\u7b97\u6cd5\u5ffd\u89c6\u4e86\u77e2\u91cf\u5d4c\u5165\u7684\u5ea6\u91cf\u7ed3\u6784\uff0c\u672a\u5145\u5206\u5229\u7528\u5176\u5e95\u5c42\u5c5e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6295\u5f71\u65b9\u6cd5\uff0c\u5c06\u5177\u6709\u4efb\u610f\u975e\u76f8\u4f3c\u6027\u5ea6\u91cf\u7684\u77e2\u91cf\u6570\u636e\u96c6\u5d4c\u5165\u5230$q$-metric\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4fdd\u6301\u6700\u8fd1\u90bb\u7279\u6027\uff1b\u540c\u65f6\u5b66\u4e60\u8be5\u6295\u5f71\u7684\u8fd1\u4f3c\u503c\u4ee5\u9ad8\u6548\u5730\u5c06\u67e5\u8be2\u70b9\u8f6c\u6362\u5230\u6ee1\u8db3\u6240\u9700\u6027\u8d28\u7684\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528\u6587\u672c\u548c\u56fe\u50cf\u77e2\u91cf\u5d4c\u5165\u8fdb\u884c$q$-metric\u8fd1\u4f3c\u5b66\u4e60\uff0c\u53ef\u4ee5\u8ba9\u7ecf\u5178\u5ea6\u91cf\u6811\u7b97\u6cd5\u5728\u9ad8\u7ef4\u6570\u636e\u4e0a\u7684\u8868\u73b0\u4e0e\u6700\u5148\u8fdb\u7684\u641c\u7d22\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u5229\u7528$q$-metric\u7a7a\u95f4\u7684\u66f4\u5f3a\u4e09\u89d2\u4e0d\u7b49\u5f0f\u53ef\u4ee5\u51cf\u5c11\u7cbe\u786e\u641c\u7d22\u4e2d\u7684\u6bd4\u8f83\u6b21\u6570\uff0c\u5f53$q$\u8d8b\u4e8e\u65e0\u7a77\u5927\u65f6\uff0c\u641c\u7d22\u590d\u6742\u5ea6\u53d8\u4e3a\u5bf9\u6570\u7ea7\u3002"}}
{"id": "2506.07154", "pdf": "https://arxiv.org/pdf/2506.07154", "abs": "https://arxiv.org/abs/2506.07154", "authors": ["Vicky Xefteri", "Tim Vieira", "Ryan Cotterell", "Afra Amini"], "title": "Syntactic Control of Language Models by Posterior Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Controlling the syntactic structure of text generated by language models is\nvaluable for applications requiring clarity, stylistic consistency, or\ninterpretability, yet it remains a challenging task. In this paper, we argue\nthat sampling algorithms based on the posterior inference can effectively\nenforce a target constituency structure during generation. Our approach\ncombines sequential Monte Carlo, which estimates the posterior distribution by\nsampling from a proposal distribution, with a syntactic tagger that ensures\nthat each generated token aligns with the desired syntactic structure. Our\nexperiments with GPT2 and Llama3-8B models show that with an appropriate\nproposal distribution, we can improve syntactic accuracy, increasing the F1\nscore from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both\ncases without compromising the language model's fluency. These results\nunderscore both the complexity of syntactic control and the effectiveness of\nsampling algorithms, offering a promising approach for applications where\nprecise control over syntax is essential.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u987a\u5e8f\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u548c\u8bed\u6cd5\u6807\u8bb0\u5668\uff0c\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u8bed\u8a00\u6a21\u578b\u6d41\u7545\u6027\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u751f\u6210\u6587\u672c\u7684\u8bed\u6cd5\u51c6\u786e\u6027\u3002", "motivation": "\u63a7\u5236\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u53e5\u6cd5\u7ed3\u6784\u5bf9\u4e8e\u9700\u8981\u6e05\u6670\u5ea6\u3001\u98ce\u683c\u4e00\u81f4\u6027\u6216\u53ef\u89e3\u91ca\u6027\u7684\u5e94\u7528\u6765\u8bf4\u5f88\u6709\u4ef7\u503c\uff0c\u4f46\u4ecd\u7136\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u540e\u9a8c\u63a8\u7406\u7684\u91c7\u6837\u7b97\u6cd5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u987a\u5e8f\u8499\u7279\u5361\u6d1b\uff08sequential Monte Carlo\uff09\u4e0e\u8bed\u6cd5\u6807\u8bb0\u5668\u7ed3\u5408\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u7684\u6bcf\u4e2a\u6807\u8bb0\u90fd\u7b26\u5408\u6240\u9700\u7684\u53e5\u6cd5\u7ed3\u6784\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5408\u9002\u7684\u5efa\u8bae\u5206\u5e03\uff0c\u53ef\u4ee5\u5c06GPT2-large\u548cLlama3-8B\u6a21\u578b\u7684F1\u5206\u6570\u4ece12.31\u548c35.33\u63d0\u9ad8\u5230\u7ea693\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u6a21\u578b\u7684\u6d41\u7545\u6027\u3002", "conclusion": "\u7ed3\u679c\u5f3a\u8c03\u4e86\u53e5\u6cd5\u63a7\u5236\u7684\u590d\u6742\u6027\u4ee5\u53ca\u91c7\u6837\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9700\u8981\u7cbe\u786e\u53e5\u6cd5\u63a7\u5236\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\u3002"}}
{"id": "2506.07169", "pdf": "https://arxiv.org/pdf/2506.07169", "abs": "https://arxiv.org/abs/2506.07169", "authors": ["Washington Cunha", "Leonardo Rocha", "Marcos Andr\u00e9 Gon\u00e7alves"], "title": "CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Disserta\u00e7\u00f5es e Trabalhos de Gradua\u00e7\u00e3o em SI -- XXI Simp\u00f3sio Brasileiro de Sistemas de Informa\u00e7\u00e3o", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Progress in Natural Language Processing (NLP) has been dictated by the rule\nof more: more data, more computing power and more complexity, best exemplified\nby the Large Language Models. However, training (or fine-tuning) large dense\nmodels for specific applications usually requires significant amounts of\ncomputing resources. This \\textbf{Ph.D. dissertation} focuses on an\nunder-investi\\-gated NLP data engineering technique, whose potential is\nenormous in the current scenario known as Instance Selection (IS). The IS goal\nis to reduce the training set size by removing noisy or redundant instances\nwhile maintaining the effectiveness of the trained models and reducing the\ntraining process cost. We provide a comprehensive and scientifically sound\ncomparison of IS methods applied to an essential NLP task -- Automatic Text\nClassification (ATC), considering several classification solutions and many\ndatasets. Our findings reveal a significant untapped potential for IS\nsolutions. We also propose two novel IS solutions that are noise-oriented and\nredundancy-aware, specifically designed for large datasets and transformer\narchitectures. Our final solution achieved an average reduction of 41\\% in\ntraining sets, while maintaining the same levels of effectiveness in all\ndatasets. Importantly, our solutions demonstrated speedup improvements of 1.67x\n(up to 2.46x), making them scalable for datasets with hundreds of thousands of\ndocuments.", "AI": {"tldr": "This Ph.D. dissertation explores Instance Selection (IS) techniques in NLP, revealing significant potential for reducing training set sizes while maintaining model effectiveness and reducing training costs.", "motivation": "Training large language models for specific applications typically requires substantial computing resources. The under-investigated technique of Instance Selection (IS) has enormous potential to reduce training set size by removing noisy or redundant instances without sacrificing model effectiveness.", "method": "The study provides a comprehensive comparison of IS methods applied to Automatic Text Classification (ATC), considering various classification solutions and datasets. Two novel IS solutions are proposed, focusing on noise and redundancy specifically designed for large datasets and transformer architectures.", "result": "The final solution achieved an average 41% reduction in training sets across all datasets while maintaining the same levels of effectiveness. Additionally, the solutions demonstrated speedup improvements of 1.67x (up to 2.46x), making them scalable for very large datasets.", "conclusion": "Instance Selection holds great promise for reducing computational requirements in NLP tasks, particularly for large datasets and transformer-based models."}}
{"id": "2506.07177", "pdf": "https://arxiv.org/pdf/2506.07177", "abs": "https://arxiv.org/abs/2506.07177", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Jaehong Yoon", "Soo Ye Kim", "Zhe Lin", "Sung Ju Hwang"], "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://frame-guidance-video.github.io/", "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.", "AI": {"tldr": "Frame Guidance\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6307\u5bfc\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e27\u7ea7\u4fe1\u53f7\uff08\u5982\u5173\u952e\u5e27\u3001\u98ce\u683c\u53c2\u8003\u56fe\u50cf\u3001\u8349\u56fe\u6216\u6df1\u5ea6\u56fe\uff09\u5b9e\u73b0\u53ef\u63a7\u89c6\u9891\u751f\u6210\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u6f5c\u5728\u5904\u7406\u65b9\u6cd5\u51cf\u5c11\u5185\u5b58\u4f7f\u7528\uff0c\u5e76\u91c7\u7528\u65b0\u7684\u6f5c\u5728\u4f18\u5316\u7b56\u7565\u786e\u4fdd\u5168\u5c40\u8fde\u8d2f\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u63a7\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u591a\u4f9d\u8d56\u4e8e\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u5927\u89c4\u6a21\u89c6\u9891\u6a21\u578b\uff0c\u4f46\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\uff0c\u8fd9\u79cd\u65b9\u6cd5\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u4e14\u80fd\u6709\u6548\u63a7\u5236\u89c6\u9891\u751f\u6210\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFrame Guidance\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5e27\u7ea7\u4fe1\u53f7\uff08\u5982\u5173\u952e\u5e27\u3001\u98ce\u683c\u53c2\u8003\u56fe\u50cf\u3001\u8349\u56fe\u6216\u6df1\u5ea6\u56fe\uff09\u8fdb\u884c\u53ef\u63a7\u89c6\u9891\u751f\u6210\uff1b\u91c7\u7528\u7b80\u5355\u7684\u6f5c\u5728\u5904\u7406\u65b9\u6cd5\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\uff1b\u5e94\u7528\u65b0\u7684\u6f5c\u5728\u4f18\u5316\u7b56\u7565\u4fdd\u8bc1\u5168\u5c40\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFrame Guidance\u53ef\u4ee5\u4e3a\u5e7f\u6cdb\u7684\u4efb\u52a1\u548c\u8f93\u5165\u4fe1\u53f7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53d7\u63a7\u89c6\u9891\uff0c\u5305\u62ec\u5173\u952e\u5e27\u5f15\u5bfc\u3001\u98ce\u683c\u5316\u548c\u5faa\u73af\u7b49\u4efb\u52a1\u3002", "conclusion": "Frame Guidance\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5404\u79cd\u89c6\u9891\u751f\u6210\u4efb\u52a1\u548c\u8f93\u5165\u4fe1\u53f7\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u4e14\u53ef\u63a7\u7684\u89c6\u9891\u3002"}}
{"id": "2506.06609", "pdf": "https://arxiv.org/pdf/2506.06609", "abs": "https://arxiv.org/abs/2506.06609", "authors": ["Alan Chen", "Jack Merullo", "Alessandro Stolfo", "Ellie Pavlick"], "title": "Transferring Features Across Language Models With Model Stitching", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we demonstrate that affine mappings between residual streams of\nlanguage models is a cheap way to effectively transfer represented features\nbetween models. We apply this technique to transfer the weights of Sparse\nAutoencoders (SAEs) between models of different sizes to compare their\nrepresentations. We find that small and large models learn highly similar\nrepresentation spaces, which motivates training expensive components like SAEs\non a smaller model and transferring to a larger model at a FLOPs savings. For\nexample, using a small-to-large transferred SAE as initialization can lead to\n50% cheaper training runs when training SAEs on larger models. Next, we show\nthat transferred probes and steering vectors can effectively recover ground\ntruth performance. Finally, we dive deeper into feature-level transferability,\nfinding that semantic and structural features transfer noticeably differently\nwhile specific classes of functional features have their roles faithfully\nmapped. Overall, our findings illustrate similarities and differences in the\nlinear representation spaces of small and large models and demonstrate a method\nfor improving the training efficiency of SAEs.", "AI": {"tldr": "Affine mappings between residual streams of language models can effectively transfer features and improve training efficiency of SAEs.", "motivation": "To find a cheap way to effectively transfer represented features between different sized models, allowing expensive components like Sparse Autoencoders (SAEs) to be trained on smaller models and then transferred to larger ones, saving computational resources.", "method": "Applying affine mappings between residual streams of language models to transfer the weights of SAEs, comparing representations across models of different sizes, analyzing the transferability of probes and steering vectors, and examining feature-level transferability.", "result": "Small and large models learn highly similar representation spaces; transferred SAEs can lead to 50% cheaper training runs; transferred probes and steering vectors can recover ground truth performance; semantic and structural features transfer differently while functional features have their roles faithfully mapped.", "conclusion": "The findings demonstrate similarities and differences in linear representation spaces of small and large models and provide a method for enhancing the training efficiency of SAEs."}}
{"id": "2506.07180", "pdf": "https://arxiv.org/pdf/2506.07180", "abs": "https://arxiv.org/abs/2506.07180", "authors": ["Wenrui Zhou", "Shu Yang", "Qingsong Yang", "Zikun Guo", "Lijie Hu", "Di Wang"], "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "24 pages", "summary": "As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated\nbenchmark designed to evaluate sycophantic behavior in state-of-the-art\nVideo-LLMs across diverse question formats, prompt biases, and visual reasoning\ntasks. Specifically, VISE pioneeringly brings linguistic perspectives on\nsycophancy into the visual domain, enabling fine-grained analysis across\nmultiple sycophancy types and interaction patterns. In addition, we explore\nkey-frame selection as an interpretable, training-free mitigation strategy,\nwhich reveals potential paths for reducing sycophantic bias by strengthening\nvisual grounding.", "AI": {"tldr": "A new benchmark VISE is proposed to evaluate sycophancy in Video-LLMs, analyzing its manifestations and exploring key-frame selection as a mitigation strategy.", "motivation": "There is a lack of systematic benchmarks and evaluations for understanding sycophantic behavior in Video-LLMs when faced with misleading user input.", "method": "Proposed VISE benchmark evaluates sycophancy across diverse question formats, prompt biases, and visual reasoning tasks. It incorporates linguistic perspectives and explores key-frame selection as a mitigation strategy.", "result": "VISE enables fine-grained analysis of sycophancy types and interaction patterns, revealing potential paths for reducing bias by strengthening visual grounding.", "conclusion": "VISE addresses the gap in evaluating sycophancy in Video-LLMs and provides insights into mitigation strategies."}}
{"id": "2506.06613", "pdf": "https://arxiv.org/pdf/2506.06613", "abs": "https://arxiv.org/abs/2506.06613", "authors": ["Arefe Boushehrian", "Amir Najafi"], "title": "Robust Learnability of Sample-Compressible Distributions under Noisy or Adversarial Perturbations", "categories": ["stat.ML", "cs.LG"], "comment": "50 pages, 1 figure", "summary": "Learning distribution families over $\\mathbb{R}^d$ is a fundamental problem\nin unsupervised learning and statistics. A central question in this setting is\nwhether a given family of distributions possesses sufficient structure to be\n(at least) information-theoretically learnable and, if so, to characterize its\nsample complexity. In 2018, Ashtiani et al. reframed \\emph{sample\ncompressibility}, originally due to Littlestone and Warmuth (1986), as a\nstructural property of distribution classes, proving that it guarantees\nPAC-learnability. This discovery subsequently enabled a series of recent\nadvancements in deriving nearly tight sample complexity bounds for various\nhigh-dimensional open problems. It has been further conjectured that the\nconverse also holds: every learnable class admits a tight sample compression\nscheme.\n  In this work, we establish that sample compressible families remain learnable\neven from perturbed samples, subject to a set of necessary and sufficient\nconditions. We analyze two models of data perturbation: (i) an additive\nindependent noise model, and (ii) an adversarial corruption model, where an\nadversary manipulates a limited subset of the samples unknown to the learner.\nOur results are general and rely on as minimal assumptions as possible. We\ndevelop a perturbation-quantization framework that interfaces naturally with\nthe compression scheme and leads to sample complexity bounds that scale\ngracefully with the noise level and corruption budget. As concrete\napplications, we establish new sample complexity bounds for learning finite\nmixtures of high-dimensional uniform distributions under both noise and\nadversarial perturbations, as well as for learning Gaussian mixture models from\nadversarially corrupted samples, resolving two open problems in the literature.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6837\u672c\u53ef\u538b\u7f29\u5206\u5e03\u65cf\u5728\u53d7\u6270\u6837\u672c\u4e0b\u7684\u53ef\u5b66\u4e60\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6270\u52a8\u91cf\u5316\u6846\u67b6\u4ee5\u5206\u6790\u4e24\u79cd\u6570\u636e\u6270\u52a8\u6a21\u578b\uff0c\u6700\u7ec8\u5bfc\u51fa\u4e86\u9ad8\u7ef4\u5747\u5300\u5206\u5e03\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\u65b0\u754c\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u77e5\u6837\u672c\u53ef\u538b\u7f29\u6027\u53ef\u4ee5\u4fdd\u8bc1PAC-\u53ef\u5b66\u4e60\u6027\uff0c\u4f46\u5176\u5728\u53d7\u6270\u6837\u672c\u4e0b\u7684\u8868\u73b0\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u6837\u672c\u53ef\u538b\u7f29\u5206\u5e03\u65cf\u662f\u5426\u5728\u6570\u636e\u6270\u52a8\u4e0b\u4ecd\u7136\u53ef\u5b66\u4e60\uff0c\u5e76\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u6837\u672c\u590d\u6742\u5ea6\u3002", "method": "\u4f5c\u8005\u5206\u6790\u4e86\u4e24\u79cd\u6570\u636e\u6270\u52a8\u6a21\u578b\uff1a\u52a0\u6027\u72ec\u7acb\u566a\u58f0\u6a21\u578b\u548c\u5bf9\u6297\u6027\u7834\u574f\u6a21\u578b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6270\u52a8\u91cf\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0e\u538b\u7f29\u65b9\u6848\u81ea\u7136\u7ed3\u5408\uff0c\u4ece\u800c\u5bfc\u51fa\u968f\u566a\u58f0\u6c34\u5e73\u548c\u7834\u574f\u9884\u7b97\u4f18\u96c5\u6269\u5c55\u7684\u6837\u672c\u590d\u6742\u5ea6\u754c\u3002", "result": "\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u4f5c\u8005\u6210\u529f\u5efa\u7acb\u4e86\u6709\u9650\u9ad8\u7ef4\u5747\u5300\u5206\u5e03\u6df7\u5408\u6a21\u578b\u5728\u566a\u58f0\u548c\u5bf9\u6297\u6027\u6270\u52a8\u4e0b\u7684\u65b0\u6837\u672c\u590d\u6742\u5ea6\u754c\uff0c\u5e76\u89e3\u51b3\u4e86\u4e24\u4e2a\u6587\u732e\u4e2d\u7684\u5f00\u653e\u95ee\u9898\uff0c\u5373\u9ad8\u7ef4\u5747\u5300\u5206\u5e03\u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4ece\u5bf9\u6297\u6027\u7834\u574f\u6837\u672c\u4e2d\u7684\u5b66\u4e60\u3002", "conclusion": "\u6837\u672c\u53ef\u538b\u7f29\u5206\u5e03\u65cf\u5373\u4f7f\u5728\u53d7\u6270\u6837\u672c\u4e0b\u4e5f\u4fdd\u6301\u53ef\u5b66\u4e60\u6027\uff0c\u4e14\u6240\u63d0\u51fa\u7684\u6270\u52a8\u91cf\u5316\u6846\u67b6\u4e3a\u672a\u6765\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2506.07211", "pdf": "https://arxiv.org/pdf/2506.07211", "abs": "https://arxiv.org/abs/2506.07211", "authors": ["Gionnieve Lim", "Bryan Chen Zhengyu Tan", "Kellie Yu Hui Sim", "Weiyan Shi", "Ming Hui Chew", "Ming Shan Hee", "Roy Ka-Wei Lee", "Simon T. Perrault", "Kenny Tsu Wei Choo"], "title": "Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) presents a dual challenge in\nthe fight against disinformation. These powerful tools, capable of generating\nhuman-like text at scale, can be weaponised to produce sophisticated and\npersuasive disinformation, yet they also hold promise for enhancing detection\nand mitigation strategies. This paper investigates the complex dynamics between\nLLMs and disinformation through a communication game that simulates online\nforums, inspired by the game Werewolf, with 25 participants. We analyse how\nDisinformers, Moderators, and Users leverage LLMs to advance their goals,\nrevealing both the potential for misuse and combating disinformation. Our\nfindings highlight the varying uses of LLMs depending on the participants'\nroles and strategies, underscoring the importance of understanding their\neffectiveness in this context. We conclude by discussing implications for\nfuture LLM development and online platform design, advocating for a balanced\napproach that empowers users and fosters trust while mitigating the risks of\nLLM-assisted disinformation.", "AI": {"tldr": "The paper explores the dual role of Large Language Models (LLMs) in producing and combating disinformation through a communication game involving 25 participants, revealing varied uses based on roles and emphasizing balanced future development.", "motivation": "To understand the complex dynamics between LLMs and disinformation, including their potential misuse and effectiveness in detection and mitigation strategies.", "method": "A communication game simulating online forums with 25 participants playing as Disinformers, Moderators, and Users, inspired by the game Werewolf.", "result": "LLMs are used differently depending on participants' roles and strategies, highlighting both risks and opportunities in combating disinformation.", "conclusion": "Future LLM development should take a balanced approach, empowering users, fostering trust, and mitigating risks of disinformation."}}
{"id": "2506.06653", "pdf": "https://arxiv.org/pdf/2506.06653", "abs": "https://arxiv.org/abs/2506.06653", "authors": ["Dangxing Chen"], "title": "Explaining Risks: Axiomatic Risk Attributions for Financial Models", "categories": ["q-fin.CP", "cs.LG", "stat.ML"], "comment": "This article has been accepted for publication in Quantitative\n  Finance, published by Taylor & Francis", "summary": "In recent years, machine learning models have achieved great success at the\nexpense of highly complex black-box structures. By using axiomatic attribution\nmethods, we can fairly allocate the contributions of each feature, thus\nallowing us to interpret the model predictions. In high-risk sectors such as\nfinance, risk is just as important as mean predictions. Throughout this work,\nwe address the following risk attribution problem: how to fairly allocate the\nrisk given a model with data? We demonstrate with analysis and empirical\nexamples that risk can be well allocated by extending the Shapley value\nframework.", "AI": {"tldr": "\u5728\u8fd1\u5e74\u6765\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4ee5\u9ad8\u5ea6\u590d\u6742\u7684\u9ed1\u7bb1\u7ed3\u6784\u4e3a\u4ee3\u4ef7\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\u3002\u901a\u8fc7\u4f7f\u7528\u516c\u7406\u5f52\u56e0\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u516c\u5e73\u5730\u5206\u914d\u6bcf\u4e2a\u7279\u5f81\u7684\u8d21\u732e\uff0c\u4ece\u800c\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u3002\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u91d1\u878d\uff09\uff0c\u98ce\u9669\u4e0e\u5e73\u5747\u9884\u6d4b\u540c\u6837\u91cd\u8981\u3002\u672c\u6587\u89e3\u51b3\u4e86\u4ee5\u4e0b\u98ce\u9669\u5f52\u56e0\u95ee\u9898\uff1a\u5982\u4f55\u5728\u7ed9\u5b9a\u6a21\u578b\u548c\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u516c\u5e73\u5206\u914d\u98ce\u9669\uff1f\u6211\u4eec\u901a\u8fc7\u5206\u6790\u548c\u5b9e\u8bc1\u4f8b\u5b50\u8bc1\u660e\uff0c\u901a\u8fc7\u6269\u5c55Shapley\u503c\u6846\u67b6\u53ef\u4ee5\u5f88\u597d\u5730\u5206\u914d\u98ce\u9669\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6210\u529f\u4f34\u968f\u7740\u590d\u6742\u6027\u548c\u4e0d\u900f\u660e\u6027\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u91d1\u878d\uff09\uff0c\u4ec5\u4ec5\u89e3\u91ca\u5e73\u5747\u9884\u6d4b\u662f\u4e0d\u591f\u7684\uff0c\u8fd8\u9700\u8981\u516c\u5e73\u5408\u7406\u5730\u5206\u914d\u98ce\u9669\u3002", "method": "\u91c7\u7528\u516c\u7406\u5f52\u56e0\u65b9\u6cd5\uff0c\u5e76\u6269\u5c55Shapley\u503c\u6846\u67b6\u6765\u89e3\u51b3\u98ce\u9669\u5206\u914d\u95ee\u9898\u3002", "result": "\u5206\u6790\u548c\u5b9e\u8bc1\u4f8b\u5b50\u8868\u660e\uff0c\u98ce\u9669\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55Shapley\u503c\u6846\u67b6\u5f97\u5230\u826f\u597d\u7684\u5206\u914d\u3002", "conclusion": "\u6269\u5c55Shapley\u503c\u6846\u67b6\u662f\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u516c\u5e73\u5206\u914d\u6a21\u578b\u4e2d\u7684\u98ce\u9669\u3002"}}
{"id": "2506.06667", "pdf": "https://arxiv.org/pdf/2506.06667", "abs": "https://arxiv.org/abs/2506.06667", "authors": ["Yu-Hsuan Ho", "Ali Mostafavi"], "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.", "AI": {"tldr": "This paper presents Flood-DamageSense, a new deep-learning framework for building-level flood-damage assessment that improves on existing models by up to 19 percentage points.", "motivation": "Most post-disaster damage classifiers perform poorly at identifying flood-related building damages due to the lack of clear spectral or structural signatures after inundation.", "method": "The model fuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical basemaps and an inherent flood-risk layer. A multimodal Mamba backbone with a semi-Siamese encoder and task-specific decoders jointly predicts graded building-damage states, floodwater extent, and building footprints.", "result": "Training and evaluation on Hurricane Harvey imagery show a mean F1 improvement of up to 19 percentage points over state-of-the-art baselines, with the largest gains in the frequently misclassified 'minor' and 'moderate' damage categories.", "conclusion": "Flood-DamageSense delivers faster, finer-grained, and more reliable flood-damage intelligence to support post-disaster decision-making and resource allocation."}}
{"id": "2506.07232", "pdf": "https://arxiv.org/pdf/2506.07232", "abs": "https://arxiv.org/abs/2506.07232", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) possess extensive knowledge bases and strong\nreasoning capabilities, making them promising tools for complex, multi-agent\nplanning in embodied environments. However, despite LLMs' advanced abilities\nand the sophisticated modular design of agentic methods, existing LLM-based\nplanning algorithms remain limited by weak adaptation capabilities to\nmulti-agent embodied scenarios. We address this limitation by introducing a\nframework that enables LLM agents to learn and evolve both before and during\ntest time, equipping them with environment-relevant knowledge for better\nplanning and enhanced communication for improved cooperation. Inspired by\ncentralized training with decentralized execution in multi-agent reinforcement\nlearning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)}\nparadigm for multi-agent LLMs adaptation. At the individual level, LLM agents\nlearn a local utility function from exploratory datasets to better comprehend\nthe embodied environment, which is then queried during test time to support\ninformed decision-making. At the team level, LLM agents collaboratively and\niteratively maintain and update a shared cooperation knowledge list based on\nnew experiences, using it to guide more effective communication. By combining\nindividual learning with team evolution, LIET enables comprehensive and\nflexible adaptation for LLM agents. Our experiments on Communicative\nWatch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate\nthat LIET, instantiated with both LLaMA and GPT-4o, outperforms existing\nbaselines and exhibits strong cooperative planning abilities.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLearn as Individuals, Evolve as a Team (LIET)\u7684\u65b0\u8303\u5f0f\uff0c\u7528\u4e8e\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u667a\u80fd\u4f53\u5177\u8eab\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u3002\u901a\u8fc7\u4e2a\u4f53\u5b66\u4e60\u548c\u56e2\u961f\u8fdb\u5316\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0cLIET\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u5728\u6d4b\u8bd5\u524d\u548c\u6d4b\u8bd5\u671f\u95f4\u4e0d\u65ad\u5b66\u4e60\u548c\u8fdb\u5316\uff0c\u4ece\u800c\u83b7\u5f97\u73af\u5883\u76f8\u5173\u7684\u77e5\u8bc6\u4ee5\u6539\u8fdb\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u589e\u5f3a\u7684\u6c9f\u901a\u6765\u4fc3\u8fdb\u5408\u4f5c\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728Communicative Watch-And-Help\u548cThreeD-World Multi-Agent Transport\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528LLaMA\u548cGPT-4o\u5b9e\u4f8b\u5316\u7684LIET\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5408\u4f5c\u89c4\u5212\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e7f\u6cdb\u7684\u77e5\u8bc6\u5e93\u548c\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u89c4\u5212\u7b97\u6cd5\u5728\u591a\u667a\u80fd\u4f53\u5177\u8eab\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b\u4ecd\u7136\u6709\u9650\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u4f7fLLM\u4ee3\u7406\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u73af\u5883\uff0c\u63d0\u9ad8\u5176\u89c4\u5212\u80fd\u529b\u548c\u5408\u4f5c\u6c34\u5e73\u3002", "method": "LIET\u8303\u5f0f\u5305\u62ec\u4e24\u4e2a\u5c42\u9762\uff1a\u4e2a\u4f53\u5c42\u9762\uff0cLLM\u4ee3\u7406\u4ece\u63a2\u7d22\u6027\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u5c40\u90e8\u6548\u7528\u51fd\u6570\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u5177\u8eab\u73af\u5883\uff0c\u5e76\u5728\u6d4b\u8bd5\u65f6\u67e5\u8be2\u8be5\u51fd\u6570\u4ee5\u652f\u6301\u51b3\u7b56\uff1b\u56e2\u961f\u5c42\u9762\uff0cLLM\u4ee3\u7406\u901a\u8fc7\u65b0\u7ecf\u9a8c\u534f\u4f5c\u7ef4\u62a4\u548c\u66f4\u65b0\u5171\u4eab\u7684\u5408\u4f5c\u77e5\u8bc6\u5217\u8868\uff0c\u4ee5\u6307\u5bfc\u66f4\u6709\u6548\u7684\u6c9f\u901a\u3002\u901a\u8fc7\u7ed3\u5408\u4e2a\u4f53\u5b66\u4e60\u4e0e\u56e2\u961f\u8fdb\u5316\uff0cLIET\u5b9e\u73b0\u4e86\u5168\u9762\u4e14\u7075\u6d3b\u7684\u9002\u5e94\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLIET\u5728Communicative Watch-And-Help\u548cThreeD-World Multi-Agent Transport\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u65e0\u8bba\u662f\u4f7f\u7528LLaMA\u8fd8\u662fGPT-4o\u5b9e\u4f8b\u5316\uff0c\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u5408\u4f5c\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "LIET\u8303\u5f0f\u6210\u529f\u5730\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u591a\u667a\u80fd\u4f53\u5177\u8eab\u573a\u666f\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u4e2a\u4f53\u5b66\u4e60\u548c\u56e2\u961f\u8fdb\u5316\u76f8\u7ed3\u5408\u53ef\u4ee5\u663e\u8457\u6539\u5584LLM\u4ee3\u7406\u7684\u89c4\u5212\u548c\u5408\u4f5c\u6027\u80fd\u3002"}}
{"id": "2506.06680", "pdf": "https://arxiv.org/pdf/2506.06680", "abs": "https://arxiv.org/abs/2506.06680", "authors": ["Radha Kodali", "Venkata Rao Dhulipalla", "Venkata Siva Kishor Tatavarty", "Madhavi Nadakuditi", "Bharadwaj Thiruveedhula", "Suryanarayana Gunnam", "Durga Prasad Bavirisetti"], "title": "Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Infertility has a considerable impact on individuals' quality of life,\naffecting them socially and psychologically, with projections indicating a rise\nin the upcoming years. In vitro fertilization (IVF) emerges as one of the\nprimary techniques within economically developed nations, employed to address\nthe rising problem of low fertility. Expert embryologists conventionally grade\nembryos by reviewing blastocyst images to select the most optimal for transfer,\nyet this process is time-consuming and lacks efficiency. Blastocyst images\nprovide a valuable resource for assessing embryo viability. In this study, we\nintroduce an explainable artificial intelligence (XAI) framework for\nclassifying embryos, employing a fusion of convolutional neural network (CNN)\nand long short-term memory (LSTM) architecture, referred to as CNN-LSTM.\nUtilizing deep learning, our model achieves high accuracy in embryo\nclassification while maintaining interpretability through XAI.", "AI": {"tldr": "An explainable AI framework using CNN-LSTM is introduced for embryo classification in IVF to improve efficiency and accuracy.", "motivation": "Infertility affects individuals' quality of life and is expected to rise. Current embryo grading by expert embryologists is time-consuming and inefficient.", "method": "The study employs a fusion of convolutional neural network (CNN) and long short-term memory (LSTM) architecture, known as CNN-LSTM, within an explainable artificial intelligence (XAI) framework for classifying embryos based on blastocyst images.", "result": "The model achieves high accuracy in embryo classification while ensuring interpretability through the use of XAI.", "conclusion": "The introduced XAI framework with CNN-LSTM offers a promising solution for improving the efficiency and accuracy of embryo selection in IVF."}}
{"id": "2506.07239", "pdf": "https://arxiv.org/pdf/2506.07239", "abs": "https://arxiv.org/abs/2506.07239", "authors": ["Raghu Vamshi Hemadri", "Jitendra Bhandari", "Johann Knechtel", "Badri P Gopalan", "Ramesh Narayanaswamy", "Ramesh Karri", "Siddharth Garg"], "title": "VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Modern chip design is complex, and there is a crucial need for early-stage\nprediction of key design-quality metrics like timing and routing congestion\ndirectly from Verilog code (a commonly used programming language for hardware\ndesign). It is especially important yet complex to predict individual lines of\ncode that cause timing violations or downstream routing congestion. Prior works\nhave tried approaches like converting Verilog into an intermediate graph\nrepresentation and using LLM embeddings alongside other features to predict\nmodule-level quality, but did not consider line-level quality prediction. We\npropose VeriLoC, the first method that predicts design quality directly from\nVerilog at both the line- and module-level. To this end, VeriLoC leverages\nrecent Verilog code-generation LLMs to extract local line-level and\nmodule-level embeddings, and train downstream classifiers/regressors on\nconcatenations of these embeddings. VeriLoC achieves high F1-scores of\n0.86-0.95 for line-level congestion and timing prediction, and reduces the mean\naverage percentage error from 14% - 18% for SOTA methods down to only 4%. We\nbelieve that VeriLoC embeddings and insights from our work will also be of\nvalue for other predictive and optimization tasks for complex hardware design.", "AI": {"tldr": "VeriLoC is the first method that predicts design quality directly from Verilog at both line- and module-level, achieving high F1-scores and significantly reducing errors.", "motivation": "There is a crucial need for early-stage prediction of key design-quality metrics like timing and routing congestion directly from Verilog code, especially predicting individual lines of code that cause violations.", "method": "VeriLoC leverages recent Verilog code-generation LLMs to extract local line-level and module-level embeddings, then trains downstream classifiers/regressors on concatenations of these embeddings.", "result": "VeriLoC achieves high F1-scores of 0.86-0.95 for line-level congestion and timing prediction, and reduces the mean average percentage error from 14%-18% for SOTA methods down to only 4%.", "conclusion": "VeriLoC embeddings and insights will also be valuable for other predictive and optimization tasks for complex hardware design."}}
{"id": "2506.06718", "pdf": "https://arxiv.org/pdf/2506.06718", "abs": "https://arxiv.org/abs/2506.06718", "authors": ["Omar Mashaal", "Hatem Abou-Zeid"], "title": "IQFM A Wireless Foundational Model for I/Q Streams in AI-Native 6G", "categories": ["eess.SP", "cs.LG"], "comment": null, "summary": "Foundational models have shown remarkable potential in natural language\nprocessing and computer vision, yet remain in their infancy in wireless\ncommunications. While a few efforts have explored image-based modalities such\nas channel state information (CSI) and frequency spectrograms, foundational\nmodels that operate directly on raw IQ data remain largely unexplored. This\npaper presents, IQFM, the first I/Q signal foundational model for wireless\ncommunications. IQFM supporting diverse tasks: modulation classification,\nangle-of-arrival (AoA), beam prediction, and RF fingerprinting, without heavy\npreprocessing or handcrafted features. We also introduce a task-aware\naugmentation strategy that categorizes transformations into core augmentations,\nsuch as cyclic time shifting, and task-specific augmentations. This strategy\nforms the basis for structured, task-dependent representation learning within a\ncontrastive self-supervised learning (SSL) framework. Using this strategy, the\nlightweight encoder, pre-trained via SSL on over-the-air multi-antenna IQ data,\nachieves up to 99.67% and 65.45% accuracy on modulation and AoA classification,\nrespectively, using only one labeled sample per class, outperforming supervised\nbaselines by up to 7x and 145x. The model also generalizes to\nout-of-distribution tasks; when adapted to new tasks using only 500 samples per\nclass and minimal parameter updates via LoRA, the same frozen encoder achieves\n94.15% on beam prediction (vs. 89.53% supervised), 50.00% on RML2016a\nmodulation classification (vs. 49.30%), and 96.05% on RF fingerprinting (vs.\n96.64%). These results demonstrate the potential of raw IQ-based foundational\nmodels as efficient, reusable encoders for multi-task learning in AI-native 6G\nsystems.", "AI": {"tldr": "This paper introduces IQFM, the first foundational model for wireless communications operating on raw IQ data. It supports multiple tasks like modulation classification and angle-of-arrival (AoA) with minimal preprocessing. Using self-supervised learning (SSL), it achieves high accuracy in various tasks with very few labeled samples, outperforming supervised methods by significant margins. The model also generalizes well to out-of-distribution tasks.", "motivation": "Foundational models have made significant advancements in fields like natural language processing and computer vision but are underexplored in wireless communications, particularly when working directly with raw IQ data rather than image-based modalities such as CSI or frequency spectrograms.", "method": "The method involves creating IQFM, a foundational model for wireless communication using raw IQ data. A task-aware augmentation strategy is introduced that includes core augmentations like cyclic time shifting and task-specific augmentations. This strategy is used within a contrastive SSL framework to pre-train a lightweight encoder on multi-antenna IQ data.", "result": "IQFM achieves up to 99.67% accuracy on modulation classification and 65.45% on AoA classification using only one labeled sample per class. When adapted to new tasks with minimal parameter updates via LoRA, it performs competitively with supervised models on tasks like beam prediction, RML2016a modulation classification, and RF fingerprinting.", "conclusion": "Raw IQ-based foundational models hold potential as efficient, reusable encoders for multi-task learning in AI-native 6G systems."}}
{"id": "2506.07245", "pdf": "https://arxiv.org/pdf/2506.07245", "abs": "https://arxiv.org/abs/2506.07245", "authors": ["Wenxuan Xie", "Yaxun Dai", "Wenhao Jiang"], "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement.", "AI": {"tldr": "Recent advancements in LLMs have improved Text-to-SQL performance, but they are limited by static database information. SDE-SQL addresses this by enabling self-driven exploration of databases through generating and executing SQL probes, improving execution accuracy on the BIRD benchmark.", "motivation": "To overcome the limitation of LLMs relying on static, pre-processed database information which restricts their ability to fully understand the database contents.", "method": "SDE-SQL framework allows large language models to perform self-driven exploration of databases during inference by generating and executing SQL probes to actively retrieve information from the database and iteratively update its understanding of the data.", "result": "SDE-SQL achieves an 8.02% relative improvement in execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline on the BIRD benchmark. With supervised fine-tuning (SFT), there is an additional 0.52% improvement.", "conclusion": "SDE-SQL establishes a new state-of-the-art among methods based on open-source models without SFT or model ensembling, and its performance can be further enhanced with SFT."}}
{"id": "2506.07274", "pdf": "https://arxiv.org/pdf/2506.07274", "abs": "https://arxiv.org/abs/2506.07274", "authors": ["Olga Kellert", "Nemika Tyagi", "Muhammad Imran", "Nelvin Licona-Guevara", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages", "summary": "Code-switching presents a complex challenge for syntactic analysis,\nespecially in low-resource language settings where annotated data is scarce.\nWhile recent work has explored the use of large language models (LLMs) for\nsequence-level tagging, few approaches systematically investigate how well\nthese models capture syntactic structure in code-switched contexts. Moreover,\nexisting parsers trained on monolingual treebanks often fail to generalize to\nmultilingual and mixed-language input. To address this gap, we introduce the\nBiLingua Parser, an LLM-based annotation pipeline designed to produce Universal\nDependencies (UD) annotations for code-switched text. First, we develop a\nprompt-based framework for Spanish-English and Spanish-Guaran\\'i data,\ncombining few-shot LLM prompting with expert review. Second, we release two\nannotated datasets, including the first Spanish-Guaran\\'i UD-parsed corpus.\nThird, we conduct a detailed syntactic analysis of switch points across\nlanguage pairs and communicative contexts. Experimental results show that\nBiLingua Parser achieves up to 95.29% LAS after expert revision, significantly\noutperforming prior baselines and multilingual parsers. These results show that\nLLMs, when carefully guided, can serve as practical tools for bootstrapping\nsyntactic resources in under-resourced, code-switched environments. Data and\nsource code are available at https://github.com/N3mika/ParsingProject", "AI": {"tldr": "Code-switching poses challenges for syntactic analysis, especially in low-resource languages. This paper introduces BiLingua Parser, an LLM-based annotation pipeline that produces UD annotations for code-switched text. It achieves up to 95.29% LAS after expert revision.", "motivation": "Code-switching is a complex challenge for syntactic analysis, particularly in low-resource language settings where annotated data is scarce. Existing parsers trained on monolingual treebanks often fail to generalize to multilingual and mixed-language input.", "method": "The authors developed a prompt-based framework for Spanish-English and Spanish-Guaran\u00ed data, combining few-shot LLM prompting with expert review. They also released two annotated datasets, including the first Spanish-Guaran\u00ed UD-parsed corpus, and conducted a detailed syntactic analysis of switch points across language pairs and communicative contexts.", "result": "Experimental results show that BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly outperforming prior baselines and multilingual parsers.", "conclusion": "LLMs, when carefully guided, can serve as practical tools for bootstrapping syntactic resources in under-resourced, code-switched environments."}}
{"id": "2506.07280", "pdf": "https://arxiv.org/pdf/2506.07280", "abs": "https://arxiv.org/abs/2506.07280", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 23 figures, 9 tables", "summary": "Video Diffusion Models (VDMs) have emerged as powerful generative tools,\ncapable of synthesizing high-quality spatiotemporal content. Yet, their\npotential goes far beyond mere video generation. We argue that the training\ndynamics of VDMs, driven by the need to model coherent sequences, naturally\npushes them to internalize structured representations and an implicit\nunderstanding of the visual world. To probe the extent of this internal\nknowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs\nfor new tasks using only a handful of examples. Our method transforms each task\ninto a visual transition, enabling the training of LoRA weights on short\ninput-output sequences without altering the generative interface of a frozen\nVDM. Despite minimal supervision, the model exhibits strong generalization\nacross diverse tasks, from low-level vision (for example, segmentation and pose\nestimation) to high-level reasoning (for example, on ARC-AGI). These results\nreframe VDMs as more than generative engines. They are adaptable visual\nlearners with the potential to serve as the backbone for future foundation\nmodels in vision.", "AI": {"tldr": "Video Diffusion Models (VDMs) can be repurposed for various tasks beyond video generation through few-shot fine-tuning, showcasing their potential as adaptable visual learners.", "motivation": "To explore the internal knowledge and structured representations that VDMs acquire during training, and to demonstrate their potential beyond video generation.", "method": "Introduce a few-shot fine-tuning framework that transforms tasks into visual transitions, enabling training of LoRA weights on short input-output sequences without altering the generative interface of a frozen VDM.", "result": "The model exhibits strong generalization across diverse tasks, including low-level vision tasks like segmentation and pose estimation, as well as high-level reasoning tasks like ARC-AGI, despite minimal supervision.", "conclusion": "VDMs are more than generative engines; they are adaptable visual learners with the potential to serve as the backbone for future foundation models in vision."}}
{"id": "2506.06773", "pdf": "https://arxiv.org/pdf/2506.06773", "abs": "https://arxiv.org/abs/2506.06773", "authors": ["Emet Behrendt", "Shing Wai Pun", "Prashant J. Nair"], "title": "Taming Wild Branches: Overcoming Hard-to-Predict Branches using the Bullseye Predictor", "categories": ["cs.AR", "cs.LG", "cs.PF", "C.1.2; B.2.1; C.4; C.0"], "comment": "Paper accepted and presented at the 6th Championship Branch\n  Prediction (CBP) workshop, co-held with ISCA 2025, on June 21, 2025, Tokyo,\n  Japan", "summary": "Branch prediction is key to the performance of out-of-order processors. While\nthe CBP-2016 winner TAGE-SC-L combines geometric-history tables, a statistical\ncorrector, and a loop predictor, over half of its remaining mispredictions stem\nfrom a small set of hard-to-predict (H2P) branches. These branches occur under\ndiverse global histories, causing repeated thrashing in TAGE and eviction\nbefore usefulness counters can mature. Prior work shows that simply enlarging\nthe tables offers only marginal improvement.\n  We augment a 159 KB TAGE-SC-L predictor with a 28 KB H2P-targeted subsystem\ncalled the Bullseye predictor. It identifies problematic PCs using a\nset-associative H2P Identification Table (HIT) and steers them to one of two\nbranch-specific perceptrons, one indexed by hashed local history and the other\nby folded global history. A short trial phase tracks head-to-head accuracy in\nan H2P cache. A branch becomes perceptron-resident only if the perceptron's\nsustained accuracy and output magnitude exceed dynamic thresholds, after which\nTAGE updates for that PC are suppressed to reduce pollution. The HIT, cache,\nand perceptron operate fully in parallel with TAGE-SC-L, providing higher\nfidelity on the H2P tail. This achieves an average MPKI of 3.4045 and CycWpPKI\nof 145.09.", "AI": {"tldr": "The paper proposes augmenting TAGE-SC-L predictor with a Bullseye predictor to address hard-to-predict (H2P) branches, leading to improved prediction accuracy and performance metrics.", "motivation": "Even the CBP-2016 winner TAGE-SC-L has over half of its remaining mispredictions from H2P branches which cause repeated thrashing and eviction before usefulness counters can mature. Simply enlarging tables provides only marginal improvement.", "method": "Augment TAGE-SC-L predictor with a 28 KB Bullseye predictor subsystem that includes a set-associative H2P Identification Table (HIT) and two branch-specific perceptrons indexed by hashed local history or folded global history respectively. A trial phase tracks accuracy in an H2P cache, and TAGE updates are suppressed for branches meeting dynamic thresholds.", "result": "Achieves an average MPKI of 3.4045 and CycWpPKI of 145.09, indicating improvements in prediction accuracy and overall processor performance.", "conclusion": "Integrating the Bullseye predictor into TAGE-SC-L enhances fidelity on hard-to-predict branches, reducing mispredictions and improving key performance indicators."}}
{"id": "2506.07281", "pdf": "https://arxiv.org/pdf/2506.07281", "abs": "https://arxiv.org/abs/2506.07281", "authors": ["Leah Hope Ajmani", "Nuredin Ali Abdelkadir", "Stevie Chancellor"], "title": "Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As AI technologies become more human-facing, there have been numerous calls\nto adapt participatory approaches to AI development -- spurring the idea of\nparticipatory AI. However, these calls often focus only on primary\nstakeholders, such as end-users, and not secondary stakeholders. This paper\nseeks to translate the ideals of participatory AI to a broader population of\nsecondary AI stakeholders through semi-structured interviews. We theorize that\nmeaningful participation involves three participatory ideals: (1) informedness,\n(2) consent, and (3) agency. We also explore how secondary stakeholders realize\nthese ideals by traversing a complicated problem space. Like walking up the\nrungs of a ladder, these ideals build on one another. We introduce three\nstakeholder archetypes: the reluctant data contributor, the unsupported\nactivist, and the well-intentioned practitioner, who must navigate systemic\nbarriers to achieving agentic AI relationships. We envision an AI future where\nsecondary stakeholders are able to meaningfully participate with the AI systems\nthey influence and are influenced by.", "AI": {"tldr": "This paper discusses the inclusion of secondary stakeholders in participatory AI via semi-structured interviews, proposing three participatory ideals - informedness, consent, and agency - and introducing three stakeholder archetypes.", "motivation": "The motivation is to extend the ideals of participatory AI to secondary stakeholders, who are often overlooked, through semi-structured interviews.", "method": "Semi-structured interviews were conducted with secondary AI stakeholders to explore their perspectives on meaningful participation.", "result": "Three participatory ideals were identified: informedness, consent, and agency. Three stakeholder archetypes were also introduced: the reluctant data contributor, the unsupported activist, and the well-intentioned practitioner.", "conclusion": "The authors envision an AI future where secondary stakeholders can meaningfully participate with the AI systems they influence and are influenced by."}}
{"id": "2506.06778", "pdf": "https://arxiv.org/pdf/2506.06778", "abs": "https://arxiv.org/abs/2506.06778", "authors": ["Longlin Yu", "Jiajun Zha", "Tong Yang", "Tianyu Xie", "Xiangyu Zhang", "S. -H. Gary Chan", "Cheng Zhang"], "title": "Continuous Semi-Implicit Models", "categories": ["stat.ML", "cs.LG"], "comment": "26 pages, 8 figures, ICML 2025", "summary": "Semi-implicit distributions have shown great promise in variational inference\nand generative modeling. Hierarchical semi-implicit models, which stack\nmultiple semi-implicit layers, enhance the expressiveness of semi-implicit\ndistributions and can be used to accelerate diffusion models given pretrained\nscore networks. However, their sequential training often suffers from slow\nconvergence. In this paper, we introduce CoSIM, a continuous semi-implicit\nmodel that extends hierarchical semi-implicit models into a continuous\nframework. By incorporating a continuous transition kernel, CoSIM enables\nefficient, simulation-free training. Furthermore, we show that CoSIM achieves\nconsistency with a carefully designed transition kernel, offering a novel\napproach for multistep distillation of generative models at the distributional\nlevel. Extensive experiments on image generation demonstrate that CoSIM\nperforms on par or better than existing diffusion model acceleration methods,\nachieving superior performance on FD-DINOv2.", "AI": {"tldr": "CoSIM, a continuous semi-implicit model that extends hierarchical semi-implicit models into a continuous framework with a continuous transition kernel, enables efficient training and performs on par or better than existing diffusion model acceleration methods.", "motivation": "Semi-implicit distributions have shown great promise in variational inference and generative modeling, but the sequential training of hierarchical semi-implicit models often suffers from slow convergence.", "method": "The paper introduces CoSIM, which incorporates a continuous transition kernel to enable efficient, simulation-free training and achieve consistency with a carefully designed transition kernel.", "result": "Extensive experiments on image generation demonstrate that CoSIM performs on par or better than existing diffusion model acceleration methods, achieving superior performance on FD-DINOv2.", "conclusion": "CoSIM offers a novel approach for multistep distillation of generative models at the distributional level and shows promising results in accelerating diffusion models."}}
{"id": "2506.07296", "pdf": "https://arxiv.org/pdf/2506.07296", "abs": "https://arxiv.org/abs/2506.07296", "authors": ["Arian Askari", "Emmanouil Stergiadis", "Ilya Gusev", "Moran Beladev"], "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025, Main track. 13 Pages, 1 figure", "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel\ndomain that enables natural language property search, addressing the\nlimitations of traditional travel search engines which require users to start\nwith a destination and editing search parameters. HotelMatch-LLM features three\nkey innovations: (1) Domain-specific multi-task optimization with three novel\nretrieval, visual, and language modeling objectives; (2) Asymmetrical dense\nretrieval architecture combining a small language model (SLM) for efficient\nonline query processing and a large language model (LLM) for embedding hotel\ndata; and (3) Extensive image processing to handle all property image\ngalleries. Experiments on four diverse test sets show HotelMatch-LLM\nsignificantly outperforms state-of-the-art models, including VISTA and MARVEL.\nSpecifically, on the test set -- main query type -- we achieve 0.681 for\nHotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our\nanalysis highlights the impact of our multi-task optimization, the\ngeneralizability of HotelMatch-LLM across LLM architectures, and its\nscalability for processing large image galleries.", "AI": {"tldr": "The paper introduces HotelMatch-LLM, a multimodal dense retrieval model for travel domain property search. It features domain-specific multi-task optimization, asymmetrical dense retrieval architecture, and extensive image processing. Experiments show it outperforms state-of-the-art models.", "motivation": "To address the limitations of traditional travel search engines which require users to start with a destination and edit search parameters.", "method": "HotelMatch-LLM has three key innovations: (1) Domain-specific multi-task optimization with novel retrieval, visual, and language modeling objectives; (2) Asymmetrical dense retrieval architecture combining a small language model for online query processing and a large language model for embedding hotel data; (3) Extensive image processing to handle all property image galleries.", "result": "Experiments on four diverse test sets show HotelMatch-LLM significantly outperforms state-of-the-art models, achieving 0.681 compared to 0.603 for the most effective baseline, MARVEL.", "conclusion": "The analysis highlights the impact of multi-task optimization, the generalizability across LLM architectures, and scalability for processing large image galleries."}}
{"id": "2506.06780", "pdf": "https://arxiv.org/pdf/2506.06780", "abs": "https://arxiv.org/abs/2506.06780", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations", "categories": ["cs.CV", "cs.LG"], "comment": "Extended abstract, presented at the CVPR Workshop on 4D Vision", "summary": "Tracking and forecasting the rotation of objects is fundamental in computer\nvision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor\nobservations can be noisy and sparse, (2) motion patterns can be governed by\ncomplex dynamics, and (3) application settings can demand long-term\nforecasting. This work proposes modeling continuous-time rotational object\ndynamics on $SO(3)$ using Neural Controlled Differential Equations guided by\nSavitzky-Golay paths. Unlike existing methods that rely on simplified motion\nassumptions, our method learns a general latent dynamical system of the\nunderlying object trajectory while respecting the geometric structure of\nrotations. Experimental results on real-world data demonstrate compelling\nforecasting capabilities compared to existing approaches.", "AI": {"tldr": "This paper presents a method for modeling continuous-time rotational object dynamics on SO(3) using Neural Controlled Differential Equations guided by Savitzky-Golay paths, which learns a general latent dynamical system of the underlying object trajectory while respecting the geometric structure of rotations.", "motivation": "Tracking and forecasting the rotation of objects is fundamental in computer vision and robotics, but SO(3) extrapolation remains challenging due to noisy and sparse sensor observations, complex motion dynamics, and long-term forecasting demands.", "method": "The method proposed in this paper models continuous-time rotational object dynamics on SO(3) using Neural Controlled Differential Equations guided by Savitzky-Golay paths. It learns a general latent dynamical system of the underlying object trajectory while respecting the geometric structure of rotations.", "result": "Experimental results on real-world data demonstrate compelling forecasting capabilities compared to existing approaches.", "conclusion": "The proposed method shows promising results in modeling continuous-time rotational object dynamics on SO(3) and provides better forecasting capabilities than existing methods."}}
{"id": "2506.06817", "pdf": "https://arxiv.org/pdf/2506.06817", "abs": "https://arxiv.org/abs/2506.06817", "authors": ["Haoran Wu", "Ce Guo", "Wayne Luk", "Robert Mullins"], "title": "ASPO: Constraint-Aware Bayesian Optimization for FPGA-based Soft Processors", "categories": ["cs.AR", "cs.LG", "cs.NE", "cs.PF"], "comment": "Accepted to International Conference on Field-Programmable Logic and\n  Applications (FPL) 2025", "summary": "Bayesian Optimization (BO) has shown promise in tuning processor design\nparameters. However, standard BO does not support constraints involving\ncategorical parameters such as types of branch predictors and division\ncircuits. In addition, optimization time of BO grows with processor complexity,\nwhich becomes increasingly significant especially for FPGA-based soft\nprocessors. This paper introduces ASPO, an approach that leverages disjunctive\nform to enable BO to handle constraints involving categorical parameters.\nUnlike existing methods that directly apply standard BO, the proposed ASPO\nmethod, for the first time, customizes the mathematical mechanism of BO to\naddress challenges faced by soft-processor designs on FPGAs. Specifically, ASPO\nsupports categorical parameters using a novel customized BO covariance kernel.\nIt also accelerates the design evaluation procedure by penalizing the BO\nacquisition function with potential evaluation time and by reusing FPGA\nsynthesis checkpoints from previously evaluated configurations. ASPO targets\nthree soft processors: RocketChip, BOOM, and EL2 VeeR. The approach is\nevaluated based on seven RISC-V benchmarks. Results show that ASPO can reduce\nexecution time for the ``multiply'' benchmark on the BOOM processor by up to\n35\\% compared to the default configuration. Furthermore, it reduces design time\nfor the BOOM processor by up to 74\\% compared to Boomerang, a state-of-the-art\nhardware-oriented BO approach.", "AI": {"tldr": "ASPO is an approach that modifies Bayesian Optimization to handle categorical parameters and accelerates design evaluation for FPGA-based soft processors, showing significant improvements in execution and design time.", "motivation": "Bayesian Optimization has been effective for tuning processor design parameters, but standard BO does not support constraints with categorical parameters and becomes time-consuming for complex processors.", "method": "ASPO uses a disjunctive form to enable BO handling of categorical parameters, a customized BO covariance kernel for such parameters, penalizes the BO acquisition function with potential evaluation time, and reuses FPGA synthesis checkpoints.", "result": "ASPO reduced execution time for the 'multiply' benchmark on the BOOM processor by up to 35% compared to default settings and cut design time by up to 74% compared to Boomerang.", "conclusion": "ASPO successfully addresses the challenges faced by soft-processor designs on FPGAs through customization of BO, demonstrating effectiveness in reducing both execution and design times."}}
{"id": "2506.06828", "pdf": "https://arxiv.org/pdf/2506.06828", "abs": "https://arxiv.org/abs/2506.06828", "authors": ["Simon P. von der Maase"], "title": "The Currents of Conflict: Decomposing Conflict Trends with Gaussian Processes", "categories": ["stat.ML", "cs.LG", "stat.AP"], "comment": "Total Words: 8122, Total pages: 28, Total figures: 6, Total Tables: 5", "summary": "I present a novel approach to estimating the temporal and spatial patterns of\nviolent conflict. I show how we can use highly temporally and spatially\ndisaggregated data on conflict events in tandem with Gaussian processes to\nestimate temporospatial conflict trends. These trends can be studied to gain\ninsight into conflict traps, diffusion and tempo-spatial conflict exposure in\ngeneral; they can also be used to control for such phenomenons given other\nestimation tasks; lastly, the approach allow us to extrapolate the estimated\ntempo-spatial conflict patterns into future temporal units, thus facilitating\npowerful, stat-of-the-art, conflict forecasts. Importantly, these results are\nachieved via a relatively parsimonious framework using only one data source:\npast conflict patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u51b2\u7a81\u4e8b\u4ef6\u7684\u9ad8\u5ea6\u65f6\u7a7a\u7ec6\u5206\u6570\u636e\u4e0e\u9ad8\u65af\u8fc7\u7a0b\u7ed3\u5408\uff0c\u4f30\u8ba1\u51b2\u7a81\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u8d8b\u52bf\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u52a9\u4e8e\u7406\u89e3\u51b2\u7a81\u9677\u9631\u3001\u6269\u6563\u548c\u65f6\u7a7a\u95f4\u51b2\u7a81\u66b4\u9732\uff0c\u5e76\u80fd\u5bf9\u672a\u6765\u51b2\u7a81\u8fdb\u884c\u9884\u6d4b\uff0c\u4e14\u4ec5\u4f9d\u8d56\u4e8e\u8fc7\u53bb\u7684\u51b2\u7a81\u6a21\u5f0f\u8fd9\u4e00\u6570\u636e\u6e90\u3002", "motivation": "\u73b0\u6709\u7684\u51b2\u7a81\u7814\u7a76\u53ef\u80fd\u7f3a\u4e4f\u5bf9\u65f6\u95f4\u548c\u7a7a\u95f4\u7ec6\u8282\u7684\u5173\u6ce8\uff0c\u65e0\u6cd5\u6709\u6548\u63ed\u793a\u51b2\u7a81\u7684\u8d8b\u52bf\u53ca\u5176\u5f71\u54cd\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u5ea6\u65f6\u7a7a\u7ec6\u5206\u7684\u51b2\u7a81\u4e8b\u4ef6\u6570\u636e\u4e0e\u9ad8\u65af\u8fc7\u7a0b\uff0c\u4f30\u8ba1\u51b2\u7a81\u7684\u65f6\u7a7a\u95f4\u8d8b\u52bf\u3002\u6b64\u65b9\u6cd5\u53ef\u4ee5\u89e3\u6790\u51b2\u7a81\u9677\u9631\u3001\u6269\u6563\u73b0\u8c61\u53ca\u65f6\u7a7a\u95f4\u51b2\u7a81\u66b4\u9732\u60c5\u51b5\uff0c\u540c\u65f6\u53ef\u7528\u4e8e\u5176\u4ed6\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u63a7\u5236\u53d8\u91cf\u6216\u5bf9\u672a\u6765\u51b2\u7a81\u6a21\u5f0f\u8fdb\u884c\u5916\u63a8\u9884\u6d4b\u3002", "result": "\u6210\u529f\u5730\u7528\u76f8\u5bf9\u7b80\u7ea6\u7684\u6846\u67b6\u4f30\u8ba1\u4e86\u51b2\u7a81\u7684\u65f6\u7a7a\u95f4\u8d8b\u52bf\uff0c\u5e76\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u51b2\u7a81\u9884\u6d4b\uff0c\u4ec5\u4f9d\u8d56\u4e8e\u8fc7\u53bb\u51b2\u7a81\u6a21\u5f0f\u7684\u6570\u636e\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u7406\u89e3\u548c\u9884\u6d4b\u51b2\u7a81\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u8fc7\u53bb\u51b2\u7a81\u6a21\u5f0f\u4f5c\u4e3a\u5355\u4e00\u6570\u636e\u6e90\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2506.07315", "pdf": "https://arxiv.org/pdf/2506.07315", "abs": "https://arxiv.org/abs/2506.07315", "authors": ["Zonghan Wu", "Junlin Wang", "Congyuan Zou", "Chenhan Wang", "Yilei Shao"], "title": "Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation", "categories": ["q-fin.ST", "cs.AI"], "comment": null, "summary": "Generative AI, particularly large language models (LLMs), is beginning to\ntransform the financial industry by automating tasks and helping to make sense\nof complex financial information. One especially promising use case is the\nautomatic creation of fundamental analysis reports, which are essential for\nmaking informed investment decisions, evaluating credit risks, guiding\ncorporate mergers, etc. While LLMs attempt to generate these reports from a\nsingle prompt, the risks of inaccuracy are significant. Poor analysis can lead\nto misguided investments, regulatory issues, and loss of trust. Existing\nfinancial benchmarks mainly evaluate how well LLMs answer financial questions\nbut do not reflect performance in real-world tasks like generating financial\nanalysis reports. In this paper, we propose FinAR-Bench, a solid benchmark\ndataset focusing on financial statement analysis, a core competence of\nfundamental analysis. To make the evaluation more precise and reliable, we\nbreak this task into three measurable steps: extracting key information,\ncalculating financial indicators, and applying logical reasoning. This\nstructured approach allows us to objectively assess how well LLMs perform each\nstep of the process. Our findings offer a clear understanding of LLMs current\nstrengths and limitations in fundamental analysis and provide a more practical\nway to benchmark their performance in real-world financial settings.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\uff0c\u7279\u522b\u662f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u6b63\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u4efb\u52a1\u548c\u5e2e\u52a9\u7406\u89e3\u590d\u6742\u7684\u91d1\u878d\u4fe1\u606f\u6765\u6539\u53d8\u91d1\u878d\u884c\u4e1a\u3002\u672c\u6587\u63d0\u51fa\u4e86FinAR-Bench\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8d22\u52a1\u62a5\u8868\u5206\u6790\u7684\u575a\u5b9e\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06\u4efb\u52a1\u5206\u4e3a\u4e09\u4e2a\u53ef\u6d4b\u91cf\u7684\u6b65\u9aa4\uff1a\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u8ba1\u7b97\u8d22\u52a1\u6307\u6807\u548c\u5e94\u7528\u903b\u8f91\u63a8\u7406\uff0c\u4ee5\u66f4\u7cbe\u786e\u548c\u53ef\u9760\u5730\u8bc4\u4f30LLMs\u5728\u5b9e\u9645\u91d1\u878d\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5c3d\u7ba1LLMs\u53ef\u4ee5\u5c1d\u8bd5\u4ece\u5355\u4e00\u63d0\u793a\u751f\u6210\u57fa\u7840\u5206\u6790\u62a5\u544a\uff0c\u4f46\u5176\u4e0d\u51c6\u786e\u6027\u98ce\u9669\u663e\u8457\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bef\u5bfc\u6027\u6295\u8d44\u3001\u76d1\u7ba1\u95ee\u9898\u548c\u4fe1\u4efb\u4e27\u5931\u3002\u73b0\u6709\u7684\u91d1\u878d\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30LLMs\u56de\u7b54\u91d1\u878d\u95ee\u9898\u7684\u80fd\u529b\uff0c\u672a\u80fd\u53cd\u6620\u5176\u5728\u5b9e\u9645\u4efb\u52a1\uff08\u5982\u751f\u6210\u91d1\u878d\u5206\u6790\u62a5\u544a\uff09\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51faFinAR-Bench\uff0c\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8d22\u52a1\u62a5\u8868\u5206\u6790\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u53ef\u6d4b\u91cf\u7684\u6b65\u9aa4\uff1a1\uff09\u63d0\u53d6\u5173\u952e\u4fe1\u606f\uff1b2\uff09\u8ba1\u7b97\u8d22\u52a1\u6307\u6807\uff1b3\uff09\u5e94\u7528\u903b\u8f91\u63a8\u7406\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5f97\u80fd\u591f\u5ba2\u89c2\u8bc4\u4f30LLMs\u5728\u6bcf\u4e2a\u6b65\u9aa4\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63d0\u4f9b\u4e86\u5bf9LLMs\u5728\u57fa\u7840\u5206\u6790\u4e2d\u7684\u5f53\u524d\u4f18\u52bf\u548c\u5c40\u9650\u6027\u7684\u6e05\u6670\u7406\u89e3\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u7684\u65b9\u5f0f\u6765\u8861\u91cf\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u91d1\u878d\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "FinAR-Bench\u4e3a\u8bc4\u4f30LLMs\u5728\u8d22\u52a1\u62a5\u8868\u5206\u6790\u65b9\u9762\u7684\u8868\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u4e14\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86LLMs\u5728\u57fa\u7840\u5206\u6790\u4e2d\u7684\u4f18\u52bf\u4e0e\u4e0d\u8db3\uff0c\u4fc3\u8fdb\u4e86\u66f4\u5b9e\u9645\u7684\u6027\u80fd\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.07323", "pdf": "https://arxiv.org/pdf/2506.07323", "abs": "https://arxiv.org/abs/2506.07323", "authors": ["Haoyuan Yang", "Yue Zhang", "Liqiang Jing"], "title": "Speech Recognition on TV Series with Video-guided Post-Correction", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) has achieved remarkable success with deep\nlearning, driving advancements in conversational artificial intelligence, media\ntranscription, and assistive technologies. However, ASR systems still struggle\nin complex environments such as TV series, where overlapping speech,\ndomain-specific terminology, and long-range contextual dependencies pose\nsignificant challenges to transcription accuracy. Existing multimodal\napproaches fail to correct ASR outputs with the rich temporal and contextual\ninformation available in video. To address this limitation, we propose a novel\nmultimodal post-correction framework that refines ASR transcriptions by\nleveraging contextual cues extracted from video. Our framework consists of two\nstages: ASR Generation and Video-based Post-Correction, where the first stage\nproduces the initial transcript and the second stage corrects errors using\nVideo-based Contextual Information Extraction and Context-aware ASR Correction.\nWe employ the Video-Large Multimodal Model (VLMM) to extract key contextual\ninformation using tailored prompts, which is then integrated with a Large\nLanguage Model (LLM) to refine the ASR output. We evaluate our method on a\nmultimodal benchmark for TV series ASR and demonstrate its effectiveness in\nimproving ASR performance by leveraging video-based context to enhance\ntranscription accuracy in complex multimedia environments.", "AI": {"tldr": "The paper proposes a multimodal post-correction framework for improving ASR in complex environments like TV series, using video-based context to enhance transcription accuracy.", "motivation": "ASR systems struggle with complex environments such as TV series due to overlapping speech, domain-specific terminology, and long-range contextual dependencies. Existing multimodal approaches fail to effectively utilize the rich temporal and contextual information available in video to correct ASR outputs.", "method": "The method involves a two-stage framework: ASR Generation and Video-based Post-Correction. In the first stage, an initial transcript is produced. The second stage uses Video-based Contextual Information Extraction and Context-aware ASR Correction to refine the output. VLMM extracts key contextual information which is integrated with a LLM to improve the ASR output.", "result": "The proposed method was evaluated on a multimodal benchmark for TV series ASR and showed effectiveness in enhancing transcription accuracy in complex multimedia environments by leveraging video-based context.", "conclusion": "A novel multimodal post-correction framework has been developed that successfully improves ASR performance in complex environments such as TV series by utilizing video-based context."}}
{"id": "2506.07326", "pdf": "https://arxiv.org/pdf/2506.07326", "abs": "https://arxiv.org/abs/2506.07326", "authors": ["Brian Christian", "Hannah Rose Kirk", "Jessica A. F. Thompson", "Christopher Summerfield", "Tsvetomira Dumbalska"], "title": "Reward Model Interpretability via Optimal and Pessimal Tokens", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7; H.5.2; J.4; K.4.2"], "comment": "Accepted for publication in Proceedings of the 2025 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT '25), to appear June 2025", "summary": "Reward modeling has emerged as a crucial component in aligning large language\nmodels with human values. Significant attention has focused on using reward\nmodels as a means for fine-tuning generative models. However, the reward models\nthemselves -- which directly encode human value judgments by turning\nprompt-response pairs into scalar rewards -- remain relatively understudied. We\npresent a novel approach to reward model interpretability through exhaustive\nanalysis of their responses across their entire vocabulary space. By examining\nhow different reward models score every possible single-token response to\nvalue-laden prompts, we uncover several striking findings: (i) substantial\nheterogeneity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens, (iii) significant\nsensitivity to prompt framing that mirrors human cognitive biases, and (iv)\novervaluation of more frequent tokens. We demonstrate these effects across ten\nrecent open-source reward models of varying parameter counts and architectures.\nOur results challenge assumptions about the interchangeability of reward\nmodels, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can encode concerning\nbiases toward certain identity groups, which may emerge as unintended\nconsequences of harmlessness training -- distortions that risk propagating\nthrough the downstream large language models now deployed to millions.", "AI": {"tldr": "Reward modeling is vital for aligning language models with human values. This paper explores reward model interpretability by analyzing their responses comprehensively, revealing heterogeneity among models, asymmetries in scoring, sensitivity to prompt framing, and biases towards frequent tokens and identity groups.", "motivation": "To better understand and address the limitations of reward models used in fine-tuning generative models, as they directly encode human value judgments.", "method": "Examine how different reward models score every possible single-token response to value-laden prompts across their entire vocabulary space.", "result": "Uncovered heterogeneity between models trained on similar objectives, asymmetries in scoring high vs low tokens, sensitivity to prompt framing akin to human cognitive biases, and overvaluation of more frequent tokens.", "conclusion": "Reward models are not interchangeable and may encode concerning biases that could propagate through downstream large language models."}}
{"id": "2506.06898", "pdf": "https://arxiv.org/pdf/2506.06898", "abs": "https://arxiv.org/abs/2506.06898", "authors": ["Reese Kneeland", "Paul S. Scotti", "Ghislain St-Yves", "Jesse Breedlove", "Kendrick Kay", "Thomas Naselaris"], "title": "NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.NC"], "comment": "Published at CVPR 2025", "summary": "We release NSD-Imagery, a benchmark dataset of human fMRI activity paired\nwith mental images, to complement the existing Natural Scenes Dataset (NSD), a\nlarge-scale dataset of fMRI activity paired with seen images that enabled\nunprecedented improvements in fMRI-to-image reconstruction efforts. Recent\nmodels trained on NSD have been evaluated only on seen image reconstruction.\nUsing NSD-Imagery, it is possible to assess how well these models perform on\nmental image reconstruction. This is a challenging generalization requirement\nbecause mental images are encoded in human brain activity with relatively lower\nsignal-to-noise and spatial resolution; however, generalization from seen to\nmental imagery is critical for real-world applications in medical domains and\nbrain-computer interfaces, where the desired information is always internally\ngenerated. We provide benchmarks for a suite of recent NSD-trained open-source\nvisual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et\nal.) on NSD-Imagery, and show that the performance of decoding methods on\nmental images is largely decoupled from performance on vision reconstruction.\nWe further demonstrate that architectural choices significantly impact\ncross-decoding performance: models employing simple linear decoding\narchitectures and multimodal feature decoding generalize better to mental\nimagery, while complex architectures tend to overfit visual training data. Our\nfindings indicate that mental imagery datasets are critical for the development\nof practical applications, and establish NSD-Imagery as a useful resource for\nbetter aligning visual decoding methods with this goal.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u53d1\u5e03\u4e86NSD-Imagery\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u73b0\u6709\u6a21\u578b\u5728\u5fc3\u7406\u56fe\u50cf\u91cd\u5efa\u4e0a\u7684\u8868\u73b0\u3002\u901a\u8fc7\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\uff0c\u7b80\u5355\u7ebf\u6027\u89e3\u7801\u67b6\u6784\u548c\u591a\u6a21\u6001\u7279\u5f81\u89e3\u7801\u7684\u6a21\u578b\u5728\u5fc3\u7406\u56fe\u50cf\u4e0a\u7684\u6cdb\u5316\u6027\u80fd\u66f4\u597d\uff0c\u800c\u590d\u6742\u67b6\u6784\u5bb9\u6613\u8fc7\u62df\u5408\u89c6\u89c9\u8bad\u7ec3\u6570\u636e\u3002\u8fd9\u8868\u660e\u5fc3\u7406\u56fe\u50cf\u6570\u636e\u96c6\u5bf9\u4e8e\u5f00\u53d1\u5b9e\u9645\u5e94\u7528\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7684NSD\u6570\u636e\u96c6\u4e3b\u8981\u7528\u4e8e\u89c6\u89c9\u56fe\u50cf\u91cd\u5efa\uff0c\u4f46\u672a\u6d89\u53ca\u5fc3\u7406\u56fe\u50cf\u7684\u91cd\u5efa\u3002\u4e3a\u4e86\u8bc4\u4f30\u6a21\u578b\u5728\u5fc3\u7406\u56fe\u50cf\u91cd\u5efa\u4e0a\u7684\u80fd\u529b\uff0c\u5e76\u63a8\u52a8\u533b\u7597\u548c\u8111\u673a\u63a5\u53e3\u7b49\u9886\u57df\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u3002", "method": "\u53d1\u5e03\u4e86\u4e00\u4e2a\u540d\u4e3aNSD-Imagery\u7684\u65b0\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4eba\u7c7bfMRI\u6d3b\u52a8\u4e0e\u5fc3\u7406\u56fe\u50cf\u7684\u914d\u5bf9\u6570\u636e\u3002\u5229\u7528\u6b64\u6570\u636e\u96c6\uff0c\u5bf9\u591a\u4e2a\u8fd1\u671f\u516c\u5f00\u7684\u89c6\u89c9\u89e3\u7801\u6a21\u578b\uff08MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et al.\uff09\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728\u5fc3\u7406\u56fe\u50cf\u91cd\u5efa\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff0c\u89e3\u7801\u65b9\u6cd5\u5728\u5fc3\u7406\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u4e0e\u5176\u5728\u89c6\u89c9\u91cd\u5efa\u4e0a\u7684\u8868\u73b0\u5f88\u5927\u7a0b\u5ea6\u4e0a\u662f\u8131\u94a9\u7684\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7684\u67b6\u6784\u9009\u62e9\u663e\u8457\u5f71\u54cd\u8de8\u89e3\u7801\u6027\u80fd\uff1a\u7b80\u5355\u7684\u7ebf\u6027\u89e3\u7801\u67b6\u6784\u548c\u591a\u6a21\u6001\u7279\u5f81\u89e3\u7801\u6a21\u578b\u5728\u5fc3\u7406\u56fe\u50cf\u4e0a\u6cdb\u5316\u5f97\u66f4\u597d\uff0c\u800c\u590d\u6742\u67b6\u6784\u5219\u503e\u5411\u4e8e\u8fc7\u62df\u5408\u89c6\u89c9\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u5fc3\u7406\u56fe\u50cf\u6570\u636e\u96c6\u5bf9\u4e8e\u5f00\u53d1\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0cNSD-Imagery\u4e3a\u6539\u8fdb\u89c6\u89c9\u89e3\u7801\u65b9\u6cd5\u4ee5\u66f4\u597d\u5730\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u7528\u7684\u8d44\u6e90\u3002"}}
{"id": "2506.07335", "pdf": "https://arxiv.org/pdf/2506.07335", "abs": "https://arxiv.org/abs/2506.07335", "authors": ["Anyi Wang", "Dong Shu", "Yifan Wang", "Yunpu Ma", "Mengnan Du"], "title": "Improving LLM Reasoning through Interpretable Role-Playing Steering", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 8 figures, 8 tables", "summary": "Role-playing has emerged as an effective technique for enhancing the\nreasoning capabilities of large language models (LLMs). However, existing\nmethods primarily rely on prompt engineering, which often lacks stability and\ninterpretability. In this paper, we introduce Sparse Autoencoder Role-Playing\nSteering (SRPS), a novel framework that identifies and manipulates internal\nmodel features associated with role-playing behavior. Our approach extracts\nlatent representations from role-play prompts, selects the most relevant\nfeatures based on activation patterns, and constructs a steering vector that\ncan be injected into the model's residual stream with controllable intensity.\nOur method enables fine-grained control over role-specific behavior and offers\ninsights into how role information influences internal model activations.\nExtensive experiments across various reasoning benchmarks and model sizes\ndemonstrate consistent performance gains. Notably, in the zero-shot\nchain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves\nfrom 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to\n45.10%. These results highlight the potential of SRPS to enhance reasoning\nability in LLMs, providing better interpretability and stability compared to\ntraditional prompt-based role-playing.", "AI": {"tldr": "This paper introduces Sparse Autoencoder Role-Playing Steering (SRPS), a framework that manipulates internal model features for role-playing in LLMs, offering improved interpretability and stability over prompt-based methods. Experiments show consistent reasoning performance gains.", "motivation": "To address the limitations of existing prompt engineering techniques for role-playing in LLMs, which lack stability and interpretability.", "method": "SRPS extracts latent representations from role-play prompts, selects relevant features based on activation patterns, and constructs a steering vector to control role-specific behavior with adjustable intensity.", "result": "Experiments across various benchmarks and model sizes demonstrate consistent performance improvements. Notably, accuracy improvements are observed in zero-shot chain-of-thought settings for different models.", "conclusion": "SRPS enhances reasoning abilities in LLMs while providing better interpretability and stability compared to traditional prompt-based role-playing."}}
{"id": "2506.06915", "pdf": "https://arxiv.org/pdf/2506.06915", "abs": "https://arxiv.org/abs/2506.06915", "authors": ["Odin Zhang", "Haitao Lin", "Xujun Zhang", "Xiaorui Wang", "Zhenxing Wu", "Qing Ye", "Weibo Zhao", "Jike Wang", "Kejun Ying", "Yu Kang", "Chang-yu Hsieh", "Tingjun Hou"], "title": "Graph Neural Networks in Modern AI-aided Drug Discovery", "categories": ["q-bio.BM", "cs.LG"], "comment": null, "summary": "Graph neural networks (GNNs), as topology/structure-aware models within deep\nlearning, have emerged as powerful tools for AI-aided drug discovery (AIDD). By\ndirectly operating on molecular graphs, GNNs offer an intuitive and expressive\nframework for learning the complex topological and geometric features of\ndrug-like molecules, cementing their role in modern molecular modeling. This\nreview provides a comprehensive overview of the methodological foundations and\nrepresentative applications of GNNs in drug discovery, spanning tasks such as\nmolecular property prediction, virtual screening, molecular generation,\nbiomedical knowledge graph construction, and synthesis planning. Particular\nattention is given to recent methodological advances, including geometric GNNs,\ninterpretable models, uncertainty quantification, scalable graph architectures,\nand graph generative frameworks. We also discuss how these models integrate\nwith modern deep learning approaches, such as self-supervised learning,\nmulti-task learning, meta-learning and pre-training. Throughout this review, we\nhighlight the practical challenges and methodological bottlenecks encountered\nwhen applying GNNs to real-world drug discovery pipelines, and conclude with a\ndiscussion on future directions.", "AI": {"tldr": "Graph neural networks (GNNs) play a significant role in AI-aided drug discovery, providing tools for molecular property prediction, virtual screening, and more. This review covers methodological foundations, applications, recent advances, and future directions of GNNs in drug discovery.", "motivation": "To explore the potential of graph neural networks (GNNs) as topology/structure-aware models within deep learning for AI-aided drug discovery.", "method": "By directly operating on molecular graphs, GNNs learn complex topological and geometric features of drug-like molecules. The review discusses various tasks such as molecular property prediction, virtual screening, molecular generation, biomedical knowledge graph construction, and synthesis planning.", "result": "The review highlights practical challenges and methodological bottlenecks encountered when applying GNNs to real-world drug discovery pipelines, and provides insights into recent methodological advances including geometric GNNs, interpretable models, uncertainty quantification, scalable graph architectures, and graph generative frameworks.", "conclusion": "GNNs have emerged as powerful tools for AI-aided drug discovery, with promising future directions in integrating modern deep learning approaches."}}
{"id": "2506.07339", "pdf": "https://arxiv.org/pdf/2506.07339", "abs": "https://arxiv.org/abs/2506.07339", "authors": ["Kevin Black", "Manuel Y. Galliker", "Sergey Levine"], "title": "Real-Time Execution of Action Chunking Flow Policies", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern AI systems, especially those interacting with the physical world,\nincreasingly require real-time performance. However, the high latency of\nstate-of-the-art generalist models, including recent vision-language action\nmodels (VLAs), poses a significant challenge. While action chunking has enabled\ntemporal consistency in high-frequency control tasks, it does not fully address\nthe latency problem, leading to pauses or out-of-distribution jerky movements\nat chunk boundaries. This paper presents a novel inference-time algorithm that\nenables smooth asynchronous execution of action chunking policies. Our method,\nreal-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out\nof the box with no re-training. It generates the next action chunk while\nexecuting the current one, \"freezing\" actions guaranteed to execute and\n\"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly\ndynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging\nreal-world bimanual manipulation tasks. Results demonstrate that RTC is fast,\nperformant, and uniquely robust to inference delay, significantly improving\ntask throughput and enabling high success rates in precise tasks\n$\\unicode{x2013}$ such as lighting a match $\\unicode{x2013}$ even in the\npresence of significant latency. See\nhttps://pi.website/research/real_time_chunking for videos.", "AI": {"tldr": "\u73b0\u4ee3AI\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u7684\u7cfb\u7edf\uff0c\u8d8a\u6765\u8d8a\u9700\u8981\u5b9e\u65f6\u6027\u80fd\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u65f6\u7b97\u6cd5RTC\uff0c\u53ef\u5b9e\u73b0\u5e73\u6ed1\u5f02\u6b65\u6267\u884c\u52a8\u4f5c\u5757\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u52a8\u6001\u4efb\u52a1\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u901a\u7528\u6a21\u578b\uff0c\u5305\u62ec\u6700\u8fd1\u7684\u89c6\u89c9-\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff08VLAs\uff09\uff0c\u5176\u9ad8\u5ef6\u8fdf\u5bf9\u5b9e\u65f6\u6027\u80fd\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002\u5c3d\u7ba1\u52a8\u4f5c\u5206\u5757\u6709\u52a9\u4e8e\u9ad8\u9891\u63a7\u5236\u4efb\u52a1\u4e2d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u4f46\u672a\u80fd\u5b8c\u5168\u89e3\u51b3\u5ef6\u8fdf\u95ee\u9898\uff0c\u5bfc\u81f4\u5206\u5757\u8fb9\u754c\u5904\u51fa\u73b0\u505c\u987f\u6216\u4e0d\u8fde\u8d2f\u7684\u52a8\u4f5c\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u540d\u4e3a\u5b9e\u65f6\u5206\u5757\uff08RTC\uff09\u7684\u65b0\u9896\u63a8\u7406\u65f6\u7b97\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4efb\u4f55\u6269\u6563\u6216\u6d41\u5f0fVLA\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002\u5b83\u901a\u8fc7\u5728\u6267\u884c\u5f53\u524d\u52a8\u4f5c\u5757\u7684\u540c\u65f6\u751f\u6210\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u5757\uff0c\u201c\u51bb\u7ed3\u201d\u786e\u5b9a\u8981\u6267\u884c\u7684\u52a8\u4f5c\u5e76\u201c\u4fee\u590d\u201d\u5176\u4f59\u90e8\u5206\uff0c\u4ece\u800c\u5b9e\u73b0\u5e73\u6ed1\u5f02\u6b65\u6267\u884c\u3002", "result": "\u901a\u8fc7\u5728Kinetix\u6a21\u62df\u5668\u4e2d\u5f15\u516512\u4e2a\u9ad8\u5ea6\u52a8\u6001\u4efb\u52a1\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u8bc4\u4f306\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u53cc\u81c2\u64cd\u4f5c\u4efb\u52a1\uff0c\u7ed3\u679c\u8868\u660eRTC\u5feb\u901f\u3001\u9ad8\u6548\u4e14\u5bf9\u63a8\u7406\u5ef6\u8fdf\u5177\u6709\u72ec\u7279\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u541e\u5410\u91cf\uff0c\u5e76\u5728\u5b58\u5728\u663e\u8457\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "RTC\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u5206\u5757\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u5c24\u5176\u5728\u7cbe\u786e\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f8b\u5982\u5728\u5b58\u5728\u663e\u8457\u5ef6\u8fdf\u7684\u60c5\u51b5\u4e0b\u6210\u529f\u70b9\u71c3\u706b\u67f4\u3002"}}
{"id": "2506.07347", "pdf": "https://arxiv.org/pdf/2506.07347", "abs": "https://arxiv.org/abs/2506.07347", "authors": ["Armin Lederer", "Erfaun Noorani", "Andreas Krause"], "title": "Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Ensuring safety in multi-agent systems is a significant challenge,\nparticularly in settings where centralized coordination is impractical. In this\nwork, we propose a novel risk-sensitive safety filter for discrete-time\nmulti-agent systems with uncertain dynamics that leverages control barrier\nfunctions (CBFs) defined through value functions. Our approach relies on\ncentralized risk-sensitive safety conditions based on exponential risk\noperators to ensure robustness against model uncertainties. We introduce a\ndistributed formulation of the safety filter by deriving two alternative\nstrategies: one based on worst-case anticipation and another on proximity to a\nknown safe policy. By allowing agents to switch between strategies, feasibility\ncan be ensured. Through detailed numerical evaluations, we demonstrate the\nefficacy of our approach in maintaining safety without being overly\nconservative.", "AI": {"tldr": "This paper proposes a novel risk-sensitive safety filter for discrete-time multi-agent systems with uncertain dynamics, using control barrier functions and centralized risk-sensitive safety conditions. It introduces a distributed formulation of the safety filter through two alternative strategies to ensure feasibility.", "motivation": "Ensuring safety in multi-agent systems is challenging, especially in settings where centralized coordination is impractical.", "method": "The method involves proposing a risk-sensitive safety filter for multi-agent systems that leverages control barrier functions defined through value functions. Centralized risk-sensitive safety conditions based on exponential risk operators are used to ensure robustness against model uncertainties. A distributed formulation of the safety filter is introduced by deriving two alternative strategies: one based on worst-case anticipation and another on proximity to a known safe policy.", "result": "Through detailed numerical evaluations, the approach was shown to be effective in maintaining safety without being overly conservative.", "conclusion": "The proposed risk-sensitive safety filter provides an effective way to ensure safety in multi-agent systems with uncertain dynamics."}}
{"id": "2506.06942", "pdf": "https://arxiv.org/pdf/2506.06942", "abs": "https://arxiv.org/abs/2506.06942", "authors": ["Mohammad Farzanullah", "Han Zhang", "Akram Bin Sediq", "Ali Afana", "Melike Erol-Kantarci"], "title": "Conditional Denoising Diffusion for ISAC Enhanced Channel Estimation in Cell-Free 6G", "categories": ["eess.SP", "cs.LG"], "comment": "IEEE PIMRC conference, 6 pages, 6 figures", "summary": "Cell-free Integrated Sensing and Communication (ISAC) aims to revolutionize\n6th Generation (6G) networks. By combining distributed access points with ISAC\ncapabilities, it boosts spectral efficiency, situational awareness, and\ncommunication reliability. Channel estimation is a critical step in cell-free\nISAC systems to ensure reliable communication, but its performance is usually\nlimited by challenges such as pilot contamination and noisy channel estimates.\nThis paper presents a novel framework leveraging sensing information as a key\ninput within a Conditional Denoising Diffusion Model (CDDM). In this framework,\nwe integrate CDDM with a Multimodal Transformer (MMT) to enhance channel\nestimation in ISAC-enabled cell-free systems. The MMT encoder effectively\ncaptures inter-modal relationships between sensing and location data, enabling\nthe CDDM to iteratively denoise and refine channel estimates. Simulation\nresults demonstrate that the proposed approach achieves significant performance\ngains. As compared with Least Squares (LS) and Minimum Mean Squared Error\n(MMSE) estimators, the proposed model achieves normalized mean squared error\n(NMSE) improvements of 8 dB and 9 dB, respectively. Moreover, we achieve a\n27.8% NMSE improvement compared to the traditional denoising diffusion model\n(TDDM), which does not incorporate sensing channel information. Additionally,\nthe model exhibits higher robustness against pilot contamination and maintains\nhigh accuracy under challenging conditions, such as low signal-to-noise ratios\n(SNRs). According to the simulation results, the model performs well for users\nnear sensing targets by leveraging the correlation between sensing and\ncommunication channels.", "AI": {"tldr": "The paper proposes a novel framework for channel estimation in cell-free ISAC systems, which integrates CDDM with MMT to leverage sensing information and enhance performance.", "motivation": "Cell-free ISAC is crucial for 6G networks, but channel estimation faces challenges like pilot contamination and noisy estimates.", "method": "A new framework combining Conditional Denoising Diffusion Model (CDDM) with Multimodal Transformer (MMT) is introduced. MMT captures relationships between sensing and location data, allowing CDDM to denoise and refine channel estimates iteratively.", "result": "Simulation results show significant NMSE improvements over LS and MMSE estimators (8 dB and 9 dB respectively), a 27.8% improvement over TDDM, higher robustness against pilot contamination, and high accuracy under low SNRs.", "conclusion": "The proposed model effectively leverages the correlation between sensing and communication channels, performing well especially for users near sensing targets."}}
{"id": "2506.07358", "pdf": "https://arxiv.org/pdf/2506.07358", "abs": "https://arxiv.org/abs/2506.07358", "authors": ["Kuiyuan Zhang", "Wenjie Pei", "Rushi Lan", "Yifang Guo", "Zhongyun Hua"], "title": "Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Deepfakes are AI-synthesized multimedia data that may be abused for spreading\nmisinformation. Deepfake generation involves both visual and audio\nmanipulation. To detect audio-visual deepfakes, previous studies commonly\nemploy two relatively independent sub-models to learn audio and visual\nfeatures, respectively, and fuse them subsequently for deepfake detection.\nHowever, this may underutilize the inherent correlations between audio and\nvisual features. Moreover, utilizing two isolated feature learning sub-models\ncan result in redundant neural layers, making the overall model inefficient and\nimpractical for resource-constrained environments.\n  In this work, we design a lightweight network for audio-visual deepfake\ndetection via a single-stream multi-modal learning framework. Specifically, we\nintroduce a collaborative audio-visual learning block to efficiently integrate\nmulti-modal information while learning the visual and audio features. By\niteratively employing this block, our single-stream network achieves a\ncontinuous fusion of multi-modal features across its layers. Thus, our network\nefficiently captures visual and audio features without the need for excessive\nblock stacking, resulting in a lightweight network design. Furthermore, we\npropose a multi-modal classification module that can boost the dependence of\nthe visual and audio classifiers on modality content. It also enhances the\nwhole resistance of the video classifier against the mismatches between audio\nand visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and\nDFDC benchmark datasets. Compared to state-of-the-art audio-visual joint\ndetection methods, our method is significantly lightweight with only 0.48M\nparameters, yet it achieves superiority in both uni-modal and multi-modal\ndeepfakes, as well as in unseen types of deepfakes.", "AI": {"tldr": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u5355\u6d41\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u7f51\u7edc\uff0c\u7528\u4e8e\u97f3\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\uff0c\u901a\u8fc7\u534f\u4f5c\u97f3\u89c6\u9891\u5b66\u4e60\u5757\u548c\u591a\u6a21\u6001\u5206\u7c7b\u6a21\u5757\uff0c\u63d0\u9ad8\u68c0\u6d4b\u6548\u7387\u4e0e\u6548\u679c\uff0c\u53c2\u6570\u4ec5\u4e3a0.48M\uff0c\u4f46\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u97f3\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u4e24\u4e2a\u76f8\u5bf9\u72ec\u7acb\u7684\u5b50\u6a21\u578b\u6765\u5206\u522b\u5b66\u4e60\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u7136\u540e\u8fdb\u884c\u878d\u5408\u68c0\u6d4b\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u672a\u5145\u5206\u5229\u7528\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\u4e4b\u95f4\u7684\u56fa\u6709\u76f8\u5173\u6027\uff0c\u5e76\u4e14\u7531\u4e8e\u4e24\u4e2a\u5b64\u7acb\u7684\u7279\u5f81\u5b66\u4e60\u5b50\u6a21\u578b\u53ef\u80fd\u5bfc\u81f4\u5197\u4f59\u795e\u7ecf\u5c42\uff0c\u4f7f\u5f97\u6574\u4f53\u6a21\u578b\u6548\u7387\u4f4e\u4e0b\uff0c\u4e0d\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u91c7\u7528\u5355\u6d41\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u5177\u4f53\u5f15\u5165\u534f\u4f5c\u97f3\u89c6\u9891\u5b66\u4e60\u5757\u4ee5\u9ad8\u6548\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u540c\u65f6\u5b66\u4e60\u89c6\u89c9\u548c\u97f3\u9891\u7279\u5f81\uff1b\u901a\u8fc7\u8fed\u4ee3\u4f7f\u7528\u8be5\u5757\uff0c\u5355\u6d41\u7f51\u7edc\u5b9e\u73b0\u8de8\u5c42\u7684\u591a\u6a21\u6001\u7279\u5f81\u8fde\u7eed\u878d\u5408\uff1b\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u591a\u6a21\u6001\u5206\u7c7b\u6a21\u5757\uff0c\u589e\u5f3a\u89c6\u89c9\u548c\u97f3\u9891\u5206\u7c7b\u5668\u5bf9\u6a21\u6001\u5185\u5bb9\u7684\u4f9d\u8d56\u6027\uff0c\u63d0\u5347\u89c6\u9891\u5206\u7c7b\u5668\u5bf9\u97f3\u9891\u548c\u89c6\u89c9\u6a21\u6001\u4e0d\u5339\u914d\u7684\u62b5\u6297\u80fd\u529b\u3002", "result": "\u5728DF-TIMIT\u3001FakeAVCeleb\u548cDFDC\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u97f3\u89c6\u9891\u8054\u5408\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6240\u63d0\u65b9\u6cd5\u663e\u8457\u66f4\u8f7b\u91cf\uff08\u4ec50.48M\u53c2\u6570\uff09\uff0c\u5e76\u4e14\u5728\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4ee5\u53ca\u672a\u89c1\u7c7b\u578b\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u5355\u6d41\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u6349\u97f3\u9891\u548c\u89c6\u89c9\u7279\u5f81\uff0c\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff0c\u540c\u65f6\u5728\u591a\u79cd\u7c7b\u578b\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2506.07363", "pdf": "https://arxiv.org/pdf/2506.07363", "abs": "https://arxiv.org/abs/2506.07363", "authors": ["Claudiu Popa", "Rex Pallath", "Liam Cunningham", "Hewad Tahiri", "Abiram Kesavarajah", "Tao Wu"], "title": "Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust", "categories": ["cs.CY", "cs.AI", "I.2.m"], "comment": "12 pages, 13 figures", "summary": "Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on\nDigital Trust. With the increasing accessibility of generative AI, tools for\nvoice cloning, face-swapping, and synthetic media creation have advanced\nsignificantly, lowering both financial and technical barriers for their use.\nWhile these technologies present innovative opportunities, their rapid growth\nraises concerns about trust, privacy, and security. This white paper explores\nthe implications of deepfake technology, analyzing its role in enabling fraud,\nmisinformation, and the erosion of authenticity in multimedia. Using\ncost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we\nexplore how realistic deepfakes can be created with limited resources,\ndemonstrating the risks posed to individuals and organizations alike. By\nanalyzing the technical and ethical challenges of deepfake mitigation and\ndetection, we emphasize the urgent need for regulatory frameworks, public\nawareness, and collaborative efforts to maintain trust in digital media.", "AI": {"tldr": "This paper explores the implications of deepfake technology, demonstrating how realistic deepfakes can be created with limited resources and emphasizing the need for regulatory frameworks, public awareness, and collaborative efforts to maintain trust in digital media.", "motivation": "The motivation is to analyze the role of deepfake technology in enabling fraud, misinformation, and the erosion of authenticity in multimedia due to the increasing accessibility of generative AI.", "method": "The method involves using cost-effective, easy-to-use tools such as Runway, Rope, and ElevenLabs to explore how realistic deepfakes can be created with limited resources.", "result": "The result demonstrates the risks posed by deepfakes to individuals and organizations alike, highlighting the technical and ethical challenges of deepfake mitigation and detection.", "conclusion": "The conclusion emphasizes the urgent need for regulatory frameworks, public awareness, and collaborative efforts to maintain trust in digital media."}}
{"id": "2506.06964", "pdf": "https://arxiv.org/pdf/2506.06964", "abs": "https://arxiv.org/abs/2506.06964", "authors": ["Subhojyoti Mukherjee", "Viet Dac Lai", "Raghavendra Addanki", "Ryan Rossi", "Seunghyun Yoon", "Trung Bui", "Anup Rao", "Jayakumar Subramanian", "Branislav Kveton"], "title": "Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning", "categories": ["cs.CL", "cs.LG"], "comment": "39 pages", "summary": "Question answering (QA) agents automatically answer questions posed in\nnatural language. In this work, we learn to ask clarifying questions in QA\nagents. The key idea in our method is to simulate conversations that contain\nclarifying questions and learn from them using reinforcement learning (RL). To\nmake RL practical, we propose and analyze offline RL objectives that can be\nviewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in\nlarge language models. Our work stands in a stark contrast to recently proposed\nmethods, based on SFT and direct preference optimization, which have additional\nhyper-parameters and do not directly optimize rewards. We compare to these\nmethods empirically and report gains in both optimized rewards and language\nquality.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8ba9\u95ee\u7b54\uff08QA\uff09\u667a\u80fd\u4f53\u5b66\u4f1a\u63d0\u51fa\u6f84\u6e05\u6027\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u4f18\u5316\u5956\u52b1\u548c\u8bed\u8a00\u8d28\u91cf\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u8fdb\u6b65\u3002", "motivation": "\u5f53\u524d\u7684\u95ee\u7b54\u7cfb\u7edf\u867d\u7136\u80fd\u591f\u81ea\u52a8\u56de\u7b54\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u51fa\u7684\u95ee\u9898\uff0c\u4f46\u5728\u9762\u5bf9\u6a21\u7cca\u6216\u4e0d\u5b8c\u6574\u4fe1\u606f\u65f6\uff0c\u7f3a\u4e4f\u4e3b\u52a8\u63d0\u51fa\u6f84\u6e05\u6027\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u4ea4\u4e92\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u5305\u542b\u6f84\u6e05\u6027\u95ee\u9898\u7684\u5bf9\u8bdd\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ece\u4e2d\u5b66\u4e60\u3002\u4e3a\u4e86\u4f7f\u5f3a\u5316\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u884c\uff0c\u63d0\u51fa\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\uff0c\u8fd9\u4e9b\u76ee\u6807\u53ef\u4ee5\u770b\u4f5c\u662f\u5956\u52b1\u52a0\u6743\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u5e76\u53ef\u4ee5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u8f7b\u677e\u4f18\u5316\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u540c\u4e8e\u57fa\u4e8eSFT\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u6700\u8fd1\u63d0\u51fa\u7684\u5176\u4ed6\u65b9\u6cd5\uff0c\u540e\u8005\u5177\u6709\u989d\u5916\u7684\u8d85\u53c2\u6570\u4e14\u4e0d\u76f4\u63a5\u4f18\u5316\u5956\u52b1\u3002", "result": "\u4e0e\u57fa\u4e8eSFT\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u5956\u52b1\u548c\u8bed\u8a00\u8d28\u91cf\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f97\u95ee\u7b54\u667a\u80fd\u4f53\u53ef\u4ee5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b66\u4f1a\u63d0\u51fa\u6f84\u6e05\u6027\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u4ea4\u4e92\u6027\u548c\u51c6\u786e\u6027\u3002\u6b64\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u66f4\u4e3a\u53ef\u884c\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7684SFT\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2506.07364", "pdf": "https://arxiv.org/pdf/2506.07364", "abs": "https://arxiv.org/abs/2506.07364", "authors": ["Chengchao Shen", "Dawei Liu", "Jianxin Wang"], "title": "Multiple Object Stitching for Unsupervised Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Contrastive learning for single object centric images has achieved remarkable\nprogress on unsupervised representation, but suffering inferior performance on\nthe widespread images with multiple objects. In this paper, we propose a simple\nbut effective method, Multiple Object Stitching (MOS), to refine the\nunsupervised representation for multi-object images. Specifically, we construct\nthe multi-object images by stitching the single object centric ones, where the\nobjects in the synthesized multi-object images are predetermined. Hence,\ncompared to the existing contrastive methods, our method provides additional\nobject correspondences between multi-object images without human annotations.\nIn this manner, our method pays more attention to the representations of each\nobject in multi-object image, thus providing more detailed representations for\ncomplicated downstream tasks, such as object detection and semantic\nsegmentation. Experimental results on ImageNet, CIFAR and COCO datasets\ndemonstrate that our proposed method achieves the leading unsupervised\nrepresentation performance on both single object centric images and\nmulti-object ones. The source code is available at\nhttps://github.com/visresearch/MultipleObjectStitching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMultiple Object Stitching (MOS)\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5355\u5bf9\u8c61\u56fe\u50cf\u7f1d\u5408\u4e3a\u591a\u5bf9\u8c61\u56fe\u50cf\u6765\u6539\u8fdb\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\uff0c\u4ece\u800c\u5728\u591a\u5bf9\u8c61\u56fe\u50cf\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u8868\u73b0\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728ImageNet\u3001CIFAR\u548cCOCO\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u9886\u5148\u7684\u65e0\u76d1\u7763\u8868\u793a\u6027\u80fd\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u5728\u5355\u5bf9\u8c61\u4e2d\u5fc3\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u8868\u793a\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u5305\u542b\u591a\u4e2a\u5bf9\u8c61\u7684\u5e7f\u6cdb\u56fe\u50cf\u4e0a\u7684\u8868\u73b0\u8f83\u5dee\u3002\u4e3a\u4e86\u63d0\u9ad8\u591a\u5bf9\u8c61\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u8868\u793a\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u2014\u2014Multiple Object Stitching (MOS)\uff0c\u901a\u8fc7\u5c06\u5355\u5bf9\u8c61\u4e2d\u5fc3\u56fe\u50cf\u7f1d\u5408\u6210\u591a\u5bf9\u8c61\u56fe\u50cf\uff0c\u5176\u4e2d\u5408\u6210\u56fe\u50cf\u4e2d\u7684\u5bf9\u8c61\u662f\u9884\u5148\u786e\u5b9a\u7684\u3002\u8fd9\u79cd\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u6ce8\u91ca\u5373\u53ef\u63d0\u4f9b\u591a\u5bf9\u8c61\u56fe\u50cf\u4e4b\u95f4\u7684\u989d\u5916\u5bf9\u8c61\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728ImageNet\u3001CIFAR\u548cCOCO\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u9886\u5148\u7684\u65e0\u76d1\u7763\u8868\u793a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5355\u5bf9\u8c61\u4e2d\u5fc3\u56fe\u50cf\u548c\u591a\u5bf9\u8c61\u56fe\u50cf\u3002", "conclusion": "Multiple Object Stitching (MOS) \u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u591a\u5bf9\u8c61\u56fe\u50cf\u7684\u65e0\u76d1\u7763\u8868\u793a\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u7684\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5bf9\u8c61\u68c0\u6d4b\u548c\u8bed\u4e49\u5206\u5272\uff09\u63d0\u4f9b\u4e86\u66f4\u8be6\u7ec6\u7684\u8868\u793a\u3002"}}
{"id": "2506.07368", "pdf": "https://arxiv.org/pdf/2506.07368", "abs": "https://arxiv.org/abs/2506.07368", "authors": ["Jiaying He", "Yitong Lin", "Jiahe Chen", "Honghui Xu", "Jianwei Zheng"], "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures, ICME2025", "summary": "For the immanent challenge of insufficiently annotated samples in the medical\nfield, semi-supervised medical image segmentation (SSMIS) offers a promising\nsolution. Despite achieving impressive results in delineating primary target\nareas, most current methodologies struggle to precisely capture the subtle\ndetails of boundaries. This deficiency often leads to significant diagnostic\ninaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised\nsegmentation model that synergistically integrates complementary competition\nand contrastive selection. This design significantly sharpens boundary\ndelineation and enhances overall precision. Specifically, we develop an\n$\\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining\nboundary localization. Additionally, we incorporate a $\\textit{Dynamic\nComplementary Competition}$ module that leverages two high-performing\nsub-networks to generate pseudo-labels, thereby further improving segmentation\nquality. The proposed C3S3 undergoes rigorous validation on two publicly\naccessible datasets, encompassing the practices of both MRI and CT scans. The\nresults demonstrate that our method achieves superior performance compared to\nprevious cutting-edge competitors. Especially, on the 95HD and ASD metrics, our\napproach achieves a notable improvement of at least $6\\%$, highlighting the\nsignificant advancements. The code is available at\nhttps://github.com/Y-TARL/C3S3.", "AI": {"tldr": "In response to the challenge of insufficient annotations in medical imaging, this paper presents C3S3, a semi-supervised segmentation model enhancing boundary delineation and overall precision via Outcome-Driven Contrastive Learning and Dynamic Complementary Competition modules. It outperforms previous methods by at least 6% on key metrics.", "motivation": "The motivation is addressing the issue of insufficiently annotated samples in medical imaging and improving the precision of boundary delineation in semi-supervised medical image segmentation beyond current methodologies.", "method": "C3S3 integrates complementary competition and contrastive selection. It features an Outcome-Driven Contrastive Learning module for refining boundary localization and a Dynamic Complementary Competition module using two sub-networks to generate pseudo-labels, thereby improving segmentation quality.", "result": "C3S3 demonstrates superior performance compared to previous state-of-the-art methods, with a significant improvement of at least 6% on the 95HD and ASD metrics when validated on public MRI and CT datasets.", "conclusion": "C3S3 successfully enhances the precision of medical image segmentation, particularly in boundary delineation, offering a significant advancement over prior art."}}
{"id": "2506.06989", "pdf": "https://arxiv.org/pdf/2506.06989", "abs": "https://arxiv.org/abs/2506.06989", "authors": ["Md Aminul Islam", "Kathryn Vasilaky", "Elena Zheleva"], "title": "Correcting for Position Bias in Learning to Rank: A Control Function Approach", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Implicit feedback data, such as user clicks, is commonly used in\nlearning-to-rank (LTR) systems because it is easy to collect and it often\nreflects user preferences. However, this data is prone to various biases, and\ntraining an LTR system directly on biased data can result in suboptimal ranking\nperformance. One of the most prominent and well-studied biases in implicit\nfeedback data is position bias, which occurs because users are more likely to\ninteract with higher-ranked documents regardless of their true relevance. In\nthis paper, we propose a novel control function-based method that accounts for\nposition bias in a two-stage process. The first stage uses exogenous variation\nfrom the residuals of the ranking process to correct for position bias in the\nsecond stage click equation. Unlike previous position bias correction methods,\nour method does not require knowledge of the click or propensity model and\nallows for nonlinearity in the underlying ranking model. Moreover, our method\nis general and allows for debiasing any state-of-the-art ranking algorithm by\nplugging it into the second stage. We also introduce a technique to debias\nvalidation clicks for hyperparameter tuning to select the optimal model in the\nabsence of unbiased validation data. Experimental results demonstrate that our\nmethod outperforms state-of-the-art approaches in correcting for position bias.", "AI": {"tldr": "The paper proposes a novel control function-based method to correct position bias in learning-to-rank systems without needing the click or propensity model, allowing nonlinearity and debiasing any ranking algorithm.", "motivation": "Implicit feedback data used in LTR systems is prone to position bias, which affects the accuracy of the rankings. Current methods for correcting this bias require knowledge of the click or propensity model and may not allow for nonlinearity in the underlying ranking model.", "method": "A two-stage process is introduced where the first stage uses exogenous variation from residuals of the ranking process to correct position bias in the second stage click equation. The method does not need the click or propensity model and allows nonlinearity in the ranking model. It can also be applied to any state-of-the-art ranking algorithm.", "result": "Experimental results indicate that the proposed method outperforms current state-of-the-art approaches in correcting position bias.", "conclusion": "The novel control function-based method effectively corrects position bias in LTR systems without requiring specific models and can enhance any ranking algorithm."}}
{"id": "2506.07373", "pdf": "https://arxiv.org/pdf/2506.07373", "abs": "https://arxiv.org/abs/2506.07373", "authors": ["Enqiang Zhu", "Yu Zhang", "Haopeng Sun", "Ziqi Wei", "Witold Pedrycz", "Chanjuan Liu", "Jin Xu"], "title": "HyColor: An Efficient Heuristic Algorithm for Graph Coloring", "categories": ["cs.DM", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "The graph coloring problem (GCP) is a classic combinatorial optimization\nproblem that aims to find the minimum number of colors assigned to vertices of\na graph such that no two adjacent vertices receive the same color. GCP has been\nextensively studied by researchers from various fields, including mathematics,\ncomputer science, and biological science. Due to the NP-hard nature, many\nheuristic algorithms have been proposed to solve GCP. However, existing GCP\nalgorithms focus on either small hard graphs or large-scale sparse graphs (with\nup to 10^7 vertices). This paper presents an efficient hybrid heuristic\nalgorithm for GCP, named HyColor, which excels in handling large-scale sparse\ngraphs while achieving impressive results on small dense graphs. The efficiency\nof HyColor comes from the following three aspects: a local decision strategy to\nimprove the lower bound on the chromatic number; a graph-reduction strategy to\nreduce the working graph; and a k-core and mixed degree-based greedy heuristic\nfor efficiently coloring graphs. HyColor is evaluated against three\nstate-of-the-art GCP algorithms across four benchmarks, comprising three\nlarge-scale sparse graph benchmarks and one small dense graph benchmark,\ntotaling 209 instances. The results demonstrate that HyColor consistently\noutperforms existing heuristic algorithms in both solution accuracy and\ncomputational efficiency for the majority of instances. Notably, HyColor\nachieved the best solutions in 194 instances (over 93%), with 34 of these\nsolutions significantly surpassing those of other algorithms. Furthermore,\nHyColor successfully determined the chromatic number and achieved optimal\ncoloring in 128 instances.", "AI": {"tldr": "The paper introduces HyColor, an efficient hybrid heuristic algorithm for solving the graph coloring problem (GCP), which is particularly effective for large-scale sparse graphs and small dense graphs. It incorporates a local decision strategy, a graph-reduction strategy, and a k-core and mixed degree-based greedy heuristic. Evaluated against three state-of-the-art algorithms across 209 instances, HyColor consistently outperforms existing methods in solution accuracy and computational efficiency, achieving the best solutions in 194 instances (over 93%) and optimal coloring in 128 instances.", "motivation": "To address the limitations of existing GCP algorithms that focus on either small hard graphs or large-scale sparse graphs, by proposing a more versatile and efficient algorithm for handling both types of graphs.", "method": "HyColor uses three key strategies: a local decision strategy to improve the lower bound on the chromatic number, a graph-reduction strategy to reduce the working graph, and a k-core and mixed degree-based greedy heuristic for efficiently coloring graphs.", "result": "HyColor outperformed three state-of-the-art GCP algorithms in both solution accuracy and computational efficiency for the majority of instances tested. It achieved the best solutions in 194 instances (over 93%), with 34 of these solutions significantly surpassing those of other algorithms. Additionally, it successfully determined the chromatic number and achieved optimal coloring in 128 instances.", "conclusion": "HyColor is an efficient hybrid heuristic algorithm for GCP that excels in handling large-scale sparse graphs while also achieving impressive results on small dense graphs. It demonstrates superior performance compared to existing algorithms in terms of solution quality and computational efficiency."}}
{"id": "2506.07376", "pdf": "https://arxiv.org/pdf/2506.07376", "abs": "https://arxiv.org/abs/2506.07376", "authors": ["Jintao Tong", "Ran Ma", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the\nmodel on a source-domain dataset with sufficient samples, and then transfer the\nmodel to target-domain datasets where only a few samples are available for\nefficient fine-tuning. There are majorly two challenges in this task: (1) the\ndomain gap and (2) fine-tuning with scarce data. To solve these challenges, we\nrevisit the adapter-based methods, and discover an intriguing insight not\nexplored in previous works: the adapter not only helps the fine-tuning of\ndownstream tasks but also naturally serves as a domain information decoupler.\nThen, we delve into this finding for an interpretation, and find the model's\ninherent structure could lead to a natural decoupling of domain information.\nBuilding upon this insight, we propose the Domain Feature Navigator (DFN),\nwhich is a structure-based decoupler instead of loss-based ones like current\nworks, to capture domain-specific information, thereby directing the model's\nattention towards domain-agnostic knowledge. Moreover, to prevent the potential\nexcessive overfitting of DFN during the source-domain training, we further\ndesign the SAM-SVN method to constrain DFN from learning sample-specific\nknowledge. On target domains, we freeze the model and fine-tune the DFN to\nlearn target-specific knowledge specific. Extensive experiments demonstrate\nthat our method surpasses the state-of-the-art method in CD-FSS significantly\nby 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.", "AI": {"tldr": "The paper proposes Domain Feature Navigator (DFN) for cross-domain few-shot segmentation, which surpasses state-of-the-art methods by 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios respectively.", "motivation": "Existing methods for cross-domain few-shot segmentation face challenges with domain gaps and fine-tuning with scarce data.", "method": "The paper revisits adapter-based methods and proposes DFN, a structure-based decoupler to capture domain-specific information while directing the model's attention towards domain-agnostic knowledge. It also introduces SAM-SVN to prevent overfitting during source-domain training.", "result": "The method significantly outperforms state-of-the-art techniques with improvements of 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios respectively.", "conclusion": "DFN is an effective approach for cross-domain few-shot segmentation that addresses domain gaps and limited data challenges."}}
{"id": "2506.07011", "pdf": "https://arxiv.org/pdf/2506.07011", "abs": "https://arxiv.org/abs/2506.07011", "authors": ["Yuan-Hao Wei", "Yan-Jie Sun"], "title": "Half-AVAE: Adversarial-Enhanced Factorized and Structured Encoder-Free VAE for Underdetermined Independent Component Analysis", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "This study advances the Variational Autoencoder (VAE) framework by addressing\nchallenges in Independent Component Analysis (ICA) under both determined and\nunderdetermined conditions, focusing on enhancing the independence and\ninterpretability of latent variables. Traditional VAEs map observed data to\nlatent variables and back via an encoder-decoder architecture, but struggle\nwith underdetermined ICA where the number of latent variables exceeds observed\nsignals. The proposed Half Adversarial VAE (Half-AVAE) builds on the\nencoder-free Half-VAE framework, eliminating explicit inverse mapping to tackle\nunderdetermined scenarios. By integrating adversarial networks and External\nEnhancement (EE) terms, Half-AVAE promotes mutual independence among latent\ndimensions, achieving factorized and interpretable representations. Experiments\nwith synthetic signals demonstrate that Half-AVAE outperforms baseline models,\nincluding GP-AVAE and Half-VAE, in recovering independent components under\nunderdetermined conditions, as evidenced by lower root mean square errors. The\nstudy highlights the flexibility of VAEs in variational inference, showing that\nencoder omission, combined with adversarial training and structured priors,\nenables effective solutions for complex ICA tasks, advancing applications in\ndisentanglement, causal inference, and generative modeling.", "AI": {"tldr": "This study proposes Half Adversarial VAE (Half-AVAE), an advancement in the VAE framework for Independent Component Analysis (ICA). It eliminates explicit inverse mapping and integrates adversarial networks and External Enhancement terms to promote mutual independence among latent dimensions, achieving better factorized and interpretable representations. Experiments show that Half-AVAE outperforms baseline models in recovering independent components under underdetermined conditions.", "motivation": "The motivation is to address the challenges in Independent Component Analysis (ICA) under both determined and underdetermined conditions within the Variational Autoencoder (VAE) framework, specifically enhancing the independence and interpretability of latent variables.", "method": "The proposed method is the Half Adversarial VAE (Half-AVAE), which builds on the encoder-free Half-VAE framework. It integrates adversarial networks and External Enhancement (EE) terms to promote mutual independence among latent dimensions.", "result": "Experiments with synthetic signals demonstrate that Half-AVAE outperforms baseline models, including GP-AVAE and Half-VAE, in recovering independent components under underdetermined conditions, as evidenced by lower root mean square errors.", "conclusion": "The study concludes that the flexibility of VAEs in variational inference, combined with adversarial training and structured priors, enables effective solutions for complex ICA tasks, advancing applications in disentanglement, causal inference, and generative modeling."}}
{"id": "2506.07388", "pdf": "https://arxiv.org/pdf/2506.07388", "abs": "https://arxiv.org/abs/2506.07388", "authors": ["Yun Hua", "Haosheng Chen", "Shiqin Wang", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show strong collaborative performance in\nmulti-agent systems with predefined roles and workflows. However, in open-ended\nenvironments lacking coordination rules, agents tend to act in self-interested\nways. The central challenge in achieving coordination lies in credit assignment\n-- fairly evaluating each agent's contribution and designing pricing mechanisms\nthat align their heterogeneous goals. This problem is critical as LLMs\nincreasingly participate in complex human-AI collaborations, where fair\ncompensation and accountability rely on effective pricing mechanisms. Inspired\nby how human societies address similar coordination challenges (e.g., through\ntemporary collaborations such as employment or subcontracting), we propose a\ncooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley\nChain-of-Thought -- leveraging marginal contributions as a principled basis for\npricing -- with structured negotiation protocols for effective price matching,\nenabling LLM agents to coordinate through rational task-time pricing and\npost-task reward redistribution. This approach aligns agent incentives, fosters\ncooperation, and maintains autonomy. We evaluate Shapley-Coop across two\nmulti-agent games and a software engineering simulation, demonstrating that it\nconsistently enhances LLM agent collaboration and facilitates equitable credit\nassignment. These results highlight the effectiveness of Shapley-Coop's pricing\nmechanisms in accurately reflecting individual contributions during task\nexecution.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u534f\u4f5c\u6027\u80fd\uff0c\u4f46\u5728\u7f3a\u4e4f\u534f\u8c03\u89c4\u5219\u7684\u5f00\u653e\u73af\u5883\u4e2d\uff0c\u667a\u80fd\u4f53\u503e\u5411\u4e8e\u4ee5\u81ea\u6211\u5229\u76ca\u4e3a\u5bfc\u5411\u884c\u52a8\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u53d7\u4eba\u7c7b\u793e\u4f1a\u534f\u8c03\u6311\u6218\u89e3\u51b3\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u672c\u6587\u63d0\u51fa\u4e86Shapley-Coop\u5408\u4f5c\u6d41\u7a0b\uff0c\u901a\u8fc7\u7406\u6027\u4efb\u52a1\u65f6\u95f4\u5b9a\u4ef7\u548c\u4efb\u52a1\u540e\u5956\u52b1\u518d\u5206\u914d\uff0c\u589e\u5f3a\u4e86LLM\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\uff0c\u5e76\u4fc3\u8fdb\u4e86\u516c\u5e73\u7684\u4fe1\u7528\u5206\u914d\u3002", "motivation": "\u5728\u5f00\u653e\u73af\u5883\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u91cc\uff0c\u667a\u80fd\u4f53\u5f80\u5f80\u4ee5\u81ea\u6211\u5229\u76ca\u4e3a\u5bfc\u5411\u884c\u52a8\uff0c\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u534f\u8c03\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u63a2\u7d22\u4e00\u79cd\u80fd\u591f\u516c\u5e73\u8bc4\u4f30\u6bcf\u4e2a\u667a\u80fd\u4f53\u8d21\u732e\u5e76\u8bbe\u8ba1\u5408\u7406\u5b9a\u4ef7\u673a\u5236\u7684\u65b9\u6cd5\uff0c\u4ee5\u4fc3\u8fdb\u590d\u6742\u7684\u4eba\u5de5\u667a\u80fd\u4e0e\u4eba\u7c7b\u534f\u4f5c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aShapley-Coop\u7684\u5408\u4f5c\u6d41\u7a0b\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86Shapley Chain-of-Thought\uff08\u5229\u7528\u8fb9\u9645\u8d21\u732e\u4f5c\u4e3a\u5b9a\u4ef7\u7684\u57fa\u672c\u539f\u5219\uff09\u548c\u7ed3\u6784\u5316\u8c08\u5224\u534f\u8bae\uff0c\u7528\u4e8e\u6709\u6548\u7684\u4ef7\u683c\u5339\u914d\uff0c\u4f7fLLM\u667a\u80fd\u4f53\u53ef\u4ee5\u901a\u8fc7\u7406\u6027\u4efb\u52a1\u65f6\u95f4\u5b9a\u4ef7\u548c\u4efb\u52a1\u540e\u5956\u52b1\u518d\u5206\u914d\u8fdb\u884c\u534f\u8c03\u3002", "result": "\u5728\u4e24\u4e2a\u591a\u667a\u80fd\u4f53\u6e38\u620f\u548c\u4e00\u4e2a\u8f6f\u4ef6\u5de5\u7a0b\u6a21\u62df\u4e2d\u7684\u8bc4\u4f30\u8868\u660e\uff0cShapley-Coop\u663e\u8457\u63d0\u9ad8\u4e86LLM\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u5e76\u4fc3\u8fdb\u4e86\u516c\u5e73\u7684\u4fe1\u7528\u5206\u914d\u3002\u7ed3\u679c\u8bc1\u660e\uff0cShapley-Coop\u7684\u5b9a\u4ef7\u673a\u5236\u80fd\u51c6\u786e\u53cd\u6620\u4e2a\u4f53\u5728\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u8d21\u732e\u3002", "conclusion": "Shapley-Coop\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5408\u7406\u7684\u5b9a\u4ef7\u673a\u5236\u6765\u534f\u8c03LLM\u667a\u80fd\u4f53\u7684\u884c\u4e3a\uff0c\u589e\u5f3a\u5176\u534f\u4f5c\u6027\u5e76\u786e\u4fdd\u516c\u5e73\u7684\u4fe1\u7528\u5206\u914d\uff0c\u4ece\u800c\u63a8\u52a8\u66f4\u9ad8\u6548\u7684\u4eba\u5de5\u667a\u80fd\u4e0e\u4eba\u7c7b\u534f\u4f5c\u3002"}}
{"id": "2506.07015", "pdf": "https://arxiv.org/pdf/2506.07015", "abs": "https://arxiv.org/abs/2506.07015", "authors": ["Qiyu Hou", "Jun Wang"], "title": "TABLET: Table Structure Recognition using Encoder-only Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "ICDAR 2025", "summary": "To address the challenges of table structure recognition, we propose a novel\nSplit-Merge-based top-down model optimized for large, densely populated tables.\nOur approach formulates row and column splitting as sequence labeling tasks,\nutilizing dual Transformer encoders to capture feature interactions. The\nmerging process is framed as a grid cell classification task, leveraging an\nadditional Transformer encoder to ensure accurate and coherent merging. By\neliminating unstable bounding box predictions, our method reduces resolution\nloss and computational complexity, achieving high accuracy while maintaining\nfast processing speed. Extensive experiments on FinTabNet and PubTabNet\ndemonstrate the superiority of our model over existing approaches, particularly\nin real-world applications. Our method offers a robust, scalable, and efficient\nsolution for large-scale table recognition, making it well-suited for\nindustrial deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Split-Merge\u6a21\u578b\uff0c\u4f18\u5316\u4e86\u5bf9\u5927\u800c\u5bc6\u96c6\u8868\u683c\u7684\u7ed3\u6784\u8bc6\u522b\uff0c\u901a\u8fc7\u6d88\u9664\u4e0d\u7a33\u5b9a\u8fb9\u754c\u6846\u9884\u6d4b\uff0c\u51cf\u5c11\u5206\u8fa8\u7387\u635f\u5931\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u548c\u5feb\u901f\u5904\u7406\u901f\u5ea6\u3002\u5728FinTabNet\u548cPubTabNet\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u6a21\u578b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5408\u5de5\u4e1a\u90e8\u7f72\u3002", "motivation": "\u5f53\u524d\u8868\u683c\u7ed3\u6784\u8bc6\u522b\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u4e14\u5bc6\u96c6\u7684\u8868\u683c\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u548c\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u884c\u548c\u5217\u5206\u5272\u89c6\u4e3a\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\uff0c\u4f7f\u7528\u53ccTransformer\u7f16\u7801\u5668\u6355\u6349\u7279\u5f81\u4ea4\u4e92\uff1b\u5c06\u5408\u5e76\u8fc7\u7a0b\u4f5c\u4e3a\u7f51\u683c\u5355\u5143\u5206\u7c7b\u4efb\u52a1\uff0c\u4f7f\u7528\u989d\u5916\u7684Transformer\u7f16\u7801\u5668\u786e\u4fdd\u51c6\u786e\u548c\u4e00\u81f4\u7684\u5408\u5e76\u3002", "result": "\u5728FinTabNet\u548cPubTabNet\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u672c\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8868\u683c\u8bc6\u522b\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u53ef\u6269\u5c55\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u975e\u5e38\u9002\u5408\u5de5\u4e1a\u90e8\u7f72\u3002"}}
{"id": "2506.07399", "pdf": "https://arxiv.org/pdf/2506.07399", "abs": "https://arxiv.org/abs/2506.07399", "authors": ["Peiru Yang", "Jinhua Yin", "Haoran Zheng", "Xueying Bai", "Huili Wang", "Yufei Sun", "Xintian Li", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses.", "AI": {"tldr": "Multimodal RAG systems, which enhance large vision-language models by integrating cross-modal knowledge, are vulnerable to privacy attacks such as membership inference attacks (MIAs). Existing MIA methods mainly focus on textual modality. This paper proposes MrM, the first black-box MIA framework for multimodal RAG systems, utilizing a multi-object data perturbation framework constrained by counterfactual attacks. It induces RAG systems to retrieve target data and generate information leaking membership details. The method employs an object-aware data perturbation approach and a counterfact-informed mask selection strategy. Experiments show that MrM performs strongly across evaluations and remains robust under defenses.", "motivation": "Multimodal RAG systems are increasingly adopted in real-world tasks but are potentially vulnerable to privacy attacks, especially MIAs. Current MIA methods primarily target the textual modality, leaving the visual modality relatively unexplored.", "method": "The proposed MrM framework uses a multi-object data perturbation framework constrained by counterfactual attacks. It includes an object-aware data perturbation method to ensure successful retrieval and a counterfact-informed mask selection strategy to prioritize informative masked regions. Statistical membership inference is performed by modeling query trials.", "result": "MrM achieves consistently strong performance in experiments on two visual datasets and eight commercial visual-language models across both sample-level and set-level evaluations. It also remains robust under adaptive defenses.", "conclusion": "MrM is the first effective black-box MIA framework for multimodal RAG systems, demonstrating strong performance and robustness against defenses."}}
{"id": "2506.07069", "pdf": "https://arxiv.org/pdf/2506.07069", "abs": "https://arxiv.org/abs/2506.07069", "authors": ["Zhican Wang", "Guanghui He", "Dantong Liu", "Lingjun Gao", "Shell Xu Hu", "Chen Zhang", "Zhuoran Song", "Nicholas Lane", "Wayne Luk", "Hongxiang Fan"], "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization", "categories": ["cs.GR", "cs.AR", "cs.CV", "cs.LG"], "comment": "Preprint. Under review", "summary": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field.", "AI": {"tldr": "3D Gaussian Splatting\u5c3d\u7ba1\u7b97\u6cd5\u6027\u80fd\u51fa\u8272\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u4ecd\u5177\u6311\u6218\u3002\u672c\u6587\u63d0\u51fa\u67b6\u6784-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u89e3\u51b3\u6b64\u95ee\u9898\uff1a1) \u63d0\u51fa\u8f74\u5411\u5149\u6805\u5316\u51cf\u5c11\u91cd\u590d\u8ba1\u7b97\uff1b2) \u5f15\u5165\u795e\u7ecf\u6392\u5e8f\u65b9\u6cd5\u63d0\u9ad8\u6548\u7387\uff1b3) \u8bbe\u8ba1\u53ef\u91cd\u6784\u5904\u7406\u9635\u5217\u548c\u03c0-trajectory\u74e6\u7247\u8c03\u5ea6\u4f18\u5316\u786c\u4ef6\u4f7f\u7528\u548c\u5185\u5b58\u8bbf\u95ee\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8bbe\u8ba1\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901f\u5ea6\u548c\u80fd\u6548\u3002", "motivation": "3D Gaussian Splatting\u5728\u9ad8\u8d28\u91cf\u548c\u9ad8\u6548\u7684\u89c6\u56fe\u5408\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u53d7\u5230\u529f\u7387\u548c\u9762\u79ef\u9884\u7b97\u7684\u9650\u5236\uff0c\u5b58\u5728\u91cd\u5927\u6311\u6218\u3002", "method": "1. \u63ed\u793a\u4f20\u7edf\u5149\u6805\u5316\u4e2d\u7684\u5197\u4f59\uff0c\u5e76\u63d0\u51fa\u8f74\u5411\u5149\u6805\u5316\u4ee5\u9884\u5148\u8ba1\u7b97\u5e76\u91cd\u7528\u5171\u4eab\u9879\uff0c\u4ece\u800c\u51cf\u5c11\u4e58\u6cd5\u548c\u52a0\u6cd5\u64cd\u4f5c\u3002\n2. \u9488\u5bf9\u6392\u5e8f\u8fc7\u7a0b\u4e2d\u7684\u8d44\u6e90\u548c\u6027\u80fd\u4f4e\u6548\uff0c\u5f15\u5165\u795e\u7ecf\u6392\u5e8f\u65b9\u6cd5\uff0c\u901a\u8fc7\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u987a\u5e8f\u65e0\u5173\u7684\u6df7\u5408\u6743\u91cd\uff0c\u53d6\u4ee3\u6602\u8d35\u7684\u786c\u4ef6\u6392\u5e8f\u5668\u3002\n3. \u8bbe\u8ba1\u53ef\u91cd\u6784\u5904\u7406\u9635\u5217\u4ee5\u652f\u6301\u5149\u6805\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\uff0c\u6700\u5927\u5316\u786c\u4ef6\u5229\u7528\u7387\u548c\u541e\u5410\u91cf\u3002\n4. \u5f15\u5165\u03c0-trajectory\u74e6\u7247\u8c03\u5ea6\uff0c\u4f18\u5316\u9ad8\u65af\u590d\u7528\u5e76\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\u3002", "result": "\u76f8\u6bd4\u8fb9\u7f18GPU\uff0c\u8be5\u8bbe\u8ba1\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u4e8623.4\u81f327.8\u500d\u7684\u901f\u5ea6\u63d0\u5347\u548c28.8\u81f351.4\u500d\u7684\u80fd\u8017\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u67b6\u6784-\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e863D Gaussian Splatting\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6\u6e32\u67d3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u901f\u5ea6\u548c\u80fd\u6548\uff0c\u4e3a\u672a\u6765\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u5f00\u6e90\u57fa\u7840\u3002"}}
{"id": "2506.07400", "pdf": "https://arxiv.org/pdf/2506.07400", "abs": "https://arxiv.org/abs/2506.07400", "authors": ["Philip Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages, 6 figures. Accepted to the 2025 IEEE 8th International\n  Conference on Multimedia Information Processing and Retrieval (MIPR). Code\n  and platform available at https://github.com/Purdue-M2/MedChat", "summary": "The integration of deep learning-based glaucoma detection with large language\nmodels (LLMs) presents an automated strategy to mitigate ophthalmologist\nshortages and improve clinical reporting efficiency. However, applying general\nLLMs to medical imaging remains challenging due to hallucinations, limited\ninterpretability, and insufficient domain-specific medical knowledge, which can\npotentially reduce clinical accuracy. Although recent approaches combining\nimaging models with LLM reasoning have improved reporting, they typically rely\non a single generalist agent, restricting their capacity to emulate the diverse\nand complex reasoning found in multidisciplinary medical teams. To address\nthese limitations, we propose MedChat, a multi-agent diagnostic framework and\nplatform that combines specialized vision models with multiple role-specific\nLLM agents, all coordinated by a director agent. This design enhances\nreliability, reduces hallucination risk, and enables interactive diagnostic\nreporting through an interface tailored for clinical review and educational\nuse. Code available at https://github.com/Purdue-M2/MedChat.", "AI": {"tldr": "An automated strategy using deep learning for glaucoma detection integrated with large language models (LLMs) to reduce ophthalmologist shortages and improve clinical reporting efficiency is proposed. However, general LLMs face challenges in medical imaging such as hallucinations, limited interpretability, and insufficient domain-specific knowledge which can decrease clinical accuracy. While combining imaging models with LLM reasoning has improved reporting, reliance on a single generalist agent restricts their ability to emulate multidisciplinary medical team reasoning. To solve these issues, MedChat, a multi-agent diagnostic framework that combines specialized vision models with multiple role-specific LLM agents coordinated by a director agent, is proposed to enhance reliability, reduce hallucination risk, and enable interactive diagnostic reporting through a tailored interface for clinical review and education.", "motivation": "The motivation behind this paper is to address the limitations of applying general LLMs to medical imaging, including hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Additionally, there is a need to overcome the restriction of relying on a single generalist agent that limits the capacity to emulate diverse and complex reasoning found in multidisciplinary medical teams.", "method": "The method proposed in this paper is MedChat, a multi-agent diagnostic framework and platform that integrates specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This approach aims to enhance reliability, reduce hallucination risk, and enable interactive diagnostic reporting through an interface designed for clinical review and educational use.", "result": "The result of this paper is the development of MedChat, which promises to improve the reliability of automated clinical reporting, reduce the risk of hallucinations, and provide an interactive platform for diagnostic reporting suitable for both clinical review and educational purposes.", "conclusion": "In conclusion, MedChat represents an advancement in integrating specialized vision models with multiple role-specific LLM agents to overcome the limitations of general LLMs in medical imaging. By employing a multi-agent system coordinated by a director agent, MedChat enhances reliability, reduces hallucination risks, and offers an interactive diagnostic reporting platform suited for clinical and educational applications."}}
{"id": "2506.07083", "pdf": "https://arxiv.org/pdf/2506.07083", "abs": "https://arxiv.org/abs/2506.07083", "authors": ["Jiawen Li", "Jiang Guo", "Yuanzhe Li", "Zetian Mao", "Jiaxing Shen", "Tashi Xu", "Diptesh Das", "Jinming He", "Run Hu", "Yaerim Lee", "Koji Tsuda", "Junichiro Shiomi"], "title": "Inverse Design of Metamaterials with Manufacturing-Guiding Spectrum-to-Structure Conditional Diffusion Model", "categories": ["physics.optics", "cs.LG"], "comment": "20 pages, 7 figures", "summary": "Metamaterials are artificially engineered structures that manipulate\nelectromagnetic waves, having optical properties absent in natural materials.\nRecently, machine learning for the inverse design of metamaterials has drawn\nattention. However, the highly nonlinear relationship between the metamaterial\nstructures and optical behaviour, coupled with fabrication difficulties, poses\nchallenges for using machine learning to design and manufacture complex\nmetamaterials. Herein, we propose a general framework that implements\ncustomised spectrum-to-shape and size parameters to address one-to-many\nmetamaterial inverse design problems using conditional diffusion models. Our\nmethod exhibits superior spectral prediction accuracy, generates a diverse\nrange of patterns compared to other typical generative models, and offers\nvaluable prior knowledge for manufacturing through the subsequent analysis of\nthe diverse generated results, thereby facilitating the experimental\nfabrication of metamaterial designs. We demonstrate the efficacy of the\nproposed method by successfully designing and fabricating a free-form\nmetamaterial with a tailored selective emission spectrum for thermal camouflage\napplications.", "AI": {"tldr": "A general framework using conditional diffusion models is proposed for metamaterial inverse design, exhibiting high spectral prediction accuracy and aiding in the fabrication of complex metamaterials.", "motivation": "To address the challenges posed by the nonlinear relationship between metamaterial structures and their optical behavior, as well as fabrication difficulties, when using machine learning for designing complex metamaterials.", "method": "The paper proposes a framework that uses customized spectrum-to-shape and size parameters with conditional diffusion models to solve one-to-many metamaterial inverse design problems.", "result": "This method shows superior spectral prediction accuracy, generates diverse patterns compared to other generative models, and provides useful prior knowledge for manufacturing. A free-form metamaterial was successfully designed and fabricated with a tailored selective emission spectrum for thermal camouflage.", "conclusion": "The proposed framework effectively facilitates the experimental fabrication of complex metamaterials through accurate spectral predictions and diverse pattern generation."}}
{"id": "2506.07140", "pdf": "https://arxiv.org/pdf/2506.07140", "abs": "https://arxiv.org/abs/2506.07140", "authors": ["Zhongren Chen", "Siyu Chen", "Zhengling Qi", "Xiaohong Chen", "Zhuoran Yang"], "title": "Quantile-Optimal Policy Learning under Unmeasured Confounding", "categories": ["stat.ML", "cs.LG", "econ.EM"], "comment": null, "summary": "We study quantile-optimal policy learning where the goal is to find a policy\nwhose reward distribution has the largest $\\alpha$-quantile for some $\\alpha\n\\in (0, 1)$. We focus on the offline setting whose generating process involves\nunobserved confounders. Such a problem suffers from three main challenges: (i)\nnonlinearity of the quantile objective as a functional of the reward\ndistribution, (ii) unobserved confounding issue, and (iii) insufficient\ncoverage of the offline dataset. To address these challenges, we propose a\nsuite of causal-assisted policy learning methods that provably enjoy strong\ntheoretical guarantees under mild conditions. In particular, to address (i) and\n(ii), using causal inference tools such as instrumental variables and negative\ncontrols, we propose to estimate the quantile objectives by solving nonlinear\nfunctional integral equations. Then we adopt a minimax estimation approach with\nnonparametric models to solve these integral equations, and propose to\nconstruct conservative policy estimates that address (iii). The final policy is\nthe one that maximizes these pessimistic estimates. In addition, we propose a\nnovel regularized policy learning method that is more amenable to computation.\nFinally, we prove that the policies learned by these methods are\n$\\tilde{\\mathscr{O}}(n^{-1/2})$ quantile-optimal under a mild coverage\nassumption on the offline dataset. Here, $\\tilde{\\mathscr{O}}(\\cdot)$ omits\npoly-logarithmic factors. To the best of our knowledge, we propose the first\nsample-efficient policy learning algorithms for estimating the quantile-optimal\npolicy when there exist unmeasured confounding.", "AI": {"tldr": "This paper focuses on quantile-optimal policy learning in offline settings with unobserved confounders. The authors propose causal-assisted policy learning methods addressing nonlinearity, confounding issues, and insufficient dataset coverage. They use tools like instrumental variables and negative controls, adopt a minimax estimation approach, and construct conservative policy estimates. A novel regularized policy learning method is also proposed for computational efficiency. The learned policies are proven to be quantile-optimal under certain assumptions.", "motivation": "The motivation is to develop effective policy learning methods that can handle the challenges of nonlinearity in quantile objectives, unobserved confounding, and insufficient coverage of offline datasets in the context of quantile-optimal policy learning.", "method": "The methods include using causal inference tools such as instrumental variables and negative controls to estimate quantile objectives by solving nonlinear functional integral equations. A minimax estimation approach with nonparametric models is adopted to solve these equations, and conservative policy estimates are constructed. Additionally, a regularized policy learning method is proposed for better computational amenability.", "result": "The policies learned by the proposed methods are proven to be $\\tilde{\\mathscr{O}}(n^{-1/2})$ quantile-optimal under mild coverage assumptions on the offline dataset.", "conclusion": "The authors claim that their work proposes the first sample-efficient policy learning algorithms for estimating quantile-optimal policies in the presence of unmeasured confounding."}}
{"id": "2506.07159", "pdf": "https://arxiv.org/pdf/2506.07159", "abs": "https://arxiv.org/abs/2506.07159", "authors": ["Mrinmay Sen", "Chalavadi Krishna Mohan"], "title": "pFedSOP : Accelerating Training Of Personalized Federated Learning Using Second-Order Optimization", "categories": ["cs.DC", "cs.LG", "68Q25, 68T05, 90C06, 90C25, 90C30", "I.2.6; G.1.6; C.2.4"], "comment": null, "summary": "Personalized Federated Learning (PFL) enables clients to collaboratively\ntrain personalized models tailored to their individual objectives, addressing\nthe challenge of model generalization in traditional Federated Learning (FL)\ndue to high data heterogeneity. However, existing PFL methods often require\nincreased communication rounds to achieve the desired performance, primarily\ndue to slow training caused by the use of first-order optimization, which has\nlinear convergence. Additionally, many of these methods increase local\ncomputation because of the additional data fed into the model during the search\nfor personalized local models. One promising solution to this slow training is\nsecond-order optimization, known for its quadratic convergence. However,\nemploying it in PFL is challenging due to the Hessian matrix and its inverse.\nIn this paper, we propose pFedSOP, which efficiently utilizes second-order\noptimization in PFL to accelerate the training of personalized models and\nenhance performance with fewer communication rounds. Our approach first\ncomputes a personalized local gradient update using the Gompertz function-based\nnormalized angle between local and global gradient updates, incorporating\nclient-specific global information. We then use a regularized Fisher\nInformation Matrix (FIM), computed from this personalized gradient update, as\nan approximation of the Hessian to update the personalized models. This\nFIM-based second-order optimization speeds up training with fewer communication\nrounds by tackling the challenges with exact Hessian and avoids additional data\nbeing fed into the model during the search for personalized local models.\nExtensive experiments on heterogeneously partitioned image classification\ndatasets with partial client participation demonstrate that pFedSOP outperforms\nstate-of-the-art FL and PFL algorithms.", "AI": {"tldr": "In this paper, the authors propose pFedSOP for Personalized Federated Learning (PFL), which uses second-order optimization to accelerate training and improve performance with fewer communication rounds. It computes personalized local gradient updates using a Gompertz function-based normalized angle and employs a regularized Fisher Information Matrix (FIM) as an approximation of the Hessian matrix.", "motivation": "Existing PFL methods often require increased communication rounds and local computation due to the use of first-order optimization techniques, which have linear convergence. The challenge is to find a way to utilize second-order optimization in PFL to achieve faster training and better performance while avoiding additional data being fed into the model during the search for personalized local models.", "method": "The method proposed is pFedSOP, which incorporates second-order optimization into PFL. It first computes a personalized local gradient update using the Gompertz function-based normalized angle between local and global gradient updates. Then, it uses a regularized Fisher Information Matrix (FIM), computed from this personalized gradient update, as an approximation of the Hessian matrix to update the personalized models.", "result": "Extensive experiments on heterogeneously partitioned image classification datasets with partial client participation demonstrate that pFedSOP outperforms state-of-the-art FL and PFL algorithms in terms of both performance and efficiency.", "conclusion": "pFedSOP efficiently utilizes second-order optimization in PFL to accelerate the training of personalized models and enhance performance with fewer communication rounds, overcoming the challenges associated with exact Hessian matrices."}}
{"id": "2506.07171", "pdf": "https://arxiv.org/pdf/2506.07171", "abs": "https://arxiv.org/abs/2506.07171", "authors": ["Chenlong Zhang", "Zhuoran Jin", "Hongbang Yuan", "Jiaheng Wei", "Tong Zhou", "Kang Liu", "Jun Zhao", "Yubo Chen"], "title": "RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality", "categories": ["cs.CL", "cs.LG"], "comment": "Paper under review", "summary": "The widespread deployment of Large Language Models (LLMs) trained on massive,\nuncurated corpora has raised growing concerns about the inclusion of sensitive,\ncopyrighted, or illegal content. This has led to increasing interest in LLM\nunlearning: the task of selectively removing specific information from a model\nwithout retraining from scratch or degrading overall utility. However, existing\nmethods often rely on large-scale forget and retain datasets, and suffer from\nunnatural responses, poor generalization, or catastrophic utility loss. In this\nwork, we propose Reinforcement UnLearning (RULE), an efficient framework that\nformulates unlearning as a refusal boundary optimization problem. RULE is\ntrained with a small portion of the forget set and synthesized boundary\nqueries, using a verifiable reward function that encourages safe refusal on\nforget--related queries while preserving helpful responses on permissible\ninputs. We provide both theoretical and empirical evidence demonstrating the\neffectiveness of RULE in achieving targeted unlearning without compromising\nmodel utility. Experimental results show that, with only $12%$ forget set and\n$8%$ synthesized boundary data, RULE outperforms existing baselines by up to\n$17.5%$ forget quality and $16.3%$ naturalness response while maintaining\ngeneral utility, achieving forget--retain Pareto optimality. Remarkably, we\nfurther observe that RULE improves the naturalness of model outputs, enhances\ntraining efficiency, and exhibits strong generalization ability, generalizing\nrefusal behavior to semantically related but unseen queries.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aReinforcement UnLearning (RULE)\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u62d2\u7edd\u8fb9\u754c\u6765\u9ad8\u6548\u5730\u4ece\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u5220\u9664\u7279\u5b9a\u4fe1\u606f\uff0c\u4ec5\u4f7f\u752812%\u7684\u9057\u5fd8\u96c6\u548c8%\u7684\u5408\u6210\u8fb9\u754c\u6570\u636e\uff0c\u5c31\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8617.5%\u7684\u9057\u5fd8\u8d28\u91cf\u548c16.3%\u7684\u81ea\u7136\u54cd\u5e94\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u603b\u4f53\u6548\u7528\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5927\u89c4\u6a21\u3001\u672a\u7ecf\u7b5b\u9009\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u6a21\u578b\u4e2d\u53ef\u80fd\u5305\u542b\u654f\u611f\u3001\u53d7\u7248\u6743\u4fdd\u62a4\u6216\u975e\u6cd5\u5185\u5bb9\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u5bf9\u9009\u62e9\u6027\u5220\u9664\u6a21\u578b\u4e2d\u7279\u5b9a\u4fe1\u606f\u7684\u9700\u6c42\u65e5\u76ca\u589e\u52a0\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u7684\u9057\u5fd8\u548c\u4fdd\u7559\u6570\u636e\u96c6\uff0c\u5e76\u4e14\u5b58\u5728\u4e0d\u81ea\u7136\u7684\u54cd\u5e94\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u6216\u707e\u96be\u6027\u7684\u6548\u7528\u635f\u5931\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u2014\u2014\u5f3a\u5316\u975e\u5b66\u4e60\uff08RULE\uff09\uff0c\u5c06\u975e\u5b66\u4e60\u4efb\u52a1\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u62d2\u7edd\u8fb9\u754c\u4f18\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u5c0f\u90e8\u5206\u7684\u9057\u5fd8\u96c6\u548c\u5408\u6210\u8fb9\u754c\u67e5\u8be2\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u9f13\u52b1\u5bf9\u4e0e\u9057\u5fd8\u76f8\u5173\u7684\u67e5\u8be2\u8fdb\u884c\u5b89\u5168\u62d2\u7edd\uff0c\u540c\u65f6\u4fdd\u7559\u5bf9\u8bb8\u53ef\u8f93\u5165\u7684\u5e2e\u52a9\u6027\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u752812%\u7684\u9057\u5fd8\u96c6\u548c8%\u7684\u5408\u6210\u8fb9\u754c\u6570\u636e\uff0cRULE\u5728\u9057\u5fd8\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u9ad8\u8fbe17.5%\uff0c\u5728\u81ea\u7136\u54cd\u5e94\u5ea6\u4e0a\u9ad8\u51fa16.3%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u6548\u7528\u3002\u6b64\u5916\uff0cRULE\u8fd8\u63d0\u9ad8\u4e86\u6a21\u578b\u8f93\u51fa\u7684\u81ea\u7136\u6027\uff0c\u589e\u5f3a\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5c06\u62d2\u7edd\u884c\u4e3a\u63a8\u5e7f\u5230\u8bed\u4e49\u76f8\u5173\u4f46\u672a\u89c1\u8fc7\u7684\u67e5\u8be2\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cRULE\u5728\u5b9e\u73b0\u6709\u9488\u5bf9\u6027\u7684\u975e\u5b66\u4e60\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u4e0d\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u6574\u4f53\u6548\u7528\u3002"}}
{"id": "2506.07424", "pdf": "https://arxiv.org/pdf/2506.07424", "abs": "https://arxiv.org/abs/2506.07424", "authors": ["Kyeonghyun Kim", "Jinhee Jang", "Juhwan Choi", "Yoonji Lee", "Kyohoon Jin", "YoungBin Kim"], "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference", "summary": "Large language models (LLMs) are renowned for their extensive linguistic\nknowledge and strong generalization capabilities, but their high computational\ndemands make them unsuitable for resource-constrained environments. In\ncontrast, small language models (SLMs) are computationally efficient but often\nlack the broad generalization capacity of LLMs. To bridge this gap, we propose\nPiFi, a novel framework that combines the strengths of both LLMs and SLMs to\nachieve high performance while maintaining efficiency. PiFi integrates a single\nfrozen layer from an LLM into a SLM and fine-tunes the combined model for\nspecific tasks, boosting performance without a significant increase in\ncomputational cost. We show that PiFi delivers consistent performance\nimprovements across a range of natural language processing tasks, including\nboth natural language understanding and generation. Moreover, our findings\ndemonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing\ngeneralization to unseen domains and facilitating the transfer of linguistic\nabilities.", "AI": {"tldr": "PiFi\u6846\u67b6\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u5c06LLM\u7684\u4e00\u4e2a\u51bb\u7ed3\u5c42\u6574\u5408\u5230SLM\u4e2d\uff0c\u5e76\u5bf9\u7ec4\u5408\u6a21\u578b\u8fdb\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e7f\u6cdb\u7684\u8bed\u8a00\u77e5\u8bc6\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u9700\u6c42\u9ad8\uff1b\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u4e3a\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6548\u53c8\u80fd\u63d0\u5347\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPiFi\u6846\u67b6\uff0c\u5c06\u4e00\u4e2a\u6765\u81eaLLM\u7684\u51bb\u7ed3\u5c42\u96c6\u6210\u5230SLM\u4e2d\uff0c\u7136\u540e\u5bf9\u7ec4\u5408\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4ee5\u9002\u5e94\u7279\u5b9a\u4efb\u52a1\u3002\u8fd9\u6837\u53ef\u4ee5\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6027\u80fd\u3002", "result": "PiFi\u5728\u5305\u62ec\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u5728\u5185\u7684\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u80fd\u6709\u6548\u5229\u7528LLM\u7684\u77e5\u8bc6\uff0c\u589e\u5f3a\u5bf9\u672a\u89c1\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u8bed\u8a00\u80fd\u529b\u7684\u8fc1\u79fb\u3002", "conclusion": "PiFi\u6846\u67b6\u6210\u529f\u5730\u7ed3\u5408\u4e86LLM\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u548cSLM\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u6539\u8fdb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2506.07199", "pdf": "https://arxiv.org/pdf/2506.07199", "abs": "https://arxiv.org/abs/2506.07199", "authors": ["Ben Hayes", "Charalampos Saitis", "Gy\u00f6rgy Fazekas"], "title": "Audio synthesizer inversion in symmetric parameter spaces with approximately equivariant flow matching", "categories": ["cs.SD", "cs.LG", "eess.AS", "eess.SP"], "comment": "Accepted at ISMIR 2025", "summary": "Many audio synthesizers can produce the same signal given different parameter\nconfigurations, meaning the inversion from sound to parameters is an inherently\nill-posed problem. We show that this is largely due to intrinsic symmetries of\nthe synthesizer, and focus in particular on permutation invariance. First, we\ndemonstrate on a synthetic task that regressing point estimates under\npermutation symmetry degrades performance, even when using a\npermutation-invariant loss function or symmetry-breaking heuristics. Then,\nviewing equivalent solutions as modes of a probability distribution, we show\nthat a conditional generative model substantially improves performance.\nFurther, acknowledging the invariance of the implicit parameter distribution,\nwe find that performance is further improved by using a permutation equivariant\ncontinuous normalizing flow. To accommodate intricate symmetries in real\nsynthesizers, we also propose a relaxed equivariance strategy that adaptively\ndiscovers relevant symmetries from data. Applying our method to Surge XT, a\nfull-featured open source synthesizer used in real world audio production, we\nfind our method outperforms regression and generative baselines across audio\nreconstruction metrics.", "AI": {"tldr": "The paper addresses the issue of parameter estimation for audio synthesizers with inherent symmetries and permutation invariance, proposing a conditional generative model with permutation equivariant continuous normalizing flow and relaxed equivariance strategy, which outperforms regression and other generative models on real-world synthesizer Surge XT.", "motivation": "Many audio synthesizers can produce the same signal given different parameter configurations, making the inversion from sound to parameters an ill-posed problem due to intrinsic symmetries like permutation invariance.", "method": "Firstly, demonstrate the degradation of performance when regressing point estimates under permutation symmetry. Then use a conditional generative model to view equivalent solutions as modes of a probability distribution. Further improve the performance by using a permutation equivariant continuous normalizing flow. Propose a relaxed equivariance strategy that adaptively discovers relevant symmetries from data.", "result": "The method outperforms regression and generative baselines across audio reconstruction metrics when applied to Surge XT, a full-featured open source synthesizer.", "conclusion": "A conditional generative model with permutation equivariant continuous normalizing flow and relaxed equivariance strategy significantly improves parameter estimation for audio synthesizers."}}
{"id": "2506.07431", "pdf": "https://arxiv.org/pdf/2506.07431", "abs": "https://arxiv.org/abs/2506.07431", "authors": ["Jie He", "Minglang Chen", "Minying Lu", "Bocheng Liang", "Junming Wei", "Guiyan Peng", "Jiaxi Chen", "Ying Tan"], "title": "FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate ultrasound image segmentation is a prerequisite for precise\nbiometrics and accurate assessment. Relying on manual delineation introduces\nsignificant errors and is time-consuming. However, existing segmentation models\nare designed based on objects in natural scenes, making them difficult to adapt\nto ultrasound objects with high noise and high similarity. This is particularly\nevident in small object segmentation, where a pronounced jagged effect occurs.\nTherefore, this paper proposes a fetal femur and cranial ultrasound image\nsegmentation model based on feature perception and Mamba enhancement to address\nthese challenges. Specifically, a longitudinal and transverse independent\nviewpoint scanning convolution block and a feature perception module were\ndesigned to enhance the ability to capture local detail information and improve\nthe fusion of contextual information. Combined with the Mamba-optimized\nresidual structure, this design suppresses the interference of raw noise and\nenhances local multi-dimensional scanning. The system builds global information\nand local feature dependencies, and is trained with a combination of different\noptimizers to achieve the optimal solution. After extensive experimental\nvalidation, the FAMSeg network achieved the fastest loss reduction and the best\nsegmentation performance across images of varying sizes and orientations.", "AI": {"tldr": "An ultrasound image segmentation model with feature perception and Mamba enhancement is proposed for fetal femur and cranial images, achieving fast loss reduction and superior segmentation performance.", "motivation": "Accurate ultrasound image segmentation is crucial but challenging due to high noise, similarity, and jagged effects in small object segmentation.", "method": "The paper introduces FAMSeg network which includes a longitudinal and transverse independent viewpoint scanning convolution block, a feature perception module, and Mamba-optimized residual structure to enhance local detail capturing, improve contextual information fusion, suppress noise interference, and build global-local feature dependencies.", "result": "FAMSeg network demonstrated the fastest loss reduction and best segmentation performance on images of different sizes and orientations after extensive experiments.", "conclusion": "The proposed model successfully addresses the challenges in ultrasound image segmentation, providing precise biometrics and accurate assessment."}}
{"id": "2506.07434", "pdf": "https://arxiv.org/pdf/2506.07434", "abs": "https://arxiv.org/abs/2506.07434", "authors": ["Feifan Song", "Shaohang Wei", "Wen Luo", "Yuxuan Fan", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth.", "AI": {"tldr": "Large Language Models (LLMs) need alignment with human preferences to generate appropriate content. The paper proposes a novel framework, Weak-to-Strong Decoding (WSD), which uses a small aligned model to guide the large base model in generating high-quality and aligned content. A new dataset GenerAlign is collected for fine-tuning the small model.", "motivation": "The motivation of this paper stems from the observation that the difficulty of generating aligned responses mainly occurs at the beginning of decoding. Current low-resource methods for aligning LLMs face challenges in producing both high-quality and aligned content.", "method": "The proposed method is called Weak-to-Strong Decoding (WSD). It involves a small aligned model first creating well-aligned beginnings of responses, followed by a large base model continuing the rest. This process is controlled by an auto-switch mechanism. Additionally, a new dataset named GenerAlign is collected to fine-tune a small-sized Pilot-3B model as the draft model within the WSD framework.", "result": "The WSD framework effectively enhances different base models, outperforming all baseline methods while avoiding degradation on downstream tasks (alignment tax). Extensive experiments confirm the impact of different settings, time efficiency, and provide insights into the intrinsic mechanisms of WSD.", "conclusion": "The paper concludes that the WSD framework successfully improves the alignment ability of base models using guidance from a small aligned model. The collection of the GenerAlign dataset contributes to enhancing the performance of the WSD framework."}}
{"id": "2506.07248", "pdf": "https://arxiv.org/pdf/2506.07248", "abs": "https://arxiv.org/abs/2506.07248", "authors": ["Prathamesh Kokate", "Mitali Sarnaik", "Manavi Khopade", "Raviraj Joshi"], "title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Long document classification poses challenges due to the computational\nlimitations of transformer-based models, particularly BERT, which are\nconstrained by fixed input lengths and quadratic attention complexity.\nMoreover, using the full document for classification is often redundant, as\nonly a subset of sentences typically carries the necessary information. To\naddress this, we propose a TF-IDF-based sentence ranking method that improves\nefficiency by selecting the most informative content. Our approach explores\nfixed-count and percentage-based sentence selection, along with an enhanced\nscoring strategy combining normalized TF-IDF scores and sentence length.\nEvaluated on the MahaNews LDC dataset of long Marathi news articles, the method\nconsistently outperforms baselines such as first, last, and random sentence\nselection. With MahaBERT-v2, we achieve near-identical classification accuracy\nwith just a 0.33 percent drop compared to the full-context baseline, while\nreducing input size by over 50 percent and inference latency by 43 percent.\nThis demonstrates that significant context reduction is possible without\nsacrificing performance, making the method practical for real-world long\ndocument classification tasks.", "AI": {"tldr": "The paper proposes a TF-IDF-based sentence ranking method for efficient long document classification, reducing input size and inference latency while maintaining near-identical accuracy with MahaBERT-v2.", "motivation": "Transformer-based models like BERT face challenges in long document classification due to fixed input lengths, quadratic attention complexity, and redundancy in using the entire document.", "method": "A TF-IDF-based sentence ranking method is developed to select the most informative sentences through fixed-count or percentage-based selection. An enhanced scoring strategy combines normalized TF-IDF scores with sentence length.", "result": "The method outperforms baselines on the MahaNews LDC dataset, achieving near-identical classification accuracy with MahaBERT-v2 while reducing input size by over 50% and inference latency by 43%.", "conclusion": "Significant context reduction is possible without sacrificing performance, making the method suitable for real-world long document classification tasks."}}
{"id": "2506.07435", "pdf": "https://arxiv.org/pdf/2506.07435", "abs": "https://arxiv.org/abs/2506.07435", "authors": ["Alexander Kolpakov", "Igor Rivin"], "title": "Fast Geometric Embedding for Node Influence Maximization", "categories": ["cs.SI", "cs.AI", "cs.LG", "E.1; G.2.2; G.4"], "comment": "8 pages, 4 figures, 18 tables; Github repository available\n  (https://github.com/sashakolpakov/graphem/); Package available on PyPi\n  (https://pypi.org/project/graphem-jax/)", "summary": "Computing classical centrality measures such as betweenness and closeness is\ncomputationally expensive on large-scale graphs. In this work, we introduce an\nefficient force layout algorithm that embeds a graph into a low-dimensional\nspace, where the radial distance from the origin serves as a proxy for various\ncentrality measures. We evaluate our method on multiple graph families and\ndemonstrate strong correlations with degree, PageRank, and paths-based\ncentralities. As an application, it turns out that the proposed embedding\nallows to find high-influence nodes in a network, and provides a fast and\nscalable alternative to the standard greedy algorithm.", "AI": {"tldr": "\u5728\u5927\u89c4\u6a21\u56fe\u4e0a\u8ba1\u7b97\u5982\u4e2d\u4ecb\u6027\u548c\u63a5\u8fd1\u6027\u7b49\u7ecf\u5178\u4e2d\u5fc3\u6027\u5ea6\u91cf\u5728\u8ba1\u7b97\u4e0a\u975e\u5e38\u6602\u8d35\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u529b\u5bfc\u5411\u5e03\u5c40\u7b97\u6cd5\uff0c\u5c06\u56fe\u5d4c\u5165\u5230\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u5176\u4e2d\u4ece\u539f\u70b9\u51fa\u53d1\u7684\u5f84\u5411\u8ddd\u79bb\u4f5c\u4e3a\u5404\u79cd\u4e2d\u5fc3\u6027\u5ea6\u91cf\u7684\u4ee3\u7406\u3002\u6211\u4eec\u5728\u591a\u4e2a\u56fe\u65cf\u4e0a\u8bc4\u4f30\u4e86\u6211\u4eec\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u5ea6\u3001PageRank\u548c\u8def\u5f84\u57fa\u7840\u4e2d\u5fc3\u6027\u7684\u5f3a\u70c8\u76f8\u5173\u6027\u3002\u4f5c\u4e3a\u4e00\u79cd\u5e94\u7528\uff0c\u6240\u63d0\u51fa\u7684\u5d4c\u5165\u65b9\u6cd5\u80fd\u591f\u627e\u5230\u7f51\u7edc\u4e2d\u7684\u9ad8\u5f71\u54cd\u529b\u8282\u70b9\uff0c\u5e76\u4e3a\u6807\u51c6\u8d2a\u5a6a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u8ba1\u7b97\u5927\u89c4\u6a21\u56fe\u4e0a\u7684\u7ecf\u5178\u4e2d\u5fc3\u6027\u5ea6\u91cf\uff08\u4f8b\u5982\u4e2d\u4ecb\u6027\u548c\u63a5\u8fd1\u6027\uff09\u5728\u8ba1\u7b97\u4e0a\u975e\u5e38\u6602\u8d35\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u4f30\u7b97\u8fd9\u4e9b\u4e2d\u5fc3\u6027\u5ea6\u91cf\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u529b\u5bfc\u5411\u5e03\u5c40\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06\u56fe\u5d4c\u5165\u5230\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u5176\u4e2d\u4ece\u539f\u70b9\u51fa\u53d1\u7684\u5f84\u5411\u8ddd\u79bb\u4f5c\u4e3a\u5404\u79cd\u4e2d\u5fc3\u6027\u5ea6\u91cf\u7684\u4ee3\u7406\u3002", "result": "\u5728\u591a\u4e2a\u56fe\u65cf\u4e0a\u5c55\u793a\u4e86\u4e0e\u5ea6\u3001PageRank\u548c\u8def\u5f84\u57fa\u7840\u4e2d\u5fc3\u6027\u7684\u5f3a\u70c8\u76f8\u5173\u6027\u3002\u80fd\u591f\u627e\u5230\u7f51\u7edc\u4e2d\u7684\u9ad8\u5f71\u54cd\u529b\u8282\u70b9\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5d4c\u5165\u65b9\u6cd5\u4e3a\u6807\u51c6\u8d2a\u5a6a\u7b97\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2506.07259", "pdf": "https://arxiv.org/pdf/2506.07259", "abs": "https://arxiv.org/abs/2506.07259", "authors": ["Daolang Huang", "Xinyi Wen", "Ayush Bharti", "Samuel Kaski", "Luigi Acerbi"], "title": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition", "categories": ["stat.ML", "cs.LG"], "comment": "27 pages, 13 figures", "summary": "Many critical applications, from autonomous scientific discovery to\npersonalized medicine, demand systems that can both strategically acquire the\nmost informative data and instantaneously perform inference based upon it.\nWhile amortized methods for Bayesian inference and experimental design offer\npart of the solution, neither approach is optimal in the most general and\nchallenging task, where new data needs to be collected for instant inference.\nTo tackle this issue, we introduce the Amortized Active Learning and Inference\nEngine (ALINE), a unified framework for amortized Bayesian inference and active\ndata acquisition. ALINE leverages a transformer architecture trained via\nreinforcement learning with a reward based on self-estimated information gain\nprovided by its own integrated inference component. This allows it to\nstrategically query informative data points while simultaneously refining its\npredictions. Moreover, ALINE can selectively direct its querying strategy\ntowards specific subsets of model parameters or designated predictive tasks,\noptimizing for posterior estimation, data prediction, or a mixture thereof.\nEmpirical results on regression-based active learning, classical Bayesian\nexperimental design benchmarks, and a psychometric model with selectively\ntargeted parameters demonstrate that ALINE delivers both instant and accurate\ninference along with efficient selection of informative points.", "AI": {"tldr": "ALINE is a unified framework for amortized Bayesian inference and active data acquisition that leverages a transformer architecture trained via reinforcement learning to strategically query informative data points while simultaneously refining predictions.", "motivation": "Critical applications require systems that can acquire the most informative data and perform instant inference. Existing methods for Bayesian inference and experimental design are not optimal for tasks requiring new data collection for instant inference.", "method": "ALINE uses a transformer architecture trained with reinforcement learning, utilizing a reward based on self-estimated information gain from its integrated inference component. It can selectively direct querying strategies towards specific model parameters or predictive tasks.", "result": "Empirical results show ALINE provides instant and accurate inference along with efficient selection of informative points in regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model.", "conclusion": "ALINE delivers both instant and accurate inference and efficiently selects informative data points, making it suitable for various tasks requiring strategic data acquisition and inference."}}
{"id": "2506.07436", "pdf": "https://arxiv.org/pdf/2506.07436", "abs": "https://arxiv.org/abs/2506.07436", "authors": ["Nishi Chaudhary", "S M Jamil Uddin", "Sathvik Sharath Chandra", "Anto Ovid", "Alex Albert"], "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "The recent emergence of multimodal large language models (LLMs) has\nintroduced new opportunities for improving visual hazard recognition on\nconstruction sites. Unlike traditional computer vision models that rely on\ndomain-specific training and extensive datasets, modern LLMs can interpret and\ndescribe complex visual scenes using simple natural language prompts. However,\ndespite growing interest in their applications, there has been limited\ninvestigation into how different LLMs perform in safety-critical visual tasks\nwithin the construction domain. To address this gap, this study conducts a\ncomparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,\nGPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify\npotential hazards from real-world construction images. Each model was tested\nunder three prompting strategies: zero-shot, few-shot, and chain-of-thought\n(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated\nbasic safety context and a hazard source mnemonic, and CoT provided\nstep-by-step reasoning examples to scaffold model thinking. Quantitative\nanalysis was performed using precision, recall, and F1-score metrics across all\nconditions. Results reveal that prompting strategy significantly influenced\nperformance, with CoT prompting consistently producing higher accuracy across\nmodels. Additionally, LLM performance varied under different conditions, with\nGPT-4.5 and GPT-o3 outperforming others in most settings. The findings also\ndemonstrate the critical role of prompt design in enhancing the accuracy and\nconsistency of multimodal LLMs for construction safety applications. This study\noffers actionable insights into the integration of prompt engineering and LLMs\nfor practical hazard recognition, contributing to the development of more\nreliable AI-assisted safety systems.", "AI": {"tldr": "\u6700\u8fd1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51fa\u73b0\u4e3a\u6539\u8fdb\u65bd\u5de5\u73b0\u573a\u7684\u89c6\u89c9\u5371\u9669\u8bc6\u522b\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002\u8fd9\u9879\u7814\u7a76\u5bf9\u6bd4\u8bc4\u4f30\u4e86\u4e94\u4e2a\u6700\u5148\u8fdb\u7684LLMs\u5728\u4ece\u771f\u5b9e\u4e16\u754c\u5efa\u7b51\u56fe\u50cf\u4e2d\u8bc6\u522b\u6f5c\u5728\u5371\u9669\u65b9\u9762\u7684\u80fd\u529b\uff0c\u6d4b\u8bd5\u4e86\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff1a\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u3002\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u793a\u7b56\u7565\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0cCoT\u63d0\u793a\u5728\u6240\u6709\u6a21\u578b\u4e2d\u59cb\u7ec8\u4ea7\u751f\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002GPT-4.5\u548cGPT-o3\u5728\u5927\u591a\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u63d0\u793a\u8bbe\u8ba1\u5728\u589e\u5f3a\u591a\u6a21\u6001LLMs\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5b9e\u9645\u5371\u9669\u8bc6\u522b\u4e2d\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548cLLMs\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\u3002", "motivation": "\u4f20\u7edf\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u4f9d\u8d56\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u8bad\u7ec3\u548c\u5927\u91cf\u6570\u636e\u96c6\uff0c\u800c\u73b0\u4ee3LLMs\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u89e3\u91ca\u548c\u63cf\u8ff0\u590d\u6742\u7684\u89c6\u89c9\u573a\u666f\u3002\u7136\u800c\uff0c\u5173\u4e8e\u4e0d\u540cLLMs\u5728\u5efa\u7b51\u9886\u57df\u5173\u952e\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u7684\u7814\u7a76\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u4ee5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5bf9\u4e94\u79cd\u6700\u5148\u8fdb\u7684LLMs\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\uff08\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\uff09\u6765\u6d4b\u8bd5\u5b83\u4eec\u4ece\u771f\u5b9e\u4e16\u754c\u5efa\u7b51\u56fe\u50cf\u4e2d\u8bc6\u522b\u6f5c\u5728\u5371\u9669\u7684\u80fd\u529b\u3002\u901a\u8fc7\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u8bc4\u4f30\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u63d0\u793a\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff0c\u5176\u4e2d\u601d\u7ef4\u94fe\u63d0\u793a\u5728\u6240\u6709\u6a21\u578b\u4e2d\u59cb\u7ec8\u4ea7\u751f\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u4e0d\u540c\u6761\u4ef6\u4e0bLLMs\u7684\u8868\u73b0\u5404\u5f02\uff0cGPT-4.5\u548cGPT-o3\u5728\u5927\u591a\u6570\u8bbe\u7f6e\u4e0b\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u63d0\u793a\u8bbe\u8ba1\u5728\u63d0\u9ad8\u591a\u6a21\u6001LLMs\u7528\u4e8e\u5efa\u7b51\u5b89\u5168\u5e94\u7528\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\u3002\u672c\u7814\u7a76\u4e3a\u5c06\u63d0\u793a\u5de5\u7a0b\u4e0eLLMs\u7ed3\u5408\u4ee5\u5b9e\u73b0\u5b9e\u7528\u7684\u5371\u9669\u8bc6\u522b\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u53ef\u9760\u7684AI\u8f85\u52a9\u5b89\u5168\u7cfb\u7edf\u3002"}}
{"id": "2506.07261", "pdf": "https://arxiv.org/pdf/2506.07261", "abs": "https://arxiv.org/abs/2506.07261", "authors": ["Amit Jaspal", "Qian Dang", "Ajantha Ramineni"], "title": "RADAR: Recall Augmentation through Deferred Asynchronous Retrieval", "categories": ["cs.IR", "cs.LG"], "comment": null, "summary": "Modern large-scale recommender systems employ multi-stage ranking funnel\n(Retrieval, Pre-ranking, Ranking) to balance engagement and computational\nconstraints (latency, CPU). However, the initial retrieval stage, often relying\non efficient but less precise methods like K-Nearest Neighbors (KNN), struggles\nto effectively surface the most engaging items from billion-scale catalogs,\nparticularly distinguishing highly relevant and engaging candidates from merely\nrelevant ones. We introduce Recall Augmentation through Deferred Asynchronous\nRetrieval (RADAR), a novel framework that leverages asynchronous, offline\ncomputation to pre-rank a significantly larger candidate set for users using\nthe full complexity ranking model. These top-ranked items are stored and\nutilized as a high-quality retrieval source during online inference, bypassing\nonline retrieval and pre-ranking stages for these candidates. We demonstrate\nthrough offline experiments that RADAR significantly boosts recall (2X\nRecall@200 vs DNN retrieval baseline) by effectively combining a larger\nretrieved candidate set with a more powerful ranking model. Online A/B tests\nconfirm a +0.8% lift in topline engagement metrics, validating RADAR as a\npractical and effective method to improve recommendation quality under strict\nonline serving constraints.", "AI": {"tldr": "RADAR\uff0c\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5f02\u6b65\u8ba1\u7b97\u9884\u6392\u66f4\u5927\u5019\u9009\u96c6\uff0c\u4f7f\u7528\u590d\u6742\u6392\u540d\u6a21\u578b\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u53ec\u56de\u7387\u548c\u53c2\u4e0e\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u7684\u521d\u59cb\u68c0\u7d22\u9636\u6bb5\u96be\u4ee5\u6709\u6548\u533a\u5206\u9ad8\u5ea6\u76f8\u5173\u548c\u4ec5\u4ec5\u76f8\u5173\u7684\u9879\u76ee\uff0c\u5f71\u54cd\u4e86\u4ece\u5341\u4ebf\u89c4\u6a21\u76ee\u5f55\u4e2d\u63d0\u53d6\u6700\u5438\u5f15\u4eba\u7684\u9879\u76ee\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165RADAR\u6846\u67b6\uff0c\u5229\u7528\u5f02\u6b65\u3001\u79bb\u7ebf\u8ba1\u7b97\u4e3a\u7528\u6237\u9884\u6392\u66f4\u5927\u5019\u9009\u96c6\uff0c\u4f7f\u7528\u5168\u590d\u6742\u6027\u6392\u540d\u6a21\u578b\uff0c\u5e76\u5c06\u9876\u7ea7\u9879\u76ee\u5b58\u50a8\u4e3a\u9ad8\u8d28\u91cf\u68c0\u7d22\u6e90\uff0c\u5728\u5728\u7ebf\u63a8\u7406\u671f\u95f4\u7ed5\u8fc7\u5728\u7ebf\u68c0\u7d22\u548c\u9884\u6392\u540d\u9636\u6bb5\u3002", "result": "\u79bb\u7ebf\u5b9e\u9a8c\u663e\u793aRADAR\u663e\u8457\u63d0\u5347\u4e86\u53ec\u56de\u7387\uff08\u4e0eDNN\u68c0\u7d22\u57fa\u7ebf\u76f8\u6bd4Recall@200\u63d0\u9ad8\u4e24\u500d\uff09\uff0c\u5728\u7ebfA/B\u6d4b\u8bd5\u786e\u8ba4\u603b\u4f53\u53c2\u4e0e\u5ea6\u6307\u6807\u63d0\u9ad8\u4e860.8%\u3002", "conclusion": "RADAR\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5728\u4e25\u683c\u7684\u5728\u7ebf\u670d\u52a1\u7ea6\u675f\u4e0b\u6539\u5584\u63a8\u8350\u8d28\u91cf\u3002"}}
{"id": "2506.07271", "pdf": "https://arxiv.org/pdf/2506.07271", "abs": "https://arxiv.org/abs/2506.07271", "authors": ["Hikaru Sawafuji", "Ryota Ozaki", "Takuto Motomura", "Toyohisa Matsuda", "Masanori Tojima", "Kento Uchida", "Shinichi Shirakawa"], "title": "Machine Learning-Based Self-Localization Using Internal Sensors for Automating Bulldozers", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Self-localization is an important technology for automating bulldozers.\nConventional bulldozer self-localization systems rely on RTK-GNSS (Real Time\nKinematic-Global Navigation Satellite Systems). However, RTK-GNSS signals are\nsometimes lost in certain mining conditions. Therefore, self-localization\nmethods that do not depend on RTK-GNSS are required. In this paper, we propose\na machine learning-based self-localization method for bulldozers. The proposed\nmethod consists of two steps: estimating local velocities using a machine\nlearning model from internal sensors, and incorporating these estimates into an\nExtended Kalman Filter (EKF) for global localization. We also created a novel\ndataset for bulldozer odometry and conducted experiments across various driving\nscenarios, including slalom, excavation, and driving on slopes. The result\ndemonstrated that the proposed self-localization method suppressed the\naccumulation of position errors compared to kinematics-based methods,\nespecially when slip occurred. Furthermore, this study showed that\nbulldozer-specific sensors, such as blade position sensors and hydraulic\npressure sensors, contributed to improving self-localization accuracy.", "AI": {"tldr": "This paper presents a machine learning-based self-localization method for bulldozers which doesn't depend on RTK-GNSS and uses an Extended Kalman Filter to improve localization accuracy.", "motivation": "Self-localization is crucial for automating bulldozers, but RTK-GNSS signals can be lost in certain mining conditions. Thus, there's a need for self-localization methods that don't rely on RTK-GNSS.", "method": "The proposed method involves estimating local velocities using a machine learning model from internal sensors, then incorporating these estimates into an Extended Kalman Filter (EKF) for global localization.", "result": "The method effectively suppressed the accumulation of position errors compared to kinematics-based methods, particularly when slip occurred. Bulldozer-specific sensors improved self-localization accuracy.", "conclusion": "A machine learning-based self-localization method for bulldozers was successfully developed and tested, demonstrating superior performance over kinematics-based methods."}}
{"id": "2506.07449", "pdf": "https://arxiv.org/pdf/2506.07449", "abs": "https://arxiv.org/abs/2506.07449", "authors": ["Vahid Azizi", "Fatemeh Koochaki"], "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in\nrecommender systems through Retrieval-Augmented Generation (RAG) frameworks.\nHowever, existing RAG approaches predominantly rely on flat, similarity-based\nretrieval that fails to leverage the rich relational structure inherent in\nuser-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,\nend-to-end trainable framework that integrates personalized knowledge graph\ncontext into LLM-based recommendation ranking. Our approach extends the\nLlamaRec architecture by incorporating a lightweight user preference module\nthat dynamically identifies salient relation paths within a heterogeneous\nknowledge graph constructed from user behavior and item metadata. These\npersonalized subgraphs are seamlessly integrated into prompts for a fine-tuned\nLlama-2 model, enabling efficient and interpretable recommendations through a\nunified inference step. Comprehensive experiments on ML-100K and Amazon Beauty\ndatasets demonstrate consistent and significant improvements over LlamaRec\nacross key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates\nthe critical value of structured reasoning in LLM-based recommendations and\nestablishes a foundation for scalable, knowledge-aware personalization in\nnext-generation recommender systems. Code is available\nat~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.", "AI": {"tldr": "Recent advances in Large Language Models (LLMs) have driven their adoption in recommender systems through Retrieval-Augmented Generation (RAG) frameworks. We introduce LlamaRec-LKG-RAG, a novel single-pass, end-to-end trainable framework that integrates personalized knowledge graph context into LLM-based recommendation ranking.", "motivation": "Existing RAG approaches predominantly rely on flat, similarity-based retrieval that fails to leverage the rich relational structure inherent in user-item interactions.", "method": "Our approach extends the LlamaRec architecture by incorporating a lightweight user preference module that dynamically identifies salient relation paths within a heterogeneous knowledge graph constructed from user behavior and item metadata. These personalized subgraphs are seamlessly integrated into prompts for a fine-tuned Llama-2 model, enabling efficient and interpretable recommendations through a unified inference step.", "result": "Comprehensive experiments on ML-100K and Amazon Beauty datasets demonstrate consistent and significant improvements over LlamaRec across key ranking metrics (MRR, NDCG, Recall).", "conclusion": "LlamaRec-LKG-RAG demonstrates the critical value of structured reasoning in LLM-based recommendations and establishes a foundation for scalable, knowledge-aware personalization in next-generation recommender systems."}}
{"id": "2506.07286", "pdf": "https://arxiv.org/pdf/2506.07286", "abs": "https://arxiv.org/abs/2506.07286", "authors": ["Aditya Chakravarty"], "title": "Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted in CVPR 2025 Embodied AI Workshop", "summary": "Diffusion models have shown remarkable flexibility for solving inverse\nproblems without task-specific retraining. However, existing approaches such as\nManifold Preserving Guided Diffusion (MPGD) apply only a single gradient update\nper denoising step, limiting restoration fidelity and robustness, especially in\nembedded or out-of-distribution settings. In this work, we introduce a\nmultistep optimization strategy within each denoising timestep, significantly\nenhancing image quality, perceptual accuracy, and generalization. Our\nexperiments on super-resolution and Gaussian deblurring demonstrate that\nincreasing the number of gradient updates per step improves LPIPS and PSNR with\nminimal latency overhead. Notably, we validate this approach on a Jetson Orin\nNano using degraded ImageNet and a UAV dataset, showing that MPGD, originally\ntrained on face datasets, generalizes effectively to natural and aerial scenes.\nOur findings highlight MPGD's potential as a lightweight, plug-and-play\nrestoration module for real-time visual perception in embodied AI agents such\nas drones and mobile robots.", "AI": {"tldr": "\u901a\u8fc7\u5728\u6bcf\u4e2a\u53bb\u566a\u6b65\u9aa4\u4e2d\u5e94\u7528\u591a\u6b65\u4f18\u5316\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u6269\u6563\u6a21\u578b\u89e3\u51b3\u9006\u95ee\u9898\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u8d85\u5206\u8fa8\u7387\u548c\u9ad8\u65af\u53bb\u6a21\u7cca\u4efb\u52a1\u4e0a\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u589e\u52a0\u6bcf\u6b65\u7684\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u80fd\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5ef6\u8fdf\u5f00\u9500\u5c0f\u3002\u6b64\u65b9\u6cd5\u5728Jetson Orin Nano\u4e0a\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u539f\u672c\u5728\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684MPGD\u6a21\u578b\u4e5f\u80fd\u6709\u6548\u63a8\u5e7f\u5230\u81ea\u7136\u548c\u822a\u62cd\u573a\u666f\uff0c\u5177\u6709\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u89c6\u89c9\u611f\u77e5\u4fee\u590d\u6a21\u5757\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff08\u5982MPGD\uff09\u5728\u6bcf\u6b21\u53bb\u566a\u6b65\u9aa4\u4e2d\u4ec5\u5e94\u7528\u5355\u4e2a\u68af\u5ea6\u66f4\u65b0\uff0c\u9650\u5236\u4e86\u6062\u590d\u7684\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u5d4c\u5165\u5f0f\u6216\u5206\u5e03\u5916\u7684\u8bbe\u7f6e\u4e2d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5728\u6bcf\u4e2a\u53bb\u566a\u65f6\u95f4\u6b65\u5185\u8fdb\u884c\u591a\u6b65\u4f18\u5316\u7684\u7b56\u7565\uff0c\u4ee5\u663e\u8457\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3001\u611f\u77e5\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u589e\u52a0\u6bcf\u6b65\u7684\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u53ef\u4ee5\u6539\u5584LPIPS\u548cPSNR\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8d85\u5206\u8fa8\u7387\u548c\u9ad8\u65af\u53bb\u6a21\u7cca\u4efb\u52a1\u4e2d\uff0c\u589e\u52a0\u68af\u5ea6\u66f4\u65b0\u6b21\u6570\u80fd\u591f\u6709\u6548\u63d0\u5347\u6027\u80fd\uff0c\u4e14\u5ef6\u8fdf\u5f00\u9500\u6781\u5c0f\u3002\u6b64\u5916\uff0c\u5728Jetson Orin Nano\u4e0a\u5bf9\u9000\u5316\u7684ImageNet\u548cUAV\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cMPGD\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u4fee\u590d\u6a21\u5757\uff0c\u5177\u6709\u5728\u5177\u8eabAI\u4ee3\u7406\uff08\u5982\u65e0\u4eba\u673a\u548c\u79fb\u52a8\u673a\u5668\u4eba\uff09\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u89c6\u89c9\u611f\u77e5\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.07454", "pdf": "https://arxiv.org/pdf/2506.07454", "abs": "https://arxiv.org/abs/2506.07454", "authors": ["Jared Strader", "Aaron Ray", "Jacob Arkin", "Mason B. Peterson", "Yun Chang", "Nathan Hughes", "Christopher Bradley", "Yi Xuan Jia", "Carlos Nieto-Granda", "Rajat Talak", "Chuchu Fan", "Luca Carlone", "Jonathan P. How", "Nicholas Roy"], "title": "Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 4 figures", "summary": "In this paper, we introduce a multi-robot system that integrates mapping,\nlocalization, and task and motion planning (TAMP) enabled by 3D scene graphs to\nexecute complex instructions expressed in natural language. Our system builds a\nshared 3D scene graph incorporating an open-set object-based map, which is\nleveraged for multi-robot 3D scene graph fusion. This representation supports\nreal-time, view-invariant relocalization (via the object-based map) and\nplanning (via the 3D scene graph), allowing a team of robots to reason about\ntheir surroundings and execute complex tasks. Additionally, we introduce a\nplanning approach that translates operator intent into Planning Domain\nDefinition Language (PDDL) goals using a Large Language Model (LLM) by\nleveraging context from the shared 3D scene graph and robot capabilities. We\nprovide an experimental assessment of the performance of our system on\nreal-world tasks in large-scale, outdoor environments.", "AI": {"tldr": "This paper presents a multi-robot system that uses 3D scene graphs for mapping, localization, and task/motion planning, enabling execution of complex natural language instructions. It also introduces a planning method using LLMs to translate operator intent into PDDL goals.", "motivation": "To create a multi-robot system capable of executing complex tasks specified in natural language by integrating mapping, localization, and task/motion planning functionalities through the use of 3D scene graphs.", "method": "Developed a multi-robot system building shared 3D scene graphs incorporating open-set object-based maps for relocalization and planning. Used a planning approach leveraging Large Language Models (LLMs) to translate operator intent into PDDL goals based on context from the 3D scene graph and robot capabilities.", "result": "The experimental assessment shows the system's performance in real-world, large-scale outdoor environments.", "conclusion": "The introduced multi-robot system successfully integrates mapping, localization, and TAMP via 3D scene graphs, allowing complex task execution from natural language instructions."}}
{"id": "2506.07299", "pdf": "https://arxiv.org/pdf/2506.07299", "abs": "https://arxiv.org/abs/2506.07299", "authors": ["Hans Buehler", "Blanka Horvath", "Yannick Limmer", "Thorsten Schmidt"], "title": "Uncertainty-Aware Strategies: A Model-Agnostic Framework for Robust Financial Optimization through Subsampling", "categories": ["q-fin.CP", "cs.LG", "q-fin.MF", "q-fin.RM"], "comment": "18 pages, 12 figures", "summary": "This paper addresses the challenge of model uncertainty in quantitative\nfinance, where decisions in portfolio allocation, derivative pricing, and risk\nmanagement rely on estimating stochastic models from limited data. In practice,\nthe unavailability of the true probability measure forces reliance on an\nempirical approximation, and even small misestimations can lead to significant\ndeviations in decision quality. Building on the framework of Klibanoff et al.\n(2005), we enhance the conventional objective - whether this is expected\nutility in an investing context or a hedging metric - by superimposing an outer\n\"uncertainty measure\", motivated by traditional monetary risk measures, on the\nspace of models. In scenarios where a natural model distribution is lacking or\nBayesian methods are impractical, we propose an ad hoc subsampling strategy,\nanalogous to bootstrapping in statistical finance and related to mini-batch\nsampling in deep learning, to approximate model uncertainty. To address the\nquadratic memory demands of naive implementations, we also present an adapted\nstochastic gradient descent algorithm that enables efficient parallelization.\nThrough analytical, simulated, and empirical studies - including multi-period,\nreal data and high-dimensional examples - we demonstrate that uncertainty\nmeasures outperform traditional mixture of measures strategies and our\nmodel-agnostic subsampling-based approach not only enhances robustness against\nmodel risk but also achieves performance comparable to more elaborate Bayesian\nmethods.", "AI": {"tldr": "This paper addresses model uncertainty in quantitative finance by enhancing the conventional objective with an outer 'uncertainty measure' and proposing a subsampling strategy to approximate model uncertainty.", "motivation": "The motivation of this paper is the challenge of model uncertainty in quantitative finance, where decisions rely on estimating stochastic models from limited data. Small misestimations can lead to significant deviations in decision quality.", "method": "The method includes enhancing the conventional objective with an outer 'uncertainty measure', proposing an ad hoc subsampling strategy to approximate model uncertainty, and presenting an adapted stochastic gradient descent algorithm for efficient parallelization.", "result": "Through various studies, uncertainty measures outperform traditional mixture of measures strategies and the model-agnostic subsampling-based approach enhances robustness against model risk while achieving performance comparable to more elaborate Bayesian methods.", "conclusion": "Uncertainty measures and the proposed subsampling strategy provide robust solutions to model uncertainty in quantitative finance."}}
{"id": "2506.07458", "pdf": "https://arxiv.org/pdf/2506.07458", "abs": "https://arxiv.org/abs/2506.07458", "authors": ["Yuxin Xiao", "Shan Chen", "Jack Gallifant", "Danielle Bitterman", "Thomas Hartvigsen", "Marzyeh Ghassemi"], "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Characterizing a large language model's (LLM's) knowledge of a given question\nis challenging. As a result, prior work has primarily examined LLM behavior\nunder knowledge conflicts, where the model's internal parametric memory\ncontradicts information in the external context. However, this does not fully\nreflect how well the model knows the answer to the question. In this paper, we\nfirst introduce a taxonomy of five knowledge statuses based on the consistency\nand correctness of LLM knowledge modes. We then propose KScope, a hierarchical\nframework of statistical tests that progressively refines hypotheses about\nknowledge modes and characterizes LLM knowledge into one of these five\nstatuses. We apply KScope to nine LLMs across four datasets and systematically\nestablish: (1) Supporting context narrows knowledge gaps across models. (2)\nContext features related to difficulty, relevance, and familiarity drive\nsuccessful knowledge updates. (3) LLMs exhibit similar feature preferences when\npartially correct or conflicted, but diverge sharply when consistently wrong.\n(4) Context summarization constrained by our feature analysis, together with\nenhanced credibility, further improves update effectiveness and generalizes\nacross LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKScope\u7684\u5206\u5c42\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4e0d\u540c\u77e5\u8bc6\u72b6\u6001\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u6570\u636e\u96c6\u5bf9\u4e5d\u4e2aLLM\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u652f\u6301\u4e0a\u4e0b\u6587\u3001\u7279\u5f81\u504f\u597d\u53ca\u4e0a\u4e0b\u6587\u603b\u7ed3\u7b49\u56e0\u7d20\u5bf9LLM\u77e5\u8bc6\u66f4\u65b0\u7684\u5f71\u54cd\u3002", "motivation": "\u76ee\u524d\u5bf9\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u77e5\u8bc6\u51b2\u7a81\u60c5\u5883\u4e0b\u6a21\u578b\u7684\u8868\u73b0\uff0c\u800c\u8fd9\u79cd\u7814\u7a76\u65b9\u6cd5\u5e76\u4e0d\u80fd\u5168\u9762\u53cd\u6620\u6a21\u578b\u5bf9\u95ee\u9898\u7b54\u6848\u7684\u5b9e\u9645\u638c\u63e1\u7a0b\u5ea6\u3002", "method": "\u9996\u5148\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u77e5\u8bc6\u6a21\u5f0f\u7684\u4e00\u81f4\u6027\u548c\u6b63\u786e\u6027\u7684\u4e94\u79cd\u77e5\u8bc6\u72b6\u6001\u5206\u7c7b\u6cd5\uff0c\u63a5\u7740\u63d0\u51fa\u4e86KScope\u2014\u2014\u4e00\u4e2a\u5206\u5c42\u7684\u7edf\u8ba1\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9010\u6b65\u7ec6\u5316\u5173\u4e8e\u77e5\u8bc6\u6a21\u5f0f\u7684\u5047\u8bbe\uff0c\u5e76\u5c06LLM\u7684\u77e5\u8bc6\u72b6\u6001\u5f52\u7c7b\u4e3a\u8fd9\u4e94\u79cd\u4e4b\u4e00\u3002", "result": "(1) \u652f\u6301\u6027\u4e0a\u4e0b\u6587\u80fd\u591f\u7f29\u5c0f\u6a21\u578b\u4e4b\u95f4\u7684\u77e5\u8bc6\u5dee\u8ddd\u3002(2) \u4e0e\u96be\u5ea6\u3001\u76f8\u5173\u6027\u548c\u719f\u6089\u5ea6\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u9a71\u52a8\u4e86\u6210\u529f\u7684\u77e5\u8bc6\u66f4\u65b0\u3002(3) LLM\u5728\u90e8\u5206\u6b63\u786e\u6216\u5b58\u5728\u51b2\u7a81\u65f6\u8868\u73b0\u51fa\u76f8\u4f3c\u7684\u7279\u5f81\u504f\u597d\uff0c\u4f46\u5728\u6301\u7eed\u9519\u8bef\u65f6\u5219\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002(4) \u53d7\u7279\u5f81\u5206\u6790\u7ea6\u675f\u7684\u4e0a\u4e0b\u6587\u603b\u7ed3\u4ee5\u53ca\u589e\u5f3a\u7684\u53ef\u4fe1\u5ea6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u66f4\u65b0\u6548\u679c\uff0c\u5e76\u4e14\u9002\u7528\u4e8e\u4e0d\u540c\u7684LLM\u3002", "conclusion": "KScope\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8868\u5f81\u548c\u7406\u89e3LLM\u7684\u77e5\u8bc6\u72b6\u6001\uff0c\u652f\u6301\u6027\u4e0a\u4e0b\u6587\u548c\u7279\u5b9a\u7684\u4e0a\u4e0b\u6587\u7279\u5f81\u6709\u52a9\u4e8e\u63d0\u9ad8LLM\u7684\u77e5\u8bc6\u66f4\u65b0\u6548\u679c\uff0c\u540c\u65f6\u4e0a\u4e0b\u6587\u603b\u7ed3\u548c\u53ef\u4fe1\u5ea6\u589e\u5f3a\u53ef\u4ee5\u8fdb\u4e00\u6b65\u6539\u5584\u8fd9\u4e00\u8fc7\u7a0b\u3002"}}
{"id": "2506.07463", "pdf": "https://arxiv.org/pdf/2506.07463", "abs": "https://arxiv.org/abs/2506.07463", "authors": ["Guang Liu", "Liangdong Wang", "Jijie Li", "Yang Yu", "Yao Xu", "Jiabei Chen", "Yu Bai", "Feng Liao", "Yonghua Lin"], "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly $35$ TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully\ncurated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract $4.5$ billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora.", "AI": {"tldr": "The paper introduces CCI4.0, a large bilingual pre-training dataset with high quality and diverse reasoning patterns. It includes two sub-datasets: CCI4.0-M2-Base and CCI4.0-M2-CoT. A novel pipeline is proposed for data quality assurance. Empirical evaluations show that LLMs pre-trained on CCI4.0 perform better in downstream tasks.", "motivation": "To create a large-scale bilingual pre-training dataset with superior data quality and diverse human-like reasoning trajectory to improve the performance of LLMs.", "method": "CCI4.0 is composed of carefully curated Chinese and English corpora, along with diverse sources from math, wiki, arxiv, and code. A novel pipeline is proposed for data quality assurance through two-stage deduplication, multiclassifier quality scoring, and domain-aware fluency filtering. 4.5 billion CoT templates are extracted using staged CoT extraction.", "result": "LLMs pre-trained on CCI4.0 benefit from cleaner, more reliable training signals, leading to consistent improvements in downstream tasks, especially in math and code reflection tasks.", "conclusion": "Rigorous data curation and human thinking templates play critical roles in advancing LLM performance. The study provides insights into automatically processing pretraining corpora."}}
{"id": "2506.07327", "pdf": "https://arxiv.org/pdf/2506.07327", "abs": "https://arxiv.org/abs/2506.07327", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "title": "\"CASE: Contrastive Activation for Saliency Estimation", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.5.5; I.2.10"], "comment": "9 pages, 5 figures. Submitted to IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)", "summary": "Saliency methods are widely used to visualize which input features are deemed\nrelevant to a model's prediction. However, their visual plausibility can\nobscure critical limitations. In this work, we propose a diagnostic test for\nclass sensitivity: a method's ability to distinguish between competing class\nlabels on the same input. Through extensive experiments, we show that many\nwidely used saliency methods produce nearly identical explanations regardless\nof the class label, calling into question their reliability. We find that\nclass-insensitive behavior persists across architectures and datasets,\nsuggesting the failure mode is structural rather than model-specific. Motivated\nby these findings, we introduce CASE, a contrastive explanation method that\nisolates features uniquely discriminative for the predicted class. We evaluate\nCASE using the proposed diagnostic and a perturbation-based fidelity test, and\nshow that it produces faithful and more class-specific explanations than\nexisting methods.", "AI": {"tldr": "Saliency methods are widely used but have critical limitations. This work proposes a diagnostic test for class sensitivity, finds many methods unreliable, and introduces CASE, a new method that produces more faithful explanations.", "motivation": "To address the critical limitations of saliency methods that their visual plausibility can obscure, particularly their inability to distinguish between competing class labels on the same input.", "method": "Propose a diagnostic test for class sensitivity and introduce CASE, a contrastive explanation method that isolates features uniquely discriminative for the predicted class.", "result": "CASE produces faithful and more class-specific explanations than existing methods, as evaluated by the proposed diagnostic and a perturbation-based fidelity test.", "conclusion": "Many widely used saliency methods are unreliable due to class-insensitive behavior, and CASE is introduced as a more effective alternative."}}
{"id": "2506.07464", "pdf": "https://arxiv.org/pdf/2506.07464", "abs": "https://arxiv.org/abs/2506.07464", "authors": ["Jinyoung Park", "Jeehye Na", "Jinyoung Kim", "Hyunwoo J. Kim"], "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.", "AI": {"tldr": "DeepVideo-R1\u662f\u4e00\u79cd\u9488\u5bf9\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u51fa\u56de\u5f52\u5f0f\u7684GRPO\uff08Reg-GRPO\uff09\u548c\u96be\u5ea6\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGRPO\u5728\u5e94\u7528\u5230\u89c6\u9891LLM\u65f6\u9047\u5230\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u5df2\u88ab\u8bc1\u660e\u53ef\u4ee5\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5c06GRPO\u5e94\u7528\u4e8e\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u8f83\u5c11\u3002\u5f53\u524d\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u4fdd\u62a4\u673a\u5236\u548c\u4f18\u52bf\u6d88\u5931\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u6709\u6548\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepVideo-R1\u7684\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u56de\u5f52\u5f0f\u7684GRPO\uff08Reg-GRPO\uff09\u548c\u96be\u5ea6\u611f\u77e5\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002Reg-GRPO\u5c06GRPO\u76ee\u6807\u91cd\u65b0\u5b9a\u4e49\u4e3a\u56de\u5f52\u4efb\u52a1\uff0c\u76f4\u63a5\u9884\u6d4b\u4f18\u52bf\u503c\uff0c\u6d88\u9664\u4e86\u5bf9\u526a\u88c1\u7b49\u4fdd\u62a4\u673a\u5236\u7684\u9700\u6c42\uff1b\u6570\u636e\u589e\u5f3a\u7b56\u7565\u52a8\u6001\u589e\u52a0\u53ef\u89e3\u51b3\u96be\u5ea6\u7ea7\u522b\u7684\u6837\u672c\uff0c\u63d0\u4f9b\u591a\u6837\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDeepVideo-R1\u5728\u591a\u4e2a\u89c6\u9891\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "DeepVideo-R1\u901a\u8fc7\u5f15\u5165Reg-GRPO\u548c\u96be\u5ea6\u611f\u77e5\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6210\u529f\u514b\u670d\u4e86GRPO\u5728\u89c6\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u5927\u5e45\u63d0\u5347\u4e86\u89c6\u9891\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.07471", "pdf": "https://arxiv.org/pdf/2506.07471", "abs": "https://arxiv.org/abs/2506.07471", "authors": ["CH Cho", "WJ Moon", "W Jun", "MS Jung", "JP Heo"], "title": "Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI 2025", "summary": "Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a\nspecific segment is relevant to a given text query. Typical training processes\nof PRVR assume a one-to-one relationship where each text query is relevant to\nonly one video. However, we point out the inherent ambiguity between text and\nvideo content based on their conceptual scope and propose a framework that\nincorporates this ambiguity into the model learning process. Specifically, we\npropose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous\ntext-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:\nuncertainty and similarity. Uncertainty represents whether instances include\ncommonly shared context across the dataset, while similarity indicates\npair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL\nhierarchically learns the semantic relationship via multi-positive contrastive\nlearning and dual triplet margin loss. Additionally, we delve into fine-grained\nrelationships within the video instances. Unlike typical training at the\ntext-video level, where pairwise information is provided, we address the\ninherent ambiguity within frames of the same untrimmed video, which often\ncontains multiple contexts. This allows us to further enhance learning at the\ntext-frame level. Lastly, we propose cross-model ambiguity detection to\nmitigate the error propagation that occurs when a single model is employed to\ndetect ambiguous pairs for its training. With all components combined, our\nproposed method demonstrates its effectiveness in PRVR.", "AI": {"tldr": "Partially Relevant Video Retrieval (PRVR) aims to retrieve videos with specific relevant segments given a text query. The paper proposes Ambiguity-Restrained representation Learning (ARL) to address ambiguity in text-video pairs through multi-positive contrastive learning, dual triplet margin loss, fine-grained relationships within video frames, and cross-model ambiguity detection.", "motivation": "Existing PRVR methods assume a one-to-one relationship between text queries and videos, ignoring inherent ambiguities based on conceptual scope.", "method": "ARL detects ambiguous text-video pairs using uncertainty and similarity criteria. It applies multi-positive contrastive learning and dual triplet margin loss for hierarchical semantic relationship learning. Fine-grained relationships within video frames are explored at the text-frame level. Cross-model ambiguity detection mitigates error propagation.", "result": "The proposed method effectively addresses ambiguities in PRVR, enhancing retrieval performance.", "conclusion": "Incorporating ambiguity into the model learning process significantly improves PRVR effectiveness."}}
{"id": "2506.07351", "pdf": "https://arxiv.org/pdf/2506.07351", "abs": "https://arxiv.org/abs/2506.07351", "authors": ["Jun Chen", "Lina Liu", "Tianyi Zhu", "Yong Liu", "Guang Dai", "Yunliang Jiang", "Ivor W. Tsang"], "title": "Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking", "categories": ["math.OC", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "This paper considers the problem of decentralized optimization on compact\nsubmanifolds, where a finite sum of smooth (possibly non-convex) local\nfunctions is minimized by $n$ agents forming an undirected and connected graph.\nHowever, the efficiency of distributed optimization is often hindered by\ncommunication bottlenecks. To mitigate this, we propose the Quantized\nRiemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local\nvariables using quantized gradients. The introduction of quantization noise\nallows our algorithm to bypass the constraints of the accurate Riemannian\nprojection operator (such as retraction), further improving iterative\nefficiency. To the best of our knowledge, this is the first algorithm to\nachieve an $\\mathcal{O}(1/K)$ convergence rate in the presence of quantization,\nmatching the convergence rate of methods without quantization. Additionally, we\nexplicitly derive lower bounds on decentralized consensus associated with a\nfunction of quantization levels. Numerical experiments demonstrate that Q-RGT\nperforms comparably to non-quantized methods while reducing communication\nbottlenecks and computational overhead.", "AI": {"tldr": "This paper introduces Quantized Riemannian Gradient Tracking (Q-RGT) algorithm for decentralized optimization on compact submanifolds, which uses quantized gradients to reduce communication bottlenecks while maintaining an O(1/K) convergence rate.", "motivation": "Decentralized optimization methods can be hindered by communication bottlenecks. The motivation is to develop an efficient method that reduces these bottlenecks without sacrificing performance.", "method": "The proposed method is the Quantized Riemannian Gradient Tracking (Q-RGT) algorithm where agents update their local variables using quantized gradients. This allows bypassing constraints of accurate Riemannian projection operators and improves iterative efficiency.", "result": "Q-RGT achieves an O(1/K) convergence rate despite quantization noise, matching the convergence rate of non-quantized methods. Numerical experiments show comparable performance with reduced communication and computational overhead.", "conclusion": "Quantized Riemannian Gradient Tracking is an effective method for decentralized optimization on compact submanifolds, offering a way to reduce communication bottlenecks without compromising convergence rates."}}
{"id": "2506.07484", "pdf": "https://arxiv.org/pdf/2506.07484", "abs": "https://arxiv.org/abs/2506.07484", "authors": ["Dasol Hong", "Wooju Lee", "Hyun Myung"], "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.5.2"], "comment": "8 pages, 5 figures; accepted at ICML 2025", "summary": "Prompt tuning, which adapts vision-language models by freezing model\nparameters and optimizing only the prompt, has proven effective for\ntask-specific adaptations. The core challenge in prompt tuning is improving\nspecialization for a specific task and generalization for unseen domains.\nHowever, frozen encoders often produce misaligned features, leading to\nconfusion between classes and limiting specialization. To overcome this issue,\nwe propose a confusion-aware loss (CoA-loss) that improves specialization by\nrefining the decision boundaries between confusing classes. Additionally, we\nmathematically demonstrate that a mixture model can enhance generalization\nwithout compromising specialization. This is achieved using confidence-aware\nweights (CoA-weights), which adjust the weights of each prediction in the\nmixture model based on its confidence within the class domains. Extensive\nexperiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,\noutperforms state-of-the-art methods by enhancing specialization and\ngeneralization. Our code is publicly available at\nhttps://github.com/url-kaist/CoCoA-Mix.", "AI": {"tldr": "The paper introduces CoCoA-Mix, a model using CoA-loss and CoA-weights to improve prompt tuning in vision-language models. It enhances specialization through refined decision boundaries and generalization via confidence-aware weights.", "motivation": "Prompt tuning has been effective for task-specific adaptations but faces challenges with specialization due to misaligned features from frozen encoders and generalization for unseen domains.", "method": "The authors propose a confusion-aware loss (CoA-loss) to refine decision boundaries between confusing classes and confidence-aware weights (CoA-weights) to adjust prediction weights in a mixture model, thus improving both specialization and generalization.", "result": "Extensive experiments demonstrate that CoCoA-Mix outperforms state-of-the-art methods in enhancing both specialization and generalization.", "conclusion": "CoCoA-Mix is an effective approach for prompt tuning in vision-language models, addressing the issues of specialization and generalization."}}
{"id": "2506.07357", "pdf": "https://arxiv.org/pdf/2506.07357", "abs": "https://arxiv.org/abs/2506.07357", "authors": ["Satvik Praveen", "Yoonsung Jung"], "title": "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object detection is vital in precision agriculture for plant monitoring,\ndisease detection, and yield estimation. However, models like YOLO struggle\nwith occlusions, irregular structures, and background noise, reducing detection\naccuracy. While Spatial Transformer Networks (STNs) improve spatial invariance\nthrough learned transformations, affine mappings are insufficient for non-rigid\ndeformations such as bent leaves and overlaps.\n  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)\ninto STNs for flexible, non-rigid spatial transformations that better align\nfeatures. Performance is further enhanced by the Convolutional Block Attention\nModule (CBAM), which suppresses background noise and emphasizes relevant\nspatial and channel-wise features.\n  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model\noutperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction\nin false positives, highlighting the benefits of improved spatial flexibility\nand attention-guided refinement. We also examine the impact of the TPS\nregularization parameter in balancing transformation smoothness and detection\nperformance.\n  This lightweight model improves spatial awareness and supports real-time edge\ndeployment, making it ideal for smart farming applications requiring accurate\nand efficient monitoring.", "AI": {"tldr": "An object detection model named CBAM-STN-TPS-YOLO is proposed for precision agriculture, integrating Thin-Plate Splines (TPS) into Spatial Transformer Networks (STNs) and using Convolutional Block Attention Module (CBAM). It shows better performance on the Plant Growth and Phenotyping (PGP) dataset.", "motivation": "Current models like YOLO have issues with occlusions, irregular structures, and background noise which reduce detection accuracy. Affine mappings used in STNs are insufficient for non-rigid deformations.", "method": "The proposed model integrates Thin-Plate Splines (TPS) into Spatial Transformer Networks (STNs) for flexible non-rigid spatial transformations and uses Convolutional Block Attention Module (CBAM) to suppress background noise and emphasize relevant features.", "result": "On the PGP dataset, the model outperforms STN-YOLO in precision, recall, and mAP with a 12% reduction in false positives. The impact of the TPS regularization parameter is also examined.", "conclusion": "The lightweight model improves spatial awareness and supports real-time edge deployment, making it suitable for smart farming applications."}}
{"id": "2506.07520", "pdf": "https://arxiv.org/pdf/2506.07520", "abs": "https://arxiv.org/abs/2506.07520", "authors": ["Shun Lei", "Yaoxun Xu", "Zhiwei Lin", "Huaicheng Zhang", "Wei Tan", "Hangting Chen", "Jianwei Yu", "Yixuan Zhang", "Chenyu Yang", "Haina Zhu", "Shuai Wang", "Zhiyong Wu", "Dong Yu"], "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Recent advances in large language models (LLMs) and audio language models\nhave significantly improved music generation, particularly in lyrics-to-song\ngeneration. However, existing approaches still struggle with the complex\ncomposition of songs and the scarcity of high-quality data, leading to\nlimitations in sound quality, musicality, instruction following, and\nvocal-instrument harmony. To address these challenges, we introduce LeVo, an\nLM-based framework consisting of LeLM and a music codec. LeLM is capable of\nparallelly modeling two types of tokens: mixed tokens, which represent the\ncombined audio of vocals and accompaniment to achieve vocal-instrument harmony,\nand dual-track tokens, which separately encode vocals and accompaniment for\nhigh-quality song generation. It employs two decoder-only transformers and a\nmodular extension training strategy to prevent interference between different\ntoken types. To further enhance musicality and instruction following, we\nintroduce a multi-preference alignment method based on Direct Preference\nOptimization (DPO). This method handles diverse human preferences through a\nsemi-automatic data construction process and DPO post-training. Experimental\nresults demonstrate that LeVo consistently outperforms existing methods on both\nobjective and subjective metrics. Ablation studies further justify the\neffectiveness of our designs. Audio examples are available at\nhttps://levo-demo.github.io/.", "AI": {"tldr": "LeVo is a new framework which can generate high-quality songs by modeling mixed and dual-track tokens, employing decoder-only transformers and modular extension training strategy, and introducing multi-preference alignment method. It surpasses existing methods in various aspects.", "motivation": "To solve the problems of complex song composition and lack of high-quality data that limit the sound quality, musicality, instruction following, and vocal-instrument harmony in music generation.", "method": "The framework LeVo consists of LeLM and a music codec. LeLM models two types of tokens - mixed tokens for vocal-instrument harmony and dual-track tokens for high-quality song generation using two decoder-only transformers and a modular extension training strategy. Also, a multi-preference alignment method based on Direct Preference Optimization is introduced to improve musicality and instruction following.", "result": "Experimental results show that LeVo outperforms current methods in both objective and subjective metrics. Ablation studies confirm the effectiveness of its designs.", "conclusion": "LeVo presents an effective solution for generating high-quality songs with better sound quality, musicality, instruction following, and vocal-instrument harmony."}}
{"id": "2506.07524", "pdf": "https://arxiv.org/pdf/2506.07524", "abs": "https://arxiv.org/abs/2506.07524", "authors": ["Shiwei Feng", "Xiangzhe Xu", "Xuan Chen", "Kaiyuan Zhang", "Syed Yusuf Ahmed", "Zian Su", "Mingwei Zheng", "Xiangyu Zhang"], "title": "IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": null, "summary": "LLM agents are increasingly deployed to automate real-world tasks by invoking\nAPIs through natural language instructions. While powerful, they often suffer\nfrom misinterpretation of user intent, leading to the agent's actions that\ndiverge from the user's intended goal, especially as external toolkits evolve.\nTraditional software testing assumes structured inputs and thus falls short in\nhandling the ambiguity of natural language. We introduce IntenTest, an\nAPI-centric stress testing framework that systematically uncovers intent\nintegrity violations in LLM agents. Unlike prior work focused on fixed\nbenchmarks or adversarial inputs, IntenTest generates realistic tasks based on\ntoolkits' documentation and applies targeted mutations to expose subtle agent\nerrors while preserving user intent. To guide testing, we propose semantic\npartitioning, which organizes natural language tasks into meaningful categories\nbased on toolkit API parameters and their equivalence classes. Within each\npartition, seed tasks are mutated and ranked by a lightweight predictor that\nestimates the likelihood of triggering agent errors. To enhance efficiency,\nIntenTest maintains a datatype-aware strategy memory that retrieves and adapts\neffective mutation patterns from past cases. Experiments on 80 toolkit APIs\ndemonstrate that IntenTest effectively uncovers intent integrity violations,\nsignificantly outperforming baselines in both error-exposing rate and query\nefficiency. Moreover, IntenTest generalizes well to stronger target models\nusing smaller LLMs for test generation, and adapts to evolving APIs across\ndomains.", "AI": {"tldr": "IntenTest is an API-centric stress testing framework that systematically uncovers intent integrity violations in LLM agents by generating realistic tasks based on toolkits' documentation and applying targeted mutations.", "motivation": "LLM agents often misinterpret user intent, leading to actions that diverge from the intended goal. Traditional software testing methods are insufficient for handling the ambiguity of natural language.", "method": "IntenTest generates realistic tasks based on toolkits' documentation and applies targeted mutations to expose subtle agent errors while preserving user intent. It uses semantic partitioning to organize tasks into meaningful categories and a datatype-aware strategy memory to enhance efficiency.", "result": "Experiments on 80 toolkit APIs demonstrate that IntenTest effectively uncovers intent integrity violations, outperforming baselines in both error-exposing rate and query efficiency.", "conclusion": "IntenTest generalizes well to stronger target models using smaller LLMs for test generation and adapts to evolving APIs across domains."}}
{"id": "2506.07539", "pdf": "https://arxiv.org/pdf/2506.07539", "abs": "https://arxiv.org/abs/2506.07539", "authors": ["Xiaomeng Zhu", "Jacob Henningsson", "Duruo Li", "P\u00e4r M\u00e5rtensson", "Lars Hanson", "M\u00e5rten Bj\u00f6rkman", "Atsuto Maki"], "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study", "categories": ["cs.CV", "cs.AI"], "comment": "This is accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), waiting for publication. 14 pages, 14 figures", "summary": "This paper addresses key aspects of domain randomization in generating\nsynthetic data for manufacturing object detection applications. To this end, we\npresent a comprehensive data generation pipeline that reflects different\nfactors: object characteristics, background, illumination, camera settings, and\npost-processing. We also introduce the Synthetic Industrial Parts Object\nDetection dataset (SIP15-OD) consisting of 15 objects from three industrial use\ncases under varying environments as a test bed for the study, while also\nemploying an industrial dataset publicly available for robotic applications. In\nour experiments, we present more abundant results and insights into the\nfeasibility as well as challenges of sim-to-real object detection. In\nparticular, we identified material properties, rendering methods,\npost-processing, and distractors as important factors. Our method, leveraging\nthese, achieves top performance on the public dataset with Yolov8 models\ntrained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics\ndataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,\nrespectively. The results showcase the effectiveness of the proposed domain\nrandomization, potentially covering the distribution close to real data for the\napplications.", "AI": {"tldr": "The paper explores domain randomization in synthetic data generation for manufacturing object detection, presenting a data generation pipeline and the SIP15-OD dataset. Experiments reveal key factors like material properties and post-processing, achieving high mAP scores with Yolov8 models.", "motivation": "To address challenges in sim-to-real object detection by investigating the impact of various factors through domain randomization.", "method": "Developed a comprehensive data generation pipeline considering object characteristics, background, illumination, camera settings, and post-processing. Introduced SIP15-OD dataset and used an existing public industrial dataset for experiments.", "result": "Identified material properties, rendering methods, post-processing, and distractors as crucial factors. Achieved top performance on public dataset with Yolov8 models trained on synthetic data, obtaining mAP@50 scores of 96.4% for robotics dataset and 94.1%, 99.5%, 95.3% for SIP15-OD use cases.", "conclusion": "Domain randomization is effective in generating synthetic data close to real data distribution for manufacturing object detection applications."}}
{"id": "2506.07398", "pdf": "https://arxiv.org/pdf/2506.07398", "abs": "https://arxiv.org/abs/2506.07398", "authors": ["Guibin Zhang", "Muxin Fu", "Guancheng Wan", "Miao Yu", "Kun Wang", "Shuicheng Yan"], "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "categories": ["cs.MA", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.", "AI": {"tldr": "G-Memory is a hierarchical memory system for multi-agent systems (MAS) inspired by organizational memory theory, improving success rates in embodied action and accuracy in knowledge QA.", "motivation": "To address the limitations of current MAS memory mechanisms which are overly simplistic and lack customization, hindering the self-evolution capacity of MAS.", "method": "Introduced G-Memory, a three-tier graph hierarchy (insight, query, and interaction graphs) that manages lengthy MAS interactions. It retrieves high-level insights and fine-grained interaction trajectories via bi-directional memory traversal and evolves by assimilating new collaborative trajectories.", "result": "Experiments across five benchmarks, three LLM backbones, and three MAS frameworks showed G-Memory improves success rates in embodied action by up to 20.89% and accuracy in knowledge QA by up to 10.12%. No modifications were needed to original frameworks.", "conclusion": "G-Memory bridges the gap in MAS memory architectures, enhancing cognitive and execution capabilities through an advanced hierarchical, agentic memory system."}}
{"id": "2506.07542", "pdf": "https://arxiv.org/pdf/2506.07542", "abs": "https://arxiv.org/abs/2506.07542", "authors": ["Bowen Liu", "Weiyi Zhang", "Peranut Chotcomwongse", "Xiaolan Chen", "Ruoyu Chen", "Pawin Pakaymaskul", "Niracha Arjkongharn", "Nattaporn Vongsa", "Xuelian Cheng", "Zongyuan Ge", "Kun Huang", "Xiaohui Li", "Yiru Duan", "Zhenbang Wang", "BaoYe Xie", "Qiang Chen", "Huazhu Fu", "Michael A. Mahr", "Jiaqi Qu", "Wangyiyang Chen", "Shiye Wang", "Yubo Tan", "Yongjie Li", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides high-resolution, 3D, and\nnon-invasive visualization of retinal layers in vivo, serving as a critical\ntool for lesion localization and disease diagnosis. However, its widespread\nadoption is limited by equipment costs and the need for specialized operators.\nIn comparison, 2D color fundus photography offers faster acquisition and\ngreater accessibility with less dependence on expensive devices. Although\ngenerative artificial intelligence has demonstrated promising results in\nmedical image synthesis, translating 2D fundus images into 3D OCT images\npresents unique challenges due to inherent differences in data dimensionality\nand biological information between modalities. To advance generative models in\nthe fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society\n(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT\nGeneration from Fundus Images. This paper details the challenge framework\n(referred to as APTOS-2024 Challenge), including: the benchmark dataset,\nevaluation methodology featuring two fidelity metrics-image-based distance\n(pixel-level OCT B-scan similarity) and video-based distance (semantic-level\nvolumetric consistency), and analysis of top-performing solutions. The\nchallenge attracted 342 participating teams, with 42 preliminary submissions\nand 9 finalists. Leading methodologies incorporated innovations in hybrid data\npreprocessing or augmentation (cross-modality collaborative paradigms),\npre-training on external ophthalmic imaging datasets, integration of vision\nfoundation models, and model architecture improvement. The APTOS-2024 Challenge\nis the first benchmark demonstrating the feasibility of fundus-to-3D-OCT\nsynthesis as a potential solution for improving ophthalmic care accessibility\nin under-resourced healthcare settings, while helping to expedite medical\nresearch and clinical applications.", "AI": {"tldr": "The APTOS-2024 Challenge explored the feasibility of generating 3D OCT images from 2D fundus images, attracting many participants and showcasing innovative methodologies.", "motivation": "To overcome the limitations of OCT such as high equipment costs and need for specialized operators by utilizing 2D color fundus photography which is faster and more accessible.", "method": "Organized a challenge with benchmark dataset and evaluation methodology including image-based distance and video-based distance metrics. Top solutions involved hybrid data preprocessing/augmentation, pre-training on external datasets, integration of vision foundation models, and model architecture improvements.", "result": "Attracted 342 teams, had 42 preliminary submissions and 9 finalists. Demonstrated the potential of fundus-to-3D-OCT synthesis for improving healthcare accessibility.", "conclusion": "The APTOS-2024 Challenge was successful in showing that fundus-to-3D-OCT synthesis could be a viable solution to enhance ophthalmic care accessibility and accelerate medical research."}}
{"id": "2506.07555", "pdf": "https://arxiv.org/pdf/2506.07555", "abs": "https://arxiv.org/abs/2506.07555", "authors": ["Haoxiang Wang", "Zinan Lin", "Da Yu", "Huishuai Zhang"], "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.", "AI": {"tldr": "\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5dee\u5f02\u9690\u79c1\uff08DP\uff09\u5408\u6210\u56fe\u50cf\u4e3a\u5171\u4eab\u548c\u5206\u6790\u654f\u611f\u89c6\u89c9\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\uff0c\u800c\u4e0d\u4f1a\u4fb5\u72af\u4e2a\u4eba\u9690\u79c1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684DP\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5fe0\u5b9e\u6355\u6349\u539f\u59cb\u6570\u636e\u7ed3\u6784\u7684\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u3002\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u79f0\u4e3a\u901a\u8fc7\u79c1\u6709\u6587\u672c\u4e2d\u4ecb\u8fdb\u884c\u5408\u6210\uff08SPTI\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u8f7b\u677e\u751f\u6210\u9ad8\u5206\u8fa8\u7387DP\u56fe\u50cf\u3002SPTI\u7684\u5173\u952e\u601d\u60f3\u662f\u901a\u8fc7\u5229\u7528\u6700\u5148\u8fdb\u7684DP\u6587\u672c\u751f\u6210\u65b9\u6cd5\uff0c\u5c06DP\u56fe\u50cf\u5408\u6210\u7684\u6311\u6218\u4ece\u56fe\u50cf\u57df\u8f6c\u79fb\u5230\u6587\u672c\u57df\u3002SPTI\u9996\u5148\u4f7f\u7528\u56fe\u50cf\u5230\u6587\u672c\u6a21\u578b\u5c06\u6bcf\u4e2a\u79c1\u6709\u56fe\u50cf\u603b\u7ed3\u4e3a\u7b80\u6d01\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u7136\u540e\u5e94\u7528\u4fee\u6539\u540e\u7684\u79c1\u6709\u8fdb\u5316\u7b97\u6cd5\u751f\u6210DP\u6587\u672c\uff0c\u6700\u540e\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u91cd\u5efa\u56fe\u50cf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSPTI\u4e0d\u9700\u8981\u6a21\u578b\u8bad\u7ec3\uff0c\u53ea\u9700\u4f7f\u7528\u73b0\u6210\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002\u7ed9\u5b9a\u4e00\u4e2a\u79c1\u6709\u6570\u636e\u96c6\uff0cSPTI\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u8d28\u91cf\u6bd4\u4e4b\u524d\u7684DP\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u3002\u5728LSUN\u5367\u5ba4\u6570\u636e\u96c6\u4e0a\uff0cSPTI\u5728epsilon\u7b49\u4e8e1.0\u65f6\u8fbe\u5230FID\u5c0f\u4e8e\u6216\u7b49\u4e8e26.71\uff0c\u4f18\u4e8e\u79c1\u6709\u8fdb\u5316FID\u768440.36\u3002\u540c\u6837\uff0c\u5728MM CelebA HQ\u4e0a\uff0cSPTI\u5728epsilon\u7b49\u4e8e1.0\u65f6\u8fbe\u5230FID\u5c0f\u4e8e\u6216\u7b49\u4e8e33.27\uff0c\u800cDP\u5fae\u8c03\u57fa\u7ebf\u4e3a57.01\u3002\u603b\u7684\u6765\u8bf4\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u79c1\u6709\u6587\u672c\u4e2d\u4ecb\u8fdb\u884c\u5408\u6210\u4e3a\u751f\u6210\u9ad8\u5206\u8fa8\u7387DP\u5408\u6210\u56fe\u50cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u9ad8\u6548\u4e14\u4e13\u6709\u6a21\u578b\u517c\u5bb9\u7684\u6846\u67b6\uff0c\u5927\u5927\u6269\u5c55\u4e86\u5bf9\u79c1\u6709\u89c6\u89c9\u6570\u636e\u96c6\u7684\u8bbf\u95ee\u3002", "motivation": "\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u5dee\u5f02\u9690\u79c1\uff08DP\uff09\u5408\u6210\u56fe\u50cf\u4e3a\u5171\u4eab\u548c\u5206\u6790\u654f\u611f\u89c6\u89c9\u6570\u636e\u63d0\u4f9b\u4e86\u6709\u5e0c\u671b\u7684\u9014\u5f84\uff0c\u800c\u4e0d\u4f1a\u4fb5\u72af\u4e2a\u4eba\u9690\u79c1\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684DP\u56fe\u50cf\u5408\u6210\u65b9\u6cd5\u96be\u4ee5\u751f\u6210\u5fe0\u5b9e\u6355\u6349\u539f\u59cb\u6570\u636e\u7ed3\u6784\u7684\u9ad8\u5206\u8fa8\u7387\u8f93\u51fa\u3002", "method": "SPTI\u9996\u5148\u4f7f\u7528\u56fe\u50cf\u5230\u6587\u672c\u6a21\u578b\u5c06\u6bcf\u4e2a\u79c1\u6709\u56fe\u50cf\u603b\u7ed3\u4e3a\u7b80\u6d01\u7684\u6587\u672c\u63cf\u8ff0\uff0c\u7136\u540e\u5e94\u7528\u4fee\u6539\u540e\u7684\u79c1\u6709\u8fdb\u5316\u7b97\u6cd5\u751f\u6210DP\u6587\u672c\uff0c\u6700\u540e\u4f7f\u7528\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u91cd\u5efa\u56fe\u50cf\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cSPTI\u4e0d\u9700\u8981\u6a21\u578b\u8bad\u7ec3\uff0c\u53ea\u9700\u4f7f\u7528\u73b0\u6210\u7684\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5728LSUN\u5367\u5ba4\u6570\u636e\u96c6\u4e0a\uff0cSPTI\u5728epsilon\u7b49\u4e8e1.0\u65f6\u8fbe\u5230FID\u5c0f\u4e8e\u6216\u7b49\u4e8e26.71\uff0c\u4f18\u4e8e\u79c1\u6709\u8fdb\u5316FID\u768440.36\u3002\u540c\u6837\uff0c\u5728MM CelebA HQ\u4e0a\uff0cSPTI\u5728epsilon\u7b49\u4e8e1.0\u65f6\u8fbe\u5230FID\u5c0f\u4e8e\u6216\u7b49\u4e8e33.27\uff0c\u800cDP\u5fae\u8c03\u57fa\u7ebf\u4e3a57.01\u3002", "conclusion": "\u901a\u8fc7\u79c1\u6709\u6587\u672c\u4e2d\u4ecb\u8fdb\u884c\u5408\u6210\u4e3a\u751f\u6210\u9ad8\u5206\u8fa8\u7387DP\u5408\u6210\u56fe\u50cf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d44\u6e90\u9ad8\u6548\u4e14\u4e13\u6709\u6a21\u578b\u517c\u5bb9\u7684\u6846\u67b6\uff0c\u5927\u5927\u6269\u5c55\u4e86\u5bf9\u79c1\u6709\u89c6\u89c9\u6570\u636e\u96c6\u7684\u8bbf\u95ee\u3002"}}
{"id": "2506.07557", "pdf": "https://arxiv.org/pdf/2506.07557", "abs": "https://arxiv.org/abs/2506.07557", "authors": ["Mengsong Wu", "Di Zhang", "Yuqiang Li", "Dongzhan Zhou", "Wenliang Chen"], "title": "SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "While Large Language Models (LLMs) have achieved remarkable success in a wide\nrange of applications, their performance often degrades in complex reasoning\ntasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a\nnovel framework that leverages a modified Monte Carlo Tree Search (MCTS) to\nenhance LLM reasoning without relying on external reward models. By redefining\nthe Upper Confidence Bound scoring to align with intrinsic self-evaluation\ncapabilities of LLMs and decomposing the inference process into atomic subtasks\naugmented with semantic clustering at each node, SELT effectively balances\nexploration and exploitation, reduces redundant reasoning paths, and mitigates\nhallucination. We validate our approach on challenging benchmarks, including\nthe knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT\nachieves significant improvements in answer accuracy and reasoning robustness\ncompared to baseline methods. Notably, our framework operates without\ntask-specific fine-tuning, demonstrating strong generalizability across diverse\nreasoning tasks. Relevant results and code are available at\nhttps://github.com/fairyshine/SELT .", "AI": {"tldr": "SELT is a new framework that improves LLMs' reasoning ability in complex tasks by modifying MCTS, showing significant improvements without task-specific fine-tuning.", "motivation": "Although LLMs have been successful in many areas, their performance in complex reasoning tasks is still limited.", "method": "SELT uses a modified Monte Carlo Tree Search to enhance LLM reasoning. It redefines Upper Confidence Bound scoring based on LLMs' self-evaluation capabilities and breaks down the inference process into smaller subtasks with semantic clustering at each node.", "result": "SELT shows significant improvements in answer accuracy and reasoning robustness on challenging benchmarks like MMLU and Seal-Tools compared to baseline methods.", "conclusion": "SELT enhances LLM reasoning without task-specific fine-tuning, demonstrating strong generalizability across diverse reasoning tasks."}}
{"id": "2506.07563", "pdf": "https://arxiv.org/pdf/2506.07563", "abs": "https://arxiv.org/abs/2506.07563", "authors": ["Ken Yagel", "Eyal German", "Aviel Ben Siman Tov"], "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Personalized recommendation systems must adapt to user interactions across\ndifferent domains. Traditional approaches like MLoRA apply a single adaptation\nper domain but lack flexibility in handling diverse user behaviors. To address\nthis, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is\nfirst trained independently to specialize in its domain before a gating network\nis trained to weight their contributions dynamically. We evaluate MoE-MLoRA\nacross eight CTR models on Movielens and Taobao, showing that it improves\nperformance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)\nbut offers limited benefits in structured datasets with low domain diversity\nand sparsity. Further analysis of the number of experts per domain reveals that\nlarger ensembles do not always improve performance, indicating the need for\nmodel-aware tuning. Our findings highlight the potential of expert-based\narchitectures for multi-domain recommendation systems, demonstrating that\ntask-aware specialization and adaptive gating can enhance predictive accuracy\nin complex environments. The implementation and code are available in our\nGitHub repository.", "AI": {"tldr": "Personalized recommendation systems need to adapt to user interactions across different domains. Traditional approaches lack flexibility, so MoE-MLoRA, a mixture-of-experts framework was proposed. Each expert is trained independently before a gating network weights their contributions dynamically. Evaluated across eight CTR models on Movielens and Taobao, it shows improved performance in large-scale dynamic datasets but limited benefits in structured datasets with low domain diversity and sparsity. Larger ensembles don't always improve performance, indicating the need for model-aware tuning.", "motivation": "Traditional approaches like MLoRA apply a single adaptation per domain but lack flexibility in handling diverse user behaviors.", "method": "Propose MoE-MLoRA, a mixture-of-experts framework where each expert is first trained independently to specialize in its domain before a gating network is trained to weight their contributions dynamically.", "result": "Improves performance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20) but offers limited benefits in structured datasets with low domain diversity and sparsity.", "conclusion": "Findings highlight the potential of expert-based architectures for multi-domain recommendation systems, demonstrating that task-aware specialization and adaptive gating can enhance predictive accuracy in complex environments."}}
{"id": "2506.07570", "pdf": "https://arxiv.org/pdf/2506.07570", "abs": "https://arxiv.org/abs/2506.07570", "authors": ["Yixuan Yang", "Zhen Luo", "Tongsheng Ding", "Junru Lu", "Mingqi Gao", "Jinyu Yang", "Victor Sanchez", "Feng Zheng"], "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automatic indoor layout generation has attracted increasing attention due to\nits potential in interior design, virtual environment construction, and\nembodied AI. Existing methods fall into two categories: prompt-driven\napproaches that leverage proprietary LLM services (e.g., GPT APIs) and\nlearning-based methods trained on layout data upon diffusion-based models.\nPrompt-driven methods often suffer from spatial inconsistency and high\ncomputational costs, while learning-based methods are typically constrained by\ncoarse relational graphs and limited datasets, restricting their generalization\nto diverse room categories. In this paper, we revisit LLM-based indoor layout\ngeneration and present 3D-SynthPlace, a large-scale dataset that combines\nsynthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,\nupgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000\nscenes, covering four common room types -- bedroom, living room, kitchen, and\nbathroom -- enriched with diverse objects and high-level spatial annotations.\nWe further introduce OptiScene, a strong open-source LLM optimized for indoor\nlayout generation, fine-tuned based on our 3D-SynthPlace dataset through our\ntwo-stage training. For the warum-up stage I, we adopt supervised fine-tuning\n(SFT), which is taught to first generate high-level spatial descriptions then\nconditionally predict concrete object placements. For the reinforcing stage II,\nto better align the generated layouts with human design preferences, we apply\nmulti-turn direct preference optimization (DPO), which significantly improving\nlayout quality and generation success rates. Extensive experiments demonstrate\nthat OptiScene outperforms traditional prompt-driven and learning-based\nbaselines. Moreover, OptiScene shows promising potential in interactive tasks\nsuch as scene editing and robot navigation.", "AI": {"tldr": "The paper introduces 3D-SynthPlace, a large-scale dataset for indoor layout generation and OptiScene, an open-source LLM optimized for this task. OptiScene surpasses traditional methods in layout quality and has potential in interactive tasks.", "motivation": "Automatic indoor layout generation is crucial for applications in interior design, virtual environment construction, and embodied AI. Existing methods have limitations such as spatial inconsistency, high computational costs, coarse relational graphs, and limited datasets.", "method": "1. Created 3D-SynthPlace dataset with synthetic layouts via 'GPT synthesize, Human inspect' pipeline.2. Introduced OptiScene, an open-source LLM fine-tuned on 3D-SynthPlace through two-stage training:- Stage I: Supervised fine-tuning (SFT) to generate spatial descriptions and predict object placements.- Stage II: Multi-turn direct preference optimization (DPO) to align with human design preferences.", "result": "OptiScene outperforms traditional prompt-driven and learning-based baselines in layout quality and generation success rates. It also shows potential in interactive tasks like scene editing and robot navigation.", "conclusion": "3D-SynthPlace and OptiScene provide a strong foundation for advancing automatic indoor layout generation, overcoming limitations of previous methods and demonstrating potential in various applications."}}
{"id": "2506.07575", "pdf": "https://arxiv.org/pdf/2506.07575", "abs": "https://arxiv.org/abs/2506.07575", "authors": ["Ruiyang Zhang", "Hu Zhang", "Hao Fei", "Zhedong Zheng"], "title": "Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://uncertainty-o.github.io/", "summary": "Large Multimodal Models (LMMs), harnessing the complementarity among diverse\nmodalities, are often considered more robust than pure Language Large Models\n(LLMs); yet do LMMs know what they do not know? There are three key open\nquestions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a\nunified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to\nquantify uncertainty for downstream tasks. In an attempt to address these\nchallenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed\nto reveal uncertainty in LMMs regardless of their modalities, architectures, or\ncapabilities, (2) an empirical exploration of multimodal prompt perturbations\nto uncover LMM uncertainty, offering insights and findings, and (3) derive the\nformulation of multimodal semantic uncertainty, which enables quantifying\nuncertainty from multimodal responses. Experiments across 18 benchmarks\nspanning various modalities and 10 LMMs (both open- and closed-source)\ndemonstrate the effectiveness of Uncertainty-o in reliably estimating LMM\nuncertainty, thereby enhancing downstream tasks such as hallucination\ndetection, hallucination mitigation, and uncertainty-aware Chain-of-Thought\nreasoning.", "AI": {"tldr": "The paper introduces Uncertainty-o, a model-agnostic framework for evaluating uncertainty in Large Multimodal Models (LMMs). It addresses key challenges such as unified evaluation, prompting LMMs to show uncertainty, and quantifying uncertainty for downstream tasks. Experiments across 18 benchmarks and 10 LMMs demonstrate its effectiveness.", "motivation": "There is a lack of understanding and methods to evaluate the uncertainty of Large Multimodal Models (LMMs), especially compared to Language Large Models (LLMs). The authors aim to address three open questions: unified evaluation of uncertainty, prompting LMMs to reveal their uncertainty, and quantifying uncertainty for downstream tasks.", "method": "The method involves introducing Uncertainty-o, which consists of: a model-agnostic framework to reveal uncertainty in LMMs, empirical exploration of multimodal prompt perturbations to uncover LMM uncertainty, and derivation of multimodal semantic uncertainty formulation to quantify uncertainty from multimodal responses.", "result": "Experiments on 18 benchmarks and 10 LMMs show that Uncertainty-o effectively estimates LMM uncertainty, enhancing downstream tasks like hallucination detection, hallucination mitigation, and uncertainty-aware Chain-of-Thought reasoning.", "conclusion": "Uncertainty-o provides a reliable way to estimate uncertainty in LMMs, regardless of their modalities, architectures, or capabilities. This enhances various downstream applications."}}
{"id": "2506.07590", "pdf": "https://arxiv.org/pdf/2506.07590", "abs": "https://arxiv.org/abs/2506.07590", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Huajie Shao", "Ashley Gao"], "title": "Explore the vulnerability of black-box models via diffusion models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in diffusion models have enabled high-fidelity and\nphotorealistic image generation across diverse applications. However, these\nmodels also present security and privacy risks, including copyright violations,\nsensitive information leakage, and the creation of harmful or offensive content\nthat could be exploited maliciously. In this study, we uncover a novel security\nthreat where an attacker leverages diffusion model APIs to generate synthetic\nimages, which are then used to train a high-performing substitute model. This\nenables the attacker to execute model extraction and transfer-based adversarial\nattacks on black-box classification models with minimal queries, without\nneeding access to the original training data. The generated images are\nsufficiently high-resolution and diverse to train a substitute model whose\noutputs closely match those of the target model. Across the seven benchmarks,\nincluding CIFAR and ImageNet subsets, our method shows an average improvement\nof 27.37% over state-of-the-art methods while using just 0.01 times of the\nquery budget, achieving a 98.68% success rate in adversarial attacks on the\ntarget model.", "AI": {"tldr": "Recent advancements in diffusion models have enabled high-fidelity and photorealistic image generation across diverse applications, but these models also present security and privacy risks. In this study, we uncover a novel security threat where an attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model.", "motivation": "The motivation is the security and privacy risks of diffusion models, including copyright violations, sensitive information leakage, and the creation of harmful or offensive content that could be exploited maliciously.", "method": "An attacker leverages diffusion model APIs to generate synthetic images, which are then used to train a high-performing substitute model. This enables the attacker to execute model extraction and transfer-based adversarial attacks on black-box classification models with minimal queries, without needing access to the original training data.", "result": "The generated images are sufficiently high-resolution and diverse to train a substitute model whose outputs closely match those of the target model. Across the seven benchmarks, including CIFAR and ImageNet subsets, our method shows an average improvement of 27.37% over state-of-the-art methods while using just 0.01 times of the query budget, achieving a 98.68% success rate in adversarial attacks on the target model.", "conclusion": "Diffusion models can be used by attackers to generate synthetic images for training substitute models, enabling effective adversarial attacks on black-box models with minimal queries."}}
{"id": "2506.07583", "pdf": "https://arxiv.org/pdf/2506.07583", "abs": "https://arxiv.org/abs/2506.07583", "authors": ["Ramakrishna Appicharla", "Baban Gain", "Santanu Pal", "Asif Ekbal"], "title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the popularity of the large language models (LLMs), their application\nto machine translation is relatively underexplored, especially in context-aware\nsettings. This work presents a literature review of context-aware translation\nwith LLMs. The existing works utilise prompting and fine-tuning approaches,\nwith few focusing on automatic post-editing and creating translation agents for\ncontext-aware machine translation. We observed that the commercial LLMs (such\nas ChatGPT and Tower LLM) achieved better results than the open-source LLMs\n(such as Llama and Bloom LLMs), and prompt-based approaches serve as good\nbaselines to assess the quality of translations. Finally, we present some\ninteresting future directions to explore.", "AI": {"tldr": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5f88\u53d7\u6b22\u8fce\uff0c\u4f46\u5728\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u5e94\u7528\u5c1a\u5f85\u6df1\u5165\u7814\u7a76\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u4f7f\u7528LLM\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u7ffb\u8bd1\u7684\u76f8\u5173\u6587\u732e\uff0c\u63a2\u8ba8\u4e86\u63d0\u793a\u548c\u7cbe\u8c03\u7b49\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u5546\u4e1aLLM\u8868\u73b0\u4f18\u4e8e\u5f00\u6e90LLM\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u4e9b\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u56e0\u6b64\u9700\u8981\u5bf9\u73b0\u6709\u5de5\u4f5c\u8fdb\u884c\u603b\u7ed3\uff0c\u4ee5\u660e\u786e\u5f53\u524d\u8fdb\u5c55\u53ca\u672a\u6765\u53ef\u80fd\u7684\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6587\u732e\u56de\u987e\uff0c\u5206\u6790\u73b0\u6709\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5305\u62ec\u5229\u7528\u63d0\u793a\u548c\u7cbe\u8c03\u6280\u672f\uff0c\u5e76\u6bd4\u8f83\u5546\u4e1aLLM\u4e0e\u5f00\u6e90LLM\u7684\u8868\u73b0\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u5546\u4e1aLLM\u6bd4\u5f00\u6e90LLM\u8868\u73b0\u66f4\u597d\uff0c\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u662f\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf\u7684\u826f\u597d\u57fa\u51c6\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7ffb\u8bd1\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6307\u51fa\u4e86\u81ea\u52a8\u540e\u7f16\u8f91\u548c\u521b\u5efa\u7ffb\u8bd1\u4ee3\u7406\u7b49\u8f83\u5c11\u5173\u6ce8\u7684\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u503c\u5f97\u63a2\u7d22\u7684\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2506.07614", "pdf": "https://arxiv.org/pdf/2506.07614", "abs": "https://arxiv.org/abs/2506.07614", "authors": ["Rishikesh Srinivasan", "Dheeraj Nagaraj"], "title": "Poisson Midpoint Method for Log Concave Sampling: Beyond the Strong Error Lower Bounds", "categories": ["math.PR", "cs.LG", "math.ST", "stat.TH"], "comment": null, "summary": "We study the problem of sampling from strongly log-concave distributions over\n$\\mathbb{R}^d$ using the Poisson midpoint discretization (a variant of the\nrandomized midpoint method) for overdamped/underdamped Langevin dynamics. We\nprove its convergence in the 2-Wasserstein distance ($W_2$), achieving a cubic\nspeedup in dependence on the target accuracy ($\\epsilon$) over the\nEuler-Maruyama discretization, surpassing existing bounds for randomized\nmidpoint methods. Notably, in the case of underdamped Langevin dynamics, we\ndemonstrate the complexity of $W_2$ convergence is much smaller than the\ncomplexity lower bounds for convergence in $L^2$ strong error established in\nthe literature.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07600", "pdf": "https://arxiv.org/pdf/2506.07600", "abs": "https://arxiv.org/abs/2506.07600", "authors": ["Nianbo Zeng", "Haowen Hou", "Fei Richard Yu", "Si Shi", "Ying Tiffany He"], "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in retrieval-augmented generation (RAG) for video\nunderstanding, effectively understanding long-form video content remains\nunderexplored due to the vast scale and high complexity of video data. Current\nRAG approaches typically segment videos into fixed-length chunks, which often\ndisrupts the continuity of contextual information and fails to capture\nauthentic scene boundaries. Inspired by the human ability to naturally organize\ncontinuous experiences into coherent scenes, we present SceneRAG, a unified\nframework that leverages large language models to segment videos into\nnarrative-consistent scenes by processing ASR transcripts alongside temporal\nmetadata. SceneRAG further sharpens these initial boundaries through\nlightweight heuristics and iterative correction. For each scene, the framework\nfuses information from both visual and textual modalities to extract entity\nrelations and dynamically builds a knowledge graph, enabling robust multi-hop\nretrieval and generation that account for long-range dependencies. Experiments\non the LongerVideos benchmark, featuring over 134 hours of diverse content,\nconfirm that SceneRAG substantially outperforms prior baselines, achieving a\nwin rate of up to 72.5 percent on generation tasks.", "AI": {"tldr": "SceneRAG is a unified framework leveraging large language models to segment videos into narrative-consistent scenes, process ASR transcripts alongside temporal metadata, extract entity relations and build knowledge graphs for robust multi-hop retrieval and generation. Experiments confirm SceneRAG outperforms prior baselines with a win rate of up to 72.5 percent on generation tasks.", "motivation": "Effective understanding of long-form video content remains underexplored due to the vast scale and high complexity of video data. Current RAG approaches disrupt continuity and fail to capture authentic scene boundaries.", "method": "SceneRAG segments videos into narrative-consistent scenes by processing ASR transcripts alongside temporal metadata, sharpens these initial boundaries through lightweight heuristics and iterative correction, fuses information from both visual and textual modalities to extract entity relations and builds knowledge graphs.", "result": "Experiments on the LongerVideos benchmark confirm that SceneRAG substantially outperforms prior baselines, achieving a win rate of up to 72.5 percent on generation tasks.", "conclusion": "SceneRAG provides an effective solution for understanding long-form video content by leveraging large language models, processing ASR transcripts alongside temporal metadata, extracting entity relations and building knowledge graphs."}}
{"id": "2506.07621", "pdf": "https://arxiv.org/pdf/2506.07621", "abs": "https://arxiv.org/abs/2506.07621", "authors": ["Harsh Bihany", "Shubham Patel", "Ashutosh Modi"], "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025; 21 pages (9 main paper + 5 pages\n  references + 7 pages appendix)", "summary": "Large Language Models have shown remarkable capabilities in the NLP domain.\nTheir effectiveness can mainly be attributed to their ability to adapt to an\narray of downstream tasks. However, generally, full fine-tuning is a\ncomputationally expensive job. To mitigate this, many techniques have been\ndeveloped that prime efficiency, a prominent one being Low-Rank Adaptation\n(LoRA). However, LoRA and its variants employ re-parametrized additive updates.\nIn this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which\nshifts the paradigm of additive updates to a richer space of matrix\nmultiplicative transformations. We tackle challenges such as computational\ncomplexity and rank bottleneck of matrix multiplication by effectively\nre-ordering operations and introducing rank inflation strategies. We conduct\nextensive experiments to demonstrate the effectiveness of our approach in terms\nof various evaluation metrics.", "AI": {"tldr": "Large Language Models are effective but full fine-tuning is expensive. LoRA improves efficiency but uses additive updates. This paper proposes LoRMA, which uses matrix multiplicative transformations instead, and addresses related challenges.", "motivation": "To improve the efficiency of adapting Large Language Models to downstream tasks while overcoming limitations of existing methods like LoRA.", "method": "Propose Low-Rank Multiplicative Adaptation (LoRMA) that uses matrix multiplicative transformations instead of additive updates. Address computational complexity and rank bottleneck issues with operation re-ordering and rank inflation strategies.", "result": "Through extensive experiments, demonstrate the effectiveness of LoRMA using various evaluation metrics.", "conclusion": "LoRMA provides a novel approach to adapting Large Language Models efficiently."}}
{"id": "2506.07603", "pdf": "https://arxiv.org/pdf/2506.07603", "abs": "https://arxiv.org/abs/2506.07603", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "AI": {"tldr": "SurgBench, a unified surgical video benchmarking framework with SurgBench-P and SurgBench-E, addresses the scarcity of large-scale datasets for pretraining and evaluation in surgical video understanding. Pretraining on SurgBench-P significantly enhances performance and cross-domain generalization in varied surgical video analysis tasks.", "motivation": "Progress in developing surgical video foundation models is limited by the lack of large-scale, diverse datasets for pretraining and systematic evaluation.", "method": "Introduced SurgBench, which includes SurgBench-P (a pretraining dataset with 53 million frames across 22 surgical procedures and 11 specialties) and SurgBench-E (an evaluation benchmark covering six categories and 72 fine-grained tasks).", "result": "Existing video FMs have difficulty generalizing across various surgical video analysis tasks, but pretraining on SurgBench-P leads to significant performance improvements and better cross-domain generalization.", "conclusion": "SurgBench provides extensive coverage of diverse surgical scenarios and enhances the development of surgical video foundation models."}}
{"id": "2506.07637", "pdf": "https://arxiv.org/pdf/2506.07637", "abs": "https://arxiv.org/abs/2506.07637", "authors": ["Yuchong Long", "Wen Sun", "Ningxiao Sun", "Wenxiao Wang", "Chao Li", "Shan Yin"], "title": "HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition", "categories": ["cs.CV", "cs.LG", "68T07, 68T45", "I.2.10; I.4.9; I.5.4"], "comment": "16 pages, 5 figures, 2 tables. The dataset at\n  https://www.kaggle.com/datasets/ayinven/hieraedgenetintegratesdatasets. The\n  models at\n  https://huggingface.co/datasets/AyinMostima/HieraEdgeNetintegratesdatasets.\n  The source code in at https://github.com/AyinMostima/PalynoKit", "summary": "Automated pollen recognition is vital to paleoclimatology, biodiversity\nmonitoring, and public health, yet conventional methods are hampered by\ninefficiency and subjectivity. Existing deep learning models often struggle to\nachieve the requisite localization accuracy for microscopic targets like\npollen, which are characterized by their minute size, indistinct edges, and\ncomplex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a\nmulti-scale edge-enhancement framework. The framework's core innovation is the\nintroduction of three synergistic modules: the Hierarchical Edge Module (HEM),\nwhich explicitly extracts a multi-scale pyramid of edge features that\ncorresponds to the semantic hierarchy at early network stages; the Synergistic\nEdge Fusion (SEF) module, for deeply fusing these edge priors with semantic\ninformation at each respective scale; and the Cross Stage Partial Omni-Kernel\nModule (CSPOKM), which maximally refines the most detail-rich feature layers\nusing an Omni-Kernel operator - comprising anisotropic large-kernel\nconvolutions and mixed-domain attention - all within a computationally\nefficient Cross-Stage Partial (CSP) framework. On a large-scale dataset\ncomprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision\n(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline\nmodels such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms\nthat our approach generates feature representations that are more precisely\nfocused on object boundaries. By systematically integrating edge information,\nHieraEdgeNet provides a robust and powerful solution for high-precision,\nhigh-efficiency automated detection of microscopic objects.", "AI": {"tldr": "HieraEdgeNet, a multi-scale edge-enhancement framework with three key modules (HEM, SEF, CSPOKM), achieves high precision in automated pollen recognition, outperforming state-of-the-art models.", "motivation": "Automated pollen recognition faces challenges due to inefficiency and subjectivity of conventional methods, as well as the difficulty deep learning models have in localizing microscopic targets like pollen accurately.", "method": "The HieraEdgeNet framework includes the Hierarchical Edge Module (HEM) for multi-scale edge feature extraction, the Synergistic Edge Fusion (SEF) module for fusing edge priors with semantic information at each scale, and the Cross Stage Partial Omni-Kernel Module (CSPOKM) for refining feature layers using an Omni-Kernel operator within a CSP framework.", "result": "On a large-scale dataset with 120 pollen classes, HieraEdgeNet achieved a mean Average Precision (mAP@.5) of 0.9501, surpassing other state-of-the-art models such as YOLOv12n and RT-DETR. Qualitative analysis also confirmed more precise feature representations focused on object boundaries.", "conclusion": "HieraEdgeNet provides a robust solution for high-precision, high-efficiency automated detection of microscopic objects by systematically integrating edge information."}}
{"id": "2506.07606", "pdf": "https://arxiv.org/pdf/2506.07606", "abs": "https://arxiv.org/abs/2506.07606", "authors": ["Peyman Rostami", "Vahid Rahimzadeh", "Ali Adibi", "Azadeh Shakery"], "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SI", "I.2.7"], "comment": "The dataset is available at https://doi.org/10.5281/zenodo.15616911", "summary": "Stance detection identifies the viewpoint expressed in text toward a specific\ntarget, such as a political figure. While previous datasets have focused\nprimarily on tweet-level stances from established platforms, user-level stance\nresources, especially on emerging platforms like Bluesky remain scarce.\nUser-level stance detection provides a more holistic view by considering a\nuser's complete posting history rather than isolated posts. We present the\nfirst stance detection dataset for the 2024 U.S. presidential election,\ncollected from Bluesky and centered on Kamala Harris and Donald Trump. The\ndataset comprises 16,044 user-target stance pairs enriched with engagement\nmetadata, interaction graphs, and user posting histories. PolitiSky24 was\ncreated using a carefully evaluated pipeline combining advanced information\nretrieval and large language models, which generates stance labels with\nsupporting rationales and text spans for transparency. The labeling approach\nachieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in\npolitical stance analysis through its timeliness, open-data nature, and\nuser-level perspective. The dataset is available at\nhttps://doi.org/10.5281/zenodo.15616911", "AI": {"tldr": "The paper presents PolitiSky24, the first stance detection dataset for the 2024 U.S. presidential election from Bluesky, focusing on Kamala Harris and Donald Trump with 16,044 user-target stance pairs. It combines advanced information retrieval and LLMs to generate stance labels with supporting rationales, achieving 81% accuracy.", "motivation": "Existing datasets primarily focus on tweet-level stances from established platforms, lacking user-level stance resources, especially on emerging platforms like Bluesky.", "method": "The dataset was created using a pipeline combining advanced information retrieval and large language models (LLMs) that generates stance labels along with supporting rationales and text spans for transparency.", "result": "The labeling approach achieves 81% accuracy with scalable LLMs, addressing gaps in political stance analysis through its timeliness, open-data nature, and user-level perspective.", "conclusion": "PolitiSky24 provides a valuable resource for stance detection research, offering a more holistic view by considering users' complete posting histories rather than isolated posts."}}
{"id": "2506.07667", "pdf": "https://arxiv.org/pdf/2506.07667", "abs": "https://arxiv.org/abs/2506.07667", "authors": ["Prarabdh Shukla", "Wei Yin Chong", "Yash Patel", "Brennan Schaffner", "Danish Pruthi", "Arjun Bhagoji"], "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "To meet the demands of content moderation, online platforms have resorted to\nautomated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users\ncommenting on live streams) on platforms like Twitch exert additional pressures\non the latency expected of such moderation systems. Despite their prevalence,\nrelatively little is known about the effectiveness of these systems. In this\npaper, we conduct an audit of Twitch's automated moderation tool\n($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful\ncontent. For our audit, we create streaming accounts to act as siloed test\nbeds, and interface with the live chat using Twitch's APIs to send over\n$107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s\naccuracy in flagging blatantly hateful content containing misogyny, racism,\nableism and homophobia. Our experiments reveal that a large fraction of hateful\nmessages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$.\nContextual addition of slurs to these messages results in $100\\%$ removal,\nrevealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We\nalso find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$\nblocks up to $89.5\\%$ of benign examples that use sensitive words in\npedagogical or empowering contexts. Overall, our audit points to large gaps in\n$\\texttt{AutoMod}$'s capabilities and underscores the importance for such\nsystems to understand context effectively.", "AI": {"tldr": "\u4e3a\u4e86\u5e94\u5bf9\u5185\u5bb9\u5ba1\u6838\u7684\u9700\u6c42\uff0c\u7814\u7a76\u4eba\u5458\u5bf9Twitch\u7684\u81ea\u52a8\u5ba1\u6838\u5de5\u5177(AutoMod)\u8fdb\u884c\u4e86\u5ba1\u8ba1\u3002\u901a\u8fc7\u521b\u5efa\u6d4b\u8bd5\u8d26\u6237\u5e76\u4f7f\u7528Twitch\u7684API\u53d1\u9001\u8d85\u8fc7107,000\u6761\u8bc4\u8bba\uff0c\u7814\u7a76\u53d1\u73b0AutoMod\u672a\u80fd\u6807\u8bb0\u5927\u91cf\u4ec7\u6068\u8a00\u8bba\uff08\u9ad8\u8fbe94%\uff09\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u4fda\u8bed\u4f5c\u4e3a\u5ba1\u6838\u4fe1\u53f7\u3002\u540c\u65f6\uff0c\u5b83\u9519\u8bef\u5730\u5c01\u9501\u4e86\u9ad8\u8fbe89.5%\u7684\u826f\u6027\u793a\u4f8b\uff0c\u663e\u793a\u51fa\u5176\u5728\u7406\u89e3\u8bed\u5883\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5b9e\u65f6\u4e92\u52a8\u5e73\u53f0\u5982Twitch\u4e0a\u7684\u8bc4\u8bba\u7b49\u65b0\u5f62\u5f0f\u589e\u52a0\u4e86\u5bf9\u4f4e\u5ef6\u8fdf\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u7684\u9700\u6c42\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u7814\u7a76\u4eba\u5458\u521b\u5efa\u4e86\u9694\u79bb\u7684\u6d4b\u8bd5\u8d26\u6237\uff0c\u5e76\u901a\u8fc7Twitch\u7684API\u5411\u76f4\u64ad\u804a\u5929\u53d1\u9001\u8d85\u8fc7107,000\u6761\u8bc4\u8bba\u8fdb\u884c\u5ba1\u8ba1\uff0c\u8fd9\u4e9b\u8bc4\u8bba\u6765\u81ea\u56db\u4e2a\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5927\u91cf\u7684\u4ec7\u6068\u4fe1\u606f\uff08\u6700\u9ad8\u8fbe94%\uff09\u672a\u88abAutoMod\u6807\u8bb0\uff1b\u6dfb\u52a0\u4fda\u8bed\u540e\uff0c\u5220\u9664\u7387\u53ef\u8fbe100%\uff0c\u663e\u793a\u5176\u8fc7\u5ea6\u4f9d\u8d56\u4fda\u8bed\u4f5c\u4e3a\u5ba1\u6838\u4f9d\u636e\u3002\u6b64\u5916\uff0cAutoMod\u8fd8\u9519\u8bef\u5730\u6807\u8bb0\u4e86\u5927\u91cf\u65e0\u5bb3\u5185\u5bb9\uff08\u9ad8\u8fbe89.5%\uff09\u3002", "conclusion": "AutoMod\u5b58\u5728\u8f83\u5927\u7684\u529f\u80fd\u7f3a\u53e3\uff0c\u5f3a\u8c03\u4e86\u81ea\u52a8\u5ba1\u6838\u7cfb\u7edf\u9700\u8981\u66f4\u6709\u6548\u5730\u7406\u89e3\u4e0a\u4e0b\u6587\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2506.07687", "pdf": "https://arxiv.org/pdf/2506.07687", "abs": "https://arxiv.org/abs/2506.07687", "authors": ["Kevin Lam", "Thang Bui", "George Deligiannidis", "Yee Whye Teh"], "title": "Rao-Blackwellised Reparameterisation Gradients", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Latent Gaussian variables have been popularised in probabilistic machine\nlearning. In turn, gradient estimators are the machinery that facilitates\ngradient-based optimisation for models with latent Gaussian variables. The\nreparameterisation trick is often used as the default estimator as it is simple\nto implement and yields low-variance gradients for variational inference. In\nthis work, we propose the R2-G2 estimator as the Rao-Blackwellisation of the\nreparameterisation gradient estimator. Interestingly, we show that the local\nreparameterisation gradient estimator for Bayesian MLPs is an instance of the\nR2-G2 estimator and Rao-Blackwellisation. This lets us extend benefits of\nRao-Blackwellised gradients to a suite of probabilistic models. We show that\ninitial training with R2-G2 consistently yields better performance in models\nwith multiple applications of the reparameterisation trick.", "AI": {"tldr": "The paper proposes R2-G2 estimator for latent Gaussian variables and shows its benefits in probabilistic models.", "motivation": "Gradient estimators are essential for optimizing models with latent Gaussian variables, where reparameterisation trick is widely used. There's a need to improve the efficiency of gradient estimation.", "method": "Propose R2-G2 as an improved Rao-Blackwellised reparameterisation gradient estimator. Show that local reparameterisation gradient estimator for Bayesian MLPs is an instance of R2-G2.", "result": "R2-G2 yields better performance during initial training in models using multiple reparameterisation tricks.", "conclusion": "R2-G2 can extend the benefits of Rao-Blackwellised gradients to various probabilistic models."}}
{"id": "2506.07652", "pdf": "https://arxiv.org/pdf/2506.07652", "abs": "https://arxiv.org/abs/2506.07652", "authors": ["Hangbei Cheng", "Xiaorong Dong", "Xueyu Liu", "Jianan Zhang", "Xuetao Ma", "Mingqiang Wei", "Liansheng Wang", "Junxin Chen", "Yongfei Wu"], "title": "FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate lesion segmentation in histopathology images is essential for\ndiagnostic interpretation and quantitative analysis, yet it remains challenging\ndue to the limited availability of costly pixel-level annotations. To address\nthis, we propose FMaMIL, a novel two-stage framework for weakly supervised\nlesion segmentation based solely on image-level labels. In the first stage, a\nlightweight Mamba-based encoder is introduced to capture long-range\ndependencies across image patches under the MIL paradigm. To enhance spatial\nsensitivity and structural awareness, we design a learnable frequency-domain\nencoding module that supplements spatial-domain features with spectrum-based\ninformation. CAMs generated in this stage are used to guide segmentation\ntraining. In the second stage, we refine the initial pseudo labels via a\nCAM-guided soft-label supervision and a self-correction mechanism, enabling\nrobust training even under label noise. Extensive experiments on both public\nand private histopathology datasets demonstrate that FMaMIL outperforms\nstate-of-the-art weakly supervised methods without relying on pixel-level\nannotations, validating its effectiveness and potential for digital pathology\napplications.", "AI": {"tldr": "This paper proposes FMaMIL, a two-stage framework for weakly supervised lesion segmentation in histopathology images using only image-level labels. It combines a lightweight Mamba-based encoder with a frequency-domain encoding module and uses CAM-guided soft-label supervision and self-correction for refining pseudo labels. FMaMIL outperforms state-of-the-art methods without needing pixel-level annotations.", "motivation": "Accurate lesion segmentation is crucial in histopathology but challenging due to limited availability of costly pixel-level annotations. The authors aim to develop a method that relies solely on image-level labels for segmentation.", "method": "The FMaMIL framework consists of two stages: (1) A Mamba-based encoder captures long-range dependencies across image patches under the MIL paradigm, supplemented by a learnable frequency-domain encoding module to enhance spatial sensitivity and structural awareness. Class Activation Maps (CAMs) generated guide segmentation training. (2) Pseudo labels are refined via CAM-guided soft-label supervision and a self-correction mechanism to ensure robustness against label noise.", "result": "Extensive experiments on both public and private histopathology datasets show that FMaMIL outperforms state-of-the-art weakly supervised methods, achieving better performance without relying on pixel-level annotations.", "conclusion": "FMaMIL demonstrates superior performance compared to existing weakly supervised methods in lesion segmentation for histopathology images using only image-level labels, proving its effectiveness and potential for digital pathology applications."}}
{"id": "2506.07691", "pdf": "https://arxiv.org/pdf/2506.07691", "abs": "https://arxiv.org/abs/2506.07691", "authors": ["Jiaming Li", "Haoran Ye", "Yukun Chen", "Xinyue Li", "Lei Zhang", "Hamid Alinejad-Rokny", "Jimmy Chih-Hsien Peng", "Min Yang"], "title": "Training Superior Sparse Autoencoders for Instruct Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) grow in scale and capability, understanding\ntheir internal mechanisms becomes increasingly critical. Sparse autoencoders\n(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the\nextraction of human-interpretable features from LLMs. However, existing SAE\ntraining methods are primarily designed for base models, resulting in reduced\nreconstruction quality and interpretability when applied to instruct models. To\nbridge this gap, we propose\n$\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned\n$\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining\n($\\textit{FAST}$), a novel training method specifically tailored for instruct\nmodels. $\\textit{FAST}$ aligns the training process with the data distribution\nand activation patterns characteristic of instruct models, resulting in\nsubstantial improvements in both reconstruction and feature interpretability.\nOn Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468\nin token reconstruction, significantly outperforming baseline methods with\nerrors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$\nyields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,\n$21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for\n$\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that\nintervening on the activations of special tokens via the SAEs leads to\nimprovements in output quality, suggesting new opportunities for fine-grained\ncontrol of model behavior. Code, data, and 240 trained SAEs are available at\nhttps://github.com/Geaming2002/FAST.", "AI": {"tldr": "The paper introduces FAST, a novel training method for SAEs tailored to instruct models, enhancing reconstruction quality and feature interpretability in LLMs. It outperforms baselines on Qwen2.5-7B-Instruct and Llama3.2-3B-Instruct.", "motivation": "Existing SAE training methods are primarily designed for base models, leading to reduced reconstruction quality and interpretability when applied to instruct models.", "method": "FAST (Finetuning-Aligned Sequential Training) is proposed, aligning the training process with the data distribution and activation patterns characteristic of instruct models.", "result": "On Qwen2.5-7B-Instruct, FAST achieves a mean squared error of 0.6468 in token reconstruction, significantly lower than baseline methods. For feature interpretability, FAST yields a higher proportion of high-quality features, with 21.1% scoring in the top range for Llama3.2-3B-Instruct compared to 7.0% and 10.2% for other methods.", "conclusion": "FAST improves both reconstruction and feature interpretability in instruct models. Intervening on activations of special tokens via SAEs can improve output quality, suggesting new opportunities for fine-grained control."}}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664", "abs": "https://arxiv.org/abs/2506.07664", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities.", "AI": {"tldr": "\u901a\u8fc7\u4ece\u6570\u5b66\u63a8\u7406\u4e2d\u751f\u6210\u95ee\u9898\u89e3\u51b3\u4ee3\u7801\u6765\u63d0\u53d6\u7ed3\u6784\u4fe1\u606f\uff0c\u5e76\u7528\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\u6307\u5bfc\u6570\u636e\u751f\u6210\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u7684\u63a8\u7406\u80fd\u529b\u3002\u5728MATH\u548cGSM8K\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u968f\u7740\u63a8\u7406\u957f\u5ea6\u7684\u589e\u52a0\uff0c\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002\u4f7f\u7528\u6240\u63d0\u51fa\u7684\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\u7684\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6211\u4eec\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6570\u5b66\u63a8\u7406\u5bf9\u4e8eLLM\u6765\u8bf4\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u9700\u8981\u590d\u6742\u7684\u903b\u8f91\u548c\u7cbe\u786e\u7684\u8ba1\u7b97\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u8868\u8ff0\u95ee\u9898\u6765\u5408\u6210\u6570\u636e\u96c6\u4ee5\u589e\u5f3aLLM\u63a8\u7406\uff0c\u4f46\u8fd9\u4e9b\u95ee\u9898\u751f\u6210\u7684\u8d28\u91cf\u548c\u590d\u6742\u5ea6\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u6570\u5b66\u63a8\u7406\u4e2d\u63d0\u53d6\u7ed3\u6784\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u751f\u6210\u7684\u95ee\u9898\u89e3\u51b3\u4ee3\u7801\uff0c\u5e76\u7528\u7ed3\u6784\u5316\u7684\u89e3\u51b3\u65b9\u6848\u6307\u5bfc\u6570\u636e\u751f\u6210\u3002", "result": "\u5728\u81ea\u5efa\u57fa\u51c6\u4e0a\uff0c\u7ed3\u679c\u8868\u660e\u6a21\u578b\u6027\u80fd\u968f\u7740\u63a8\u7406\u957f\u5ea6\u7684\u589e\u52a0\u800c\u4e0b\u964d\u3002\u5bf9\u4e00\u7cfb\u5217LLM\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\u7684\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5e0c\u671b\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6570\u636e\u96c6\u80fd\u591f\u4e3a\u672a\u6765\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2506.07695", "pdf": "https://arxiv.org/pdf/2506.07695", "abs": "https://arxiv.org/abs/2506.07695", "authors": ["Parsa Miraghaei", "Sergio Moreschini", "Antti Kolehmainen", "David H\u00e4stbacka"], "title": "Towards a Small Language Model Lifecycle Framework", "categories": ["cs.SE", "cs.LG"], "comment": null, "summary": "Background: The growing demand for efficient and deployable language models\nhas led to increased interest in Small Language Models (SLMs). However,\nexisting research remains fragmented, lacking a unified lifecycle perspective.\n  Objective: This study aims to define a comprehensive lifecycle framework for\nSLMs by synthesizing insights from academic literature and practitioner\nsources.\n  Method: We conducted a comprehensive survey of 36 works, analyzing and\ncategorizing lifecycle-relevant techniques.\n  Results: We propose a modular lifecycle model structured into main, optional,\nand cross-cutting components. The model captures key interconnections across\nstages, supporting method reuse, co-adaptation, and lifecycle-awareness.\n  Conclusion: Our framework provides a coherent foundation for developing and\nmaintaining SLMs, bridging theory and practice, and guiding future research and\ntool development.", "AI": {"tldr": "This paper defines a comprehensive lifecycle framework for Small Language Models (SLMs) through synthesizing insights from academic literature and practitioner sources.", "motivation": "The growing demand for efficient and deployable language models has led to increased interest in Small Language Models (SLMs). However, existing research remains fragmented, lacking a unified lifecycle perspective.", "method": "Conducted a comprehensive survey of 36 works, analyzing and categorizing lifecycle-relevant techniques.", "result": "Proposed a modular lifecycle model structured into main, optional, and cross-cutting components. The model captures key interconnections across stages, supporting method reuse, co-adaptation, and lifecycle-awareness.", "conclusion": "The framework provides a coherent foundation for developing and maintaining SLMs, bridging theory and practice, and guiding future research and tool development."}}
{"id": "2506.07671", "pdf": "https://arxiv.org/pdf/2506.07671", "abs": "https://arxiv.org/abs/2506.07671", "authors": ["Ionut-Teodor Sorodoc", "Leonardo F. R. Ribeiro", "Rexhina Blloshmi", "Christopher Davis", "Adri\u00e0 de Gispert"], "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers\nand annotations of each grounding passage, allowing a fine-grained evaluation\nof whether LLMs can identify relevant grounding when generating RAG answers.\nOur benchmark contains 2366 questions of diverse complexity, dynamism, and\ntopics, and includes over 35K annotated passages retrieved from both private\ndocument sets and the Web, to reflect real-world RAG use cases. This makes it\nan ideal test bed to evaluate an LLM's ability to identify only the relevant\ninformation necessary to compose a response, or provide a deflective response\nwhen there is insufficient information. Evaluations of multiple\nstate-of-the-art LLMs on GaRAGe show that the models tend to over-summarise\nrather than (a) ground their answers strictly on the annotated relevant\npassages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)\ndeflect when no relevant grounding is available (reaching at most 31% true\npositive rate in deflections). The F1 in attribution to relevant sources is at\nmost 58.9%, and we show that performance is particularly reduced when answering\ntime-sensitive questions and when having to draw knowledge from sparser private\ngrounding sources.", "AI": {"tldr": "The paper introduces GaRAGe, a large RAG benchmark for evaluating LLMs' ability to identify relevant grounding when generating answers. It contains 2366 questions and over 35K annotated passages. Evaluations show that state-of-the-art LLMs tend to over-summarise rather than ground their answers strictly on the relevant passages or deflect when no relevant grounding is available.", "motivation": "To create a benchmark that allows fine-grained evaluation of whether LLMs can identify relevant grounding when generating RAG answers, reflecting real-world use cases.", "method": "Developed GaRAGe, a benchmark with human-curated long-form answers and annotations of each grounding passage. It includes diverse questions and annotated passages from private document sets and the Web.", "result": "State-of-the-art LLMs evaluated on GaRAGe tend to over-summarise, reaching at most 60% Relevance-Aware Factuality Score and 31% true positive rate in deflections. Attribution F1 score is at most 58.9%. Performance drops when answering time-sensitive questions or using sparser private grounding sources.", "conclusion": "GaRAGe provides an ideal test bed to evaluate LLMs' abilities to identify relevant information or provide deflective responses. Current models struggle with strict grounding and appropriate deflection."}}
{"id": "2506.07698", "pdf": "https://arxiv.org/pdf/2506.07698", "abs": "https://arxiv.org/abs/2506.07698", "authors": ["Yuxiao Yang", "Peihao Li", "Yuhong Zhang", "Junzhe Lu", "Xianglong He", "Minghan Qin", "Weitao Wang", "Haoqian Wang"], "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 7 figures, accepted by ICME 2025", "summary": "3D AI-generated content (AIGC) has made it increasingly accessible for anyone\nto become a 3D content creator. While recent methods leverage Score\nDistillation Sampling to distill 3D objects from pretrained image diffusion\nmodels, they often suffer from inadequate 3D priors, leading to insufficient\nmulti-view consistency. In this work, we introduce NOVA3D, an innovative\nsingle-image-to-3D generation framework. Our key insight lies in leveraging\nstrong 3D priors from a pretrained video diffusion model and integrating\ngeometric information during multi-view video fine-tuning. To facilitate\ninformation exchange between color and geometric domains, we propose the\nGeometry-Temporal Alignment (GTA) attention mechanism, thereby improving\ngeneralization and multi-view consistency. Moreover, we introduce the\nde-conflict geometry fusion algorithm, which improves texture fidelity by\naddressing multi-view inaccuracies and resolving discrepancies in pose\nalignment. Extensive experiments validate the superiority of NOVA3D over\nexisting baselines.", "AI": {"tldr": "NOVA3D\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u5355\u56fe\u50cf\u52303D\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5f3a3D\u5148\u9a8c\uff0c\u5e76\u5728\u591a\u89c6\u56fe\u89c6\u9891\u5fae\u8c03\u671f\u95f4\u6574\u5408\u51e0\u4f55\u4fe1\u606f\uff0c\u63d0\u51faGTA\u6ce8\u610f\u529b\u673a\u5236\u548c\u53bb\u51b2\u7a81\u51e0\u4f55\u878d\u5408\u7b97\u6cd5\u4ee5\u63d0\u9ad8\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u6700\u8fd1\u7684\u65b9\u6cd5\u5229\u7528\u5206\u6570\u84b8\u998f\u91c7\u6837\u4ece\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d63D\u5bf9\u8c61\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u7531\u4e8e3D\u5148\u9a8c\u4e0d\u8db3\u800c\u5bfc\u81f4\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4e0d\u8db3\u3002", "method": "NOVA3D\u5f15\u5165\u4e86\u6765\u81ea\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5f3a3D\u5148\u9a8c\uff0c\u5e76\u5728\u591a\u89c6\u56fe\u89c6\u9891\u5fae\u8c03\u671f\u95f4\u6574\u5408\u4e86\u51e0\u4f55\u4fe1\u606f\u3002\u4e3a\u4e86\u4fc3\u8fdb\u989c\u8272\u548c\u51e0\u4f55\u57df\u4e4b\u95f4\u7684\u4fe1\u606f\u4ea4\u6362\uff0c\u63d0\u51fa\u4e86\u51e0\u4f55-\u65f6\u95f4\u5bf9\u9f50\uff08GTA\uff09\u6ce8\u610f\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u53bb\u51b2\u7a81\u51e0\u4f55\u878d\u5408\u7b97\u6cd5\uff0c\u901a\u8fc7\u89e3\u51b3\u591a\u89c6\u56fe\u4e0d\u51c6\u786e\u6027\u5e76\u89e3\u51b3\u59ff\u6001\u5bf9\u9f50\u4e2d\u7684\u5dee\u5f02\u6765\u63d0\u9ad8\u7eb9\u7406\u4fdd\u771f\u5ea6\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86NOVA3D\u5728\u73b0\u6709\u57fa\u7ebf\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "NOVA3D\u63d0\u9ad8\u4e86\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u7eb9\u7406\u4fdd\u771f\u5ea6\uff0c\u4e3a3D\u5185\u5bb9\u521b\u5efa\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.07713", "pdf": "https://arxiv.org/pdf/2506.07713", "abs": "https://arxiv.org/abs/2506.07713", "authors": ["Ge Wang", "Songlin Fan", "Hangxu Liu", "Quanjian Song", "Hewei Wang", "Jinfeng Xu"], "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures", "summary": "With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.", "AI": {"tldr": "FlowV2V\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\uff0c\u901a\u8fc7\u5149\u6d41\u5efa\u6a21\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\uff0c\u63d0\u4f9b\u66f4\u597d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u9891\u7f16\u8f91\u65b9\u6cd5\u65e0\u6cd5\u5f88\u597d\u5730\u5904\u7406\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u975e\u521a\u6027\u7269\u4f53\u8fd0\u52a8\uff0c\u5982\u591a\u7269\u4f53\u548c\u8096\u50cf\u7f16\u8f91\u3002\u8fd9\u4fc3\u4f7f\u4e86\u5bf9\u66f4\u6709\u6548\u65b9\u6cd5\u7684\u9700\u6c42\u3002", "method": "FlowV2V\u5c06\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u7531\u5149\u6d41\u9a71\u52a8\u7684\u56fe\u50cf\u5230\u89c6\u9891\uff08I2V\uff09\u751f\u6210\u4efb\u52a1\u3002\u5b83\u5c06\u6574\u4e2a\u6d41\u7a0b\u5206\u89e3\u4e3a\u7b2c\u4e00\u5e27\u7f16\u8f91\u548c\u6761\u4ef6I2V\u751f\u6210\uff0c\u5e76\u6a21\u62df\u4e0e\u53d8\u5f62\u5f62\u72b6\u5bf9\u9f50\u7684\u4f2a\u6d41\u5e8f\u5217\u4ee5\u786e\u4fdd\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlowV2V\u5728DAVIS-EDIT\u6570\u636e\u96c6\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8613.67%\u548c50.66%\u7684DOVER\u548c\u626d\u66f2\u8bef\u5dee\u6307\u6807\uff0c\u663e\u793a\u4e86\u5176\u4f18\u8d8a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u6837\u672c\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u6765\u5206\u6790\u6240\u63d0\u65b9\u6cd5\u5185\u90e8\u529f\u80fd\u3002", "conclusion": "FlowV2V\u901a\u8fc7\u5229\u7528\u5149\u6d41\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u8fd0\u52a8\u5efa\u6a21\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3002"}}
{"id": "2506.07760", "pdf": "https://arxiv.org/pdf/2506.07760", "abs": "https://arxiv.org/abs/2506.07760", "authors": ["Haijie Xu", "Chen Zhang"], "title": "Quickest Causal Change Point Detection by Adaptive Intervention", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "We propose an algorithm for change point monitoring in linear causal models\nthat accounts for interventions. Through a special centralization technique, we\ncan concentrate the changes arising from causal propagation across nodes into a\nsingle dimension. Additionally, by selecting appropriate intervention nodes\nbased on Kullback-Leibler divergence, we can amplify the change magnitude. We\nalso present an algorithm for selecting the intervention values, which aids in\nthe identification of the most effective intervention nodes. Two monitoring\nmethods are proposed, each with an adaptive intervention policy to make a\nbalance between exploration and exploitation. We theoretically demonstrate the\nfirst-order optimality of the proposed methods and validate their properties\nusing simulation datasets and two real-world case studies.", "AI": {"tldr": "An algorithm for change point monitoring in linear causal models is proposed, which can concentrate changes into a single dimension through centralization technique and amplify change magnitude by selecting intervention nodes based on Kullback-Leibler divergence. Two monitoring methods with adaptive intervention policies are also presented, achieving a balance between exploration and exploitation. Theoretical first-order optimality is demonstrated and validated via simulation and real-world case studies.", "motivation": "To develop an effective method for monitoring change points in linear causal models while accounting for interventions.", "method": "The algorithm uses a special centralization technique to concentrate changes from causal propagation across nodes into one dimension. Intervention nodes are selected based on Kullback-Leibler divergence to amplify the change magnitude. An algorithm for selecting intervention values is provided to identify the most effective intervention nodes. Two monitoring methods with adaptive intervention policies are introduced to balance exploration and exploitation.", "result": "Theoretical first-order optimality of the proposed methods is demonstrated. Properties are validated using simulation datasets and two real-world case studies.", "conclusion": "The proposed algorithm effectively monitors change points in linear causal models considering interventions, concentrating changes into a single dimension and amplifying change magnitude. The two monitoring methods with adaptive intervention policies achieve a good balance between exploration and exploitation."}}
{"id": "2506.07725", "pdf": "https://arxiv.org/pdf/2506.07725", "abs": "https://arxiv.org/abs/2506.07725", "authors": ["Shadi Hamdan", "Chonghao Sima", "Zetong Yang", "Hongyang Li", "Fatma G\u00fcney"], "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 submission. For code, see\n  https://github.com/opendrivelab/ETA", "summary": "How can we benefit from large models without sacrificing inference speed, a\ncommon dilemma in self-driving systems? A prevalent solution is a dual-system\narchitecture, employing a small model for rapid, reactive decisions and a\nlarger model for slower but more informative analyses. Existing dual-system\ndesigns often implement parallel architectures where inference is either\ndirectly conducted using the large model at each current frame or retrieved\nfrom previously stored inference results. However, these works still struggle\nto enable large models for a timely response to every online frame. Our key\ninsight is to shift intensive computations of the current frame to previous\ntime steps and perform a batch inference of multiple time steps to make large\nmodels respond promptly to each time step. To achieve the shifting, we\nintroduce Efficiency through Thinking Ahead (ETA), an asynchronous system\ndesigned to: (1) propagate informative features from the past to the current\nframe using future predictions from the large model, (2) extract current frame\nfeatures using a small model for real-time responsiveness, and (3) integrate\nthese dual features via an action mask mechanism that emphasizes\naction-critical image regions. Evaluated on the Bench2Drive CARLA\nLeaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with\na driving score of 69.53 while maintaining a near-real-time inference speed at\n50 ms.", "AI": {"tldr": "The paper proposes Efficiency through Thinking Ahead (ETA), an asynchronous dual-model system that shifts intensive computations of the current frame to previous time steps for faster inference in self-driving systems.", "motivation": "To solve the dilemma of benefiting from large models without sacrificing inference speed in self-driving systems.", "method": "Employing a small model for real-time reactive decisions and a larger model for informative analyses, with computations shifted to previous time steps via future predictions, current frame features extracted using a small model, and integration of these features via an action mask mechanism.", "result": "Advances state-of-the-art performance by 8% with a driving score of 69.53 on the Bench2Drive CARLA Leaderboard-v2 benchmark while maintaining near-real-time inference speed at 50 ms.", "conclusion": "ETA successfully enables large models to respond promptly to each time step in self-driving systems."}}
{"id": "2506.07770", "pdf": "https://arxiv.org/pdf/2506.07770", "abs": "https://arxiv.org/abs/2506.07770", "authors": ["Yang Wang", "Yin Xu", "Cixiao Zhang", "Zhiyong Chen", "Xiaowu Ou", "Mingzeng Dai", "Meixia Tao", "Wenjun Zhang"], "title": "Diffusion Models-Aided Uplink Channel Estimation for RIS-Assisted Systems", "categories": ["eess.SP", "cs.LG"], "comment": "5 pages", "summary": "This letter proposes a channel estimation method for reconfigurable\nintelligent surface (RIS)-assisted systems through a novel diffusion model (DM)\nframework. We reformulate the channel estimation problem as a denoising\nprocess, which aligns with the reverse process of the DM. To overcome the\ninherent randomness in the reverse process of conventional DM approaches, we\nadopt a deterministic sampling strategy with a step alignment mechanism that\nensures the accuracy of channel estimation while adapting to different\nsignal-to-noise ratio (SNR). Furthermore, to reduce the number of parameters of\nthe U-Net, we meticulously design a lightweight network that achieves\ncomparable performance, thereby enhancing the practicality of our proposed\nmethod. Extensive simulations demonstrate superior performance over a wide\nrange of SNRs compared to baselines. For instance, the proposed method achieves\nperformance improvements of up to 13.5 dB in normalized mean square error\n(NMSE) at SNR = 0 dB. Notably, the proposed lightweight network exhibits almost\nno performance loss compared to the original U-Net, while requiring only 6.59\\%\nof its parameters.", "AI": {"tldr": "This letter proposes a channel estimation method for RIS-assisted systems through a novel diffusion model (DM) framework, achieving superior performance over a wide range of SNRs compared to baselines.", "motivation": "To overcome the inherent randomness in the reverse process of conventional DM approaches and reduce the number of parameters of the U-Net.", "method": "Reformulate the channel estimation problem as a denoising process aligning with the reverse process of the DM, adopt a deterministic sampling strategy with a step alignment mechanism, and design a lightweight network.", "result": "Superior performance over a wide range of SNRs compared to baselines, with performance improvements of up to 13.5 dB in normalized mean square error (NMSE) at SNR = 0 dB. The lightweight network exhibits almost no performance loss compared to the original U-Net while requiring only 6.59% of its parameters.", "conclusion": "The proposed channel estimation method achieves superior performance and practicality."}}
{"id": "2506.07739", "pdf": "https://arxiv.org/pdf/2506.07739", "abs": "https://arxiv.org/abs/2506.07739", "authors": ["Jing Zhong", "Jun Yin", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Shuai Lu", "Ran Luo"], "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Architectural cultures across regions are characterized by stylistic\ndiversity, shaped by historical, social, and technological contexts in addition\nto geograph-ical conditions. Understanding architectural styles requires the\nability to describe and analyze the stylistic features of different architects\nfrom various regions through visual observations of architectural imagery.\nHowever, traditional studies of architectural culture have largely relied on\nsubjective expert interpretations and historical literature reviews, often\nsuffering from regional biases and limited ex-planatory scope. To address these\nchallenges, this study proposes three core contributions: (1) We construct a\nprofessional architectural style dataset named ArchDiffBench, which comprises\n1,765 high-quality architectural images and their corresponding style\nannotations, collected from different regions and historical periods. (2) We\npropose ArchiLense, an analytical framework grounded in Vision-Language Models\nand constructed using the ArchDiffBench dataset. By integrating ad-vanced\ncomputer vision techniques, deep learning, and machine learning algo-rithms,\nArchiLense enables automatic recognition, comparison, and precise\nclassi-fication of architectural imagery, producing descriptive language\noutputs that ar-ticulate stylistic differences. (3) Extensive evaluations show\nthat ArchiLense achieves strong performance in architectural style recognition,\nwith a 92.4% con-sistency rate with expert annotations and 84.5% classification\naccuracy, effec-tively capturing stylistic distinctions across images. The\nproposed approach transcends the subjectivity inherent in traditional analyses\nand offers a more objective and accurate perspective for comparative studies of\narchitectural culture.", "AI": {"tldr": "This paper proposes ArchDiffBench, a professional architectural style dataset, and ArchiLense, an analytical framework based on Vision-Language Models. ArchiLense enables automatic recognition, comparison, and classification of architectural imagery with high consistency and accuracy rates.", "motivation": "Traditional studies of architectural culture suffer from regional biases and limited explanatory scope due to reliance on subjective expert interpretations and historical literature reviews.", "method": "The study constructs the ArchDiffBench dataset with 1,765 high-quality architectural images and their corresponding style annotations. It also proposes ArchiLense, an analytical framework using Vision-Language Models that integrates advanced computer vision techniques, deep learning, and machine learning algorithms for automatic recognition, comparison, and classification of architectural imagery.", "result": "ArchiLense achieves a 92.4% consistency rate with expert annotations and 84.5% classification accuracy, effectively capturing stylistic distinctions across images.", "conclusion": "The proposed approach overcomes the subjectivity in traditional analyses and provides a more objective and accurate perspective for comparative studies of architectural culture."}}
{"id": "2506.07773", "pdf": "https://arxiv.org/pdf/2506.07773", "abs": "https://arxiv.org/abs/2506.07773", "authors": ["Mohamed Djilani", "Nassim Ali Ousalah", "Nidhal Eddine Chenni"], "title": "Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a trend-aware and visually-grounded fashion recommendation\nsystem that integrates deep visual representations, garment-aware segmentation,\nsemantic category similarity and user behavior simulation. Our pipeline\nextracts focused visual embeddings by masking non-garment regions via semantic\nsegmentation followed by feature extraction using pretrained CNN backbones\n(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we\ngenerate synthetic purchase histories influenced by user-specific trendiness\nand item popularity. Recommendations are computed using a weighted scoring\nfunction that fuses visual similarity, semantic coherence and popularity\nalignment. Experiments on the DeepFashion dataset demonstrate consistent gender\nalignment and improved category relevance, with ResNet-50 achieving 64.95%\ncategory similarity and lowest popularity MAE. An ablation study confirms the\ncomplementary roles of visual and popularity cues. Our method provides a\nscalable framework for personalized fashion recommendations that balances\nindividual style with emerging trends. Our implementation is available at\nhttps://github.com/meddjilani/FashionRecommender", "AI": {"tldr": "A trend-aware and visually-grounded fashion recommendation system is introduced, which integrates deep visual representations, garment-aware segmentation, semantic category similarity and user behavior simulation. Experiments on the DeepFashion dataset show gender alignment and improved category relevance.", "motivation": "To create a more personalized and trend-aware fashion recommendation system that balances individual style with emerging trends.", "method": "The system uses a pipeline that extracts focused visual embeddings by masking non-garment regions via semantic segmentation followed by feature extraction using pretrained CNN backbones (ResNet-50, DenseNet-121, VGG16). It generates synthetic purchase histories influenced by user-specific trendiness and item popularity. Recommendations are computed using a weighted scoring function that fuses visual similarity, semantic coherence and popularity alignment.", "result": "Experiments on the DeepFashion dataset demonstrate consistent gender alignment and improved category relevance, with ResNet-50 achieving 64.95% category similarity and lowest popularity MAE. An ablation study confirms the complementary roles of visual and popularity cues.", "conclusion": "This method provides a scalable framework for personalized fashion recommendations that balances individual style with emerging trends."}}
{"id": "2506.07785", "pdf": "https://arxiv.org/pdf/2506.07785", "abs": "https://arxiv.org/abs/2506.07785", "authors": ["Qi Yang", "Chenghao Zhang", "Lubin Fan", "Kun Ding", "Jieping Ye", "Shiming Xiang"], "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight. 22 pages, 16 figures", "summary": "Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.", "AI": {"tldr": "Recent advancements in LVLMs have improved VQA tasks. However, challenges remain. This study proposes RCTS, a multimodal RAG framework that constructs a reasoning context-enriched knowledge base and uses Tree Search re-ranking to enhance LVLMs.", "motivation": "To overcome the challenges faced by existing methods in VQA tasks, such as lack of reasoning examples and inconsistent responses from retrieved knowledge.", "method": "Propose RCTS, a multimodal RAG framework which includes constructing a Reasoning Context-enriched knowledge base with a self-consistent evaluation mechanism, and using Monte Carlo Tree Search with Heuristic Rewards (MCTS-HR) for re-ranking.", "result": "Extensive experiments show that RCTS achieves state-of-the-art performance on multiple VQA datasets, significantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.", "conclusion": "The study concludes that the proposed knowledge base and re-ranking method effectively improve LVLMs in VQA tasks."}}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751", "abs": "https://arxiv.org/abs/2506.07751", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstraL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks.", "AI": {"tldr": "The paper proposes AbstraL, a method using reinforcement learning to promote abstract reasoning in large language models (LLMs), which significantly mitigates performance degradation on GSM perturbation benchmarks.", "motivation": "Recent studies indicate that LLMs, particularly smaller ones, lack robustness in reasoning when facing distribution shifts, such as changes in variables or distracting clauses. To address this issue, while generating synthetic data is one strategy, the authors focus on 'abstracting' reasoning problems to counteract these shifts and connect with symbolic tools for solutions.", "method": "The method involves using reinforcement learning (RL) to acquire the abstraction process of reasoning problems rather than relying solely on supervised fine-tuning, which often fails to produce faithful abstractions. This approach aims to promote abstract reasoning in LLMs.", "result": "AbstraL significantly reduces performance degradation on recent GSM perturbation benchmarks, showing its effectiveness in enhancing robustness in reasoning for LLMs.", "conclusion": "Abstraction through reinforcement learning is more effective than supervised fine-tuning in promoting abstract reasoning in LLMs, leading to better performance stability under distribution shifts."}}
{"id": "2506.07801", "pdf": "https://arxiv.org/pdf/2506.07801", "abs": "https://arxiv.org/abs/2506.07801", "authors": ["Iustin Sirbu", "Robert-Adrian Popovici", "Cornelia Caragea", "Stefan Trausan-Matu", "Traian Rebedea"], "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.", "AI": {"tldr": "The paper introduces MultiMatch, a novel SSL algorithm that combines co-training and consistency regularization with pseudo-labeling, featuring a three-fold pseudo-label weighting module. It achieves SOTA results on 9 out of 10 setups from 5 NLP datasets and shows exceptional robustness in imbalanced settings.", "motivation": "To improve the performance and robustness of semi-supervised learning algorithms by enhancing the pseudo-labeling process.", "method": "MultiMatch is a SSL algorithm that integrates heads agreement from Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average Pseudo-Margins from MarginMatch into a three-fold pseudo-label weighting module.", "result": "MultiMatch achieves state-of-the-art results on 9 out of 10 setups from 5 natural language processing datasets and ranks first according to the Friedman test among 19 methods. It also demonstrates exceptional robustness in highly imbalanced settings, outperforming the second-best approach by 3.26%.", "conclusion": "MultiMatch enhances and unifies existing SSL techniques, resulting in superior performance and robustness in both balanced and imbalanced data settings."}}
{"id": "2506.07810", "pdf": "https://arxiv.org/pdf/2506.07810", "abs": "https://arxiv.org/abs/2506.07810", "authors": ["Emiliano Tolotti", "Enrico Blanzieri", "Davide Pastorello"], "title": "A weighted quantum ensemble of homogeneous quantum classifiers", "categories": ["quant-ph", "cs.ET", "cs.LG"], "comment": "21 pages, 4 figures", "summary": "Ensemble methods in machine learning aim to improve prediction accuracy by\ncombining multiple models. This is achieved by ensuring diversity among\npredictors to capture different data aspects. Homogeneous ensembles use\nidentical models, achieving diversity through different data subsets, and\nweighted-average ensembles assign higher influence to more accurate models\nthrough a weight learning procedure. We propose a method to achieve a weighted\nhomogeneous quantum ensemble using quantum classifiers with indexing registers\nfor data encoding. This approach leverages instance-based quantum classifiers,\nenabling feature and training point subsampling through superposition and\ncontrolled unitaries, and allowing for a quantum-parallel execution of diverse\ninternal classifiers with different data compositions in superposition. The\nmethod integrates a learning process involving circuit execution and classical\nweight optimization, for a trained ensemble execution with weights encoded in\nthe circuit at test-time. Empirical evaluation demonstrate the effectiveness of\nthe proposed method, offering insights into its performance.", "AI": {"tldr": "This paper proposes a weighted homogeneous quantum ensemble method using quantum classifiers with indexing registers for data encoding, which enables feature and training point subsampling through superposition and controlled unitaries.", "motivation": "To improve prediction accuracy in machine learning by combining multiple models while leveraging quantum computing capabilities.", "method": "The method uses instance-based quantum classifiers with indexing registers for data encoding. It allows for feature and training point subsampling through superposition and controlled unitaries, leading to quantum-parallel execution of diverse internal classifiers. The approach integrates a learning process involving circuit execution and classical weight optimization.", "result": "Empirical evaluation shows the effectiveness of the proposed method, providing insights into its performance.", "conclusion": "The proposed weighted homogeneous quantum ensemble method demonstrates potential in enhancing prediction accuracy via quantum computing techniques."}}
{"id": "2506.07816", "pdf": "https://arxiv.org/pdf/2506.07816", "abs": "https://arxiv.org/abs/2506.07816", "authors": ["Yingli Wang", "Changwei Tu", "Xiaoyu Wang", "Lingjiong Zhu"], "title": "Accelerating Constrained Sampling: A Large Deviations Approach", "categories": ["stat.ML", "cs.LG", "math.PR"], "comment": "40 pages, 7 figures", "summary": "The problem of sampling a target probability distribution on a constrained\ndomain arises in many applications including machine learning. For constrained\nsampling, various Langevin algorithms such as projected Langevin Monte Carlo\n(PLMC) based on the discretization of reflected Langevin dynamics (RLD) and\nmore generally skew-reflected non-reversible Langevin Monte Carlo (SRNLMC)\nbased on the discretization of skew-reflected non-reversible Langevin dynamics\n(SRNLD) have been proposed and studied in the literature. This work focuses on\nthe long-time behavior of SRNLD, where a skew-symmetric matrix is added to RLD.\nAlthough the non-asymptotic convergence analysis for SRNLD (and SRNLMC) and the\nacceleration compared to RLD (and PMLC) have been studied in the literature, it\nis not clear how one should design the skew-symmetric matrix in the dynamics to\nachieve good performance in practice. We establish a large deviation principle\n(LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is\nchosen such that its product with the inward unit normal vector field on the\nboundary is zero. By explicitly characterizing the rate functions, we show that\nSRNLD can accelerate the convergence to the target distribution compared to RLD\nwith this choice of the skew-symmetric matrix. Numerical experiments for SRNLMC\nbased on the proposed skew-symmetric matrix show superior performance which\nvalidate the theoretical findings from the large deviations theory.", "AI": {"tldr": "The paper focuses on the long-time behavior of skew-reflected non-reversible Langevin dynamics (SRNLD) and proposes a method to design the skew-symmetric matrix in SRNLD for better performance. It establishes a large deviation principle (LDP) for SRNLD and shows that SRNLD can accelerate convergence to the target distribution compared to RLD with this choice of the skew-symmetric matrix.", "motivation": "To address the problem of sampling a target probability distribution on a constrained domain, which is important in many applications including machine learning.", "method": "Establishes a large deviation principle (LDP) for the empirical measure of SRNLD when the skew-symmetric matrix is chosen such that its product with the inward unit normal vector field on the boundary is zero.", "result": "Shows that SRNLD can accelerate the convergence to the target distribution compared to RLD with this choice of the skew-symmetric matrix. Numerical experiments validate the theoretical findings from the large deviations theory.", "conclusion": "SRNLD can achieve good performance in practice by designing the skew-symmetric matrix appropriately."}}
{"id": "2506.07826", "pdf": "https://arxiv.org/pdf/2506.07826", "abs": "https://arxiv.org/abs/2506.07826", "authors": ["William Ljungbergh", "Bernardo Taveira", "Wenzhao Zheng", "Adam Tonderski", "Chensheng Peng", "Fredrik Kahl", "Christoffer Petersson", "Michael Felsberg", "Kurt Keutzer", "Masayoshi Tomizuka", "Wei Zhan"], "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Validating autonomous driving (AD) systems requires diverse and\nsafety-critical testing, making photorealistic virtual environments essential.\nTraditional simulation platforms, while controllable, are resource-intensive to\nscale and often suffer from a domain gap with real-world data. In contrast,\nneural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a\nscalable solution for creating photorealistic digital twins of real-world\ndriving scenes. However, they struggle with dynamic object manipulation and\nreusability as their per-scene optimization-based methodology tends to result\nin incomplete object models with integrated illumination effects. This paper\nintroduces R3D2, a lightweight, one-step diffusion model designed to overcome\nthese limitations and enable realistic insertion of complete 3D assets into\nexisting scenes by generating plausible rendering effects-such as shadows and\nconsistent lighting-in real time. This is achieved by training R3D2 on a novel\ndataset: 3DGS object assets are generated from in-the-wild AD data using an\nimage-conditioned 3D generative model, and then synthetically placed into\nneural rendering-based virtual environments, allowing R3D2 to learn realistic\nintegration. Quantitative and qualitative evaluations demonstrate that R3D2\nsignificantly enhances the realism of inserted assets, enabling use-cases like\ntext-to-3D asset insertion and cross-scene/dataset object transfer, allowing\nfor true scalability in AD validation. To promote further research in scalable\nand realistic AD simulation, we will release our dataset and code, see\nhttps://research.zenseact.com/publications/R3D2/.", "AI": {"tldr": "R3D2 is a lightweight diffusion model that enhances the realism of inserted 3D assets in autonomous driving simulations by generating plausible rendering effects like shadows and consistent lighting.", "motivation": "Current neural reconstruction methods, while scalable, struggle with dynamic object manipulation and reusability due to incomplete object models with integrated illumination effects. There is a need for a solution that can realistically insert complete 3D assets into existing scenes.", "method": "R3D2 is trained on a novel dataset where 3DGS object assets are generated from AD data using an image-conditioned 3D generative model and then synthetically placed into virtual environments. This enables the model to learn realistic integration of objects into scenes.", "result": "Quantitative and qualitative evaluations show that R3D2 significantly improves the realism of inserted assets, supporting use-cases such as text-to-3D asset insertion and cross-scene/dataset object transfer.", "conclusion": "R3D2 offers a scalable and efficient solution for enhancing the realism of 3D assets in autonomous driving simulations, promoting further research in this field."}}
{"id": "2506.07813", "pdf": "https://arxiv.org/pdf/2506.07813", "abs": "https://arxiv.org/abs/2506.07813", "authors": ["Junseo Bang", "Joonhee Lee", "Kyeonghyun Lee", "Haechang Lee", "Dong Un Kang", "Se Young Chun"], "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Arbitrary-scale image super-resolution aims to upsample images to any desired\nresolution, offering greater flexibility than traditional fixed-scale\nsuper-resolution. Recent approaches in this domain utilize regression-based or\ngenerative models, but many of them are a single-stage upsampling process,\nwhich may be challenging to learn across a wide, continuous distribution of\nscaling factors. Progressive upsampling strategies have shown promise in\nmitigating this issue, yet their integration with diffusion models for flexible\nupscaling remains underexplored. Here, we present CasArbi, a novel\nself-cascaded diffusion framework for arbitrary-scale image super-resolution.\nCasArbi meets the varying scaling demands by breaking them down into smaller\nsequential factors and progressively enhancing the image resolution at each\nstep with seamless transitions for arbitrary scales. Our novel\ncoordinate-guided residual diffusion model allows for the learning of\ncontinuous image representations while enabling efficient diffusion sampling.\nExtensive experiments demonstrate that our CasArbi outperforms prior arts in\nboth perceptual and distortion performance metrics across diverse\narbitrary-scale super-resolution benchmarks.", "AI": {"tldr": "The paper introduces CasArbi, a self-cascaded diffusion framework for arbitrary-scale image super-resolution that breaks down scaling demands into smaller sequential factors and progressively enhances image resolution.", "motivation": "Existing methods for arbitrary-scale image super-resolution either use single-stage upsampling processes or have not fully explored the integration of progressive upsampling strategies with diffusion models.", "method": "CasArbi utilizes a novel coordinate-guided residual diffusion model to learn continuous image representations and enable efficient diffusion sampling, progressively enhancing image resolution at each step.", "result": "CasArbi outperforms prior arts in both perceptual and distortion performance metrics across diverse arbitrary-scale super-resolution benchmarks.", "conclusion": "CasArbi provides a promising approach for arbitrary-scale image super-resolution by effectively meeting varying scaling demands through a progressive enhancement process."}}
{"id": "2506.07844", "pdf": "https://arxiv.org/pdf/2506.07844", "abs": "https://arxiv.org/abs/2506.07844", "authors": ["Mingzhou Liu", "Xinwei Sun", "Yizhou Wang"], "title": "Conditional Local Independence Testing with Application to Dynamic Causal Discovery", "categories": ["stat.ME", "cs.LG"], "comment": "Working paper", "summary": "In this note, we extend the conditional local independence testing theory\ndeveloped in Christgau et al. (2024) to Ito processes. The result can be\napplied to causal discovery in dynamic systems.", "AI": {"tldr": "The paper extends conditional local independence testing theory to Ito processes for causal discovery in dynamic systems.", "motivation": "To apply the conditional local independence testing theory for causal discovery in dynamic systems.", "method": "Extending the conditional local independence testing theory developed in Christgau et al. (2024) to Ito processes.", "result": "The extension allows for application in causal discovery within dynamic systems.", "conclusion": "This extension of the theory provides a new approach for analyzing causality in dynamic systems through Ito processes."}}
{"id": "2506.07857", "pdf": "https://arxiv.org/pdf/2506.07857", "abs": "https://arxiv.org/abs/2506.07857", "authors": ["Zihui Zhang", "Weisheng Dai", "Hongtao Wen", "Bo Yang"], "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/LogoSP", "summary": "We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.", "AI": {"tldr": "This paper presents LogoSP, a method for unsupervised 3D semantic segmentation that learns from local and global point features, discovering semantic information through superpoint grouping in the frequency domain to generate pseudo-labels.", "motivation": "The motivation is to improve unsupervised 3D semantic segmentation by going beyond local features and discovering richer semantic priors without requiring human-provided labels.", "method": "LogoSP learns 3D semantics by using both local and global point features. It discovers 3D semantic information via grouping superpoints according to their global patterns in the frequency domain, which are used to create accurate semantic pseudo-labels for training a segmentation network.", "result": "Extensive experiments on two indoor and one outdoor datasets demonstrate that LogoSP outperforms all existing unsupervised methods by significant margins, achieving state-of-the-art performance for unsupervised 3D semantic segmentation.", "conclusion": "LogoSP successfully learns meaningful 3D semantics from raw point clouds without human labels during training, surpassing current unsupervised methods."}}
{"id": "2506.07859", "pdf": "https://arxiv.org/pdf/2506.07859", "abs": "https://arxiv.org/abs/2506.07859", "authors": ["Amanuel Anteneh L\u00e9andre Brunel", "Carlos Gonz\u00e1lez-Arciniegas", "Olivier Pfister"], "title": "Deep reinforcement learning for near-deterministic preparation of cubic- and quartic-phase gates in photonic quantum computing", "categories": ["quant-ph", "cs.LG"], "comment": null, "summary": "Cubic-phase states are a sufficient resource for universal quantum computing\nover continuous variables. We present results from numerical experiments in\nwhich deep neural networks are trained via reinforcement learning to control a\nquantum optical circuit for generating cubic-phase states, with an average\nsuccess rate of 96%. The only non-Gaussian resource required is\nphoton-number-resolving measurements. We also show that the exact same\nresources enable the direct generation of a quartic-phase gate, with no need\nfor a cubic gate decomposition.", "AI": {"tldr": "Cubic-phase states are crucial for universal quantum computing in continuous variables. Researchers trained deep neural networks using reinforcement learning to control quantum optical circuits for generating these states with a 96% success rate. Only photon-number-resolving measurements were needed as non-Gaussian resources. Additionally, the same resources can generate a quartic-phase gate directly.", "motivation": "To explore the potential of cubic-phase states as a resource for universal quantum computing over continuous variables and demonstrate the feasibility of using deep neural networks controlled by reinforcement learning to generate these states efficiently.", "method": "The researchers conducted numerical experiments where deep neural networks were trained via reinforcement learning techniques to control quantum optical circuits aimed at generating cubic-phase states. The only non-Gaussian resource utilized was photon-number-resolving measurements.", "result": "The method achieved an average success rate of 96% in generating cubic-phase states. Furthermore, it was shown that the same resources could be used to directly generate a quartic-phase gate without needing a cubic gate decomposition.", "conclusion": "Cubic-phase states can serve as a sufficient resource for universal quantum computing in continuous variables when generated using deep neural network-controlled quantum optical circuits with reinforcement learning. The same setup also enables the direct generation of a quartic-phase gate."}}
{"id": "2506.07863", "pdf": "https://arxiv.org/pdf/2506.07863", "abs": "https://arxiv.org/abs/2506.07863", "authors": ["Lev Novitskiy", "Viacheslav Vasilev", "Maria Kovaleva", "Vladimir Arkhipkin", "Denis Dimitrov"], "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer\nvision, yet their training is often plagued by artifacts that degrade\nreconstruction and generation quality. This paper introduces VIVAT, a\nsystematic approach to mitigating common artifacts in KL-VAE training without\nrequiring radical architectural changes. We present a detailed taxonomy of five\nprevalent artifacts - color shift, grid patterns, blur, corner and droplet\nartifacts - and analyze their root causes. Through straightforward\nmodifications, including adjustments to loss weights, padding strategies, and\nthe integration of Spatially Conditional Normalization, we demonstrate\nsignificant improvements in VAE performance. Our method achieves\nstate-of-the-art results in image reconstruction metrics (PSNR and SSIM) across\nmultiple benchmarks and enhances text-to-image generation quality, as evidenced\nby superior CLIP scores. By preserving the simplicity of the KL-VAE framework\nwhile addressing its practical challenges, VIVAT offers actionable insights for\nresearchers and practitioners aiming to optimize VAE training.", "AI": {"tldr": "The paper introduces VIVAT, a method to reduce common artifacts in Variational Autoencoders (VAEs) training without major architectural changes, leading to state-of-the-art results in image reconstruction and text-to-image generation.", "motivation": "VAEs are crucial in generative computer vision, but their training is often affected by artifacts that lower the quality of reconstruction and generation. There's a need for a systematic approach to mitigate these issues without significant changes to the architecture.", "method": "VIVAT presents a taxonomy of five prevalent artifacts (color shift, grid patterns, blur, corner, and droplet artifacts) and addresses them through simple modifications like adjusting loss weights, padding strategies, and incorporating Spatially Conditional Normalization.", "result": "This method significantly improves VAE performance, achieving state-of-the-art results in image reconstruction metrics (PSNR and SSIM) across multiple benchmarks and enhancing text-to-image generation quality with higher CLIP scores.", "conclusion": "VIVAT preserves the simplicity of the KL-VAE framework while effectively addressing practical challenges, providing valuable insights for optimizing VAE training."}}
{"id": "2506.07865", "pdf": "https://arxiv.org/pdf/2506.07865", "abs": "https://arxiv.org/abs/2506.07865", "authors": ["Jinxi Li", "Ziyang Song", "Siyuan Zhou", "Bo Yang"], "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/FreeGave", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.", "AI": {"tldr": "This paper proposes FreeGave, a method for modeling 3D scene geometry, appearance, and physics from multi-view videos without object priors. It introduces a physics code and divergence-free module to estimate velocity fields, showing superior performance in future frame extrapolation and motion segmentation.", "motivation": "Existing works on modeling 3D scenes using governing PDEs as PINN losses or incorporating physics simulation into neural networks either fail to learn complex physical motions at boundaries or require object priors such as masks or types.", "method": "The proposed method, FreeGave, learns the physics of complex dynamic 3D scenes without needing any object priors by introducing a physics code followed by a carefully designed divergence-free module for estimating a per-Gaussian velocity field, without relying on inefficient PINN losses.", "result": "Extensive experiments on three public datasets and a newly collected challenging real-world dataset demonstrate the superior performance of the method for future frame extrapolation and motion segmentation.", "conclusion": "FreeGave successfully learns meaningful 3D physical motion patterns without any human labels in training."}}
{"id": "2506.07841", "pdf": "https://arxiv.org/pdf/2506.07841", "abs": "https://arxiv.org/abs/2506.07841", "authors": ["Elizabeth Pavlova", "Xue-Xin Wei"], "title": "Diffusion models under low-noise regime", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent work on diffusion models proposed that they operate in two regimes:\nmemorization, in which models reproduce their training data, and\ngeneralization, in which they generate novel samples. While this has been\ntested in high-noise settings, the behavior of diffusion models as effective\ndenoisers when the corruption level is small remains unclear. To address this\ngap, we systematically investigated the behavior of diffusion models under\nlow-noise diffusion dynamics, with implications for model robustness and\ninterpretability. Using (i) CelebA subsets of varying sample sizes and (ii)\nanalytic Gaussian mixture benchmarks, we reveal that models trained on disjoint\ndata diverge near the data manifold even when their high-noise outputs\nconverge. We quantify how training set size, data geometry, and model objective\nchoice shape denoising trajectories and affect score accuracy, providing\ninsights into how these models actually learn representations of data\ndistributions. This work starts to address gaps in our understanding of\ngenerative model reliability in practical applications where small\nperturbations are common.", "AI": {"tldr": "\u4e3a\u4e86\u586b\u8865\u5bf9\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u9760\u6027\u7684\u7406\u89e3\u7a7a\u767d\uff0c\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u6269\u6563\u6a21\u578b\u5728\u4f4e\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u3002\u901a\u8fc7\u4f7f\u7528CelebA\u5b50\u96c6\u548c\u89e3\u6790\u9ad8\u65af\u6df7\u5408\u57fa\u51c6\uff0c\u7814\u7a76\u53d1\u73b0\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u8f93\u51fa\u6536\u655b\u7684\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u4e8e\u4e0d\u76f8\u4ea4\u6570\u636e\u7684\u6a21\u578b\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u4e5f\u4f1a\u51fa\u73b0\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u8fd8\u91cf\u5316\u4e86\u8bad\u7ec3\u96c6\u5927\u5c0f\u3001\u6570\u636e\u51e0\u4f55\u7ed3\u6784\u548c\u6a21\u578b\u76ee\u6807\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u53bb\u566a\u8f68\u8ff9\u548c\u8bc4\u5206\u51c6\u786e\u6027\uff0c\u4ece\u800c\u63ed\u793a\u8fd9\u4e9b\u6a21\u578b\u5b66\u4e60\u6570\u636e\u5206\u5e03\u8868\u793a\u7684\u65b9\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\u7684\u884c\u4e3a\u5df2\u88ab\u6d4b\u8bd5\uff0c\u4f46\u5176\u5728\u4f4e\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u5c1a\u4e0d\u6e05\u695a\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7cfb\u7edf\u5730\u7814\u7a76\u6269\u6563\u6a21\u578b\u5728\u4f4e\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u4ee5\u4e86\u89e3\u5176\u4f5c\u4e3a\u6709\u6548\u53bb\u566a\u5668\u7684\u7279\u6027\uff0c\u5e76\u63a2\u8ba8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86CelebA\u7684\u4e0d\u540c\u6837\u672c\u5927\u5c0f\u5b50\u96c6\u548c\u89e3\u6790\u9ad8\u65af\u6df7\u5408\u57fa\u51c6\uff0c\u6765\u5206\u6790\u6269\u6563\u6a21\u578b\u7684\u884c\u4e3a\u3002\u91cd\u70b9\u5728\u4e8e\u89c2\u5bdf\u6a21\u578b\u5728\u4f4e\u566a\u58f0\u6269\u6563\u52a8\u6001\u4e0b\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5f53\u8bad\u7ec3\u6570\u636e\u4e0d\u76f8\u4ea4\u65f6\u6a21\u578b\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u4ee5\u53ca\u8bad\u7ec3\u96c6\u5927\u5c0f\u3001\u6570\u636e\u51e0\u4f55\u7ed3\u6784\u548c\u6a21\u578b\u76ee\u6807\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u53bb\u566a\u8f68\u8ff9\u548c\u8bc4\u5206\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u8f93\u51fa\u6536\u655b\u7684\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u4e8e\u4e0d\u76f8\u4ea4\u6570\u636e\u7684\u6a21\u578b\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u4e5f\u4f1a\u51fa\u73b0\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7684\u53bb\u566a\u8f68\u8ff9\u548c\u8bc4\u5206\u51c6\u786e\u6027\u53d7\u5230\u8bad\u7ec3\u96c6\u5927\u5c0f\u3001\u6570\u636e\u51e0\u4f55\u7ed3\u6784\u548c\u6a21\u578b\u76ee\u6807\u9009\u62e9\u7684\u5f71\u54cd\u3002\u8fd9\u4e3a\u7406\u89e3\u6269\u6563\u6a21\u578b\u5982\u4f55\u5b66\u4e60\u6570\u636e\u5206\u5e03\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u4f4e\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u8868\u660e\uff0c\u5373\u4f7f\u5728\u9ad8\u566a\u58f0\u6761\u4ef6\u4e0b\u6a21\u578b\u8f93\u51fa\u76f8\u4f3c\uff0c\u5b83\u4eec\u5728\u6570\u636e\u6d41\u5f62\u9644\u8fd1\u7684\u8868\u793a\u4e5f\u53ef\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u8fd9\u4e00\u53d1\u73b0\u6709\u52a9\u4e8e\u63d0\u5347\u5bf9\u751f\u6210\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u7684\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u5c0f\u6270\u52a8\u5e38\u89c1\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2506.07848", "pdf": "https://arxiv.org/pdf/2506.07848", "abs": "https://arxiv.org/abs/2506.07848", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Jiangning Zhang", "Yuan Zhou", "Qinglin Lu", "Ran Yi"], "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.", "AI": {"tldr": "\u63d0\u51faPolyVivid\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u4e3b\u4f53\u89c6\u9891\u5b9a\u5236\u4e2d\u7684\u8eab\u4efd\u4e00\u81f4\u6027\u4e0e\u4ea4\u4e92\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u79cd\u6a21\u5757\u589e\u5f3a\u8eab\u4efd\u4fdd\u7559\u548c\u4e3b\u4f53\u95f4\u4e92\u52a8\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u591a\u4e3b\u4f53\u6570\u636e\u7ba1\u9053\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u89c6\u9891\u751f\u6210\u6a21\u578b\u7f3a\u4e4f\u7cbe\u7ec6\u63a7\u5236\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u591a\u4e3b\u4f53\u5b9a\u5236\u4e2d\u96be\u4ee5\u4fdd\u6301\u4e00\u81f4\u7684\u8eab\u4efd\u548c\u5b9e\u73b0\u4ea4\u4e92\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8eVLLM\u7684\u6587\u672c-\u56fe\u50cf\u878d\u5408\u6a21\u5757\u3001\u57fa\u4e8e3D-RoPE\u7684\u589e\u5f3a\u6a21\u5757\u3001\u6ce8\u610f\u529b\u7ee7\u627f\u7684\u8eab\u4efd\u6ce8\u5165\u6a21\u5757\u4ee5\u53ca\u57fa\u4e8eMLLM\u7684\u6570\u636e\u7ba1\u9053\uff0c\u4ee5\u5206\u522b\u5b9e\u73b0\u7cbe\u786e\u63a5\u5730\u3001\u7ed3\u6784\u5316\u53cc\u5411\u878d\u5408\u3001\u6709\u6548\u7279\u5f81\u6ce8\u5165\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u751f\u6210\u3002", "result": "\u5728\u8eab\u4efd\u4fdd\u771f\u5ea6\u3001\u89c6\u9891\u771f\u5b9e\u611f\u548c\u4e3b\u4f53\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u73b0\u6709\u7684\u5f00\u6e90\u548c\u5546\u4e1a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "PolyVivid\u6846\u67b6\u5728\u591a\u4e3b\u4f53\u89c6\u9891\u5b9a\u5236\u9886\u57df\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u4e14\u4e00\u81f4\u6027\u7684\u751f\u6210\u6548\u679c\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.07897", "pdf": "https://arxiv.org/pdf/2506.07897", "abs": "https://arxiv.org/abs/2506.07897", "authors": ["Shuja Khalid", "Mohamed Ibrahim", "Yang Liu"], "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.", "AI": {"tldr": "This paper proposes a new method to enhance the resolution and geometric fidelity of 3D Gaussian Splatting beyond native training resolution.", "motivation": "Current 3DGS methods are limited by their input resolution, unable to extrapolate finer details than present in the training views.", "method": "A lightweight generative model that predicts and refines additional 3D Gaussians where needed most, with Hessian-assisted sampling strategy to identify regions for densification.", "result": "Comprehensive experiments demonstrate significant improvements in both geometric accuracy and rendering quality compared to state-of-the-art methods.", "conclusion": "The proposed method establishes a new paradigm for resolution-free 3D scene enhancement."}}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899", "abs": "https://arxiv.org/abs/2506.07899", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably - without retraining or forgetting previous\ninformation - remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks across LLaMA-3 and Mistral\ndemonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting.", "AI": {"tldr": "MEMOIR\u662f\u4e00\u79cd\u65b0\u7684\u53ef\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u8bb0\u5fc6\u6ce8\u5165\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\u3002\u5b83\u5728\u53ef\u9760\u6027\u548c\u6cdb\u5316\u6027\u6307\u6807\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u6570\u5343\u4e2a\u987a\u5e8f\u7f16\u8f91\uff0c\u51e0\u4e4e\u4e0d\u4f1a\u5fd8\u8bb0\u4ee5\u524d\u7684\u77e5\u8bc6\u3002", "motivation": "\u5f53\u524d\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u540e\u9a8c\u66f4\u65b0\u4ee5\u5305\u542b\u65b0\u77e5\u8bc6\u6216\u7ea0\u6b63\u9519\u8bef\u77e5\u8bc6\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\uff0c\u8981\u4e48\u5e72\u6270\u8fc7\u53bb\u7684\u7f16\u8f91\uff0c\u6216\u8005\u65e0\u6cd5\u6269\u5c55\u5230\u957f\u7f16\u8f91\u5e8f\u5217\u3002", "method": "MEMOIR\u901a\u8fc7\u4e00\u4e2a\u4e13\u95e8\u7684\u53c2\u6570\u6a21\u5757\uff08\u5373\u6b8b\u5dee\u8bb0\u5fc6\uff09\u6ce8\u5165\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6838\u5fc3\u80fd\u529b\u3002\u901a\u8fc7\u5bf9\u6837\u672c\u4f9d\u8d56\u7684\u63a9\u7801\u7a00\u758f\u5316\u8f93\u5165\u6fc0\u6d3b\uff0c\u5c06\u6bcf\u6b21\u7f16\u8f91\u9650\u5236\u5728\u8bb0\u5fc6\u53c2\u6570\u7684\u4e0d\u540c\u5b50\u96c6\u4e2d\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u7f16\u8f91\u4e4b\u95f4\u7684\u5e72\u6270\u3002\u5728\u63a8\u7406\u65f6\uff0c\u901a\u8fc7\u6bd4\u8f83\u65b0\u67e5\u8be2\u7684\u7a00\u758f\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u7f16\u8f91\u671f\u95f4\u5b58\u50a8\u7684\u6a21\u5f0f\uff0c\u8bc6\u522b\u76f8\u5173\u7f16\u8f91\u3002", "result": "\u5728\u95ee\u7b54\u3001\u5e7b\u89c9\u6821\u6b63\u548c\u5206\u5e03\u5916\u6cdb\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528LLaMA-3\u548cMistral\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eMEMOIR\u5728\u53ef\u9760\u6027\u3001\u6cdb\u5316\u6027\u548c\u5c40\u90e8\u6027\u6307\u6807\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u6570\u5343\u4e2a\u987a\u5e8f\u7f16\u8f91\uff0c\u51e0\u4e4e\u4e0d\u4f1a\u5fd8\u8bb0\u4ee5\u524d\u7684\u77e5\u8bc6\u3002", "conclusion": "MEMOIR\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6216\u5fd8\u8bb0\u5148\u524d\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5bf9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7f16\u8f91\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2506.07925", "pdf": "https://arxiv.org/pdf/2506.07925", "abs": "https://arxiv.org/abs/2506.07925", "authors": ["Yaxita Amin", "Naimisha S Trivedi", "Rashmi Bhattad"], "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.", "AI": {"tldr": "Remote sensing change detection using U-Net variations is analyzed in this paper, which evaluated 18 different U-Net architectures for their potential in detecting changes in remote sensing. Aspects such as managing data from different time periods and collecting relationships over a long distance were emphasized to improve the precision of change detection.", "motivation": "To fill the gap in the application of U-Net architecture in remote sensing field by conducting a comprehensive analysis of 34 papers and assessing 18 different U-Net variations for detecting changes in remote sensing.", "method": "Comparison and analysis of 18 different U-Net variations for remote sensing change detection, emphasizing variations explicitly built for change detection, such as Siamese Swin-U-Net.", "result": "Provided valuable insights on choosing U-Net versions for remote sensing change detection tasks, highlighting aspects such as managing data from different time periods and collecting relationships over a long distance.", "conclusion": "This study offers guidance for researchers and practitioners in selecting appropriate U-Net variations for remote sensing change detection."}}
{"id": "2506.07932", "pdf": "https://arxiv.org/pdf/2506.07932", "abs": "https://arxiv.org/abs/2506.07932", "authors": ["Rishit Dagli", "Yushi Guan", "Sankeerth Durvasula", "Mohammadreza Mofayezi", "Nandita Vijaykumar"], "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.", "AI": {"tldr": "The paper introduces Squeeze3D, a framework that uses pre-trained 3D generative models to compress 3D data at high ratios without significant quality loss.", "motivation": "To address the need for efficient compression of 3D data formats such as meshes, point clouds, and radiance fields while maintaining visual quality.", "method": "Squeeze3D bridges latent spaces between a pre-trained encoder and a pre-trained generation model through trainable mapping networks. It transforms 3D models into compact latent codes and then back into original models using generative models.", "result": "Achieves compression ratios of up to 2187x for textured meshes, 55x for point clouds, and 619x for radiance fields with comparable visual quality to existing methods.", "conclusion": "Squeeze3D provides an effective solution for compressing various 3D data formats at high ratios with minimal latency and no requirement for specific datasets."}}
{"id": "2506.07936", "pdf": "https://arxiv.org/pdf/2506.07936", "abs": "https://arxiv.org/abs/2506.07936", "authors": ["Chengyue Huang", "Yuchen Zhu", "Sichen Zhu", "Jingyun Xiao", "Moises Andrade", "Shivang Chopra", "Zsolt Kira"], "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.", "AI": {"tldr": "\u5c3d\u7ba1\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\u5177\u6709\u4e0a\u4e0b\u6587\u5185\u5b66\u4e60(ICL)\u80fd\u529b\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u901a\u5e38\u4f9d\u8d56\u6d45\u5c42\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u975e\u771f\u6b63\u7406\u89e3\u4efb\u52a1\u3002\u672c\u6587\u901a\u8fc7\u8bc4\u4f30VLMs\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5e26\u63a8\u7406\u7684\u591a\u6a21\u6001ICL\u7ba1\u9053\uff0c\u5e76\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u5f53\u524dVLMs\u672a\u80fd\u6709\u6548\u5229\u7528\u6f14\u793a\u4fe1\u606f\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u8bc4\u4f30\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5185\u5b66\u4e60(MM-ICL)\u80fd\u529b\uff0c\u63a2\u7d22\u5176\u662f\u5426\u771f\u6b63\u57fa\u4e8e\u793a\u4f8b\u8fdb\u884c\u5b66\u4e60\u800c\u975e\u4f9d\u8d56\u7b80\u5355\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "method": "1. \u8bc4\u4f30VLMs\u5728\u4e0d\u540c\u6570\u636e\u5206\u5e03\u4e0b\u7684\u8868\u73b0\uff0c\u5206\u6790\u66f4\u591a\u793a\u4f8b\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002\n2. \u63d0\u51fa\u4e00\u79cd\u65b0\u7684MM-ICL with Reasoning\u7ba1\u9053\uff0c\u5728\u6bcf\u4e2a\u793a\u4f8b\u4e2d\u52a0\u5165\u751f\u6210\u7684\u63a8\u7406\u4f9d\u636e\u3002\n3. \u4f7f\u7528\u591a\u79cd\u5f00\u6e90\u53ca\u4e13\u6709\u6a21\u578b(\u5982Gemini 2.0)\uff0c\u5728\u611f\u77e5\u548c\u63a8\u7406\u9700\u6c42\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002\n4. \u63a7\u5236\u53d8\u91cf\u5305\u62ec\u793a\u4f8b\u6570\u91cf\u3001\u68c0\u7d22\u65b9\u6cd5\u3001\u63a8\u7406\u8d28\u91cf\u53ca\u5206\u5e03\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u589e\u52a0\u793a\u4f8b\u6570\u91cf\u5e76\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u590d\u5236\u7b54\u6848\u800c\u975e\u4ece\u793a\u4f8b\u4e2d\u5b66\u4e60\uff0c\u4e14\u5bf9\u5404\u79cd\u56e0\u7d20\u7684\u53d8\u5316\u8868\u73b0\u51fa\u6709\u9650\u7684\u654f\u611f\u6027\u3002", "conclusion": "\u5f53\u524dVLMs\u672a\u80fd\u6709\u6548\u5229\u7528\u793a\u4f8b\u7ea7\u522b\u7684\u4fe1\u606f\u4ee5\u5b9e\u73b0\u771f\u6b63\u7684\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5185\u5b66\u4e60\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1\u6216\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2506.07952", "pdf": "https://arxiv.org/pdf/2506.07952", "abs": "https://arxiv.org/abs/2506.07952", "authors": ["George Orfanides", "Tim Hoheisel", "Marwa El Halabi"], "title": "Discrete and Continuous Difference of Submodular Minimization", "categories": ["math.OC", "cs.DS", "cs.LG"], "comment": null, "summary": "Submodular functions, defined on continuous or discrete domains, arise in\nnumerous applications. We study the minimization of the difference of two\nsubmodular (DS) functions, over both domains, extending prior work restricted\nto set functions. We show that all functions on discrete domains and all smooth\nfunctions on continuous domains are DS. For discrete domains, we observe that\nDS minimization is equivalent to minimizing the difference of two convex (DC)\nfunctions, as in the set function case. We propose a novel variant of the DC\nAlgorithm (DCA) and apply it to the resulting DC Program, obtaining comparable\ntheoretical guarantees as in the set function case. The algorithm can be\napplied to continuous domains via discretization. Experiments demonstrate that\nour method outperforms baselines in integer compressive sensing and integer\nleast squares.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u7814\u7a76\u4e86\u5728\u8fde\u7eed\u548c\u79bb\u6563\u57df\u4e0a\u4e24\u4e2a\u5b50\u6a21\u51fd\u6570\u4e4b\u5dee\uff08DS\uff09\u7684\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684DC\u7b97\u6cd5\u53d8\u4f53\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u6574\u6570\u538b\u7f29\u611f\u77e5\u548c\u6574\u6570\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u4e2d\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u5b50\u6a21\u51fd\u6570\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u51fa\u73b0\uff0c\u800c\u5bf9\u4e24\u4e2a\u5b50\u6a21\u51fd\u6570\u4e4b\u5dee\uff08DS\uff09\u7684\u6700\u5c0f\u5316\u95ee\u9898\u7684\u7814\u7a76\u53ef\u4ee5\u6269\u5c55\u5148\u524d\u4ec5\u9650\u4e8e\u96c6\u5408\u51fd\u6570\u7684\u5de5\u4f5c\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc1\u660e\u4e86\u6240\u6709\u79bb\u6563\u57df\u4e0a\u7684\u51fd\u6570\u548c\u6240\u6709\u8fde\u7eed\u57df\u4e0a\u7684\u5149\u6ed1\u51fd\u6570\u90fd\u662fDS\u51fd\u6570\u3002\u5bf9\u4e8e\u79bb\u6563\u57df\uff0c\u53d1\u73b0DS\u6700\u5c0f\u5316\u7b49\u4ef7\u4e8e\u4e24\u4e2a\u51f8\u51fd\u6570\u4e4b\u5dee\uff08DC\uff09\u7684\u6700\u5c0f\u5316\u3002\u63a5\u7740\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684DC\u7b97\u6cd5\uff08DCA\uff09\u53d8\u4f53\u5e76\u5c06\u5176\u5e94\u7528\u4e8eDC\u89c4\u5212\uff0c\u83b7\u5f97\u4e86\u4e0e\u96c6\u5408\u51fd\u6570\u60c5\u51b5\u76f8\u5f53\u7684\u7406\u8bba\u4fdd\u8bc1\u3002\u8be5\u7b97\u6cd5\u53ef\u4ee5\u901a\u8fc7\u79bb\u6563\u5316\u5e94\u7528\u4e8e\u8fde\u7eed\u57df\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6574\u6570\u538b\u7f29\u611f\u77e5\u548c\u6574\u6570\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06DS\u6700\u5c0f\u5316\u95ee\u9898\u4ece\u96c6\u5408\u51fd\u6570\u6269\u5c55\u5230\u8fde\u7eed\u548c\u79bb\u6563\u57df\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7b97\u6cd5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.07900", "pdf": "https://arxiv.org/pdf/2506.07900", "abs": "https://arxiv.org/abs/2506.07900", "authors": ["MiniCPM Team", "Chaojun Xiao", "Yuxuan Li", "Xu Han", "Yuzhuo Bai", "Jie Cai", "Haotian Chen", "Wentong Chen", "Xin Cong", "Ganqu Cui", "Ning Ding", "Shengdan Fan", "Yewei Fang", "Zixuan Fu", "Wenyu Guan", "Yitong Guan", "Junshao Guo", "Yufeng Han", "Bingxiang He", "Yuxiang Huang", "Cunliang Kong", "Qiuzuo Li", "Siyuan Li", "Wenhao Li", "Yanghao Li", "Yishan Li", "Zhen Li", "Dan Liu", "Biyuan Lin", "Yankai Lin", "Xiang Long", "Quanyu Lu", "Yaxi Lu", "Peiyan Luo", "Hongya Lyu", "Litu Ou", "Yinxu Pan", "Zekai Qu", "Qundong Shi", "Zijun Song", "Jiayuan Su", "Zhou Su", "Ao Sun", "Xianghui Sun", "Peijun Tang", "Fangzheng Wang", "Feng Wang", "Shuo Wang", "Yudong Wang", "Yesai Wu", "Zhenyu Xiao", "Jie Xie", "Zihao Xie", "Yukun Yan", "Jiarui Yuan", "Kaihuo Zhang", "Lei Zhang", "Linyue Zhang", "Xueren Zhang", "Yudi Zhang", "Hengyu Zhao", "Weilin Zhao", "Weilun Zhao", "Yuanqian Zhao", "Zhi Zheng", "Ge Zhou", "Jie Zhou", "Wei Zhou", "Zihan Zhou", "Zixuan Zhou", "Zhiyuan Liu", "Guoyang Zeng", "Chao Jia", "Dahai Li", "Maosong Sun"], "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "categories": ["cs.CL", "cs.AI"], "comment": "MiniCPM4 Technical Report", "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.", "AI": {"tldr": "This paper presents MiniCPM4, a compact and efficient large language model designed for end-side devices. It introduces innovations in model architecture (InfLLM v2), training data (UltraClean and UltraChat v2), training algorithms (ModelTunnel v2, chunk-wise rollout, and BitCPM), and inference systems (CPM.cu). MiniCPM4 is available in two versions (0.5B and 8B parameters) and outperforms similar-sized open-source models on multiple benchmarks.", "motivation": "To create a highly efficient large language model suitable for end-side devices, which can handle long-context processing effectively and support diverse applications.", "method": "1. **Model Architecture**: Developed InfLLM v2, a trainable sparse attention mechanism that accelerates prefilling and decoding phases.\n2. **Training Data**: Proposed UltraClean for filtering and generating pre-training data, and UltraChat v2 as a supervised fine-tuning dataset.\n3. **Training Algorithms**: Introduced ModelTunnel v2 for efficient pre-training strategy search, chunk-wise rollout for load-balanced reinforcement learning, and BitCPM for data-efficient ternary LLMs.\n4. **Inference Systems**: Created CPM.cu to integrate sparse attention, model quantization, and speculative sampling for efficient prefilling and decoding.", "result": "MiniCPM4 outperforms similar-sized open-source models across multiple benchmarks, with notable speed improvements over Qwen3-8B when processing long sequences. The model successfully powers diverse applications such as trustworthy survey generation and tool use with model context protocol.", "conclusion": "MiniCPM4 demonstrates both efficiency and effectiveness, making it suitable for various on-device requirements and applications."}}
{"id": "2506.07956", "pdf": "https://arxiv.org/pdf/2506.07956", "abs": "https://arxiv.org/abs/2506.07956", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "title": "Language Models over Canonical Byte-Pair Encodings", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "ICML 2025", "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "AI": {"tldr": "Modern language models have a problem where they assign probability to invalid token strings. This paper proposes two methods to fix this issue, improving model performance.", "motivation": "Current language models assign nonzero probability to many invalid (noncanonical) token encodings which never appear in training data, leading to erroneous and wasteful probability mass distribution.", "method": "Two approaches are proposed: 1) canonicality by conditioning, using test-time inference strategies without extra training; 2) canonicality by construction, parameterizing the model to ensure only canonical outputs through training.", "result": "Fixing canonicality mistakes enhances the likelihood of held-out data across several models and corpora.", "conclusion": "Enforcing canonicality in token-level language models can avoid erroneous and wasteful probability mass allocation, thereby improving model performance."}}
{"id": "2506.07981", "pdf": "https://arxiv.org/pdf/2506.07981", "abs": "https://arxiv.org/abs/2506.07981", "authors": ["Dmitrii Vorobev", "Artem Prosvetov", "Karim Elhadji Daou"], "title": "Real-time Localization of a Soccer Ball from a Single Camera", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u4e2a\u5e7f\u64ad\u6444\u50cf\u673a\u8fdb\u884c\u5b9e\u65f6\u4e09\u7ef4\u8db3\u7403\u8f68\u8ff9\u91cd\u5efa\u7684\u8ba1\u7b97\u9ad8\u6548\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u6a21\u5f0f\u72b6\u6001\u6a21\u578b\u663e\u8457\u52a0\u901f\u4f18\u5316\u5e76\u4fdd\u6301\u5398\u7c73\u7ea7\u7cbe\u5ea6\u3002\u7cfb\u7edf\u5728\u6807\u51c6CPU\u4e0a\u8fd0\u884c\uff0c\u4f4e\u5ef6\u8fdf\u9002\u7528\u4e8e\u73b0\u573a\u76f4\u64ad\u73af\u5883\uff0c\u5e76\u4e14\u5728\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\u5176\u6027\u80fd\u53ef\u4e0e\u591a\u6444\u50cf\u673a\u7cfb\u7edf\u76f8\u5ab2\u7f8e\u3002", "motivation": "\u73b0\u6709\u7684\u8db3\u7403\u8f68\u8ff9\u91cd\u5efa\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u591a\u6444\u50cf\u673a\u7cfb\u7edf\u6216\u4e13\u7528\u6602\u8d35\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u8005\u4eec\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u5355\u4e2a\u5e7f\u64ad\u6444\u50cf\u673a\u4e0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u6210\u672c\u76843D\u7403\u8ddf\u8e2a\u6280\u672f\u3002", "method": "\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u5177\u6709W\u4e2a\u79bb\u6563\u6a21\u5f0f\u7684\u591a\u6a21\u5f0f\u72b6\u6001\u6a21\u578b\uff0c\u4ee5\u52a0\u901f\u4f18\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u7559\u5398\u7c73\u7ea7\u7684\u7cbe\u5ea6\u3002\u6b64\u6a21\u578b\u53ef\u4ee5\u5904\u7406\u4e25\u91cd\u906e\u6321\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u590d\u6742\u80cc\u666f\u7b49\u95ee\u9898\u3002\u7cfb\u7edf\u5728\u6807\u51c6CPU\u4e0a\u8fd0\u884c\uff0c\u80fd\u591f\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\uff0c\u9002\u5408\u7528\u4e8e\u73b0\u573a\u76f4\u64ad\u73af\u5883\u3002", "result": "\u901a\u8fc7\u57286K\u5206\u8fa8\u7387\u4fc4\u7f57\u65af\u8d85\u7ea7\u8054\u8d5b\u6bd4\u8d5b\u7684\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u7684\u6027\u80fd\u53ef\u4ee5\u4e0e\u591a\u6444\u50cf\u673a\u7cfb\u7edf\u76f8\u5ab2\u7f8e\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u7279\u6b8a\u7684\u6216\u6602\u8d35\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4e13\u4e1a\u8db3\u7403\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u8bbf\u95ee\u4e14\u51c6\u786e\u76843D\u7403\u8ddf\u8e2a\u3002"}}
{"id": "2506.07984", "pdf": "https://arxiv.org/pdf/2506.07984", "abs": "https://arxiv.org/abs/2506.07984", "authors": ["Mingquan Lin", "Gregory Holste", "Song Wang", "Yiliang Zhou", "Yishu Wei", "Imon Banerjee", "Pengyi Chen", "Tianjie Dai", "Yuexi Du", "Nicha C. Dvornek", "Yuyan Ge", "Zuowei Guo", "Shouhei Hanaoka", "Dongkyun Kim", "Pablo Messina", "Yang Lu", "Denis Parra", "Donghyun Son", "\u00c1lvaro Soto", "Aisha Urooj", "Ren\u00e9 Vidal", "Yosuke Yamagishi", "Zefan Yang", "Ruichi Zhang", "Yang Zhou", "Leo Anthony Celi", "Ronald M. Summers", "Zhiyong Lu", "Hao Chen", "Adam Flanders", "George Shih", "Zhangyang Wang", "Yifan Peng"], "title": "CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 3 figures", "summary": "The CXR-LT series is a community-driven initiative designed to enhance lung\ndisease classification using chest X-rays (CXR). It tackles challenges in open\nlong-tailed lung disease classification and enhances the measurability of\nstate-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve\nthese goals by providing high-quality benchmark CXR data for model development\nand conducting comprehensive evaluations to identify ongoing issues impacting\nlung disease classification performance. Building on the success of CXR-LT\n2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45\ndisease labels, including 19 new rare disease findings. It also introduces a\nnew focus on zero-shot learning to address limitations identified in the\nprevious event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed\nclassification on a large, noisy test set, (ii) long-tailed classification on a\nmanually annotated \"gold standard\" subset, and (iii) zero-shot generalization\nto five previously unseen disease findings. This paper provides an overview of\nCXR-LT 2024, detailing the data curation process and consolidating\nstate-of-the-art solutions, including the use of multimodal models for rare\ndisease detection, advanced generative approaches to handle noisy labels, and\nzero-shot learning strategies for unseen diseases. Additionally, the expanded\ndataset enhances disease coverage to better represent real-world clinical\nsettings, offering a valuable resource for future research. By synthesizing the\ninsights and innovations of participating teams, we aim to advance the\ndevelopment of clinically realistic and generalizable diagnostic models for\nchest radiography.", "AI": {"tldr": "CXR-LT 2024 is an initiative that expands a dataset to 377,110 CXRs and 45 disease labels for improving lung disease classification using chest X-rays. It introduces zero-shot learning and provides three tasks.", "motivation": "To enhance lung disease classification using chest X-rays by tackling challenges in open long-tailed lung disease classification and improving the measurability of state-of-the-art techniques.", "method": "Providing high-quality benchmark CXR data, conducting comprehensive evaluations, expanding the dataset to include more disease labels and introducing zero-shot learning.", "result": "Details the data curation process and consolidates state-of-the-art solutions including multimodal models, advanced generative approaches, and zero-shot learning strategies.", "conclusion": "Aims to advance the development of clinically realistic and generalizable diagnostic models for chest radiography."}}
{"id": "2506.07935", "pdf": "https://arxiv.org/pdf/2506.07935", "abs": "https://arxiv.org/abs/2506.07935", "authors": ["Pavel Naumov", "Jia Tao"], "title": "Diffusion of Responsibility in Collective Decision Making", "categories": ["cs.MA", "cs.AI", "cs.GT"], "comment": null, "summary": "The term \"diffusion of responsibility'' refers to situations in which\nmultiple agents share responsibility for an outcome, obscuring individual\naccountability. This paper examines this frequently undesirable phenomenon in\nthe context of collective decision-making mechanisms.\n  The work shows that if a decision is made by two agents, then the only way to\navoid diffusion of responsibility is for one agent to act as a \"dictator'',\nmaking the decision unilaterally. In scenarios with more than two agents, any\ndiffusion-free mechanism is an \"elected dictatorship'' where the agents elect a\nsingle agent to make a unilateral decision.\n  The technical results are obtained by defining a bisimulation of\ndecision-making mechanisms, proving that bisimulation preserves\nresponsibility-related properties, and establishing the results for a smallest\nbisimular mechanism.", "AI": {"tldr": "\u5728\u591a\u4ee3\u7406\u96c6\u4f53\u51b3\u7b56\u4e2d\uff0c\u907f\u514d\u8d23\u4efb\u6269\u6563\u7684\u552f\u4e00\u65b9\u6cd5\u662f\u5b9e\u884c'\u72ec\u88c1'\u6216'\u9009\u4e3e\u72ec\u88c1'\u3002", "motivation": "\u7814\u7a76\u5728\u96c6\u4f53\u51b3\u7b56\u673a\u5236\u4e2d\u7684'\u8d23\u4efb\u6269\u6563'\u73b0\u8c61\uff0c\u4ee5\u671f\u660e\u786e\u4e2a\u4f53\u8d23\u4efb\u5e76\u6539\u8fdb\u51b3\u7b56\u673a\u5236\u3002", "method": "\u5b9a\u4e49\u4e86\u51b3\u7b56\u673a\u5236\u7684\u53cc\u6a21\u62df\uff08bisimulation\uff09\uff0c\u8bc1\u660e\u53cc\u6a21\u62df\u4fdd\u7559\u4e86\u4e0e\u8d23\u4efb\u76f8\u5173\u7684\u5c5e\u6027\uff0c\u5e76\u5bf9\u6700\u5c0f\u7684\u53cc\u6a21\u62df\u673a\u5236\u5efa\u7acb\u4e86\u7ed3\u679c\u3002", "result": "\u5bf9\u4e8e\u4e24\u4e2a\u4ee3\u7406\u4eba\u7684\u51b3\u7b56\uff0c\u907f\u514d\u8d23\u4efb\u6269\u6563\u7684\u552f\u4e00\u65b9\u6cd5\u662f\u4e00\u4e2a\u4ee3\u7406\u4eba\u5145\u5f53'\u72ec\u88c1\u8005'\uff1b\u5bf9\u4e8e\u8d85\u8fc7\u4e24\u4e2a\u4ee3\u7406\u4eba\u7684\u573a\u666f\uff0c\u4efb\u4f55\u65e0\u8d23\u4efb\u6269\u6563\u7684\u673a\u5236\u90fd\u662f\u4e00\u4e2a'\u9009\u4e3e\u72ec\u88c1'\uff0c\u5373\u4ee3\u7406\u4eba\u9009\u4e3e\u4e00\u540d\u4ee3\u8868\u8fdb\u884c\u5355\u65b9\u9762\u51b3\u7b56\u3002", "conclusion": "\u5728\u96c6\u4f53\u51b3\u7b56\u4e2d\uff0c\u5b8c\u5168\u907f\u514d\u8d23\u4efb\u6269\u6563\u9700\u8981\u727a\u7272\u6c11\u4e3b\u6027\uff0c\u8f6c\u800c\u91c7\u7528\u72ec\u88c1\u5f62\u5f0f\u3002"}}
{"id": "2506.07985", "pdf": "https://arxiv.org/pdf/2506.07985", "abs": "https://arxiv.org/abs/2506.07985", "authors": ["Tuomas Oikarinen", "Ge Yan", "Akshay Kulkarni", "Tsui-Wei Weng"], "title": "Rethinking Crowd-Sourced Evaluation of Neuron Explanations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Interpreting individual neurons or directions in activations space is an\nimportant component of mechanistic interpretability. As such, many algorithms\nhave been proposed to automatically produce neuron explanations, but it is\noften not clear how reliable these explanations are, or which methods produce\nthe best explanations. This can be measured via crowd-sourced evaluations, but\nthey can often be noisy and expensive, leading to unreliable results. In this\npaper, we carefully analyze the evaluation pipeline and develop a\ncost-effective and highly accurate crowdsourced evaluation strategy. In\ncontrast to previous human studies that only rate whether the explanation\nmatches the most highly activating inputs, we estimate whether the explanation\ndescribes neuron activations across all inputs. To estimate this effectively,\nwe introduce a novel application of importance sampling to determine which\ninputs are the most valuable to show to raters, leading to around 30x cost\nreduction compared to uniform sampling. We also analyze the label noise present\nin crowd-sourced evaluations and propose a Bayesian method to aggregate\nmultiple ratings leading to a further ~5x reduction in number of ratings\nrequired for the same accuracy. Finally, we use these methods to conduct a\nlarge-scale study comparing the quality of neuron explanations produced by the\nmost popular methods for two different vision models.", "AI": {"tldr": "\u4e3a\u4e86\u63d0\u9ad8\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u8bc4\u4f30\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cd\u8981\u6027\u91c7\u6837\u7684\u4f17\u5305\u8bc4\u4f30\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u51cf\u5c11\u6240\u9700\u8bc4\u5206\u6570\u91cf\uff0c\u6700\u7ec8\u5bf9\u4e24\u79cd\u89c6\u89c9\u6a21\u578b\u7684\u70ed\u95e8\u795e\u7ecf\u5143\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8d28\u91cf\u6bd4\u8f83\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u4ea7\u751f\u795e\u7ecf\u5143\u89e3\u91ca\u7684\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u9760\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u4f20\u7edf\u7684\u4f17\u5305\u8bc4\u4f30\u5b58\u5728\u566a\u58f0\u4e14\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5206\u6790\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5f15\u5165\u91cd\u8981\u6027\u91c7\u6837\u786e\u5b9a\u6700\u6709\u4ef7\u503c\u7684\u8f93\u5165\u4ee5\u964d\u4f4e30\u500d\u6210\u672c\uff1b\u5206\u6790\u4f17\u5305\u6807\u7b7e\u566a\u58f0\u5e76\u91c7\u7528\u8d1d\u53f6\u65af\u65b9\u6cd5\u805a\u5408\u8bc4\u5206\u4ee5\u8fdb\u4e00\u6b65\u51cf\u5c11\u8bc4\u5206\u9700\u6c425\u500d\u3002", "result": "\u5f00\u53d1\u51fa\u4e00\u79cd\u9ad8\u6027\u4ef7\u6bd4\u7684\u4f17\u5305\u8bc4\u4f30\u7b56\u7565\uff0c\u6210\u529f\u7528\u4e8e\u5927\u89c4\u6a21\u6bd4\u8f83\u4e24\u79cd\u89c6\u89c9\u6a21\u578b\u4e2d\u6d41\u884c\u795e\u7ecf\u5143\u89e3\u91ca\u65b9\u6cd5\u7684\u8d28\u91cf\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684\u8bc4\u4f30\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u795e\u7ecf\u5143\u89e3\u91ca\u8bc4\u4f30\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u5de5\u5177\u3002"}}
{"id": "2506.07999", "pdf": "https://arxiv.org/pdf/2506.07999", "abs": "https://arxiv.org/abs/2506.07999", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "AI": {"tldr": "MADFormer, a Mixed Autoregressive and Diffusion Transformer, partitions image generation into spatial blocks using AR and diffusion layers for global conditioning and local refinement respectively. Block-wise partitioning improves high-resolution image performance, while vertically mixing AR and diffusion layers offers better quality-efficiency balances.", "motivation": "Existing hybrids of autoregressive (AR) and diffusion-based approaches in multimodal generation often lack systematic guidance on how to allocate model capacity between these paradigms.", "method": "MADFormer partitions image generation into spatial blocks, using AR layers for one-pass global conditioning across blocks and diffusion layers for iterative local refinement within each block.", "result": "Block-wise partitioning significantly improves performance on high-resolution images. Vertically mixing AR and diffusion layers yields better quality-efficiency balances--improving FID by up to 75% under constrained inference compute.", "conclusion": "The findings offer practical design principles for future hybrid generative models."}}
{"id": "2506.07943", "pdf": "https://arxiv.org/pdf/2506.07943", "abs": "https://arxiv.org/abs/2506.07943", "authors": ["Yizhen Li", "Dell Zhang", "Xuelong Li", "Yiqing Shen"], "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "AI": {"tldr": "DTwinSeger\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u5206\u5272\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u8868\u793a\u4f5c\u4e3a\u4e2d\u95f4\u5c42\u6765\u89e3\u8026\u611f\u77e5\u548c\u63a8\u7406\uff0c\u5c06RS\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff0c\u9996\u5148\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\u548c\u8bed\u4e49\u5c5e\u6027\u7684\u7ed3\u6784\u5316\u6570\u5b57\u5b6a\u751f\u8868\u793a\uff0c\u7136\u540e\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6b64\u8868\u793a\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u4ee5\u8bc6\u522b\u76ee\u6807\u5bf9\u8c61\u3002\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u56fe\u50cf\u63a8\u7406\u5206\u5272\u57fa\u51c6\u548c\u4e09\u4e2a\u56fe\u50cf\u6307\u4ee3\u5206\u5272\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684\u63a8\u7406\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u6765\u8fdb\u884c\u611f\u77e5\u548c\u63a8\u7406\uff0c\u4f46\u5176\u56fe\u50cf\u7684\u6807\u8bb0\u5316\u4ece\u6839\u672c\u4e0a\u7834\u574f\u4e86\u7269\u4f53\u4e4b\u95f4\u7684\u8fde\u7eed\u7a7a\u95f4\u5173\u7cfb\u3002", "method": "DTwinSeger\u5f15\u5165\u4e86\u6570\u5b57\u5b6a\u751f\u8868\u793a\u4f5c\u4e3a\u4e2d\u95f4\u5c42\u6765\u89e3\u8026\u611f\u77e5\u548c\u63a8\u7406\uff0c\u5e76\u5c06RS\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4e24\u9636\u6bb5\u8fc7\u7a0b\uff1a\u7b2c\u4e00\u9636\u6bb5\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\u548c\u8bed\u4e49\u5c5e\u6027\u7684\u7ed3\u6784\u5316\u6570\u5b57\u5b6a\u751f\u8868\u793a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8be5\u8868\u793a\u8fdb\u884c\u663e\u5f0f\u63a8\u7406\u4ee5\u8bc6\u522b\u76ee\u6807\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u5177\u6709\u6570\u5b57\u5b6a\u751f\u8868\u793a\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u4ee5\u53ca\u76f8\u5e94\u7684\u5fae\u8c03\u6570\u636e\u96c6Seg-DT\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e24\u4e2a\u56fe\u50cf\u63a8\u7406\u5206\u5272\u57fa\u51c6\u548c\u4e09\u4e2a\u56fe\u50cf\u6307\u4ee3\u5206\u5272\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u6570\u5b57\u5b6a\u751f\u8868\u793a\u4f5c\u4e3a\u89c6\u89c9\u548c\u6587\u672c\u4e4b\u95f4\u6709\u6548\u7684\u6865\u6881\uff0c\u4f7f\u590d\u6742\u7684\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u80fd\u591f\u4ec5\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b8c\u6210\u3002", "conclusion": "DTwinSeger\u901a\u8fc7\u5f15\u5165\u6570\u5b57\u5b6a\u751f\u8868\u793a\u4f5c\u4e3a\u4e2d\u95f4\u5c42\uff0c\u6210\u529f\u5730\u89e3\u8026\u4e86\u611f\u77e5\u4e0e\u63a8\u7406\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u6570\u5b57\u5b6a\u751f\u8868\u793a\u5728\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.08008", "pdf": "https://arxiv.org/pdf/2506.08008", "abs": "https://arxiv.org/abs/2506.08008", "authors": ["Stephanie Fu", "Tyler Bonnen", "Devin Guillory", "Trevor Darrell"], "title": "Hidden in plain sight: VLMs overlook their visual representations", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://hidden-plain-sight.github.io/", "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.", "AI": {"tldr": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdc\u4e0d\u5982\u5176\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4e3b\u8981\u95ee\u9898\u5728\u4e8e\u672a\u80fd\u6709\u6548\u5229\u7528\u89c6\u89c9\u4fe1\u606f\uff0c\u800c\u662f\u7ee7\u627f\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5148\u9a8c\u3002", "motivation": "\u7814\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u6574\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u5e76\u8bca\u65ad\u5f00\u6e90VLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u5c06VLMs\u4e0e\u5176\u89c6\u89c9\u7f16\u7801\u5668\u8fdb\u884c\u6bd4\u8f83\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u89c6\u89c9\u4e2d\u5fc3\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982\u6df1\u5ea6\u4f30\u8ba1\u3001\u5bf9\u5e94\u5173\u7cfb\u7b49\uff09\uff0c\u5206\u6790VLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u8fdb\u4e00\u6b65\u4ece\u89c6\u89c9\u8868\u793a\u9000\u5316\u3001\u4efb\u52a1\u63d0\u793a\u7684\u8106\u5f31\u6027\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u89e3\u51b3\u4e2d\u7684\u4f5c\u7528\u4e09\u4e2a\u65b9\u9762\u8fdb\u884c\u6df1\u5165\u5206\u6790\u3002", "result": "\u53d1\u73b0VLMs\u5728\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u5176\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u51e0\u4e4e\u63a5\u8fd1\u968f\u673a\u6c34\u5e73\u3002\u95ee\u9898\u4e3b\u8981\u96c6\u4e2d\u5728\u8bed\u8a00\u6a21\u578b\u90e8\u5206\uff0cVLMs\u672a\u80fd\u6709\u6548\u5229\u7528\u6574\u4e2a\u6a21\u578b\u4e2d\u53ef\u83b7\u5f97\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u53cd\u800c\u7ee7\u627f\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u5148\u9a8c\u3002", "conclusion": "\u672c\u7814\u7a76\u6709\u52a9\u4e8e\u8bca\u65ad\u5f00\u6e90VLMs\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5bf9\u672a\u6765VLMs\u89c6\u89c9\u7406\u89e3\u7814\u7a76\u6709\u7528\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2506.07945", "pdf": "https://arxiv.org/pdf/2506.07945", "abs": "https://arxiv.org/abs/2506.07945", "authors": ["Arnav Sheth", "Ivaxi Sheth", "Mario Fritz"], "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols", "categories": ["cs.AR", "cs.AI", "cs.CL"], "comment": "Accepted at MLSysArch@ISCA 2025", "summary": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches.", "AI": {"tldr": "The paper explores the capabilities of LLMs in generating SystemVerilog implementations for communication protocols, introducing a benchmark suite for SPI, I2C, UART, and AXI protocols.", "motivation": "Recent advances in Large Language Models (LLMs) have shown promising capabilities in generating code for general-purpose programming languages. However, their applicability for hardware description languages remains underexplored.", "method": "The paper introduces a benchmark suite targeting four widely used protocols: SPI, I2C, UART, and AXI. It defines code generation tasks that capture varying levels of design abstraction and prompt specificity.", "result": "The generated designs are assessed for syntactic correctness, synthesizability, and functional fidelity via waveform simulation and test benches.", "conclusion": "This study aims to analyze the capabilities of state-of-the-art LLMs in generating SystemVerilog implementations of standard communication protocols."}}
{"id": "2506.08009", "pdf": "https://arxiv.org/pdf/2506.08009", "abs": "https://arxiv.org/abs/2506.08009", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project website: http://self-forcing.github.io/", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "AI": {"tldr": "The paper introduces Self Forcing, a new training method for autoregressive video diffusion models that tackles exposure bias and achieves real-time streaming video generation.", "motivation": "To address the issue of exposure bias in autoregressive video diffusion models, where models must generate sequences conditioned on their own imperfect outputs during inference rather than ground-truth context.", "method": "Self Forcing conditions each frame's generation on previously self-generated outputs using autoregressive rollout with key-value (KV) caching during training. It employs a holistic loss at the video level, a few-step diffusion model, stochastic gradient truncation, and a rolling KV cache mechanism.", "result": "Self Forcing enables real-time streaming video generation with sub-second latency on a single GPU while matching or surpassing the quality of slower non-causal diffusion models.", "conclusion": "Self Forcing is an effective solution to exposure bias in video diffusion models, achieving high-quality real-time video generation."}}
{"id": "2506.07961", "pdf": "https://arxiv.org/pdf/2506.07961", "abs": "https://arxiv.org/abs/2506.07961", "authors": ["Peiyan Li", "Yixiang Chen", "Hongtao Wu", "Xiao Ma", "Xiangnan Wu", "Yan Huang", "Liang Wang", "Tao Kong", "Tieniu Tan"], "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "In Submission", "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/", "AI": {"tldr": "BridgeVLA is a novel 3D vision-language-action (VLA) model that efficiently incorporates 3D signals into pre-trained vision-language models (VLMs) through projecting 3D inputs to 2D images and utilizing 2D heatmaps for action prediction. It demonstrates superior performance in simulation benchmarks and real-robot experiments, with remarkable sample efficiency.", "motivation": "Current methods of incorporating 3D signals into VLMs for action prediction do not fully leverage the spatial structure inherent in 3D data, resulting in low sample efficiency.", "method": "The BridgeVLA model projects 3D inputs to multiple 2D images for alignment with the VLM backbone and uses 2D heatmaps for action prediction, unifying input and output spaces within a consistent 2D image space. A scalable pre-training method is proposed to equip the VLM backbone with the capability to predict 2D heatmaps before downstream policy learning.", "result": "BridgeVLA outperforms state-of-the-art baseline methods across three simulation benchmarks: RLBench, COLOSSEUM, and GemBench. In real-robot experiments, it achieves an average success rate improvement of 32% over the best baseline, generalizes robustly in out-of-distribution settings, and achieves a success rate of 96.8% on 10+ tasks with only 3 trajectories per task.", "conclusion": "BridgeVLA effectively leverages 3D data for robot manipulation learning, demonstrating superior performance and sample efficiency compared to existing methods."}}
{"id": "2506.08013", "pdf": "https://arxiv.org/pdf/2506.08013", "abs": "https://arxiv.org/abs/2506.08013", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/astra-vision/StableMTL", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "AI": {"tldr": "StableMTL is a multi-task learning method that uses diffusion models for zero-shot learning, with a unified latent loss and task-attention mechanism, outperforming baselines on various benchmarks.", "motivation": "Multi-task learning for dense prediction is constrained by the need for extensive annotations. Recent works have explored training with partial task labels, but there is a need to extend this to a zero-shot setting where tasks can be performed without specific task annotations during training.", "method": "The method, StableMTL, repurposes image generators for latent regression using a denoising framework with task encoding and per-task conditioning. It adopts a unified latent loss instead of individual task losses, allowing it to scale to more tasks. A multi-stream model with a task-attention mechanism is introduced to promote cross-task sharing and synergy.", "result": "StableMTL outperforms baseline methods on 7 tasks across 8 different benchmarks.", "conclusion": "StableMTL successfully extends multi-task learning to a zero-shot setting, leveraging diffusion models and introducing innovations such as a unified latent loss and task-attention mechanism, achieving superior performance."}}
{"id": "2506.07962", "pdf": "https://arxiv.org/pdf/2506.07962", "abs": "https://arxiv.org/abs/2506.07962", "authors": ["Elliot Kim", "Avi Garg", "Kenny Peng", "Nikhil Garg"], "title": "Correlated Errors in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.", "AI": {"tldr": "\u5c3d\u7ba1\u8bad\u7ec3\u6570\u636e\u3001\u67b6\u6784\u548c\u4f9b\u5e94\u5546\u7684\u591a\u6837\u6027\u88ab\u8ba4\u4e3a\u662f\u7f13\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u540c\u8d28\u5316\u7684\u5173\u952e\uff0c\u4f46\u4e0d\u540cLLM\u4e4b\u95f4\u662f\u5426\u5177\u6709\u5b9e\u8d28\u6027\u5dee\u5f02\u5c1a\u7f3a\u4e4f\u5b9e\u8bc1\u8bc1\u636e\u3002\u901a\u8fc7\u5bf9\u8d85\u8fc7350\u4e2aLLM\u7684\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u9519\u8bef\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u5728\u66f4\u51c6\u786e\u7684\u5927\u578b\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u67b6\u6784\u548c\u4f9b\u5e94\u5546\u4e0d\u540c\uff0c\u9519\u8bef\u4e5f\u9ad8\u5ea6\u76f8\u5173\u3002\u8fd9\u79cd\u76f8\u5173\u6027\u5728\u4e24\u4e2a\u4e0b\u6e38\u4efb\u52a1\uff08LLM-as-judge\u8bc4\u4f30\u548c\u62db\u8058\uff09\u4e2d\u4ea7\u751f\u4e86\u5f71\u54cd\uff0c\u53cd\u6620\u4e86\u7b97\u6cd5\u5355\u4e00\u79cd\u7fa4\u7684\u7406\u8bba\u9884\u6d4b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\uff0c\u63a2\u8ba8\u4e0d\u540cLLM\u4e4b\u95f4\u7684\u5b9e\u8d28\u6027\u5dee\u5f02\u53ca\u5176\u9519\u8bef\u76f8\u5173\u6027\u7684\u9a71\u52a8\u56e0\u7d20\uff0c\u4ee5\u9a8c\u8bc1\u591a\u6837\u6027\u662f\u5426\u80fd\u6709\u6548\u7f13\u89e3\u6a21\u578b\u540c\u8d28\u5316\u95ee\u9898\u3002", "method": "\u5bf9\u8d85\u8fc7350\u4e2aLLM\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u4f7f\u7528\u4e24\u4e2a\u6d41\u884c\u7684\u6392\u884c\u699c\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u7b80\u5386\u7b5b\u9009\u4efb\u52a1\uff0c\u5206\u6790\u6a21\u578b\u9519\u8bef\u7684\u76f8\u5173\u6027\uff0c\u5e76\u8bc6\u522b\u5bfc\u81f4\u8fd9\u79cd\u76f8\u5173\u6027\u7684\u56e0\u7d20\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u9519\u8bef\u5b58\u5728\u663e\u8457\u76f8\u5173\u6027\uff0c\u5728\u4e00\u4e2a\u6392\u884c\u699c\u6570\u636e\u96c6\u4e2d\uff0c\u5f53\u4e24\u4e2a\u6a21\u578b\u90fd\u51fa\u9519\u65f6\uff0c\u5b83\u4eec\u7684\u5224\u65ad\u4e00\u81f4\u7387\u8fbe\u523060%\u3002\u7814\u7a76\u8868\u660e\uff0c\u5171\u4eab\u67b6\u6784\u548c\u4f9b\u5e94\u5546\u662f\u9a71\u52a8\u6a21\u578b\u76f8\u5173\u6027\u7684\u56e0\u7d20\uff0c\u800c\u66f4\u5927\u3001\u66f4\u51c6\u786e\u7684\u6a21\u578b\u5373\u4f7f\u5728\u4e0d\u540c\u67b6\u6784\u548c\u4f9b\u5e94\u5546\u7684\u60c5\u51b5\u4e0b\uff0c\u9519\u8bef\u4e5f\u9ad8\u5ea6\u76f8\u5173\u3002", "conclusion": "LLM\u4e4b\u95f4\u7684\u9519\u8bef\u76f8\u5173\u6027\u663e\u8457\uff0c\u7279\u522b\u662f\u9ad8\u6027\u80fd\u6a21\u578b\uff0c\u8fd9\u8868\u660e\u5355\u7eaf\u4f9d\u9760\u591a\u6837\u6027\u53ef\u80fd\u4e0d\u8db3\u4ee5\u7f13\u89e3\u6a21\u578b\u540c\u8d28\u5316\u95ee\u9898\u3002\u8fd9\u79cd\u76f8\u5173\u6027\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u98ce\u9669\uff0c\u4f8b\u5982\u5728\u62db\u8058\u7b49\u573a\u666f\u4e2d\u53cd\u6620\u7b97\u6cd5\u5355\u4e00\u79cd\u7fa4\u7684\u5f71\u54cd\u3002"}}
{"id": "2506.07964", "pdf": "https://arxiv.org/pdf/2506.07964", "abs": "https://arxiv.org/abs/2506.07964", "authors": ["Wenxin Tang", "Jingyu Xiao", "Wenxuan Jiang", "Xi Xiao", "Yuhang Wang", "Xuxin Tang", "Qing Li", "Yuehe Ma", "Junliang Liu", "Shisong Tang", "Michael R. Lyu"], "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.", "AI": {"tldr": "This paper proposes Slide2Code, a benchmark for Reference Image to Slide Generation task with a new Slide Complexity Metric. It also introduces SlideCoder, a framework using Color Gradient-based Segmentation and Hierarchical Retrieval-Augmented Generation methods for creating editable slides from reference images. Additionally, the 7B-parameter SlideMaster model is released. Experiments indicate SlideCoder significantly outperforms existing baselines.", "motivation": "Slide creation manually is labor-intensive and demands expert knowledge. Current natural language-based LLM generation methods fail to capture the visual and structural nuances of slide designs.", "method": "The authors formalize the Reference Image to Slide Generation task and propose Slide2Code benchmark with difficulty-tiered samples based on a novel Slide Complexity Metric. They introduce SlideCoder, a layout-aware, retrieval-augmented framework integrating Color Gradient-based Segmentation algorithm and Hierarchical Retrieval-Augmented Generation method. They also release SlideMaster, an open-source model fine-tuned with improved reverse-engineered data.", "result": "Experiments show that SlideCoder surpasses state-of-the-art baselines by up to 40.5 points across layout fidelity, execution accuracy, and visual consistency.", "conclusion": "The proposed SlideCoder framework effectively generates editable slides from reference images and outperforms existing methods. The associated Slide2Code benchmark and SlideMaster model provide valuable resources for future research."}}
{"id": "2506.08003", "pdf": "https://arxiv.org/pdf/2506.08003", "abs": "https://arxiv.org/abs/2506.08003", "authors": ["Shuchen Weng", "Haojie Zheng", "Zheng Chang", "Si Li", "Boxin Shi", "Xinlong Wang"], "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.", "AI": {"tldr": "The paper introduces MTV, a framework for audio-sync video generation that separates audio into speech, effects, and music tracks for fine-grained control, and DEMIX, a dataset supporting scalable training. Experiments show state-of-the-art performance in various metrics.", "motivation": "Existing methods fail to generate high-quality videos with precise audio-visual synchronization, especially across diverse and complex audio types.", "method": "MTV explicitly separates audios into speech, effects, and music tracks, enabling disentangled control over lip motion, event timing, and visual mood respectively.", "result": "MTV achieves state-of-the-art performance across six standard metrics spanning video quality, text-video consistency, and audio-video alignment.", "conclusion": "MTV is a versatile framework for audio-sync video generation and DEMIX is a valuable dataset for scalable multi-stage training."}}
{"id": "2506.08004", "pdf": "https://arxiv.org/pdf/2506.08004", "abs": "https://arxiv.org/abs/2506.08004", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "title": "Dynamic View Synthesis as an Inverse Problem", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://inverse-dvs.github.io/", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "AI": {"tldr": "This paper presents a method for dynamic view synthesis from monocular videos without training, by redesigning the noise initialization phase of a pre-trained video diffusion model.", "motivation": "To achieve high-fidelity dynamic view synthesis from monocular videos without any weight updates or auxiliary modules, addressing the issue of deterministic inversion arising from zero-terminal signal-to-noise ratio (SNR) schedules.", "method": "Redesign the noise initialization phase using K-order Recursive Noise Representation and Stochastic Latent Modulation to perform visibility aware sampling over the latent space.", "result": "Comprehensive experiments demonstrate effective dynamic view synthesis through structured latent manipulation in the noise initialization phase.", "conclusion": "Dynamic view synthesis can be successfully performed without training by manipulating latents in the noise initialization phase of a pre-trained video diffusion model."}}
{"id": "2506.08010", "pdf": "https://arxiv.org/pdf/2506.08010", "abs": "https://arxiv.org/abs/2506.08010", "authors": ["Nick Jiang", "Amil Dravid", "Alexei Efros", "Yossi Gandelsman"], "title": "Vision Transformers Don't Need Trained Registers", "categories": ["cs.CV", "cs.AI"], "comment": "Project page and code: https://avdravid.github.io/test-time-registers", "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.", "AI": {"tldr": "The paper investigates high-norm tokens in Vision Transformers that cause noisy attention maps and proposes a training-free solution to mitigate this issue.", "motivation": "To address the problem of high-norm tokens leading to noisy attention maps in Vision Transformers without requiring retraining of models.", "method": "By shifting high-norm activations from specific neurons into an additional untrained token, effectively mimicking register tokens in models not originally trained with them.", "result": "Produces cleaner attention and feature maps, enhances performance on downstream tasks, and achieves results comparable to models trained with register tokens. Also improves interpretability in vision-language models.", "conclusion": "Test-time registers can effectively replace register tokens at test-time, providing a training-free solution for pre-trained models."}}
